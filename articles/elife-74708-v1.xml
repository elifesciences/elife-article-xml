<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74708</article-id><article-id pub-id-type="doi">10.7554/eLife.74708</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Distance estimation from monocular cues in an ethological visuomotor task</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-258236"><name><surname>Parker</surname><given-names>Philip RL</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6224-9747</contrib-id><email>prlparker@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-182025"><name><surname>Abe</surname><given-names>Elliott TT</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-258237"><name><surname>Beatie</surname><given-names>Natalie T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-258238"><name><surname>Leonard</surname><given-names>Emmalyn SP</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-258239"><name><surname>Martins</surname><given-names>Dylan M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-258240"><name><surname>Sharp</surname><given-names>Shelby L</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-258241"><name><surname>Wyrick</surname><given-names>David G</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-134069"><name><surname>Mazzucato</surname><given-names>Luca</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8525-7539</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-21688"><name><surname>Niell</surname><given-names>Cristopher M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6283-3540</contrib-id><email>cniell@uoregon.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0293rh119</institution-id><institution>Institute of Neuroscience, University of Oregon</institution></institution-wrap><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0293rh119</institution-id><institution>Department of Mathematics, University of Oregon</institution></institution-wrap><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0293rh119</institution-id><institution>Department of Biology, University of Oregon</institution></institution-wrap><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cowan</surname><given-names>Noah J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>20</day><month>09</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e74708</elocation-id><history><date date-type="received" iso-8601-date="2021-10-14"><day>14</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-08-29"><day>29</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-10-01"><day>01</day><month>10</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.29.462468"/></event></pub-history><permissions><copyright-statement>Â© 2022, Parker et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Parker et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-74708-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-74708-figures-v1.pdf"/><abstract><p>In natural contexts, sensory processing and motor output are closely coupled, which is reflected in the fact that many brain areas contain both sensory and movement signals. However, standard reductionist paradigms decouple sensory decisions from their natural motor consequences, and head-fixation prevents the natural sensory consequences of self-motion. In particular, movement through the environment provides a number of depth cues beyond stereo vision that are poorly understood. To study the integration of visual processing and motor output in a naturalistic task, we investigated distance estimation in freely moving mice. We found that mice use vision to accurately jump across a variable gap, thus directly coupling a visual computation to its corresponding ethological motor output. Monocular eyelid suture did not affect gap jumping success, thus mice can use cues that do not depend on binocular disparity and stereo vision. Under monocular conditions, mice altered their head positioning and performed more vertical head movements, consistent with a shift from using stereopsis to other monocular cues, such as motion or position parallax. Finally, optogenetic suppression of primary visual cortex impaired task performance under both binocular and monocular conditions when optical fiber placement was localized to binocular or monocular zone V1, respectively. Together, these results show that mice can use monocular cues, relying on visual cortex, to accurately judge distance. Furthermore, this behavioral paradigm provides a foundation for studying how neural circuits convert sensory information into ethological motor output.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>distance estimation</kwd><kwd>visual cortex</kwd><kwd>ethology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5F32EY027696</award-id><principal-award-recipient><name><surname>Parker</surname><given-names>Philip RL</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R21EY032708</award-id><principal-award-recipient><name><surname>Parker</surname><given-names>Philip RL</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R34NS111669</award-id><principal-award-recipient><name><surname>Niell</surname><given-names>Cristopher M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Mice accurately perform an ethological distance estimation task based on gap jumping and can use monocular cues to estimate distance in addition to binocular cues such as stereopsis.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Vision is an active process â we continuously move our eyes, head, and body to gain information about the world around us. One core function of active vision is to determine the distance between the observer and objects in its environment. This ability is so critical that many species have evolved to use multiple distinct cues to estimate depth, including retinal image size, motion and position parallax, and binocular disparity (<xref ref-type="bibr" rid="bib19">Kral, 2003</xref>; <xref ref-type="bibr" rid="bib37">Shinkman, 1962</xref>). In particular, depth perception through stereo vision has been heavily studied, but other cues that provide important complements are less well understood. Furthermore, some of these monocular cues, such as motion parallax and loom, are closely integrated with movement. How does the brain make use of these diverse cues to guide different behaviors? For instance, is distance explicitly computed and represented in neural activity for some behaviors and implicitly encoded for others? Furthermore, how is this sensory representation converted into the appropriate motor output? Neurophysiological studies are often performed on head-fixed subjects, limiting the range of depth cues and behaviors that can be studied. Addressing these questions requires behaviors where experimental subjects amenable to neural circuit interrogation can engage in distance estimation behaviors unrestrained (<xref ref-type="bibr" rid="bib22">Leopold and Park, 2020</xref>; <xref ref-type="bibr" rid="bib34">Parker et al., 2020</xref>).</p><p>The mouse is an important model for vision, yet relatively few behavioral paradigms exist for studying natural, active vision in the mouse (<xref ref-type="bibr" rid="bib5">Boone et al., 2021</xref>; <xref ref-type="bibr" rid="bib15">Hoy et al., 2016</xref>; <xref ref-type="bibr" rid="bib42">Yilmaz and Meister, 2013</xref>). Previous work in other rodent models, including rats and gerbils, showed that animals will accurately jump to distant platforms for a reward, and that changing experimental conditions can bias animals toward the use of certain depth cues, including monocular ones (<xref ref-type="bibr" rid="bib6">Carey et al., 1990</xref>; <xref ref-type="bibr" rid="bib8">Ellard et al., 1984</xref>; <xref ref-type="bibr" rid="bib12">Goodale et al., 1990</xref>). Here, we report that mice are capable of using vision to estimate the distance across a variable gap and execute an accurate ballistic jump. Using this behavior, we show that mice can use monocular vision to judge distance, and suppressing the activity of primary visual cortex (V1) disrupts task performance. Furthermore, this paradigm provides a foundation for studying various visual computations related to depth, and the corresponding motor output, in a species amenable to measurement and manipulation of neural activity in genetically defined populations.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Mouse distance estimation (jumping) task</title><p>In order to establish a framework for studying distance estimation in the mouse, we adapted a gerbil/rat jumping task (<xref ref-type="bibr" rid="bib8">Ellard et al., 1984</xref>; <xref ref-type="bibr" rid="bib21">Legg and Lambert, 1990</xref>; <xref ref-type="bibr" rid="bib35">Richardson, 1909</xref>), where animals were rewarded for successfully jumping across a variable gap (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Mice were free to roam around the arena, then initiated trials by mounting a take-off platform. An occluding barrier was introduced to block the mouseâs view while the experimenter randomly placed one of three landing platforms at one of seven distances from the take-off platform (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We used landing platforms of variable size to minimize the use of retinal image size cues, which may not require visual cortex for accurate distance estimation after learning (<xref ref-type="bibr" rid="bib6">Carey et al., 1990</xref>). The trial began as soon as the occluding barrier was removed, and the decision period comprised the time between barrier removal and the last video frame before the mouse executed one of three outcomes (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). On âsuccessâ trials, the mouse jumped and landed on the landing platform, and received a reward (see <xref ref-type="video" rid="video1">Video 1</xref>). On âfailureâ trials, the mouse jumped and missed the landing platform, landing on the arena floor, and received no reward. On âabortâ trials, mice dismounted the take-off platform onto the arena floor and received a mild air puff. Training, which usually took one to two weeks, was complete when mice successfully jumped to each of the three landing platforms at the maximum distance (22 cm). To quantify behavior, markerless pose estimation was performed on side- and top-view video with DeepLabCut (DLC; <xref ref-type="bibr" rid="bib27">Mathis et al., 2018</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Mouse distance estimation (jumping) task.</title><p>(<bold>A</bold>) Example side and top-down video frames (three overlaid) from a single trial. (<bold>B</bold>) A random combination of landing platform width (three sizes) and gap distance (seven distances) is chosen for each trial. (<bold>C</bold>) Trial logic.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" id="video1" xlink:href="elife-74708-video1.mp4"><label>Video 1.</label><caption><title>Mouse performing the task under binocular conditions with DeepLabCut labels overlaid.</title></caption></media></sec><sec id="s2-2"><title>Mice accurately estimate distance under binocular and monocular conditions</title><p>Mice successfully jumped to all three sizes of platforms at all gap distances (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, example, blue lines in B top; N = 3580 trials in eight mice), with only a minor effect of gap distance on success rate (ANOVA, <italic>F</italic> = 2.316, p=0.048). On success trials, the distance jumped increased as a function of gap distance (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, blue line in bottom panel; ANOVA, distance <italic>F</italic> = 12.845, p=1.17e-8), showing that mice accurately jumped rather than adapting an alternative strategy (e.g., picking one of two jump forces across the five distances). The gap was too large for mice to reach across with their whiskers, preventing the use of somatosensation to judge the distance. Furthermore, mice did not perform any jumps in the dark (n = 4 mice, four sessions), suggesting that they relied on vision.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Mice accurately judge distance under binocular and monocular conditions.</title><p>(<bold>A</bold>) Example jump trajectories from a single mouse (red line is trajectory of left ear tracked by DeepLabCut, blue dot is end point of jump) at three distances for binocular (top row) and monocular (bottom row) trials. (<bold>B</bold>) Performance (top) and accuracy (bottom) in binocular (blue, n = 8 mice) and monocular (magenta, n = 8 mice) conditions averaged across landing platform widths. Thin lines are individual animal data. (<bold>C</bold>) Performance (top) and distance jumped (bottom) for bi/monocular conditions by landing platform width (indicated by line style). (<bold>D</bold>) Change in the mean landing position (top) and standard deviation of landing position (bottom) for binocular vs. monocular conditions. Smaller points are individual animal data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Binocular vs. monocular task performance.</title><p>(<bold>A</bold>) Rates of failure (left column), success (middle column), and abort trials (right column) for binocular (blue) and monocular (magenta) conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig2-figsupp1-v1.tif"/></fig></fig-group><p>A number of depth cues are available in natural contexts. To test the need for stereopsis, we performed monocular eyelid suture (N = 1613 trials in eight mice), after which mice performed equally well at the task with no significant difference in the fraction of success, failure, and abort trials (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, magenta lines, and <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>; ANOVA binocular vs. monocular, failure <italic>F</italic> = 0.002, p=0.965, success <italic>F</italic> = 0.101, p=0.752, abort <italic>F</italic> = 0.275, p=0.601). There was no effect of gap distance on success (ANOVA, <italic>F</italic> = 1.593, p=0.169), and mice similarly increased distance jumped as a function of gap distance under monocular conditions (ANOVA, <italic>F</italic> = 5.623, p=1.68e-4). These data suggest that binocular vision is not required for accurate distance estimation under these conditions and demonstrate that mice can use monocular cues to accurately judge distance. We also tested for a role of retinal image size by analyzing performance across the three different landing platforms (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Mice performed similarly across all three sizes, suggesting that they did not rely primarily on retinal image size, although distance jumped was influenced by platform size under monocular conditions (ANOVA; success, binocular <italic>F</italic> = 2.345, p=0.099, monocular <italic>F</italic> = 0.774, p=0.463; distance jumped, binocular <italic>F</italic> = 4.436, p=0.013, monocular <italic>F</italic> = 3.261, p=0.041). Finally, to determine whether accuracy or precision were altered after monocular occlusion, we calculated the change in the mean landing position between the two conditions, and the standard deviation of the landing position in each condition (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The standard deviation of landing position was similar across conditions and was greater than the difference in the mean landing position, suggesting that mice were equally accurate and precise after monocular occlusion.</p></sec><sec id="s2-3"><title>Mice perform more head movements under monocular conditions</title><p>To quantify the fine-scale structure of behavior leading up to the jump, we analyzed both the movement and position of the mouse during the decision period (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) as differences between binocular and monocular conditions could indicate a change in the use of visual cues. To analyze movements, we identified zero crossings in the velocity of eye position from the side-view camera data, then took a 500 ms window around these time points, discarding any with vertical amplitudes less than 1 cm (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). We then performed principal component analysis on the concatenated x/y traces, and k-means clustering on the reduced data (k = 10, see âMaterials and methodsâ for details). The resulting movement clusters (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; ordered by total variance, high to low) showed a diversity of trajectories that together capture most of the head movements that the mice made leading up to the jump (example clusters in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref>; see <xref ref-type="video" rid="video2">Video 2</xref>). The average trajectories of these movement clusters were highly similar between the binocular and monocular conditions (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The frequency of movements per trial was significantly increased under monocular conditions across clusters (<xref ref-type="fig" rid="fig3">Figure 3D</xref>; ANOVA; binocular vs. monocular, <italic>F</italic> = 16.633, p=7.58e-5), though no individual cluster showed a significant increase after accounting for repeated measures (p&gt;0.005). Both the amplitude (<xref ref-type="fig" rid="fig3">Figure 3E</xref>; ANOVA; binocular vs. monocular <italic>F</italic> = 1.349, p=0.247) and relative timing (<xref ref-type="fig" rid="fig3">Figure 3F</xref>) of movement clusters were unchanged. We confirmed that movement frequency per trial was increased in an additional dataset by performing autoregressive hidden Markov modeling on the nose, eye, and ear positions, and found that binocular and monocular conditions could be differentiated by a simple decoder using the transition probabilities between movement states (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Mice perform more head movements during the decision period under monocular conditions.</title><p>(<bold>A</bold>) Example decision period trajectory of the left eye position overlaid with movements identified through velocity zero crossings. Color corresponds to movement cluster identity in (<bold>B ,C</bold>). Image is the last time point in the decision period. (<bold>B</bold>) Horizontal (x) and vertical (y) positions of eye across time from the trace in (<bold>A</bold>). Individual movements are plotted above as x/y traces, with dotted lines corresponding to the middle time point, and blue and red points indicating the start and end, respectively. Colors correspond to clusters in (<bold>C</bold>). (<bold>C</bold>) Top: example individual movements from 10 k-means clusters; magenta is the trajectory, blue and red are start and end points, respectively. Bottom: individual movement clusters for binocular (top row) and monocular (bottom row) conditions, with means plotted over 100 individual examples in gray. (<bold>D</bold>) Mean number of movements per trial for each cluster in binocular (blue) vs. monocular (magenta) conditions. (<bold>E</bold>) Mean amplitude of movement clusters for binocular (blue) and monocular (magenta) conditions. (<bold>F</bold>) Normalized movement frequency as a function of time before the jump for all clusters. (<bold>G</bold>) Mean trial duration (decision period only) for the two conditions. (<bold>H</bold>) Mean of the total distance traveled by the eye during the decision period for the two conditions. (<bold>I</bold>) Mean head pitch, measured as the angle between the eye and ear, across the decision period for the two conditions. (<bold>J</bold>) Relationship between the change in head pitch and change in cluster frequency between the binocular and monocular conditions. Dotted line is fit from linear regression.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Autoregressive hidden Markov(ARHMM) modeling of decision period behavior.</title><p>(<bold>A</bold>) Example traces of eye position from five movement states labeled with autoregressive hidden Markov modeling of DeepLabCut-tracked points during the decision period (progressing blue to red in time) in average temporal order. Arrow line widths are proportional to transition probabilities between states (gray &lt; 0.035 â¤ black). (<bold>B</bold>) Transition count matrix for binocular condition, showing the frequency of transitioning from one state (y-axis) to another state (x-axis) as a fraction of all unique state transitions; these values were used to generate the arrows in panel (<bold>A</bold>). (<bold>C</bold>) Frequency of each state for binocular (black) and monocular (gray) conditions. Asterisk indicates p&lt;0.01, paired <italic>t</italic>-test. (<bold>D</bold>) Heat maps of start time histograms for each state normalized to the total number of trials for binocular (top) and monocular (bottom) conditions. (<bold>E</bold>) Twofold decoding analysis on transition count matrices for binocular vs. monocular conditions (performed within-animal, averaged across animals). (<bold>F</bold>) Z-scored weights used to decode binocular vs. monocular condition. (<bold>G</bold>) Difference between monocular and binocular transition count matrices; red transitions are more frequent in monocular, blue in binocular. n = 8 mice for all plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig3-figsupp1-v1.tif"/></fig></fig-group><media mimetype="video" mime-subtype="mp4" id="video2" xlink:href="elife-74708-video2.mp4"><label>Video 2.</label><caption><title>Same trial as <xref ref-type="video" rid="video1">Video 1</xref>, but with a 500 ms history of eye position labeled, along with cluster identities of movements and trial events.</title></caption></media><p>To determine whether the increased frequency of head movements reflected an increase in sampling rate, or an increase in temporal integration, we compared the decision period duration (<xref ref-type="fig" rid="fig3">Figure 3G</xref>) and total distance moved (<xref ref-type="fig" rid="fig3">Figure 3H</xref>). Monocular animals spent more time making the decision to jump (Wilcoxon signed-rank test; binocular 2.48 Â± 0.27 s vs. monocular 5.56 Â± 1.31 s, p=0.008) and moved a greater distance overall than binocular animals (Wilcoxon signed-rank test; binocular 14.65 Â± 1.27 cm vs. monocular 28.73 Â± 5.01 cm, p=0.016). These results suggest that mice use a temporal integration strategy when binocular cues are unavailable.</p><p>Finally, to determine whether animals changed their head position under monocular conditions, we calculated head angle from the side (pitch) and top (yaw) camera data. On average, mice decreased the pitch of their head (downward tilt) under monocular conditions (<xref ref-type="fig" rid="fig3">Figure 3I</xref>; Wilcoxon signed-rank test; binocular â19.76 Â± 2.39Â° vs. monocular â26.84 Â± 3.28Â°, p=0.008) without changing the range (standard deviation) of pitch values (data not shown; Wilcoxon signed-rank test; binocular 15.25 Â± 0.81Â° vs. monocular 16.79 Â± 0.72Â°, p=0.109). Interestingly, the mean change in pitch was inversely correlated with overall change in cluster frequency (<xref ref-type="fig" rid="fig3">Figure 3J</xref>; Spearman correlation, p=0.047), suggesting that mice either increase the frequency of vertical head movements or change the positioning of their head. Yaw was not significantly changed (data not shown; Wilcoxon signed-rank test; binocular 17.15 Â± 2.44Â° vs. monocular 17.45 Â± 2.93Â°, p=0.641) while the range of yaw values was significantly increased (data not shown; Wilcoxon signed-rank test; binocular 8.33 Â± 1.45Â° vs. monocular 16.24 Â± 4.16Â°, p=0.039), suggesting an overall increase in the range of side-to-side head movements. These changes were not associated with changes in movement cluster frequency (data not shown; Spearman correlation, p=0.320).</p><p>Together, these results show that movement and position of the head during distance estimation differ when binocular vision is no longer available, consistent with a switch from the use of binocular cues to other cues such as motion and/or position parallax that require temporal integration.</p></sec><sec id="s2-4"><title>Eye movements compensate for head movements to stabilize gaze</title><p>Previous work shows that the majority of eye movements in rodents, including mice, are compensatory for head movements, and that saccades occur primarily as a consequence of large-amplitude head movements (<xref ref-type="bibr" rid="bib29">Meyer et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Michaiel et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Wallace et al., 2013</xref>). Some species make horizontal vergence eye movements to increase binocular overlap during behaviors such as prey capture (<xref ref-type="bibr" rid="bib3">Bianco et al., 2011</xref>). To determine how mice target their gaze during binocular distance estimation, we performed bilateral eye tracking using miniature head-mounted cameras (<xref ref-type="bibr" rid="bib28">Meyer et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Michaiel et al., 2020</xref>; <xref ref-type="bibr" rid="bib36">Sattler and Wehr, 2020</xref>), then used DLC to track the pupil in order to quantify horizontal and vertical eye movements (<xref ref-type="fig" rid="fig4">Figure 4A</xref>; see <xref ref-type="video" rid="video3">Video 3</xref>). Importantly, mice continued to accurately perform the task despite the head-mounted hardware and tether (~3 g weight; <xref ref-type="fig" rid="fig4">Figure 4B and C</xref>). It should be noted that these experiments were performed in a different set of animals with narrower landing platforms than the other experiments in this study, so performance is worse relative to the data in <xref ref-type="fig" rid="fig2">Figure 2</xref>; however, subjects showed no difference in their performance relative to their performance under control conditions (ANOVA; control vs. eye cameras <italic>F</italic> = 0.373, p=0.543). Head pitch (vertical head angle) was anticorrelated with both eye vergence (horizontal angle of the two eyes) and eye phi (vertical eye movements) both during the early portion of the decision period when mice were approaching the jump (start of trial to 2 s before jump; pitch vs. vergence R<sup>2</sup> = 0.51, phi R<sup>2</sup> = 0.28) and in the late portion of the decision period immediately prior to the jump (2 s prior to jump; pitch vs. vergence R<sup>2</sup> = 0.70, phi R<sup>2</sup> = 0.54; <xref ref-type="fig" rid="fig4">Figure 4D and E</xref>). Thus, upward head movements caused the eyes to move down and toward the nose, while downward head movements caused upward and outward eye movements, consistent with vestibulo-ocular reflex-mediated gaze maintenance throughout the decision period. Additionally, while there was a slight change in vergence between the early and late periods of the trial (vergence early 1.77 Â± 0.21Â°, late â1.66 Â± â0.29Â°; p=1.57e-4), this was explained by a similar difference in head pitch between these two periods (pitch early â45.45 Â± 1.24Â°, late â38.47 Â± 1.43Â°; p=4.97e-5), demonstrating that mice do not move their eyes to increase binocular overlap preceding the jump.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Eye movements compensate for head movements to stabilize gaze.</title><p>(<bold>A</bold>) Schematic of experimental setup for measuring head and eye movements; bilateral eye tracking with miniature head-mounted cameras (top) and ellipse fitting of DLC-tracked pupil points (bottom). (<bold>B</bold>) Side and top-view images of a mouse performing the task with the eye tracking system (three frames overlaid). (<bold>C</bold>) Performance (left) and distance jumped (right) for eye-tracking experiments. Gray lines are individual animal data. (<bold>D</bold>) Horizontal angle between the two eyes (eye theta divergence) as a function of head pitch during the decision period. âEarlyâ is from the start of the trial to 2 s before the jump, and âlateâ is the 2 s preceding the jump. (<bold>E</bold>) Mean eye theta, eye theta vergence, and eye phi cross-correlations with head pitch angle for early (left) and late (right) portions of the decision period; n = 8 mice for all plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig4-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" id="video3" xlink:href="elife-74708-video3.mp4"><label>Video 3.</label><caption><title>Mouse performing the task with miniature head-mounted cameras tracking both eyes.</title></caption></media></sec><sec id="s2-5"><title>V1 optogenetic suppression disrupts distance estimation task performance</title><p>Finally, we tested whether distance estimation behavior requires visual cortex. We first asked whether suppressing the activity of binocular zone V1, which corresponds retinotopically to the central visual field in front of the mouse, affected task performance. Bilateral optic fibers were implanted at the surface of the cortex above binocular V1 in either control mice (PV-Cre) or mice expressing channelrhodopsin-2 (ChR2) in parvalbumin-expressing inhibitory interneurons (PV-Cre:Ai32, referred to as PV-ChR2 here; <xref ref-type="bibr" rid="bib13">Hippenmeyer et al., 2005</xref>; <xref ref-type="bibr" rid="bib26">Madisen et al., 2012</xref>), all of which had intact binocular vision (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left column; control n = 948 trials in four mice, PV-ChR2 n = 911 trials in four mice). On a third of trials, light was delivered through the implanted optic fibers during the decision period (470 nm, 5 mW/mm<sup>2</sup>, 40 Hz, 50% duty cycle). Control animals showed no change in performance with the laser on (ANOVA; laser off vs. on, failure <italic>F</italic> = 0.030, p=0.864, success <italic>F</italic> = 0.026, p=0.872, abort <italic>F</italic> = 0.070, p=0.793), whereas PV-ChR2 animals (see <xref ref-type="video" rid="video4">Video 4</xref>) showed a significant reduction in performance across distances (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; ANOVA; laser off vs. on, failure <italic>F</italic> = 7.836, p=0.008, success <italic>F</italic> = 15.252, p=3.35e-4, abort <italic>F</italic> = 10.876, p=0.002; see <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref> for a breakdown of all three outcomes). On success trials, the mean landing position of PV-ChR2 mice was significantly changed compared to controls (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; <italic>t</italic>-test, PV-ChR2 vs. control, p=0.019) while the standard deviation of landing position was not significantly different (<italic>t</italic>-test, PV-ChR2 vs. control, p=0.249). Interestingly, similar to mice that underwent monocular occlusion, PV-ChR2 mice showed decreased head pitch on success trials with the laser on (<xref ref-type="fig" rid="fig5">Figure 5D</xref>; <italic>t</italic>-test, PV-ChR2 vs. control, p=0.004), with no change in the standard deviation of pitch (<italic>t</italic>-test, PV-ChR2 vs. control, p=0.268) or in yaw (<italic>t</italic>-test, PV-ChR2 vs. control, mean p=0.116, SD p=0.164). There was no laser-associated change in trial duration, movement cluster frequency, or amplitude (data not shown; <italic>t</italic>-test, trial duration p=0.391; ANOVA, frequency p=0.106, amplitude p=0.930). Together, these data show that suppression of binocular zone V1 in animals with intact binocular vision significantly decreases distance estimation task performance, and that the ability to successfully perform the task is associated with changes in both pre-jump behavior and landing position.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>V1 optogenetic suppression disrupts distance estimation task performance.</title><p>(<bold>A</bold>) Schematic of experimental setup for optogenetic experiments; bilateral illumination of either binocular or monocular zone V1 in either binocular or monocular animals during the decision period on one-third of trials. All plots within a column correspond to the schematized condition. (<bold>B</bold>) Performance curves for laser-off (black) and laser-on (cyan) conditions in mice expressing ChR2 in PV+ inhibitory interneurons (ChR2+, top row) or PV-Cre only mice (ChR2-, bottom row). Thin lines are individual animal data. (<bold>C</bold>) Change in the mean and standard deviation of landing positions, averaged across mice. Small circles are individual animal data. (<bold>D</bold>) Change in the mean head angle for up-down (pitch) and side-to-side (yaw) head position. (<bold>E</bold>) Same as (<bold>D</bold>) but change in standard deviation of pitch and yaw.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>V1 optogenetic suppression task performance.</title><p>(<bold>A</bold>) Rates of failure (left column), success (middle column), and abort trials (right column) for binocular vision with optogenetic suppression of binocular zone V1. Top row shows data from animals expressing ChR2 in PV+ interneurons, and bottom row is animals lacking ChR2. Blue lines indicate laser on trials, and black lines indicate laser-off trials. (<bold>B</bold>) Same as (<bold>A</bold>) but for monocular vision with binocular zone V1 suppression. (<bold>C</bold>) Same as (<bold>B</bold>) but for monocular vision with monocular zone V1 suppression. Note that data in (C) were collected with different platform geometry and gap distances (see âMaterials and methodsâ for details).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74708-fig5-figsupp1-v1.tif"/></fig></fig-group><media mimetype="video" mime-subtype="mp4" id="video4" xlink:href="elife-74708-video4.mp4"><label>Video 4.</label><caption><title>PV-ChR2 mouse with binocular vision during a laser-off and a laser-on trial for binocular zone V1 suppression.</title></caption></media><p>We next asked whether mice with monocular vision would be affected by binocular zone V1 suppression (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, middle column). Interestingly, PV-ChR2 mice showed no change in success rate (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; ANOVA; laser off vs. on, failure <italic>F</italic> = 2.388, p=0.130, success <italic>F</italic> = 0.389, p=0.536, abort <italic>F</italic> = 0.090, p=0.766), and no change in landing position (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; <italic>t</italic>-test; PV-ChR2 vs. control, mean p=0.189, SD p=0.955) or head position (<xref ref-type="fig" rid="fig5">Figure 5D and E</xref>; <italic>t</italic>-test; PV-ChR2 vs. control, pitch mean p=0.153, pitch SD p=0.117, yaw mean p=0.903, yaw SD p=0.849). There were also no laser-associated changes in trial duration, movement cluster frequency, or amplitude (data not shown; <italic>t</italic>-test, trial duration p=0.712; ANOVA, frequency p=0.882, amplitude p=0.296). This suggests that once animals switch to using monocular cues, binocular zone V1 is no longer required for accurate task performance, and that therefore the peripheral visual field is engaged.</p><p>Lastly, we asked whether suppression of monocular zone V1, which corresponds retinotopically to the peripheral visual field, affected task performance of mice with monocular vision (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right column). PV-ChR2 mice were less successful at the task with optogenetic suppression due to an increase in the number of abort trials, whereas the fraction of failure trials remained unchanged (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; ANOVA; laser off vs. on, failure <italic>F</italic> = 0.555, p=0.462, success <italic>F</italic> = 10.120, p=0.003, abort <italic>F</italic> = 4.663, p=0.039). On success trials, PV-ChR2 mice showed no significant change in landing position (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; <italic>t</italic>-test; PV-ChR2 vs. control, mean p=0.189, SD p=0.200); however, they showed changes in both pitch mean (<xref ref-type="fig" rid="fig5">Figure 5D</xref>; <italic>t</italic>-test; PV-ChR2 vs. control, mean p=0.002, SD p=0.486) and yaw standard deviation (<xref ref-type="fig" rid="fig5">Figure 5E</xref>; <italic>t</italic>-test; PV-ChR2 vs. control, mean p=0.388, SD p=0.018), consistent with shifting head position to utilize a portion of the visual field unaffected by optogenetic suppression. There was no laser-associated change in trial duration, movement cluster frequency, or amplitude (data not shown; <italic>t</italic>-test, trial duration p=0.050; ANOVA, frequency p=0.476, amplitude p=0.344). It should be noted these experiments were performed in a separate group of mice with narrower landing platforms, thus comparisons of success rate and landing position to the other experimental groups may not be insightful.</p><p>In summary, these experiments show that manipulating V1 activity significantly alters the behavior of mice on the distance estimation task, and the correspondence between the anatomical locus of suppression and the ocular condition supports a role for V1 in both binocular and monocular cue-mediated distance estimation task performance.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have established a visual distance estimation task in mice that engages an ethological, freely moving behavior. Previous research using similar versions of this task suggests that gerbils and rats utilize multiple cues to determine the distance to objects in the environment, including retinal image size, binocular vision, and motion parallax (<xref ref-type="bibr" rid="bib6">Carey et al., 1990</xref>; <xref ref-type="bibr" rid="bib8">Ellard et al., 1984</xref>; <xref ref-type="bibr" rid="bib12">Goodale et al., 1990</xref>; <xref ref-type="bibr" rid="bib21">Legg and Lambert, 1990</xref>). Importantly, this task is distinct from âgap-crossingâ tasks (<xref ref-type="bibr" rid="bib17">Hutson and Masterton, 1986</xref>) where animals can use the whisker somatosensory system to determine the distance across a short gap. Furthermore, in contrast to other recently developed tasks that are designed to probe binocular depth perception and stereopsis (<xref ref-type="bibr" rid="bib5">Boone et al., 2021</xref>), in this task mice are able to use monocular cues for depth, including those that are generated by self-movement. It can therefore be flexibly used to investigate a variety of distance estimation tactics by manipulating experimental conditions.</p><sec id="s3-1"><title>Cues for distance estimation</title><p>Binocular vision (and therefore stereopsis) was not required for accurate distance estimation in this task, consistent with previous studies on gerbils (<xref ref-type="bibr" rid="bib8">Ellard et al., 1984</xref>). This provides the first demonstration that mice are able to use depth cues that are available besides stereopsis. Vertical head movements sufficient to generate motion parallax cues are increased in frequency under monocular conditions, suggesting that mice may use motion parallax under monocular vision in this task. Previous work found increasing frequency of vertical head movements as a function of gap distance in gerbils and rats (<xref ref-type="bibr" rid="bib8">Ellard et al., 1984</xref>; <xref ref-type="bibr" rid="bib21">Legg and Lambert, 1990</xref>). We did not see such a relationship, which could be due to species-specific differences or differences in task design. Interestingly, there was an inverse relationship between changes in movement frequency and head angle â animals with the smallest change in frequency of head movements showed the largest change in downward head angle. This could reflect the use of position parallax cues (comparing two perspectives, initial and final, rather than motion cues) and was also present in animals with binocular vision when binocular zone V1 activity was suppressed. These results do not rule out the use of binocular disparity under normal conditions â in fact, changes in decision period head position and movement between binocular and monocular conditions, in addition to our optogenetic suppression experiments in binocular zone V1, provide evidence that animals use binocular cues as well under normal conditions.</p><p>The increase in decision period duration and total distance moved by mice in the monocular relative to binocular condition is consistent with an active sensing strategy that requires temporal integration. The inherent separation of the decision period from the jump execution in this task helps separate potential active sensing behavior from pure jumping behavior â in theory, the animal need not make any movements except those required to jump (<xref ref-type="bibr" rid="bib38">Stamper et al., 2019</xref>). Closed-loop control of the sensory environment is a powerful tool for investigating active sensing (<xref ref-type="bibr" rid="bib4">Biswas et al., 2018</xref>) in this task, altering the landing platform based on head movements would provide a causal test to determine whether mice use motion parallax cues, as was performed in locusts (<xref ref-type="bibr" rid="bib39">Wallace, 1959</xref>). Furthermore, investigating the relationship between active sensing and memory-guided visual behavior could provide new insights into natural behavior and its neural basis. For example, experiments with a modified version of this task suggest that parietal cortex is necessary for context-dependent use of retinal image size cues (<xref ref-type="bibr" rid="bib10">Ellard and Sharma, 1996</xref>). Future research could investigate how these contextual associations are formed through active sensing and bound into memory in the brain.</p></sec><sec id="s3-2"><title>Eye movements during distance estimation</title><p>Using miniature cameras to track the eyes, we found that eye movements compensate for head movements to stabilize gaze leading up to the jump. This would allow the mouse to both maintain gaze toward the platform and reduce motion blur throughout large-amplitude head movements (<xref ref-type="bibr" rid="bib20">Land, 1999</xref>). This is consistent with previous work showing that mouse eye movements stabilize gaze during both operant behavior (<xref ref-type="bibr" rid="bib29">Meyer et al., 2020</xref>) and a natural behavior, prey capture (<xref ref-type="bibr" rid="bib30">Michaiel et al., 2020</xref>). It will be interesting to determine whether jumping mice control their gaze to localize the platform on a specific subregion of the retina; that is, whether there exists a retinal specialization for determining distance. In the case of prey capture, the image of the cricket is stabilized in the retinal region with the highest concentration of alpha-ganglion cells (<xref ref-type="bibr" rid="bib14">Holmgren et al., 2021</xref>). We also found that mice did not move their eyes to increase binocular overlap during the period immediately preceding the jump, similar to a previous finding demonstrating that rats do not align the gaze of the two eyes before crossing a short gap (<xref ref-type="bibr" rid="bib40">Wallace et al., 2013</xref>). Smooth eye movements provide extra-retinal signals for computing depth from motion parallax in primates (<xref ref-type="bibr" rid="bib18">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Nadler et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Nadler et al., 2009</xref>), and future studies may address whether the compensatory movements we observed play a similar role in mice. Finally, these experiments show that mice are capable of performing this task with a tether and significant hardware weight on the head, which is a critical requirement for introducing additional techniques such as electrophysiology into this paradigm.</p></sec><sec id="s3-3"><title>Neural circuits underlying distance estimation</title><p>We provide evidence for V1 specifically being important for distance estimation behavior in mice. Given the large volume of V1 (~5 mm<sup>3</sup>) relative to the spread of laser-induced suppression in PV-ChR2 mice (~1 mm<sup>3</sup>, <xref ref-type="bibr" rid="bib23">Li et al., 2019</xref>), animals likely maintained some V1 function across all conditions, which would explain why animals in all conditions could still perform the task to some degree, and did so with associated changes in how they orient their heads (i.e., using different parts of the visual field). Whether animals would perform this task with total V1 suppression is unclear given the option to abort trials. However, the fact that the anatomical locus of suppression (binocular vs. monocular zone V1) determined whether there was an effect on behavior between ocular conditions (binocular vs. monocular vision) supports the hypothesis that mice use V1 to estimate distance in this task. In fact, binocular V1 suppression led to the same changes in decision period head angle in mice with binocular vision that monocular occlusion induced, suggesting a common shift in strategy with the loss of stereopsis.</p><p>These results are consistent with previous work showing broad lesions of occipital cortex disrupt performance without affecting head movements, whereas lesions to superior colliculus and preoptic area had no effect on either (<xref ref-type="bibr" rid="bib9">Ellard et al., 1986</xref>). This task could therefore be a useful tool for studying the specific computations performed in V1 that mediate accurate distance estimation, and both the visual and nonvisual input signals required to perform these computations. Additionally, the neural circuits that convert visual information into a jump command are also not well understood. Most work has examined jumping in nocifensive and defensive contexts rather than navigation (<xref ref-type="bibr" rid="bib1">Barik et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Wang et al., 2015</xref>), although a recent behavioral study demonstrated that squirrels learn to integrate multiple factors, including gap distance and branch flexibility, in executing a jump (<xref ref-type="bibr" rid="bib16">Hunt et al., 2021</xref>).</p></sec><sec id="s3-4"><title>Utility of studying natural distance estimation behavior</title><p>Natural behavior is often a continuous control process, which is fundamentally closed-loop, unlike stimulus-response paradigms that dominate behavior literature (<xref ref-type="bibr" rid="bib7">Cisek, 1999</xref>). This task accordingly permits investigation of both how movement through the environment generates sensory cues useful for judging distance, and how the visual information is directly converted into a motor output. Furthermore, perception of spatial layout is an embodied process, and thus body- and action-scaling cues that are not available under conditions of restraint could provide distance information under the freely moving conditions of this task (<xref ref-type="bibr" rid="bib11">Fajen, 2021</xref>). Critically, natural behaviors may be the most appropriate tool for studying the neural basis of sensory processing since theoretical considerations suggest that neural circuits may perform suboptimal inference under non-natural conditions (<xref ref-type="bibr" rid="bib2">Beck et al., 2012</xref>). Finally, beyond studies of visual distance estimation, this task could provide a framework for integrated studies of motivation, motor control, and decision-making within an ethological context.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>Male and female mice between postnatal day 40 (P40) and P365 were bred in-house in a C56BL/6J background. For optogenetic experiments, transgenic mice were used to target the expression of channelrhodopsin-2 to parvalbumin-expressing neurons (PV-Cre [B6;129P2-Pvalbtm1(cre)Arbr/J, Jax #008069] crossed to Ai32 [B6;129S-Gt(ROSA)26Sortm32(CAG-COP4*H134R/EYFP)Hze/J, Jax #012569]; <xref ref-type="bibr" rid="bib13">Hippenmeyer et al., 2005</xref>; <xref ref-type="bibr" rid="bib26">Madisen et al., 2012</xref>). Mice were housed in a reverse 12 hr lightâdark cycle room. Mice were placed under a water restriction schedule at the start of training, only receiving fluids during training/task periods. All procedures were performed in accordance with the University of Oregon Institute for Animal Care and Use Committee and Animal Care Services standard operating procedures.</p></sec><sec id="s4-2"><title>Behavioral apparatus and jumping task</title><p>Two cohorts of mice were used in this study, with a subset of experiments (eye cameras and monocular V1 suppression) using a smaller arena, narrower landing platforms, and fewer gap distances. All other experimental components of the task were identical. The jumping arena was roughly 45 cm high, 70 cm wide, and 100 cm across for most experiments, and was 30 cm high and 60 cm across for eye camera/monocular V1 suppression. Mice self-initiated trials by mounting a take-off platform (15 cm height, 10 cm width, 10 cm depth, with 4 Ã 5 cm overhang in front). While blocking the mouseâs view of the arena with a barrier, the experimenter then placed one of three platforms (15, 20, or 25 cm width, 30 cm depth, 19 cm height for most experiments; 10, 20, or 30 cm width, 5 cm depth, 19 cm height for eye camera/monocular V1 suppression) at a random distance (10, 12, 14, 16, 18, 20, or 22 cm for most experiments; 8, 12, 16, 20, or 24 cm for eye camera/monocular V1 suppression) from the edge of the take-off platform. Platforms were custom built from Â¼â²â² acrylic, and tops were coated in white rubberized coating (Plasti-Dip) or fine-grained white sandpaper to prevent animals from slipping. For eye camera/monocular V1 experiments, the platforms were white and a black strip was placed across the top leading edge of the landing platform, matched proportionally in height to platform width to maintain height/width ratio. Arena and platforms were constructed by the University of Oregon Technical Science Administration. A static white noise background composed of grayscale squares (~1Â° each of visual angle from take-off platform) was mounted at the back of the arena. Six LED puck lights were evenly spaced around the top of the arena for even illumination. Cameras (FLIR BlackFly S USB3) were mounted above and to the side of the arena, and the entire behavioral session was recorded (1440 Ã 1080 pixels at 99.97 fps, or 720 Ã 540 pixels at 60 fps) with camera timestamps using a custom Bonsai workflow (<xref ref-type="bibr" rid="bib25">Lopes et al., 2015</xref>). A custom Python script was used to generate randomized platform/distance combinations for the experimenter and to log trial outcomes and approximate jump times. The moment the barrier was lifted and the mouse was able to see the landing platform constituted the trial start, and the time elapsed until the mouse jumped was the âdecision period.â There were three possible trial outcomes: (1) the mouse jumped and successfully reached the landing platform and received a reward (success), (2) the mouse jumped and missed the landing platform and received no reward (failure), or (3) the mouse dismounted the take-off platform and received a light airpuff and a time-out (abort).</p><p>All experiments included data from eight mice (same mice for binocular, monocular, and binocular V1 suppression; same mice for eye cameras, monocular V1 suppression, and supplementary ARHMM figure) and typically lasted 30â45 min. The mean Â± standard error for the number of sessions and trials per mouse was as follows: binocular sessions 11.4 Â± 0.6, trials 42.1 Â± 2.3; monocular sessions 5.6 Â± 0.2, trials 39.0 Â± 3.4; eye cameras sessions 2.8 Â± 0.3, trials 13.9 Â± 0.7; binocular vision/binocular zone V1 suppression sessions 6.8 Â± 0.2, trials 34.3 Â± 2.6; monocular vision/binocular zone V1 suppression sessions 4.8 Â± 0.2, trials 33.5 Â± 1.8; monocular vision/monocular zone V1 suppression sessions 3.6 Â± 0.2, trials 27.2 Â± 1.7. The number of trials was partly limited by the manual nature of the task, which required an experimenter to manually place a platform at a specific distance. However, this process typically took less than 3 s, whereas the time spent consuming the reward and freely investigating the arena between trials was significantly longer. Future versions of the task with automated platform placement and reward delivery could increase the number of trials per session, and increase the length of individual sessions. Acquisition files are available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/nielllab/nlab-behavior/tree/master/jumping/bonsai">https://github.com/nielllab/nlab-behavior/tree/master/jumping/bonsai</ext-link>(copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bc5202960bda19144ba0aadcb6a6604159dd1a07;origin=https://github.com/nielllab/nlab-behavior;visit=swh:1:snp:b692137691508f05b97151263c1cf76feae543dc;anchor=swh:1:rev:44a40fdbd63ed7740a73b8d085333c8d1b22c592;path=/jumping/bonsai/">swh:1:rev:44a40fdbd63ed7740a73b8d085333c8d1b22c592; path=/jumping/bonsai/</ext-link>; <xref ref-type="bibr" rid="bib33">niellab, 2022</xref>).</p></sec><sec id="s4-3"><title>Behavioral training</title><p>Mice were habituated to the arena for 3 days with their cage mates, during which time they were individually handled and introduced to a clicker that indicated water reward (~25â50 ul), where each click is immediately followed by a reward. Mice were then individually clicker trained to mount a short take-off platform (10 cm height; click and reward upon mounting the platform), receiving water (administered by hand using a 1 ml syringe), and a small piece of tortilla chip (Juanitaâs). After 3â5 successful mounts, a landing platform (19 cm height) was placed against the take-off platform, and mice were clicker-rewarded for climbing up onto the landing platform. After three successful trials, the landing platform was moved slightly farther back, increasing the gap distance until jumping is required to reach the landing platform. At this point, the clicker was typically no longer required. Once the mouse could jump to the maximum distance, the taller take-off platform used in the task was introduced, and landing platforms were again introduced at short distances and slowly moved farther away. Training was complete when mice could jump to all three landing platforms at the farthest distance, and typically took 1â2 weeks with all mice successfully learning the task.</p></sec><sec id="s4-4"><title>Surgical procedures</title><p>For all procedures, anesthesia was induced at 3% isoflurane and maintained at 1.5â2% in O<sub>2</sub> at a 1 l/min flow rate. Ophthalmic ointment was applied to both eyes, and body temperature was maintained using a closed-loop heating pad at 37Â°C. In order to minimize stress when plugging in optical tethers or miniature cameras, a small steel headplate was mounted on the skull using dental acrylic (Unifast LC) to allow for brief head-fixation before the experiment.</p></sec><sec id="s4-5"><title>Monocular suture</title><p>The area immediately surrounding the eye to be sutured was wiped with 70% ethanol before ophthalmic ointment was applied. Two to three mattress sutures were placed using 6-0 silk suture, opposing the full extent of the lid. The forepaw and hindpaw nails ipsilateral to the sutured eye were trimmed to help minimize postprocedural self-inflicted trauma.</p></sec><sec id="s4-6"><title>Optic fiber implant</title><p>A minimal portion of scalp was resected bilaterally over visual cortex, and a small trepanation was made over each primary visual cortex (+1.0, Â±2.5 mm for monocular zone or +1.0, Â±3.0 mm for binocular zone, relative to lambda suture). Bilateral optic fibers (ceramic ferrules, thorlab fiber 0.5 mm length from end of ferrule) were stereotactically lowered into the burr hole and secured in place with dental acrylic. Vetbond was then applied to secure the skin in place around the implant. Fiber transmission rates were measured prior to implant and accounted for during experiments.</p></sec><sec id="s4-7"><title>Miniature head-mounted cameras</title><p>To obtain high-resolution video of the eyes during behavior, a miniature camera (iSecurity), magnifying lens (12 mm focus, 6 mm diameter), and an infrared LED were mounted on a custom-designed 3D-printed plastic camera arm (<xref ref-type="bibr" rid="bib30">Michaiel et al., 2020</xref>). Two miniature connectors (Mill Max 853-93-100-10-001000, cut to 2 Ã 4 pin) were glued to the headplate, and an equivalent connector on the camera arm was plugged in prior to the experiment. Camera power and data were passed through thin tethering wire (Cooner #CZ1174CLEAR) and acquired in Bonsai with system timestamps. The total hardware weight was approximately 3 grams. Eye videos were deinterlaced to achieve 60 fps (matching top/side cameras) prior to analysis.</p></sec><sec id="s4-8"><title>Data analysis</title><p>Full task videos were first split into individual trials using custom Python software; the trial start, jump, and landing frame numbers were determined and individual trial videos were saved. These trial videos were then labeled using markerless pose estimation with DLC. A set of sample frames were manually labeled and used to train two networks (top/side cameras and eye cameras) that were then used to track features in all video data. The distance jumped was calculated using the position of the left ear in the top-camera frame where the front paw touched the platform and the animal decelerated (success trials) or when the front paw passed below the edge of the landing platform (failure trials). DLC points from the side-camera data during the decision period were passed through a median filter (n = 3) and a convolutional smoothing filter (box, n = 5). To extract individual movements, we identified all zero crossings in the velocity trace calculated from eye position, then extracted the 500 ms period around those time points in the eye position data. Movements that overlapped by more than 250 ms with a previous movement were excluded from further analysis. The x and y values were concatenated, and all movements across all conditions were fed into principal component analysis, after which k-means clustering was performed on the reduced data. We tried varying the number of clusters across a range of values (4â20) and found that k = 10 resulted in clusters that appeared to contain a single type of movement while minimizing repeated clusters of the same movement type. Pitch and yaw were both calculated from the angle between the eye and ear in the side and top video data, respectively.</p><p>For hidden Markov model (ARHMM) analysis, values of the points tracking the nose, eye, and ear were used as inputs for training after centering across experiments by subtracting off values of the point that tracked the edge of the take-off platform. Model training was performed using the SSM package in Python (<xref ref-type="bibr" rid="bib24">Linderman et al., 2019</xref>). Model selection was based on the elbow in twofold cross-validation log-likelihood curves across model iterations while balancing model interpretability with model fit (final model: K = 6, lag = 1, kappa = 1e04, data temporally downsampled 2Ã, and one state discarded due to extremely low prevalence). ARHMM states were determined based on a posterior probability threshold of 0.8. Time points below the threshold were excluded from analysis. For lexical transition matrices, trials were first separated into binocular and monocular conditions. During the decision period of each trial, the transitions between unique ARHMM states were counted. The number of state transitions was then normalized by the total number of unique transitions per condition to calculate the relative frequency of transitions. For all summary analyses, data were first averaged within animal (across days) and then across animals within a group (e.g., monocular, binocular). Statistical significance was determined using ANOVA and the Studentâs <italic>t</italic>-test with Bonferroni corrections for multiple comparisons. Unless otherwise noted, data are presented as mean Â± standard error of the mean. Analysis code is available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/nielllab/nlab-behavior/tree/master/jumping/python%20files">https://github.com/nielllab/nlab-behavior/tree/master/jumping/python%20files</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:72a8f3bfd315266d4e686bf7bff7ee50c2e3dd6d;origin=https://github.com/nielllab/nlab-behavior;visit=swh:1:snp:b692137691508f05b97151263c1cf76feae543dc;anchor=swh:1:rev:44a40fdbd63ed7740a73b8d085333c8d1b22c592;path=/jumping/python%2520files/">swh:1:rev:44a40fdbd63ed7740a73b8d085333c8d1b22c592; path=/jumping/python%20files/</ext-link>; <xref ref-type="bibr" rid="bib33">niellab, 2022</xref>). Data are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.r7sqv9sg2">https://doi.org/10.5061/dryad.r7sqv9sg2</ext-link>.</p></sec><sec id="s4-9"><title>Decoding analysis</title><p>We decoded the experimental condition (binocular vs. monocular, laser on vs. off) per animal from single-trial maximum a priori (MAP) motif sequences inferred using the ARHMM. Specifically, we trained binary decoders with a linear decision boundary (linear discriminant analysis) to decode the above categorical variables from the single-trial empirical state transition probability matrices derived from the MAP sequence of each trial, thus providing not only state usage information, but transitions between states as information the classifier could use. For each animal, correct trials were pooled across distances to provide enough trials per class for decoding. Data were split into training and test datasets in a stratified 10-fold cross-validation manner, ensuring equal proportions of trials of different types (distance, platform width, visual condition, laser) in both datasets. To calculate the statistical significance of decoding accuracies, we performed an iterative shuffle procedure on each fold of the cross-validation, shuffling training labels and testing on unshuffled test labels 100 times to create a shuffle distribution for each fold of the cross-validation. From these distributions, we calculated the z-score of decoding accuracy for each class in each cross-validation fold. These z-scores were then averaged across the folds of cross-validation and used to calculate the overall p-value of the decoding accuracy obtained on the original data. The decoding weights of the binary classifiers were examined as well to identify the significant transitions that contributed to decoding between visual conditions. The same shuffle procedure was used to assess significant elements of the classifier.</p></sec><sec id="s4-10"><title>Statistics</title><p>All summary data in text and plots, unless noted otherwise, are mean Â± standard error. All statistical tests were performed with SciPy, and in cases where data were not normally distributed, nonparametric tests were used.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Supervision, Funding acquisition, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Data curation, Software, Formal analysis</p></fn><fn fn-type="con" id="con6"><p>Investigation</p></fn><fn fn-type="con" id="con7"><p>Software, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con8"><p>Formal analysis, Supervision, Methodology</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing â review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All procedures were performed in accordance with the University of Oregon Institute for Animal Care and Use Committee and Animal Care Services standard operating procedures, under approved protocol AUP-21-21.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-74708-transrepform1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.r7sqv9sg2">https://doi.org/10.5061/dryad.r7sqv9sg2</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Distance estimation from monocular cues in an ethological visuomotor task</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.r7sqv9sg2</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank members of the Niell lab for helpful discussions, and Dr. Michael Goard, Dr. David Leopold, Dr. Matt Smear, and Dr. Michael Stryker for feedback on the manuscript. This work benefited from access to the University of Oregon high-performance computer, Talapas, and the University of Oregon Technical Science Administration. This work was supported by grants from the National Institutes of Health: 5F32EY027696 (Parker), 1R21EY032708 (Niell, Parker), 1R34NS111669 (Niell). Some figure panels were made using <ext-link ext-link-type="uri" xlink:href="https://biorender.com/">https://biorender.com/</ext-link>.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barik</surname><given-names>A</given-names></name><name><surname>Thompson</surname><given-names>JH</given-names></name><name><surname>Seltzer</surname><given-names>M</given-names></name><name><surname>Ghitani</surname><given-names>N</given-names></name><name><surname>Chesler</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A brainstem-spinal circuit controlling nocifensive behavior</article-title><source>Neuron</source><volume>100</volume><fpage>1491</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.037</pub-id><pub-id pub-id-type="pmid">30449655</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Not noisy, just wrong: the role of suboptimal inference in behavioral variability</article-title><source>Neuron</source><volume>74</volume><fpage>30</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.016</pub-id><pub-id pub-id-type="pmid">22500627</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>IH</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prey capture behavior evoked by simple visual stimuli in larval zebrafish</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>101</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00101</pub-id><pub-id pub-id-type="pmid">22203793</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswas</surname><given-names>D</given-names></name><name><surname>Arend</surname><given-names>LA</given-names></name><name><surname>Stamper</surname><given-names>SA</given-names></name><name><surname>VÃ¡gvÃ¶lgyi</surname><given-names>BP</given-names></name><name><surname>Fortune</surname><given-names>ES</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Closed-loop control of active sensing movements regulates sensory slip</article-title><source>Current Biology</source><volume>28</volume><fpage>4029</fpage><lpage>4036</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.002</pub-id><pub-id pub-id-type="pmid">30503617</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boone</surname><given-names>HC</given-names></name><name><surname>Samonds</surname><given-names>JM</given-names></name><name><surname>Crouse</surname><given-names>EC</given-names></name><name><surname>Barr</surname><given-names>C</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name><name><surname>McGee</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Natural binocular depth discrimination behavior in mice explained by visual cortical activity</article-title><source>Current Biology</source><volume>31</volume><fpage>2191</fpage><lpage>2198</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.02.031</pub-id><pub-id pub-id-type="pmid">33705714</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carey</surname><given-names>DP</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Sprowl</surname><given-names>EG</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Blindsight in rodents: the use of a âhigh-levelâ distance cue in gerbils with lesions of primary visual cortex</article-title><source>Behavioural Brain Research</source><volume>38</volume><fpage>283</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(90)90182-e</pub-id><pub-id pub-id-type="pmid">2363844</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Beyond the computer metaphor: behaviour as interaction</article-title><source>Journal of Consciousness Studies</source><volume>6</volume><fpage>125</fpage><lpage>142</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellard</surname><given-names>CG</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Timney</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Distance estimation in the mongolian gerbil: the role of dynamic depth cues</article-title><source>Behavioural Brain Research</source><volume>14</volume><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(84)90017-2</pub-id><pub-id pub-id-type="pmid">6518079</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellard</surname><given-names>CG</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Scorfield</surname><given-names>DM</given-names></name><name><surname>Lawrence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Visual cortical lesions abolish the use of motion parallax in the mongolian gerbil</article-title><source>Experimental Brain Research</source><volume>64</volume><fpage>599</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1007/BF00340498</pub-id><pub-id pub-id-type="pmid">3803494</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellard</surname><given-names>CG</given-names></name><name><surname>Sharma</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The effects of cortical lesions on recognition of object context in a visuomotor task in the mongolian gerbil</article-title><source>Behavioural Brain Research</source><volume>82</volume><fpage>13</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/s0166-4328(97)81104-7</pub-id><pub-id pub-id-type="pmid">9021066</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fajen</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>Visual Control of Locomotion</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/9781108870474</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Ellard</surname><given-names>CG</given-names></name><name><surname>Booth</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The role of image size and retinal motion in the computation of absolute distance by the mongolian gerbil (meriones unguiculatus)</article-title><source>Vision Research</source><volume>30</volume><fpage>399</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90082-v</pub-id><pub-id pub-id-type="pmid">2336799</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hippenmeyer</surname><given-names>S</given-names></name><name><surname>Vrieseling</surname><given-names>E</given-names></name><name><surname>Sigrist</surname><given-names>M</given-names></name><name><surname>Portmann</surname><given-names>T</given-names></name><name><surname>Laengle</surname><given-names>C</given-names></name><name><surname>Ladle</surname><given-names>DR</given-names></name><name><surname>Arber</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A developmental switch in the response of DRG neurons to ETS transcription factor signaling</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e159</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030159</pub-id><pub-id pub-id-type="pmid">15836427</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmgren</surname><given-names>CD</given-names></name><name><surname>Stahr</surname><given-names>P</given-names></name><name><surname>Wallace</surname><given-names>DJ</given-names></name><name><surname>Voit</surname><given-names>KM</given-names></name><name><surname>Matheson</surname><given-names>EJ</given-names></name><name><surname>Sawinski</surname><given-names>J</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Kerr</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual pursuit behavior in mice maintains the pursued prey on the retinal region with least optic flow</article-title><source>eLife</source><volume>10</volume><elocation-id>e70838</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.70838</pub-id><pub-id pub-id-type="pmid">34698633</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Yavorska</surname><given-names>I</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vision drives accurate approach behavior during prey capture in laboratory mice</article-title><source>Current Biology</source><volume>26</volume><fpage>3046</fpage><lpage>3052</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.09.009</pub-id><pub-id pub-id-type="pmid">27773567</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>NH</given-names></name><name><surname>Jinn</surname><given-names>J</given-names></name><name><surname>Jacobs</surname><given-names>LF</given-names></name><name><surname>Full</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Acrobatic squirrels learn to leap and land on tree branches without falling</article-title><source>Science</source><volume>373</volume><fpage>697</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1126/science.abe5753</pub-id><pub-id pub-id-type="pmid">34353955</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutson</surname><given-names>KA</given-names></name><name><surname>Masterton</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The sensory contribution of a single vibrissaâs cortical barrel</article-title><source>Journal of Neurophysiology</source><volume>56</volume><fpage>1196</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1152/jn.1986.56.4.1196</pub-id><pub-id pub-id-type="pmid">3783236</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gain modulation as a mechanism for coding depth from motion parallax in macaque area MT</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8180</fpage><lpage>8197</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0393-17.2017</pub-id><pub-id pub-id-type="pmid">28739582</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kral</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Behavioural-analytical studies of the role of head movements in depth perception in insects, birds and mammals</article-title><source>Behavioural Processes</source><volume>64</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/s0376-6357(03)00054-8</pub-id><pub-id pub-id-type="pmid">12914988</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Motion and vision: why animals move their eyes</article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><volume>185</volume><fpage>341</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s003590050393</pub-id><pub-id pub-id-type="pmid">10555268</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legg</surname><given-names>CR</given-names></name><name><surname>Lambert</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Distance estimation in the hooded rat: experimental evidence for the role of motion cues</article-title><source>Behavioural Brain Research</source><volume>41</volume><fpage>11</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(90)90049-k</pub-id><pub-id pub-id-type="pmid">2073352</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Park</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Studying the visual brain in its natural rhythm</article-title><source>NeuroImage</source><volume>216</volume><elocation-id>116790</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116790</pub-id><pub-id pub-id-type="pmid">32278093</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Huo</surname><given-names>Y</given-names></name><name><surname>Inagaki</surname><given-names>HK</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Davis</surname><given-names>C</given-names></name><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spatiotemporal constraints on optogenetic inactivation in cortical circuits</article-title><source>eLife</source><volume>8</volume><elocation-id>e48622</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48622</pub-id><pub-id pub-id-type="pmid">31736463</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Linderman</surname><given-names>S</given-names></name><name><surname>Nichols</surname><given-names>A</given-names></name><name><surname>Blei</surname><given-names>D</given-names></name><name><surname>Zimmer</surname><given-names>M</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical Recurrent State Space Models Reveal Discrete and Continuous Dynamics of Neural Activity in <italic>C. elegans</italic></article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/621540</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>FrazÃ£o</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><name><surname>Calcaterra</surname><given-names>L</given-names></name><name><surname>Dreosti</surname><given-names>E</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madisen</surname><given-names>L</given-names></name><name><surname>Mao</surname><given-names>T</given-names></name><name><surname>Koch</surname><given-names>H</given-names></name><name><surname>Zhuo</surname><given-names>J</given-names></name><name><surname>Berenyi</surname><given-names>A</given-names></name><name><surname>Fujisawa</surname><given-names>S</given-names></name><name><surname>Hsu</surname><given-names>Y-WA</given-names></name><name><surname>Garcia</surname><given-names>AJ</given-names></name><name><surname>Gu</surname><given-names>X</given-names></name><name><surname>Zanella</surname><given-names>S</given-names></name><name><surname>Kidney</surname><given-names>J</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Mao</surname><given-names>Y</given-names></name><name><surname>Hooks</surname><given-names>BM</given-names></name><name><surname>Boyden</surname><given-names>ES</given-names></name><name><surname>BuzsÃ¡ki</surname><given-names>G</given-names></name><name><surname>Ramirez</surname><given-names>JM</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Turner</surname><given-names>EE</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A toolbox of cre-dependent optogenetic transgenic mice for light-induced activation and silencing</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>793</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1038/nn.3078</pub-id><pub-id pub-id-type="pmid">22446880</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>AF</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>OâKeefe</surname><given-names>J</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A head-mounted camera system integrates detailed behavioral monitoring with multichannel electrophysiology in freely moving mice</article-title><source>Neuron</source><volume>100</volume><fpage>46</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.020</pub-id><pub-id pub-id-type="pmid">30308171</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>AF</given-names></name><name><surname>OâKeefe</surname><given-names>J</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two distinct types of eye-head coupling in freely moving mice</article-title><source>Current Biology</source><volume>30</volume><fpage>2116</fpage><lpage>2130</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.042</pub-id><pub-id pub-id-type="pmid">32413309</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaiel</surname><given-names>AM</given-names></name><name><surname>Abe</surname><given-names>ET</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamics of gaze control during prey capture in freely moving mice</article-title><source>eLife</source><volume>9</volume><elocation-id>e57458</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57458</pub-id><pub-id pub-id-type="pmid">32706335</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A neural representation of depth from motion parallax in macaque visual cortex</article-title><source>Nature</source><volume>452</volume><fpage>642</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1038/nature06814</pub-id><pub-id pub-id-type="pmid">18344979</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Nawrot</surname><given-names>M</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>MT neurons combine visual motion with a smooth eye movement signal to code depth-sign from motion parallax</article-title><source>Neuron</source><volume>63</volume><fpage>523</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.029</pub-id><pub-id pub-id-type="pmid">19709633</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><collab>niellab</collab></person-group><year iso-8601-date="2022">2022</year><data-title>Nlab-behavior</data-title><version designator="b0ea1fc">b0ea1fc</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/nielllab/nlab-behavior/">https://github.com/nielllab/nlab-behavior/</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>PRL</given-names></name><name><surname>Brown</surname><given-names>MA</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Movement-related signals in sensory areas: roles in natural behavior</article-title><source>Trends in Neurosciences</source><volume>43</volume><fpage>581</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2020.05.005</pub-id><pub-id pub-id-type="pmid">32580899</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1909">1909</year><article-title>A study of the sensory control in the rat</article-title><source>The Psychological Review</source><volume>12</volume><fpage>1</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1037/h0093009</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sattler</surname><given-names>NJ</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A head-mounted multi-camera system for electrophysiology and behavior in freely-moving mice</article-title><source>Frontiers in Neuroscience</source><volume>14</volume><elocation-id>592417</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.592417</pub-id><pub-id pub-id-type="pmid">33584174</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinkman</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Visual depth discrimination in animals</article-title><source>Psychological Bulletin</source><volume>59</volume><fpage>489</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1037/h0040912</pub-id><pub-id pub-id-type="pmid">13977333</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stamper</surname><given-names>SA</given-names></name><name><surname>Madhav</surname><given-names>MS</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name><name><surname>Fortune</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Using control theory to characterize active sensing in weakly electric fishes</chapter-title><person-group person-group-type="editor"><name><surname>Stamper</surname><given-names>SA</given-names></name></person-group><source>Electroreception: Fundamental Insights from Comparative Approaches</source><publisher-name>Springer</publisher-name><fpage>227</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-29105-1_8</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>visual scanning in the desert locust <italic>schistocerca gregaria</italic> forskaÌl</article-title><source>Journal of Experimental Biology</source><volume>36</volume><fpage>512</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1242/jeb.36.3.512</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>DJ</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Sawinski</surname><given-names>J</given-names></name><name><surname>Rulla</surname><given-names>S</given-names></name><name><surname>Notaro</surname><given-names>G</given-names></name><name><surname>Kerr</surname><given-names>JND</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats maintain an overhead binocular field at the expense of constant fusion</article-title><source>Nature</source><volume>498</volume><fpage>65</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1038/nature12153</pub-id><pub-id pub-id-type="pmid">23708965</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>IZ</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Collateral pathways from the ventromedial hypothalamus mediate defensive behaviors</article-title><source>Neuron</source><volume>85</volume><fpage>1344</fpage><lpage>1358</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.025</pub-id><pub-id pub-id-type="pmid">25754823</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yilmaz</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid innate defensive responses of mice to looming visual stimuli</article-title><source>Current Biology</source><volume>23</volume><fpage>2011</fpage><lpage>2015</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.08.015</pub-id><pub-id pub-id-type="pmid">24120636</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74708.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cowan</surname><given-names>Noah J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.09.29.462468" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.29.462468"/></front-stub><body><p>This is an important article with compelling experimental methods, including ethologically relevant behavior, sophisticated physiological methods including optogenetic suppression of primary visual cortical activity, careful behavioral experiments, and clear, convincing, quantitative analysis of the resulting data. The article enhances our understanding of the role of active visual estimation of distance under multiple factors of visual degradation (binocular/monocular, and V1 suppression), demonstrating how robust task performance can emerge from compensatory active sensing strategies.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74708.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cowan</surname><given-names>Noah J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Cowan</surname><given-names>Noah J</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Johns Hopkins University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.29.462468">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.09.29.462468v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Distance estimation from monocular cues in an ethological visuomotor task&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Noah J Cowan as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Tirin Moore as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers all agree that the manuscript has potential for publication, and that the data are sound, but there was broad and unanimous agreement that the paper needs significantly stronger analysis of the data. While one reviewer mentioned that neural recordings would broaden the appeal, during consultation it was agreed that such recordings were beyond the scope of this work. In short, it was agreed that deeper analysis of the behavioral data is needed and will strengthen this paper, and that neural recordings or other additional data are not necessary.</p><p>The level of effort and care that went into the experiment design, experiments, and data analysis is highly commendable. This treasure trove of data, as currently analyzed, appears to establish these basic results:</p><p>â Monocular cues are <italic>sufficient</italic> for jumping nearly equally well.</p><p>â There is some sort of increased activity before jumping in the monocular setting.</p><p>â V1 plays some general role.</p><p>â Eye movements are anti-correlated with head movements (a broadly known idea across taxa and completely expected) and do not seem to increase binocular overlap (much more relevant to the current task).</p><p>All three reviewers, using different language, felt that the analysis of the pre-jump behavior was incomplete, not well illustrated, and unclear. It is evident that a more thoughtful and deeper dive into the behavioral data is essential to be considered for publication. Relating these data, for example, to gaining additional disparity or other possible behavioral mechanisms for gaining sensory information for the decision making task would greatly enhance the manuscript.</p><p>Likewise, the analysis of the V1 data just scratches the surface. The authors simply noted that there were more aborts, but provided no mechanistic insights, and the manuscript failed to firmly establish that V1 is used in the distance estimation component.</p><p>The reviewers all raised individual questions that should be addressed in a response document in a point-by-point manner, if the authors choose to revise the manuscript for resubmission.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>As mentioned above in the public review, there are two major weaknesses with the HMM-only approach to data analysis of the binocular vs. monocular behavior. First, it would seem that if there are major differences in pre-jump behavior, that much simpler statistical measures (root-mean-squared velocities of the head, for instance?) that should pull out the differences in active sensing, and such &quot;simple&quot; analysis which not be as obfuscating. If that fails, then the measures by which you tried this and they failed should be reported, and the fancy HMMs should be thusly motivated. Second, no mechanistic insight was garnered. What is the nature of the additional active sensation? Is it larger amplitude, for instance, so as to gain better disparity? Or is it just occurring over a longer period of time, suggesting a temporal integration strategy?</p><p>However, maybe a deeper dive into the data could reveal more. Rather than simply looking at success / fail, would it be possible to examine jumping distance in the various conditions? Since the authors used DLC, they have metric information about jumping that does not seem to be getting used. Did the V1-inhibited mice have a wider variance on their jumping distance when they did jump, for example? Such analyses would be more fine grained and might pick up subtle differences that the binary &quot;success / abort / fail&quot; cannot, in order to better quantify the importance of V1 in distance estimation vs. cognitive decision making about gap jumping. It was also unclear what the authors meant when they said the HMM's concluded there was no change in behavior â was this only measured for cases where there was an attempted jump? Again, the HMMs really obscure these results and it would be good to check other simpler movement statistics to motivate and/or confirm the HMM results.</p><p>The authors state that the V1-inhibited group showed little-to-no change failure rate (which I didn't not see defined, but I assume this means the proportion of failures compared to jumping attempts). This suggests that, as an alternative to the authors' primary claim that distance judgements are impaired, perhaps the <italic>decision</italic> to make a jump is inhibited, but not necessarily the distance estimation itself. I'm not enough of an expert in the various visual pathways, but it seems like this paper might support the view that distance judgements for the purpose of decision making may use different mechanisms and/or pathways than distance estimation during execution. Can the authors clarify this?</p><p>The gaze stabilization results are interesting, but it seems that the main point is this statement: &quot;demonstrating that mice do not move their eyes to increase binocular overlap preceding the jump.&quot; In the discussion, I suggest this be brought up again, I think, in the section &quot;Cues for distance estimation&quot;. After the authors state &quot;This provides the first demonstration that mice are able to use depth cues that are available besides stereopsis.&quot; it would be good to point out that the eyes do not appear to be attempting to improve stereopsis based on eye movement patterns, and that instead the results are consistent with the fact that binocular vision may be used in a standard sensory fusion kind of way, i.e. two measurements simply reduce uncertainty. The animals compensate for this via more active sensing and then achieve similar performance. (I really appreciate the reference to Wallace (1959) and hope this experiment is one-day performed in mice!) To summarize my point in this paragraph, could it be that the evidence from this paper supports that, at least for this task, binocular cue integration in mice is just &quot;extra sensing&quot; i.e. the use of two eyes is the &quot;sum of the parts&quot; are not &quot;stereoscopic vision&quot; i.e. &quot;greater than the sum of the parts&quot;?</p><p>Finally, I feel there is a wide range of active sensing literature that the authors could include in a discussion, since ultimately the most compelling potential finding in this paper is the increase in active sensing as a function of loss of binocular vision. As a starting point you could check out our recent papers for pretty extensive bibliographies on active sensing (no need to cite our papers unless you deem some aspect as appropriate):</p><p>S. A. Stamper, M. S. Madhav, N. J. Cowan, and E. S. Fortune, &quot;Using Control Theory to Characterize Active Sensing in Weakly Electric Fishes,&quot; in Electroreception: Fundamental Insights from Comparative Approaches, B. Carlson, J. Sisneros, A. Popper, and R. Fay, Eds., New York: Springer Science+Business Media, LLC, 2019, vol. 70.</p><p>D. Biswas, L. A. Arend, S. Stamper, B. P. VÃ¡gvÃ¶lgyi, E. S. Fortune, and N. J. Cowan, &quot;Closed-Loop Control of Active Sensing Movements Regulates Sensory Slip,&quot; Curr Biol, vol. 28, iss. 4, 2018.</p><p>Perhaps the best connection of any to our work is the increase in active sensing with the loss of a sensory system â in this case perhaps the loss of a single eye would be analogous to the loss of a modality that causes increases to active sensing in a different modality. The change in active sensing strategy here is similar but suggests that the partial loss of a modality can lead to increases in active sensing in the same modality. Again, this connection is not critically important, just an observation that the authors may or may not find relevant to their discussion.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. The animal's success rate drops with gap distance. Is that caused by errors in vision (distance estimation) or jumping (motor accuracy). I wonder if the authors have data to address this question.</p><p>2. It is true that mice can use monocular cues to estimate distance in this task, but the observed differences between monocular and binocular conditions (more head movements and the difference in the modeling results) actually suggest that mice normally use binocular cues. The authors should state this conclusion more prominently.</p><p>3. Related to the point #2 above, it would be interesting to know if the performance requires visual cortex under binocular condition.</p><p>4. The performance was impaired upon suppressing the visual cortex, but the animals can still do it. The authors discussed this as incomplete suppression. This is reasonable, but are there data to support this, like recording across the cortical depth and visual cortex? Related, the suppression was considered &quot;V1&quot;, but how did the authors confirm that this was restricted to V1, and how much of V1 was illuminated by the light?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Since the development of a new task is a major part of the manuscript, I recommend more details in the Materials and methods for describing the task. Based on the current methods section, I assume the platform and distance were manually changed by the experimenter for each trial. Please provide more detail on the time it took for this manual step and discuss how this might or might not affect the animals' engagement in the task. Were the animals conditioned to this experimenter intervention? In addition, this labor-intensive step limits the number of trials mice can perform (30-60 per session). There should be information regarding the number of sessions each mouse performed. Related, I would like to see the behavior and optogenetics data from each animal plotted separately.</p><p>If the claim of mice accurately estimated the distance was supported by the bottom panel of B and C on Figure 2, then please show both the jump distance for both the success trials and failed trials. Do mice often fall short on the failed trials compared to the success trials?</p><p>What is the image inside of each circle in figure 3A? Is it a mouse? I am unclear how it is related to each movement state of the hidden Markov model. Please consider more informative illustrations. Related, I find the video showing the Deeplabcut markers used very helpful, please consider having a panel in figure 3 showing the markers feed into the hidden Markov model. This would also help make the example eye position traces in panel A clearer.</p><p>The font of the numbers inside the matrix on figure 3B is too small, please make them bigger or remove them.</p><p>Most importantly, the conclusion from Figure 3 is Mice perform more head movements under monocular conditions. This conclusion wasn't intuitive from more transitions between states in the monocular condition. It is clear from the example traces that the states represent different movements, but more transition in states being the equivalent of more head movement would at least require example traces of head position around the transition in states.</p><p>How many animals were used in the eye tracking experiments? The jump distance in Figure 4 panel C seems to be lower than the jump distance in Figure 2B and C, especially for the lower gap distance. Is this the strategy of the animals used in eye tracking, or is the eye tracking equipment weighing down the animals? Showing the success rate and jump distance of individual animals would be helpful.</p><p>As mentioned in the text, the change in performance during optogenetic suppression trials was largely due to an increase in abort with little change in failure rate. Therefore, showing an example video of an animal that failed a trial might be misleading.</p><p>Reference in the middle of the paragraph at Discussion, Eye movements during distance estimation section. Holmgren et al., is missing a date.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74708.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>As mentioned above in the public review, there are two major weaknesses with the HMM-only approach to data analysis of the binocular vs. monocular behavior. First, it would seem that if there are major differences in pre-jump behavior, that much simpler statistical measures (root-mean-squared velocities of the head, for instance?) that should pull out the differences in active sensing, and such &quot;simple&quot; analysis which not be as obfuscating. If that fails, then the measures by which you tried this and they failed should be reported, and the fancy HMMs should be thusly motivated. Second, no mechanistic insight was garnered. What is the nature of the additional active sensation? Is it larger amplitude, for instance, so as to gain better disparity? Or is it just occurring over a longer period of time, suggesting a temporal integration strategy?</p></disp-quote><p>We have made significant changes to the analyses to incorporate simpler and more interpretable measures of behavior. The simplest analysis is the duration of the trials and the distance traveled during that period. Animals in the monocular condition showed significantly longer decision periods with more overall movement, consistent with a temporal integration strategy. Next, in order to make the individual movements more intuitive, we identified movements using zero-velocity crossings of x/y eye position, and clustering with PCA/k-means, and found that the amplitude of individual movements was unchanged between binocular and monocular conditions, while frequency was increased. Finally, we analyzed the angle of the head, which showed a systematic within-animal change between binocular and monocular conditions. Together, we feel these findings (Figure 3) provide more mechanistic insight into the decision strategy.</p><disp-quote content-type="editor-comment"><p>However, maybe a deeper dive into the data could reveal more. Rather than simply looking at success / fail, would it be possible to examine jumping distance in the various conditions? Since the authors used DLC, they have metric information about jumping that does not seem to be getting used. Did the V1-inhibited mice have a wider variance on their jumping distance when they did jump, for example? Such analyses would be more fine grained and might pick up subtle differences that the binary &quot;success / abort / fail&quot; cannot, in order to better quantify the importance of V1 in distance estimation vs. cognitive decision making about gap jumping. It was also unclear what the authors meant when they said the HMM's concluded there was no change in behavior â was this only measured for cases where there was an attempted jump? Again, the HMMs really obscure these results and it would be good to check other simpler movement statistics to motivate and/or confirm the HMM results.</p></disp-quote><p>In order to provide greater insight into the various conditions, we have performed further experiments and analysis. We collected additional data for optogenetic experiments to include binocular/monocular vision with binocular zone V1 suppression, and analyzed both landing position and the positioning of the head during the decision period (Figure 5). We found changes in landing position and head angle (similar to monocular suture behavior) in animals with binocular vision and binocular zone V1 suppression, and found changes in head position (including more variance in side-to-side head angle) for monocular vision and monocular zone V1 suppression. However, there was not a significant change in the variance of landing position, and head movements (using our new analysis) were unchanged even when including abort trials. These new data suggest our suppression is fairly localized relative to the size of V1, so animals are able to change their head position to perform the task, and this is specific to binocular vision/V1 or monocular vision/V1. This finding in itself supports the notion that V1 plays a role in the behavior. We have updated Figure 5 to incorporate these new data, and added relevant discussion.</p><disp-quote content-type="editor-comment"><p>The authors state that the V1-inhibited group showed little-to-no change failure rate (which I didn't not see defined, but I assume this means the proportion of failures compared to jumping attempts). This suggests that, as an alternative to the authors' primary claim that distance judgements are impaired, perhaps the decision to make a jump is inhibited, but not necessarily the distance estimation itself. I'm not enough of an expert in the various visual pathways, but it seems like this paper might support the view that distance judgements for the purpose of decision making may use different mechanisms and/or pathways than distance estimation during execution. Can the authors clarify this?</p></disp-quote><p>It is true that the increase in the number of aborts without a change in the number of failures makes it unclear whether distance estimation or the decision to make a jump is affected â we have changed the text to clarify that V1 suppression disrupts task performance, rather than distance estimation per se.</p><disp-quote content-type="editor-comment"><p>The gaze stabilization results are interesting, but it seems that the main point is this statement: &quot;demonstrating that mice do not move their eyes to increase binocular overlap preceding the jump.&quot; In the discussion, I suggest this be brought up again, I think, in the section &quot;Cues for distance estimation&quot;. After the authors state &quot;This provides the first demonstration that mice are able to use depth cues that are available besides stereopsis.&quot; it would be good to point out that the eyes do not appear to be attempting to improve stereopsis based on eye movement patterns, and that instead the results are consistent with the fact that binocular vision may be used in a standard sensory fusion kind of way, i.e. two measurements simply reduce uncertainty. The animals compensate for this via more active sensing and then achieve similar performance. (I really appreciate the reference to Wallace (1959) and hope this experiment is one-day performed in mice!) To summarize my point in this paragraph, could it be that the evidence from this paper supports that, at least for this task, binocular cue integration in mice is just &quot;extra sensing&quot; i.e. the use of two eyes is the &quot;sum of the parts&quot; are not &quot;stereoscopic vision&quot; i.e. &quot;greater than the sum of the parts&quot;?</p></disp-quote><p>First, regarding whether the eyes are moving to improve stereopsis, mice have a ~40 deg zone of binocular overlap at baseline, so it does not seem likely that increasing binocular overlap would improve stereopsis, perhaps explaining why we do not see a change. Regarding whether binocular cue integration in mice is the âsum of the partsâ or âextra sensing,â we cannot speak to this issue with the current dataset. However, we presume that mice use all of the cues they have available, which includes stereopsis under binocular conditions. Furthermore, analysis from our new experiments show that decision period behavior changes under monocular conditions, suggesting animals are relying on different cues under the two conditions. We have updated the discussion to include these points.</p><disp-quote content-type="editor-comment"><p>Finally, I feel there is a wide range of active sensing literature that the authors could include in a discussion, since ultimately the most compelling potential finding in this paper is the increase in active sensing as a function of loss of binocular vision. As a starting point you could check out our recent papers for pretty extensive bibliographies on active sensing (no need to cite our papers unless you deem some aspect as appropriate):</p><p>S. A. Stamper, M. S. Madhav, N. J. Cowan, and E. S. Fortune, &quot;Using Control Theory to Characterize Active Sensing in Weakly Electric Fishes,&quot; in Electroreception: Fundamental Insights from Comparative Approaches, B. Carlson, J. Sisneros, A. Popper, and R. Fay, Eds., New York: Springer Science+Business Media, LLC, 2019, vol. 70.</p><p>D. Biswas, L. A. Arend, S. Stamper, B. P. VÃ¡gvÃ¶lgyi, E. S. Fortune, and N. J. Cowan, &quot;Closed-Loop Control of Active Sensing Movements Regulates Sensory Slip,&quot; Curr Biol, vol. 28, iss. 4, 2018.</p><p>Perhaps the best connection of any to our work is the increase in active sensing with the loss of a sensory system â in this case perhaps the loss of a single eye would be analogous to the loss of a modality that causes increases to active sensing in a different modality. The change in active sensing strategy here is similar but suggests that the partial loss of a modality can lead to increases in active sensing in the same modality. Again, this connection is not critically important, just an observation that the authors may or may not find relevant to their discussion.</p></disp-quote><p>We appreciate the Reviewerâs suggestions and have incorporated more active sensing literature to the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. The animal's success rate drops with gap distance. Is that caused by errors in vision (distance estimation) or jumping (motor accuracy). I wonder if the authors have data to address this question.</p></disp-quote><p>We have now incorporated additional experiments with larger landing platforms, which showed that failures at longer distances decreased relative to narrower platforms, suggesting that at longer distances the mice had difficulty landing on the relatively narrow platforms in the original experiments. In the new dataset, monocular animals showed no effect of gap distance on success rate, and binocular animals showed only a slight decrease in success with longer distances (p=0.48). These results have been incorporated into Figure 2.</p><disp-quote content-type="editor-comment"><p>2. It is true that mice can use monocular cues to estimate distance in this task, but the observed differences between monocular and binocular conditions (more head movements and the difference in the modeling results) actually suggest that mice normally use binocular cues. The authors should state this conclusion more prominently.</p></disp-quote><p>We agree, and think that mice will use all of the cues available, which under baseline conditions includes stereopsis. We did not mean to imply that they would not use binocular cues, but instead were focused on their ability to perform the task without binocular cues, and found this interesting given that monocular cues for depth are less well studied. We have updated the Discussion to include this point.</p><disp-quote content-type="editor-comment"><p>3. Related to the point #2 above, it would be interesting to know if the performance requires visual cortex under binocular condition.</p></disp-quote><p>We performed additional experiments with optogenetic suppression (binocular zone V1 suppression in binocular animals) and found a decrease in success rate, as well as changes in both pre-jump behavior (head angle) and landing position, consistent with V1 being utilized to perform the task under binocular conditions. Interestingly, animals with binocular vision showed no effect of binocular zone V1 suppression. These data have been incorporated in Figure 5.</p><disp-quote content-type="editor-comment"><p>4. The performance was impaired upon suppressing the visual cortex, but the animals can still do it. The authors discussed this as incomplete suppression. This is reasonable, but are there data to support this, like recording across the cortical depth and visual cortex? Related, the suppression was considered &quot;V1&quot;, but how did the authors confirm that this was restricted to V1, and how much of V1 was illuminated by the light?</p></disp-quote><p>While we do not have electrophysiological recordings to characterize the spread of suppression, this has been done previously, and in cortex the PV-ChR2-mediated suppression is about 1 mm<sup>3</sup> (Li et al., <italic>eLife</italic> 2019). Given the large volume of V1 (~5 mm<sup>3</sup>) it is unlikely we are completely suppressing all of V1. The changes in pre-jump behavior and landing position we have further characterized are consistent with the animals being unable to use the part of the visual field they normally would, and instead turning or lifting their heads to sample a different region of visual space that has not been affected by the manipulation. Furthermore, while binocular animals with binocular zone V1 suppression showed significant changes in behavior, monocular animals with binocular zone V1 suppression showed little change in behavior, consistent with the idea that they have already switched to using a different part of visual space after monocular occlusion.</p><p>Additional analyses have been included in Figure 5, and are further addressed in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Since the development of a new task is a major part of the manuscript, I recommend more details in the Materials and methods for describing the task. Based on the current methods section, I assume the platform and distance were manually changed by the experimenter for each trial. Please provide more detail on the time it took for this manual step and discuss how this might or might not affect the animals' engagement in the task. Were the animals conditioned to this experimenter intervention? In addition, this labor-intensive step limits the number of trials mice can perform (30-60 per session). There should be information regarding the number of sessions each mouse performed. Related, I would like to see the behavior and optogenetics data from each animal plotted separately.</p></disp-quote><p>While manually placing the platforms takes some additional time, the bulk of the âdown-timeâ is between trials when mice can roam freely about the arena. Automating the system (which would be ideal) would allow for parallelization of experiments and more trials per session/longer sessions. We have added these additional details and discussion to the Methods section. We have also added the mean Â± standard error for number of sessions and trials per session for each experiment to the Methods, and have plotted individual animal data on figure panels when practical.</p><disp-quote content-type="editor-comment"><p>If the claim of mice accurately estimated the distance was supported by the bottom panel of B and C on Figure 2, then please show both the jump distance for both the success trials and failed trials. Do mice often fall short on the failed trials compared to the success trials?</p><p>What is the image inside of each circle in figure 3A? Is it a mouse? I am unclear how it is related to each movement state of the hidden Markov model. Please consider more informative illustrations. Related, I find the video showing the Deeplabcut markers used very helpful, please consider having a panel in figure 3 showing the markers feed into the hidden Markov model. This would also help make the example eye position traces in panel A clearer.</p><p>The font of the numbers inside the matrix on figure 3B is too small, please make them bigger or remove them.</p></disp-quote><p>In order to measure accuracy, we performed additional experiments with larger landing platforms as described above. Under these conditions, mice only fail by jumping short. Having a larger potential landing area allows us to measure the accuracy and precision of the mice under the various experimental manipulations. We have also updated our decision period figure to show more intuitive example movement trajectories, and updated other panels per the Reviewerâs suggestions.</p><disp-quote content-type="editor-comment"><p>Most importantly, the conclusion from Figure 3 is Mice perform more head movements under monocular conditions. This conclusion wasn't intuitive from more transitions between states in the monocular condition. It is clear from the example traces that the states represent different movements, but more transition in states being the equivalent of more head movement would at least require example traces of head position around the transition in states.</p></disp-quote><p>We now use a more intuitive approach to movement identification and clustering than the ARHMM, and analyze the frequency (Figure 3D) and amplitude (Figure 3E) of these movements.</p><disp-quote content-type="editor-comment"><p>How many animals were used in the eye tracking experiments? The jump distance in Figure 4 panel C seems to be lower than the jump distance in Figure 2B and C, especially for the lower gap distance. Is this the strategy of the animals used in eye tracking, or is the eye tracking equipment weighing down the animals? Showing the success rate and jump distance of individual animals would be helpful.</p></disp-quote><p>The same animals were used for the eye tracking experiments as for the other experiments in the original version of the manuscript, thus we were able to directly compare their performance with the head-mounted hardware. There was not a significant change in performance. We have added these results and the number of mice to the Results section and updated the plots to include individual data.</p><disp-quote content-type="editor-comment"><p>As mentioned in the text, the change in performance during optogenetic suppression trials was largely due to an increase in abort with little change in failure rate. Therefore, showing an example video of an animal that failed a trial might be misleading.</p></disp-quote><p>We have found more exemplary videos to show the changes in precision with optogenetic V1 suppression.</p><disp-quote content-type="editor-comment"><p>Reference in the middle of the paragraph at Discussion, Eye movements during distance estimation section. Holmgren et al., is missing a date.</p></disp-quote><p>This preprint has since been published in a peer-reviewed journal, and we have updated the reference accordingly.</p></body></sub-article></article>