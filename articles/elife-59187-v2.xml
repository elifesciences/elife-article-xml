<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">59187</article-id><article-id pub-id-type="doi">10.7554/eLife.59187</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>3DeeCellTracker, a deep learning-based pipeline for segmenting and tracking cells in 3D time lapse images</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-189195"><name><surname>Wen</surname><given-names>Chentao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8609-476X</contrib-id><email>chentao-wen@umin.ac.jp</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-190021"><name><surname>Miura</surname><given-names>Takuya</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-190022"><name><surname>Voleti</surname><given-names>Venkatakaushik</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-225179"><name><surname>Yamaguchi</surname><given-names>Kazushi</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-225180"><name><surname>Tsutsumi</surname><given-names>Motosuke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5832-3828</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-225181"><name><surname>Yamamoto</surname><given-names>Kei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2712-1550</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-225182"><name><surname>Otomo</surname><given-names>Kohei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5322-6295</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-190037"><name><surname>Fujie</surname><given-names>Yukako</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-190038"><name><surname>Teramoto</surname><given-names>Takayuki</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7060-7148</contrib-id><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-189301"><name><surname>Ishihara</surname><given-names>Takeshi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9175-3072</contrib-id><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-146873"><name><surname>Aoki</surname><given-names>Kazuhiro</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7263-1555</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-225183"><name><surname>Nemoto</surname><given-names>Tomomi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6102-1495</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-45317"><name><surname>Hillman</surname><given-names>Elizabeth MC</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5511-1451</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund12"/><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152374"><name><surname>Kimura</surname><given-names>Koutarou D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3359-1578</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Graduate School of Science, Nagoya City University</institution><addr-line><named-content content-type="city">Nagoya</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution>Department of Biological Sciences, Graduate School of Science, Osaka University</institution><addr-line><named-content content-type="city">Toyonaka</named-content></addr-line><country>Japan</country></aff><aff id="aff3"><label>3</label><institution>Departments of Biomedical Engineering and Radiology and the Zuckerman Mind Brain Behavior Institute, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Graduate School of Information Science and Technology, Hokkaido University</institution><addr-line><named-content content-type="city">Sapporo</named-content></addr-line><country>Japan</country></aff><aff id="aff5"><label>5</label><institution>National Institute for Physiological Sciences</institution><addr-line><named-content content-type="city">Okazaki</named-content></addr-line><country>Japan</country></aff><aff id="aff6"><label>6</label><institution>Exploratory Research Center on Life and Living Systems</institution><addr-line><named-content content-type="city">Okazaki</named-content></addr-line><country>Japan</country></aff><aff id="aff7"><label>7</label><institution>National Institute for Basic Biology, National Institutes of Natural Sciences</institution><addr-line><named-content content-type="city">Okazaki</named-content></addr-line><country>Japan</country></aff><aff id="aff8"><label>8</label><institution>The Graduate School for Advanced Study</institution><addr-line><named-content content-type="city">Hayama</named-content></addr-line><country>Japan</country></aff><aff id="aff9"><label>9</label><institution>Department of Biology, Faculty of Science, Kyushu University</institution><addr-line><named-content content-type="city">Fukuoka</named-content></addr-line><country>Japan</country></aff><aff id="aff10"><label>10</label><institution>RIKEN center for Advanced Intelligence Project</institution><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zimmer</surname><given-names>Manuel</given-names></name><role>Reviewing Editor</role><aff><institution>Research Institute of Molecular Pathology, Vienna Biocenter and University of Vienna</institution><country>Austria</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>30</day><month>03</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e59187</elocation-id><history><date date-type="received" iso-8601-date="2020-05-21"><day>21</day><month>05</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-02-23"><day>23</day><month>02</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Wen et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Wen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-59187-v2.pdf"/><abstract><p>Despite recent improvements in microscope technologies, segmenting and tracking cells in three-dimensional time-lapse images (3D + T images) to extract their dynamic positions and activities remains a considerable bottleneck in the field. We developed a deep learning-based software pipeline, 3DeeCellTracker, by integrating multiple existing and new techniques including deep learning for tracking. With only one volume of training data, one initial correction, and a few parameter changes, 3DeeCellTracker successfully segmented and tracked ~100 cells in both semi-immobilized and ‘straightened’ freely moving worm's brain, in a naturally beating zebrafish heart, and ~1000 cells in a 3D cultured tumor spheroid. While these datasets were imaged with highly divergent optical systems, our method tracked 90–100% of the cells in most cases, which is comparable or superior to previous results. These results suggest that 3DeeCellTracker could pave the way for revealing dynamic cell activities in image datasets that have been difficult to analyze.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Microscopes have been used to decrypt the tiny details of life since the 17th century. Now, the advent of 3D microscopy allows scientists to build up detailed pictures of living cells and tissues. In that effort, automation is becoming increasingly important so that scientists can analyze the resulting images and understand how bodies grow, heal and respond to changes such as drug therapies.</p><p>In particular, algorithms can help to spot cells in the picture (called cell segmentation), and then to follow these cells over time across multiple images (known as cell tracking). However, performing these analyses on 3D images over a given period has been quite challenging. In addition, the algorithms that have already been created are often not user-friendly, and they can only be applied to a specific dataset gathered through a particular scientific method.</p><p>As a response, Wen et al. developed a new program called 3DeeCellTracker, which runs on a desktop computer and uses a type of artificial intelligence known as deep learning to produce consistent results. Crucially, 3DeeCellTracker can be used to analyze various types of images taken using different types of cutting-edge microscope systems. And indeed, the algorithm was then harnessed to track the activity of nerve cells in moving microscopic worms, of beating heart cells in a young small fish, and of cancer cells grown in the lab. This versatile tool can now be used across biology, medical research and drug development to help monitor cell activities.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cell tracking</kwd><kwd>bioimaging</kwd><kwd>deep learning</kwd><kwd>quantitative biology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>C. elegans</italic></kwd><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>KAKENHI JP16H06545</award-id><principal-award-recipient><name><surname>Kimura</surname><given-names>Koutarou D</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>KAKENHI JP20H05700</award-id><principal-award-recipient><name><surname>Kimura</surname><given-names>Koutarou D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>KAKENHI JP18H05135</award-id><principal-award-recipient><name><surname>Ishihara</surname><given-names>Takeshi</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>KAKENHI JP19K15406</award-id><principal-award-recipient><name><surname>Tsutsumi</surname><given-names>Motosuke</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution>NIH/NINDS</institution></institution-wrap></funding-source><award-id>U01NS094296 UF1NS108213</award-id><principal-award-recipient><name><surname>Hillman</surname><given-names>Elizabeth MC</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution>NIH/NCI</institution></institution-wrap></funding-source><award-id>U01CA236554</award-id><principal-award-recipient><name><surname>Hillman</surname><given-names>Elizabeth MC</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006321</institution-id><institution>National Institutes of Natural Sciences</institution></institution-wrap></funding-source><award-id>01112002</award-id><principal-award-recipient><name><surname>Kimura</surname><given-names>Koutarou D</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Grant-in-Aid for Research in Nagoya City University</institution></institution-wrap></funding-source><award-id>48 1912011 1921102</award-id><principal-award-recipient><name><surname>Kimura</surname><given-names>Koutarou D</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>RIKEN Center for Advanced Intelligence Project</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kimura</surname><given-names>Koutarou D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>A program for Leading Graduate Schools entitled 'Interdisciplinary graduate school program for systematic understanding of health and disease'</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Miura</surname><given-names>Takuya</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>NTT-Kyushu University Collaborative Research Program on Basic Science</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Ishihara</surname><given-names>Takeshi</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A deep learning-based pipeline was developed for extracting cellular signals flexibly from moving cells in 3D time lapse images, and it outperformed previous methods under different imaging conditions.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Imaging cells to reveal the dynamic activities of life has become considerably more feasible because of the remarkable developments in microscope hardware in recent years (<xref ref-type="bibr" rid="bib11">Frigault et al., 2009</xref>; <xref ref-type="bibr" rid="bib1">Ahrens and Engert, 2015</xref>; <xref ref-type="bibr" rid="bib5">Bouchard et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Weisenburger and Vaziri, 2018</xref>). In addition, multiple software platforms for processing 2D/3D still images and 2D + T images have been developed (<xref ref-type="bibr" rid="bib10">Eliceiri et al., 2012</xref>).</p><p>However, processing cells in 3D + T images has remained difficult, especially when cells cannot be clearly separated and/or their movements are relatively large, such as cells in deforming organs. For processing objects in 3D + T images, the following two steps are required: (1) segmentation: segmenting the regions of interest in each 3D image into individual objects (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, left) and (2) tracking: linking an object in a particular volume to the same object in the temporally adjacent volume (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, right). For segmenting and tracking (hereafter collectively called ‘tracking’) cells of deforming organs in 3D + T images, programs optimized for processing images in particular conditions have been developed (<xref ref-type="bibr" rid="bib34">Schrödel et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>). However, these methods cannot be used under conditions other than those for which they were designed, at least without a loss in processing efficiency. In other words, 3D + T images, especially those obtained under challenging conditions, can be efficiently processed only when customized software is developed specifically for those images. One reason for this is that many parameters must be optimized to achieve good results—For example, even in 2D image processing, changes in lighting could require the re-optimization of parameters for segmentation and tracking (<xref ref-type="bibr" rid="bib9">Egnor and Branson, 2016</xref>).</p><p>One way to solve this problem is to optimize the parameters automatically using machine learning, especially deep learning. Deep learning methods use an artificial neural network with multiple layers, that is, a deep network, to process complex data and automatically optimize a number of parameters from training data, which allows users to easily apply a single method to images obtained under different conditions. In addition to this flexibility, deep learning methods have outperformed conventional methods in some image processing tasks such as image classification (<xref ref-type="bibr" rid="bib19">LeCun et al., 2015</xref>; <xref ref-type="bibr" rid="bib18">Krizhevsky et al., 2012</xref>). Nevertheless, deep learning methods are used mostly for segmentation and/or object detection, but not for tracking because of the difficulty in preparing training data (<xref ref-type="bibr" rid="bib24">Moen et al., 2019</xref>): Correctly tracking a number of objects manually to obtain training data, especially from 3D + T images recorded over long period of time, is extremely challenging. In addition, designing a multiple-step pipeline for segmenting cells and tracking their positions to work under various conditions has been difficult.</p><p>In this study, we developed 3DeeCellTracker, a new pipeline utilizing deep learning techniques in segmentation and, for the first time to our knowledge, in tracking of cells in 3D + T images. We solved the problem of training data preparation for tracking by creating a synthetic dataset (see below). We also designed a multiple-step pipeline to achieve accurate tracking. 3DeeCellTracker was implemented on a desktop PC to efficiently and flexibly track cells over hundreds of volumes of 3D + T images recorded under highly divergent optical or imaging conditions. Using only one volume for training and one initial correction, 3DeeCellTracker efficiently tracked 100–150 neurons in the brains of semi-immobilized or ‘straightened’ (special pre-processing step based on the worm's posture) freely moving <italic>Caenorhabditis elegans</italic> roundworms from four image datasets, obtained using spinning disk confocal systems in three different laboratories. With a few modifications, 3DeeCellTracker also tracked ~100 cells in the naturally beating heart of a zebrafish larva monitored using swept confocally aligned planar excitation (SCAPE) microscopy, a novel oblique light sheet microscope system for very rapid 3D + T imaging (<xref ref-type="bibr" rid="bib5">Bouchard et al., 2015</xref>; <xref ref-type="bibr" rid="bib42">Voleti et al., 2019</xref>). Furthermore, 3DeeCellTracker tracked ~900 cells in a cultured 3D tumor spheroid, which were imaged with a two photon microscope system. Our pipeline provided robust tracking results from the above-mentioned real datasets as well as from degenerated datasets, which differed in terms of signal-to-noise ratios, cell movements, and resolutions along the <italic>z</italic>-axis. Notably, 3DeeCellTracker's performance was better in terms of the tracking results than those from recently developed 2D/3D tracking software running on a desktop PC, and comparable to software running on ﻿a high-performance computing cluster (<xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bannon et al., 2018</xref>). Furthermore, by using the positional information of the tracked cells, we extracted dynamics of the cells: the worm's neurons exhibited complex activity patterns in the brain, the zebrafish heart cells exhibited activities synchronized with heart chamber movement, and the tumor spheroid cells exhibited spontaneous activities without stimulation. These results indicate that, 3DeeCellTracker is a robust and flexible tool for tracking cell movements in 3D + T images, and can potentially enable the analysis of cellular dynamics that were previously difficult to investigate.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Overview</title><p>We developed a new pipeline, 3DeeCellTracker, which integrates novel and existing techniques (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). After preprocessing (see Materials and methods), it performs automatic segmentation of cells in all 3D + T images using 3D U-Net for classifying individual voxels into cell or non-cell categories (<xref ref-type="bibr" rid="bib32">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Çiçek et al., 2016</xref>). Continuous regions of ‘cell’ voxels are separated into individual cell regions using the watershed method (<xref ref-type="bibr" rid="bib4">Beucher and Meyer, 1993</xref>), and then numbered. The segmented cells are manually corrected only in the first volume of 3D images. In the following 3D tracking step, we considerably increased the efficiency by introducing a deep learning technique, feedforward network (FFN), to predict cell positions based on spatial patterns of cells maintained between previous and current images. The predicted positions are corrected with a non-rigid point set registration method called PR-GLS (<xref ref-type="bibr" rid="bib22">Ma et al., 2016</xref>) and by our custom method to obtain precise cell locations, which are critical for extracting the accurate dynamics of cellular signals. The 3D U-Net and the FFN are pre-trained using manually confirmed data or synthetic data (<xref ref-type="fig" rid="fig1">Figure 1B,C</xref>) from a single 3D image volume. The tracking results were visually inspected by comparing the locations of tracked cells with the corresponding raw images (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The keys to our method are the use of simulation to produce large amounts of training data for the FFN and the carefully designed post-processing methods for the FFN, which result in the flexible and robust tracking of moving cells in very different 3D + T datasets. In the following two sections, we describe details of the segmentation and tracking method.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overall procedures of our tracking method and the training procedures of the deep networks.</title><p>(<bold>A</bold>) Procedures of segmentation and tracking. 3D + T images are taken as a series of 2D images at different <italic>z</italic> levels (step 1) and are preprocessed and segmented into discrete cell regions (step 2). The first volume (<italic>t</italic> = 1) of the segmentation is manually corrected (step 3). In the following volumes (<italic>t</italic> ≥ 2), by applying the inferred transformation functions generated by FFN + PR-GLS (step 4), the positions of the manually confirmed cells are successively updated by tracking followed by accurate corrections (A.C.) (step 5). The circled numbers indicate the five different procedure steps. (<bold>B</bold>) Procedures for training the 3D U-net. The cell-like regions predicted by 3D U-net are compared with manually labeled cell regions. The errors (orange) are used to update U-net weights. (<bold>C</bold>) Procedures for training the feedforward network. The movements of cells used for training are generated from simulations, thus their correct matches are known. The errors in the predictions are used to update the feedforward network weights.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Illustration of segmentation and tracking.</title><p>Cell nuclei labeled with a fluorescent protein are scanned into 3D + T images and segmented into individual nuclei. The positions of segmented nuclei varied over time due to organ movement and/or deformation. These nuclei are tracked by updating their positions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Illustration of the visual inspection of the tracking results.</title><p>The tracking results were compared with the raw images in each volume visually to find tracking mistakes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig1-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Segmentation</title><p>For segmentation, cell-like regions in an image should be segmented into individual cells which may differ in intensities, sizes, shapes, and textures. Cell segmentation in 2D images using deep networks has been previously reported (<xref ref-type="bibr" rid="bib32">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Van Valen et al., 2016</xref>; <xref ref-type="bibr" rid="bib3">Bannon et al., 2018</xref>). In this study, we utilized a deep network called 3D U-Net to segment cells in 3D images to predict the class labels (cell or non-cell) of individual voxels based on information contained in neighboring voxels (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; <xref ref-type="bibr" rid="bib32">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Çiçek et al., 2016</xref>). The U-Net can generate precise segmentation under diverse imaging conditions and can be trained with very few annotated images (e.g. only one 3D image volume in this study; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Detailed descriptions of segmentation and tracking.</title><p>(<bold>A</bold>) Details of segmentation. The circled numbers correspond to the numbers in <xref ref-type="fig" rid="fig1">Figure 1</xref>, while the numbers 2–1 and 2–2 indicate cell-like voxels detected by 3D U-net and individual cells segmented by watershed, respectively. One volume of a worm neuron dataset is used as an example. (<bold>B</bold>) Left: Definition of the positions in two point sets corresponding to two volumes. Right: Structure of the feedforward network for calculating the similarity score between two points in two volumes. The numbers on each layer indicate the shape of the layer. ReLU: Rectified linear unit. BN: Batch normalization. (<bold>C</bold>) Details of tracking. 4–1 and 4–2 indicate the initial matching from <italic>t</italic> = 1 to <italic>t</italic> = 2 using our custom feedforward network and the more coherent transformation function inferred by PR-GLS, respectively. The orange lines in 4–1 and 4–2 indicate the inferred matching/transformation from <italic>t</italic> = 1 to <italic>t</italic> = 2.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Structures of the 3D U-Net used in this study.</title><p>We designed different structures for adopting different resolutions of images, which is preferred but not necessary. End-users can freely choose one of the structures for their own data for simplicity. (<bold>A</bold>) Used for datasets worm #1 and #2, and for the binned zebrafish dataset. (<bold>B</bold>) Used for dataset worm #3 and the degenerated worm datasets. (<bold>C</bold>) Used for the raw zebrafish dataset. Each raw 3D image was split into sub-images and then sent into the network as input (top left) to generate maps of cell regions as output (top right), finally these maps for sub-images were combined. In the network, images were filtered and compressed to lower-resolution then recovered to the original resolution again, in order to utilize both local and global features. Numbers on each intermediate layer indicate the number of convolutional filters, while numbers on the left of each row indicate the size of the 3D images of input, output, and intermediate layers. Conv: Convolution; BN: Batch normalization.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Illustration of the FFN training method.</title><p>(<bold>A</bold>) Generation of the synthetic data for training the FFN. The locations of a point set (orange circles) were transformed by affine function with different parameters, and additional movements of the transformed points were incorporated by adding random movements to each point. (<bold>B</bold>) The matching by FFN between two test point sets, which was improved in accuracy over the course of training.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Comparison of our method with previous tracking techniques.</title><p>(<bold>A and B</bold>) Comparison of the initial matchings made by FPFH (<xref ref-type="bibr" rid="bib33">Rusu et al., 2009</xref>) and FNN. Cell centers were generated from the 3D U-net + watershed segmentation of two volumes in worm #3 dataset. Blue dots indicate cell centers, and orange lines indicate the estimated matches between these two point sets. We manually counted the error rates. In the regular process, matching is between temporally adjacent volumes. In this panel, we performed matching between the points separated for 10 volumes (t1 and t2) as an example of a challenging condition. (<bold>C</bold>) The predicted positions from the three methods (affine alignment, FPFH + PR-GLS, and FFN + PR-GLS) and the real positions of cells at <italic>t2</italic>. (<bold>D</bold>) The distances between the predicted positions and their real positions for the three methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Illustration of the accurate correction.</title><p>(<bold>A and B</bold>) Our method for accurately correcting cell locations. (<bold>A</bold>) Correction method in the single cell case. We obtained the region (the large solid black ellipse) and the center (small black circle) of a cell predicted by PR-GLS from the previous volume. We also obtained cell regions detected by 3D U-Net (the dashed blue ellipse). Subsequently, we calculated the intersection of the PR-GLS region and 3D U-Net regions (filled with diagonal blue lines) and its center (small blue circle). The final accurate correction follows the small black arrow, that is the center of the cell should move from the small black circle to the small blue circle. (<bold>B</bold>) Correction method when two regions predicted by PR-GLS partially overlapped. The method is essentially the same as in (<bold>A</bold>), except that areas where two PR-GLS regions overlapped were removed from the intersection region of PR-GLS and 3D U-net, generating two smaller regions (diagonal blue lines). We used this technique to prevent the two cells from merging in tracking results. (<bold>C</bold>) An example of accurate correction with a worm neuron dataset. The locations of the cells predicted by FFN + PR-GLS (middle) had small discrepancies with their real positions in the raw image (top). Such discrepancies were corrected (white arrows) by our method (bottom). All panels are correspoinding to a 2D layer of a 3D image at <italic>z</italic> = 66.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig2-figsupp4-v2.tif"/></fig></fig-group><p>The pre-processed images of the first volume are used to train the 3D U-Net (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), and the trained U-net is then used for the segmentation of cells in all the following volumes. Once trained on one dataset, the 3D U-Net can be directly reused for different datasets obtained under similar optical conditions. The cell-like regions detected using 3D U-Net are grouped and separated into individual cells using the watershed method (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> and Materials and methods).</p></sec><sec id="s2-3"><title>Tracking</title><p>For tracking cells, two major strategies can be considered. One strategy is to utilize the information contained in each cell region, for example, local peaks or distributions of intensities (<xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>). Using this information, a program can update the position of each cell by searching for its new position in nearby regions in the next volume. However, the obvious drawback of this strategy is that cells can be mistracked if their movements are comparable to or greater than the distances between adjacent cells (see below).</p><p>Another strategy is to represent cells by their center points, ignoring the information in each cell region and treating the cells as a set of points. In this strategy, the new positions of a cell set in the next volume can be estimated based on the patterns of their spatial distributions, and cells with large movements can also be tracked based on the global trend of the movements. Previously reported methods utilizing this strategy, called point set registration (<xref ref-type="bibr" rid="bib22">Ma et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">Jian and Vemuri, 2005</xref>; <xref ref-type="bibr" rid="bib25">Myronenko et al., 2006</xref>), used the spatial distributions and coherency of movements to track points within artificial datasets. However, the spatial patterns are conventionally characterized by features designed by experts, an approach that did not work well for the cell images used in this study (refer to the results below concerning Fast Point Feature Histograms [FPFH; <xref ref-type="bibr" rid="bib33">Rusu et al., 2009</xref>] versus FFN). Another problem with this strategy is that the estimated positions are not always accurate because the information contained in each cell region is ignored (see the results below concerning our method for accurate correction).</p><p>In order to obtain more robust and accurate tracking results, we integrated the spatial pattern (i.e. point sets) and local cell region strategies and used a deep learning technique, FFN, for the former. We used FFN to match temporally adjacent cells based on the distance pattern between each cell and its 20 surrounding cells (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Using the pattern obtained by the FFN, all the cells at <italic>t</italic><sub>1</sub> are compared with all the cells at <italic>t</italic><sub>2</sub>, and the most similar ones are regarded as the same cells at <italic>t</italic><sub>1</sub> and <italic>t</italic><sub>2</sub>, a process we call initial matching.</p><p>Although deep network techniques are expected to produce superior cell tracking results, the method had not been used previously because it requires large amounts of training data. These data are difficult to prepare manually, especially for 3D + T images, as validating and correcting large numbers of cell positions over time by switching multiple layers along the <italic>z</italic> axis is virtually impossible. To solve this difficulty in preparing training data for FFN, we generated &gt;500,000,000 synthetic training data points by simulating cell movements (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> and Materials and methods).</p><p>In the pipeline, the center points of cells are extracted from the cell regions segmented by 3D U-Net and the watershed method, and a pre-trained FFN (<xref ref-type="fig" rid="fig1">Figures 1C</xref> and <xref ref-type="fig" rid="fig2">2B</xref>) is applied to the cell points to generate the initial matching from volumes <italic>t</italic> to <italic>t</italic>+1 (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, panels 2–2 and 4–1). To improve the initial matching, a non-rigid point set registration method (PR-GLS) (<xref ref-type="bibr" rid="bib22">Ma et al., 2016</xref>) is used to generate a coherent transformation, that is, neighboring cells should have similar movements (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, panel 4–2). Originally, PR-GLS was used in conjunction with the FPFH method to predict cell positions (<xref ref-type="bibr" rid="bib22">Ma et al., 2016</xref>); however, our FFN generates more accurate initial matchings than FPFH (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A and B</xref>), and our combination of FFN + PR-GLS generates more accurate predictions of cell positions than the FPFH + PR-GLS or the classic affine alignment (<xref ref-type="bibr" rid="bib25">Myronenko et al., 2006</xref>) does (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3C and D</xref>). Nevertheless, our method sometimes generates subtle errors because some cells may show slightly different movements from those of their neighboring cells, which can accumulate over time. To overcome these errors, the estimated positions are accurately corrected to compensate for their differences by utilizing information from local cell regions contained in the 3D U-Net output (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, panel 5, <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>, and Materials and methods).</p></sec><sec id="s2-4"><title>Tracking neurons in the deforming worm’s whole brain</title><p>To test the performance of 3DeeCellTracker, we analyzed 3D + T images of neurons in the deforming brain of <italic>C. elegans. C. elegans</italic> has been used as a model for imaging all neuronal activities in the brain (‘whole brain imaging’) owing to its small brain (~40 µm<sup>3</sup> in an adult) in a transparent body, the complete description of all connections of 302 neurons, feasibility in the use of genetically encoded calcium indicators (GECI), and the capability of perception, memory, and decision-making in its small brain (<xref ref-type="bibr" rid="bib8">de Bono and Maricq, 2005</xref>). Whole brain imaging of <italic>C. elegans</italic> has been reported from several laboratories, and the most popular optical system currently is the spinning disk confocal system, for which each laboratory has developed their own tracking software (<xref ref-type="bibr" rid="bib34">Schrödel et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Venkatachalam et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>). We established our own spinning disk confocal system for whole-brain imaging, OSB-3D (see Materials and methods for details), and obtained 3D + T images of whole brain activity from a strain established in our laboratory and from the one used in a previous study by <xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref> (datasets worm #1 and #2, respectively). In addition, we analyzed the whole-brain images published previously using a different optical system (<xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>; dataset worm #3). In all datasets, the worms were semi-immobilized either by anesthetization (worm #1 and #2) or by constriction (worm #3), and red fluorescent proteins and GECI were used to monitor cell positions and cell activities, respectively.</p><p>After segmentation, we manually confirmed 164 neurons in the head in the first volume of 3D + T images, in which the distribution of cell signals was mostly, but not completely, separated from the background signals (dataset worm #1a; see <xref ref-type="fig" rid="fig3">Figure 3A–C</xref>). While the worm was anesthetized, its head deformed and the cells moved during imaging. Our method tracked all the neurons in all 171 volumes except for those that moved out of the field of view of the camera (<xref ref-type="fig" rid="fig3">Figure 3D–F</xref>, <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Results for dataset worm #1a.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) The distribution (top) and the time-course changes (middle) of the intensities in cell regions and in the background, and the distribution of relative movements (bottom). The distributions were estimated using the ‘KernelDensity’ function in Scikit-learn library in Python. The intensities were calculated using 8-bit images transformed from the 16-bit raw images. The relative movement (<italic>RM</italic>) is the normalized cell movement: <italic>RM</italic> = movement/(distance to the closest cell). The critical values, <italic>RM</italic> = 0.5 and 1.0, are emphasized by dashed and dash-dotted lines, respectively (see <xref ref-type="fig" rid="fig4">Figure 4</xref> and Materials and methods for details). CI: confidence interval. (<bold>C</bold>) 3D image and its segmentation result in volume #1. Top left: Five layers (Z4–Z8) of the raw 2D images. Top right: Cell-like regions corresponding to the 2D images at left, detected by the 3D U-Net. Middle: Cell-like regions of the 3D view including all layers. Bottom: Final segmentations using watershed plus manual correction. Different colors represent different cells. (<bold>D</bold>) Tracking results. Tracked cells in volume #86 and volume #171 are shown, which are transformed from the segmentation in volume #1. In each panel, the top shows the cell-like regions detected by 3D U-Net, and the bottom shows tracked cells. Arrows indicate two example cells with large (N98) and small movement (N27). All cells were correctly tracked. (<bold>E</bold>) The tracking accuracy through time. (<bold>F</bold>) Movements of the two representative cells N98 and N27 in <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> axes, indicating an expansion of the worm mainly in the <italic>x-y</italic> plane. The initial positions at volume #1 were set to zero for comparison. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Results for dataset worm #1b.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) Intensities and movements. (<bold>C</bold>) 3D image and its segmentation result in volume #1. (<bold>D</bold>) Tracking results. Tracked cells in volume #120 and volume #240 are shown. Arrows and arrowheads indicate two tracked cells. All the cells were correctly tracked except for those that moved out of the field of view of the camera. (<bold>E</bold>) Tracking accuracy through time. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Results for dataset worm #2.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) Intensities and movements. (<bold>C</bold>) 3D image and its segmentation result in volume #1. (<bold>D</bold>) Tracking results. Tracked cells in volume #19 and #36 are shown. Arrows and arrowheads indicate two tracked cells. All cells were correctly tracked. (<bold>E</bold>) Tracking accuracy through time. (<bold>F</bold>) Movements of the two representative cells N29 and N99. When compared with <xref ref-type="fig" rid="fig3">Figure 3F</xref>, these cells showed small but irregular movements in the <italic>x-y</italic> plane. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Results for dataset worm #3.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) Intensities and movements. (<bold>C</bold>) 3D image and its segmentation result in volume #1. (<bold>D</bold>) Tracking results. The tracked cells in volume #275 and #591 are shown. Arrows and arrowheads indicate two tracked cells. For their movements, see <xref ref-type="fig" rid="fig9">Figure 9L</xref>. Only four neurons were mistracked. (<bold>E</bold>) Tracking accuracy through time. (<bold>F</bold>) The positions of cells with/without tracking mistakes. Some small cells are invisible. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig3-figsupp3-v2.tif"/></fig><media id="fig3video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig3-video1.mp4"><label>Figure 3—video 1.</label><caption><title>Tracking results for dataset worm #1a.</title><p>The animation of cell regions (top) detected by 3D U-net and corresponding individual cells (bottom) in dataset #1a tracked by our method.</p></caption></media><media id="fig3video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig3-video2.mp4"><label>Figure 3—video 2.</label><caption><title>Tracking results for dataset worm #2.</title><p>The animation of cell regions (top) detected by 3D U-net and corresponding individual cells (bottom) in dataset #2 tracked by our method.</p></caption></media><media id="fig3video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig3-video3.mp4"><label>Figure 3—video 3.</label><caption><title>Tracking results for dataset worm #3.</title><p>The animation of cell regions (top) detected by 3D U-net and corresponding individual cells (bottom) in dataset #3 tracked by our method.</p></caption></media></fig-group><p>To evaluate the difficulty in tracking cells with large movements, we calculated a score for the relative movements (<italic>RM</italic>), which is the movement of a cell divided by the distance from that cell to its closest neighboring cell. When <italic>RM</italic> is small, searching for the closest cell is the simplest method to find the new position of a given cell in the next volume. However, such a simple approach may lead to tracking errors when <italic>RM</italic> ≥0.5 (<xref ref-type="fig" rid="fig4">Figure 4</xref> and Materials and methods). Although most of the cell <italic>RM</italic> values were ≤0.5 in the worm #1 dataset (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, bottom), other datasets had cells with <italic>RM</italic> ≥0.5; nevertheless, these were also successfully tracked by our program (see below).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Quantitative evaluation of the challenging relative movement (<italic>RM</italic>) of cells.</title><p>(<bold>A</bold>) Illustrations of movements in one dimensional space with <italic>RM</italic> = 0.2, 0.5, and 1.0. Here <italic>RM</italic> = (movement a cell traveled per volume)/(distance to the closest neighboring cell). When <italic>RM</italic> ≥0.5, cell A at <italic>t</italic> = 2 will be incorrectly assigned to cell B at <italic>t</italic> = 1 if we simply search for its closest cell instead of considering the spatial relationship between cells A and B. When <italic>RM</italic> = 0.5, cell assignment is also impossible. Please also see Materials and methods for details. (<bold>B</bold>) Examples of large cell movements with <italic>RM</italic> ≈ 1. Also see <xref ref-type="fig" rid="fig10">Figure 10</xref> and <xref ref-type="table" rid="table3">Table 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig4-v2.tif"/></fig><p>We also tested worm #1b dataset, obtained from the same strain with worm #1a (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), in which we again achieved 100% tracking accuracy without changing any parameters aside from the noise level (see <xref ref-type="table" rid="table1">Tables 1</xref> and <xref ref-type="table" rid="table2">2</xref>). The positional information of cell nuclei in both datasets was then used to extract the calcium dynamics of the neurons based on GECI intensities, which reflect spontaneous activities of neurons in the worm's brain (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Localization and calcium dynamics in all 137 neurons for dataset worm #1b.</title><p>(<bold>A</bold>) The ID numbers of neurons were superimposed on the image of cell nuclei (tdTomato) projected onto the 2D plane. The orientation of the image has been rotated to align the anterior-posterior axis with the horizontal direction. To increase the visibility of both nuclei and IDs, we inverted the intensity of the nuclei image and displayed it using the pseudocolor ‘glow’. (<bold>B</bold>) Extracted calcium dynamics (GCaMP / tdTomato) in all 137 neurons. All cells were correctly tracked, while some of them moved out of the field of the camera and thus their activities were recorded for shorter periods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Localization and calcium dynamics in all 164 neurons for dataset worm #1a.</title><p>(<bold>A</bold>) The ID numbers of neurons were superimposed on the image of cell nuclei (tdTomato) projected onto the 2D plane. (<bold>B</bold>) Extracted activities (GCaMP/tdTomato) in all 164 neurons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig5-figsupp1-v2.tif"/></fig><media id="fig5video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig5-video1.mp4"><label>Figure 5—video 1.</label><caption><title>GCaMP signals in dataset worm #1a.</title><p>The dynamics and movements of GCaMP signals in dataset #1a projected onto the 2D plane. The intensity was displayed using the pseudocolor ‘fire’.</p></caption></media><media id="fig5video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig5-video2.mp4"><label>Figure 5—video 2.</label><caption><title>GCaMP signals in dataset worm #2.</title><p>The dynamics and movements of GCaMP signals in dataset #2 projected onto the 2D plane.</p></caption></media></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Comparison between the complexities of our method and a previous method.</title><p>For each method, the procedures are listed along with the number of parameters (in bold) to be manually determined by researcher (see the guide in our GitHub repository). Our method requires manual determination of less than half of the parameters required by the previous method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" valign="top"/><th colspan="2" valign="top">Our method</th><th colspan="2" valign="top"><xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref></th></tr><tr><th valign="top">Procedures</th><th valign="top">Parameters</th><th valign="top">Procedures</th><th valign="top">Parameters</th></tr></thead><tbody><tr><td rowspan="4">Pre-processing</td><td valign="top">Alignment</td><td valign="top">-</td><td valign="top">Alignment</td><td valign="top">-</td></tr><tr><td rowspan="3">Local contrast normalization</td><td rowspan="3"><bold>(1) Noise level</bold></td><td valign="top">Denoising (median filter)</td><td><bold>(1) Radius</bold></td></tr><tr><td valign="top">Background subtraction</td><td><bold>(2) Radius</bold></td></tr><tr><td valign="top">Gaussian blurring</td><td><bold>(3) Radius</bold></td></tr><tr><td rowspan="4">Segmentation</td><td rowspan="2">3D U-net <break/>(detect cell/non-cell regions)</td><td rowspan="2">Automatically learned</td><td>Thresholding</td><td><bold>(4) Method (e.g., ‘mean’,’ triangle’)</bold></td></tr><tr><td>Watershed 1</td><td><bold>(5) Radius</bold> <break/><bold>(6) Minimum size of cells</bold></td></tr><tr><td rowspan="2">Watershed (twice)</td><td rowspan="2"><bold>(2) Minimum size of cells</bold></td><td>Counting number of negative curvature</td><td><bold>(7) Threshold</bold> <break/> <bold>(8) Distance to borders</bold></td></tr><tr><td>Watershed 2</td><td> <bold>-</bold></td></tr><tr><td rowspan="3">Tracking</td><td>Feedforward network (generate an initial matching)</td><td>Automatically learned</td><td rowspan="2">Least squares fitting of Gaussian mixture</td><td rowspan="2"><bold>(9-11) Default value of covariance matrix (3d vector)</bold></td></tr><tr><td>PR-GLS (generate a coherent transformation from the initial matching)</td><td><bold>(3,4) Coherence level controlled by β and λ</bold> <break/><bold>(5) Maximum iteration</bold></td></tr><tr><td>Accurate correction using intensity information</td><td>-</td><td>Removing over-segmentation</td><td><bold>(12) Distance</bold></td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Values of parameters used for tracking each dataset.</title><p>Noise levels are more variable and dependent on image quality and the pre-trained 3D U-net. Other parameters are less variable and dependent on the sizes of cells or the coherence levels of cell movements, thus do not need to be intensively explored for optimization when imaging conditions are fixed. See the user-guide in GitHub for how to set these parameters. 3D U-net structure corresponds to <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A,B and C</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" rowspan="2">Dataset</th><th rowspan="2">Resolution <break/>(µm/voxel)</th><th rowspan="2">3D U-net structure</th><th colspan="5">Parameters</th></tr><tr><th>Noise level</th><th>Minimum size of cells</th><th>Coherence level: β</th><th>Coherence level: λ</th><th>Maximum iteration</th></tr></thead><tbody><tr><td colspan="2">Worm #1a</td><td rowspan="3"><italic>x-y</italic>: 0.16 <break/><italic>z</italic>:1.5</td><td rowspan="3">A</td><td>20</td><td rowspan="3">100</td><td rowspan="3">300</td><td rowspan="10">0.1</td><td rowspan="3">20</td></tr><tr><td colspan="2">Worm #1b</td><td>8</td></tr><tr><td colspan="2">Worm #2</td><td>20</td></tr><tr><td colspan="2">Worm #3</td><td rowspan="7"><italic>x-y</italic>: 0.33 <break/><italic>z</italic>: 1.4</td><td rowspan="7">B</td><td>70</td><td rowspan="7">10</td><td rowspan="7">150</td><td rowspan="7">20</td></tr><tr><td rowspan="3">Worm #3 + noise:</td><td>sd = 60</td><td>190</td></tr><tr><td>sd = 100</td><td>320</td></tr><tr><td>sd = 140</td><td>450</td></tr><tr><td rowspan="3">Worm #3 + reduced sampling</td><td>1/2</td><td rowspan="3">70</td></tr><tr><td>1/3</td></tr><tr><td>1/5</td></tr><tr><td colspan="2">Worm #4 <break/>(freely moving)</td><td>~0.3 (estimated)</td><td>B</td><td>200</td><td>400</td><td>1000</td><td>0.00001</td><td>50</td></tr><tr><td rowspan="2">Zebrafish</td><td>raw</td><td><italic>x</italic>: 0.86 <break/><italic>y</italic>: 1.07 <break/><italic>z</italic>: 2.09</td><td>C</td><td>200</td><td>40</td><td rowspan="2">50</td><td rowspan="2">0.1</td><td rowspan="2">50</td></tr><tr><td>binned</td><td><italic>x</italic>: 0.86 <break/><italic>y</italic>: 1.07 <break/><italic>z</italic>: 4.18</td><td>A</td><td>1</td><td>10</td></tr><tr><td colspan="2">Tumor spheroid</td><td><italic>x-y</italic>: 0.76 <break/><italic>z</italic>: 4.0</td><td>B</td><td>500</td><td>100</td><td>1000</td><td>0.00001</td><td>50</td></tr></tbody></table></table-wrap><p>The same method was applied to 3D + T images obtained using the same OSB-3D system in our laboratory but from the neurons of a worm strain used in a previous whole brain imaging study (<xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref>; AML14 strain, dataset worm #2; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). Our method again achieved a 100% accuracy of tracking and extracted calcium signals from all 101 neurons even though this dataset differs considerably from the worm #1 dataset in terms of nuclear marker intensity and movement patterns (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A–F</xref>, <xref ref-type="video" rid="fig3video2">Figure 3—video 2</xref> and <xref ref-type="video" rid="fig5video2">Figure 5—video 2</xref>). It should be noted that more neurons were detected using our method than using the original method (approximately 90 neurons or less) (<xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref>).</p><p>We also applied our method to a publicly available dataset, which was obtained using a different strain, a different microscopy setup, and different imaging conditions from worm #1 and #2 datasets (worm #3, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>; <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>). In this case, the worm was loosely constrained in a microfluidic device and not anesthetized, thus exhibited larger head deformations and cell displacements between volumes (by a factor of approximately three times) compared to the worm #1 and #2 datasets (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A and B</xref>). In addition, this dataset had a lower resolution (half the resolution in the <italic>x</italic> and <italic>y</italic> directions). Nevertheless, with a few parameter modifications (<xref ref-type="table" rid="table2">Table 2</xref> and Materials and methods), our method correctly tracked 171/175 (=98%) of the neurons (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3C–F</xref> and <xref ref-type="video" rid="fig3video3">Figure 3—video 3</xref>). Our result was comparable to that of the original report, in which 171 out of 198 neurons were tracked without error (<xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>). These results indicate that our method can flexibly process 3D + T images of neurons in a semi-immobilized worm's brain obtained under different conditions.</p></sec><sec id="s2-5"><title>Tracking neurons in freely moving worm’s brains</title><p>To reveal the relationships between neuronal activities and animal behavior, Nguyen et al., developed a method to track neurons in a freely moving worm, in which the deformation of the brain and the movements of neurons are considerably larger than those occurring in semi-immobilized worms (<xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>). After ‘straightening’ the worm and segmentation of its neurons, they made a set of reference volumes to which each neuron was matched to assign its identity. Although this method is powerful, it requires a high-performance scientific computing cluster running on up to 200 cores simultaneously. We therefore tested whether the 3DeeCellTracker implemented on a desktop PC could process such a challenging dataset (worm #4).</p><p>With a few modifications (see Materials and methods), we used 3DeeCellTracker to segment and track 113 cells in the initial 500 volumes from the worm #4 dataset. Here we analyzed the images preprocessed by the straightening method (<xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>), which is necessary for our current method. Even after the straightening, the cell movements were quite large, with many comparable to the distances between cells, that is, <italic>RM</italic> ≥0.5 (<xref ref-type="fig" rid="fig6">Figure 6A and B</xref>). We visually inspected the tracking of 90 cells while the remaining 23 were not checked because of difficulties arising from weak intensities/photobleaching and/or the cells being in dense regions. Note that 70 cells were inspected in the original report (<xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>). We found that 66 out of 90 cells (73%) were correctly tracked without errors (<xref ref-type="fig" rid="fig6">Figure 6D–F</xref>, single mode), which is acceptable but not ideal.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Results for dataset worm #4 – ‘straightened’ freely moving worm.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) Intensities and movements. (<bold>C</bold>) The two methods (single mode and ensemble mode) we used for tracking this dataset. (<bold>D</bold>) The results of segmentation and tracking using the two methods. The intensity of the raw images (top) was enhanced for the visibility (adjusted ‘gamma’ value from 1 to 0.5). (<bold>E</bold>) The tracking accuracy through time. (<bold>F</bold>) The positions of 90 cells with/without tracking mistakes in the single (top) and ensemble (bottom) modes. (<bold>G</bold>) Movements of the two representative cells N14 and N102 in <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> axes. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig6-v2.tif"/></fig><media id="fig6video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig6-video1.mp4"><label>Figure 6—video 1.</label><caption><title>Tracking results for the ‘straightened’ freely moving dataset worm #4.</title><p>The animation of cell regions (top) detected by 3D U-net and corresponding individual cells (bottom) in dataset #3 tracked by our method in ensemble mode.</p></caption></media></fig-group><p>To further improve our method for the larger cell movements, we developed a new mode, in which multiple predictions of cell positions are made from different previous time points, and the final prediction is taken as the average of these predictions. We call this ‘ensemble mode’ and the previous method ‘single mode’, wherein the predictions of cell positions at time <italic>t</italic> are derived from the results at <italic>t</italic>-1 (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p><p>When applying the ensemble mode, we again found 66 cells correctly tracked without errors, and that the remaining 24 cells were correctly tracked in most volumes (94%–98.2%). This was a substantial improvement over the single mode, in which errors at <italic>t</italic> are maintained until the end of tracking (<xref ref-type="fig" rid="fig6">Figure 6D–F</xref>, ensemble mode; <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref>). In total, the ensemble mode correctly tracked 44,905 out of 45,000 cell movements (99.8%), a result at least comparable to that in the original report (<xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>). From the trajectories of two example cells, we found that large movements, including ones along the z-axis (across ~30 layers), occurred frequently (<xref ref-type="fig" rid="fig6">Figure 6G</xref>), demonstrating the excellent performance of our method on a desktop PC in this challenging dataset containing considerably large scale movements. However, because the ensemble mode requires longer times for the tracking than the single mode (10 versus 2 min/volume, respectively), we used the single mode in the following analyses.</p></sec><sec id="s2-6"><title>Tracking cells in beating zebrafish heart images obtained using the SCAPE system</title><p>To test the general applicability of the 3DeeCellTracker, we applied our method to the 3D + T images of a naturally beating heart in a zebrafish larva obtained at 100 volumes per second using a substantially different optical system, the SCAPE 2.0 system (<xref ref-type="bibr" rid="bib42">Voleti et al., 2019</xref>; <xref ref-type="fig" rid="fig7">Figure 7A</xref>). The speed of image acquisition of this system is extraordinary relative to that of the spinning confocal system, which generally allows for the recording of ≤10 volumes/s. This dataset includes both large cells with stronger intensities that are easy to segment and small cells with weaker intensities that are difficult to segment (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, top and middle). The photobleaching in the dataset made it challenging to detect and track the weak intensity cells in the later part of the imaging, because of the substantial overlap between the small cell signals and background signals (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). This weak intensity is unavoidable because of the extremely quick scanning rate of the system (10,568 frames per second). In addition, the rapid beating of the heart caused relatively large movements of all the cells in the <italic>x-y-z</italic> directions (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, bottom; <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>; see below), making cell tracking more challenging than that for worm #1–3, which predominantly moved in the <italic>x-y</italic> plane.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Results from tracking beating cardiac cells in zebrafish.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) Intensities and movements of the correctly tracked cells with large size (26 cells) or small size (40 cells). (<bold>C</bold>) 2D and 3D images and its segmentation result in volume #1. The large bright areas in Z110 and Z120 indicated by arrows are irrelevant for the tracking task, and our 3D U-net correctly classified them as background. Orange and blue outlines in bottom panels indicate the regions of the ventricle and atrium, respectively. (<bold>D</bold>) Tracking results. Tracked cells in volume #531 and #1000 are shown; we showed #531 because #500 looks similar to #1. Arrows indicate two correctly tracked representative cells in the ventricle and atrium. The sizes of the ventricle and atrium changed periodically (orange and blue outlines). (<bold>E</bold>) Tracking accuracy in 30 large cells or in all 98 cells through time. (<bold>F</bold>) The positions of 98 cells with/without tracking mistakes. (<bold>G</bold>) Movements of the two representative cells N9 and N95, indicating regular oscillations of cells in whole 3D space. (<bold>H</bold>) Dynamics of ventricle size, atrium size, and calcium signals in ventricle cells and atrium cells. The sizes of the ventricle and atrium cannot be directly measured, so we instead estimated them as <inline-formula><mml:math id="inf1"><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, where <italic>sd</italic> is standard deviation (more robust than range of <italic>x</italic>, y, z), and <italic>x</italic>, <italic>y</italic>, <italic>z</italic> are coordinates of correctly tracked cells in the ventricle or in the atrium. To improve visibility, these sizes were normalized by <inline-formula><mml:math id="inf2"><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi> <mml:mi/><mml:mo>(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">z</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>/</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Calcium signals (GCaMP) were also normalized by <inline-formula><mml:math id="inf3"><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">z</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></inline-formula>. (<bold>I</bold>) Phase differences between intracellular calcium dynamics and the reciprocal of segment sizes in ventricle and atrium were estimated. Here we used reciprocal of segment sizes because we observed the anti-synchronization relationships in (<bold>H</bold>). The phase differences were estimated using cross-correlation as a lag with the largest correlation. Most cells showed similar phase differences (mean = −0.110 π; standard deviation = 0.106 π). All scale bars, 40 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Examples of cells with weak intensities and photobleaching in the zebrafish dataset.</title><p>All cells were visualized in 3D view. The left and right images are displayed under the same brightness/contrast setting; the right image (Volume #1000) was much darker than the left image (Volume #1) due to photobleaching. Arrows: example cells with weaker intensities that were more difficult to segment and track than the stronger ones. All scale bars, 40 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig7-figsupp1-v2.tif"/></fig><media id="fig7video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig7-video1.mp4"><label>Figure 7—video 1.</label><caption><title>dsRed signals in the zebrafish dataset.</title><p>The intensity and movements of cell nuclei (dsRed) in the zebrafish dataset projected onto the 2D plane. The intensity of nuclei changed due to contraction and expansion of cells during beating.</p></caption></media><media id="fig7video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig7-video2.mp4"><label>Figure 7—video 2.</label><caption><title>Tracking results of the zebrafish dataset.</title><p>The animation of beating cardiac cell regions (left) detected by 3D U-net and corresponding individual cells (right) in the zebrafish dataset tracked by our method.</p></caption></media></fig-group><p>After segmentation, we manually confirmed 98 cells in the first volume and tracked them automatically (<xref ref-type="fig" rid="fig7">Figure 7C and D</xref>; <xref ref-type="video" rid="fig7video2">Figure 7—video 2</xref>). We found that among the 30 larger cells (size: 157–836 voxels) with higher intensities, 26 of them (87%) were correctly tracked in all 1000 volumes. Even though the smaller cells with lower intensities were more difficult to track, when including them, we still correctly tracked 66 out of 98 cells (67%) (<xref ref-type="fig" rid="fig7">Figure 7E and F</xref>). The tracked movements of two example cells showed regular oscillations in 3D space (<italic>x</italic>, <italic>y</italic>, and <italic>z</italic> axes; <xref ref-type="fig" rid="fig7">Figure 7G</xref>), consistent with the regular beating movement of the heart. It should be noted that we did not optimize the pipeline procedures for the zebrafish data, except for a few parameter changes (<xref ref-type="table" rid="table2">Table 2</xref>). Our results indicate that 3DeeCellTracker is also capable of analyzing images with rapid and dynamic movements in 3D space, obtained from a substantially different optical system.</p><p>We then used the tracking results to answer a biological question: What is the relationship between the intracellular calcium signals and the beating cycles of the heart? We extracted the calcium dynamics of the correctly tracked 66 heart cells, which co-express GECI as in the worm datasets, and analyzed the phases of activities in these cells relative to the beating cycles of the ventricular and atrial regions, respectively. As shown in <xref ref-type="fig" rid="fig7">Figure 7H and I</xref>, the calcium signals were largely synchronized with the beating cycles. Although a portion of the heart cells (32/98 total) was not correctly tracked and therefore not analyzed, this result is still remarkable because the tracked cells show the relationships between calcium dynamics and the natural heartbeats in vivo. Observation of this relationship was only made available by the developments of a state-of-the-art microscope system that can monitor 100 volumes per second and of our software pipeline that can correctly track large portions of the corresponding cell movements in 3D space.</p></sec><sec id="s2-7"><title>Tracking ~900 cells in a 3D tumor spheroid imaged with a two-photon microscope</title><p>We also tested our method on a dataset more related to biomedical application, namely a dataset of ~900 cells in a 3D multicellular tumor spheroid (MCTS) imaged with a two-photon microscope. 3D MCTS is increasingly being used for drug screening because of its similarity to tumor cells in vivo (<xref ref-type="bibr" rid="bib30">Riedl et al., 2017</xref>). Therefore, the measurement of individual cell activities in 3D MCTS has become necessary, although tracking the movements of large numbers of cells in 3D + T images of 3D MCTS has presented a considerable challenge.</p><p>We obtained 3D + T images of a 3D MCTS cells expressing FRET type ERK sensor, EKAREV-NSL (<xref ref-type="bibr" rid="bib17">Komatsu et al., 2011</xref>) using a two-photon microscope (see Materials and methods). This dataset shows normal distributions of intensities and movements but includes a much larger number of cells than either the worm brain or the zebrafish heart dataset (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>; <xref ref-type="video" rid="fig8video1">Figure 8—video 1</xref>). Furthermore, cell division and death occurred during the imaging. Our method segmented and tracked 901 cells, of which we visually inspected the tracking results of 894 cells (the remaining seven cells were found to have segmentation errors in volume #1). We excluded the cells that experienced cell death or cell division after such events occurred, and found that 869 out of 894 cells (97%) were correctly tracked (<xref ref-type="fig" rid="fig8">Figure 8C–E</xref>, <xref ref-type="video" rid="fig8video2">Figure 8—video 2</xref>). Using the tracking results, we extracted the ERK activity from the FRET signal. In three representative cells with cross-layer movements, we confirmed that the extracted signals correctly reflected intensity changes in the cells (<xref ref-type="fig" rid="fig8">Figure 8F</xref>). We also found that the spheroid as a whole moved downwards, although each cell moved in a different direction (<xref ref-type="fig" rid="fig8">Figure 8G</xref>).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Results from tracking the cells in a 3D tumor spheroid.</title><p>(<bold>A</bold>) Experimental conditions. (<bold>B</bold>) Intensities and movements. (<bold>C</bold>) Raw 2D images (top), cell-like regions (middle left and bottom left), segmented cells in volume #1 (middle right) and the tracking result in volume #31 (bottom right). (<bold>D</bold>) Tracking accuracy through time with a small decrease from 100% (volume #1) to 97% (volume 31). (<bold>E</bold>) The positions of cells with/without tracking mistakes. Seven cells (white) were not checked because segmentation mistakes were found in volume #1 in these cells during the confirmation of tracking. (<bold>F</bold>) An example of the extracted activity (FRET = YFP/CFP). Left: one layer of the FRET image at <italic>z</italic> = 28 in volume #1. The movements and intensities of three cells in the square are shown on the right side. The extracted activities of the three cells are shown in the graph. (<bold>G</bold>) Movements of the 741 cells that experienced neither tracking mistakes nor cell death/division. Arrows indicate the location changes from volume #1 to #31. All scale bars, 50 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig8-v2.tif"/></fig><media id="fig8video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig8-video1.mp4"><label>Figure 8—video 1.</label><caption><title>CFP signals in the tumor spheroid dataset.</title><p>The intensity and movements of cell nuclei (CFP) in the tumor spheroid dataset (3D view).</p></caption></media><media id="fig8video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig8-video2.mp4"><label>Figure 8—video 2.</label><caption><title>Tracking results of the tumor spheroid dataset.</title><p>Upper: The cell regions detected by 3D U-net. Lower: Individual cells tracked by our method.</p></caption></media></fig-group></sec><sec id="s2-8"><title>Evaluation of the method under challenging conditions using degenerated datasets</title><p>In the assessments described in the preceding sections, we successfully analyzed multiple types of datasets that differ in terms of image resolution, signal-to-noise ratio, and types of cell movement etc. We then systematically evaluated the performance of our method (single mode) under a variety of conditions using a series of degenerated datasets obtained by modifying the worm #3 dataset. For a fair comparison, we used the same pre-trained 3D U-Net and the same manually corrected segmentation at <italic>t</italic> = 1 used on the original worm #3 dataset.</p><p>A general difficulty in segmentation arises from images with low signal-to-noise ratios. Excessive noise can obscure the real cell signal, leading to incorrect segmentation and ultimately incorrect tracking. We tracked cells in three degenerated datasets with different levels of Poisson noise added to the original images: <italic>sd</italic> = 60, <italic>sd</italic> = 100, and <italic>sd</italic> = 140 (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). The noise level in the original images in the non-cell regions was <italic>sd</italic> = 4.05 and the median intensity was 411, whereas the median intensity of cell regions was 567 with a 95% confidence interval of 430–934, indicating a tiny overlap between non-cell and cell regions (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3B</xref>). In the <italic>sd</italic> = 60 condition, the intensities of non-cell and cell regions overlapped to a greater extent (<xref ref-type="fig" rid="fig9">Figure 9B</xref>), and image quality appeared poorer than that of the original image (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Nevertheless, our method achieved a low error rate (6/175 = 3%; <xref ref-type="fig" rid="fig9">Figure 9C–E</xref>). Even in the <italic>sd</italic> = 140 condition, in which the intensities overlapped extensively (<xref ref-type="fig" rid="fig9">Figure 9B</xref>) and the image quality was quite poor (<xref ref-type="fig" rid="fig9">Figure 9A</xref>), our method achieved an acceptable error rate (16/175 = 9%, that is, 91% correct; <xref ref-type="fig" rid="fig9">Figure 9C–E</xref>). Note that the tracking error rate was much lower than the segmentation error rate (16/175 vs 57/175, respectively; <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>), indicating that our FFN-based tracking method can compensate for errors in cell segmentation. Considering that the overlap of intensity distributions between cell regions and background in the degenerated datasets were much more severe than the overlap in the real datasets (<xref ref-type="fig" rid="fig9">Figure 9B</xref> and panel B in <xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig8">8</xref>), these results suggest that our method can robustly track cells in 3D + T images with severe noise.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Robustness of our method in artificially generated challenging conditions.</title><p>(<bold>A</bold>) Tracking results when Poisson noise was added to dataset worm #3. One layer (<italic>z</italic> = 6) of 2D images (left, volume #1) and the corresponding tracking results in the last volume were shown (right). (<bold>B</bold>) Distributions of intensities of cell regions (solid lines) and background (dashed lines) from images with added noise. All distribution curves were normalized so that their peaks have the same height. (<bold>C</bold>) Bar graph showing the numbers of incorrectly tracked and correctly tracked cells for the four noise levels. (<bold>D</bold>) Tracking accuracies of the datasets with added noise through time. (<bold>E</bold>) The positions of cells with/without tracking mistakes. The numbers of cells with correct/incorrect tracking are shown in panel C. (<bold>F</bold>) Tracking results in the last volume when sampling rates were reduced by removing intermediate volumes in dataset worm #3. Arrows indicate two representative cells. (<bold>G</bold>) Statistics of cell speed based on the tracking results of the 171 correctly tracked cells in the original images. The speeds in 1/5 sampling conditions are much larger than in the original conditions, which makes the cells more challenging to track. (<bold>H</bold>) Distributions of movements. (<bold>I</bold>) Bar graph showing the numbers of incorrectly tracked and correctly tracked cells for the four sampling rates. (<bold>J</bold>) Tracking accuracies of the downsampled datasets through time. (<bold>K</bold>) The positions of cells with/without tracking mistakes. The numbers of cells with correct/incorrect tracking are shown in panel I. (<bold>L</bold>) Movements of the two representative cells N64 and N1, indicating iterative expansion and contraction of the worm mainly in the <italic>x</italic>-axis. The movements between neighboring volumes after applying 1/5 sampling (black dots and crosses) are larger than the movements in the original dataset worm #3 (blue and orange lines). See <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> and <xref ref-type="video" rid="fig3video3">Figure 3—video 3</xref> for tracking results in the original dataset worm #3. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Errors in segmentation and tracking in two example image datasets.</title><p>Numbers of the cells that were incorrectly segmented (in volume #1) and incorrectly tracked (in any volume) are shown. We manually checked the segmentation errors in two images. Compared to the number of segmentation errors (13 and 57), the number of tracking errors are much lower (4 and 16), implying that most incorrectly segmented cells were still correctly tracked. Note that segmentation errors were manually corrected in vol #1 but not in the following volumes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig9-figsupp1-v2.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>Cross-layer movements of two cells.</title><p>(<bold>A</bold>) Examples of cross-layer movements of two cells in dataset worm #3. The positions of the centroids of each cell (z) were calculated as the average locations of the segmented regions corresponding to that cell. (<bold>B</bold>) The positions of the two cells (arrows) and their centroids (blue and orange) in raw images at volumes #100 and #230.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig9-figsupp2-v2.tif"/></fig></fig-group><p>Another difficulty comes from large displacements of cells between volumes during tracking. We enhanced this effect by removing intermediate volumes in the worm's 3D + T images, and then tested the three datasets with 1/2, 1/3, and 1/5 of the volume of the original dataset (<xref ref-type="fig" rid="fig9">Figure 9F</xref>). As expected, when more volumes were removed, the movements of cells and the number of tracking errors increased (<xref ref-type="fig" rid="fig9">Figure 9G–L</xref>). Nevertheless, the error rate in the 1/3 vol condition was acceptable (14/175 = 8%, i.e. 92% correct), while the error rate in the 1/5 vol condition was relatively high (25/175 = 14%, i.e. 86% correct).</p><p>We then tested whether our method can track cell movements along the <italic>z</italic>-axis, in which the resolution is much lower than that in the <italic>x-y</italic> plane. In such conditions, 3D + T tracking is more challenging along the <italic>z</italic>-axis than in the <italic>x-y</italic> 2D plane. In the deforming worm and the tumor spheroid datasets, the resolution along the <italic>z</italic>-axis was approximately 1/10-1/5 of that in the <italic>x-y</italic> plane (panel A in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>, <xref ref-type="fig" rid="fig3s2">2</xref> and <xref ref-type="fig" rid="fig3s3">3</xref>, and <xref ref-type="fig" rid="fig8">Figure 8</xref>). We confirmed that 5, 16, 25, and 668 cells in worm #1a, #1b, and #3, and tumor spheroid datasets, respectively, showed cross-layer movements along the <italic>z</italic>-axis. Still, our method correctly tracked all of those cells. For example, while two cells in the worm #3 dataset exhibited multiple cross-layer movements (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>), they were correctly tracked until the end the sequence. Furthermore, we also degenerated the zebrafish heart data to test whether our method can track dynamic 3D cell movements with unequal resolutions. Even when the zebrafish image resolution in the <italic>z</italic>-axis was reduced by sum binning with a shrink factor = 2, our method was able to correctly track a majority of the cells (80% of the 30 larger cells and 53% of all 98 cells; <xref ref-type="table" rid="table3">Table 3</xref>). Together, these results indicate that our method can correctly track cells in various conditions including severe noise, large movements and unequal resolution.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Evaluation of large RM values in each dataset.</title><p>We evaluated how challenging the tracking task is in each dataset using the metric ‘relative movement’ (<italic>RM</italic>). When <italic>RM</italic> ≥0.5, the cell cannot be simply tracked by identifying it as the closest cell in the next volume (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>). When <italic>RM</italic> ≥1.0, the task becomes even more challenging. Note that large <italic>RM</italic> is just one factor making the segmentation/tracking task challenging. Lower image quality, photobleaching, three dimensional movements with unequal resolutions, lower coherency of cell movements, etc. also make tracking tasks more challenging. Some datasets share the same movement statistics because they are degenerated from the same dataset by adding noise or by modifying resolution. Also see <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig10">10</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" rowspan="3">Dataset</th><th colspan="4">Number of challenging movements</th><th rowspan="3">Accuracy</th></tr><tr><th colspan="2">With <italic>RM</italic> ≥ 0.5</th><th colspan="2">With <italic>RM</italic> ≥ 1.0</th></tr><tr><th>Total</th><th>Per cell</th><th>Total</th><th>Per cell</th></tr></thead><tbody><tr><td colspan="2">Worm #1a</td><td>110</td><td><bold>0.7</bold></td><td>0</td><td><bold>0.0</bold></td><td><bold>100%</bold></td></tr><tr><td colspan="2">Worm #1b</td><td>0</td><td><bold>0.0</bold></td><td>0</td><td><bold>0.0</bold></td><td><bold>100%</bold></td></tr><tr><td colspan="2">Worm #2</td><td>0</td><td><bold>0.0</bold></td><td>0</td><td><bold>0.0</bold></td><td><bold>100%</bold></td></tr><tr><td colspan="2">Worm #3</td><td rowspan="4">6678</td><td rowspan="4"><bold>39.1</bold></td><td rowspan="4">798</td><td rowspan="4"><bold>4.7</bold></td><td><bold>98%</bold></td></tr><tr><td rowspan="3">Worm #3 <break/>+ noise</td><td>sd = 60</td><td><bold>97%</bold></td></tr><tr><td>sd = 100</td><td><bold>95%</bold></td></tr><tr><td>sd = 140</td><td><bold>91%</bold></td></tr><tr><td rowspan="3">Worm #3 <break/>+ reduced sampling</td><td>1/2</td><td>9965</td><td><bold>58.3</bold></td><td>2908</td><td><bold>17.0</bold></td><td><bold>97%</bold></td></tr><tr><td>1/3</td><td>10255</td><td><bold>60.0</bold></td><td>4049</td><td><bold>23.7</bold></td><td><bold>92%</bold></td></tr><tr><td>1/5</td><td>9117</td><td><bold>53.5</bold></td><td>4593</td><td><bold>26.9</bold></td><td><bold>86%</bold></td></tr><tr><td colspan="2">Worm #4 (Freely moving)</td><td>20988</td><td><bold>318.0</bold></td><td>6743</td><td><bold>102.2</bold></td><td><bold>99.8% (ensemble mode)</bold> <break/> <bold>73% (single mode)</bold></td></tr><tr><td rowspan="2">Zebrafish</td><td>raw</td><td rowspan="2">5337</td><td rowspan="2"><bold>80.9</bold></td><td rowspan="2">1185</td><td rowspan="2"><bold>18.0</bold></td><td><bold>87%</bold> (30 larger cells); <break/><bold>67%</bold> (all 98 cells)</td></tr><tr><td>binned</td><td><bold>80%</bold> (30 larger cells); <break/><bold>53%</bold> (all 98 cells)</td></tr><tr><td colspan="2">Tumor spheroid</td><td>47</td><td><bold>0.1</bold></td><td>5</td><td><bold>0.0</bold></td><td><bold>97%</bold></td></tr></tbody></table></table-wrap></sec><sec id="s2-9"><title>Challenging movement and its relationship with tracking error rate</title><p>To evaluate the tracking performance of our method on datasets with large movements, we summarized the <italic>RM</italic> values, which are listed in <xref ref-type="table" rid="table3">Table 3</xref> and <xref ref-type="fig" rid="fig10">Figure 10</xref> (see also <xref ref-type="fig" rid="fig4">Figure 4</xref>). Although many cell movements in the worm #3 and the zebrafish datasets had <italic>RM</italic> ≥0.5, most of the worm neurons and the larger cells in the zebrafish heart were correctly tracked by the single mode. In addition, our program with the ensemble mode achieved 99.8% tracking of the neurons of a ‘straightened’ freely moving worm while many cell movements in this dataset had RM ≥1. These results indicate that our method is capable of analyzing images with challenging displacements.</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title><italic>RM</italic> values at different time points in three datasets.</title><p>Also see <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="table" rid="table3">Table 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig10-v2.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>The relationship between movements and the error rate of our method.</title><p>(<bold>A</bold>) The relationship in the real datasets. (<bold>B</bold>) The relationship in the downsampled worm #3 datasets. The <italic>RM</italic> is the averaged <italic>RM</italic> in each volume. The error rate is the number of new errors divided by the number of correctly tracked cells until the last volume. We fitted regression lines using the kernel ridge regression with RBF kernel. The top and right parts show the marginal distributions of <italic>RM</italic> and error rate, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig10-figsupp1-v2.tif"/></fig><fig id="fig10s2" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 2.</label><caption><title>The relationship between movements and the error rate of our method in ensemble mode.</title><p>The result is based on the freely moving worm dataset (worm #4).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig10-figsupp2-v2.tif"/></fig></fig-group><p>To investigate the relationships between the cell movements and tracking error rate, we drew a scatter plot of the averaged <italic>RM</italic> and corresponding error rate for each volume (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>). The results indicated by the regression lines suggest positive correlations between the <italic>RM</italic> and error rate, although the correlation trends appear to differ by dataset, implying that movement is not the only factor affecting the error rate. We also drew a scatter plot and regression line for the worm #4 dataset tracked with the ensemble mode (<xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref>). The result suggests that the error rate is not correlated with movement, perhaps because in ensemble mode the cell positions are predicted from multiple different volumes, and therefore movement from a previous volume is not closely connected with the error rate.</p></sec><sec id="s2-10"><title>Comparison of the tracking accuracies of our and previous methods</title><p>To further assess the capabilities of our method, we compared the tracking performance of our method with that of two other state-of-the-art methods for cell tracking. The first was DeepCell 2.0, a newer version of DeepCell, which is a pioneer in segmenting and tracking cells in 2D + T images using deep learning (<xref ref-type="bibr" rid="bib3">Bannon et al., 2018</xref>). Unlike our method, DeepCell 2.0 has been primarily tested on images that include cell divisions, birth and death, but not on images of deforming organs. The second was the software developed by Toyoshima et al., which does not use a deep learning technique, but has achieved a higher accuracy in both segmenting and tracking of worm whole brain datasets than any other previous methods (<xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>).</p><p>On our worm brain datasets, DeepCell 2.0 tracked ~10% of cells properly (<xref ref-type="fig" rid="fig11">Figure 11A</xref>, <xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1A</xref>, <xref ref-type="video" rid="fig11video1">Figure 11—video 1</xref>, and <xref ref-type="table" rid="table4">Table 4</xref>); this is probably because the tracking algorithm of DeepCell 2.0 is optimized for the movements associated with cell divisions, but not for quickly deforming organs. Toyoshima’s method tracked ~90% of the original worm #3 neurons but only ~10% of the degenerated ‘worm #3, 1/5 sampling’ (<xref ref-type="fig" rid="fig11">Figure 11B</xref>, <xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1B</xref>, and <xref ref-type="table" rid="table4">Table 4</xref>). When tested on the zebrafish dataset, Toyoshima's method was able to detect only 76 cells, and missed some weak-intensity cells, performing worse than our method (detected 98 cells). Of the detected 76 cells, 21 were incorrectly tracked from the first volume, probably because their method automatically re-fits the cell shape using a Gaussian distribution after segmentation, which can lead to a failure when fitting weak-intensity cells. Their tracking accuracy was also lower than ours (<xref ref-type="table" rid="table4">Table 4</xref>). In addition, our method was found comparable to or more efficient in terms of runtime than DeepCell 2.0 and Toyoshima's methods (<xref ref-type="table" rid="table5">Table 5</xref>). These results suggest that our method is more capable of tracking cells in 3D + T images than previous methods.</p><fig-group><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Comparison of the tracking results between our method, DeepCell 2.0, and <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>.</title><p>(<bold>A</bold>) Comparison with DeepCell 2.0 tested using the layer <italic>z</italic> = 9 in worm #3. The raw images (top) are displayed using the pseudocolor ‘fire’. Colors in the tracking results (middle and bottom) indicate the initial positions (volume #1, left) and tracked positions of cells (volume #519, right). Note that the spatial patterns of cell colors in the initial and last volumes are similar in our method (middle) but not in DeepCell 2.0 (bottom). Arrows indicate all the correctly tracked cells in our method and in DeepCell 2.0. Cells without arrows were mistracked. The asterisk indicates a cell whose centroid moved to the neighboring layer (<italic>z</italic> = 10) and thus was not included in the evaluation. Also see <xref ref-type="video" rid="fig11video1">Figure 11—video 1</xref> and <xref ref-type="table" rid="table4">Table 4</xref>. (<bold>B</bold>) Comparison with Toyoshima’s software tested using all layers in worm #3 with 1/5 sampling rate. For demonstration, we only show tracking results at <italic>z</italic> = 9. Again arrows indicate all the correctly tracked cells in our method and in Toyoshima’s software. Because Toyoshima’s software is not able to label the tracked cells using different colors, all cells here are shown by purple circles. Some cells were not marked by circles because they were too far from the centroids of the cells (in other layers). See also <xref ref-type="table" rid="table4">Table 4</xref>. All scale bars, 20 µm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig11-v2.tif"/></fig><fig id="fig11s1" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 1.</label><caption><title>Comparision of the tracking accuracies between our method and two previous methods.</title><p>(<bold>A</bold>) Tracking accuracies by DeepCell 2.0 and by our method for two 2D image datasets. (<bold>B</bold>) Tracking accuracies by <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref> and by our method for three 3D image datasets. For the zebrafish dataset, Toyoshima ‘s method showed low accuracy from the first volume because of segmentation errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59187-fig11-figsupp1-v2.tif"/></fig><media id="fig11video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-59187-fig11-video1.mp4"><label>Figure 11—video 1.</label><caption><title>Tracking results of a 2D + T image (<italic>z</italic> = 9 of worm #3) using our method and DeepCell 2.0.</title><p>The tracked cells are shown in different colors. In our method, most cells were correctly tracked and their colors did not change (top). In DeepCell 2.0, most cells were incorrectly tracked and their colors changed during the tracking (bottom).</p></caption></media></fig-group><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Tracking accuracy of our method compared with DeepCell 2.0 and <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>.</title><p>We evaluated the tracking accuracy in two 2D + T image datasets for the comparision with DeepCell 2.0, because DeepCell 2.0 currently cannot process 3D + T images. We also evaluated the tracking accuracy in three 3D + T image datasets for comparision with <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>. For the zebrafish dataset, we only processed the initial 100 volumes because Toyoshima’s software requires a very long processing time (<xref ref-type="table" rid="table5">Table 5</xref>).</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Dataset</th><th valign="top">Method</th><th valign="top">Correct tracking</th><th valign="top">Accuracy</th></tr></thead><tbody><tr><td rowspan="2">Worm #3 (z = 9)</td><td valign="top"><bold>Our method</bold></td><td valign="top"><bold>37 (in 39 cells)</bold></td><td valign="top"><bold>95%</bold></td></tr><tr><td valign="top">DeepCell 2.0</td><td valign="top">5 (in 39 cells)</td><td valign="top">13%</td></tr><tr><td rowspan="2">Worm #3 (z = 16)</td><td valign="top"><bold>Our method</bold></td><td valign="top"><bold>36 (in 36 cells)</bold></td><td valign="top"><bold>100%</bold></td></tr><tr><td valign="top">DeepCell 2.0</td><td valign="top">2 (in 36 cells)</td><td valign="top">6%</td></tr><tr><td rowspan="2">Worm #3</td><td valign="top"><bold>Our method</bold></td><td valign="top"><bold>171 (in 175 cells)</bold></td><td valign="top"><bold>98%</bold></td></tr><tr><td valign="top"><xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref></td><td valign="top">157 (in 175 cells)</td><td valign="top">90%</td></tr><tr><td rowspan="2">Worm #3: 1/5 sampling</td><td valign="top"><bold>Our method</bold></td><td valign="top"><bold>150 (in 175 cells)</bold></td><td valign="top"><bold>86%</bold></td></tr><tr><td valign="top"><xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref></td><td valign="top">21 (in 175 cells)</td><td valign="top">12%</td></tr><tr><td rowspan="2">Zebrafish (100 volumes)</td><td valign="top"><bold>Our method</bold></td><td valign="top"><bold>84 (in 98 cells)</bold></td><td valign="top"><bold>86%</bold></td></tr><tr><td valign="top"><xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref></td><td valign="top">35 (in 76 cells)</td><td valign="top">46%</td></tr></tbody></table></table-wrap><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Runtimes of our method and two other methods.</title><p>We tested the runtime of our method and two other methods on our desktop PC.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Method</th><th>Runtime per volume</th><th>Comments</th></tr></thead><tbody><tr><td rowspan="2">Worm #3</td><td><bold>Our method</bold></td><td><bold>~38 s</bold></td><td>21 layers</td></tr><tr><td>DeepCell 2.0 (for 2D)</td><td><bold>~</bold>33 s</td><td>= 1.57 s x 21 layers</td></tr><tr><td rowspan="2">Worm #3</td><td><bold>Our method</bold></td><td><bold>~38 s</bold></td><td/></tr><tr><td><xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref></td><td><bold>~</bold>140 s</td><td/></tr><tr><td rowspan="2">Zebrafish</td><td><bold>Our method</bold></td><td><bold>~1 min</bold></td><td/></tr><tr><td><xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref></td><td><bold>~</bold>15 min</td><td/></tr></tbody></table></table-wrap></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Tracking biological objects in 3D + T images has proven to be difficult, and individual laboratories still frequently need to develop their own software to extract important features from the images obtained using different optical systems and/or imaging conditions. Moreover, even when identical optical systems are used, the optimization of many parameters is often required for different datasets. To solve these problems, we have developed a deep learning-based pipeline, 3DeeCellTracker, and demonstrated that it can be flexibly applied to divergent datasets obtained under varying conditions and/or different qualities. We analyzed multiple image series of worms, zebrafish, and tumor spheroids, which differed in terms of nuclear marker, intensity level, noise level, numbers of cells per image, image resolutions and sizes, imaging rates, and cell speed. Notably, we showed that our method successfully tracked cells in these datasets under challenging conditions, such as large movements (see <xref ref-type="fig" rid="fig10">Figure 10</xref>, <xref ref-type="table" rid="table3">Table 3</xref> and Materials and methods), cross-layer movements (<xref ref-type="fig" rid="fig6">Figures 6G</xref> and <xref ref-type="fig" rid="fig7">7G</xref>, and <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>), and weak intensity conditions (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>). In addition, our method outperformed two previous state-of-the-art cell tracking methods, at least in deforming organs under challenging imaging conditions (<xref ref-type="fig" rid="fig11">Figure 11</xref>, <xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>, and <xref ref-type="table" rid="table4">Table 4</xref>), while the other methods are likely more suited for other conditions. Furthermore, our method is comparable to or more efficient in runtime than previous methods (<xref ref-type="table" rid="table5">Table 5</xref> and Materials and methods). Running in ensemble mode on a desktop PC, our method tracked the neurons of a ‘straightened’ freely moving worm with high accuracy, which required a computing cluster with up to 200 cores in the previous study (<xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>).</p><p>We consider that the high accuracy and robustness of our method is based on its use of FFN, a deep learning technique, together with its post-processing methods for tracking (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). As mentioned above, although deep learning techniques have been predicted to exhibit superior performance in 3D cell tracking, they had not been used to date because of the difficulty in manually preparing large quantities of training data, especially for 3D + T images. To solve this problem, we generated a synthetic training dataset via artificial modification of a single volume of worm 3D cell positional data, which produced excellent performance by our method. These results demonstrate that the deep network technique can be used for cell tracking by using an synthetic point set dataset, although the procedures for generating the dataset are simple.</p><p>Not only is our method flexible and efficient, it can be easily used by researchers. For example, our method worked well in all the diverse conditions tested with only minor modifications. Notably, under constant imaging conditions, our method can be directly reused without modifying any parameters except for the noise level (<xref ref-type="table" rid="table1">Tables 1</xref> and <xref ref-type="table" rid="table2">2</xref>), making it convenient for end-users. This differs from conventional image processing methods, in which slight differences in the obtained data, such as in light intensity, resolution, the size of the target object etc, generally require the re-setting of multiple parameters through trial-and-error. Even when imaging conditions are substantially changed, our method requires only a few modifications, primarily in the segmentation process: (1) modifying the structure of the 3D U-Net (this step can be skipped because a 3D U-Net of the same structure can be adapted to a new dataset by re-training; see Materials and methods); (2) re-training the 3D U-Net; and (3) modifying parameters according to the imaging conditions (see <xref ref-type="table" rid="table1">Tables 1</xref> and <xref ref-type="table" rid="table2">2</xref> and the ‘Guide for parameters.md’ file in <ext-link ext-link-type="uri" xlink:href="https://github.com/WenChentao/3DeeCellTracker">https://github.com/WenChentao/3DeeCellTracker</ext-link>). For re-training the 3D U-Net, manual annotation usually takes 2–3 hr for 150–200 cells, and the network can be automatically trained in 1–2 hr on our desktop PC with a single GPU. The FFN for tracking generally does not require re-training. The number of parameters to be manually determined is much smaller in our method than in conventional methods (<xref ref-type="table" rid="table1">Table 1</xref>) due to its use of deep learning. The parameters can be quickly modified (within 1 hr) following the guide we have provided in the GitHub repository.</p><p>Nevertheless, our method can be improved in two ways: more reliable tracking and a simplified procedure. Tracking reliability can be affected by large movements, weak intensities, and/or photobleaching. As revealed in our results (<xref ref-type="fig" rid="fig6">Figure 6</xref>), large movements such as in a freely moving worm can be resolved using the ensemble mode, which borrows the idea from ensemble learning in machine learning, that is using the average of multiple predictions to reduce the prediction error (<xref ref-type="bibr" rid="bib29">Polikar, 2009</xref>). A similar idea, matching cells using multiple reference volumes and a clustering method instead of averaging, was applied in the previous study (<xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>), suggesting that ensemble learning is a good approach to resolving challenging movements. On the other hand, the problem of weak intensity and photobleaching remains to be solved. One possible approach would be to normalize the intensities to obtain similar images over time, although doing so might not be easy.</p><p>To further simplify the entire procedure, we contemplate developing new network structures that combine additional steps. The U-Net and 3D U-Net are networks for semantic segmentation, which classify each voxel as a specific category, that is as either a cell or a non-cell region. Beyond this, networks have been designed to achieve instance segmentation by further separating objects in the same category into individual objects, eliminating the need to use a watershed for separating connected cells. Although recent advances have been made in these architectures, the focus is still on segmenting common objects in 2D images (<xref ref-type="bibr" rid="bib20">Liang et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Romera-Paredes and Torr, 2016</xref>; <xref ref-type="bibr" rid="bib15">He et al., 2017</xref>). We suggest that instance segmentation is a possible approach for simplifying and improving cell segmentation in future studies. Another possible area for improvement is the use of FFN for tracking. By further improving the FFN structure and using more training data, the network should be able to generate more accurate matches that can directly be used for tracking cells without point set registration.</p><p>We developed 3DeeCellTracker mainly using semi-immobilized worm datasets. However, it also successfully processed 3D + T images of a zebrafish dataset obtained using the SCAPE 2.0 system (<xref ref-type="bibr" rid="bib42">Voleti et al., 2019</xref>). This system is quite different from the spinning disk confocal system used for worm datasets in resolution, <italic>z</italic>-depth, and applied optical sectioning principle (<xref ref-type="bibr" rid="bib5">Bouchard et al., 2015</xref>). While SCAPE is an original and outstanding method for enabling ultra high-speed 3D + T image acquisition, it had been difficult to obtain or develop software that can efficiently process the 3D + T images produced by the system. In this study we tracked 3D + T images obtained from the SCAPE system by simply modifying a few parameters, which allowed us to obtained an acceptable result (87% of large cells correctly tracked). Considering that the lower performance relative to other datasets might have arisen from the difficulty in segmenting the smaller, low-intensity cells (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, the upper and the middle panels), the result may be improved by further optimization of the segmentation.</p><p>We also successfully tracked a large number of cells (~900) in a 3D MTCS monitored using a two-photon microscope, a result that further supports the wide applicability of our method. Our method cannot track cells that are dividing or fusing, or many cells that enter the field of view during the recording. This is because it operates under the assumption that each cell has a unique corresponding cell in another volume in order to match cells with large movements. To handle cells with division, fusion, or entry, it will be necessary to integrate our algorithms with additional algorithms.</p><p>In summary, we have demonstrated that 3DeeCellTracker can perform cell segmentation and tracking on 3D + T images acquired under different conditions. Compared with the tracking of slowly deforming cells in 2D + T images, it is a more challenging task to track cell nuclei in a semi-constrained/freely moving worm brain, beating zebrafish heart, or 3D tumor spheroid, all of which undergo considerable movements in 3D space. We consider this to be the first report on a pipeline that efficiently and flexibly tracks moving cells in 3D + T images from multiple, substantially different datasets. Our method should enable the segmentation and tracking of cells in 3D + T images acquired by various optical systems, a task that has not yet been performed.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type <break/>(species) or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional information</th></tr></thead><tbody><tr><td>Strain, strain background (<italic>Caenorhabditis elegans</italic>, hermaphrodite)</td><td valign="top">KDK54165 <break/><break/>AML14</td><td valign="top">This paper <break/><break/><xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref></td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:WB-STRAIN:KDK54165">WB-STRAIN:KDK54165</ext-link> <break/><break/>RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:WB-STRAIN:AML14">WB-STRAIN:AML14</ext-link></td><td valign="top"><italic>lite-1(xu7);oskEx54165[rab-3p::GCaMP5G-NLS, rab-3p::tdTomato] ;wtfEx4[rab-3p::NLS::GCaMP6s, rab-3p::NLS::tagRFP]</italic></td></tr><tr><td>Cell line (<italic>Homo sapience</italic>)</td><td valign="top">HeLa cells</td><td valign="top">Riken Cell Bank</td><td valign="top">RCB0007, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:CVCL_0030">CVCL_0030</ext-link></td><td valign="top"/></tr><tr><td>Transfected construct</td><td valign="top">EKAREV-NLS</td><td valign="top"><xref ref-type="bibr" rid="bib17">Komatsu et al., 2011</xref></td><td valign="top"/><td valign="top">FRET-type ERK sensor</td></tr><tr><td>Software, algorithm</td><td valign="top">3DeeCellTracker <break/>ImageJ <break/>ITK-SNAP <break/>IMARIS</td><td valign="top">This paper <break/>ImageJ <break/>ITK-SNAP <break/>IMARIS</td><td valign="top"><break/>RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_003070">SCR_003070</ext-link> <break/>RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002010">SCR_002010</ext-link> <break/>RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_007370">SCR_007370</ext-link></td><td valign="top"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Computational environment</title><p>Our image processing task was performed on a personal computer with an Intel Core i7-6800K CPU @ 3.40 GHz x 12 processor, 16 GB of RAM, and an Ubuntu 16.04 LTS 64-bit operating system. We trained and implemented the neural networks with an NVIDIA GeForce GTX 1080 GPU (8 GB). The neural networks were constructed and implemented using the Keras high-level neural network API (<ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>) running on top of the TensorFlow machine-learning framework (Google, USA). All programs were implemented within a Python environment except for the image alignment, which was implemented in ImageJ (NIH; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_003070">SCR_003070</ext-link>), and the manual labeling, manual correction and manual confirmation, which were implemented in ITK-SNAP (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002010">SCR_002010</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://www.itksnap.org">http://www.itksnap.org</ext-link>) or IMARIS (Bitplane, UK; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_007370">SCR_007370</ext-link>). Instead of ITK-SNAP and IMARIS, one can use napari (<ext-link ext-link-type="uri" xlink:href="https://napari.org">https://napari.org</ext-link>) in the Python environment.</p></sec><sec id="s4-2"><title>Pre-processing</title><p>Step 1: Because 2D images were taken successively along the <italic>z</italic> axis, rather than simultaneously, small or large displacements could exist between different layers of a 3D volume. Ideally, this should be compensated for before the segmentation procedure. Using the StackReg plugin (<xref ref-type="bibr" rid="bib37">Thevenaz et al., 1998</xref>) in ImageJ (NIH), we compensated for the displacements by using rigid-body transformations to align each layer with the center layer in worm #1 and #2 datasets. However, we did not apply this alignment in worm #3, #4, zebrafish, and tumor spheroid datasets but still obtained acceptable results, indicating this step may be skipped.</p><p>Step 2: Cells in the same image could have very different intensities, and detecting weak cells is generally difficult. To solve this problem, we applied local contrast normalization (<xref ref-type="bibr" rid="bib13">Goodfellow et al., 2017</xref>) through a sliding window (27 × 27 × 3 voxels) so that all cells had similar intensities. This normalization was applied to the nucleus marker images only for tracking and did not affect the calculation of the signal intensities for either red fluorescent protein or GECI.</p></sec><sec id="s4-3"><title>3D U-Net</title><p>We used 3D U-Net structures similar to that shown in the original study (<xref ref-type="bibr" rid="bib7">Çiçek et al., 2016</xref>). The network received a 3D image as input and generated a 3D image of the same size with values between 0 and 1 for each voxel, indicating the probability that the voxel belonged to a cell region (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We used different structures of 3D U-Net for different imaging conditions, in order to capture as much information as possible of each cell within the limitations of the GPU memory. Such modification of the U-Net structures is preferred but not necessary. The same structure can be reused on different datasets because of the flexibility of deep learning methods. The 3D U-Net structure shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> was used on our datasets worm #1 and worm #2, which have identical resolution, but we also successfully used the same structure in the binned zebrafish dataset (see below), which had very different resolution. For dataset worm #3, which had a lower resolution, we reduced the number of maxpooling and upsampling operations so that each voxel in the lowest layers corresponds to similar sizes as in the datasets worm #1 and worm #2. We also reduced the sizes of the input, output, and intermediate layers because of the lower resolution. As the smaller structure occupied less GPU memory, this allowed us to increase the number of convolutional filters on each layer so that the capacity of the network was increased (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). For the zebrafish dataset, although it has even lower resolution in the <italic>x</italic> and <italic>y</italic> axes, because the sizes of zebrafish cardiac cells are larger than worm neurons, we used the same number of maxpooling and upsampling operations as in datasets worm #1 and worm #2. We adjusted the sizes of layers in the <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> dimensions to a unified value (=64) because the resolution in the three dimensions are not very different in the zebrafish dataset (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). For simplicity, we reused the structures A and B in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for worm #4, tumor spheroid and binned zebrafish dataset (see <xref ref-type="table" rid="table2">Table 2</xref>).</p><p>The U-Net can be trained using very few annotated images (<xref ref-type="bibr" rid="bib32">Ronneberger et al., 2015</xref>). In this study, we trained six 3D U-Nets: (1) for datasets worm #1 and #2, (2) for dataset worm #3, (3) for freely moving dataset worm #4, (4) for the zebrafish dataset, (5) for the dataset tumor spheroid, and (6) for the binned zebrafish dataset. Each 3D U-Net used one 3D image for training. Note that, although datasets worm #1 and #2 are substantially different with respect to signal intensity and cell movements, the same trained 3D U-Net was used. The image was manually annotated into cell regions and non-cell regions using the ITK-SNAP software (<ext-link ext-link-type="uri" xlink:href="http://www.itksnap.org">http://www.itksnap.org</ext-link>). We used the binary cross-entropy as the loss function to train the 3D U-Net. Because the raw image sizes were too large (512 × 1024 × 28, 256 × 512 × 20, 180 × 260 × 165, etc.) for computing in the GPU, we divided the raw images into small sub-images that fit the input sizes of the three 3D U-Nets structures (160 × 160 × 16, 96 × 96 × 8, or 64 × 64 × 64), and combined the cell/non-cell classifications of sub-images to form a final classification of the whole image. To improve the 3D U-Net performance, we increased the training data by data augmentation: We applied random affine transformations to the annotated 3D images by ‘ImageDataGenerator’ class in Keras. The affine transformation was restricted in the <italic>x-y</italic> plane but not in the <italic>z</italic>-direction because the resolution in the <italic>z</italic>-direction is much lower than that in the <italic>x-y</italic> plane for worm datasets and the tumor spheroid dataset (see panel A in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>, <xref ref-type="fig" rid="fig3s2">2</xref> and <xref ref-type="fig" rid="fig3s3">3</xref>, and <xref ref-type="fig" rid="fig8">Figure 8</xref>). Although the zebrafish dataset has similar resolutions in the <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> directions, we applied the same affine transformation for simplicity. We trained the U-net for datasets worm #1 and #2 using a 3D image from another dataset independent of #1 and #2 but obtained under the same optical conditions, and its classifications on datasets worm #1 and #2 were still good (panel C in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref> and <xref ref-type="fig" rid="fig3s2">2</xref>), indicating a superior generalization ability of the 3D U-net. Because only one dataset is available for the specific resolutions of dataset worm #3, #4, the zebrafish heart, and the tumor spheroid, we trained the 3D U-Net by using the first volume of 3D + T images of each dataset and then applied the 3D U-Net to all the following 3D images of the datasets.</p></sec><sec id="s4-4"><title>Watershed</title><p>The 3D U-Net generated probability outputs between 0 and 1, which indicated the probability that a voxel belonged to a cell-like region. By setting the threshold to 0.5, we divided the 3D image into cell-like regions (&gt;0.5) and non-cell regions (≤0.5). The cell-like regions in the binary images were further transformed into distance maps, where each value indicated the distance from the current voxel to the nearest non-cell region voxel. We applied a Gaussian blur to the distance map to smooth it, and searched for local peaks which were assumed to be cell centers. We then applied watershed segmentation (<xref ref-type="bibr" rid="bib4">Beucher and Meyer, 1993</xref>), using these centers as seeds. Watershed segmentation was applied twice; the first application was 2D watershed segmentation for each <italic>x-y</italic> plane, and the second application was 3D watershed segmentation for the entire 3D space. Two segmentations were required because the resolutions in the <italic>x-y</italic> plane and the <italic>z</italic>-dimension differed.</p></sec><sec id="s4-5"><title>Feedforward network: architecture</title><p>An initial matching, that is, a set of correspondences between cells in two temporally adjacent volumes, is the first step for cell tracking in our pipeline and critical for the final tracking accuracy. The correspondences can be estimated based on the relative cell positions, assuming that these positions do not substantially change even during organ deformation. By comparing the similarity between the relative positions of two cells in different volumes, we can determine whether they are the same cells.</p><p>One conventional method to represent relative positions is fast point feature histograms (FPFH) (<xref ref-type="bibr" rid="bib33">Rusu et al., 2009</xref>). The PR-GLS study (<xref ref-type="bibr" rid="bib22">Ma et al., 2016</xref>) successfully used the FPFH method to match artificial point set datasets. However, we found that FPFH yielded a poor initial match for the datasets considered in this study (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>), perhaps because of the sparse distribution of the cells. We thus designed a three-layer feedforward network (FFN) to improve the initial match (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The three-layer structure generated good match results comparable with those of more complex structures with four or six layers. By comparing the representations between two points, the network generated a similarity score between two cells. The initial matching based on the similarity score by the FFN was more accurate than that achieved by the FPFH method (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3B</xref>). Our FFN + PR-GLS approach generated more accurate predictions of cell positions than FPFH + PR-GLS and the simple method of affine alignment (using an implementation of the Coherent Point Drift Algorithm: <ext-link ext-link-type="uri" xlink:href="https://github.com/siavashk/pycpd">https://github.com/siavashk/pycpd</ext-link>) (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3C and D</xref>).</p><p>In our FFN, the input of the network contained the position information of two points. Each point was represented by the normalized positions of the 20 nearest neighboring points (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The 20 nearest neighbor positions (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) were given by 3D vectors because they were extracted from the 3D image. To normalize the points, each of the 20 positions was divided by the mean distance d (<inline-formula><mml:math id="inf5"><mml:mi mathvariant="normal">d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>). The normalized positions were then sorted by their absolute values in ascending order. Finally, the mean distance d was included as the last value, so each point was represented by a 61D vector.</p><p>We utilized the first fully connected layer to calculate the learned representation of the relative positions of each point as a 512D vector (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, the first hidden layer after the input). We then applied a second fully connected layer on these two 512D vectors to compare the representations of the two points. The resulting 512D vectors (the second hidden layer after the input) were processed by a third fully connected layer to obtain a single similarity score between 0 and 1, which indicated the probability of two points originating from the same cell. We matched the two points with the highest scores in two different volumes, ignored these two points, and matched the next set of two points with the highest scores. By repeating this process, we obtained an initial match (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, panel 4–1).</p></sec><sec id="s4-6"><title>Feedforward network: training</title><p>In this study, we only trained one FFN based on one original image of dataset worm #3 and used the network in all the datasets including worms, zebrafish and tumor spheroid. To train the network, we first performed segmentation on a single volume of dataset worm #3 and obtained a point set for the centers of all cells. Because we required a large number of matched point sets for training, and because manually matching point sets is time-consuming and impractical, we created an synthetic training dataset by applying random affine transformations to the point set described above and adding small random movements to each point according to following equations:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Also see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref> for an illustration. Here <inline-formula><mml:math id="inf6"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> is the mean-removed 3D position {<inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>} of each point in the original point set, while <inline-formula><mml:math id="inf10"><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> is the transformed 3D position. <inline-formula><mml:math id="inf11"><mml:mi>A</mml:mi></mml:math></inline-formula> is a matrix to apply the random affine transformation. More specifically, <inline-formula><mml:math id="inf12"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>U</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf13"><mml:mi>I</mml:mi></mml:math></inline-formula> is a 3 x 3 identity matrix, and <inline-formula><mml:math id="inf14"><mml:mi>U</mml:mi></mml:math></inline-formula> is a 3 x 3 random matrix with each element <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from a uniform distribution. We used <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo> <mml:mi/><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>0.05</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> in this study. <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> is the 3D vector for adding random movements to each point in a point set, while <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> is for adding even larger random movements in a subset of points (20 out of 175 cells) to simulate serious errors from segmentation procedures. We used <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> in this study. By randomly generating <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we could generate an arbitrarily large number of new point sets with new positions from the original point set. After that, we chose a specific point A and another point B from each generated point set and the original point set, respectively, and we calculated their relative positions as inputs for the FFN. In half of the cases, points A and B are corresponding, that is, they come from the same cell, while in the other half, point A is from a cell adjacent to the cell of point B, thus they are not corresponding. In this study, we used 576,000 newly generated pairs of points A and B for training the FFN (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We used binary cross-entropy as the loss function to train the FFN. During the training, the performance of matching by FFN was gradually improved using an independent test dataset of two point sets (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>).</p></sec><sec id="s4-7"><title>PR-GLS method</title><p>The initial match calculated using FFN was corrected using the expectation–maximization (EM) algorithm in the PR-GLS method, as described in the original paper (<xref ref-type="bibr" rid="bib22">Ma et al., 2016</xref>). In the original study, the initial match (by FPFH) was recalculated during the EM iterations; however, in most datasets, we calculated the initial match only once (by the FFN) before the EM steps were performed, which did not cause problems. Only for the dataset worm #4 with very large movements, we recalculated initial matching by FFN after every 10 iterations, in order to improve accuracy. After the PR-GLS corrections, we obtained coherent transformations from the points of each volume to the subsequent volume (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, panel 4–2).</p></sec><sec id="s4-8"><title>Single and ensemble modes</title><p>The FFN + PR-GLS can predict new cell positions at time <italic>t</italic> from the cell positions at <italic>t</italic>-1; this is the default mode of our pipeline, which is referred to as the single mode (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). At a sufficiently high sampling rate, the single mode is reasonable because the movement from <italic>t</italic>-1 to <italic>t</italic> is much smaller than that from <italic>t-i</italic> (<italic>i</italic> &gt; 1) to <italic>t</italic>, making the prediction from <italic>t</italic>-1 more reliable.</p><p>When cell movements are very large (such as in a freely moving worm), and the sampling rate is not sufficient, the prediction from <italic>t</italic>-1 to <italic>t</italic> becomes less reliable, and the average of multiple predictions from <italic>t-i</italic> to <italic>t</italic> may be more accurate. Therefore, we developed an approach using the average of multiple predictions, referred to as the ensemble mode (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). We tested this mode only on the worm #4 dataset, which had quite large movements (<xref ref-type="fig" rid="fig6">Figures 6B</xref> and <xref ref-type="fig" rid="fig10">10</xref>, and <xref ref-type="table" rid="table3">Table 3</xref>), because the runtime of the ensemble mode is much longer than that of the single mode, that is, the runtime of the FFN + PR-GLS component is proportional to the number of predictions used. Specifically, the runtime including segmentation and tracking for worm #4 was approximately 30 volume/h in the single mode and 6 volumes/h in the ensemble mode.</p><p>In the ensemble mode, we calculated an average of up to 20 predictions from previous time points. In cases for which t ≤ 20, the averages was calculated from time points [<italic>t</italic>-1, <italic>t</italic>-2, …, 1]; in cases for which t &gt; 20, it was calculated over [<italic>t-d</italic>, <italic>t</italic>-2<italic>d</italic>, …, <italic>t</italic>-20<italic>d</italic>], where <italic>d</italic> is the quotient (<italic>t</italic>-1)//20.</p></sec><sec id="s4-9"><title>Accurate correction for tracking</title><p>By applying the PR-GLS method to the initial match, we obtained a more reliable transformation function in which all obvious incorrect matches were corrected. However, small differences still existed in a few cells, which could accumulate over time to become large differences without correction. Thus, we included an additional automatic correction step, in which the center position of each cell was moved slightly toward the centers of each 3D U-Net detected region (for details, see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). After correction, all cells were moved to the estimated positions with their shapes unchanged from volume #1. For most of the datasets, we applied only one correction; for the worm #4 and tumor spheroid datasets, we applied corrections up to 20 times, until achieving convergence. If multiple cells overlapped in the new positions, we applied the watershed method again to assign their boundaries. In carrying out this step, we calculated the images of the tracked cell regions based on an interpolation of the raw image from volume #1, when the resolution along the <italic>z</italic> axis was much lower than that in the <italic>x-y</italic> plane (i.e. all datasets except for the worm #4 and the zebrafish). We did this in order to obtain more accurate estimates for each cell region.</p></sec><sec id="s4-10"><title>Manual correction of segmentation</title><p>We manually corrected the segmentation only in volume #1. We superimposed the automatically segmented regions of the volume #1 3D image onto the raw 3D image in the ITK-SNAP software, and we discarded false positive regions, such as autofluorescence regions and neuronal processes. If any cells were not detected (false negative error), we reduced the noise level parameter in the pre-processing (<xref ref-type="table" rid="table1">Table 1</xref>) which can eliminate such errors, or we manually added these cells in ITK-SNAP. Oversegmentation and undersegmentation regions were corrected carefully by considering the sizes and shapes of the majority of cells. The overall error rates depend on the image quality, but we usually found that around 10% of cells required manual correction which usually took 2–3 hr (for 100–200 cells).</p></sec><sec id="s4-11"><title>Visual inspection of tracking results</title><p>We counted the tracking errors of all cells by visually inspecting the tracking results in each volume (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). To confirm the tracking of worm’s neurons, we combined two 3D + T images—the raw images and the tracked labels—in a top–bottom arrangement displayed as a hyperstack by the ImageJ software to compare the cell locations in each volume. As the cells in worms’ datasets primarily moved in the x-y plane (<xref ref-type="fig" rid="fig3">Figures 3F</xref> and <xref ref-type="fig" rid="fig9">9L</xref>, and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2F</xref>), we observed the correspondence between the raw images and the tracked labels in each x-y plane to identify tracking errors in the results. To confirm our tracking of the tumor spheroid dataset, we applied the same method although there were many small cross-layer movements in the images (<xref ref-type="fig" rid="fig8">Figure 8F and G</xref>).</p><p>It was more difficult to confirm the tracking results in the hyperstack images for the cells in the freely moving worm and the zebrafish heart than in the semi-immobilized worm and tumor spheroid, due to the frequent occurrence of large movements of cells across layers (<xref ref-type="fig" rid="fig6">Figures 6G</xref> and <xref ref-type="fig" rid="fig7">7G</xref>). Therefore, we superimposed images of the tracked labels onto the raw images and imported them into IMARIS (Bitplane, UK), and then visually checked the tracking results of each cell individually in 3D mode. For the zebrafish dataset with repeated oscillations, we tracked and checked all 98 cells from 1000 volumes. Because the freely moving worm engaged in irregular movements in three directions, visual checking was more challenging, and thus we only tracked and checked the initial 500 volumes of 3D images out of 1519 original volumes (<xref ref-type="fig" rid="fig6">Figure 6F</xref>).</p></sec><sec id="s4-12"><title>Evaluating large movements</title><p>Large movements of cells is one issue that makes tracking challenging. To evaluate how challenging each cell movement is, we defined the 'relative movement' (<italic>RM</italic>) of cell A at time <italic>t</italic> as: <italic>RM</italic> = (movement of cell A from <italic>t</italic>-1 to <italic>t</italic>) / (distance between cell A and its closest neighboring cell at <italic>t</italic>).</p><p><xref ref-type="fig" rid="fig4">Figure 4A</xref> middle and right panels illustrate two cells moving in one dimensional space with <italic>RM</italic> ≥0.5. In this condition, a very simple tracking method, ‘search for the closest cell at the next time point’ will mistakenly match the cell A at <italic>t</italic> = 2 to the cell B at <italic>t</italic> = 1. Therefore, we argue that movements with <italic>RM</italic> ≥0.5 are more challenging than movements with <italic>RM</italic> &lt;0.5.</p><p>It should be noted that large movement is not the only challenge for tracking. For example, the zebrafish datasets have much higher error rates if we evaluate the tracking in all cells (<xref ref-type="table" rid="table3">Table 3</xref>), which is likely caused by the weak intensities in these small cells and the photobleaching (<xref ref-type="fig" rid="fig7">Figure 7B and F</xref>; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p></sec><sec id="s4-13"><title>Extracting activities</title><p>After having tracked the worm’s neurons from the first volume to the last volume, we extracted activities from the regions corresponding to each neuron. By measuring the mean intensities of each neuron in both channels corresponding to the GECI and the positional markers, the activity was computed as GCaMP5G/tdTomato in dataset worm #1. We similarly extracted calcium activities (GCaMP) of heart cells in the zebrafish dataset and the FRET activity in the tumor spheroid dataset.</p></sec><sec id="s4-14"><title>Comparison of tracking accuracy between our method and two other methods</title><p>Because DeepCell 2.0 currently only supports tracking cells in 2D datasets, we tested it using two layers of images (<italic>z</italic> = 9 and <italic>z</italic> = 16) from worm dataset #3, which include relatively large numbers of cells. We excluded cells that disappeared or appeared due to cross-layer movements for a fair comparison. We supplied DeepCell 2.0 with the precise segmentations from our method in order to focus on comparing the performance of the tracking algorithms.</p><p>We tested Toyoshima’s software using two worm datasets and one zebrafish dataset. For the zebrafish dataset, we only tested the initial 100 volumes because their method required a longer time for the imaging processing than ours did (see <xref ref-type="table" rid="table5">Table 5</xref>).</p><p>For these two methods, we communicated with the corresponding authors and did our best to optimize the parameters based on their suggestions. Nevertheless, it is possible that those analyses could be further optimized for our datasets.</p></sec><sec id="s4-15"><title>The runtimes of our pipeline and previous methods for the tested datasets</title><p>We tested the runtimes of our method and the previous methods for different datasets (see <xref ref-type="table" rid="table5">Table 5</xref>). Because DeepCell 2.0 currently can only track 2D + T images, the runtime was estimated by using the average runtime for one layer and multiplying that by 21 layers, in order to compare it with our method. As a result, DeepCell 2.0 required a runtime comparable with our method. On the other hand, Toyoshima’s software took a much longer time than our method to process the worm and zebrafish datasets.</p><p>In our method, the initial matching using our custom feedforward network is performed in a pairwise fashion, so the time complexity is <italic>O(n<sup>2</sup>)</italic>, where <italic>n</italic> is the number of detected cells. In our tested datasets with 100 ~ 200 cells, this did not take a long time, for example,~3.3 s were required for matching 98 cells between two volumes (zebrafish), or ~8.6 s for 164 cells (worm #1a) using our desktop PC. In cases where the <italic>n</italic> is too large, the runtime may become much longer, for example,~4 min for 901 cells (tumor spheroid). If both <italic>n</italic> and volume number are too large, it may be required to optimize the method to reduce the runtime, for example, by restricting the calculation of matchings in a set (100 ~ 200 or more) of representative cells (e.g. large/bright cells), while movements of other non-representative cells can be estimated from the movements of these representative cells utilizing the coherency of these movements in a deforming organ.</p></sec><sec id="s4-16"><title>Worm strains and cultivation</title><p>The techniques used to culture and handle <italic>C. elegans</italic> were essentially the same as those described previously (<xref ref-type="bibr" rid="bib6">Brenner, 1974</xref>). Both TQ1101 <italic>lite-1(xu7)</italic> and AML14 were obtained from the Caenorhabditis Genetics Center (University of Minnesota, USA). Young adult hermaphrodites were used in the imaging experiments.</p><p>For pan-neuronal expression, <italic>NLS::tdTomato::NLS</italic> (<xref ref-type="bibr" rid="bib12">Frøkjær-Jensen et al., 2016</xref>) and <italic>NLS::GCaMP5G::NLS</italic> (in which <italic>GCaMP5G</italic> (<xref ref-type="bibr" rid="bib2">Akerboom et al., 2012</xref>) was codon-optimized for <italic>C. elegans</italic> and attached to <italic>NLS</italic> at N- and C-termini) were fused with the <italic>rab-3</italic> promoter (<xref ref-type="bibr" rid="bib35">Stefanakis et al., 2015</xref>) using a GATEWAY system (Thermo Fisher Scientific). Germline transformation into <italic>lite-1(xu7)</italic> (<xref ref-type="bibr" rid="bib21">Liu et al., 2010</xref>) was performed using microinjection (<xref ref-type="bibr" rid="bib23">Mello et al., 1992</xref>) with a solution containing pYFU251 <italic>rab-3p::NLS::GCaMP5G::NLS</italic> (25 ng/µl), pYFU258 <italic>rab-3p::NLS::tdTomato::NLS</italic> (20 ng/µl) and OP50 genome (55 ng/µl) to obtain the strain KDK54165. The strain <italic>lite-1(xu7)</italic> was used to reduce the blue light-induced activation of the worm’s sensory system (<xref ref-type="bibr" rid="bib21">Liu et al., 2010</xref>). Independent transgenic lines obtained from the injection produced similar results.</p></sec><sec id="s4-17"><title>Worm datasets</title><p>In this study, we used four worm datasets. The 3D images in datasets worm #1 and #2 were obtained using our custom-made microscope system, OSB3D (see below). The worm strains for datasets worm #1 and #2 are KDK54165 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:WB-STRAIN:KDK54165">WB-STRAIN:KDK54165</ext-link>) and AML14 <italic>wtfEx4[rab-3p::NLS::GCaMP6s: rab-3p::NLS::tagRFP]</italic> (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:WB-STRAIN:AML14">WB-STRAIN:AML14</ext-link>) (<xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref>), respectively. The 3D + T images of the dataset worm #3 were published previously with the worm strain JN2101 <italic>Is[H20p::NLS4::mCherry]; Ex[tax-4p::NLS-YC2.60, lin-44p::GFP]</italic> (<xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>). The 3D + T images of the freely moving worm dataset (worm #4) were published previously with the worm strain AML14 (<xref ref-type="bibr" rid="bib26">Nguyen et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Nguyen et al., 2017</xref>). In three (#79, #135, and #406) out of the 500 volumes of the worm #4 dataset, most of the cells disappeared from the volume or a large amount of noise occurred, which rendered the dataset impossible to analyze. These volumes were therefore manually skipped for tracking, that is the cells were assumed not moved from the previous volumes.</p></sec><sec id="s4-18"><title>Spinning disk confocal system for 3D + T imaging of the worm's brain</title><p>We upgraded our robotic microscope system (<xref ref-type="bibr" rid="bib36">Tanimoto et al., 2017</xref>) to a 3D version. We used a custom-made microscope system that integrated the Nikon Eclipse Ti-U inverted microscope system with an LV Focusing Module and a FN1 Epi-fl attachment (Flovel, Japan). The excitation light was a 488 nm laser from OBIS 488–60 LS (Coherent) that was introduced into a confocal unit with a filter wheel controller (CSU-X1 and CSU-X1CU, respectively; Yokogawa, Japan) to increase the rotation speed to 5,000 rpm. The CSU-X1 was equipped with a dichroic mirror (Di01-T-405/488/561, Semrock) to reflect the 488 nm light to an objective lens (CFI S Fluor 40X Oil, Nikon, Japan), which transmitted the GCaMP fluorescence used for calcium imaging and the red fluorescence used for cell positional markers. The laser power was set to 60 mW (100%). The fluorescence was introduced through the CSU-X1 into an image splitting optic (W-VIEW GEMINI, Hamamatsu, Japan) with a dichroic mirror (FF560-FDi01, Opto-line, Japan) and two bandpass filters (BA495-540 and BA570-625HQ, Olympus, Japan). The two fluorescent images were captured side-by-side on an sCMOS camera (ORCA Flash 4.0v3, Hamamatsu, Japan), which was controlled by a Precision T5810 (Dell) computer with 128 GB RAM using the HCImage Live software (Hamamatsu) for Windows 10 Pro. A series of images for one experiment (approximately 1–4 min) required approximately 4–15 GB of space, and were stored in memory during the experiment, then transferred to a 1-TB USB 3.0 external solid-state drive (TS1TESD400K, Transcend, Taiwan) for further processing.</p><p>For 3D imaging, the <italic>z</italic>-position of the objective lens was regulated by a piezo objective positioner (P-721) with a piezo controller (E665) and the PIMikroMove software (PI, Germany). The timings of the piezo movement and the image capture were regulated by synchronized external edge triggers from an Arduino Uno (Arduino, Italy) using 35 ms intervals for each step, in which the image capture was 29.9 ms. For each step, the piezo moved 1.5 µm, and one cycle consisted of 29 steps. We discarded the top-most step because it frequently deviated from the correct position, and we used the remaining 28 steps. Note that one 3D image was 42 µm in length along the <italic>z</italic>-axis, which was determined based on the typical diameters of neuronal cell bodies (2–3 µm) and of a young adult worm’s body (30–40 µm). Each cycle required 1015 ms; thus, one 3D image was obtained per second. This condition was reasonable for monitoring neuronal activities because the worm’s neurons do not generate action potentials (<xref ref-type="bibr" rid="bib14">Goodman et al., 1998</xref>) and because many neuronal responses change on the order of seconds (<xref ref-type="bibr" rid="bib28">Nichols et al., 2017</xref>). We also tested a condition using 10 ms for each step and 4.9 ms for an exposure with the same step size and step number per cycle (i.e. 2.3 volumes of 3D images per second), which yielded a comparable result. For cyclic regulation of the piezo position, we used a sawtooth wave instead of a triangle wave to assign positional information because the sawtooth wave produced more accurate <italic>z</italic> positions with less variance between cycles.</p></sec><sec id="s4-19"><title>Zebrafish heart cells</title><p>Sample preparation and imaging have been described previously (<xref ref-type="bibr" rid="bib42">Voleti et al., 2019</xref>). In brief, GCaMP and dsRed were expressed in the cytosol and the nuclei of myocardial cells, respectively. The 3D + T images obtained with the SCAPE 2.0 system were skew corrected to account for the oblique imaging geometry and analyzed with 3DeeCellTracker. The 3D coordinates obtained from this study were used to extract calcium dynamics in the cells in the previous study (<xref ref-type="bibr" rid="bib42">Voleti et al., 2019</xref>).</p></sec><sec id="s4-20"><title>Tumor spheroid</title><p>The tumor spheroid was cultured according to the previous procedures (<xref ref-type="bibr" rid="bib41">Vinci et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Yamaguchi et al., 2021</xref>). In brief, a suspension of HeLa cells (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:CVCL_0030">CVCL_0030</ext-link>, RCB0007, Riken Cell Bank; certified as mycoplasma-free and authenticated by using STR profiling) expressing the FRET-type ERK sensor EKAREV-NLS (<xref ref-type="bibr" rid="bib17">Komatsu et al., 2011</xref>) was added to a PrimeSurface 96 well plate (MS-9096U, Sumitomo Bakelite) for 3D cell culture at a density of 1200 cells/well. The grown spheroids were transferred to 35 mm-dishes coated with poly-L-lysine (Sigma-Aldrich) and further grown with DMEM/F-12, no phenol red (ThermoFisher) containing 10% FBS and 1% penicillin and streptomycin.</p><p>The 3D + T images of the spheroid were recorded using a two-photon microscope equipped with a water-immersion objective lens (N25X-APO-MP 25x CFI APO LWD objective, Nikon) and a high-sensitivity gallium arsenide phosphide (GaAsP) detector (A1R-MP+, Nikon). An 820 nm optical pulse generated by a Ti:Sapphire laser was used for the excitation. The fluorescence was split using two dichroic mirrors (FF495-Di03 and FF593-Di02, Opto-line), and the split lights below 495 nm and between 495–593 nm were detected by independent channels (CH1 and CH2, respectively). The fluorescence signals were integrated four times to increase the signal-to-noise ratio. Each step size was 4 µm along the <italic>z-</italic>axis, and each volume comprised of 54 steps, requiring 5 min for recording. Cells that experienced cell death or cell division were excluded manually after the events when evaluating the tracking accuracy.</p></sec><sec id="s4-21"><title>Code availability statement</title><p>The code for tracking cells and for training neural networks, the demo data, the pre-trained weights of the neural networks, and instructions for installation and use of the code are available in <ext-link ext-link-type="uri" xlink:href="http://ssbd.qbic.riken.jp/set/20190602/">http://ssbd.qbic.riken.jp/set/20190602/</ext-link> (Demos190610.zip). An updated version of the code can be found in <ext-link ext-link-type="uri" xlink:href="https://github.com/WenChentao/3DeeCellTracker">https://github.com/WenChentao/3DeeCellTracker</ext-link>. The guides for using the codes and for setting parameters have also been included in the same GitHub repository.</p></sec></sec><sec id="s5"><title>Note in proof</title><p>In the original version of the paper, we tracked the datasets by our original version of programs based on the package &quot;3DeeCellTracker 0.2&quot; (see the version information at <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/3DeeCellTracker/#history">https://pypi.org/project/3DeeCellTracker/#history</ext-link>). The runtime for the tracking process was not optimized at that time, which sometimes could lead to a long runtime for tracking, especially in the ensemble mode and/or when the cell number is large. In addition, our previous tracking program did not hide the details irrelevant to the end-users and did not provide useful feedback on the intermediate segmentation/tracking results to users.</p><p>To solve these two issues, we first improved the performance of the codes related to “FFN” and “PR-GLS” using the vectorization techniques available in the “NumPy” package in Python. We also improved the performance of the codes related to “accurate correction” by extracting and performing only once the previously repeated calculations in the loops. Note that we did not change our programs related to segmentation (3D U-Net + watershed), which may still require considerable time depending on factors such as the image size and structure of the 3D U-Net. As a result, our new program accelerated the speed by 1.7–6.8 times in the tested datasets (<xref ref-type="table" rid="table6">Table 6</xref>). Such acceleration was especially pronounced in ensemble mode (worm #4) and when the cell number is large (3D tumor spheroid). Second, we simplified the program by moving the details into the package. As a result, users can track cells by simply running a few commands in Jupyter notebooks. We also added new functions to show the intermediate results of segmentation and tracking so that users can now use these results to guide their setup of the segmentation/tracking parameters. To use these new features, users can follow the updated instructions in the README file in our GitHub repository to install the latest version of 3DeeCellTracker (currently 0.4.0) and use our Jupyter notebooks.</p><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Runtime and speed of our programs, including both the segmentation and tracking.</title><p>The speeds were calculated as the reciprocals of runtimes.</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th/><th align="center" colspan="2">Runtime (s/volume)</th><th align="center" colspan="2">Speed (volume/h)</th></tr><tr valign="top"><th/><th>Current (0.4.0)</th><th>Previous (0.2)</th><th>Current (0.4.0)</th><th>Previous (0.2)</th></tr></thead><tbody><tr valign="top"><td>Worm #1a</td><td>21</td><td>106</td><td>171</td><td>34</td></tr><tr valign="top"><td>Worm #3</td><td>23</td><td>38</td><td>156</td><td>95</td></tr><tr valign="top"><td>Worm #4 (single)</td><td>73</td><td>122</td><td>49</td><td>30</td></tr><tr valign="top"><td>Worm #4 (ensemble)</td><td>86</td><td>589</td><td>42</td><td>6</td></tr><tr valign="top"><td>Zebrafish</td><td>21</td><td>60</td><td>171</td><td>60</td></tr><tr valign="top"><td>﻿Tumor spheroid</td><td>128</td><td>834</td><td>28</td><td>4</td></tr></tbody></table></table-wrap></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Nobutoshi Odajima (Flovel Co. Ltd.), Hideki Tanaka (Yokogawa Corp.), Kenichi Matsumoto (Hamamatsu Photonics KK), and Kazuma Etani (Nikon Solutions Co. Ltd.) for setting up the OSB-3D. We also thank William Graf and David Van Valen for their kind advice in testing DeepCell 2.0, and Yu Toyoshima for his kind advice and help in testing the software by <xref ref-type="bibr" rid="bib38">Toyoshima et al., 2016</xref>. We also thank Toru Tamaki, Ichiro Takeuchi, Takuto Sakuma, Katsuyoshi Matsushita, Hiroyuki Kaneko, Taro Sakurai, Jared Young, Richard Yan, Yuto Endo and the other Kimura laboratory members for their valuable advice, comments and technical assistance for this study. Nematode strains were provided by the Caenorhabditis Genetics Center (funded by the NIH Office of Research Infrastructure Programs P40 OD010440). Zebrafish samples were provided by Kimara Targoff, Caitlin Ford and Carmen de Sena Tomás and imaged with assistance from Citlali Perez-Campos and Wenze Li.</p></ack><sec id="s6" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Investigation</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Investigation</p></fn><fn fn-type="con" id="con7"><p>Investigation</p></fn><fn fn-type="con" id="con8"><p>Investigation</p></fn><fn fn-type="con" id="con9"><p>Resources</p></fn><fn fn-type="con" id="con10"><p>Resources, Writing - review and editing</p></fn><fn fn-type="con" id="con11"><p>Investigation</p></fn><fn fn-type="con" id="con12"><p>Investigation</p></fn><fn fn-type="con" id="con13"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con14"><p>Conceptualization, Resources, Supervision, Investigation, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s7" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-59187-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s8" sec-type="data-availability"><title>Data availability</title><p>Datasets of worm #1a, #1b, and #2, and the tumor spheroid (referred to in Figure 3, Figure 3-figure supplement 1 and 2, and Figure 8, respectively), consisting of nuclear and calcium images are available in <ext-link ext-link-type="uri" xlink:href="http://ssbd.qbic.riken.jp/set/20190602/">http://ssbd.qbic.riken.jp/set/20190602/</ext-link>. The nuclear and calcium images of dataset worm #3 (Figure 3-figure supplement 3) have been described in the previous study (Toyoshima et al., 2016) and are available in <ext-link ext-link-type="uri" xlink:href="http://ssbd.qbic.riken.jp/search/D4D6ACB6-3CF4-11E6-B957-D3D8A1F734EE/">http://ssbd.qbic.riken.jp/search/D4D6ACB6-3CF4-11E6-B957-D3D8A1F734EE/</ext-link>. The freely moving worm dataset worm#4 has been described in the previous study (Nguyen et al. 2017) and is available in: <ext-link ext-link-type="uri" xlink:href="https://ieee-dataport.org/open-access/tracking-neurons-moving-and-deforming-brain-dataset">https://ieee-dataport.org/open-access/tracking-neurons-moving-and-deforming-brain-dataset</ext-link>. The nuclear and calcium images of the zebrafish dataset (Figure 7) are available upon request according to the previous study (Voleti et al., 2019).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>C</given-names></name><name><surname>Miura</surname><given-names>T</given-names></name><name><surname>Voleti</surname><given-names>V</given-names></name><name><surname>Yamaguchi</surname><given-names>K</given-names></name><name><surname>Tsutsumi</surname><given-names>M</given-names></name><name><surname>Yamamoto</surname><given-names>K</given-names></name><name><surname>Otomo</surname><given-names>K</given-names></name><name><surname>Fujie</surname><given-names>Y</given-names></name><name><surname>Teramoto</surname><given-names>T</given-names></name><name><surname>Ishihara</surname><given-names>T</given-names></name><name><surname>Aoki</surname><given-names>K</given-names></name><name><surname>Nemoto</surname><given-names>T</given-names></name><name><surname>Hillman</surname><given-names>EMC</given-names></name><name><surname>Kimura</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Time-lapse 3D imaging data of cell nuclei</data-title><source>Systems Science of Biological Dynamics database</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="http://ssbd.qbic.riken.jp/set/20190602/">http://ssbd.qbic.riken.jp/set/20190602/</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Toyoshima</surname><given-names>Y</given-names></name><name><surname>Tokunaga</surname><given-names>T</given-names></name><name><surname>Hirose</surname><given-names>O</given-names></name><name><surname>Kanamori</surname><given-names>M</given-names></name><name><surname>Teramoto</surname><given-names>T</given-names></name><name><surname>Jang</surname><given-names>MS</given-names></name><name><surname>Kuge</surname><given-names>S</given-names></name><name><surname>Ishihara</surname><given-names>T</given-names></name><name><surname>Yoshida</surname><given-names>R</given-names></name><name><surname>Iino</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>BDML file for quantitative information about neuronal nuclei in C. elegans (strain JN2101) adult extracted from time-lapse 3D multi-channel spinning disk confocal microscopy (SDCM) images</data-title><source>Systems Science of Biological Dynamics database</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="http://ssbd.qbic.riken.jp/search/D4D6ACB6-3CF4-11E6-B957-D3D8A1F734EE/">http://ssbd.qbic.riken.jp/search/D4D6ACB6-3CF4-11E6-B957-D3D8A1F734EE/</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>JP</given-names></name><name><surname>Linder</surname><given-names>AN</given-names></name><name><surname>Plummer</surname><given-names>GS</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Leifer</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>TRACKING NEURONS IN A MOVING AND DEFORMING BRAIN DATASET</data-title><source>IEEE Dataport</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://ieee-dataport.org/open-access/tracking-neurons-moving-and-deforming-brain-dataset">DOI:10.21227/H2901H</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Large-scale imaging in small brains</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>78</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.01.007</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akerboom</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Marvin</surname> <given-names>JS</given-names></name><name><surname>Mutlu</surname> <given-names>S</given-names></name><name><surname>Calderón</surname> <given-names>NC</given-names></name><name><surname>Esposti</surname> <given-names>F</given-names></name><name><surname>Borghuis</surname> <given-names>BG</given-names></name><name><surname>Sun</surname> <given-names>XR</given-names></name><name><surname>Gordus</surname> <given-names>A</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Macklin</surname> <given-names>JJ</given-names></name><name><surname>Filosa</surname> <given-names>A</given-names></name><name><surname>Aggarwal</surname> <given-names>A</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name><name><surname>Takagi</surname> <given-names>R</given-names></name><name><surname>Kracun</surname> <given-names>S</given-names></name><name><surname>Shigetomi</surname> <given-names>E</given-names></name><name><surname>Khakh</surname> <given-names>BS</given-names></name><name><surname>Baier</surname> <given-names>H</given-names></name><name><surname>Lagnado</surname> <given-names>L</given-names></name><name><surname>Wang</surname> <given-names>SS</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name><name><surname>Kimmel</surname> <given-names>BE</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Optimization of a GCaMP calcium Indicator for neural activity imaging</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>13819</fpage><lpage>13840</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2601-12.2012</pub-id><pub-id pub-id-type="pmid">23035093</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bannon</surname> <given-names>D</given-names></name><name><surname>Moen</surname> <given-names>E</given-names></name><name><surname>Borba</surname> <given-names>E</given-names></name><name><surname>Ho</surname> <given-names>A</given-names></name><name><surname>Camplisson</surname> <given-names>I</given-names></name><name><surname>Chang</surname> <given-names>B</given-names></name><name><surname>Osterman</surname> <given-names>E</given-names></name><name><surname>Graf</surname> <given-names>W</given-names></name><name><surname>Van Valen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepCell 2.0: automated cloud deployment of deep learning models for large-scale cellular image analysis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/505032</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beucher</surname> <given-names>S</given-names></name><name><surname>Meyer</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1993">1993</year><chapter-title>The morphological approach to segmentation: the watershed transformation</chapter-title><person-group person-group-type="editor"><name><surname>Dougherty</surname> <given-names>E. R</given-names></name></person-group><source>Mathematical Morphology in Image Processing</source><publisher-name>Marcel Dekker</publisher-name><fpage>433</fpage><lpage>481</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchard</surname> <given-names>MB</given-names></name><name><surname>Voleti</surname> <given-names>V</given-names></name><name><surname>Mendes</surname> <given-names>CS</given-names></name><name><surname>Lacefield</surname> <given-names>C</given-names></name><name><surname>Grueber</surname> <given-names>WB</given-names></name><name><surname>Mann</surname> <given-names>RS</given-names></name><name><surname>Bruno</surname> <given-names>RM</given-names></name><name><surname>Hillman</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Swept confocally-aligned planar excitation (SCAPE) microscopy for high speed volumetric imaging of behaving organisms</article-title><source>Nature Photonics</source><volume>9</volume><fpage>113</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1038/nphoton.2014.323</pub-id><pub-id pub-id-type="pmid">25663846</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The genetics of <italic>Caenorhabditis elegans</italic></article-title><source>Genetics</source><volume>77</volume><fpage>71</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">4366476</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Çiçek</surname> <given-names>O</given-names></name><name><surname>Abdulkadir</surname> <given-names>A</given-names></name><name><surname>Lienkamp</surname> <given-names>SS</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name><name><surname>Ronneberger</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>3D U-Net: Learning Dense Volumetric Segmentation From Sparse Annotation. Medical Image Computing and Computer- Assisted Intervention—MICCAI</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Bono</surname> <given-names>M</given-names></name><name><surname>Maricq</surname> <given-names>AV</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuronal substrates of complex behaviors in <italic>C. elegans</italic></article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>451</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144259</pub-id><pub-id pub-id-type="pmid">16022603</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egnor</surname> <given-names>SE</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational analysis of behavior</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>217</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013845</pub-id><pub-id pub-id-type="pmid">27090952</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliceiri</surname> <given-names>KW</given-names></name><name><surname>Berthold</surname> <given-names>MR</given-names></name><name><surname>Goldberg</surname> <given-names>IG</given-names></name><name><surname>Ibáñez</surname> <given-names>L</given-names></name><name><surname>Manjunath</surname> <given-names>BS</given-names></name><name><surname>Martone</surname> <given-names>ME</given-names></name><name><surname>Murphy</surname> <given-names>RF</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Plant</surname> <given-names>AL</given-names></name><name><surname>Roysam</surname> <given-names>B</given-names></name><name><surname>Stuurman</surname> <given-names>N</given-names></name><name><surname>Stuurmann</surname> <given-names>N</given-names></name><name><surname>Swedlow</surname> <given-names>JR</given-names></name><name><surname>Tomancak</surname> <given-names>P</given-names></name><name><surname>Carpenter</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Biological imaging software tools</article-title><source>Nature Methods</source><volume>9</volume><fpage>697</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2084</pub-id><pub-id pub-id-type="pmid">22743775</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frigault</surname> <given-names>MM</given-names></name><name><surname>Lacoste</surname> <given-names>J</given-names></name><name><surname>Swift</surname> <given-names>JL</given-names></name><name><surname>Brown</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Live-cell microscopy - tips and tools</article-title><source>Journal of Cell Science</source><volume>122</volume><fpage>753</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1242/jcs.033837</pub-id><pub-id pub-id-type="pmid">19261845</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frøkjær-Jensen</surname> <given-names>C</given-names></name><name><surname>Jain</surname> <given-names>N</given-names></name><name><surname>Hansen</surname> <given-names>L</given-names></name><name><surname>Davis</surname> <given-names>MW</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Zhao</surname> <given-names>D</given-names></name><name><surname>Rebora</surname> <given-names>K</given-names></name><name><surname>Millet</surname> <given-names>JRM</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>SK</given-names></name><name><surname>Dupuy</surname> <given-names>D</given-names></name><name><surname>Jorgensen</surname> <given-names>EM</given-names></name><name><surname>Fire</surname> <given-names>AZ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An abundant class of Non-coding DNA can prevent stochastic gene silencing in the <italic>C. elegans</italic> Germline</article-title><source>Cell</source><volume>166</volume><fpage>343</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.05.072</pub-id><pub-id pub-id-type="pmid">27374334</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Deep Learning</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>MB</given-names></name><name><surname>Hall</surname> <given-names>DH</given-names></name><name><surname>Avery</surname> <given-names>L</given-names></name><name><surname>Lockery</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Active currents regulate sensitivity and dynamic range in <italic>C. elegans</italic> neurons</article-title><source>Neuron</source><volume>20</volume><fpage>763</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81014-4</pub-id><pub-id pub-id-type="pmid">9581767</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Gkioxari</surname> <given-names>G</given-names></name><name><surname>Dollar</surname> <given-names>P</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mask R-CNN</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV)</conf-name><fpage>2980</fpage><lpage>2988</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.322</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jian</surname> <given-names>B</given-names></name><name><surname>Vemuri</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A robust algorithm for point set registration using mixture of gaussians</article-title><conf-name> 10th IEEE International Conference on Computer Vision</conf-name><fpage>1246</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2005.17</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komatsu</surname> <given-names>N</given-names></name><name><surname>Aoki</surname> <given-names>K</given-names></name><name><surname>Yamada</surname> <given-names>M</given-names></name><name><surname>Yukinaga</surname> <given-names>H</given-names></name><name><surname>Fujita</surname> <given-names>Y</given-names></name><name><surname>Kamioka</surname> <given-names>Y</given-names></name><name><surname>Matsuda</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Development of an optimized backbone of FRET biosensors for kinases and GTPases</article-title><source>Molecular Biology of the Cell</source><volume>22</volume><fpage>4647</fpage><lpage>4656</lpage><pub-id pub-id-type="doi">10.1091/mbc.e11-01-0072</pub-id><pub-id pub-id-type="pmid">21976697</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1090</fpage><lpage>1098</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liang</surname> <given-names>X</given-names></name><name><surname>Wei</surname> <given-names>Y</given-names></name><name><surname>Shen</surname> <given-names>X</given-names></name><name><surname>Jie</surname> <given-names>Z</given-names></name><name><surname>Feng</surname> <given-names>J</given-names></name><name><surname>Lin</surname> <given-names>L</given-names></name><name><surname>Yan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reversible recursive Instance-Level object segmentation</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>633</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.75</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Ward</surname> <given-names>A</given-names></name><name><surname>Gao</surname> <given-names>J</given-names></name><name><surname>Dong</surname> <given-names>Y</given-names></name><name><surname>Nishio</surname> <given-names>N</given-names></name><name><surname>Inada</surname> <given-names>H</given-names></name><name><surname>Kang</surname> <given-names>L</given-names></name><name><surname>Yu</surname> <given-names>Y</given-names></name><name><surname>Ma</surname> <given-names>D</given-names></name><name><surname>Xu</surname> <given-names>T</given-names></name><name><surname>Mori</surname> <given-names>I</given-names></name><name><surname>Xie</surname> <given-names>Z</given-names></name><name><surname>Xu</surname> <given-names>XZ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title><italic>C. elegans</italic> phototransduction requires a G protein-dependent cGMP pathway and a taste receptor homolog</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>715</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1038/nn.2540</pub-id><pub-id pub-id-type="pmid">20436480</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>J</given-names></name><name><surname>Zhao</surname> <given-names>J</given-names></name><name><surname>Yuille</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Non-Rigid point set registration by preserving global and local structures</article-title><source>IEEE Transactions on Image Processing : A Publication of the IEEE Signal Processing Society</source><volume>25</volume><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1109/TIP.2015.2467217</pub-id><pub-id pub-id-type="pmid">26276991</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname> <given-names>CC</given-names></name><name><surname>Kramer</surname> <given-names>JM</given-names></name><name><surname>Stinchcomb</surname> <given-names>D</given-names></name><name><surname>Ambros</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Efficient gene transfer in <italic>C. elegans</italic>: extrachromosomal maintenance and integration of transforming sequences</article-title><source>Trends in Genetics</source><volume>8</volume><elocation-id>50</elocation-id><pub-id pub-id-type="doi">10.1002/j.1460-2075.1991.tb04966.x</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moen</surname> <given-names>E</given-names></name><name><surname>Bannon</surname> <given-names>D</given-names></name><name><surname>Kudo</surname> <given-names>T</given-names></name><name><surname>Graf</surname> <given-names>W</given-names></name><name><surname>Covert</surname> <given-names>M</given-names></name><name><surname>Van Valen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning for cellular image analysis</article-title><source>Nature Methods</source><volume>16</volume><fpage>1233</fpage><lpage>1246</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id><pub-id pub-id-type="pmid">31133758</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Myronenko</surname> <given-names>A</given-names></name><name><surname>Myronenko</surname> <given-names>A</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name><name><surname>MÁ</surname> <given-names>C-P</given-names></name><name><surname>MÁ</surname> <given-names>C-P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Non-rigid point set registration: coherent point drift</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1009</fpage><lpage>1016</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Shipley</surname> <given-names>FB</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Liu</surname> <given-names>M</given-names></name><name><surname>Setru</surname> <given-names>SU</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whole-brain calcium imaging with cellular resolution in freely behaving <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1074</fpage><lpage>E1081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507110112</pub-id><pub-id pub-id-type="pmid">26712014</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>JP</given-names></name><name><surname>Linder</surname> <given-names>AN</given-names></name><name><surname>Plummer</surname> <given-names>GS</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Leifer</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automatically tracking neurons in a moving and deforming brain</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005517</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005517</pub-id><pub-id pub-id-type="pmid">28545068</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname> <given-names>ALA</given-names></name><name><surname>Eichler</surname> <given-names>T</given-names></name><name><surname>Latham</surname> <given-names>R</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A global brain state underlies <italic>C. elegans</italic> sleep behavior</article-title><source>Science</source><volume>356</volume><elocation-id>eaam6851</elocation-id><pub-id pub-id-type="doi">10.1126/science.aam6851</pub-id><pub-id pub-id-type="pmid">28642382</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polikar</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Ensemble learning</article-title><source>Scholarpedia</source><volume>4</volume><elocation-id>2776</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.2776</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riedl</surname> <given-names>A</given-names></name><name><surname>Schlederer</surname> <given-names>M</given-names></name><name><surname>Pudelko</surname> <given-names>K</given-names></name><name><surname>Stadler</surname> <given-names>M</given-names></name><name><surname>Walter</surname> <given-names>S</given-names></name><name><surname>Unterleuthner</surname> <given-names>D</given-names></name><name><surname>Unger</surname> <given-names>C</given-names></name><name><surname>Kramer</surname> <given-names>N</given-names></name><name><surname>Hengstschläger</surname> <given-names>M</given-names></name><name><surname>Kenner</surname> <given-names>L</given-names></name><name><surname>Pfeiffer</surname> <given-names>D</given-names></name><name><surname>Krupitza</surname> <given-names>G</given-names></name><name><surname>Dolznig</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Comparison of Cancer cells in 2D vs 3D culture reveals differences in AKT-mTOR-S6K signaling and drug responses</article-title><source>Journal of Cell Science</source><volume>130</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1242/jcs.188102</pub-id><pub-id pub-id-type="pmid">27663511</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Romera-Paredes</surname> <given-names>B</given-names></name><name><surname>Torr</surname> <given-names>PHS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recurrent instance segmentation</article-title><conf-name>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</conf-name><fpage>312</fpage><lpage>329</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>U-Net: Convolutional Networks for Biomedical Image Segmentation. Medical Image Computing and Computer- Assisted Intervention—MICCAI</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rusu</surname> <given-names>RB</given-names></name><name><surname>Blodow</surname> <given-names>N</given-names></name><name><surname>Beetz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fast point feature histograms (FPFH) for 3D registration. <italic>2009</italic></article-title><conf-name>IEEE International Conference on Robotics and Automation</conf-name><fpage>3212</fpage><lpage>3217</lpage><pub-id pub-id-type="doi">10.1109/ROBOT.2009.5152473</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrödel</surname> <given-names>T</given-names></name><name><surname>Prevedel</surname> <given-names>R</given-names></name><name><surname>Aumayr</surname> <given-names>K</given-names></name><name><surname>Zimmer</surname> <given-names>M</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain-wide 3D imaging of neuronal activity in <italic>Caenorhabditis elegans</italic> with sculpted light</article-title><source>Nature Methods</source><volume>10</volume><fpage>1013</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2637</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanakis</surname> <given-names>N</given-names></name><name><surname>Carrera</surname> <given-names>I</given-names></name><name><surname>Hobert</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Regulatory Logic of Pan-Neuronal Gene Expression in <italic>C. elegans</italic></article-title><source>Neuron</source><volume>87</volume><fpage>733</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.031</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanimoto</surname> <given-names>Y</given-names></name><name><surname>Yamazoe-Umemoto</surname> <given-names>A</given-names></name><name><surname>Fujita</surname> <given-names>K</given-names></name><name><surname>Kawazoe</surname> <given-names>Y</given-names></name><name><surname>Miyanishi</surname> <given-names>Y</given-names></name><name><surname>Yamazaki</surname> <given-names>SJ</given-names></name><name><surname>Fei</surname> <given-names>X</given-names></name><name><surname>Busch</surname> <given-names>KE</given-names></name><name><surname>Gengyo-Ando</surname> <given-names>K</given-names></name><name><surname>Nakai</surname> <given-names>J</given-names></name><name><surname>Iino</surname> <given-names>Y</given-names></name><name><surname>Iwasaki</surname> <given-names>Y</given-names></name><name><surname>Hashimoto</surname> <given-names>K</given-names></name><name><surname>Kimura</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Calcium dynamics regulating the timing of decision-making in <italic>C. elegans</italic></article-title><source>eLife</source><volume>6</volume><elocation-id>e21629</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21629</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thevenaz</surname> <given-names>P</given-names></name><name><surname>Ruttimann</surname> <given-names>UE</given-names></name><name><surname>Unser</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A pyramid approach to subpixel registration based on intensity</article-title><source>IEEE Transactions on Image Processing</source><volume>7</volume><fpage>27</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1109/83.650848</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoshima</surname> <given-names>Y</given-names></name><name><surname>Tokunaga</surname> <given-names>T</given-names></name><name><surname>Hirose</surname> <given-names>O</given-names></name><name><surname>Kanamori</surname> <given-names>M</given-names></name><name><surname>Teramoto</surname> <given-names>T</given-names></name><name><surname>Jang</surname> <given-names>MS</given-names></name><name><surname>Kuge</surname> <given-names>S</given-names></name><name><surname>Ishihara</surname> <given-names>T</given-names></name><name><surname>Yoshida</surname> <given-names>R</given-names></name><name><surname>Iino</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Accurate Automatic Detection of Densely Distributed Cell Nuclei in 3D Space</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004970</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004970</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Valen</surname> <given-names>DA</given-names></name><name><surname>Kudo</surname> <given-names>T</given-names></name><name><surname>Lane</surname> <given-names>KM</given-names></name><name><surname>Macklin</surname> <given-names>DN</given-names></name><name><surname>Quach</surname> <given-names>NT</given-names></name><name><surname>DeFelice</surname> <given-names>MM</given-names></name><name><surname>Maayan</surname> <given-names>I</given-names></name><name><surname>Tanouchi</surname> <given-names>Y</given-names></name><name><surname>Ashley</surname> <given-names>EA</given-names></name><name><surname>Covert</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005177</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005177</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatachalam</surname> <given-names>V</given-names></name><name><surname>Ji</surname> <given-names>N</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Clark</surname> <given-names>C</given-names></name><name><surname>Mitchell</surname> <given-names>JK</given-names></name><name><surname>Klein</surname> <given-names>M</given-names></name><name><surname>Tabone</surname> <given-names>CJ</given-names></name><name><surname>Florman</surname> <given-names>J</given-names></name><name><surname>Ji</surname> <given-names>H</given-names></name><name><surname>Greenwood</surname> <given-names>J</given-names></name><name><surname>Chisholm</surname> <given-names>AD</given-names></name><name><surname>Srinivasan</surname> <given-names>J</given-names></name><name><surname>Alkema</surname> <given-names>M</given-names></name><name><surname>Zhen</surname> <given-names>M</given-names></name><name><surname>Samuel</surname> <given-names>ADT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pan-neuronal imaging in roaming <italic>Caenorhabditis elegans</italic></article-title><source>PNAS</source><volume>113</volume><fpage>E1082</fpage><lpage>E1088</lpage><pub-id pub-id-type="doi">10.1073/pnas.1507109113</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinci</surname> <given-names>M</given-names></name><name><surname>Gowan</surname> <given-names>S</given-names></name><name><surname>Boxall</surname> <given-names>F</given-names></name><name><surname>Patterson</surname> <given-names>L</given-names></name><name><surname>Zimmermann</surname> <given-names>M</given-names></name><name><surname>Court</surname> <given-names>W</given-names></name><name><surname>Lomas</surname> <given-names>C</given-names></name><name><surname>Mendiola</surname> <given-names>M</given-names></name><name><surname>Hardisson</surname> <given-names>D</given-names></name><name><surname>Eccles</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Advances in establishment and analysis of three-dimensional tumor spheroid-based functional assays for target validation and drug evaluation</article-title><source>BMC Biology</source><volume>10</volume><elocation-id>29</elocation-id><pub-id pub-id-type="doi">10.1186/1741-7007-10-29</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voleti</surname> <given-names>V</given-names></name><name><surname>Patel</surname> <given-names>KB</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Perez Campos</surname> <given-names>C</given-names></name><name><surname>Bharadwaj</surname> <given-names>S</given-names></name><name><surname>Yu</surname> <given-names>H</given-names></name><name><surname>Ford</surname> <given-names>C</given-names></name><name><surname>Casper</surname> <given-names>MJ</given-names></name><name><surname>Yan</surname> <given-names>RW</given-names></name><name><surname>Liang</surname> <given-names>W</given-names></name><name><surname>Wen</surname> <given-names>C</given-names></name><name><surname>Kimura</surname> <given-names>KD</given-names></name><name><surname>Targoff</surname> <given-names>KL</given-names></name><name><surname>Hillman</surname> <given-names>EMC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Real-time volumetric microscopy of in vivo dynamics and large-scale samples with SCAPE 2.0</article-title><source>Nature Methods</source><volume>16</volume><fpage>1054</fpage><lpage>1062</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0579-4</pub-id><pub-id pub-id-type="pmid">31562489</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisenburger</surname> <given-names>S</given-names></name><name><surname>Vaziri</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A guide to emerging technologies for Large-Scale and Whole-Brain optical imaging of neuronal activity</article-title><source>Annual Review of Neuroscience</source><volume>41</volume><fpage>431</fpage><lpage>452</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031458</pub-id><pub-id pub-id-type="pmid">29709208</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamaguchi</surname> <given-names>K</given-names></name><name><surname>Otomo</surname> <given-names>K</given-names></name><name><surname>Kozawa</surname> <given-names>Y</given-names></name><name><surname>Tsutsumi</surname> <given-names>M</given-names></name><name><surname>Inose</surname> <given-names>T</given-names></name><name><surname>Hirai</surname> <given-names>K</given-names></name><name><surname>Sato</surname> <given-names>S</given-names></name><name><surname>Nemoto</surname> <given-names>T</given-names></name><name><surname>Uji-I</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Adaptive optical Two-Photon microscopy for Surface-Profiled living biological specimens</article-title><source>ACS Omega</source><volume>6</volume><fpage>438</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1021/acsomega.0c04888</pub-id><pub-id pub-id-type="pmid">33458495</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59187.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Zimmer</surname><given-names>Manuel</given-names></name><role>Reviewing Editor</role><aff><institution>Research Institute of Molecular Pathology, Vienna Biocenter and University of Vienna</institution><country>Austria</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper is of great interest to system-biologists who aim to analyze large datasets of image-volume time series that contain objects to segment in continuously deforming life tissues. Wen et al. present a deep-learning method for mostly automatic 1) U-net based segmentation and 2) point-registration-based tracking of cells in tissues with ongoing deformations. The authors test their pipeline on various example systems: whole brain Ca<sup>2+</sup>-activity in partially immobilized and freely crawling <italic>C. elegans</italic>, beating zebrafish heart and a 3D culture of tumor tissue. This work presents significant improvements in tracking capabilities on various metrics including the possible number of tracked objects, robustness, and computing requirements.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;3DeeCellTracker, a deep learning-based pipeline for segmenting and tracking cells in deforming organs&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Ronald Calabrese as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>This paper presents a deep-learning method for mostly automatic 1) U-net based segmentation and 2) point-registration-based tracking of cells with some deformations. Their training data consists of a single volume of corrected annotated data. The authors test their pipeline on two example systems: 3 datasets of ~150 neurons each in partially immobilized <italic>C. elegans</italic> and ~100 cells in a beating zebrafish heart. Some datasets are from previous work [1,2] and some are newly acquired. Both of these previous works propose tracking pipelines, and this work claims improvements in tracking capabilities on various metrics including number of segmented neurons, percentage of correct identifications, and runtime. In addition, the authors degrade their datasets in various ways (adding noise, removing frames), and still retain good performance. However, both of these claims lack proper quantifications.</p><p>The general idea of using deep learning for segmentation, and for tracking, is timely. However, the manuscript feels at times anecdotal and it is not convincing that it is truly ready for the diverse applications it claims. The consensus among the reviewers is that the approach should be tested on more ground truth data-sets, which are available for worms and on repeated measurements from zebrafish heart. Moreover, it is unclear how well the approach would work from truly deforming tissues like unrestrained moving worms. Such datasets are available from the Leifer lab as well as from the Hillman lab, who is co-authoring this manuscript. Claims on the performance of their approach should be based on more solid quantifications.</p><p>Essential revisions:</p><p><italic>Reviewer #1:</italic></p><p>1) Motion in data</p><p>a) This pipeline would be a major breakthrough if tracking succeeded in the freely moving scenario across different species, as in previous custom-made trackers for <italic>C. elegans</italic> [3]. Indeed, this is the core challenge in video processing of calcium imaging data, as classical algorithms largely suffice for the no motion or slow-motion cases. To my understanding Nguyen et al., 2016, was from freely moving worms but the datasets used here seems all from an immobilized worm (worm #1-2). Please clarify. A better description of the used datasets should be provided instead of simply citing the original sources.</p><p>b) However, it is unclear how much of a contribution this pipeline is in this respect. While the zebrafish heart data are clearly moving and deforming (Figure 5—video 2), the motion is extremely stereotyped. The <italic>C. elegans</italic> videos however, do not appear to be moving (Figure 3—videos 1-2, Figure 4—videos 1-2). If this algorithm does not work with the motion present in freely moving animals, this should be clarified.</p><p>c) If it does, more explicit comparisons with the state of the art are required, e.g. [3].</p><p>d) Regardless of the exact claim, some quantification of tracking performance as a function of increased motion should be included. Although robustness tests are performed by removing intermediate volumes from the videos, due to the nature of the motion (cyclic in zebrafish and intermittent in <italic>C. elegans</italic>), it is not obvious if or in what way this increases the motion of the neurons</p><p>2) Feed-Forward Network (FFN) for initial tracking</p><p>a) This network is the sole deep-learning component of the second stage of the algorithm, finding correspondence between segmentations in different volumes. However, it is unclear what benefit this element brings, and the statement starting on &quot;We believe that the high accuracy and robustness of our method is based on the introduction of FFN for tracking&quot; seems unwarranted.</p><p>b) In particular: the FFN is trained on random affine transformations of segmented volumes (Materials and methods); thus, it is surprising that it would perform better than a direct affine alignment.</p><p>c) This comparison should be included, and if the FFN is better, some discussion motivating this surprising result should also be included.</p><p>3) Figure 2, part 2:</p><p>Panels A and B show FPFH and FNN performance (respectively) on a matching task, but neither appears to be the algorithm used in the actual pipeline, which is a combination of both. In addition, why is the matching shown between t=1 and t=10? I agree with the caption that this should be more challenging for the algorithm, but without the context of t=1 -&gt; t=2 or further quantification, it is hard to understand the goal of these panels.</p><p>This figure could be significantly improved by adding quantifications due to robustness to motion as discussed above.</p><p>4) Figure 5:</p><p>Photobleaching is mentioned as causing difficulties with segmentation, but this is not quantified.</p><p>5) Related to usability: A key claim of this paper is that the authors' package is easy to use. Indeed, I applaud the use of github and freely available software like Tensorflow. This claim is generally supported by 1) the fact that the work runs on a desktop (favorably comparing to other trackers, e.g., which require a cluster [3]) and 2) the multiple example systems and optical systems tested on. However, there are several key concerns:</p><p>a) Basic automatic conda environment creation should be supported; see the.yaml files in e.g. DeepLabCut and the associated tutorial</p><p>https://github.com/DeepLabCut/DeepLabCut/blob/master/conda-environments/DLC-CPU.yaml</p><p>b) Why is Fiji a dependency? Although I personally love Fiji, simple rigid alignment can be performed in python and would remove a fragile dependency</p><p>c) What implementation of PR-GLS is being used? The original publication references a MATLAB implementation: https://github.com/jiayi-ma/PR-GLS. However, the authors are using entirely Python… did the authors rewrite the original algorithm? Or is there another implementation not associated with the paper? This should be clarified</p><p><italic>Reviewer #2:</italic></p><p>1) The work uses way too few sets of validation data. Altogether, it only used three sets of <italic>C. elegans</italic> wholebrain imaging data and a single set of zebrafish heart imaging data. Drawing general conclusions from this on the algorithm's performance (especially comparisons with other algorithms) is premature.</p><p>2) In the comparisons with other methods, while the intention was good, the results are anecdotal. Testing the algorithms is done on a very limiting set of data; whether these datasets represent the breadth of realistic problems that would be encountered for the variety of applications (e.g. neuroscience, or other physiological or developmental functional imaging, in <italic>C. elegans</italic> or zebrafish or other model systems) is hard to tell. Also quite importantly, it is difficult to say whether the optimal choice of parameters were used for the competing methods.</p><p>3) Noises added to the real data (as in Figure 6) aren't necessarily representative of the real data's properties/features. There should be characterizations of the variability in real data on size, intensity, shape, etc. Further, it does seem that the performance of the algorithm is sensitive to these noises, and thus it is not trivial to prove that the algorithm is robust to these noises. The manuscript did not convince me that the algorithm currently can indeed deal with real noises in real datasets.</p><p>4) &quot;To solve the difficulty in preparing training data for FFN, we used artificial training data by simulating cell movements in deforming organs.&quot; Why is this good? How is the artificial data representing real organ deformation?</p><p>5) Results, comparing other datasets – the comparisons are very qualitative. Validation is vague, descriptive, rather than quantitative.</p><p>6) Several places of the manuscript are also anecdotal, e.g. Results (corrupt data, still get 53% correct). Even the in silico studies (e.g. noise or reduced sampling) aren't systematic.</p><p>7) 60-80% of zebrafish heart cells tracked – is it useful? Which ones are the missing cells? The qualitative conclusions seem to be fairly trivial.</p><p>The manuscript made a few assertions without backing up with data/results. For example, the problems identified are real (Results), but no specifics to back up, and no systematic characterizations are done, using real data, comparing the work with other approaches. Another example – &quot;our method became robust to tracking errors in the initial matching&quot;.</p><p>References:</p><p>1) &quot;Accurate Automatic Detection of Densely Distributed.… – PLoS.&quot; 6 Jun. 2016, https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004970. Accessed 26 Jun. 2020.</p><p>2) &quot;Real-time volumetric microscopy of in vivo dynamics and large.….&quot; 27 Sep. 2019, https://www.nature.com/articles/s41592-019-0579-4. Accessed 26 Jun. 2020.</p><p>3) &quot;Automatically tracking neurons in a moving and.… – PLoS.&quot; 18 May. 2017, https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005517. Accessed 26 Jun. 2020.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59187.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>This paper presents a deep-learning method for mostly automatic 1) U-net based segmentation and 2) point-registration-based tracking of cells with some deformations. Their training data consists of a single volume of corrected annotated data. The authors test their pipeline on two example systems: 3 datasets of ~150 neurons each in partially immobilized <italic>C. elegans</italic> and ~100 cells in a beating zebrafish heart. Some datasets are from previous work [1,2] and some are newly acquired. Both of these previous works propose tracking pipelines, and this work claims improvements in tracking capabilities on various metrics including number of segmented neurons, percentage of correct identifications, and runtime. In addition, the authors degrade their datasets in various ways (adding noise, removing frames), and still retain good performance. However, both of these claims lack proper quantifications.</p><p>The general idea of using deep learning for segmentation, and for tracking, is timely. However, the manuscript feels at times anecdotal and it is not convincing that it is truly ready for the diverse applications it claims. The consensus among the reviewers is that the approach should be tested on more ground truth data-sets, which are available for worms and on repeated measurements from zebrafish heart. Moreover, it is unclear how well the approach would work from truly deforming tissues like unrestrained moving worms. Such datasets are available from the Leifer lab as well as from the Hillman lab, who is co-authoring this manuscript. Claims on the performance of their approach should be based on more solid quantifications.</p></disp-quote><p>First of all, we would like to thank the editor and the reviewers for their positive and constructive comments on our original manuscript. To address the comments, we have performed the following:</p><p>1) We have tested two additional types of ground truth datasets. The first is a previously published dataset of a freely moving worm from the Leifer lab, from which we were able to track 99.8% of all cell movements by developing a new &quot;ensemble mode&quot;. The second is a 3D culture of ~900 tumor cell dataset, which was newly obtained for this study to demonstrate the applicability of our method for general biomedical research; the registration of many cell positions in a 3D culture to monitor their activities has become one of the crucial issues in recent microscopy-based life science research and in drug discovery. Because of these data additions, we modified the manuscript title partially. Additional zebrafish data was not obtained unfortunately under the current difficult quarantine conditions, and the neuronal images of a freely moving worm from the SCAPE system does not have sufficient resolution to segment currently for our method.</p><p>2) For most of the figures, we have added solid quantifications, such as cell intensities, cell movements, accuracy curves and positions of correctly/incorrectly tracked cells in the tracking results, etc.</p><p>3) We have also deleted as much anecdotal phrases as possible from the entire text and toned down the discussions on diverse applications.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1:</p><p>1) Motion in data</p><p>a) This pipeline would be a major breakthrough if tracking succeeded in the freely moving scenario across different species, as in previous custom-made trackers for <italic>C. elegans</italic> [3]. Indeed, this is the core challenge in video processing of calcium imaging data, as classical algorithms largely suffice for the no motion or slow-motion cases. To my understanding Nguyen et al., 2016, was from freely moving worms but the datasets used here seems all from an immobilized worm (worm #1-2). Please clarify. A better description of the used datasets should be provided instead of simply citing the original sources.</p></disp-quote><p>We apologize for the unclear description in the original manuscript. In the original manuscript, we used our own microscope system to image neurons in anesthetized AML14, the same strain used in Nguyen et al., 2016 (worm #2). We have clarified the experimental conditions of AML14 and other worms in the main text.</p><p>In addition, in this revised version, we analyze the freely moving worm dataset previously published by Nguyen et al., and describe how we were able to track most of the cell movements. For details, please see below and Figure 6. We appreciate the reviewer's comment, which has motivated us to track the neurons of a freely moving worm.</p><disp-quote content-type="editor-comment"><p>b) However, it is unclear how much of a contribution this pipeline is in this respect. While the zebrafish heart data are clearly moving and deforming (Figure 5—video 2), the motion is extremely stereotyped. The <italic>C. elegans</italic> videos however, do not appear to be moving (Figure 3—videos 1-2, Figure 4—videos 1-2). If this algorithm does not work with the motion present in freely moving animals, this should be clarified.</p></disp-quote><p>We showed (1) that the deep learning-based DeepCell 2.0 tracked ~10% of neurons in a semi-immobilized worm, whereas our method achieved 95–100% tracking (Figure 11—figure supplement 1A, Figure 11—video 1 and Table 4), and (2) that Toyoshima's method was able to track only ~40% of the zebrafish heart cells that our method could track (35 versus 84; Figure 11—figure supplement 1B and Table 4) with a run time that was 15 times longer than ours (Table 5). In addition, because our original program (&quot;single mode&quot;; see below) predicts cell positions from the previous volume, whether the motion is stereotyped (e.g., zebrafish heart) or not does not affect the tracking efficiency. We consider that these results demonstrate the advantages of our method to the previous methods even with the original version (&quot;single mode&quot;). Furthermore, we show that our program is now able to track the neurons of a freely moving worm with &quot;ensemble mode&quot; (please see next).</p><disp-quote content-type="editor-comment"><p>c) If it does, more explicit comparisons with the state of the art are required, e.g. [3].</p></disp-quote><p>Our original method (&quot;single mode&quot;) was able to track 73% of the cells, which was acceptable but not ideal. Therefore, we developed a new &quot;ensemble mode&quot;. To obtain robust tracking in the ensemble mode, we use the FFN to predict cell positions based on a calculation of the average of multiple predictions from different volumes (i.e. time points). Our method with ensemble mode was able to correctly track 99.8% of cell movements. Unfortunately, we were not able to find a corresponding description of the accuracy in the Nguyen et al., and therefore were unable to explicitly compare our outcomes. Nevertheless, we consider that our obtained 99.8% accuracy is sufficient. In addition, we would like to note that, while the method by Nguyen et al. requires a high-performance computing cluster, our method runs on a desktop PC with a GPU, which is more accessible to general researchers (Figure 6 and Results, Discussion and Materials and methods).</p><disp-quote content-type="editor-comment"><p>d) Regardless of the exact claim, some quantification of tracking performance as a function of increased motion should be included. Although robustness tests are performed by removing intermediate volumes from the videos, due to the nature of the motion (cyclic in zebrafish and intermittent in <italic>C. elegans</italic>), it is not obvious if or in what way this increases the motion of the neurons</p></disp-quote><p>We appreciate the comment. We have added graphs demonstrating the relationships between motion and error rates. Please refer to Figure 10—figure supplement 1 and 2.</p><disp-quote content-type="editor-comment"><p>2) Feed-Forward Network (FFN) for initial tracking</p><p>a) This network is the sole deep-learning component of the second stage of the algorithm, finding correspondence between segmentations in different volumes. However, it is unclear what benefit this element brings, and the statement starting on &quot;We believe that the high accuracy and robustness of our method is based on the introduction of FFN for tracking&quot; seems unwarranted.</p></disp-quote><p>We apologize for the unclear description and the absence of quantitative information in the original manuscript. We have quantified the differences in tracking accuracies obtained by our FFN in combination with the point set registration method (FFN + PR-GLS), the previous FPFH + PR-GLS, and the affine alignment (Figure 2—figure supplement 3C and D).</p><disp-quote content-type="editor-comment"><p>b) In particular: the FFN is trained on random affine transformations of segmented volumes (Materials and methods); thus, it is surprising that it would perform better than a direct affine alignment.</p></disp-quote><p>Again, we apologize for the unclear description in the original manuscript. Our method also adds random displacements for each cell independently after affine transformation. Thus, the simulated motions are not simply affine transformation but also include more complex deformations (Figure 2—figure supplement 2A and B).</p><disp-quote content-type="editor-comment"><p>c) This comparison should be included, and if the FFN is better, some discussion motivating this surprising result should also be included.</p></disp-quote><p>Affine alignment (used CPD algorithm from https://github.com/siavashk/pycpd) generated poorer predictions of cell positions than our FFN + PR-GLS. Please see Figure 2—figure supplement 3C and D. We appreciate the reviewer for having motivated us to compare our method with the affine alignment method.</p><disp-quote content-type="editor-comment"><p>3) Figure 2, part 2:</p><p>Panels A and B show FPFH and FNN performance (respectively) on a matching task, but neither appears to be the algorithm used in the actual pipeline, which is a combination of both. In addition, why is the matching shown between t=1 and t=10? I agree with the caption that this should be more challenging for the algorithm, but without the context of t=1 -&gt; t=2 or further quantification, it is hard to understand the goal of these panels.</p><p>This figure could be significantly improved by adding quantifications due to robustness to motion as discussed above.</p></disp-quote><p>We apologize for the confusion caused by the description in the original manuscript. The panels A and B in Figure 2—figure supplement 3 are to compare the results of initial matching by FPFH and FFN. After the initial matching by the FPFH (Ma et al.) or by the FFN (our method), the point set registration method (PR-GLS) was used in both of the methods. To clarify this point and to demonstrate the differences after the matching, we added panels C and D in Figure 2—figure supplement 3.</p><p>We compared volumes #1 (<italic>t</italic>1) and #10 (<italic>t</italic>2) as an example of a challenging condition. We chose this comparison because its average value of relative movements (<italic>RM</italic>) of all the cells between volumes #1 and #10 is 1.22, while between volumes #1 and #2 the average <italic>RM</italic> is 0.08, indicating that the cell movements between volumes #1 and #2 are smaller and easier.</p><disp-quote content-type="editor-comment"><p>4) Figure 5:</p><p>Photobleaching is mentioned as causing difficulties with segmentation, but this is not quantified.</p></disp-quote><p>Thank you for pointing this out. We have added panels for the quantification of time course changes in the intensities of cell regions and backgrounds, some of which showed photobleaching (panel B middle in Figures 3, 6, 7, and 8). As shown in Figure 7B top and middle, the intensities of the small cells substantially overlap with background regions, and the overlap became even larger through time because of the photobleaching, causing difficulties to segment these small cells.</p><disp-quote content-type="editor-comment"><p>5) Related to usability: A key claim of this paper is that the authors' package is easy to use. Indeed, I applaud the use of github and freely available software like Tensorflow. This claim is generally supported by 1) the fact that the work runs on a desktop (favorably comparing to other trackers, e.g., which require a cluster [3]) and 2) the multiple example systems and optical systems tested on.</p></disp-quote><p>To clarify the point (2), we have added information about the microscope system to panel A of Figures 3, 6, 7, and 8.</p><disp-quote content-type="editor-comment"><p>However, there are several key concerns:</p><p>a) Basic automatic conda environment creation should be supported; see the.yaml files in e.g. DeepLabCut and the associated tutorial</p><p>https://github.com/DeepLabCut/DeepLabCut/blob/master/conda-environments/DLC-CPU.yaml</p></disp-quote><p>Thank you for the suggestion. We have added the “3DCT.yml” file and the related instructions in the “README.md” file in our “3DeeCellTracker” repository in GitHub to help users to create the same conda environment that we have used.</p><disp-quote content-type="editor-comment"><p>b) Why is Fiji a dependency? Although I personally love Fiji, simple rigid alignment can be performed in python and would remove a fragile dependency</p></disp-quote><p>The rigid alignment is not the core of our study. We only applied the alignment in worm #1 and #2 but not in other datasets. This step is not always required, and we did not include this step into our program. If users would prefer, this step can be easily replaced with a function written in Python. (Materials and methods)</p><disp-quote content-type="editor-comment"><p>c) What implementation of PR-GLS is being used? The original publication references a MATLAB implementation: https://github.com/jiayi-ma/PR-GLS. However, the authors are using entirely Python… did the authors rewrite the original algorithm? Or is there another implementation not associated with the paper? This should be clarified</p></disp-quote><p>We wrote the algorithm by ourselves with Python (see the “pr_gls” function in our “3DeeCellTracker” repository in GitHub) because the implementation had not yet been released when we were building our pipeline.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>1) The work uses way too few sets of validation data. Altogether, it only used three sets of <italic>C. elegans</italic> wholebrain imaging data and a single set of zebrafish heart imaging data. Drawing general conclusions from this on the algorithm's performance (especially comparisons with other algorithms) is premature.</p></disp-quote><p>In the revised version, we analyzed a dataset of a freely moving worm as suggested by reviewer #1 (Figure 6). In addition, we performed the registration of ~900 tumor cells in a 3D culture and measured ERK (=MAPK) activities, which are related to broader applications in biomedical research (Figure 8). In both of these cases, our pipeline achieved ≥ 97% of tracking. Additional zebrafish data was unfortunately not obtained under the current difficult quarantine conditions.</p><disp-quote content-type="editor-comment"><p>2) In the comparisons with other methods, while the intention was good, the results are anecdotal.</p></disp-quote><p>We appreciate the comment. We have added tracking accuracy curves for quantifying the comparison results (Figure 11—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>Testing the algorithms is done on a very limiting set of data; whether these datasets represent the breadth of realistic problems that would be encountered for the variety of applications (e.g. neuroscience, or other physiological or developmental functional imaging, in <italic>C. elegans</italic> or zebrafish or other model systems) is hard to tell.</p></disp-quote><p>As noted above, we have added the analyses of a freely moving worm (Figure 6) and the ~900 tumor cells (Figure 8) throughout the manuscript. We also toned down the discussions on diverse applications.</p><disp-quote content-type="editor-comment"><p>Also quite importantly, it is difficult to say whether the optimal choice of parameters were used for the competing methods.</p></disp-quote><p>Prior to the initial submission, we had contacted the corresponding authors of the two methods, and optimized the parameters as much as possible according to their suggestions. However, it is still possible that their method could be further improved by applying unused parameter sets; we describe this possibility in the text. We have also modified the corresponding sentences to be more objective.</p><disp-quote content-type="editor-comment"><p>3) Noises added to the real data (as in Figure 6) aren't necessarily representative of the real data's properties/features. There should be characterizations of the variability in real data on size, intensity, shape, etc. Further, it does seem that the performance of the algorithm is sensitive to these noises, and thus it is not trivial to prove that the algorithm is robust to these noises. The manuscript did not convince me that the algorithm currently can indeed deal with real noises in real datasets.</p></disp-quote><p>We appreciate the comment. In this revision, we have described the overall distributions and time course changes in the cell/background intensities and the relative movements of cells (panel B in Figures 3, 6, 7, and 8) as well as the cell diameters, intensity and cell speeds (panel A in the same figures), all of which are from real datasets.</p><p>The background signal is particularly high in the zebrafish dataset (Figure 7B): even in the larger and higher-intensity cells, a portion of the cell signals overlapped with the background. If cell signal overlaps with background signal, even researchers will not be able to properly segment the cell, thus this is a very challenging condition. We achieved 87% tracking of the large and high intensity cells in all the 1000 volumes (Results). Moreover, in the first 100 volumes, our method tracked 86% of all the cells detected, which was considerably better than a previous method did (Toyoshima et al., 2016; Figure 11—figure supplement 1B and Table 4). It should be noted that the high background signal was caused by the extremely high speed image acquisition rate of the SCAPE system (10,568 fps); in all the other conditions on real datasets (≤ 100-200 fps), the cell signals were well separated from the background signals, and our method demonstrated ≥ 97% tracking efficiency.</p><disp-quote content-type="editor-comment"><p>4) &quot;To solve the difficulty in preparing training data for FFN, we used artificial training data by simulating cell movements in deforming organs.&quot; Why is this good? How is the artificial data representing real organ deformation?</p></disp-quote><p>As mentioned in the response to the question 2b of reviewer #1, deep learning had not been previously used for 3D cell tracking because of the difficulty in preparing large amounts of training data. Therefore, we needed to develop a new technique to solve this problem, and decided to prepare artificial training data by simulating real cell movement. We used affine transformation to simulate the global coherent motions of cells (neighboring cells with similar movements), and added independent random movements to each cell to simulate local incoherent motions of cells (Figure 2—figure supplement 2). As a result, our method (FFN + PR-GLS) exhibited superior tracking results to the previous method (FPFH + PR-GLS) and to the affine alignment (Figure 2—figure supplement 3). Thus we consider that this result indicates that the use of artificially generated datasets for training is an effective method of employing deep network for cell tracking. (Results, Discussion and Materials and methods).</p><disp-quote content-type="editor-comment"><p>5) Results, comparing other datasets – the comparisons are very qualitative. Validation is vague, descriptive, rather than quantitative.</p></disp-quote><p>We apologize for the inappropriate description in the original manuscript. The tracking accuracy of worm #2 was also 100%. We have also added tracking accuracy curves for all the datasets (panel E in Figures 3 (and its figure supplements), 6 and 7; panel D in Figure 8; and panels D and J in Figure 9). Validation was based on visual inspection according to previous studies (for example Nguyen et al., 2017). We have added the details of our visual inspection in Figure 1—figure supplement 2 and in the Materials and methods.</p><disp-quote content-type="editor-comment"><p>6) Several places of the manuscript are also anecdotal, e.g. Results (corrupt data, still get 53% correct). Even the in silico studies (e.g. noise or reduced sampling) aren't systematic.</p></disp-quote><p>As mentioned above, we have added tracking accuracy curves for all datasets. We would like to note that this particular experiment (the zebrafish data on a reduced z-axis) was performed according to a suggestion by one of the reviewers for a previous submission to a different journal. We also described the reasons for the degenerate dataset analysis (Results).</p><disp-quote content-type="editor-comment"><p>7) 60-80% of zebrafish heart cells tracked – is it useful? Which ones are the missing cells? The qualitative conclusions seem to be fairly trivial.</p></disp-quote><p>We consider 60-80% of cell tracking is still informative because the tracked cells allow analysis of the relationships between calcium dynamics and the natural heartbeats in vivo (Figure 7H and I). This has finally become available due to the developments of a state-of-the-art SCAPE 2.0 system that can monitor 100 volumes per second, and of our software pipeline that can correctly track a large portion of these cell movements in 3D space (Results). Also, we have added the spatial localizations of the cells with correct/incorrect tracking (panel F in Figure 6 and 7; panel E in Figure 8; and panels E and K in Figure 9) for the datasets that did not have 100% accurate tracking.</p><disp-quote content-type="editor-comment"><p>The manuscript made a few assertions without backing up with data/results. For example, the problems identified are real (Results), but no specifics to back up, and no systematic characterizations are done, using real data, comparing the work with other approaches. Another example – &quot;our method became robust to tracking errors in the initial matching&quot;.</p></disp-quote><p>We appreciate the comment. For the corresponding sentences, we have clarified our solutions to the problems and described quantitative comparisons with the previous methods: namely, we changed the sentence (Results) and added quantitative descriptions and detailed explanations (Figure 2—figure supplements 3 and 4).</p></body></sub-article></article>