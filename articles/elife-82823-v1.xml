<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82823</article-id><article-id pub-id-type="doi">10.7554/eLife.82823</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Quantifying decision-making in dynamic, continuously evolving environments</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-291873"><name><surname>Ruesseler</surname><given-names>Maria</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-291874"><name><surname>Weber</surname><given-names>Lilian Aline</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9727-9623</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-336740"><name><surname>Marshall</surname><given-names>Tom Rhys</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-103450"><name><surname>O'Reilly</surname><given-names>Jill</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-23646"><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8393-8533</contrib-id><email>laurence.hunt@psy.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford, Oxford Centre for Human Brain Activity (OHBA) University Department of Psychiatry Warneford Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Experimental Psychology, University of Oxford, Anna Watts Building, Radcliffe Observatory Quarter</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>Centre for Human Brain Health, University of Birmingham</institution></institution-wrap><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82823</elocation-id><history><date date-type="received" iso-8601-date="2022-08-18"><day>18</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-10-13"><day>13</day><month>10</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-08-19"><day>19</day><month>08</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.18.504278"/></event></pub-history><permissions><copyright-statement>© 2023, Ruesseler, Weber et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ruesseler, Weber et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82823-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82823-figures-v1.pdf"/><abstract><p>During perceptual decision-making tasks, centroparietal electroencephalographic (EEG) potentials report an evidence accumulation-to-bound process that is time locked to trial onset. However, decisions in real-world environments are rarely confined to discrete trials; they instead unfold continuously, with accumulation of time-varying evidence being recency-weighted towards its immediate past. The neural mechanisms supporting recency-weighted continuous decision-making remain unclear. Here, we use a novel continuous task design to study how the centroparietal positivity (CPP) adapts to different environments that place different constraints on evidence accumulation. We show that adaptations in evidence weighting to these different environments are reflected in changes in the CPP. The CPP becomes more sensitive to fluctuations in sensory evidence when large shifts in evidence are less frequent, and the potential is primarily sensitive to fluctuations in decision-relevant (not decision-irrelevant) sensory input. A complementary triphasic component over occipito-parietal cortex encodes the sum of recently accumulated sensory evidence, and its magnitude covaries with parameters describing how different individuals integrate sensory evidence over time. A computational model based on leaky evidence accumulation suggests that these findings can be accounted for by a shift in decision threshold between different environments, which is also reflected in the magnitude of pre-decision EEG activity. Our findings reveal how adaptations in EEG responses reflect flexibility in evidence accumulation to the statistics of dynamic sensory environments.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision-making</kwd><kwd>temporal response function</kwd><kwd>EEG</kwd><kwd>random dot kinetogram</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>109064/Z/15/Z</award-id><principal-award-recipient><name><surname>Ruesseler</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/L019639/1</award-id><principal-award-recipient><name><surname>O'Reilly</surname><given-names>Jill</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/T031344/1</award-id><principal-award-recipient><name><surname>O'Reilly</surname><given-names>Jill</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>208789/Z/17/Z</award-id><principal-award-recipient><name><surname>Weber</surname><given-names>Lilian Aline</given-names></name><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>203139/Z/16/Z</award-id><principal-award-recipient><name><surname>Ruesseler</surname><given-names>Maria</given-names></name><name><surname>Weber</surname><given-names>Lilian Aline</given-names></name><name><surname>Marshall</surname><given-names>Tom Rhys</given-names></name><name><surname>O'Reilly</surname><given-names>Jill</given-names></name><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Human behaviour in a continuous decision making task adapts to the overall statistics of the sensory environment, and these adaptations are also reflected in changes in neural responses to incoming sensory evidence.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Unlike in most experiments, the choices that we make in daily life rarely occur in discrete trials. Naturalistic decisions instead arise organically and continuously in dynamic environments that evolve over time (<xref ref-type="bibr" rid="bib29">Huk et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Hunt et al., 2021</xref>), leading to uncertainty regarding the onset of decision-relevant changes in the environment (<xref ref-type="bibr" rid="bib56">Orsolic et al., 2021</xref>; <xref ref-type="bibr" rid="bib64">Shinn et al., 2022</xref>). When making decisions in such dynamic environments, more recently presented evidence should therefore usually be given greater weight and more historical evidence gradually discounted – a strategy known as ‘leaky’ evidence accumulation. Optimising leaky evidence accumulation involves adapting one’s behaviour to the overall statistics of the environment. This can be achieved by changing the rate at which previous evidence is leaked (the ‘decay’) and the amount of cumulative evidence required before a categorical decision is made (the ‘decision threshold’) (<xref ref-type="bibr" rid="bib21">Glaze et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Kilpatrick et al., 2019</xref>; <xref ref-type="bibr" rid="bib70">Veliz-Cuba et al., 2016</xref>).</p><p>While it is known that humans (<xref ref-type="bibr" rid="bib19">Ganupuru et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Glaze et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Harun et al., 2020</xref>; <xref ref-type="bibr" rid="bib57">Ossmy et al., 2013</xref>) and other animals (<xref ref-type="bibr" rid="bib44">Levi et al., 2018</xref>; <xref ref-type="bibr" rid="bib59">Piet et al., 2018</xref>) can adapt the decay and decision threshold of sensory evidence accumulation to different dynamic environments, the neural mechanisms that underlie this adaptation remain unclear. In conventional trial-based paradigms, neurophysiological correlates of perceptual decision-making have been well characterised, particularly signals that resemble an evidence accumulation-to-bound process (<xref ref-type="bibr" rid="bib23">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib25">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="bib54">O’Connell and Kelly, 2021</xref>). Two of the best-studied human electroencephalographic (EEG) correlates of decision formation are an effector-independent centroparietal positivity (CPP) that shows accumulator-like dynamics during decision formation (<xref ref-type="bibr" rid="bib35">Kelly and O’Connell, 2013</xref>; <xref ref-type="bibr" rid="bib53">O’Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib60">Pisauro et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Twomey et al., 2015</xref>), and motor preparation signals that emerge prior to a response (<xref ref-type="bibr" rid="bib16">Donner et al., 2009</xref>; <xref ref-type="bibr" rid="bib67">Steinemann et al., 2018</xref>; <xref ref-type="bibr" rid="bib71">Wyart et al., 2012</xref>). Both signals can adapt their properties according to the overall statistics of a task. For example, CPP amplitude at the time of making a response is increased by emphasising speed over accuracy (<xref ref-type="bibr" rid="bib67">Steinemann et al., 2018</xref>), while pre-trial motor lateralisation can reflect the prior expectation of a leftward or rightward action in the upcoming trial (<xref ref-type="bibr" rid="bib13">de Lange et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Kelly et al., 2021</xref>). Yet it remains unclear whether and how these signals reflect the ability to behaviourally adapt the decay of past information in continuous (dynamic) environments, which are more akin to many decisions faced in naturalistic settings.</p><p>Perhaps one reason why decision-making in dynamic environments has been less studied than trial-based choice is the uncertainty concerning how best to analyse the time-varying neural data. For example, without clearly defined discrete trials, it appears unclear to which timepoint data should be epoched. As the stimulus is continuously changing, it is also ambiguous how to disentangle responses to previous versus current sensory evidence, which may be overlapping in time. However, recent innovations in trial-based task design (<xref ref-type="bibr" rid="bib8">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cheadle et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Orsolic et al., 2021</xref>) and unmixing of overlapping EEG responses (<xref ref-type="bibr" rid="bib12">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Ehinger and Dimigen, 2019</xref>; <xref ref-type="bibr" rid="bib27">Hassall et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Smith and Kutas, 2015</xref>) have suggested potential solutions to some of these challenges. By tightly controlling how sensory evidence fluctuates over time, it becomes possible to relate moment-to-moment stimulus fluctuations to subsequent behavioural and neural responses. In addition, by using data analysis techniques that explicitly target overlapping neural responses, it is also possible to establish the temporal response function (TRF) to each new fluctuation in a continuous sensory evidence stream (<xref ref-type="bibr" rid="bib24">Gonçalves et al., 2014</xref>). By combining these two approaches, we hypothesised that we would be able to characterise decision-related EEG responses in a continuous and dynamic setting, even in the absence of repeated experimental trials.</p><p>In this study, we examine how the EEG response to evidence fluctuations during a continuous perceptual decision task is affected by the overall statistics of the sensory environment. Participants were trained to attend to a continuously changing sensory evidence stream, in which brief ‘response periods’ were embedded that were reported via buttonpress. We demonstrate a CPP-like potential that is sensitive to each fluctuation in the stream of continuous sensory evidence, a motor preparation signal prior to buttonpress, and a triphasic occipito-parietal component that reflects the integrated sum of recently presented evidence. We then show that subjects’ behaviour adapts appropriately to different sensory environments, and that changes in centroparietal and motor preparation prior to buttonpress signals reflect adaptation of leaky sensory evidence integration to different environments. This is not a simple feature of adaptation to sensory surprise as the CPP-like potential largely responds to decision-relevant, not decision-irrelevant, evidence fluctuations.</p><p>We also show substantial between-subject variability in the decay time constant of the ‘integration kernel’, a measure that reflects the structure of evidence that is presented prior to a participant’s response. This behavioural measure correlates across subjects with a neural measure of evidence accumulation: it predicts the amplitude of the triphasic centroparietal TRF to absolute recent sensory evidence. We show via computational modelling that these changes in integration kernels are most likely explained via a change in the decision threshold of a leaky evidence accumulator. Collectively, these results provide a neural characterisation of human decision-making in a dynamic, continuously evolving perceptual environment and how this can adapt to the overall statistics of the environment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A novel task for exploring behavioural and EEG adaptations to the statistics of dynamic sensory environments</title><p>To study evidence accumulation in a continuous setting, we designed a novel variant of the classic random dot kinematogram (RDK) paradigm (<xref ref-type="bibr" rid="bib7">Britten et al., 1992</xref>; <xref ref-type="bibr" rid="bib16">Donner et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">Kelly and O’Connell, 2013</xref>; <xref ref-type="bibr" rid="bib52">Newsome and Paré, 1988</xref>). Subjects continuously monitored a stream of time-varying sensory evidence (hereafter referred to as ‘motion coherence’) for blocks of 5 min (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). During extended ‘baseline periods’ (grey shaded area in <xref ref-type="fig" rid="fig1">Figure 1b</xref>), the average level of motion coherence (black line) in the stimulus was zero, whereas during shorter intermittent ‘response periods’ (green shaded area), the mean level of motion coherence became non-zero (either 30, 40, or 50% motion coherence). The participants’ task was to report whenever they detected such a period of coherent motion using a left or right buttonpress. Importantly, the onset of response periods was not explicitly signalled to the participant. If they responded accurately (during a response period or within 500 ms of it ending), they received a reward (+3 points); if they failed to report a response period (‘missed response period’), or they responded during a baseline period (‘false alarm’), they received a small punishment (–1.5 points). Participants also received a larger punishment (–3 points) if they reported the incorrect motion direction during a response period; in practice, such errors were very rare. Feedback was presented by changing the colour of the central fixation point for 500 ms (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), and they were trained on the meaning of these colours as part of extensive pre-experiment training (see ‘Methods’). The accumulated total points were then converted into a monetary pay-out at the end of the task. Participants completed six runs, each consisting of four 5 min blocks; they were given a short break between each block and a longer break between runs.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>A novel, continuous version of the random dot kinematogram (RDK) paradigm allows empirical measurement of participants’ leaky evidence integration kernels in dynamic environments.</title><p>(<bold>a</bold>) Task design. Participants continuously attend to a centrally presented RDK stimulus, for 5 min at a time. They aim to successfully report motion direction during ‘response periods’ (when coherent motion signal is on average non-zero) and withhold responding during ‘baseline periods’ (when signal is on average zero). (<bold>b</bold>) Task structure (example block; response periods are ‘rare’). During both baseline (grey) and response periods (green), the signal (black line) is corrupted with experimenter-controlled noise (grey line). The noise fluctuations that precede each response (arrows) can be averaged to obtain the evidence integration kernel. (<bold>c</bold>) The resulting evidence integration kernel for false alarms is well described by an exponential decay function, whose decay time constant in seconds is controlled by the free parameter <inline-formula><mml:math id="inf1"><mml:mi>τ</mml:mi></mml:math></inline-formula>. The equation for this kernel is in the main text, and details of kernel fitting are provided in ‘Methods’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig1-v1.tif"/></fig><p>Crucially, the net motion presented to the participant on each frame of the stimulus was not the <italic>average</italic> level of motion coherence (black line in <xref ref-type="fig" rid="fig1">Figure 1b</xref>), but instead was a noisy sample from a Gaussian distribution about this mean (grey line). This noisy sample was resampled on average every 280 ms (inter-sample interval drawn from an exponential distribution, truncated at 1000 ms). This ‘experimenter-controlled sensory noise’ confers several benefits.</p><p>First, the injection of sensory noise places a stronger demand on temporal evidence integration than a classical RDK task. This is because any individual period of strong motion coherence could be driven by a noisy sample during a baseline period, rather than necessarily signalling the onset of a response period (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). As such, continuous and temporally extended integration is essential to successfully disambiguate changes in the mean from noisy samples around the baseline. Indeed, participants would occasionally make ‘false alarms’ during baseline periods in which the structure of the preceding noise stream mistakenly convinced them they were in a response period (see Figure 3, below). Indeed, this means that a ‘false alarm’ in our paradigm has a slightly different meaning than in most psychophysics experiments; rather than it referring to participants responding when a stimulus was <italic>not present</italic>, we use the term to refer to participants responding when there was no shift in the <italic>mean signal</italic> from baseline.</p><p>Second, the noise fluctuations allow a ‘reverse correlation’ approach to studying subjects’ evidence integration. Simply by averaging the noisy stimulus that was presented prior to each response, we could extract an ‘integration kernel’ that empirically reveals how far back in time the motion coherence is being integrated – in other words, how quickly previous motion is decaying in the participant’s mind – and how strong this motion coherence needed to be on average to support a choice. We performed this reverse correlation for both false alarm responses (example shown in <xref ref-type="fig" rid="fig1">Figure 1c</xref>; these responses are well described by an exponential decay function detailed below) and correct responses. The fact that integration kernels naturally arise from false alarms, in the same manner as from correct responses, demonstrates that false alarms were not due to motor noise or other spurious causes. Instead, false alarms were driven by participants treating noise fluctuations during baseline periods as sensory evidence to be integrated across time, and the motion coherence that preceded ‘false alarms’ need not even distinguish targets from non-targets.</p><p>Finally, and perhaps most importantly, the experimenter-controlled sensory noise allows us to characterise how continuous sensory evidence fluctuations cause changes in the simultaneously recorded continuous EEG signal. To study this, we used a deconvolutional general linear model (GLM) approach (<xref ref-type="bibr" rid="bib12">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Ehinger and Dimigen, 2019</xref>; <xref ref-type="bibr" rid="bib24">Gonçalves et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Hassall et al., 2021</xref>) to estimate TRFs to various events relating to the time-varying sensory evidence. We describe this approach and the resulting TRFs in more detail below.</p></sec><sec id="s2-2"><title>Behavioural adaptations to environments with different statistical properties</title><p>We used this paradigm to investigate whether and how participants adapted their evidence integration behaviour to the overall statistics of the sensory environment. To test this, we manipulated both the duration and frequency of ‘response periods’ in the task. We hypothesised that this would affect the decay of past sensory evidence and/or the decision threshold used to commit to a response. Importantly for our subsequent analyses, we kept the generative statistics of the Gaussian noise during ‘baseline periods’ consistent across conditions. This allowed us to directly compare behavioural evidence integration kernels for false alarms and EEG TRFs across conditions without any potential confound from how the noise was structured.</p><p>Within each 20 min run, participants completed four pseudorandomly ordered 5 min blocks drawn from a 2 * 2 factorial design (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Response periods were either LONG (5 s) or SHORT (3 s), and either FREQUENT (baseline periods between 3 and 8 s in duration) or RARE (baseline periods between 5 and 40 s). Participants were extensively trained on these trial statistics prior to completing the task and were then explicitly cued which environment they were currently in. As a consequence, participants neither had to learn nor infer the higher-order statistics of the sensory environment during the task; instead, they had to adapt their decision behaviour according to the pre-learnt statistics of the cued environment.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Variations in response period structure across different environments elicit behavioural adaptations in decision-making.</title><p>(<bold>a</bold>) Structure of response periods (signal only, before noise was added to the stimulus stream) across the different environments. This was manipulated in a 2 * 2 design, where response periods were either FREQUENT or RARE, and LONG or SHORT. Participants were extensively trained on these statistics prior to the task, and the current environment was explicitly cued to the participant. (<bold>b</bold>) Correct detection rate for all response periods. Participants successfully detected more response periods when they were LONG than SHORT (as would be expected, because the response period is longer), but also detected more when they were FREQUENT than RARE. (<bold>c</bold>) Median reaction time (time taken to respond after start of response period) for successfully reported response periods across the four conditions. Participants took longer in RARE versus FREQUENT conditions, and in LONG versus SHORT conditions. (<bold>d</bold>) Integration kernels for ‘response periods’ shows a main effect of FREQUENT versus RARE response periods, but unexpectedly no effect of LONG versus SHORT response periods. See main text for further discussion of this analysis. All plots in (<bold>b–d</bold>) show mean ± s.e. across 24 participants. Note that to make reaction times and integration kernels comparable between the four conditions, we only include those responses that were shorter than 3.5 s in analyses for (<bold>c</bold>) and (<bold>d</bold>) (i.e. the maximum response time in SHORT response periods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Logistic mixed model of subjects choices during response periods, with regressors of mean motion coherence (avgCoh), variance of motion coherence (cohVar), response period Frequency (trlFrq), response period length (trlLen), and interaction terms between these regressors.</title><p>The results indicate that both the mean motion coherence and variance of the motion coherence influenced participants’ choice, suggesting that participants used a strategy of detecting shifts in both mean and variance to detect response periods. * denotes p&lt;0.05 significant effect across participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig2-figsupp1-v1.tif"/></fig></fig-group><sec id="s2-2-1"><title>Response periods</title><p>We first tested whether participants adjusted their behaviour across the four conditions by analysing detection behaviour for response periods. We used a three-way repeated-measures ANOVA to test for effects of motion coherence, response period length, and response period frequency.</p><p>As expected, participants were faster (F(2,46) = 29.51, p=7.45 * 10<sup>–9</sup>) and more accurate (F(2,46) = 17.6, p=0.00035) at detecting response periods with a higher level of average motion coherence (<xref ref-type="fig" rid="fig2">Figure 2b and c</xref>).</p><p>Examining the effects of response period frequency, participants were less likely (F(1,23) = 41.99, p=1.30 * 10<sup>–6</sup>) and slower (F(1,23) = 83.24, p=6.23 * 10<sup>–9</sup>) to detect response periods when these were RARE (blue lines in <xref ref-type="fig" rid="fig2">Figure 2b and c</xref>) than when they were FREQUENT (red lines in <xref ref-type="fig" rid="fig2">Figure 2b and c</xref>). We hypothesised that this could be explained by participants requiring a stronger overall level of recently accumulated evidence before committing to a response when response periods were rare. Indeed, when we examined the ‘integration kernel’ of average sensory evidence for successful responses, significantly more cumulative evidence was required for RARE than FREQUENT response periods, stretching back up to 3.5 s prior to the commitment to a response (<xref ref-type="fig" rid="fig2">Figure 2d</xref>; <italic>F</italic>-test with permutation-based correction for multiple comparisons, p&lt;0.05).</p><p>We also found that participants detected response periods more frequently when these were LONG than when they were SHORT (<xref ref-type="fig" rid="fig2">Figure 2b</xref>; F(1,23) = 178.52, p=2.51 * 10<sup>–12</sup>). This is unsurprising as there was simply more time to detect the change in motion during these response periods. In fact, participants were slightly more conservative in their responding when trials were LONG, as shown by a longer average reaction time for LONG response periods relative to SHORT (even when restricting this analysis to focus on LONG responses that were less than 3.5 s, the maximum possible SHORT response duration including 500 ms response tolerance period; <xref ref-type="fig" rid="fig2">Figure 2c</xref>; F(1,23) = 70.00, p=2.78 * 10<sup>–8</sup>). This was also reflected in their false alarm frequency, as shown below. Surprisingly, the sensory evidence integration properties (i.e. the ‘integration kernels’, calculated by averaging the signal prior to the decision, collapsing across all levels of mean motion coherence) were <italic>not</italic> affected by the length of response periods (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). This ran contrary to our initial hypothesis that participants would integrate evidence for longer when response periods were LONG. We suggest that this may result from the manipulation of response period duration being relatively small (3 s versus 5 s) compared to the manipulation of response period frequency. We also note that the significant difference between FREQUENT and RARE trials in <xref ref-type="fig" rid="fig2">Figure 2d</xref> should not be over-interpreted as it could be influenced by RT differences (<xref ref-type="fig" rid="fig2">Figure 2c</xref>) and the associated shift in the onset of the signal contribution and/or the difference in average coherence detection across conditions (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Importantly, we control for these confounds below by examining the integration kernels to false alarms (in the absence of changes in mean signal).</p><p>We also considered an alternative stimulus detection strategy of changes in stimulus variance across time rather than changes in stimulus mean. This hypothesis relied upon the fact that response periods had smaller standard deviations in the Gaussian noise distribution than baseline periods – a stimulus feature that we introduced to avoid excessive samples of ‘maximal’ (100%) motion coherence when the mean was non-zero. To test whether the variance of the stimulus might also affect participants’ detection, we performed a logistic mixed effects model on participants’ responses (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Detection probability was the dependent variable, and mean motion coherence, variance of motion coherence, response period frequency, and length were independent variables, along with interaction terms. We found that stimulus variance during response periods did indeed impact detection probability; response periods with a higher variance in motion coherence were less likely to be detected. Crucially, however, the main effects of mean motion coherence, trial frequency, and trial length (equivalent to the effects plotted in main <xref ref-type="fig" rid="fig2">Figure 2b</xref>) were left unaffected by the inclusion of this coregressor.</p></sec><sec id="s2-2-2"><title>False alarms</title><p>We then examined whether false alarms differed across conditions, testing for effects of response period frequency and length on false alarm rate using a two-way repeated-measures ANOVA. We found that despite the structure of the noise stream being identical across the four conditions, there was a lower overall frequency of false alarms in LONG versus SHORT conditions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>; F(1,23) = 58.67, p=8.98 * 10<sup>–8</sup>). This provides further evidence that participants were overall more conservative in their responses in LONG conditions than SHORT. (In other words, for an equivalent level of sensory evidence, the participants were less likely to make a response.) There was no effect of response period frequency on false alarm rate (F(1,23) = 0.37, p=0.55).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Changes in false alarm response frequency and evidence integration kernels across environments with different statistical structure.</title><p>(<bold>a</bold>) False alarm rates (responses during baseline periods) showed a main effect of response period duration – participants showed significantly lower false alarm rates when response periods were LONG versus SHORT (F(1,23) = 58.67, p=8.98 * 10<sup>–8</sup>). This is consistent with having a more cautious response threshold (also evidenced by longer reaction times during response periods, see <xref ref-type="fig" rid="fig2">Figure 2c</xref>), although it could also be interpreted as shorter response periods inducing more confusion between signal and noise. (<bold>b</bold>) Integration kernels calculated for false alarms across the four conditions. Lines show mean +/- s.e. across 24 participants. (<bold>c</bold>) Exponential decay model fitted to individual participants’ kernels during false alarms shows a significantly longer decay time constant when response periods were RARE versus FREQUENT. The data points show the time constant, <inline-formula><mml:math id="inf2"><mml:mi>τ</mml:mi></mml:math></inline-formula>, for each participant after fitting a model of exponential decay to the integration kernel. The equation for this kernel is in the main text, and details of kernel fitting are provided in ‘Methods’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Between-subject variability in evidence integration kernels exceeds between-condition variability.</title><p>(<bold>a</bold>) Left panel: unlike for RARE versus FREQUENT (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), there was no significant difference in decay time constant for integration kernels for LONG versus SHORT. Similarly, there was no difference in amplitude parameter A for either of these comparisons (middle/right panels). (<bold>b</bold>) Variability across individuals, and comparative consistency across conditions, of integration kernels. This figure shows integration kernels from three example subjects in the experiment across the four conditions. Although our experiment primarily aimed to test whether integration kernels would be adapted to different environmental statistics (columns of figure), we found (unexpectedly) that different individuals had very different integration kernels (rows); some would integrate evidence over longer durations (e.g. top row), and others over far shorter durations (e.g. bottom row). All analyses are from false alarm responses only.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig3-figsupp1-v1.tif"/></fig></fig-group><p>We then examined what <italic>caused</italic> participants to false alarm during baseline periods. Were participants still integrating evidence continuously during these periods of the task, or might false alarms be driven by other spurious factors, such as motor noise? We tested this by calculating integration kernels derived from these responses. We found the recovered evidence integration kernels showed exponential decay weighting, implying that participants were indeed performing continuous evidence integration throughout baseline periods as well as response periods, and that evidence accumulation was more temporally extended when response periods were RARE rather than FREQUENT (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). The slight differences in integration kernels between <xref ref-type="fig" rid="fig3">Figure 3b</xref> and <xref ref-type="fig" rid="fig2">Figure 2d</xref> (shorter duration, and return to baseline close to the response) are due to the inclusion of the average motion signal in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, rather than just the noise.</p></sec><sec id="s2-2-3"><title>Between-participant variation in evidence integration</title><p>We then sought to characterise the time constant of leaky evidence integration within each individual participant. To do this, we fit an exponential decay model (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) to the empirical integration kernel from false alarm responses:<disp-formula id="equ1"> <mml:math id="m1"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <italic>k</italic>(<italic>t</italic>) is the height of the integration kernel <italic>t</italic> seconds before its peak; <italic>A</italic> is the peak amplitude of the integration kernel (in units that denote the fraction of dots moving towards the chosen response direction); and <inline-formula><mml:math id="inf3"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the decay time constant (in units of seconds). We note that this exponential decay model is theoretically motivated by the leaky evidence accumulation model, which implies that past evidence will leak from the accumulator with an exponential decay (<xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>).</p><p>Our exponential decay model provided a good fit to data at a single-subject level (median R<sup>2</sup> = 0.82, 95% confidence intervals for R<sup>2</sup> = [0.42,0.93]; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for example fits), as demonstrated by the strong reliability across conditions for both <italic>A</italic> and <inline-formula><mml:math id="inf4"><mml:mi>τ</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>; Pearson’s correlation between SHORT and LONG conditions: <inline-formula><mml:math id="inf5"><mml:mi>τ</mml:mi></mml:math></inline-formula>: R(23) = 0.71; A: R(23) = 0.72; Pearson’s correlation between RARE and FREQUENT conditions: <inline-formula><mml:math id="inf6"><mml:mi>τ</mml:mi></mml:math></inline-formula>: R(23) = 0.87; <italic>A</italic>: R(23) = 0.81; all p&lt;0.0001). Indeed, a striking feature of these integration kernels was that variation across <italic>individuals</italic> exceeded the variation observed across <italic>conditions</italic> (e.g. see <xref ref-type="fig" rid="fig3">Figure 3c</xref>).</p><p>Consistent with our earlier analyses (<xref ref-type="fig" rid="fig2">Figu<xref ref-type="fig" rid="fig2">2</xref>,<xref ref-type="fig" rid="fig3">3</xref>res 2d and 3b</xref>), we found that by fitting this single-subject model, <inline-formula><mml:math id="inf7"><mml:mi>τ</mml:mi></mml:math></inline-formula> was significantly longer when response periods were RARE than FREQUENT (paired T(23) = 3.62, p=0.0014; <xref ref-type="fig" rid="fig3">Figure 3c</xref>) but <italic>A</italic> did not differ between these conditions (paired T(23) = 0.03, p=0.97; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>). Again consistent with our analyses of behaviour during response periods (<xref ref-type="fig" rid="fig2">Figure 2d</xref>), there was no difference between these parameters for LONG versus SHORT response periods (<inline-formula><mml:math id="inf8"><mml:mi>τ</mml:mi></mml:math></inline-formula>: paired T(23) = 0.82, p=0.42; <italic>A</italic>: paired T(23) = -0.97, p=0.34; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>).</p><p>In summary, these results indicate that participants adapted to response periods being rarer by accumulating sensory evidence with a longer time constant of integration, but that there was also substantial between-subject variability in evidence accumulation across participants.</p></sec></sec><sec id="s2-3"><title>Computational modelling of leaky evidence accumulation</title><p>We next considered what adjustments within a computational model of leaky evidence accumulation might account for the behavioural adaptation across different environments, and the variability across participants. We simulated a well-established model of leaky evidence accumulation, the Ornstein–Uhlenbeck process (<xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib8">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Ossmy et al., 2013</xref>). Here, evidence is accumulated over time according to<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where is a parameter that (when constrained to be negative) determines the <italic>leak</italic> of past sensory evidence in the decision variable; <inline-formula><mml:math id="inf9"><mml:mi>g</mml:mi></mml:math></inline-formula> is a parameter that determines the <italic>gain</italic> applied to the momentary sensory evidence at each timepoint <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ; and <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow/><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes Gaussian-distributed white noise with mean 0 and variance σ. The model emits a response every time that a decision <italic>threshold</italic> ±<italic>θ</italic> is exceeded, at which point <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is reset to 0. Note that if were set to 0 rather than negative, this model would be equivalent to the widely used Drift Diffusion Model, in which previously accumulated evidence is perfectly retained in the decision variable <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Such a model would be inappropriate in the current paradigm as the structure of the task demands that past sensory evidence should gradually be discounted.</p><p>We first considered what adjustments of the model parameters governing leak and decision threshold, and <italic>θ</italic>, would lead to optimal performance in terms of points gained across the entire block (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We simulated model behaviour using a range of possible values of these parameters, while holding <inline-formula><mml:math id="inf14"><mml:mi>g</mml:mi></mml:math></inline-formula> constant and assuming σ is primarily a property of low-level sensory processing and so also remains constant. The optimal parameterisation of the Ornstein–Uhlenbeck process depended upon the number of correct responses/missed trials in response periods versus the number of false alarms during baseline periods (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). For example, setting the response threshold <italic>θ</italic> to a low value (e.g. &lt;1 in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) leads to the model correctly detecting virtually all response periods, but also emitting so many false alarms that the total points obtained would be negative. By contrast, setting the threshold slightly higher still allows for correct responses, but they now outnumber false alarms, meaning that the model accumulates points across the block.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Optimal leak and threshold for a leaky accumulator model differs as a function of task condition.</title><p>(<bold>a</bold>) We performed a grid search over the parameters and <italic>θ</italic> to evaluate the performance (points won) for different parameterisations (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The shaded area denotes the areas of model performance that lay in the top 10% of all models considered. The optimal area differs across conditions, and the optimal setting for leak and threshold co-vary with one another. (<bold>b</bold>) We used the evidence stream presented to each participant (each dot = one 5 min block), to identify the model parameterisation that would maximise total reward gained for each subject in each condition (see also <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Grid search across parameter space for Ornstein–Uhlenbeck process.</title><p>We considered a plausible range of settings for the leak and threshold parameters that might maximise the total points won (left-hand column; top 10% of models enclosed in black line; see also main <xref ref-type="fig" rid="fig4">Figure 4a</xref>). The optimal model parameterisation depended upon a trade-off between the frequency of correct responses during response periods (second column) against the number of false alarms (third column) and missed trials (fourth column). As in human behaviour, the total number of ‘incorrect responses’ made (i.e. wrong response emitted during response window) were negligible.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>The optimal model parameters for leak and threshold correlate with each other.</title><p>The four graphs show the optimal model parameterisations for each individual subjects’ sensory evidence stream, for each of the four conditions (i.e. each dot = one 5 min stream of evidence, <xref ref-type="fig" rid="fig4">Figure 4b</xref>). In all four conditions, a model that is ‘less leaky’ (i.e. is closer to 0) is typically compensated by setting the decision threshold <italic>θ</italic> to be higher to achieve optimal performance (as also seen in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig4-figsupp2-v1.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig4">Figure 4a</xref> shows the area of model performance that performs in the top 10% of all parameterisations that we considered, and <xref ref-type="fig" rid="fig4">Figure 4b</xref> shows the best parameterisations for 30 streams of evidence that were presented to participants in the task. Notably, the optimal decision threshold <italic>θ</italic> was <italic>traded off</italic> against the optimal setting for leak (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). In other words, a model in which past sensory evidence leaked more rapidly (i.e. was more negative) could be compensated by a decrease in <italic>θ</italic>, to retain a high level of overall points gained. This led to a ‘ridge’ in parameter space where a given set of values for and <italic>θ</italic> would provide high task performance. The location of this ridge differed across the four environments, implying that participants would indeed need to adapt these parameters across conditions.</p><p>We confirmed the optimal settings for and θ by presenting the actual stimulus streams that were presented to our participants, and identifying the values of these two parameters that maximised total points won (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). This demonstrated that when response periods were LONG rather than SHORT, the optimal adjustment was to reduce the amount of leak in the model, so that incoming sensory evidence persisted for longer within the decision variable. When response periods were RARE rather than FREQUENT, the model could be optimised by increasing the decision threshold. This, in turn, would make the model more conservative, consistent with the reduced detection rates and accuracy shown in <xref ref-type="fig" rid="fig2">Figure 2b and c</xref>. (We note, however, that this is slightly inconsistent with the pattern of behavioural false alarm rates shown in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. We suggest that this may be under the control of further factors such as time-varying urgency [<xref ref-type="bibr" rid="bib20">Geuzebroek et al., 2022</xref>], something we do not consider in the current model.)</p><p>If changes in <italic>θ</italic> are primarily driven by the frequency of response periods, and changes in are primarily driven by their length, then how can we explain the fact that the decay time constant <inline-formula><mml:math id="inf15"><mml:mi>τ</mml:mi></mml:math></inline-formula> of integration kernels is affected by frequency but not length? To answer this, we performed an equivalent analysis of integration kernels on our model simulations. We epoched and averaged the sensory evidence that preceded each response made by the decision model and examined the effects of and <italic>θ</italic> on the recovered integration kernels (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Surprisingly, we found that <italic>θ</italic>, not <italic>λ</italic>, was primarily responsible for the recovered decay time constant <inline-formula><mml:math id="inf16"><mml:mi>τ</mml:mi></mml:math></inline-formula>. At first sight, this appears counterintuitive because is directly responsible for the decay of past sensory evidence in the model of leaky accumulation. However, this is counteracted by the fact that the only data that enters this analysis is when the model has passed decision threshold, and a response is emitted – if the threshold is set higher, then a consistent stream of positive evidence is required before threshold will be reached, producing the effect shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. By contrast, we found that the amplitude of the integration kernel <italic>A</italic> was affected by manipulations of both <italic>θ</italic> and <italic>λ</italic>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Variation in model threshold primarily determines decay time constant of integration kernels.</title><p>We performed an integration kernel analysis on false alarms emitted by the Ornstein–Uhlenbeck process with different settings for leak (<italic>λ</italic>, left column) and threshold (<italic>θ</italic>, right column) while holding the other parameter constant. Variation in <italic>θ</italic> would invariably affect the requirement for temporally sustained evidence to emit a false alarm (a higher threshold requiring sustained evidence); variation in primarily affect the amplitude of the eventual kernel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig5-v1.tif"/></fig><p>In summary, our conclusions from the computational modelling are threefold: (i) our manipulations of response period frequency and length elicited different settings for model threshold and leak respectively to maximise reward (<xref ref-type="fig" rid="fig4">Figure 4b</xref>); (ii) the ‘ridge’ in parameter space that performed well (top 10%) for each condition showed a trade-off between threshold and leak (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), and may explain how different participants could show very different integration kernels (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) while still performing well on the task; and (iii) the between-condition and between-subject variation in integration kernel time constants <inline-formula><mml:math id="inf17"><mml:mi>τ</mml:mi></mml:math></inline-formula> is principally driven by variation in response threshold, <italic>θ</italic> (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></sec><sec id="s2-4"><title>EEG correlates of continuous sensory evidence integration</title><p>Having established behavioural differences in evidence integration across individuals and across environments with different statistical structures, we then examined how participants’ EEG responses reflected these differences. To test this, we examined the effects of the <italic>noise fluctuations</italic> on the EEG signal during baseline periods. We focussed on this time period for three reasons: (i) the generative statistics of the noise were identically matched across all four task conditions; (ii) behavioural evidence from ‘false alarms’ clearly indicated that participants were still integrating sensory evidence during baseline (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>); and (iii) the large number of noise fluctuations embedded in the stimulus (&gt;1000 per 5 min block) meant that we had many events of interest to recover EEG TRFs with a high signal-to-noise ratio (<xref ref-type="bibr" rid="bib24">Gonçalves et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Lalor et al., 2006</xref>).</p><p>We therefore built a deconvolutional GLM to estimate TRFs to various events relating to the time-varying noise fluctuations during baseline periods. In particular, this GLM included regressors that described (<xref ref-type="fig" rid="fig6">Figure 6</xref>) (a) ‘jump events’ in the experimenter-controlled noise (‘stick functions’ that were 1 whenever the motion coherence changed, and 0 elsewhere); (b) the ‘change in evidence’ associated with each jump event (stick functions with a parametric modulator of |Δevidence|, i.e. absolute difference between previous and current motion coherence); and (c) the current |evidence| (a continuous regressor, reflecting the absolute difference from 0 across time; note that this regressor is absoluted to look for effector-independent signals processing current motion strength, as opposed to those signed towards leftward/rightward motion). We also included several further regressors to capture EEG correlates of the onset of response periods, the level of motion coherence, and correct and false alarm buttonpresses (see ‘Methods’ for full details).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Deconvolutional general linear model to estimate electroencephalographic (EEG) temporal response functions to continuous, time-varying decision regressors.</title><p>The left-hand side of the figure shows an example evidence stream during the baseline period (note that inter-sample intervals are shown as fixed duration for clarity, rather than Poisson distributed as in the real experiment). Three example regressors are shown: (<bold>a</bold>) ‘jump event’, when there was a change in the noise coherence level; (<bold>b</bold>) '|Δ evidence|’, reflecting the magnitude of the jump update at each jump event; and (<bold>c</bold>) continuous |evidence|, reflecting the continuous absolute motion strength. For each of these regressors, a lagged version of the regressor timeseries is created to estimate the temporal response function (TRF) at each peri-event timepoint. This is then included in a large design matrix <italic>X</italic>, which is regressed onto continuous data <italic>Y</italic> at each sensor. This leads to a set of temporal response functions for each regressor at each sensor, shown on the right-hand side of the figure. The timecourse for each regressor shows the average regression weights at the three sensors highlighted with triangles on the scalp topography. Full details of the design matrix used in our analysis of the EEG data are provided in ‘Methods’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig6-v1.tif"/></fig><p>Using this approach, we found a set of consistent TRFs that reliably reflected the continuous updates in the time-evolving sensory evidence during baseline (<xref ref-type="fig" rid="fig6">Figure 6</xref>). In particular, the two regressors that reflected changes in the sensory evidence (‘jump events’) and the magnitude of |Δevidence| both elicited positive-going scalp topographies over centroparietal electrodes, peaking ~300 ms after this change occurred (<xref ref-type="fig" rid="fig6">Figure 6a and b</xref>). This scalp topography, timecourse, and reporting of |Δevidence| are consistent with the P300 component (<xref ref-type="bibr" rid="bib15">Donchin, 1981</xref>; <xref ref-type="bibr" rid="bib17">Duncan-Johnson and Donchin, 1977</xref>; <xref ref-type="bibr" rid="bib49">Mars et al., 2008</xref>; <xref ref-type="bibr" rid="bib66">Squires et al., 1976</xref>). The scalp topography is also consistent with the CPP (<xref ref-type="bibr" rid="bib35">Kelly and O’Connell, 2013</xref>; <xref ref-type="bibr" rid="bib53">O’Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">O’Connell and Kelly, 2021</xref>), whose ramp-to-threshold dynamics have been proposed to account for many established effects in the P300 literature (<xref ref-type="bibr" rid="bib69">Twomey et al., 2015</xref>). In addition, the continuous |evidence| regressor elicited a triphasic potential over centroparietal electrodes (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). This triphasic potential is notably similar to EEG potentials reflecting ‘decision update’ signals during trial-based tasks that require integration of multiple, discrete pieces of evidence (<xref ref-type="bibr" rid="bib71">Wyart et al., 2012</xref>).</p></sec><sec id="s2-5"><title>Increased CPP responses to Δevidence and response thresholds when response periods are rare</title><p>We then examined whether these TRFs to noise fluctuations were adapting across the different sensory environments. Given our behavioural findings concerning integration kernels (<xref ref-type="fig" rid="fig2">Figures 2d</xref> and <xref ref-type="fig" rid="fig3">3b</xref>), we reasoned that we would most likely identify differences as a function of response period frequency rather than length. Indeed, we found that the centroparietal response to the same change in sensory evidence was larger when response periods were RARE than when they were FREQUENT (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>; p=0.017, cluster-based permutation test). As large changes in mean evidence are less frequent in the RARE condition, the increased neural response to |Δevidence| may reflect the increased statistical surprise associated with the same magnitude of change in evidence in this condition. In addition, when making a correct response, preparatory motor activity over central electrodes reached a larger decision threshold for RARE versus FREQUENT response periods (<xref ref-type="fig" rid="fig7">Figure 7b</xref>; p=0.041, cluster-based permutation test). We found similar effects in beta-band desynchronisation prior, averaged over the same electrodes; beta desynchronisation was greater in RARE than FREQUENT response periods. As discussed in the computational modelling section above, this is consistent with the changes in integration kernels between these conditions as it may reflect a change in decision threshold. It is also consistent with the lower detection rates and slower reaction times when response periods are RARE (<xref ref-type="fig" rid="fig2">Figure 2b and c</xref>), which also imply a higher response threshold. By contrast, we found no statistically significant difference for either of these regressors between SHORT versus LONG response periods (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref> and <xref ref-type="fig" rid="fig7s2">2</xref>). We also found qualitatively similar results for false alarm responses.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Adaptations of electroencephalographic (EEG) responses to sensory environments where response periods are RARE versus FREQUENT.</title><p>(<bold>a</bold>) Centroparietal electrodes (see triangles in scalp topography) showed a significantly greater response to Δevidence during ‘jump events’ in the noise stream when response periods were RARE than when they were FREQUENT. (<bold>b</bold>) Central and centroparietal electrodes showed a significantly greater negative-going potential immediately prior to a buttonpress during response periods. Lines and error bars show mean ± s.e.m. across 24 participants. * (solid black line at top of figure) denotes significant difference between FREQUENT and RARE (p&lt;0.05, cluster corrected for multiple comparisons across time). Details of the permutation testing used for multiple-comparisons correction are provided in ‘Methods’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>No significant differences in electroencephalographic (EEG) responses between conditions where response periods were SHORT versus LONG.</title><p>Panels arranged as in <xref ref-type="fig" rid="fig7">Figure 7</xref>; permutation tests were performed as in <xref ref-type="fig" rid="fig7">Figure 7</xref> (and described in ‘Methods’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Individual subject electroencephalographic (EEG) effects for the Δevidence regressor over centroparietal electrodes from 200 to 400 ms after the change in evidence.</title><p>The left-hand panel shows the individual effects for the Δevidence regressor over centroparietal electrodes (see triangles in <xref ref-type="fig" rid="fig7">Figure 7</xref>), averaged within-participant for both FREQUENT and RARE conditions (each dot = 1 participant). The plot shows the significantly increased response to this regressor in RARE versus FREQUENT (<italic>T</italic>(23) = 2.83, p=0.0095). No such difference is seen for the equivalent plot comparing SHORT and LONG conditions (right-hand panel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig7-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Responses to |Δevidence| reflect decision-relevant, not decision-irrelevant, statistics of stimulus</title><p>Given the potential role of |Δevidence| in surprise detection, we next asked whether the centroparietal response to |Δevidence| reflected low-level sensory properties of changes in the motion stimulus, or higher-level signals relevant to decision-making. To test this, we collected an additional control dataset where the stimulus contained both horizontal motion (decision-relevant) that subjects had to integrate, as in the main experiment, but also vertical motion (decision-irrelevant) that had the same low-level sensory statistics. As in the main experiment, we found that centroparietal responses reflected both ‘jump events’ and their associated |Δevidence| for decision-relevant motion, but these were substantially reduced for regressors that reflected changes in decision-irrelevant evidence (<xref ref-type="fig" rid="fig8">Figure 8</xref>). This implies that low-level sensory surprise alone does not account for the centroparietal responses to |Δevidence| in our continuous paradigm. Instead, the neural response is better described as reporting change detection that is relevant to signal detection and discrimination. It is possible that such change detection would be useful to indicate when a response period is more likely to arise in the task (<xref ref-type="bibr" rid="bib64">Shinn et al., 2022</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Control experiment demonstrates that response to |Δevidence| is primarily found to decision-relevant horizontal motion, but not decision-irrelevant vertical motion (with identical generative statistics).</title><p>Lines show mean +/- s.e.m. across 6 participants. * denotes timepoints where the response to |Δevidence| is significantly greater for decision-relevant motion than decision-irrelevant motion, while controlling for multiple-comparisons across time (see ‘Methods’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig8-v1.tif"/></fig></sec><sec id="s2-7"><title>Behavioural‐neural correlation between evidence integration kernels and TRFs to continuous sensory evidence</title><p>Finally, given the consistency and between-subject variability in integration time constants shown in <xref ref-type="fig" rid="fig3">Figure 3c</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, we explored whether any components relating to processing of sensory evidence might reflect <italic>cross-subject</italic> variation in evidence integration. We therefore performed a behavioural-neural correlation between participants’ integration time constants <inline-formula><mml:math id="inf18"><mml:mi>τ</mml:mi></mml:math></inline-formula> and their TRFs to sensory noise fluctuations. (Note that the integration time constants were fit using the equation described above, fit [using the approach described in ‘Methods’] separately to the empirical integration kernels from each of the four conditions.)</p><p>We found such a correlation for the triphasic potential elicited by the continuous ‘absoluted sensory evidence’ regressor (see <xref ref-type="fig" rid="fig6">Figure 6c</xref>). From approximately 420 ms onwards, the amplitude of the final, negative component of this component showed a negative correlation with <inline-formula><mml:math id="inf19"><mml:mi>τ</mml:mi></mml:math></inline-formula> across participants (<xref ref-type="fig" rid="fig9">Figure 9</xref>). In other words, this negative-going component was <italic>larger</italic> in amplitude (i.e. <italic>more</italic> negative) in participants who would integrate sensory evidence over <italic>longer</italic> durations (i.e. had a <italic>higher</italic> value of <inline-formula><mml:math id="inf20"><mml:mi>τ</mml:mi></mml:math></inline-formula>). We suggest that this may be consistent with variation in the encoding strength of previously studied correlates of continuous decision evidence. For example, Wyart et al. found a positive centroparietal potential 500 ms after decision information that positively encoded the current sample, but negatively encoded adjacent samples (<xref ref-type="bibr" rid="bib71">Wyart et al., 2012</xref>); our finding extends this work to explore variation in the response across participants.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Behavioural-neural correlation (across subjects) of integration decay time constant and response to absolute sensory evidence in stimulus (see <xref ref-type="fig" rid="fig6">Figure 6c</xref>).</title><p>Top panel shows Spearman’s rank correlation between the time-varying electroencephalographic (EEG) beta for absolute sensory evidence and individual subjects’ <inline-formula><mml:math id="inf21"><mml:mi>τ</mml:mi></mml:math></inline-formula> parameter, separately for each of the four conditions. The negative-going correlation found in all four conditions from ~420 ms onwards coincides with the third, negative-going limb of the triphasic response to absolute sensory evidence shown in <xref ref-type="fig" rid="fig6">Figure 6c</xref>. Bottom panels show the correlation plotted separately for each of the four conditions. We plot the average EEG effect size against log(<inline-formula><mml:math id="inf22"><mml:mi>τ</mml:mi></mml:math></inline-formula>) to allow for a straight-line fit (lines show mean ± 95% confidence intervals of a first-order polynomial fit between these two variables); we used Spearman’s rho to calculate the relationship, as it does not assume linearity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-fig9-v1.tif"/></fig><p>Although this across-subject correlation was discovered via exploratory analyses, it replicated across all four independent conditions, substantially increasing the likelihood of it being a true positive result (response periods FREQUENT and SHORT: Spearman’s <italic>ρ</italic> = –0.45, p=0.027; FREQUENT and LONG: <italic>ρ</italic> = –0.52, p=0.0099; RARE and SHORT: <italic>ρ</italic> = –0.53, p=0.0080; RARE and LONG: <italic>ρ</italic> = –0.46, p=0.025). By contrast, we found no evidence for an across-subject correlation between fitted integration decay time constants and EEG regressors encoding evidence updates at jump events (Δevidence), nor between fitted amplitude parameters (<italic>A</italic>) and any EEG regressors.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Real-world decisions typically demand continuous integration of incoming evidence in dynamic environments, without external cues as to when evidence integration should be initiated or responses made. This contrasts with a long-standing tradition in decision-making and psychophysics research to confine decisions to discrete trials, which are typically externally cued to the participant. In this article, we developed an approach to measure how participants weighed their recent history of sensory evidence when performing a continuous perceptual decision task. We found that participants' behaviour was well described by an exponentially decaying integration kernel, and that participants adapted the properties of this process to the overall statistics of the sensory environment (<xref ref-type="bibr" rid="bib57">Ossmy et al., 2013</xref>) across four different experimental conditions. We also found that there was substantive inter-individual variability in leaky evidence integration, with some participants rapidly discounting past evidence and others integrating over several seconds. We then demonstrated that both sources of variability (between-condition and between-subject) were reflected by changes in centroparietal EEG responses to time-varying fluctuations in sensory evidence.</p><p>Understanding the neural mechanisms supporting simple perceptual decision-making remains a key goal for the neurosciences, and the scalp topographies and signals that we have identified in this study match well with known findings in the literature on changepoint detection and sensory evidence accumulation. In particular, the well-known P300 component has long been argued to be associated with detection of statistical surprise (<xref ref-type="bibr" rid="bib15">Donchin, 1981</xref>; <xref ref-type="bibr" rid="bib17">Duncan-Johnson and Donchin, 1977</xref>; <xref ref-type="bibr" rid="bib49">Mars et al., 2008</xref>; <xref ref-type="bibr" rid="bib66">Squires et al., 1976</xref>) or, more recently, a correlate of a continuous time-evolving decision variable (<xref ref-type="bibr" rid="bib69">Twomey et al., 2015</xref>) that is equivalent to the CPP (<xref ref-type="bibr" rid="bib35">Kelly and O’Connell, 2013</xref>; <xref ref-type="bibr" rid="bib53">O’Connell et al., 2012</xref>). Our centroparietal responses to |Δevidence| (<xref ref-type="fig" rid="fig6">Figure 6b</xref>) are consistent with a changepoint detection account of continuous decision-making (<xref ref-type="bibr" rid="bib5">Booras et al., 2021</xref>), in which decision-relevant input (<xref ref-type="fig" rid="fig8">Figure 8</xref>) is evaluated for a change in latent state from a baseline period to a response period (<xref ref-type="bibr" rid="bib51">Nassar et al., 2019</xref>). This account would also explain why these signals are enhanced when response periods are rarer as a large |Δevidence| is more statistically surprising when response periods are rare than when they are common. Such changepoint detection may be useful to transiently suppress neuronal activity and attend to salient incoming sensory evidence to guide choices (<xref ref-type="bibr" rid="bib64">Shinn et al., 2022</xref>). Alongside this, we also identify a triphasic potential that is similar in timecourse and scalp topography to previous studies of evidence accumulation (<xref ref-type="bibr" rid="bib71">Wyart et al., 2012</xref>), sensitive to the continuous incoming sensory evidence (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Although we did not find variation in this potential across conditions, we did find that its amplitude reliably predicted the time constant of leaky evidence accumulation across participants (<xref ref-type="fig" rid="fig9">Figure 9</xref>). This suggests a key role for this component in translating incoming sensory evidence into a continuous representation of the decision variable across time, but further work is needed to understand how this variable then supports the sensorimotor transformation into a final commitment to making a choice (<xref ref-type="bibr" rid="bib67">Steinemann et al., 2018</xref>). Relatedly, it is unclear what precise functional role of the pre-response central potential and centralised beta-band signals may play. One possibility is that they may reflect a change in decision threshold between RARE and FREQUENT conditions (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). Yet we were unable to detect any equivalent change in lateralised beta power (i.e. a signal related to the formation of a specific choice <xref ref-type="bibr" rid="bib31">Hunt et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Kirschner et al., 2023</xref>). One alternative possibility is that the central ERP is a readiness potential (<xref ref-type="bibr" rid="bib62">Schurger et al., 2021</xref>).</p><p>Our work sits within a broader trend in recent research of moving away from trial-based designs towards continuous decision paradigms (<xref ref-type="bibr" rid="bib29">Huk et al., 2018</xref>). In addition to being more naturalistic, a key advantage of such paradigms is that it is possible to relate time-varying properties of the continuous input to continuous output variables. In ‘tracking’ paradigms, for example, not only is the sensory input continuous and time-varying but also the behavioural responses made by participants. This approach has been used to dramatically reduce the length of time needed from individual participants, meaning that experiments that previously required many thousands of trials over several hours to recover psychophysical functions can now be completed in a matter of minutes (<xref ref-type="bibr" rid="bib4">Bonnen et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Knöll et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Straub and Rothkopf, 2021</xref>). In our paradigm, the behavioural responses remained discrete and sparse (as participants were completing a signal detection/discrimination task, rather than a tracking task), but the EEG data is continuous and time-varying, and our analysis approach similarly benefits from being able to relate continuous variations in sensory input to this continuous neural signal. This has been shown in previous work to yield considerable improvements in signal-to-noise ratio compared to traditional trial-based event-related potentials (<xref ref-type="bibr" rid="bib14">Dimigen and Ehinger, 2021</xref>; <xref ref-type="bibr" rid="bib24">Gonçalves et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Lalor et al., 2006</xref>), meaning that our approach should allow us to characterise the neural response to sensory evidence integration in less recording time than in previous work. Indeed, when we examined neural responses from individual participants in this study, we found a high degree of individual-subject reliability. This efficiency is due to the high density of events in the continuous task design (&gt;1000 jump events in each 5 min block), limiting the amount of ‘dead time’ present in the experimental design (<xref ref-type="bibr" rid="bib28">Henson, 2007</xref>). In sum, this leads to a more efficient experimental design than in conventional trial-based experiments and may potentially allow for more rapid and reliable estimation of single-subject responses.</p><p>An intriguing finding in this work is the substantive variability in integration decay time constants across individuals. Indeed, such inter-individual variability exceeded the between-condition variability that was observed due to our experimental manipulations. We consider two possible explanations of this inter-individual variability. The first is that it is a stable, trait-like feature of sensory evidence integration that is not unique to our task, but instead reflects true variability in perceptual evidence integration across individuals. Such a hypothesis would imply that it would predict variability in integration time constants in other domains (e.g., auditory evidence integration [<xref ref-type="bibr" rid="bib8">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Keung et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">McWalter and McDermott, 2018</xref>] or more broadly cognitive tasks that involve continuous maintenance and manipulation of information across time in working memory). If so, it may also be possible to relate variability in behavioural time constants to underlying neurobiological causes by measuring the resting autocorrelation structure of neural activity, for example, in MEG or fMRI data (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Manea et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Raut et al., 2020</xref>).</p><p>An alternative hypothesis is that the individual variability we observe may be a consequence of the prior expectations that our participants have about the overall task structure, combined with learning over the course of training. One result in support of this hypothesis comes from the modelling shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Not only does the result in <xref ref-type="fig" rid="fig4">Figure 4a</xref> show that behaviour should be adapted across different conditions, but it also shows that different individuals might potentially achieve similar performance by ending up at very different locations in this parameter space. This could in turn explain why between-subject variability in these kernels exceeded between-condition variability (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). During training, different participants could have optimised their parameters to maximise points gained, but in doing so ended up at different locations on the ‘ridge’ of parameters shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. To adapt behaviour between conditions, they may have then made a small adjustment in these parameters to optimise performance for each environment.</p><p>Further work will be needed to distinguish these explanations of between-subject variability in integration kernels and test competing models of participant behaviour. Although the Ornstein–Uhlenbeck process that we use is an appropriate and widely used model of the task, alternative models might also consider a dynamically changing threshold as a function of progress through the inter-trial interval (<xref ref-type="bibr" rid="bib20">Geuzebroek et al., 2022</xref>); or consider tracking the mean and variance of the stimulus over time, rather than just the mean (<xref ref-type="bibr" rid="bib2">Bill et al., 2022</xref>). In this work, we also did not directly fit parameters of the Ornstein–Uhlenbeck process to participant behaviour. Although progress has recently been made in model fitting for decision-making in continuous decision-making paradigms (<xref ref-type="bibr" rid="bib20">Geuzebroek et al., 2022</xref>), a key feature of our paradigm is that many responses result from the structured noise that we inject into the sensory evidence stream, which complicates the use of aggregate measures such as reaction time quantiles for model fitting. Model estimation could potentially be improved by having continuous behavioural output, as recently demonstrated in tracking paradigms (<xref ref-type="bibr" rid="bib29">Huk et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Straub and Rothkopf, 2021</xref>).</p><p>Our findings of behavioural adaptations according to the overall statistics of the sensory environment are consistent with findings from previous research that have examined the same question in the absence of neural measures (<xref ref-type="bibr" rid="bib57">Ossmy et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Piet et al., 2018</xref>). <xref ref-type="bibr" rid="bib57">Ossmy et al., 2013</xref> used a trial-based paradigm with similarities to ours, involving unpredictable signal detection in the context of time-varying background noise in combination with a manipulation of signal-period duration. They used this to show that participants adapted the time constants of evidence integration in different environments. In our paradigm, we benefitted from being able to directly recover empirical integration kernels as opposed to estimating them in a model, and we also found behavioural differences that resulted from varying response period duration. However, we surprisingly found little effect on integration kernels from this manipulation; we instead found that our manipulation of response period frequency had a greater effect on the weighting of past sensory evidence. We hypothesise that this difference between our results and those of Ossmy et al. may simply result from our manipulation of response period duration not being sufficiently large (3 s versus 5 s) to require a substantial change in evidence weighting to optimise rewards. This is also consistent with our neural results, where the between-condition variation in responses to time-varying evidence was primarily found as a function of response period frequency, rather than duration.</p><p>In conclusion, our work demonstrates that it is possible to accurately measure the timecourse and neural correlates of sensory evidence integration in continuous tasks, and how this adapts to the overall properties of the environment. This work provides a framework for future work to investigate how evidence integration is adapted to other features of decision tasks, and how this may vary across individual participants. Our approach will also be useful for future work to investigate how the properties of evidence integration change in clinical populations and how they are affected by various interventions (e.g. pharmacological, electrical/magnetic stimulation, cognitive training).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Task design</title><p>In the continuous task, participants observed a stream of randomly moving dots in a circular aperture (<xref ref-type="fig" rid="fig1">Figure 1</xref>). A fraction of these dots move coherently to the left or to the right; the motion coherence is the proportion of dots moving in the same direction, whereas the other dots move randomly. In this study, the coherence varied between –1 (all dots move to the left) and 1 (all dots move to the right). At 0 coherence, all dots move randomly.</p><p>Unlike in trial-based versions of the task, and even previous RDM tasks where motion is continuous (e.g. <xref ref-type="bibr" rid="bib35">Kelly and O’Connell, 2013</xref>), during the present task the coherence <italic>changed</italic> constantly. During ‘baseline’ periods, the <italic>average</italic> of these constantly changing values remains 0. Within this stream of constantly changing coherence, there were response periods in which the <italic>average</italic> coherence was either to the left or to the right (see <xref ref-type="fig" rid="fig1">Figures 1b</xref> and <xref ref-type="fig" rid="fig2">2a</xref>). The aim of the subject performing this task was to detect these stable periods of predominantly leftward or rightward motion (response periods). For participants, this means that they should not respond as soon as they think they know in which direction the dots are moving coherently, as is the case for a discrete trial version of the RDM task. Instead, participants must weigh the recent history of motion directions to detect periods where the average motion direction of the dots was consistently leftwards or rightwards.</p><p>Participants indicated their decision about the average motion direction by pressing keyboard button ‘L’ to report a response period with average rightward motion and ‘A’ to report a response period with average leftward motion. Every time a button was pressed, a change in colour of the central fixation point provided feedback: correct responses were indicated by a green fixation point, a red fixation point followed an incorrect response during the response period, and false alarms (i.e. buttonpresses during baseline periods) were indicated by a yellow fixation point. Whenever a response period was missed (no response made), the fixation point turned blue after 500 ms (note that a buttonpress within these 500 ms was still counted as a correct response, to account for non-decision time and allow participants to integrate over the entire length of the response period). Following correct or incorrect responses made during a ‘response period’, the response period was terminated immediately, and the stimulus returned to baseline.</p><p>Participants were rewarded for correct responses but lost points for any other response. They received 3 points for correct responses, punished with –3 points for incorrect responses, and missed response periods or false alarms were both punished with –1.5 points. A reward bar was shown at the end of each 5 min block to indicate how many points participants have won in total (the reward bar was shown continuously onscreen during training, but not during task performance to avoid distraction). As participants won more points, their reward increased to the right until they hit the right border of the reward bar (equivalent to a net gain of 15 points), the bar was reset to the middle of the screen and they received £0.50 bonus to take at the end of the experiment. In rare cases where participants were performing poorly and losing points on average, they hit the left border of the reward bar (–15 points) and had £0.50 deducted from their take-home bonus.</p></sec><sec id="s4-2"><title>Structure of the noise</title><p>An essential feature of this task paradigm was the noise structure, which leads to continuously varying coherence levels. Notably, the noise was placed under experimental control rather than randomly generated, meaning that we could examine how fluctuations in the noise impact participants’ behavioural and neural data. More generally, the noise can be described as a series of short intervals that vary in duration and coherence (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The interval duration was sampled from an exponential distribution with a mean duration of 270 ms. This distribution was then truncated, with a minimum duration of 10 ms and a maximum duration of 1000 ms for each step. The level of motion coherence at each step was sampled randomly from a normal distribution. The mean of this normal distribution depends on whether the step occurred during baseline or a response period. During a baseline period, the mean of the normal distribution was 0. That means it was equally likely that negative or positive coherences were drawn. During response periods, the mean of the normal distribution was sampled uniformly from the set [-0.5, –0.4, –0.3, 0.3, 0.4, 0.5]. Any samples that exceeded 100% motion were set to be [+1, –1]. To limit the number of times this occurred, we set the standard deviation of the distribution to 0.3 for response periods and 0.5 for baseline periods. (We note that this could allow a strategy of tracking changes in the variance in the stimulus as well as the mean, something that we address in the supplementary note.).</p></sec><sec id="s4-3"><title>Design of the random dot motion stimulus</title><p>The task was coded in Psychtoolbox (<xref ref-type="bibr" rid="bib6">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib40">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="bib58">Pelli, 1997</xref>) and parameters were chosen similar to <xref ref-type="bibr" rid="bib63">Shadlen and Newsome, 2001</xref>. Participants were seated 87 cm in front of the screen. Moving dots were displayed in a circular aperture subtending with a radius of 5 visual degrees on a Dell monitor with a refresh rate of 100 Hz. Dots had a size of 0.1 visual degrees and were displayed with a density of 2.5 dots per squared visual degree. The fixation point in the centre of the screen had a size of 0.3 visual degrees. All dots were black and displayed on a mid-grey background (rgb: 0.5, 0.5, 0.5).</p><p>Dots were equally divided into three sets. These sets were shown sequentially, meaning only one set per frame was shown. Each time a set reappeared on the screen the coherence on that frame dictated, the likelihood of that dot either being displaced randomly or in the direction of the coherence. Randomly displayed dots moved like Brownian motion particles, with no particular speed. Dots that moved coherently were displaced according to a speed of 7° per second (<xref ref-type="bibr" rid="bib63">Shadlen and Newsome, 2001</xref>). This approach means that subjects were forced to integrate across the entire field of moving dots to establish the motion direction; tracking a single dot is not reliable because it only reappears on every third frame and does not necessarily move coherently for more than two frames.</p></sec><sec id="s4-4"><title>Conditions</title><p>To understand whether participants can flexibly adapt their integration kernel, we tested the continuous evidence integration task under different conditions following a within-subject 2 × 2 design. In different 5 min blocks, participants were told that they would either have long (5 s) or short (3 s) response periods, and either frequent (baseline period range 3–8 s) or rare (baseline period range 5–40 s) response periods. Subjects received extensive training (see below) so that these environmental statistics were well learnt prior to the experimental session. During the experiment, they were cued as to which condition they were currently in, by (i) displaying in text at the beginning of each block (e.g. ‘response periods are LONG and FREQUENT’); and (ii) having a different shape of fixation point (triangle, square, circle, star) for each of the four blocks. This meant that there was no inference nor memory required from the subjects to know which condition they were currently in.</p></sec><sec id="s4-5"><title>Training</title><p>Our training protocol was designed to overtrain participants to reach a high level of performance on the task and to minimise learning effects during the main testing session. Our training taught participants about the structure of the long/short and rare/frequent trial periods, how to discriminate such trial periods from background noise fluctuations, and crucially incentivised participants to maximise their overall reward rate.</p><p>Training consisted of a sequence of different tasks that incrementally trained participants on the random dot motion stimulus and the continuous nature of the task. First, participants were introduced to the conventional RDM task based on discrete trials (<xref ref-type="bibr" rid="bib63">Shadlen and Newsome, 2001</xref>), initially with very strong motion coherences (–0.9, 0.9), and then progressively with motion coherences resembling those found in the main experiment [-0.5,–0.4, –0.3, 0.3, 0.4, 0.5].</p><p>Next, participants completed intermediate task versions that still consisted of discrete trials, but had features of the continuous task: in particular, noise fluctuations was superimposed on the mean motion coherence, as in the final continuous task version coherences would fluctuate throughout the trial. This meant that participants had to estimate the average motion direction across the entire trial, but respond before the trial ended. Trial lengths were 5 s (length of long response periods) or 3 s (length of short response periods) and the fixation point shrank over the course of the trial so that participants would have an idea of how much time they had to respond. Trials with a mean coherence of 0% were also included, to simulate baseline periods of the continuous task that participants had to contrast to the other trials; participants had to suppress a response in these trials.</p><p>Then, participants moved on to the continuous version of the task, but they were first trained on a paradigm with higher mean coherences during response periods; in addition, the fixation dot would change its colour to white to indicate the onset of response periods. Gradually, all conditions of the experiment and the final mean coherence levels of signal periods were introduced, and a change of the fixation dot colour to white was disabled. At this point, when the paradigm was the same as in the full task, they had an extended period of practice on the task across all four conditions. Note that during this time the colours of the fixation dot feedback were the same as in the main experiment, and participants were instructed about the meaning of these dots.</p><p>We progressed participants through the different versions of training by checking psychometric functions to establish that they had fully learnt each stage before progressing onto the next stage of the task. Participants had to perform at 80% correct or higher on discrete trials to move on to the continuous tasks, and performance in the continuous task was checked qualitatively by plotting the stimulus stream and responses after each block. Verbal feedback was given to participants based on their performance, which also helped participants to improve their behaviour during training. If, in the latter parts of training, participants still missed more than half of the response periods, they were excluded from the subsequent EEG session. Participants (n = 3) who failed to progress from the training session were paid for that session and did not progress on to the EEG session.</p><p>After participants completed training successfully, they participated in the EEG testing session not more than 1 wk later. In this session, participants first performed a ‘reminder’ where they practised one run of the full task for 20 min (all four conditions, presented for 5 min each, in randomised order); this was performed while the experimenter put on the EEG cap. Then, while EEG data was collected, they completed 5–6 task ‘runs’, each lasting 20 min. Each run consisted of all four conditions in randomised order.</p></sec><sec id="s4-6"><title>Data collection</title><p>We tested 33 participants (13 male). Of those, three were unable to learn the continuous task and did not progress beyond training. One participant was excluded for falling asleep during the EEG session. Another five were excluded from the analysis due to technical issues matching the continuous EEG with the stimulus stream and/or issues with EEG data quality after pre-processing. This means 24 subjects were included in the analysis. Each subject completed six runs, except for one subject who completed only five runs. For the control experiment with superimposed vertical motion (<xref ref-type="fig" rid="fig8">Figure 8</xref>), a further six participants were tested (four males). All participants were aged 18–40, had normal or corrected-to-normal vision, and gave written consent prior to taking part in the study. The study was approved by the University of Oxford local ethics committee (CUREC R60298).</p></sec><sec id="s4-7"><title>Behavioural analysis</title><sec id="s4-7-1"><title>Detection rate/reaction times</title><p>We calculated correct detection rate (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) as the proportion of response periods in which correct responses were made. We calculated this separately for each run within each level of motion coherence (collapsing across leftward/rightward correct responses), and then averaged across the six runs, to obtain three values (0.3, 0.4, and 0.5 motion coherence) for each of the four conditions per subject. We performed a similar analysis on reaction times for these correct responses (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), but here we excluded responses in LONG conditions that exceeded 3.5 s, such that the average response time could be directly compared between LONG and SHORT conditions. (We note that due to noise and the associated uncertainty concerning response period onset, participants are incentivised to respond as quickly as possible whenever they thought they were in a response period, as delaying responses would lead to ‘missed trials’.) We analysed the effects of coherence, response period length and response period frequency on detection rate and reaction time across the 24 participants using a three-way repeated-measures ANOVA.</p></sec><sec id="s4-7-2"><title>Integration kernels</title><p>We calculated integration kernels by averaging the presented motion coherence for 5 s preceding every buttonpress (having first multiplied this by –1 for leftward buttonpresses, so that left and right responses can be averaged together). We did this separately for false alarms (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) and for correct responses (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). We excluded correct responses in LONG conditions that exceeded 3.5 s for similar reasons as outlined above. We also note that the integration kernel in this period includes a mixture of ‘signal’ (shift in mean coherence) plus noise, whereas the integration kernel from false alarms is driven by noise alone. This explains why the segment of the integration kernel that reflects non-decision time (i.e. immediately prior to buttonpress) returns close to 0 in <xref ref-type="fig" rid="fig3">Figure 3b</xref> but is closer to 0.5 in <xref ref-type="fig" rid="fig2">Figure 2c</xref>, and also why the false alarm integration kernel is more clearly an exponential decay function.</p><p>We fit an exponential decay model to the empirical integration kernel from false alarm responses:<disp-formula id="equ3"><mml:math id="m3"><mml:mi>k</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></disp-formula></p><p>where <italic>k</italic>(<italic>t</italic>) is the height of the integration kernel <italic>t</italic> seconds before its peak; <italic>A</italic> is the peak amplitude of the integration kernel (in units that denote the fraction of dots moving towards the chosen response direction), and <inline-formula><mml:math id="inf23"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the decay time constant (in units of seconds). To fit the exponential decay function, we first found the peak of the empirical integration function (using <italic>max</italic> in MATLAB), and set this timepoint to <italic>t</italic> = 0 in the equation above. We then fit <italic>A</italic> and <inline-formula><mml:math id="inf24"><mml:mi>τ</mml:mi></mml:math></inline-formula> to the empirical integration kernel for all timepoints up to and including <italic>t</italic> = 0 using <italic>fminsearch</italic> in MATLAB using a least-squares cost function between the fitted model and data with an L2 regularisation term that penalised large values of either <italic>A</italic> or <inline-formula><mml:math id="inf25"><mml:mi>τ</mml:mi></mml:math></inline-formula> (λ = 0.01). To calculate the quality of the model fit, we calculated <italic>R</italic><sup>2</sup> for this function:<disp-formula id="equ4"><mml:math id="m4"><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>with RSS being the residual sum of squares after model fitting and TSS being the total sum of squares.</p></sec><sec id="s4-7-3"><title>False alarm rates</title><p>To calculate false alarm rates (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we counted the total number of responses made during baseline periods and divided this by the total amount of time where subjects could possibly have made a false alarm (i.e. total time spent in baseline periods). We repeated this separately for each of the four conditions within each participant.</p></sec></sec><sec id="s4-8"><title>EEG acquisition</title><p>EEG data was collected at a sampling rate of 1000 Hz with Synamps amplifiers and Neuroscan data acquisition software (Compumedics) and 61 scalp electrodes following the 10–20 layout. Additionally, bipolar electrodes were placed below and above the right eye and on the temples to measure eyeblinks as well as horizontal and vertical eye movements (HEOG and VEOG channels). A ground electrode was attached to the left elbow bone. The EEG signal was referenced to the left mastoid but later re-referenced to the average of left and right mastoids. Impedances of electrodes were kept below 15 kΩ.</p></sec><sec id="s4-9"><title>EEG pre-processing</title><p>Data were pre-processed using spm12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>; <xref ref-type="bibr" rid="bib45">Litvak et al., 2011</xref>), the FieldTrip toolbox for EEG/MEG-analysis (<ext-link ext-link-type="uri" xlink:href="http://fieldtriptoolbox.org">http://fieldtriptoolbox.org</ext-link>; <xref ref-type="bibr" rid="bib55">Oostenveld et al., 2011</xref>), and MATLAB (Version R2018b, The MathWorks, Inc, Natick, MA). Each session for each participant was pre-processed as continuous data. First, each session was downsampled to 100 Hz. Then, the data was rereferenced to the average of left and right mastoid electrodes and bandpass filtered the data between 0.1 Hz and 30 Hz using the function <italic>spm_eeg_filter</italic> with default settings (fifth-order Butterworth filter, passed in both directions). In a next step, we used signal space projection methods in SPM to perform eyeblink correction. The bipolarised VEOG channel was used to build a spatial confound topography of eye blinks to delineate ocular source components, Segments of 1000 ms around eye blink events in the VEOG channel were generated and averaged. Principal component analysis was then used to define the noise subspace of eyeblinks across all channels, and the first principal component was regressed out of the continuous EEG data (<xref ref-type="bibr" rid="bib1">Berg and Scherg, 1994</xref>; <xref ref-type="bibr" rid="bib30">Hunt et al., 2012</xref>). For each participant and session, the spatial confound map of the first component was visually checked to ensure it showed a typical eye blink topography before the regression was applied. The EEG data was further thresholded to remove artefacts that were ≥100 µV in a single channel by labelling a 500 ms window around the peak of the artefact, and removing these time windows when estimating the deconvolutional GLM.</p></sec><sec id="s4-10"><title>Deconvolutional GLM analysis</title><p>We used triggers sent to each jump in the noise stream to align the continuous EEG data with the continuous stream of sensory evidence (and other experimental events, such as buttonpresses). As the downsampled EEG was at the same sampling rate as the refresh rate as the display (100 Hz), we simply used the continuous stream of evidence presented on each frame of the experiment from then onwards. In addition to the five participants excluded due to technical issues with trigger recording and alignment (see ‘Data collection’ above), there was one further participant in our main EEG sample (n = 24) who had 3 out of 24 blocks missing due to technical issues; this participant was nevertheless taken forward into the main analysis with the remaining 21 recorded blocks.</p><p>We then constructed a design matrix <italic>X</italic> for the continuous EEG data, with 11 regressors in total:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>The ‘buttonpress’ regressors and the regressors with subscript ‘event’ are ‘stick functions’ (1 at the timepoint that they occurred, and 0 at all other timepoints). Other regressors are parametric modulators of these, except for the two continuous regressors which were valued at all timepoints of the experiment (reflecting the current motion onscreen, either absoluted [reported in the main text] or signed [not discussed]). In this article, we focus on responses to <inline-formula><mml:math id="inf26"><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6a</xref>), <inline-formula><mml:math id="inf27"><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figures 6b</xref> and <xref ref-type="fig" rid="fig7">7a</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1a</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref>), <inline-formula><mml:math id="inf28"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figures 6c and 9</xref>), and <inline-formula><mml:math id="inf29"><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7b</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1b</xref>). For all of these except for the <inline-formula><mml:math id="inf30"><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , we only estimate the EEG response during the baseline periods, when participants are still integrating evidence (as shown empirically in <xref ref-type="fig" rid="fig3">Figure 3</xref>), but the statistics of the stimulus stream across all four experimental conditions are matched. We calculated the correlation between the key regressors of interest (<xref ref-type="table" rid="table1">Table 1</xref>) to ensure that they were sufficiently decorrelated from one another to reliably obtain parameter estimates in the GLM.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>(Average) explained variance between regressors of the convolutional general linear model (GLM) for baseline periods.</title><p>Columns and rows are the different regressors used to investigate baseline periods. Between each pair of regressors for the key continuous variables, the explained variance (squared correlation coefficient) was calculated to ensure that these regressors were not correlated with each other prior to estimating the GLM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><italic>R</italic><sup>2</sup></th><th align="left" valign="bottom">Jump</th><th align="left" valign="bottom">Jump level</th><th align="left" valign="bottom">Jump |Δevidence|</th><th align="left" valign="bottom">Continuous |evidence|</th></tr></thead><tbody><tr><td align="left" valign="bottom">Jump</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Jump level</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.18</td><td align="char" char="." valign="bottom">0.03</td></tr><tr><td align="left" valign="bottom">Jump |Δevidence|</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0.18</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.01</td></tr><tr><td align="left" valign="bottom">Continuous |evidence|</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.03</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">1</td></tr></tbody></table></table-wrap><p>To obtain the deconvolved response to each of these regressors, we time-expanded the design matrix into a large design matrix <italic>X</italic><sub>dc</sub> (see <xref ref-type="bibr" rid="bib18">Ehinger and Dimigen, 2019</xref> for a recent review; <xref ref-type="table" rid="table2">Table 2</xref>). We used simple ‘staircasing’ of the regressors to create this design matrix (as illustrated in <xref ref-type="fig" rid="fig6">Figure 6</xref>), rather than a time-Fourier basis set (<xref ref-type="bibr" rid="bib46">Litvak et al., 2013</xref>) or time-Spline basis set (<xref ref-type="bibr" rid="bib18">Ehinger and Dimigen, 2019</xref>); any of these approaches might be suitable for future studies. The number of timepoints for each of the regressors varied slightly between different regressors (e.g. we were primarily interested in activity <italic>after</italic> stimulus changes but <italic>before</italic> buttonpresses; the number of pre- and post-event lags reflected this). We then estimated parameter estimates for the deconvolved regressor at each sensor for each subject with ordinary least squares (using the method of <xref ref-type="bibr" rid="bib11">Courrieu, 2008</xref> to facilitate fast computation of the pseudoinverse of the design matrix).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Design matrix.</title><p>This table describes the size of the design matrix assuming a sampling frequency of the electroencephalographic (EEG) signals of 100 Hz. For each regressor the number of lags pre- and post event and the total number of rows this regressor covers in the design matrix are described. The same number of lags was applied to vertical motion regressors for the control study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Regressor</th><th align="left" valign="bottom">Pre-event time in time-expanded design matrix (ms)</th><th align="left" valign="bottom">Post-event time in time-expanded design matrix (ms)</th><th align="left" valign="bottom">Total rows in the time-expanded design matrix</th></tr></thead><tbody><tr><td align="left" valign="bottom">Jump event (stick function)</td><td align="char" char="." valign="bottom">1000</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">251</td></tr><tr><td align="left" valign="bottom">Jump level (|evidence| at each jump event)</td><td align="char" char="." valign="bottom">1000</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">251</td></tr><tr><td align="left" valign="bottom">Jump |Δevidence| (at each jump event)</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">301</td></tr><tr><td align="left" valign="bottom">Continuous |evidence|</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">301</td></tr><tr><td align="left" valign="bottom">Continuous (signed) evidence</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">1500</td><td align="char" char="." valign="bottom">301</td></tr><tr><td align="left" valign="bottom">Correct buttonpresses (stick function)</td><td align="char" char="." valign="bottom">5000</td><td align="char" char="." valign="bottom">3500</td><td align="char" char="." valign="bottom">851</td></tr><tr><td align="left" valign="bottom">Correct buttonpresses (+1 for right, –1 for left)</td><td align="char" char="." valign="bottom">5000</td><td align="char" char="." valign="bottom">3500</td><td align="char" char="." valign="bottom">851</td></tr><tr><td align="left" valign="bottom">False alarm buttonpresses (stick function)</td><td align="char" char="." valign="bottom">5000</td><td align="char" char="." valign="bottom">3500</td><td align="char" char="." valign="bottom">851</td></tr><tr><td align="left" valign="bottom">False alarm buttonpresses (+1 for right, –1 for left)</td><td align="char" char="." valign="bottom">5000</td><td align="char" char="." valign="bottom">3500</td><td align="char" char="." valign="bottom">851</td></tr><tr><td align="left" valign="bottom">Onset of response period (stick function)</td><td align="char" char="." valign="bottom">500</td><td align="char" char="." valign="bottom">8000</td><td align="char" char="." valign="bottom">851</td></tr><tr><td align="left" valign="bottom">Response period |coherence| of response period (stick function)</td><td align="char" char="." valign="bottom">500</td><td align="char" char="." valign="bottom">8000</td><td align="char" char="." valign="bottom">851</td></tr></tbody></table></table-wrap></sec><sec id="s4-11"><title>Permutation test for convolutional GLM analysis</title><p>To test for significant differences between deconvolved EEG responses for LONG versus SHORT response periods, and for RARE versus FREQUENT response periods, we performed a non-parametric paired <italic>t</italic>-test controlling for multiple comparisons across time, using the FieldTrip function <italic>ft_timelockstatistics</italic> (<xref ref-type="bibr" rid="bib48">Maris and Oostenveld, 2007</xref>). We first selected electrodes and time windows of interest based upon the average response to the key regressors across all four conditions (see <xref ref-type="fig" rid="fig6">Figure 6</xref>); we note that because this selection vector is orthogonal to the difference between conditions (and the number of observations are matched between conditions), then it provides an unbiased method for selecting a window of interest (<xref ref-type="bibr" rid="bib42">Kriegeskorte et al., 2009</xref>). In practice, this meant that the cluster-based permutation test was performed on an average of three centroparietal electrodes (CP1, CP2, CPz) and a time window from 0 to 800 ms post-event for jump-locked events (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1a</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref>), and an average of six centroparietal and central electrodes (C1, C2, Cz, CP1, CP2, CPz) and a time window from 2000 ms to 0 ms pre-event for buttonpress events (<xref ref-type="fig" rid="fig7">Figure 7b</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1b</xref>). In total, 1000 permutations were generated with the Monte Carlo method, and clusters were selected based on a <italic>T</italic>-statistic threshold of 2.07 for initial cluster formation (except for | <inline-formula><mml:math id="inf31"><mml:mi>j</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></inline-formula>, where a slightly lower threshold of <italic>T</italic> &gt; 1.80 was used), and an alpha of 0.05 (two-tailed) was then used for significance detection of clusters, corrected for multiple comparisons across time.</p><p>For the behavioural-neural correlations in <xref ref-type="fig" rid="fig9">Figure 9</xref>, we first temporally smoothed single subject betas with a Gaussian kernel with 75 ms FWHM (to further improve single subject SNR), and then calculated the Spearman’s correlation at each timepoint between the estimated betas for the <inline-formula><mml:math id="inf32"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></inline-formula> regressor and the <inline-formula><mml:math id="inf33"><mml:mi>τ</mml:mi></mml:math></inline-formula> parameters fit to the empirical evidence integration kernels for false alarms. We did this separately for the four conditions, providing four separate tests of the same behavioural-neural correlation (we note that these tests are independent in the sense that they consist of separate data for each correlation, but not in the sense that different participants were used to generate the data). In <xref ref-type="fig" rid="fig9">Figure 9b</xref>, we report the behavioural-neural correlation for the time window 420–750 ms after the evidence.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation, Software, Formal analysis, Validation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Supervision, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Supervision, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave written consent prior to taking part in the study. The study was approved by the University of Oxford local ethics committee (CUREC R60298).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82823-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Code repositories are available at our lab GitHub site for recreating the experimental paradigm within Psychtoolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/CCNHuntLab/continuous-rdm-task">https://github.com/CCNHuntLab/continuous-rdm-task</ext-link>, copy archived at <xref ref-type="bibr" rid="bib33">Hunt, 2022a</xref>) and for analysis of both behavioural and EEG data in MATLAB (<ext-link ext-link-type="uri" xlink:href="https://github.com/CCNHuntLab/ruesseler-eeg-analysis">https://github.com/CCNHuntLab/ruesseler-eeg-analysis</ext-link>, copy archived at <xref ref-type="bibr" rid="bib34">Hunt, 2022b</xref>). A resource containing both raw and pre-processed anonymised EEG and behavioural data has been uploaded to DataDryad, and is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.02v6wwq6b">https://doi.org/10.5061/dryad.02v6wwq6b</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ruesseler</surname><given-names>M</given-names></name><name><surname>Weber</surname><given-names>LA</given-names></name><name><surname>Marshall</surname><given-names>TR</given-names></name><name><surname>O'Reilly</surname><given-names>J</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Decision-making in dynamic, continuously evolving environments: Quantifying the flexibility of human choice</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.02v6wwq6b</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Neb Jojanovic for help in collecting the control experiment with simultaneous vertical and horizontal motion. MR is supported by a PhD studentship from the Wellcome Trust (109064/Z/15/Z). JOR is supported by a Career Development Fellowship from the Medical Research Council (MR/L019639/1) and an MRC Transition Support award (MR/T031344/1). LTH is supported by a Sir Henry Dale Fellowship from the Royal Society and the Wellcome Trust (208789/Z/17/Z). The Wellcome Centre for Integrative Neuroimaging is supported by core funding from the Wellcome Trust (203139/Z/16/Z). This research was funded in whole, or in part, by the Wellcome Trust. For the purpose of Open Access, the authors have applied a CC-BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>P</given-names></name><name><surname>Scherg</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>A multiple source approach to the correction of eye artifacts</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>90</volume><fpage>229</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(94)90094-9</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bill</surname><given-names>J</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual motion perception as online hierarchical inference</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>7403</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-34805-5</pub-id><pub-id pub-id-type="pmid">36456546</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>E</given-names></name><name><surname>Moehlis</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Yates</surname><given-names>J</given-names></name><name><surname>Pillow</surname><given-names>J</given-names></name><name><surname>Cormack</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Continuous psychophysics: Target-tracking to measure visual sensitivity</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.3.14</pub-id><pub-id pub-id-type="pmid">25795437</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Booras</surname><given-names>A</given-names></name><name><surname>Stevenson</surname><given-names>T</given-names></name><name><surname>McCormack</surname><given-names>CN</given-names></name><name><surname>Rhoads</surname><given-names>ME</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Change point detection with multiple alternatives reveals parallel evaluation of the same stream of evidence along distinct timescales</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>13098</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-92470-y</pub-id><pub-id pub-id-type="pmid">34162943</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-12-04745.1992</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title><source>Science</source><volume>340</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1126/science.1233912</pub-id><pub-id pub-id-type="pmid">23559254</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>SE</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A diversity of intrinsic timescales underlie neural computations</article-title><source>Frontiers in Neural Circuits</source><volume>14</volume><elocation-id>615626</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2020.615626</pub-id><pub-id pub-id-type="pmid">33408616</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheadle</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Myers</surname><given-names>N</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Herce Castañón</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adaptive gain control during human perceptual choice</article-title><source>Neuron</source><volume>81</volume><fpage>1429</fpage><lpage>1441</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.020</pub-id><pub-id pub-id-type="pmid">24656259</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Courrieu</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Fast Computation of Moore-Penrose Inverse Matrices</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/0804.4809">https://arxiv.org/abs/0804.4809</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.0804.4809</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Rahnev</surname><given-names>DA</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Lau</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Prestimulus oscillatory activity over motor cortex reflects perceptual expectations</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>1400</fpage><lpage>1410</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1094-12.2013</pub-id><pub-id pub-id-type="pmid">23345216</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimigen</surname><given-names>O</given-names></name><name><surname>Ehinger</surname><given-names>BV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Regression-based analysis of combined EEG and eye-tracking data: Theory and applications</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.1.3</pub-id><pub-id pub-id-type="pmid">33410892</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donchin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Surprise!? Surprise?</article-title><source>Psychophysiology</source><volume>18</volume><fpage>493</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1981.tb01815.x</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Buildup of choice-predictive activity in human motor cortex during perceptual decision making</article-title><source>Current Biology</source><volume>19</volume><fpage>1581</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.07.066</pub-id><pub-id pub-id-type="pmid">19747828</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan-Johnson</surname><given-names>CC</given-names></name><name><surname>Donchin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>On quantifying surprise: the variation of event-related potentials with subjective probability</article-title><source>Psychophysiology</source><volume>14</volume><fpage>456</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1977.tb01312.x</pub-id><pub-id pub-id-type="pmid">905483</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehinger</surname><given-names>BV</given-names></name><name><surname>Dimigen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unfold: an integrated toolbox for overlap correction, non-linear modeling, and regression-based EEG analysis</article-title><source>PeerJ</source><volume>7</volume><elocation-id>e7838</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.7838</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganupuru</surname><given-names>P</given-names></name><name><surname>Goldring</surname><given-names>AB</given-names></name><name><surname>Harun</surname><given-names>R</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Flexibility of timescales of evidence evaluation for decision making</article-title><source>Current Biology</source><volume>29</volume><fpage>2091</fpage><lpage>2097</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.05.037</pub-id><pub-id pub-id-type="pmid">31178325</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Geuzebroek</surname><given-names>AC</given-names></name><name><surname>Craddock</surname><given-names>H</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Balancing True and False Detection of Intermittent Sensory Targets by Adjusting the Inputs to the Evidence Accumulation Process</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.09.01.505650</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaze</surname><given-names>CM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Normative evidence accumulation in unpredictable environments</article-title><source>eLife</source><volume>4</volume><elocation-id>e08825</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08825</pub-id><pub-id pub-id-type="pmid">26322383</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaze</surname><given-names>CM</given-names></name><name><surname>Filipowicz</surname><given-names>ALS</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A bias–variance trade-off governs individual differences in on-line learning in an unpredictable environment</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>213</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0297-4</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonçalves</surname><given-names>NR</given-names></name><name><surname>Whelan</surname><given-names>R</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Towards obtaining spatiotemporally precise responses to continuous sensory stimuli in humans: A general linear modeling approach to EEG</article-title><source>NeuroImage</source><volume>97</volume><fpage>196</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.012</pub-id><pub-id pub-id-type="pmid">24736185</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceptual decision making in rodents, monkeys, and humans</article-title><source>Neuron</source><volume>93</volume><fpage>15</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.003</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harun</surname><given-names>R</given-names></name><name><surname>Jun</surname><given-names>E</given-names></name><name><surname>Park</surname><given-names>HH</given-names></name><name><surname>Ganupuru</surname><given-names>P</given-names></name><name><surname>Goldring</surname><given-names>AB</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Timescales of evidence evaluation for decision making and associated confidence judgments are adapted to task demands</article-title><source>Frontiers in Neuroscience</source><volume>14</volume><elocation-id>826</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00826</pub-id><pub-id pub-id-type="pmid">32903672</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hassall</surname><given-names>CD</given-names></name><name><surname>Harley</surname><given-names>J</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Temporal Scaling of Human Scalp-Recorded Potentials</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.11.421180</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Henson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Efficient Experimental Design for fMRI Statistical Parametric Mapping</source><publisher-name>Elsevier</publisher-name><fpage>193</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/B978-012372560-8/50015-2</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huk</surname><given-names>A</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Beyond trial-based paradigms: Continuous behavior, ongoing neural activity, and natural stimuli</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7551</fpage><lpage>7558</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1920-17.2018</pub-id><pub-id pub-id-type="pmid">30037835</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms underlying cortical activity during value-guided choice</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>470</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nn.3017</pub-id><pub-id pub-id-type="pmid">22231429</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Trial-type dependent frames of reference for value comparison</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003225</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003225</pub-id><pub-id pub-id-type="pmid">24068906</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Kaanders</surname><given-names>P</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Mugan</surname><given-names>U</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Russo</surname><given-names>E</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Stachenfeld</surname><given-names>K</given-names></name><name><surname>Wilson</surname><given-names>CRE</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Formalizing planning and information search in naturalistic decision-making</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1051</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00866-w</pub-id><pub-id pub-id-type="pmid">34155400</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022a</year><data-title>Continuous-Rdm-task</data-title><version designator="swh:1:rev:da5c5daa032d180b4ddf1db7507a6f2ab9079244">swh:1:rev:da5c5daa032d180b4ddf1db7507a6f2ab9079244</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:fb904f6e42829b8c4e82585dfafb700b36d70057;origin=https://github.com/CCNHuntLab/continuous-rdm-task;visit=swh:1:snp:0abccc81df21b25fe7f019696ba5a9219864acf2;anchor=swh:1:rev:da5c5daa032d180b4ddf1db7507a6f2ab9079244">https://archive.softwareheritage.org/swh:1:dir:fb904f6e42829b8c4e82585dfafb700b36d70057;origin=https://github.com/CCNHuntLab/continuous-rdm-task;visit=swh:1:snp:0abccc81df21b25fe7f019696ba5a9219864acf2;anchor=swh:1:rev:da5c5daa032d180b4ddf1db7507a6f2ab9079244</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022b</year><data-title>Ruesseler-EEG-analysis</data-title><version designator="swh:1:rev:d62771cf3e814fae7242db32868508c7e2e74e3c">swh:1:rev:d62771cf3e814fae7242db32868508c7e2e74e3c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7ecc1a011b18d390f28f2cb8ec850e653ae82c36;origin=https://github.com/CCNHuntLab/ruesseler-eeg-analysis;visit=swh:1:snp:dd89b962cf4a01c2b9fb038e4a45fd8202774aca;anchor=swh:1:rev:d62771cf3e814fae7242db32868508c7e2e74e3c">https://archive.softwareheritage.org/swh:1:dir:7ecc1a011b18d390f28f2cb8ec850e653ae82c36;origin=https://github.com/CCNHuntLab/ruesseler-eeg-analysis;visit=swh:1:snp:dd89b962cf4a01c2b9fb038e4a45fd8202774aca;anchor=swh:1:rev:d62771cf3e814fae7242db32868508c7e2e74e3c</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Internal and external influences on the rate of sensory evidence accumulation in the human brain</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19434</fpage><lpage>19441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3355-13.2013</pub-id><pub-id pub-id-type="pmid">24336710</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>Corbett</surname><given-names>EA</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neurocomputational mechanisms of prior-informed perceptual decision-making in humans</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>467</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00967-9</pub-id><pub-id pub-id-type="pmid">33318661</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keung</surname><given-names>W</given-names></name><name><surname>Hagen</surname><given-names>TA</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Regulation of evidence accumulation by pupil-linked arousal processes</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>636</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0551-4</pub-id><pub-id pub-id-type="pmid">31190022</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name><name><surname>Holmes</surname><given-names>WR</given-names></name><name><surname>Eissa</surname><given-names>TL</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimal models of decision-making in dynamic environments</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>54</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.06.006</pub-id><pub-id pub-id-type="pmid">31326724</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kirschner</surname><given-names>H</given-names></name><name><surname>Fischer</surname><given-names>AG</given-names></name><name><surname>Danielmeier</surname><given-names>C</given-names></name><name><surname>Klein</surname><given-names>TA</given-names></name><name><surname>Ullsperger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Cortical beta power reflects a neural implementation of decision boundary collapse in speeded decisions</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.13.523918</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1068/v070821</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knöll</surname><given-names>J</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Lawful tracking of visual motion in humans, macaques, and marmosets in a naturalistic, continuous, and untrained behavioral context</article-title><source>PNAS</source><volume>115</volume><fpage>E10486</fpage><lpage>E10494</lpage><pub-id pub-id-type="doi">10.1073/pnas.1807192115</pub-id><pub-id pub-id-type="pmid">30322919</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Simmons</surname><given-names>WK</given-names></name><name><surname>Bellgowan</surname><given-names>PSF</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Circular analysis in systems neuroscience: the dangers of double dipping</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>535</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1038/nn.2303</pub-id><pub-id pub-id-type="pmid">19396166</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Pearlmutter</surname><given-names>BA</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>McDarby</surname><given-names>G</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The VESPA: A method for the rapid estimation of A visual evoked potential</article-title><source>NeuroImage</source><volume>32</volume><fpage>1549</fpage><lpage>1561</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.054</pub-id><pub-id pub-id-type="pmid">16875844</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levi</surname><given-names>AJ</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Katz</surname><given-names>LN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Strategic and dynamic temporal weighting for perceptual decisions in humans and macaques</article-title><source>eNeuro</source><volume>5</volume><elocation-id>ENEURO.0169-18.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0169-18.2018</pub-id><pub-id pub-id-type="pmid">30406190</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Mattout</surname><given-names>J</given-names></name><name><surname>Kiebel</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Kilner</surname><given-names>J</given-names></name><name><surname>Barnes</surname><given-names>G</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Penny</surname><given-names>W</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>EEG and MEG data analysis in SPM8</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>852961</elocation-id><pub-id pub-id-type="doi">10.1155/2011/852961</pub-id><pub-id pub-id-type="pmid">21437221</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Jha</surname><given-names>A</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Convolution models for induced electromagnetic responses</article-title><source>NeuroImage</source><volume>64</volume><fpage>388</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.014</pub-id><pub-id pub-id-type="pmid">22982359</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manea</surname><given-names>AMG</given-names></name><name><surname>Zilverstand</surname><given-names>A</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Heilbronner</surname><given-names>SR</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Intrinsic timescales as an organizational principle of neural processing across the whole rhesus macaque brain</article-title><source>eLife</source><volume>11</volume><elocation-id>e75540</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.75540</pub-id><pub-id pub-id-type="pmid">35234612</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Debener</surname><given-names>S</given-names></name><name><surname>Gladwin</surname><given-names>TE</given-names></name><name><surname>Harrison</surname><given-names>LM</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Rothwell</surname><given-names>JC</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Trial-by-trial fluctuations in the event-related electroencephalogram reflect dynamic changes in the degree of surprise</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>12539</fpage><lpage>12545</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2925-08.2008</pub-id><pub-id pub-id-type="pmid">19020046</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McWalter</surname><given-names>R</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adaptive and selective time averaging of auditory scenes</article-title><source>Current Biology</source><volume>28</volume><fpage>1405</fpage><lpage>1418</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.049</pub-id><pub-id pub-id-type="pmid">29681472</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Bruckner</surname><given-names>R</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical context dictates the relationship between feedback-related EEG signals and learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e46975</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46975</pub-id><pub-id pub-id-type="pmid">31433294</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Paré</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A selective impairment of motion perception following lesions of the middle temporal visual area (MT)</article-title><source>The Journal of Neuroscience</source><volume>8</volume><fpage>2201</fpage><lpage>2211</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.08-06-02201.1988</pub-id><pub-id pub-id-type="pmid">3385495</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Dockree</surname><given-names>PM</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1729</fpage><lpage>1735</lpage><pub-id pub-id-type="doi">10.1038/nn.3248</pub-id><pub-id pub-id-type="pmid">23103963</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neurophysiology of human perceptual decision-making</article-title><source>Annual Review of Neuroscience</source><volume>44</volume><fpage>495</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-092019-100200</pub-id><pub-id pub-id-type="pmid">33945693</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsolic</surname><given-names>I</given-names></name><name><surname>Rio</surname><given-names>M</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Znamenskiy</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mesoscale cortical dynamics reflect the interaction of sensory evidence and temporal expectation during perceptual decision-making</article-title><source>Neuron</source><volume>109</volume><fpage>1861</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.031</pub-id><pub-id pub-id-type="pmid">33861941</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ossmy</surname><given-names>O</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Pfeffer</surname><given-names>T</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The timescale of perceptual evidence integration can be adapted to the environment</article-title><source>Current Biology</source><volume>23</volume><fpage>981</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.039</pub-id><pub-id pub-id-type="pmid">23684972</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piet</surname><given-names>AT</given-names></name><name><surname>El Hady</surname><given-names>A</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rats adopt the optimal timescale for evidence integration in a dynamic environment</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4265</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06561-y</pub-id><pub-id pub-id-type="pmid">30323280</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisauro</surname><given-names>MA</given-names></name><name><surname>Fouragnan</surname><given-names>E</given-names></name><name><surname>Retzler</surname><given-names>C</given-names></name><name><surname>Philiastides</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural correlates of evidence accumulation during value-based decisions revealed via simultaneous EEG-fMRI</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15808</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15808</pub-id><pub-id pub-id-type="pmid">28598432</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raut</surname><given-names>RV</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hierarchical dynamics as a macroscopic organizing principle of the human brain</article-title><source>PNAS</source><volume>117</volume><fpage>20890</fpage><lpage>20897</lpage><pub-id pub-id-type="doi">10.1073/pnas.2003383117</pub-id><pub-id pub-id-type="pmid">32817467</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurger</surname><given-names>A</given-names></name><name><surname>Pak</surname><given-names>J</given-names></name><name><surname>Roskies</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What is the readiness potential?</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>558</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.04.001</pub-id><pub-id pub-id-type="pmid">33931306</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Murray</surname><given-names>JD</given-names></name><name><surname>Seo</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Transient neuronal suppression for exploitation of new sensory evidence</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-27697-4</pub-id><pub-id pub-id-type="pmid">35013222</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Regression-based estimation of ERP waveforms: I. The rERP framework</article-title><source>Psychophysiology</source><volume>52</volume><fpage>157</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1111/psyp.12317</pub-id><pub-id pub-id-type="pmid">25141770</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squires</surname><given-names>KC</given-names></name><name><surname>Wickens</surname><given-names>C</given-names></name><name><surname>Squires</surname><given-names>NK</given-names></name><name><surname>Donchin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>The effect of stimulus sequence on the waveform of the cortical event-related potential</article-title><source>Science</source><volume>193</volume><fpage>1142</fpage><lpage>1146</lpage><pub-id pub-id-type="doi">10.1126/science.959831</pub-id><pub-id pub-id-type="pmid">959831</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinemann</surname><given-names>NA</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decisions are expedited through multiple neural adjustments spanning the sensorimotor hierarchy</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3627</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06117-0</pub-id><pub-id pub-id-type="pmid">30194305</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Straub</surname><given-names>D</given-names></name><name><surname>Rothkopf</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Putting Perception into Action: Inverse Optimal Control for Continuous Psychophysics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.12.23.473976</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twomey</surname><given-names>DM</given-names></name><name><surname>Murphy</surname><given-names>PR</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The classic P300 encodes a build-to-threshold decision variable</article-title><source>The European Journal of Neuroscience</source><volume>42</volume><fpage>1636</fpage><lpage>1643</lpage><pub-id pub-id-type="doi">10.1111/ejn.12936</pub-id><pub-id pub-id-type="pmid">25925534</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veliz-Cuba</surname><given-names>A</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stochastic models of evidence accumulation in changing environments</article-title><source>SIAM Review</source><volume>58</volume><fpage>264</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1137/15M1028443</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rhythmic fluctuations in evidence accumulation during decision making in the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>847</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.015</pub-id><pub-id pub-id-type="pmid">23177968</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82823.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.18.504278" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.18.504278"/></front-stub><body><p>This important study by Ruesseler, Weber and colleagues employs psychophysical kernels and EEG reverse correlation methods to identify the decision process adjustments used to account for variations in target frequency and duration in a task in which targets emerge periodically within a continuous stimulus stream. The paper provides solid evidence for the role of leak and threshold adjustments. The paper will be of interest to researchers studying mathematical models and neurophysiological correlates of decision making and more broadly authors with an interest in the application of reverse correlation techniques for neural signal analysis.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82823.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Tessereau</surname><given-names>Charline</given-names></name><role>Reviewer</role><aff><institution>MPI for biological Cybernetics</institution><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.18.504278">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.18.504278v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Decision-making in dynamic, continuously evolving environments: quantifying the flexibility of human choice&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Redmond G O'Connell as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Charline Tessereau (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The conclusions drawn from the integration kernel analyses appear difficult to reconcile with key features of the behavioural data. For example, the authors argue that the kernel analyses point to a difference in leak between the frequent and rare conditions but no corresponding change in false alarm rate is observed. Similarly, where the authors conclude from kernel analyses that participants did not alter their decision bounds between frequent and rare conditions the motor preparation ERP signal suggests that there is a bound adjustment. A key issue here is that it is not clear that integration kernels can be used to conclusively discriminate between a leak, bound or drift rate adjustment as each of these could conceivably impact the kernel. In order to make sense of the dataset, the authors should consider fitting alternative model variants to the data or, at minimum, simulating the impact of different decision process parameter adjustments on the integration kernel in order to determine whether their conclusions are warranted.</p><p>2) The reviewers raise questions regarding the nature of the decision required by the particular task implemented by the authors. The authors' approach relies on the assumption that participants are simply integrating coherent motion as they might for a standard variant of the random dot motion task. However on this task, targets were actually defined not by the presence of coherent motion but by a criterion duration of coherent motion, during which time the frame-to-frame coherent motion variance diminished. Can the authors rule out the possibility that decisions were partly or entirely based on detecting changes in stimulus variance and or coherent motion duration? If not, can it still be assumed that the rare and frequent targets were matched in terms of the ease with which they could be distinguished from the intervening noise intervals?</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>My principle suggestion would be that the authors consider running simulations of alternative models to see if they can reproduce the current behavioural trends. Drift rate adjustment to Rare vs Frequent target conditions is an obvious candidate.</p><p>In general, I found it hard to marry up all the different behavioural/kernel/EEG findings. I recommend the authors examine whether leak adjustment really provides a comprehensive account of their data or if additional/alternative parameter adjustments must be invoked to reconcile their data.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1) While I don't think it is problematic, the authors should acknowledge (even if just in methods) that subjects might be able to use the stimulus variance to identify response periods. Also, strictly speaking, the description of the noise as following a normal distribution cannot be correct because it is limited to the range of [-1,1]. I presume this also explains why the authors reduced the standard deviation in the response periods – to avoid hitting those edges too often.</p><p>2) Figure 1d is very challenging to interpret. I appreciate that the authors fully acknowledge that it involves signal and noise. However, I think this introduces so many confounds to that analysis that I'm not sure you can say much with confidence from it. Most obviously, the dynamics will reflect the RT differences and the associated shift in the onset of the signal contribution. Also, because it seems to be based on all coherences together, the signal contribution will critically depend on the relative proportion of each coherence in each condition. I'm not sure there is any way to disentangle all of that.</p><p>3) I wasn't quite sure how false alarm rate was calculated in Figure 3. Conditions differ in the total time for every run that subjects were in the baseline state where they could have a false alarm. Is the denominator of this rate calculation total task time or total time where subjects could possibly have a false alarm. I think the latter is the most useful comparison. I would guess the authors did that, but I couldn't find the description.</p><p>4) It would be useful to have a figure showing individual subject CPP effects shown in aggregate in Figure 5? A scatter plot comparing effect size for FREQUENT and RARE would be a good way to illustrate that.</p><p>5) In the discussion, the authors state that initial simulations suggest that a range of decay time constants could be optimal. This may provide another explanation for the lack of change in time constant in different conditions. If a range are possibly optimal for any given condition, perhaps the subjects can remain close to optimal in this task without changing their decay time constant between conditions.</p><p>6) I noticed two typos/formatting errors in the methods. In the subscripts for the regression, a &quot;D&quot; was shown for the &quot;δ&quot; symbol. Also, the words &quot;regressed out&quot; are duplicated in EEG pre-processing section.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>General notes:</p><p>– Figure captions should describe what is being shown, with a link to the method and/or supplementary material for lengthy computations.</p><p>– Throughout the text, the authors sometimes refer to 'net motion', 'coherence', and 'evidence'. I would make sure to use the same terminology.</p><p>– Some analyses are missing, e.g. the proportion of long answers that have been ignored (that could indicate a level of engagement in the task).</p><p>Detailed comments/suggestions:</p><p>Task design:</p><p>– Methods: are subjects instructed about what the dots mean in advance? How long do the dots stay colored for during feedback?</p><p>– How much do participants get in total? The total bar is 30 points and they get only 50 cts to have 15 points which are after 5 good responses without error on average. Are the feedback distracting the participants? Do the authors think that the money aspect here provides a substantial motivation for participants?</p><p>– Figure 1c:&quot; The resulting evidence integration kernel is well described by an exponential decay function, whose decay time constant in seconds is controlled by the free parameter tau &quot;: As formulated, the caption hints that the figure analyses how well described is the kernel. As I understand, the exponential kernel is a choice that is imposed by the authors, and how well it fits the coherence level before a decision is not addressed in the paper. I would reformulate and clearly state that the authors define an exponential kernel from the past coherence levels. More details on the definition of this kernel are required, and statistics about how well it fits the data would enable stronger claims of its use throughout the text. In particular, the average coherence can take 3 values (30, 40, or 50), are the authors averaging for all coherence levels together? Should A be correlated with those values? What is the effect of the mean coherence level during response periods?</p><p>P7, Figure 2:</p><p>– b and c: is that the mean or median (for b) and the error bar the std/95expectiles (for c) of participants' detection rate/RT? Is the RT tau? I know both variables are defined in the methods section but they should also be explained here or a link should be made to the method section so that the reader can know what they are looking at.</p><p>– d: is the integration kernel here referring to the exponential, or simply the average signal before the decision, averaged across all participants?</p><p>– d again: why is the maximum coherence 0.5 here? is that only for switches from 0 to 0.5 coherence? What happens for switches from 0 to 30 coherence?</p><p>– d again: the authors observe no effect on the response window duration on the response kernel, but have applied a threshold on the duration of the response on the long response period in order to make this plot. What happens if the authors include all response lengths? (also applies to the discussion p17 and 18).</p><p>P 8:</p><p>– &quot;This provides further evidence that participants were overall more conservative in their responses in LONG conditions than SHORT&quot; can the authors define what conservative means?</p><p>– It is not surprising that the integration kernels are not affected by the length of the response period if the long responses are not considered.</p><p>– I think the kernel analysis during baseline is necessary to show that participants are not randomly making motor movements, however, the authors could describe in a bit more detail the difference between a correct response and a false alarm: how are the integration kernels different in the baseline vs response period? (the kernels for false alarms seem a bit shorter?)</p><p>– When do false alarms occur compared to the response periods? Are they regularly after the end of a response period? What is the pattern of the coherence that causes those false alarms? Are they always when the frame update is 1s, for example?</p><p>Figure 3:</p><p>a) the authors mention significance but do not provide p values. Why is it consistent with having a more cautious response threshold? I'd favor the reverse interpretation that short response periods induce more confusion between signal and noise.</p><p>b) is that the averaged signal before false alarms? Do they always occur when the mean average coherence reaches 0.5? Why is the maximum value 0.5?</p><p>c) explain what the points show and how these are computed.</p><p>P.11</p><p>(ii) To me, it is not clear that the behavioral evidence from 'false alarms' indicate that participants are still integrating sensory evidence – a stricter definition and existence of the kernel (how well it fits for every response) could help support this statement</p><p>– Is 'net motion' coherence here?</p><p>– On the same page – the regressor 'absoluted sensory evidence' has not been named like this before.</p><p>P. 12:</p><p>– Why is it consistent with the behavioral finding of lower detection rate and slower RT in rare conditions?</p><p>– I understand the analysis is done regardless of the response. What about missed response periods/responses during the baseline?</p><p>Figure 4:</p><p>– The caption is incomplete, what does the right-hand side show? Is it one weight for one regressor across time for one example electrode?</p><p>– What is the goodness of fit of the regression?</p><p>P13 – Figure 5:</p><p>– What are the lines showing? What is the black thick line on top?</p><p>– P.14 To test whether these signals are decision-related, the authors could also compare the results of the regression when false alarms are made and when the response period is missed.</p><p>– Figure 6: has the effect of previous exposure been controlled for the vertical motion? Could the authors discuss whether this would have an impact on the CPP.</p><p>– P 15: could the authors give more details on the behavioral correlate and how is it computed.</p><p>– &quot;this negative-going component was larger in amplitude (i.e. more negative) in participants who would integrate sensory evidence over longer durations (i.e. had a higher value of τ).&quot;: could the authors elaborate on how to interpret this finding with regard to existing literature?</p><p>– Figure 7 P 16 spearman correlation: can the authors describe the top plot? what are the lines showing? if that's the mean it should be mentioned and the standard deviation should be shown. Can the authors describe the bottom plots? why are the authors using log(tau) for the x-axis?</p><p>Discussion:</p><p>- p 17: 'The behavior is well described by a leaky evidence accumulator' the exponential kernel is a reverse correlation that the authors have done themselves, but they have not shown causality nor statistical analysis on what those kernels mean in relationship with the behavior.</p><p>Methods:</p><p>– In the regressor table, what is 'prediction error' regressor?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82823.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The conclusions drawn from the integration kernel analyses appear difficult to reconcile with key features of the behavioural data. For example, the authors argue that the kernel analyses point to a difference in leak between the frequent and rare conditions but no corresponding change in false alarm rate is observed. Similarly, where the authors conclude from kernel analyses that participants did not alter their decision bounds between frequent and rare conditions the motor preparation ERP signal suggests that there is a bound adjustment. A key issue here is that it is not clear that integration kernels can be used to conclusively discriminate between a leak, bound or drift rate adjustment as each of these could conceivably impact the kernel. In order to make sense of the dataset, the authors should consider fitting alternative model variants to the data or, at minimum, simulating the impact of different decision process parameter adjustments on the integration kernel in order to determine whether their conclusions are warranted.</p></disp-quote><p>We thank the reviewers and editor for this suggestion – it has been a very important comment for reshaping our paper and the interpretation of several of the behavioural findings. In short, we agree with the reviewers that the integration kernels do not conclusively point to a change in leak, and in fact they are equally (if not more) likely to reflect a change in decision threshold. We have rewritten and extended parts of the paper substantially to reflect this change.</p><p>With respect to model fitting, we should first mention that we have found it challenging to <italic>directly</italic> fit models to the behavioural data. This is because, unlike for trial-based paradigms, there are not straightforward solutions to parameter fitting for continuous paradigms such as ours at present. We have quite extensively explored use of toolboxes such as the generalized drift diffusion model to study our data (Shinn et al., 2020), but it is difficult to adapt these toolboxes to settings in which responses can be emitted at any point in a continuous motion stream. We note that there are ongoing efforts that address model fitting using similar continuous paradigms (Geuzebroek et al., 2022), but these depend upon using a cost function that makes use of reaction time quantiles after target onset as well as for false alarms. A key difference in our paradigm is that the injection of continuous, structured noise into the sensory evidence complicates the interpretation of RT quantiles, and particularly false alarms, as the noise affects when false alarms will occur (and also the starting point of the accumulator at target onset). A more appropriate approach to fitting subjects’ behaviour in our task would take account of the impact of the structured noise on when the accumulator hits the decision threshold. This is something that we hope to pursue further in future work but were unable to successfully implement for the current paper.</p><p>We have therefore instead pursued the second suggestion in this comment (“simulating the impact of different decision process parameter adjustments on the integration kernel in order to determine whether their conclusions are warranted”). We have inserted a new section into the paper, entitled “Computational modelling of leaky evidence accumulation” (p.11 onwards in revised manuscript), with three additional figures to present these results.</p><p>We have simulated a leaky evidence accumulation model (Ornstein-Uhlenbeck process) that takes the actual presented stimulus stream (signal+structured noise) as its input, and have then simulated the impact of variations in leak and threshold on the model’s behaviour. (We adopted reviewer 2’s suggestion of discussing changes in threshold rather than reviewer 1’s suggestion of changes in gain; we discuss this further below). We present two sets of analyses from these simulations.</p><p>Our first set of model analyses explores how variation across leak and threshold parameters affects low-level behavioural features of the data: correct responses, missed responses, false alarm rates and overall reward obtained. The full exploration of these effects is shown in new Figure 4 —figure supplement 1 in the revised paper, and associated new main Figure 4.</p><p>This analysis highlights how the optimal strategy for solving the task (in terms of maximising reward gained) differs between the four conditions of interest, and in ways that we would have expected. For example, a higher decision threshold is beneficial when response periods are RARE rather than FREQUENT. The analysis also highlights an important point about the <italic>relationship between</italic> these parameters. In particular, there tends to be a ‘ridge’ in parameter space where several different model parameterisations perform similarly well in terms of maximising total points gained. This ‘ridge’ of peak performance falls along a line where leak (λ) is decreasing and decision threshold is increasing. In other words, a less ‘leaky’ integration model retains more evidence in the accumulator; this can be accommodated by an increase in threshold to perform similarly on the task.</p><p>We have included a new Figure 4 that illustrates these points:</p><p>We note that this may explain why we observe greater levels of <italic>between-subject</italic> variation than <italic>between-condition</italic> variation in task performance. There are a wide number of combinations of leak/threshold parameters that can perform well on the task (see new Figure 4 —figure supplement 1 for further detail), and the difference between conditions is comparably small. So, one possibility is that when learning the task, participants may perform gradient ascent on this parameter space to maximise the reward gained. They may all be able to perform well on the task, but in doing so they may reach different locations on this ‘ridge’. Between-condition variation could then account for a small movement of these parameters within each participant. As a result, participants may show considerable variation in integration kernels (Figure 3 —figure supplement 1), and closely related changes in neural measures of evidence integration (Figure 9), without showing much variation in reward obtained.</p><p>We suggested that this might be a possibility in our previous discussion, and we have now amended this to include the new findings from the modelling:</p><p>“An alternative hypothesis is that the individual variability we observe may be a consequence of the prior expectations that our participants have about the overall task structure, combined with learning over the course of training. One result in support of this hypothesis comes from the modelling shown in Figure 4 Not only does the result in Figure 4a show that behaviour should be adapted across different conditions, but it also shows that different individuals might potentially achieve similar performance by ending up at very different locations in this parameter space. This could in turn explain why between-subject variability in these kernels exceeded between-condition variability (Figure 3c; Figure 3 —figure supplement 1). During training, different participants could have optimised their parameters to maximise points gained, but in doing so ended up at different locations on the ‘ridge’ of parameters shown in Figure 4a. To adapt behaviour between conditions, they may have then made a small adjustment in these parameters to optimise performance for each environment…”</p><p>We also note that this analysis suggests that changes in one parameter are unlikely to occur truly independently of changes in other parameters. Optimising performance may involve a joint adjustment in leak and threshold, and conversely it may be difficult to uniquely determine the parameters from behaviour in this version of the task. In future work, it may be worth considering reparameterizations of the model that allow for true independence in parameter space, or designing experiments in which only small area of parameter space is optimal rather than a ‘ridge’ of parameter space. We have also mentioned these points in our revised discussion:</p><p>“Further work will be needed to distinguish these explanations of between subject variability in integration kernels, and to test competing models of participant behaviour. Although the Ornstein-Uhlenbeck process that we use is an appropriate and widely used model of the task, alternative models might also consider a dynamically changing threshold as a function of progress through the inter-trial interval (Geuzebroek et al., 2022); or consider tracking the mean and variance of the stimulus over time, rather than just the mean (Bill et al., 2022). In the current work, we also did not directly fit parameters of the Ornstein-Uhlenbeck process to participant behaviour. Although progress has recently been made in model fitting for decision making in continuous decision-making paradigms (Geuzebroek et al., 2022), a key feature of our paradigm is that many responses result from the structured noise that we inject into the sensory evidence stream, which complicates the use of aggregate measures such as reaction time quantiles for model fitting. Model estimation could potentially be improved by having continuous behavioural output, as recently demonstrated in tracking paradigms (Huk et al., 2018; Straub and Rothkopf, 2021).”</p><p>With this comment in mind, we turn to our second set of model analyses, which address the reviewers’ comments more directly. These assess the effects of manipulating leak and decision bound on task performance, and in particular on the integration kernels in our task. (See response to reviewer 1, below, for why we decided to principally focus on these two parameters rather than also considering changes in gain).</p><p>Our principal finding from this analysis was the (counterintuitive) finding that variation in decision threshold<italic>,</italic> rather than variation in leak, was the principal controller of the decay time constant t of the ‘evidence integration’ kernels that we obtain behaviourally. We demonstrate this in a new Figure 5 which shows the effect of independently manipulating these three parameters on the recovered evidence integration kernels from the model.</p><p>This finding was a surprise to us (but perhaps not the reviewers, given their comments above). After all, it is the leak (not the threshold) that affects the internal decay time constant of the accumulator, so why does the threshold (not the leak) primarily affect the recovered integration kernels in this way?</p><p>The answer lies in the fact that the data that enters the integration kernels is itself a ‘thresholded’ process. Only when the model reaches threshold and a response is emitted does the evidence preceding that response enter the integration kernel analysis. As such, an elevated threshold will require a more sustained (over time) level of sensory evidence to emit a response, and so give rise to the effects on integration kernels shown above.</p><p>We thank the reviewers for clarifying this important point in our analysis. We hope that the additional modelling section that we have added to the paper (which we do not reproduce in full here, due to length) fairly reflects the results of this attempt at modelling our data. We think that it adds substantially to the paper, and the resulting conclusion (the key result is likely a change in threshold, rather than/in addition to any change leak) is indeed more consistent with other features of the data. For example, the greater negative-going potential immediately prior to a button press (Figure 7b) is consistent with a higher threshold during rare vs. frequent trials; the reduced detection rate and increased reaction times seen in these trials is also consistent (Figure 2b/c).</p><p>Nonetheless, completely disentangling the effects of threshold and leak is a thorny issue, as adaptations in one parameter can be compensated by a change in the other parameter, as shown in new Figure 4 —figure supplement 2 in the modelling section. To <italic>truly</italic> derive integration kernels in which the leak of evidence is more directly accessible to the researcher, we suggest that it may be necessary to design tasks in which the response outputs are not binary when a threshold is reached, but instead where the subject continuously reports their current estimates of the decision variable (Huk et al., 2018; Straub and Rothkopf, 2021). This problem is in fact directly analogous to a previous set of ideas in reinforcement learning, in which it was established that the learning rate (»leak) and softmax (»threshold) parameters are surprisingly difficult to disambiguate from one another in binary choice tasks (Nassar and Gold, 2013), and it was instead suggested to design tasks where the learning rate was reported directly (Nassar et al., 2012; O’Reilly et al., 2013).</p><disp-quote content-type="editor-comment"><p>2) The reviewers raise questions regarding the nature of the decision required by the particular task implemented by the authors. The authors' approach relies on the assumption that participants are simply integrating coherent motion as they might for a standard variant of the random dot motion task. However on this task, targets were actually defined not by the presence of coherent motion but by a criterion duration of coherent motion, during which time the frame-to-frame coherent motion variance diminished. Can the authors rule out the possibility that decisions were partly or entirely based on detecting changes in stimulus variance and or coherent motion duration? If not, can it still be assumed that the rare and frequent targets were matched in terms of the ease with which they could be distinguished from the intervening noise intervals?</p></disp-quote><p>We thank the reviewers for raising this important point. As reviewer #2 correctly infers below, we reduced the variance during the response periods to avoid hitting the edges of the normal distribution too often (as the distribution is truncated at [-1,+1]). We have now made this more explicit in the methods section:</p><p>“The level of motion coherence at each step was sampled randomly from a normal distribution. The mean of this normal distribution depends on whether the step occurred during baseline or a response period. During a baseline period the mean of the normal distribution is 0. That means it is equally likely that negative or positive coherences are drawn. During response periods, the mean of the normal distribution was sampled uniformly from the set [-0.5, -0.4, -0.3, 0.3, 0.4, 0.5]. Any samples that exceeded 100% motion were set to be [+1,-1]. To limit the number of times this occurred, we set the standard deviation of the distribution to 0.3 for response periods and 0.5 for baseline periods. (We note that this could allow a strategy of tracking changes in the variance in the stimulus as well as the mean, something that we address in Figure 2 —figure supplement 1).”</p><p>We hope that this clarifies why the variance changed between response and baseline periods. We now consider whether participants might track changes in stimulus variance and how this might affect our results.</p><p>Let us address the final question from the reviewers’ comment first. The rare and frequent targets are indeed matched in terms of the ease with which they can be distinguished from the intervening noise intervals. To confirm this, we directly calculated the variance (across frames) of the motion coherence presented during baseline periods and response periods (until response) in all four conditions:</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>The average empirical standard deviation of the stimulus stream presented during each baseline period (‘baseline’) and response period (‘trial’), separated by each of the four conditions (F = frequent response periods, R = rare, L = long response periods, S = short).</title><p>Data were averaged across all response/baseline periods within the stimuli presented to each participant (each dot = 1 participant). Note that the standard deviation shown here is the standard deviation of motion coherence across frames of sensory evidence. This is smaller than the standard deviation of the generative distribution of ‘step’-changes in the motion coherence (std = 0.5 for baseline and 0.3 for response periods), because motion coherence remains constant for a period after each ‘step’ occurs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-sa2-fig1-v1.tif"/></fig><p>The empirical standard deviation of the stimulus streams is well matched across the four conditions. As such, rare and frequent trials are equivalently difficult to distinguish from noise based on their variance. This means that any effects that we see across the four conditions are unlikely to be due to differences in low-level stimulus statistics.</p><p>However, the stimulus does substantially reduce its variance during response periods, as was part of the stimulus generation procedure. The reviewers therefore raise the important question about whether participants could use a strategy of variance tracking across time.</p><p>To explore this, we first considered an analogous analysis to the ‘integration kernels’ approach that we previously used to analyse how participants tracked changes in the mean in the experiment. We calculated a local running estimate of the variance of the previous 200 frames (2 seconds) of sensory evidence at each timepoint in the experiment. We then epoched and averaged this local estimate of variance immediately prior to each ‘false alarm’ response.</p><p>Using this approach, we found that the average variance of the stimulus stream decreased prior to false alarms emitted by the participants. This implies that participants did partially use variance tracking as a strategy for solving the task:</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Integration kernels for stimulus variance, timelocked to false alarms made by participants in each of the four conditions.</title><p>Each line shows mean +/- s.e.m. (across participants) of the integration kernel for stimulus variance; in all four conditions, the reduction instimulus variance prior to a false alarm indicates that participants were likely performing stimulus detection in part using information about stimulus variance as well as stimulus mean (main Figures 2d, 3b).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-sa2-fig2-v1.tif"/></fig><p>To confirm these effects weren’t artifacts, we also considered whether these kernels might occur because of the truncation of the normal distribution at [-1, +1]. (I.e. as the mean of the stimulus increases prior to a response, the variance may also potentially decrease as more samples may have been drawn from truncated values). To rule this out, we repeated this analysis using simulated date from the Ornstein-Uhlenbeck model of evidence accumulation explicated above. This model only tracks the mean of the stimulus by definition, and not its variance:</p><fig id="sa2fig3" position="float"><label>Author response image 3.</label><caption><title>The same analysis as in Reviewer Response Figure 2, but now performed on simulated data from the Ornstein-Uhlenbeck model rather than participant behaviour.</title><p>The absence of any integration kernel confirms that the results in the previous figure are not artifactual.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82823-sa2-fig3-v1.tif"/></fig><p>The absence of any integration kernel here confirms that the analysis is not prone to artifacts. We conclude that participants’ decisions are therefore at least <italic>partially</italic> based on detecting changes in stimulus variance across time, as well as changes in stimulus mean.</p><p>Given these findings, we next considered the <italic>relative</italic> contributions of mean and variance to participants’ decision-making. We performed logistic mixed effects modelling of detection probability during response periods, including both the mean <italic>and</italic> variance of the stimulus as co-regressors. We show these new results in the main paper as a new Figure 2 —figure supplement 1.</p><p>Note that this is essentially an extension of the analysis originally reported in main figure 2b, where we observed main effects of mean coherence, response period frequency and length on detection probability. We therefore included response period frequency and length as coregressors, along with relevant interaction terms.</p><p>We have described these results in the text below:</p><p>“We also considered an alternative stimulus detection strategy, of changes in stimulus variance across time rather than changes in stimulus mean. This hypothesis relied upon the fact that response periods had smaller standard deviations in the gaussian noise distribution than baseline periods – a stimulus feature that we introduced to avoid excessive samples of ‘maximal’ (100%) motion coherence when the mean was non-zero. To test whether the variance of the stimulus might also affect participants’ detection, we performed a logistic mixed effects model on participants’ responses (Figure 2 —figure supplement 1). Detection probability was the dependent variable, and mean motion coherence, variance of motion coherence, response period frequency and length were independent variables, along with interaction terms. We found that stimulus variance during response periods did indeed impact detection probability; response periods with a higher variance in motion coherence were less likely to be detected. Crucially, however, the main effects of mean motion coherence, trial frequency and trial length (equivalent to the effects plotted in main Figure 2b) were left unaffected by the inclusion of this coregressor.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>My principle suggestion would be that the authors consider running simulations of alternative models to see if they can reproduce the current behavioural trends. Drift rate adjustment to Rare vs Frequent target conditions is an obvious candidate.</p><p>In general, I found it hard to marry up all the different behavioural/kernel/EEG findings. I recommend the authors examine whether leak adjustment really provides a comprehensive account of their data or if additional/alternative parameter adjustments must be invoked to reconcile their data.</p><p>Reviewer #2 (Recommendations for the authors):</p><p>1) While I don't think it is problematic, the authors should acknowledge (even if just in methods) that subjects might be able to use the stimulus variance to identify response periods. Also, strictly speaking, the description of the noise as following a normal distribution cannot be correct because it is limited to the range of [-1,1]. I presume this also explains why the authors reduced the standard deviation in the response periods – to avoid hitting those edges too often.</p></disp-quote><p>The reviewer is correct on both counts. We have now acknowledged this in the methods section, as suggested.</p><p>“The level of motion coherence at each step was sampled randomly from a normal distribution. The mean of this normal distribution depends on whether the step occurred during baseline or a response period. During a baseline period the mean of the normal distribution is 0. That means it is equally likely that negative or positive coherences are drawn. During response periods, the mean of the normal distribution was sampled uniformly from the set [-0.5, -0.4, -0.3, 0.3, 0.4, 0.5]. Any samples that exceeded 100% motion were set to be [+1,-1]. To limit the number of times this occurred, we set the standard deviation of the distribution to 0.3 for response periods and 0.5 for baseline periods. (We note that this could allow a strategy of tracking changes in the variance in the stimulus as well as the mean, something that we address in Figure 2 —figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>2) Figure 1d is very challenging to interpret. I appreciate that the authors fully acknowledge that it involves signal and noise. However, I think this introduces so many confounds to that analysis that I'm not sure you can say much with confidence from it. Most obviously, the dynamics will reflect the RT differences and the associated shift in the onset of the signal contribution. Also, because it seems to be based on all coherences together, the signal contribution will critically depend on the relative proportion of each coherence in each condition. I'm not sure there is any way to disentangle all of that.</p></disp-quote><p>This is a fair criticism. Nevertheless, we would be keen to leave this analysis in – not because of the <italic>presence</italic> of the difference between FREQUENT and RARE trials, but because of the <italic>absence</italic> of any difference between LONG and SHORT trials. As mentioned in the text, our original hypothesis was that participants might integrate longer in LONG than in SHORT, and this appears not to be the case:</p><p>“Surprisingly, the sensory evidence integration properties (i.e. the ‘integration kernels’, calculated by averaging the signal prior to the decision) were not affected by length of response periods (Figure 2d). This ran contrary to our initial hypothesis that participants would integrate evidence for longer when response periods were LONG. We suggest that this may result from the manipulation of response period duration being relatively small (3s vs. 5s) compared to the manipulation of response period frequency….”</p><p>The reviewer could counter that we draw the same conclusions from our integration kernels from false alarms – so why include a potentially confounded analysis at all? A potential concern is that there are generally far fewer false alarms than correct responses, and so the absence of any difference in LONG vs. SHORT in figure 3b/Figure 3 —figure supplement 1a could potentially be due to lack of SNR. So, we prefer to keep in the response period analysis, just as further evidence that the kernels really appear the same on LONG vs. SHORT.</p><p>We have, however, added additional text to further clarify how the analysis has the potential to suffer from confounds, and crucially that the subsequent false alarm analyses (which show the same direction of effects) are not affected by these confounds:</p><p>“…We also note that the significant difference between FREQUENT and RARE trials in Figure 2d should not be over-interpreted, as it could be influenced by RT differences (Figure 2c) and the associated shift in the onset of the signal contribution, and/or the difference in average coherence detection across conditions (Figure 2b). Importantly, we control for these confounds below, by examining the integration kernels to false alarms (in the absence of changes in mean signal).”</p><disp-quote content-type="editor-comment"><p>3) I wasn't quite sure how false alarm rate was calculated in Figure 3. Conditions differ in the total time for every run that subjects were in the baseline state where they could have a false alarm. Is the denominator of this rate calculation total task time or total time where subjects could possibly have a false alarm. I think the latter is the most useful comparison. I would guess the authors did that, but I couldn't find the description.</p></disp-quote><p>The reviewer is correct, the denominator here is total time where subjects could possibly have a false alarm (i.e. total time in baseline periods). We have added the following clarification to the methods:</p><p>“False alarm rates. To calculate false alarm rates (main Figure 3), we counted the total number of responses made during baseline periods, and divided this by the total amount of time where subjects could possibly have made a false alarm (i.e. total time spent in baseline periods). We repeated this separately for each of the four conditions within each participant.”</p><disp-quote content-type="editor-comment"><p>4) It would be useful to have a figure showing individual subject CPP effects shown in aggregate in Figure 5? A scatter plot comparing effect size for FREQUENT and RARE would be a good way to illustrate that.</p></disp-quote><p>Thanks for the suggestion! We have now included such a plot in the manuscript. To also facilitate comparison of the effects between FREQUENT/RARE and SHORT/LONG, we decided that it was easiest to include this as an additional figure supplement (new Figure 7 —figure supplement 2).</p><disp-quote content-type="editor-comment"><p>5) In the discussion, the authors state that initial simulations suggest that a range of decay time constants could be optimal. This may provide another explanation for the lack of change in time constant in different conditions. If a range are possibly optimal for any given condition, perhaps the subjects can remain close to optimal in this task without changing their decay time constant between conditions.</p></disp-quote><p>Yes – as described above, we have now explored this much more fully by simulating the O-U model and showing the range of parameter values where subjects can remain close to optimal. Although there are clear differences between conditions in terms of the exact parameters that are optimal between conditions, there is also a ‘ridge’ in parameter space where performance is close to optimal (see new main Figure 4 and Figure 4 —figure supplement 1). This may explain why between-participant variability exceeds between-condition variability in task performance.</p><disp-quote content-type="editor-comment"><p>6) I noticed two typos/formatting errors in the methods. In the subscripts for the regression, a &quot;D&quot; was shown for the &quot;δ&quot; symbol. Also, the words &quot;regressed out&quot; are duplicated in EEG pre-processing section.</p></disp-quote><p>Thanks – corrected.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>General notes:</p><p>– Figure captions should describe what is being shown, with a link to the method and/or supplementary material for lengthy computations.</p></disp-quote><p>We have gone through all the figure captions, and amended these/added links to the methods/supplementary material wherever appropriate.</p><disp-quote content-type="editor-comment"><p>– Throughout the text, the authors sometimes refer to 'net motion', 'coherence', and 'evidence'. I would make sure to use the same terminology.</p></disp-quote><p>Thanks for this comment. In general, we try to use ‘sensory evidence’ or ‘evidence’ whenever we are making a general point about decision making via evidence accumulation across time, but use ‘motion coherence’ when we are referring specifically to our experimental paradigm. So, for example, we discuss evidence accumulation in general terms in the introduction, so we only discuss ‘sensory evidence’ there, but we start using the term ‘motion coherence’ at the beginning of the Results section where we first introduce our random dot kinetogram task. We have now made this transition in terminology more explicit in the text:</p><p>“Subjects continuously monitored a stream of time-varying sensory evidence (hereafter referred to as ‘motion coherence’) for blocks of five minutes…”</p><disp-quote content-type="editor-comment"><p>– Some analyses are missing, e.g. the proportion of long answers that have been ignored (that could indicate a level of engagement in the task).</p></disp-quote><p>We are not quite sure what the reviewer is referring to here – but we suspect that they mean the proportion of ‘missed response periods’ (i.e. responses that should have been made, but were not)? If so, this is simply 1 minus the ‘correct detection rate’, which is plotted in figure 2b. This is plotted separately for the four conditions, and the different levels of motion coherence, and so the ‘proportion of long answers that have been ignored’ can be read off directly from this graph.</p><disp-quote content-type="editor-comment"><p>Detailed comments/suggestions:</p><p>Task design:</p><p>– Methods: are subjects instructed about what the dots mean in advance? How long do the dots stay colored for during feedback?</p></disp-quote><p>We have added to the introduction:</p><p>“Feedback was presented by changing the colour of the central fixation point for 500ms (Figure 1a), and they were trained on the meaning of these colours as part of extensive pre-experiment training (see methods).”</p><p>And to the methods:</p><p>“Note that during this time the colours of the fixation dot feedback were the same as in the main experiment, and participants were instructed about the meaning of these dots.”</p><disp-quote content-type="editor-comment"><p>– How much do participants get in total? The total bar is 30 points and they get only 50 cts to have 15 points which are after 5 good responses without error on average. Are the feedback distracting the participants? Do the authors think that the money aspect here provides a substantial motivation for participants?</p></disp-quote><p>The exact pay-out will of course vary somewhat between blocks and participants. However, the average returns can be inferred from <italic>p,</italic> the probability of correct response period detection (Figure 2b); <italic>n</italic>, the number of response periods in a block (Figure 2a); and FA, the false alarm rate during non-response periods (Figure 3a).</p><p>For example, in a typical block with response periods LONG and FREQUENT, where <italic>N=30, p=0.9</italic> and <italic>FA = 2</italic> responses/min, then the total points earned would be:</p><p>Where <italic>n*p*3</italic> reflects the total reward for correct responses, -1.5*(1-p)*n is the total penalty for missed responses, and -1.5 *5*FA is the total penalty for 5 minutes’ worth of false alarms. In this block, the participant would have earned (61.5/15) * 50 = £2.05, which is quite a high incentive for a five-minute task. We note that this is the condition with the highest pay-out, but the average pay-out across all four blocks (when combined across six repetitions of each block) was approximately £22. We therefore suspect that our participants were well motivated by this bonus pay-out.</p><p>To address any concerns about the reward feedback distracting the participants, we deliberately showed the reward bar only during training, and not during task performance. While playing the task during EEG recording, participants knew that the coloured dots feedback would translate into the money bar, but they were only shown their overall performance at the end of each block. We have amended the methods section to reflect this:</p><p>“A reward bar was shown at the end of each 5-minute block to indicate how many points participants have won in total (the reward bar was shown continuously onscreen during training<bold>,</bold> <italic>but not during task performance to avoid distraction</italic>).”</p><disp-quote content-type="editor-comment"><p>– Figure 1c:&quot; The resulting evidence integration kernel is well described by an exponential decay function, whose decay time constant in seconds is controlled by the free parameter tau &quot;: As formulated, the caption hints that the figure analyses how well described is the kernel. As I understand, the exponential kernel is a choice that is imposed by the authors, and how well it fits the coherence level before a decision is not addressed in the paper. I would reformulate and clearly state that the authors define an exponential kernel from the past coherence levels. More details on the definition of this kernel are required, and statistics about how well it fits the data would enable stronger claims of its use throughout the text. In particular, the average coherence can take 3 values (30, 40, or 50), are the authors averaging for all coherence levels together? Should A be correlated with those values? What is the effect of the mean coherence level during response periods?</p></disp-quote><p>Thanks for these comments.</p><p>We should first note that although we plot integration kernels for both responses made during response periods (Figure 2d) and ‘false alarms’ (Figure 1c example, Figure 3b, Figure 3 —figure supplement 1), the exponential decay kernel is in fact only fit to ‘false alarm’ responses. The main reason for this is that the integration kernels from response periods include a shift in the mean coherence at some point between t=-5s and t=0s; this gives rise to the ‘shoulder’ in the integration kernel that can be seen in Figure 2d, and so this is not so well described by an exponential decay. We address the reviewer’s other point (about different levels of mean coherence entering into Figure 2d).</p><p>Although we had stated this point later in the original submission, we can see that it is not so clear from the very first place where we introduce the kernels in Figure 1c. We have therefore clarified it there as well:</p><p>“The resulting evidence integration kernel for false alarms is well described by an exponential decay function…”</p><p>And to the main text:</p><p>“We performed this reverse correlation for both false alarm responses (example shown in Figure 1c; these responses are well described by an exponential decay function detailed below) and correct responses.”</p><p>We also agree with the reviewer that the exponential kernel is a choice imposed by us.</p><p>However, we note that it is a theoretically-motivated choice, rather than an arbitrary one. This is because the ‘leak’ term in a leaky evidence accumulation model implies an exponential decay of previously weighted evidence across time. We have clarified this in the text:</p><p>“We note that this exponential decay model is theoretically motivated by the leaky evidence accumulation model, which implies that past evidence will leak from the accumulator with an exponential decay (Bogacz et al., 2006).”</p><p>Nevertheless, we agree that it would be worth including more statistics on how well this model fits the data, to complement the qualitative example fits that are shown in Figure 3 —figure supplement 1. To calculate the quality of the model fit, we therefore now report the <italic>R<sup>2</sup></italic> value for the exponential decay function. We have added to the methods:</p><p>“We then fit A and τ to the empirical integration kernel for all timepoints up to and including t=0 using <italic>fminsearch</italic> in MATLAB, using a least squares cost function between the fitted model and data with an L2 regularisation term that penalised large values of either A or τ (l = 0.01). To calculate the quality of the model fit, we calculated R<sup>2</sup> for this function: <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>RSS</mml:mtext><mml:mtext>TSS</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> with RSS being the Residual Sum of Squares after model fitting, and TSS being the Total Sum of Squares.”</p><p>And we have added this to the main text:</p><p>“Our exponential decay model provided a good fit to data at a single subject level (median R2=0.82, 95% confidence intervals for R2 = [0.42,0.93]; see Figure 3 —figure supplement 1 for example fits)…”</p><p>Our claim that the exponential decay provides a good fit to the integration kernels from false alarms is therefore now justified by the fact that it explains (on average) 82% of the variance in these integration kernels.</p><disp-quote content-type="editor-comment"><p>P7, Figure 2:</p><p>– b and c: is that the mean or median (for b) and the error bar the std/95expectiles (for c) of participants' detection rate/RT?</p></disp-quote><p>As was already noted in the figure legend, both plots show mean +/- s.e across participants. We have added an extra subclause to the figure legend to make this even clearer:</p><p>“All plots in (b), (c) and (d) show mean +/- s.e. across participants.”</p><disp-quote content-type="editor-comment"><p>Is the RT tau?</p></disp-quote><p>No, RT and tau are quite different from one another. Tau relates to the fit of the exponential decay function as mentioned above, this is only calculated for false alarms, not for response periods. RT is the more standard measure of reaction time, i.e. time taken to respond during the response period after the mean coherence has changed from 0. We clarify this in the legend:</p><p>“Median reaction time (time taken to respond after start of response period) for successfully reported response periods.”</p><p>Note that the ‘median reaction time’ here refers to the median RT <italic>within-subject</italic> (we take the median as RTs are typically not normally distributed); as noted above, we then plot the mean +/- s.e. <italic>across</italic> subjects.</p><disp-quote content-type="editor-comment"><p>– d: is the integration kernel here referring to the exponential, or simply the average signal before the decision, averaged across all participants?</p></disp-quote><p>This is simply the average signal before the decision. We have clarified that we are only fitting the exponential decay model to ‘false alarms’ (see changes to text made above), and we also have added the following clarification:</p><p>“the sensory evidence integration properties (i.e. the ‘integration kernels’, calculated by averaging the signal prior to the decision)…”</p><disp-quote content-type="editor-comment"><p>– d again: why is the maximum coherence 0.5 here? is that only for switches from 0 to 0.5 coherence? What happens for switches from 0 to 30 coherence?</p></disp-quote><p>No, this plot <italic>collapses</italic> across all mean motion coherences (0.3, 0.4, and 0.5). We have now clarified this in the text:</p><p>“…the ‘integration kernels’, calculated by averaging the signal prior to the decision, collapsing across all levels of mean motion coherence…”</p><p>We adopt this approach because the participant is of course unaware of the level of motion coherence in the stimulus, and also the stimulus is also corrupted by noise (so in a 0.3 coherence response period, the average motion coherence can be higher than this due to noise).</p><p>In fact, it appears highly likely that the threshold shown in Figure 2d is the same used across all types of trial. This is because the same threshold is also reached in Figure 3b – responses during the <italic>noise</italic> periods, when the average motion coherence is zero.</p><disp-quote content-type="editor-comment"><p>– d again: the authors observe no effect on the response window duration on the response kernel, but have applied a threshold on the duration of the response on the long response period in order to make this plot. What happens if the authors include all response lengths? (also applies to the discussion p17 and 18).</p></disp-quote><p>If we were to include all response lengths, this does indeed give a difference in response window duration on response kernels. (If interested, we refer the reviewer to chapter 3, figure 8 of (Ruesseler, 2021), and also the preceding chapter in this doctoral dissertation). But the problem with this approach, expanded upon more extensively in simulation in this dissertation, is that it can be an entirely artefactual consequence of the change in mean signal at different latencies (due to the difference in RTs between conditions).</p><p>We therefore think that it would be misleading to include such a plot in the manuscript, and we note that reviewer #2 shares these concerns. As discussed above, the cleanest test of any difference between conditions has to be Figure 3b rather than Figure 2d, because the statistics of the stimulus are exactly matched between conditions in the noise periods rather than the response periods. This figure also shows a difference due to manipulations of response period frequency, but not response period length.</p><disp-quote content-type="editor-comment"><p>P 8:</p><p>– &quot;This provides further evidence that participants were overall more conservative in their responses in LONG conditions than SHORT&quot; can the authors define what conservative means?</p></disp-quote><p>Conservative here means that for an equivalent level of sensory evidence, the participants were less likely to make a response. We can make this claim because the statistics of the noise periods are matched between LONG and SHORT conditions, but participants are less likely to false alarm in the LONG condition. We have now added to the text:</p><p>“This provides further evidence that participants were overall more conservative in their responses in LONG conditions than SHORT. (In other words, for an equivalent level of sensory evidence, the participants were less likely to make a response.)”</p><p>This is similar to the finding in response periods that participants have longer RTs in LONG than in SHORT, when matching for levels of sensory evidence (another point where we state that participants are more ‘conservative’).</p><disp-quote content-type="editor-comment"><p>– It is not surprising that the integration kernels are not affected by the length of the response period if the long responses are not considered.</p></disp-quote><p>Please see answer above. It would be entirely possible for this analysis to show an effect of response periods on integration kernels even without long responses being considered. This would also show up in the analysis in Figure 3b as well. This is something that we have now explored more extensively in simulation, in particular the new section on computational modelling of the Ornstein-Uhlenbeck model.</p><disp-quote content-type="editor-comment"><p>– I think the kernel analysis during baseline is necessary to show that participants are not randomly making motor movements, however, the authors could describe in a bit more detail the difference between a correct response and a false alarm: how are the integration kernels different in the baseline vs response period? (the kernels for false alarms seem a bit shorter?)</p></disp-quote><p>This is a good point. There are two main differences.</p><p>First, as the reviewer mentions, the kernels appear a bit shorter. This is because they are not in any way confounded by the inclusion of changes in the mean motion coherence in the analysis (see response to reviewer #2 above). Again, this means that the kernel analysis in Figure 3b is a ‘cleaner’ estimate of evidence integration in participants – which is why we fit kernels to this, use this in all our simulations, and focus on between-subject variability in these noise integration kernels.</p><p>The second difference is that right before the decision, the noise integration kernels drop towards zero, whereas the response period integration kernels stay elevated. This is because of non-decision time in the decision process. In Figure 3b, we can see that the motion coherence immediately prior to the response has less influence on the decision, because of the time needed to translate the decision into a response. In Figure 2d, by contrast, the integration kernel stays high, and this is because this time period includes the shift in average motion (i.e. the <italic>mean</italic> motion coherence is non-zero).</p><p>We have briefly mentioned these two main differences in the revision:</p><p>“The slight differences in integration kernels between Figure 3b and Figure 2d (shorter duration, and return to baseline close to the response) are due to the inclusion of the average motion signal in Figure 3b, rather than just the noise.”</p><disp-quote content-type="editor-comment"><p>– When do false alarms occur compared to the response periods? Are they regularly after the end of a response period?</p></disp-quote><p>False alarms cannot occur immediately after the end of a response period, because (as mentioned in the methods) any responses made in the 500ms after a response period were actually counted as correct responses (to allow for non-decision time in the decision process). They are also very unlikely to follow immediately after this, because the participant then gets feedback that they have missed a response, and so if anything they typically withhold responses in the period that immediately follows.</p><disp-quote content-type="editor-comment"><p>What is the pattern of the coherence that causes those false alarms? Are they always when the frame update is 1s, for example?</p></disp-quote><p>We are not sure what the reviewer means here.</p><p>The most obvious conclusion about the pattern of coherence that causes false alarms can be drawn from the evidence integration kernels from the noise periods, Figure 3b.</p><p>It is possible that the reviewer is asking whether responses are more likely after an intersample interval of maximum length, i.e. 1000ms. While this is an interesting idea, there are several reasons why this is very unlikely, and also difficult to analyse. Firstly, remember that the <italic>frame</italic> update is in fact constant throughout the entire experiment (100Hz) – the updates shown in Figure 1b are just when the underlying motion coherence changes. Secondly, the actual percept experienced by the subject is not just a function of signal plus experimenter noise (as shown in Figure 1b) – it will also have the addition of <italic>perceptual</italic> noise on top of this, which will presumably be constant throughout the entire experiment. Finally, even if the reviewer were correct, it would be difficult to make such a claim, due to the uncertain contribution of non-decision time in each participants’ responses (e.g. Figure 3b) – meaning that would be difficult to know exactly <italic>which</italic> of the inter-sample intervals preceding the response should be included in the analysis.</p><p>In short, although we can see that this is an interesting idea, we think that the most clear representation of the pattern of coherence that causes false alarms can already be observed from Figure 3b (and associated Figures).</p><disp-quote content-type="editor-comment"><p>Figure 3:</p><p>a) the authors mention significance but do not provide p values.</p></disp-quote><p>We previously provided an F-statistic and associated p-value in the main text:</p><p>F(1,23)=58.67, p=8.98*10<sup>-8</sup>. We have now copied this into the figure legend.</p><disp-quote content-type="editor-comment"><p>Why is it consistent with having a more cautious response threshold? I'd favor the reverse interpretation that short response periods induce more confusion between signal and noise.</p></disp-quote><p>Thanks for suggesting this alternative interpretation. We have added this idea to figure legend:</p><p>“This is consistent with having a more cautious response threshold (also evidenced by longer reaction times during response periods, main Figure 3c), although could also be interpreted as shorter response periods inducing more confusion between signal and noise.”</p><disp-quote content-type="editor-comment"><p>b) is that the averaged signal before false alarms?</p></disp-quote><p>Yes.</p><disp-quote content-type="editor-comment"><p>Do they always occur when the mean average coherence reaches 0.5?</p></disp-quote><p>No, this is the average across many false alarms (which will have differing values).</p><disp-quote content-type="editor-comment"><p>Why is the maximum value 0.5?</p></disp-quote><p>This is just a consequence of the behaviour of the subjects – it shows on average the strength of signal that was needed to trigger a response (and is entirely consistent with what is found in response periods, Figure 2d). As shown in our new simulations of the Ornstein-Uhlenbeck process, there is no <italic>a priori</italic> reason why the maximum value has to be 0.5, as it will depend upon the internal parameters of the decision process (and indeed, it varies across participants, as can be seen in Figure 3 —figure supplement 1).</p><disp-quote content-type="editor-comment"><p>c) explain what the points show and how these are computed.</p></disp-quote><p>We have now added to the figure legend:</p><p>“The data points show the time constant, τ , for each participant after fitting a model of exponential decay to the integration kernel. The equation for this kernel is in the main text, and details of kernel fitting are provided in Methods.”</p><disp-quote content-type="editor-comment"><p>P.11</p><p>(ii) To me, it is not clear that the behavioral evidence from 'false alarms' indicate that participants are still integrating sensory evidence – a stricter definition and existence of the kernel (how well it fits for every response) could help support this statement</p></disp-quote><p>While we agree with the spirit of this comment, it would be very difficult to measure exactly how the kernel fits for every single response, without a clear articulation of what the control model might be in this comparison. We have, however, considered elsewhere what impact a strategy like ‘peak detection’ might have on the resulting evidence integration kernels, and how this would compare with what we observe in the data. If the reviewer is interested, this can be examined by looking at chapter 3, figure 4 of (Ruesseler, 2021), which shows that such a strategy elicits integration kernels that look quite different from those we observe in the data.</p><disp-quote content-type="editor-comment"><p>– Is 'net motion' coherence here?</p></disp-quote><p>Yes – thanks, we have changed this to align it with other mentions of ‘coherence’ elsewhere in the text.</p><disp-quote content-type="editor-comment"><p>– On the same page – the regressor 'absoluted sensory evidence' has not been named like this before.</p></disp-quote><p>Thanks – we have renamed this to |evidence|, to make it clearer that we are referring to the same regressor as introduced earlier on the same page.</p><disp-quote content-type="editor-comment"><p>P. 12:</p><p>– Why is it consistent with the behavioral finding of lower detection rate and slower RT in rare conditions?</p></disp-quote><p>We have now expanded on this conclusion, and made clear how it relates to the findings in our computational modelling section above:</p><p>“In addition, when making a correct response, preparatory motor activity over central electrodes reached a larger decision threshold for RARE vs. FREQUENT response periods (Figure 7b; p=0.041, cluster-based permutation test). We found similar effects in β-band desynchronisation prior, averaged over the same electrodes; β desynchronisation was greater in RARE than FREQUENT response periods. As discussed in the computational modelling section above, this is consistent with the changes in integration kernels between these conditions as it may reflect a change in decision threshold (Figure 2d, 3c/d). It is also consistent with the lower detection rates and slower reaction times when response periods are RARE (Figure 2 b/c), which also imply a higher response threshold.”</p><disp-quote content-type="editor-comment"><p>– I understand the analysis is done regardless of the response. What about missed response periods/responses during the baseline?</p></disp-quote><p>As discussed in the methods, we include regressors for responses during the baseline, to ensure that we separately model out the EEG response to making a buttonpress during this period. Missed response periods are less relevant, because there is no additional event to model out of the EEG data (as there is no response being made).</p><disp-quote content-type="editor-comment"><p>Figure 4:</p><p>– The caption is incomplete, what does the right-hand side show? Is it one weight for one regressor across time for one example electrode?</p></disp-quote><p>Thanks – we have now clarified the caption:</p><p>“This leads to a set of temporal response functions for each regressor at each sensor, shown on the right-hand side of the figure. The timecourse for each regressor shows the average regression weights at the three sensors highlighted with triangles on the scalp topography.”</p><disp-quote content-type="editor-comment"><p>– What is the goodness of fit of the regression?</p></disp-quote><p>In this case, a measure of overall goodness-of-fit of the regression (such as R<sup>2</sup>) is unlikely to be a useful measure, and will be very low – this is simply because the residuals (unexplained variance) in our model will remain very large at a single subject level (simply due to the low SNR of EEG data, and the contribution of many other factors to neural responses). For this reason, the goodness-of-fit of lower-level GLMs is rarely considered in either convolutional (or conventional ERP) studies of EEG data, or standard event-related analysis of fMRI data. Instead, the conventional way to assess reliability of the regression model fit is to assess the consistency of the β coefficients across participants, which we do by plotting the error bars (across participants) e.g. in Figures 7, Figure 7 —figure supplement 1, Figure 8.</p><disp-quote content-type="editor-comment"><p>P13 – Figure 5:</p><p>– What are the lines showing? What is the black thick line on top?</p></disp-quote><p>Apologies, we should have been clearer in this figure legend. The thick black line on top is in fact a large number of * symbols super-imposed. We have now added:</p><p>“Lines and error bars show mean +/- s.e.m. across participants. * (solid black line at top of figure) denotes significant difference between FREQUENT and RARE (p&lt;0.05, cluster corrected for multiple comparisons across time).”</p><disp-quote content-type="editor-comment"><p>– P.14 To test whether these signals are decision-related, the authors could also compare the results of the regression when false alarms are made and when the response period is missed.</p></disp-quote><p>We found qualitatively similar results for false alarms, but the results were somewhat noisier (simply because there are more responses made during response periods than during false alarms). We now mention this in the text.</p><disp-quote content-type="editor-comment"><p>– Figure 6: has the effect of previous exposure been controlled for the vertical motion? Could the authors discuss whether this would have an impact on the CPP.</p></disp-quote><p>During training for the combined horizontal/vertical motion task, participants were exposed to both horizontal and vertical motion in the training stimuli. We therefore consider this to be an unlikely explanation for the differences shown in Figure 6 (now figure 8).</p><disp-quote content-type="editor-comment"><p>– P 15: could the authors give more details on the behavioral correlate and how is it computed.</p></disp-quote><p>Certainly. We have added a sentence to clarify where this behavioural correlate is coming from:</p><p>“We therefore performed a behavioural-neural correlation between participants’ integration time constants τ and their TRFs to sensory noise fluctuations. (Note that the integration time constants were fit using the equation described above, fit (using the approach described in methods) separately to the empirical integration kernels from each of the four conditions).”</p><disp-quote content-type="editor-comment"><p>– &quot;this negative-going component was larger in amplitude (i.e. more negative) in participants who would integrate sensory evidence over longer durations (i.e. had a higher value of τ).&quot;: could the authors elaborate on how to interpret this finding with regard to existing literature?</p></disp-quote><p>We have added the following comment to the manuscript:</p><p>“We suggest that this may be consistent with variation in the encoding strength of previously studied correlates of continuous decision evidence. For example, Wyart et al. found a positive centroparietal potential 500ms after decision information that positively encoded the current sample, but negatively encoded adjacent samples (Wyart et al., 2012); our finding extends this work to explore variation in the response across participants.”</p><disp-quote content-type="editor-comment"><p>– Figure 7 P 16 spearman correlation: can the authors describe the top plot? what are the lines showing? if that's the mean it should be mentioned and the standard deviation should be shown.</p></disp-quote><p>Perhaps the reviewer missed it, but this was already described in the figure legend. The lines show Spearman’s rho (as also mentioned on the y-axis of the plot), not the mean.</p><p>“Top panel shows Spearman’s rank correlation between the time-varying EEG β for absolute sensory evidence and individual subjects’ τ parameter, separately for each of the four conditions.”</p><disp-quote content-type="editor-comment"><p>Can the authors describe the bottom plots? why are the authors using log(tau) for the x-axis?</p></disp-quote><p>We now clarify this in the figure legend.</p><p>“We plot the average EEG effect size against log(τ) to allow for a straight-line fit (lines show mean +/- 95% confidence intervals of a first-order polynomial fit between these two variables); we used Spearman’s rho to calculate the relationship, as it does not assume linearity.”</p><disp-quote content-type="editor-comment"><p>Discussion:</p><p>- p 17: 'The behavior is well described by a leaky evidence accumulator' the exponential kernel is a reverse correlation that the authors have done themselves, but they have not shown causality nor statistical analysis on what those kernels mean in relationship with the behavior.</p></disp-quote><p>In response to the reviewer’s earlier comment, we have now added statistics to justify our claim of the goodness of fit of the exponential kernel. We have reworded the sentence to remove the claim of it being an accumulator, and instead refer just to the kernel:</p><p>“We found that participants' behaviour was well described by an exponentially decaying integration kernel, and that participants adapted the properties of this process to the overall statistics of the sensory environment (Ossmy et al., 2013) across four different experimental conditions.”</p><disp-quote content-type="editor-comment"><p>Methods:</p><p>– In the regressor table, what is 'prediction error' regressor?</p></disp-quote><p>Thanks for spotting this – this had slipped through from an earlier naming convention that we used in the lab for these regressors, before we decided on a more precise set of names for them. We have now renamed these in the regressor table so that they are consistent with the rest of the paper.</p><p>We thank for reviewer again for the detailed feedback on our paper, and we hope that we have successfully addressed their comments and concerns with our revision.</p></body></sub-article></article>