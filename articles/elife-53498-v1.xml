<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53498</article-id><article-id pub-id-type="doi">10.7554/eLife.53498</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Meta-Research</subject></subj-group></article-categories><title-group><article-title>Dataset decay and the problem of sequential analyses on open datasets</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-158311"><name><surname>Thompson</surname><given-names>William Hedley</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0533-6035</contrib-id><email>william.thompson@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>William Hedley Thompson</bold> is in the Department of Psychology, Stanford University, Stanford, United States, and the Department of Clinical Neuroscience, Karolinska Institutet, Stockholm, Sweden</p></bio></contrib><contrib contrib-type="author" id="author-164460"><name><surname>Wright</surname><given-names>Jessey</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5003-0572</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Jessey Wright</bold> is in the Department of Psychology and the Department of Philosophy, Stanford University, Stanford, United States</p></bio></contrib><contrib contrib-type="author" id="author-139617"><name><surname>Bissett</surname><given-names>Patrick G</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Patrick G Bissett</bold> is in the Department of Psychology, Stanford University, Stanford, United States</p></bio></contrib><contrib contrib-type="author" id="author-27700"><name><surname>Poldrack</surname><given-names>Russell A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6755-0259</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Russell A Poldrack</bold> is in the Department of Psychology, Stanford University, Stanford, United States</p></bio></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Clinical Neuroscience, Karolinska Institutet</institution><addr-line><named-content content-type="city">Stockholm</named-content></addr-line><country>Sweden</country></aff><aff id="aff3"><label>3</label><institution>Department of Philosophy, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>19</day><month>05</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53498</elocation-id><history><date date-type="received" iso-8601-date="2019-11-12"><day>12</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-05-04"><day>04</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Thompson et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Thompson et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53498-v1.pdf"/><abstract><p>Open data allows researchers to explore pre-existing datasets in new ways. However, if many researchers reuse the same dataset, multiple statistical testing may increase false positives. Here we demonstrate that sequential hypothesis testing on the same dataset by multiple researchers can inflate error rates. We go on to discuss a number of correction procedures that can reduce the number of false positives, and the challenges associated with these correction procedures.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>open data</kwd><kwd>sequential testing</kwd><kwd>multiple comparisons</kwd><kwd>multiple comparison correction</kwd><kwd>meta-research</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004063</institution-id><institution>Knut och Alice Wallenbergs Stiftelse</institution></institution-wrap></funding-source><award-id>2016.0473</award-id><principal-award-recipient><name><surname>Thompson</surname><given-names>William Hedley</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Open data provides an opportunity to perform new analyses on preexisting data, but trade-offs are required to limit an increase in false positives.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In recent years, there has been a push to make the scientific datasets associated with published papers openly available to other researchers (<xref ref-type="bibr" rid="bib29">Nosek et al., 2015</xref>). Making data open will allow other researchers to both reproduce published analyses and ask new questions of existing datasets (<xref ref-type="bibr" rid="bib27">Molloy, 2011</xref>; <xref ref-type="bibr" rid="bib32">Pisani et al., 2016</xref>). The ability to explore pre-existing datasets in new ways should make research more efficient and has the potential to yield new discoveries (<xref ref-type="bibr" rid="bib47">Weston et al., 2019</xref>).</p><p>The availability of open datasets will increase over time as funders mandate and reward data sharing and other open research practices (<xref ref-type="bibr" rid="bib24">McKiernan et al., 2016</xref>). However, researchers re-analyzing these datasets will need to exercise caution if they intend to perform hypothesis testing. At present, researchers reusing datasets tend to correct for the number of statistical tests that they perform on the datasets. However, as we discuss in this article, when performing hypothesis testing it is important to take into account all of the statistical tests that have been performed on the datasets.</p><p>A distinction can be made between <italic>simultaneous</italic> and <italic>sequential</italic> correction procedures when correcting for multiple tests. Simultaneous procedures correct for all tests at once, while sequential procedures correct for the latest in a non-simultaneous series of tests. There are several proposed solutions to address multiple sequential analyses, namely <inline-formula><mml:math id="inf1"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-spending</italic> and <inline-formula><mml:math id="inf2"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-investing</italic> procedures (<xref ref-type="bibr" rid="bib1">Aharoni and Rosset, 2014</xref>; <xref ref-type="bibr" rid="bib12">Foster and Stine, 2008</xref>), which strictly control false positive rate. Here we will also propose a third, <inline-formula><mml:math id="inf3"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-debt</italic>, which does not maintain a constant false positive rate but allows it to grow controllably.</p><p>Sequential correction procedures are harder to implement than simultaneous procedures as they require keeping track of the total number of tests that have been performed by others. Further, in order to ensure data are still shared, the sequential correction procedures should not be antagonistic with current data-sharing incentives and infrastructure. Thus, we have identified three desiderata regarding open data and multiple hypothesis testing:</p><sec id="s1-1"><title>Sharing incentive</title><p>Data producers should be able to share their data without negatively impacting their initial statistical tests. Otherwise, this reduces the incentive to share data.</p></sec><sec id="s1-2"><title>Open access</title><p>Minimal to no restrictions should be placed on accessing open data, other than those necessary to protect the confidentiality of human subjects. Otherwise, the data are no longer open.</p></sec><sec id="s1-3"><title>Stable false positive rate</title><p>The false positive rate (i.e., type I error) should not increase due to reusing open data. Otherwise, scientific results become less reliable with each reuse.</p><p>We will show that obtaining all three of these desiderata is not possible. We will demonstrate below that the current practice of ignoring sequential tests leads to an increased false positive rate in the scientific literature. Further, we show that sequentially correcting for data reuse can reduce the number of false positives compared to current practice. However, all the proposals considered here must still compromise (to some degree) on one of the above desiderata.</p></sec></sec><sec id="s2"><title>An intuitive example of the problem</title><p>Before proceeding with technical details of the problem, we outline an intuitive problem regarding sequential statistical testing and open data. Imagine there is a dataset which contains the variables (<inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). Let us now imagine that one researcher performs the statistical tests to analyze the relationship between <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and decides that a <inline-formula><mml:math id="inf9"><mml:mi>p</mml:mi></mml:math></inline-formula> &lt; 0.05 is treated as a positive finding (i.e. null hypothesis rejected). The analysis yields p-values of <inline-formula><mml:math id="inf10"><mml:mi>p</mml:mi></mml:math></inline-formula> = 0.001 and <inline-formula><mml:math id="inf11"><mml:mi>p</mml:mi></mml:math></inline-formula> = 0.04 respectively. In many cases, we expect the researcher to correct for the fact that two statistical tests are being performed. Thus, the researcher chooses to apply a Bonferroni correction such that p &lt; 0.025 is the adjusted threshold for statistical significance. In this case, both tests are published, but only one of the findings is treated as a positive finding.</p><p>Alternatively, let us consider a different scenario with sequential analyses and open data. Instead, the researcher only performs one statistical test (<inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, p = 0.001). No correction is performed, and it is considered a positive finding (i.e. null hypothesis rejected). The dataset is then published online. A second researcher now performs the second test (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, p = 0.04) and deems this a positive finding too because it is under a <inline-formula><mml:math id="inf14"><mml:mi>p</mml:mi></mml:math></inline-formula> &lt; 0.05 threshold and they have only performed one statistical test. In this scenario, with the same data, we have two published positive findings compared to the single positive finding in the previous scenario. Unless a reasonable justification exists for this difference between the two scenarios, this is troubling.</p><p>What are the consequences of these two different scenarios? A famous example of the consequences of uncorrected multiple simultaneous statistical tests is the finding of fMRI BOLD activation in a dead salmon when appropriate corrections for multiple tests were not performed (<xref ref-type="bibr" rid="bib4">Bennett et al., 2010</xref>; <xref ref-type="bibr" rid="bib3">Bennett et al., 2009</xref>). Now let us imagine this dead salmon dataset is published online but, in the original analysis, only one part of the salmon was analyzed, and no evidence was found supporting the hypothesis of neural activity in a dead salmon. Subsequent researchers could access this dataset, test different regions of the salmon and report their uncorrected findings. Eventually, we would see reports of dead salmon activations if no sequential correction strategy is applied, but each of these individual findings would appear completely legitimate by current correction standards.</p><p>We will now explore the idea of sequential tests in more detail, but this example highlights some crucial arguments that need to be discussed. Can we justify the sequential analysis without correcting for sequential tests? If not, what methods could sequentially correct for the multiple statistical tests? In order to fully grapple with these questions, we first need to discuss the notion of a <italic>statistical family</italic> and whether sequential analyses create new families.</p></sec><sec id="s3"><title>Statistical families</title><p>A family is a set of tests which we relate the same error rate to (familywise error). What constitutes a family has been challenging to precisely define, and the existing guidelines often contain additional imprecise terminology (e.g. <xref ref-type="bibr" rid="bib6">Cox, 1965</xref>; <xref ref-type="bibr" rid="bib13">Games, 1971</xref>; <xref ref-type="bibr" rid="bib16">Hancock and Klockars, 1996</xref>; <xref ref-type="bibr" rid="bib17">Hochberg and Tamhane, 1987</xref>; <xref ref-type="bibr" rid="bib26">Miller, 1981</xref>). Generally, tests are considered part of a family when: (i) multiple variables are being tested with no predefined hypothesis (i.e. exploration or data-dredging), or (ii) multiple pre-specified tests together help support the same or associated research questions (<xref ref-type="bibr" rid="bib16">Hancock and Klockars, 1996</xref>; <xref ref-type="bibr" rid="bib17">Hochberg and Tamhane, 1987</xref>). Even if following these guidelines, there can still be considerable disagreements about what constituents a statistical family, which can include both very liberal and very conservative inclusion criteria. An example of this discrepancy is seen in using a factorial ANOVA. Some have argued that the main effect and interaction are separate families as they answer 'conceptually distinct questions' (e.g. page 291 of <xref ref-type="bibr" rid="bib22">Maxwell and Delaney, 2004</xref>), while others would argue the opposite and state they are the same family (e.g. <xref ref-type="bibr" rid="bib7">Cramer et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">Hancock and Klockars, 1996</xref>). Given the substantial leeway regarding the definition of family, recommendations have directed researchers to define and justify their family of tests a priori (<xref ref-type="bibr" rid="bib16">Hancock and Klockars, 1996</xref>; <xref ref-type="bibr" rid="bib26">Miller, 1981</xref>).</p><p>A crucial distinction in the definition of a family is whether the analysis is confirmatory (i.e. hypothesis-driven) or exploratory. Given issues regarding replication in recent years (<xref ref-type="bibr" rid="bib30">Open Science Collaboration, 2015</xref>), there has been considerable effort placed into clearly demarcating what is exploratory and what is confirmatory. One prominent definition is that confirmatory research requires preregistration before seeing the data (<xref ref-type="bibr" rid="bib45">Wagenmakers et al., 2012</xref>). However, current practice often involves releasing open data with the original research article. Thus, all data reuse may be guided by the original or subsequent analyses (a HARKing-like problem where methods are formulated after some results are known [<xref ref-type="bibr" rid="bib5">Button, 2019</xref>]). Therefore, if adopting this prominent definition of confirmatory research (<xref ref-type="bibr" rid="bib45">Wagenmakers et al., 2012</xref>), it follows that any reuse of open data after publication must be exploratory unless the analysis is preregistered before the data release.</p><p>Some may find <xref ref-type="bibr" rid="bib45">Wagenmakers et al., 2012</xref> definition to be too stringent and instead would rather allow that confirmatory hypotheses can be stated at later dates despite the researchers having some information about the data from previous use. Others have said confirmatory analyses may not require preregistrations (<xref ref-type="bibr" rid="bib19">Jebb et al., 2017</xref>) and have argued that confirmatory analyses on open data are possible (<xref ref-type="bibr" rid="bib47">Weston et al., 2019</xref>). If analyses on open data can be considered confirmatory, then we need to consider the second guideline about whether statistical tests are answering similar or the same research questions. The answer to this question is not always obvious, as was highlighted above regarding factorial ANOVA. However, if a study reusing data can justify itself as confirmatory, then it must also justify that it is asking a 'conceptually distinct question' from previous instances that used the data. We are not claiming that this is not possible to justify, but the justification ought to be done if no sequential correction is applied as new families are not created just because the data is being reused (see next section).</p><p>We stress that our intention here is not to establish the absolute definition of the term <italic>family</italic>; it has been an ongoing debate for decades, which we do not intend to solve. We believe our upcoming argument holds regardless of the definition. This section aimed to provide a working definition of family that allows for both small and large families to be justified. In the next section, we argue that regardless of the specific definition of family, sequential testing by itself does not create a new family by virtue of it being a sequential test.</p></sec><sec id="s4"><title>Families of tests through time</title><p>The crucial question for the present purpose is whether the reuse of data constitutes a new family of tests. If data reuse creates a new family of tests, then there is no need to perform a sequential correction procedure in order to maintain control over familywise error. Alternatively, if a new family has not been created simply by reusing data, then we need to consider sequential correction procedures.</p><p>There are two ways in which sequential tests with open data can differ from simultaneous tests (where correction is needed): a time lag between tests and/or different individuals performing the tests. Neither of these two properties is sufficient to justify the emergence of a new family of tests. First, the temporal displacement of statistical tests can not be considered sufficient reason for creating a new family of statistical tests, as the speed with which a researcher analyzes a dataset is not relevant to the need to control for multiple statistical tests. If it were, then a simple correction procedure would be to wait a specified length of time before performing the next statistical test. Second, it should not matter who performs the tests; otherwise, one could correct for multiple tests by crowd-sourcing the analysis. Thus if we were to decide that either of the two differentiating properties of sequential tests on open data creates a new family, undesirable correction procedures would be allowable. To prevent this, statistical tests on open data, which can be run by different people, and at different times, can be part of the same family of tests. Since they can be in the same family, sequential tests on open data need to consider correction procedures to control the rate of false positives across the family.</p><p>We have demonstrated the possibility that families of tests can belong to sequential analyses. However, in practice, when does this occur? The scale of the problem rests partly in what is classed as an exploratory analysis or not. If all data reuse is considered part of the same family due to it being exploratory, this creates a large family. If however, this definition is rejected, then it depends on the research question. Due to the fuzzy nature of 'family', and the argument above showing that data reuse does not create new families automatically, we propose a simple rule-of-thumb: if the sequential tests would be considered within the same family if performed simultaneously, then they are part of the same family in sequential tests. The definition of exploratory analyses and this rule-of-thumb indicate that many sequential tests should be considered part of the same family when reusing open data. We, therefore, suggest that researchers should apply corrections for multiple tests when reusing data or provide a justification for the lack of such corrections (as they would need to in the case of simultaneous tests belonging to different families).</p></sec><sec id="s5"><title>The consequence of not taking multiple sequential testing seriously</title><p>In this section, we consider the consequences of uncorrected sequential testing and several procedures to correct for them. We start with a simulation to test the false positive rate of the different sequential correction procedures by performing 100 sequential statistical tests (Pearson correlations) where the simulated covariance between all variables was 0 (see Methods for additional details). The simulations ran for 1000 iterations, and the familywise error was calculated using a two-tailed statistical significance threshold of p&lt;0.05.</p><p>We first consider what happens when the sequential tests are uncorrected. Unsurprisingly, the results are identical to not correcting for simultaneous tests (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). There will almost always be at least one false positive any time one performs 100 sequential analyses with this simulation. This rate of false positives is dramatically above the desired familywise error rate of at least one false positive in 5% of the simulation's iterations: uncorrected sequential tests necessarily lead to more false positives.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Correction procedures can reduce the probability of false positives.</title><p>(<bold>A</bold>) The probability of there being at least one false positive (y-axis) increases as the number of statistical tests increases (x-axis). The use of a correction procedure reduces the probability of there being at least one false positive (B: α-debt; C: α-spending; D: α-investing). Plots are based on simulations: see main text for details. Dotted line in each panel indicates a probability of 0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53498-fig1-v1.tif"/></fig><p>To correct for this false positive increase, we consider several correction procedures. The first sequential procedure we consider is <inline-formula><mml:math id="inf15"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-debt</italic>. For the ith sequential test, this procedure considers there to be <inline-formula><mml:math id="inf16"><mml:mi>i</mml:mi></mml:math></inline-formula> tests that should be corrected. This procedure effectively performs a Bonferroni correction – i.e. the threshold of statistical significance becomes <inline-formula><mml:math id="inf17"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> where <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the first statistical threshold (here 0.05). Thus, on the first test <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> = 0.05, then on the second sequential test <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> = 0.025, <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> = 0.0167, and so on. While each sequential test is effectively a Bonferroni correction considering all previous tests, this does not retroactively change the inference of any previous statistical tests. When a new test is performed, the previous test's <inline-formula><mml:math id="inf22"><mml:mi>α</mml:mi></mml:math></inline-formula> is now too lenient considering all the tests that have been performed. Thus, when considering all tests together, the false positive rate will increase, accumulating a false positive 'debt'. This debt entails that the method does not ensure the type I error rate remains under a specific value, instead allows it to controllably increase under a 'debt ceiling' with each sequential test (the debt ceiling is the sum of all <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at <inline-formula><mml:math id="inf25"><mml:mi>t</mml:mi></mml:math></inline-formula>). The debt ceiling will always increase, but the rate of increase in debt slows down. These phenomena were confirmed in the simulations (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Finally, the method can mathematically ensure that the false negative rate (i.e., type II error) is equal to or better than simultaneous correction with Bonferroni (See Methods).</p><p>The next two procedures we consider have previously been suggested in the literature <inline-formula><mml:math id="inf26"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending and <inline-formula><mml:math id="inf27"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing (<xref ref-type="bibr" rid="bib1">Aharoni and Rosset, 2014</xref>; <xref ref-type="bibr" rid="bib12">Foster and Stine, 2008</xref>). The first has a total amount of 'α wealth', and the sum of all the statistical thresholds for all sequential tests can never exceed this amount (i.e., if the α wealth is 0.05 then the sum of all thresholds on sequential tests must be less than 0.05). Here, for each sequential test, we spend half the remaining wealth (i.e., α<sub>1</sub> is 0.025, α<sub>2</sub> is 0.0125, and so on). In the simulations, the sequential tests limit the probability of there being at least one false positive to less than 0.05 (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Finally, α-investing allows for the significance threshold to increase or decrease as researchers perform additional tests. Again there is a concept of <inline-formula><mml:math id="inf28"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth. If a test rejects the null hypothesis, there is an increase in the remaining α-wealth that future tests can use and, if the reverse occurs, the remaining <inline-formula><mml:math id="inf29"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth decreases (see methods). α-investing ensures control of the false discovery rate at an assigned level. Here we invest 50% of the remaining wealth for each statistical test. In the simulations, this method also remains under 0.05 familywise error rate as the sequential tests increase (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><p>The main conclusion from this set of simulations is that the current practice of not correcting for open data reuse results in a substantial increase in the number of false positives presented in the literature.</p></sec><sec id="s6"><title>Sensitivity to the order of sequential tests</title><p>The previous simulation did not consider any true positives in the data (i.e. cases where we should reject the null hypothesis). Since the statistical threshold for significance changes as the number of sequential tests increases, it becomes crucial to evaluate the sensitivity of each method to both type I and type II errors in regards to the order of sequential tests. Thus, we simulated true positives (between 1-10) where the covariance of these variables and the dependent variable were set to <inline-formula><mml:math id="inf30"><mml:mi>p</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf31"><mml:mi>p</mml:mi></mml:math></inline-formula> ranged between 0 and 1). Further, <inline-formula><mml:math id="inf32"><mml:mi>λ</mml:mi></mml:math></inline-formula> controlled the sequential test order determining the probability that a test was a true positive. When <inline-formula><mml:math id="inf33"><mml:mi>λ</mml:mi></mml:math></inline-formula> is positive, it entails a higher likelihood that earlier tests will be one of the true positives (and vice versa when <inline-formula><mml:math id="inf34"><mml:mi>λ</mml:mi></mml:math></inline-formula> was negative; see methods). All other parameters are the same as the previous simulation. Simultaneous correction procedures (Bonferroni and FDR) of all 100 tests were also included to contrast the different sequential procedures to these methods.</p><p>The results reveal that the order of the tests is pivotal for sequential correction procedures. Unsurprisingly, the uncorrected and simultaneous correction procedures do not depend on the sequential order of tests (<xref ref-type="fig" rid="fig2">Figure 2ABC</xref>). The sequential correction procedures all increased their true positive rate (i.e., fewer type II errors) when the true positives were earlier in the analysis order (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We also observe that <inline-formula><mml:math id="inf35"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt had the highest true positive rate of the sequential procedures and, when the true positives were later in the test sequence, performed on par with Bonferroni. Further, when the true positives were earlier, <inline-formula><mml:math id="inf36"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt outperformed Bonferroni at identifying them. <inline-formula><mml:math id="inf37"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing and <inline-formula><mml:math id="inf38"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending cannot give such assurances when the true positives are later in the analysis sequence (i.e. <inline-formula><mml:math id="inf39"><mml:mi>λ</mml:mi></mml:math></inline-formula> is negative) there is less sensitivity to true positives (i.e. type II errors). <inline-formula><mml:math id="inf40"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt is more sensitive to true positives compared to <inline-formula><mml:math id="inf41"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending because the threshold for the mth sequential test decreases linearly in <inline-formula><mml:math id="inf42"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt and exponentially in <inline-formula><mml:math id="inf43"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending. This fact results in a more lenient statistical threshold for <inline-formula><mml:math id="inf44"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt in later sequential tests.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The order of sequential tests can impact true positive sensitivity.</title><p>(<bold>A</bold>) The true positive rate in the uncorrected case (left-most panel), in two cases of simultaneous correction (second and third panels), and in three cases of sequential correction (fourth, fifth and sixth panels). In each panel the true positive rate after 100 tests is plotted as a function of two simulation parameters: λ (x-axis) and the simulated covariance of the true positives (y-axis). When λ is positive (negative), it increases the probability of the true positives being an earlier (later) test. Plots are based on simulations in which there are ten true positives in the data: see main text for details. (<bold>B</bold>) Same as A for the false positive rate. (<bold>C</bold>) Same as A for the false discovery rate. (<bold>D</bold>) Same as C for the average false discovery rate in four quadrants. Q1 has λ &lt;0; covariance &gt;0.25. Q2 has λ &gt;0; covariance &gt;0.25. Q3 has λ &lt;0; covariance &lt;0.25. Q4 has λ &gt;0; covariance &lt;0.25. The probability of true positives being an earlier test is highest in Q2 and Q4 as λ &gt;0 in these quadrants. (<bold>E</bold>) Same as D with the false discovery rate (y-axis) plotted against the percentage of true positives (x-axis) for the four quadrants. The dotted lines in D and E indicate a false discovery rate of 0.05. Code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/wiheto/datasetdecay">https://github.com/wiheto/datasetdecay</ext-link> (<xref ref-type="bibr" rid="bib39">Thompson, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/datasetdecay">https://github.com/elifesciences-publications/datasetdecay</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53498-fig2-v1.tif"/></fig><p>The false positive rate and false discovery rate are both very high for the uncorrected procedure (<xref ref-type="fig" rid="fig2">Figure 2BC</xref>). <inline-formula><mml:math id="inf45"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt and <inline-formula><mml:math id="inf46"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending both have a decrease in false positives and false discovery rate when <inline-formula><mml:math id="inf47"><mml:mi>λ</mml:mi></mml:math></inline-formula> is positive (<xref ref-type="fig" rid="fig2">Figure 2BC</xref>). The false discovery rate for <inline-formula><mml:math id="inf48"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt generally lies between the spending (smallest) and investing procedures (largest and one that aims to be below 0.05). Also, for all methods, the true positive rate breaks down as expected when the covariance between variables approaches the noise level. Thus we split the false discovery rate along four quadrants based on <inline-formula><mml:math id="inf49"><mml:mi>λ</mml:mi></mml:math></inline-formula> and the noise floor (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The quadrants where true positive covariance is above the noise floor (Q1 and Q2) has a false discovery rate of less than 0.05 for all procedures except uncorrected (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Finally, when varying the number of true positives in the dataset, we found that Q1 and Q2 generally decrease as the number of true positives grows for <inline-formula><mml:math id="inf50"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending and <inline-formula><mml:math id="inf51"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt, whereas <inline-formula><mml:math id="inf52"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing remains the 0.05 mark regardless of the number of true positives (<xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p><p>All three sequential correction procedures performed well at identifying true positives when these tests were made early on in the analysis sequence. When the true positive tests are performed later, <inline-formula><mml:math id="inf53"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt has the most sensitivity for true positives and <inline-formula><mml:math id="inf54"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing is the only procedure that has a stable false discovery rate regardless of the number of true positives (the other two methods appear to be more conservative). The true positive sensitivity and false discovery rate of each of the three sequential correction methods considered depend on the order of statistical tests and how many true positives are in the data.</p></sec><sec id="s7"><title>Uncorrected sequential tests will flood the scientific literature with false positives</title><p>We have demonstrated a possible problem with sequential tests on simulations. These results show that sequential correction strategies are more liberal than their simultaneous counterparts. Therefore we should expect more false positives if sequential correction methods were performed on a dataset. We now turn our attention to empirical data from a well-known shared dataset in neuroscience to examine the effect of multiple reuses of the dataset. This empirical example is to confirm the simulations and show that more positive findings (i.e. null hypothesis rejected) will be identified with sequential correction. We used 68 cortical thickness estimates from the 1200 subject release of the HCP dataset (<xref ref-type="bibr" rid="bib42">Van Essen et al., 2012</xref>). All subjects belonging to this dataset gave informed consent (see <xref ref-type="bibr" rid="bib43">Van Essen et al., 2013</xref> for more details). IRB protocol #31848 approved by the Stanford IRB approves the analysis of shared data. We then used 182 behavioral measures ranging from task performance to survey responses (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). For simplicity, we ignore all previous publications using the HCP dataset (of which there are now several hundred) for our p-value correction calculation.</p><p>We fit 182 linear models in which each behavior (dependent variable) was modeled as a function of each of the 68 cortical thickness estimates (independent variables), resulting in a total of 12,376 statistical tests. As a baseline, we corrected all statistical tests simultaneously with Bonferroni and FDR. For all other procedures, the independent variables within each mode (i.e. cortical thickness) had simultaneous FDR correction while considering each linear model (i.e. each behavior) sequentially. The procedures considered were: uncorrected sequential analysis with both Bonferroni and FDR simultaneous correction procedures; all three sequential correction procedures with FDR simultaneous correction within each model. For the sequential tests, the orders were randomized in two ways: (i) uniformly; (ii) weighting the earlier tests to be the significant findings found during the baseline conditions (see Methods). The latter considers how the methods perform if there is a higher chance that researchers test hypotheses that produce positive findings earlier in the analysis sequence rather than later. Sequential analyses had the order of tests randomized 100 times.</p><p>We asked two questions with these models. First, we identified the number of positive findings that would be reported for the different correction methods (a positive finding is considered to be when the null hypothesis is rejected at p &lt; 0.05, two tail). Second, we asked how many additional scientific articles would be published claiming to have identified a positive result (i.e. a null hypothesis has been rejected) for the different correction methods. Importantly, in this evaluation of empirical data, we are not necessarily concerned with the number of true relationships with this analysis. Primarily, we consider the differences in the inferred statistical relationships when comparing the different sequential correction procedures to a baseline of the simultaneous correction procedures. These simultaneous procedures allow us to contrast the sequential approaches with current practices (Bonferroni, a conservative procedure, and FDR, a more liberal measure). Thus any procedure that is more stringent than the Bonferroni baseline will be too conservative (more type II errors). Any procedure that is less stringent than FDR will have an increased false discovery rate, implying more false positives (relative to the true positives). Note that, we are tackling only issues regarding correction procedures to multiple hypothesis tests; determining the truth of any particular outcome would require additional replication.</p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows the results for all correction procedures. Using sequentially uncorrected tests leads to an increase in positive findings (30/44 Bonferroni/FDR), compared to a baseline of 2 findings when correcting for all tests simultaneously (for both Bonferroni and FDR procedures). The sequentially uncorrected procedures would also result in 29/30 (Bonferroni/FDR) publications that claim to identify at least one positive result instead of the simultaneous baseline of two publications (Bonferroni and FDR), reflecting a 1,400% increase in publications claiming positive results. If we accept that the two baseline estimates are a good trade-off between error rates, then we have good reason to believe this increase reflects false positives.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Demonstrating the impact of different correction procedures with a real dataset.</title><p>(<bold>A</bold>) The number of significant statistical tests (x-axis) that are possible for various correction procedures in a real dataset from the <ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org/">Human Connectome Project</ext-link>: see the main text for more details, <xref ref-type="supplementary-material" rid="supp1">supplementary file 1</xref> for a list of the variables used in the analysis, and <ext-link ext-link-type="uri" xlink:href="https://github.com/wiheto/datasetdecay">https://github.com/wiheto/datasetdecay</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/datasetdecay">https://github.com/elifesciences-publications/datasetdecay</ext-link> for the code. (<bold>B</bold>) The potential number of publications (x-axis) that could result from the tests shown in panel A. This assumes that a publication requires a null hypothesis to be rejected in order to yield a positive finding. The dotted line shows the baseline from the two simultaneous correction procedures. Error bars show the standard deviation and circles mark min/max number of findings/studies for the sequential correction procedures with a randomly permuted test order.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53498-fig3-v1.tif"/></fig><p>The sequential correction procedures were closer to baseline but saw divergence based on the order of the statistical tests. If the order was completely random, then <inline-formula><mml:math id="inf55"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt found, on average, 2.77 positive findings (min/max: 2/6) and 2.53 publications claiming positive results (min/max: 2/4) would be published. The random order leads to an increase in the number of false positives compared to baseline but considerably less than the sequentially uncorrected procedure. In contrast, <inline-formula><mml:math id="inf56"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending found 0.33 positive findings (min/max: 0/5) resulting in 0.22 studies with positive findings (min/max: 0/2) and <inline-formula><mml:math id="inf57"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing found 0.48 (min/max: 0/8) positive findings and 0.37 (min/max 0/4) studies with positive findings; both of which are below the conservative baseline of 2. When the order is informed by the baseline findings, the sequential corrections procedures increase in the number of findings (findings [min/max]: <inline-formula><mml:math id="inf58"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt: 3.49 [2/7], <inline-formula><mml:math id="inf59"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending: 2.58 [1/4], <inline-formula><mml:math id="inf60"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing: 3.54 [1/10]; and publications with positive findings [min/max]: <inline-formula><mml:math id="inf61"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt: 2.38 [2/4], <inline-formula><mml:math id="inf62"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending: 1.97 [1/3], <inline-formula><mml:math id="inf63"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing: 2.54 [1/5]). All procedures now increase their number of findings above baseline. On average <inline-formula><mml:math id="inf64"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt with a random order has a 19% increase in the number of published studies with positive findings, substantially less than the increase in the number of uncorrected studies. Two conclusions emerge. First, <inline-formula><mml:math id="inf65"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt remains sensitive to the number of findings found regardless of the sequence of tests (fewer type II errors) and can never fall above the Bonferroni in regards to type II errors. At the same time, the other two sequential procedures can be more conservative than Bonferroni. Second, while <inline-formula><mml:math id="inf66"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt does not ensure the false positive rate remains under a specific level (more type I errors), it dramatically closes the gap between the uncorrected and simultaneous number of findings.</p><p>We have shown with both simulation and an empirical example of how sequential statistical tests, if left uncorrected, will lead to a rise of false positive results. Further, we have explored different sequential correction procedures and shown their susceptibility to both false negatives and false positives. Broadly, we conclude that the potential of a dataset to identify new statistically significant relationships will decay over time as the number of sequential statistical tests increases when controlling for sequential tests. In the rest of the discussion section, we first discuss the implications the different sequential procedures have in regards to the desiderata outlined in the introduction. Then we discuss other possible solutions that could potentially mitigate dataset decay.</p></sec><sec id="s8"><title>Consequence for sequential tests and open data</title><p>We stated three desiderata for open data in the introduction: sharing incentive, open access, and a stable false positive rate. Having demonstrated some properties of sequential correction procedures, we revisit these aims and consider how the implementation of sequential correction procedures in practice would meet these desiderata. The current practice of leaving sequential hypothesis tests uncorrected leads to a dramatic increase in the false positive rate. While our proposed sequential correction techniques would mitigate this problem, all three require compromising on one or more of the desiderata (summarized in <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of the different sequential correction methods and the open-data desiderata.</title><p>Yes indicates that the method is compatible with the desideratum.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">Sharing incentive</th><th valign="top">Open access</th><th valign="top">Stable false positive rate</th></tr></thead><tbody><tr><th valign="top"><inline-formula><mml:math id="inf67"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-spending</italic></th><td valign="top">No</td><td valign="top">No</td><td valign="top">Yes</td></tr><tr><th valign="top"><inline-formula><mml:math id="inf68"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-investing</italic></th><td valign="top">Yes</td><td valign="top">No</td><td valign="top">Yes</td></tr><tr><th valign="top"><inline-formula><mml:math id="inf69"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-debt</italic></th><td valign="top">Yes</td><td valign="top">Yes</td><td valign="top">No</td></tr></tbody></table></table-wrap><p>Implementing <inline-formula><mml:math id="inf70"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending would violate the sharing incentive desideratum as it forces the initial analysis to use a smaller statistical threshold to avoid using the entire wealth of <inline-formula><mml:math id="inf71"><mml:mi>α</mml:mi></mml:math></inline-formula>. This change could potentially happen with appropriate institutional change, but placing restrictions on the initial investigator(s) (and increased type II error rate) would likely serve as a disincentive for those researchers to share their data. It also places incentives to restrict access to open data (violating the open access desideratum) as performing additional tests would lead to a more rapid decay in the ability to detect true positives in a given dataset.</p><p>Implementing <inline-formula><mml:math id="inf72"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing, would violate the open access desideratum for two reasons. First, like <inline-formula><mml:math id="inf73"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending there is an incentive to restrict incorrect statistical tests due to the sensitivity to order. Second, <inline-formula><mml:math id="inf74"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing would require tracking and time-stamping all statistical tests made on the dataset. Given the known issues of file drawer problem (<xref ref-type="bibr" rid="bib35">Rosenthal, 1979</xref>), this is currently problematic to implement (see below). Also, publication bias for positive outcomes would result in statistical thresholds becoming more lenient over time with this correction procedure, which might lead to even more false positives (thus violating the no increase in false positives desideratum). Unless all statistical tests are time-stamped, which is possible but would require significant institutional change, this procedure would be hard to implement.</p><p>Implementing <inline-formula><mml:math id="inf75"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt would improve upon current practices but will compromise on the stable false positive rate desideratum. However, it will have little effect on the sharing incentive desideratum as the original study does not need to account for any future sequential tests. The open-access desideratum is also less likely to be compromised as it is less critical to identify the true-positives directly (i.e. it has the lowest type II error rate of the sequential procedures). Finally, while compromising the false positive desideratum, its false positive rate a marked improvement compared to sequentially uncorrected tests.</p><p>Finally, a practical issue that must be taken into consideration with all sequential correction procedures is whether it is ever possible to know the actual number of tests performed on an unrestricted dataset. This issue relates to the file drawer problem where there is a bias towards the publication of positive findings compared to null findings (<xref ref-type="bibr" rid="bib35">Rosenthal, 1979</xref>). Until this is resolved, to fully sequentially correct for the number of previous tests corrected, an estimation of the number of tests may be required (e.g. by identifying publication biases; <xref ref-type="bibr" rid="bib36">Samartsidis et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Simonsohn et al., 2013</xref>). Using such estimations is less problematic with <inline-formula><mml:math id="inf76"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt because this only requires the number of tests to be known. Comparatively, <inline-formula><mml:math id="inf77"><mml:mi>α</mml:mi></mml:math></inline-formula>-investing requires the entire results chain of statistical tests to be known and <inline-formula><mml:math id="inf78"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending requires knowing every <inline-formula><mml:math id="inf79"><mml:mi>α</mml:mi></mml:math></inline-formula> value that has been used, both of which would require additional assumptions to estimate. However, even if <inline-formula><mml:math id="inf80"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt correction underestimates the number of previous statistical tests, the number of false positives will be reduced compared to no sequential correction.</p></sec><sec id="s9"><title>Towards a solution</title><p>Statistics is a multifaceted tool for experimental researchers to use, but it (rarely) aims to provide universal solutions for all problems and use cases. Thus, it may be hard to expect a one size fits all solution to the problem of sequential tests on open data. Indeed, the idiosyncrasies within different disciplines regarding the size of data, open data infrastructure, and how often new data is collected, may necessitate that they adopt different solutions. Thus, any prescription we offer now is, at best, tentative. Further, the solutions also often compromise the desiderata in some way. That being said, there are some suggestions which should assist in mitigating the problem to different degrees. Some of these suggestions only require the individual researcher to adapt their practices, others require entire disciplines to form a consensus, and others require infrastructural changes. This section deals with solutions compatible with the null hypothesis testing framework, the next section considers solutions specific to other perspectives.</p><sec id="s9-1"><title>Preregistration grace period of analyses prior to open data release</title><p>To increase the number of confirmatory analyses that can be performed on an open dataset, one possible solution is to have a 'preregistration grace period'. Here a description of the data can be provided, and data re-users will have the opportunity to write a preregistration prior to the data being released. This solution allows for confirmatory analyses to be performed on open data while simultaneously being part of different statistical families. This idea follows <xref ref-type="bibr" rid="bib45">Wagenmakers et al., 2012</xref> definition of confirmatory analysis. Consequently, once the dataset or the first study using the dataset are published, the problems outlined in this paper will remain for all subsequent (non pre-registered) analyses reusing the data.</p></sec><sec id="s9-2"><title>Increased justification of the statistical family</title><p>One of the recurring problems regarding statistical testing is that, given the <xref ref-type="bibr" rid="bib45">Wagenmakers et al., 2012</xref> definition, it is hard to class open data reuse as confirmatory after data release. However, if disciplines decide that confirmatory analyses on open data (post-publication) are possible, one of our main arguments above is that a new paper does not automatically create a new statistical family. If researchers can, for other reasons, justify why their statistical family is separate in their analysis and state how it is different from previous statistical tests performed on the data, there is no necessity to sequentially correct. Thus providing sufficient justification for new a family in a paper can effectively reset the alpha wealth.</p></sec><sec id="s9-3"><title>Restrained or coordinated alpha-levels</title><p>One of the reasons the <inline-formula><mml:math id="inf81"><mml:mi>α</mml:mi></mml:math></inline-formula>-values decays quickly in <inline-formula><mml:math id="inf82"><mml:mi>α</mml:mi></mml:math></inline-formula>-invest and <inline-formula><mml:math id="inf83"><mml:mi>α</mml:mi></mml:math></inline-formula>-spend is the 50% invest/spend rate that we chose in this article uses a large portion of the total <inline-formula><mml:math id="inf84"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth in the initial statistical tests. For example, the first two tests in <inline-formula><mml:math id="inf85"><mml:mi>α</mml:mi></mml:math></inline-formula>-spend, use 75% of the overall <inline-formula><mml:math id="inf86"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth. Different spending or investing strategies are possible, which could restrain the decay of the remaining <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-wealth, allowing for more discoveries in later statistical tests. For example, a discipline could decide that the first ten statistical tests spend 5% of the <inline-formula><mml:math id="inf88"><mml:mi>α</mml:mi></mml:math></inline-formula> wealth, then the next ten spends 2.5% of the overall wealth. Such a strategy would still always remain under the overall wealth, but allow more people to utilize the dataset. However, imposing this restrained or fair-use of <inline-formula><mml:math id="inf89"><mml:mi>α</mml:mi></mml:math></inline-formula>-spending would either require consensus from all researchers (however, this strategy would be in vain if just one researcher fails to comply) or restricting data access (compromising the open access desideratum). Importantly, this solution does not mitigate the decay of the alpha threshold; it just reduces the decay.</p></sec><sec id="s9-4"><title>Metadata about reuse coupled to datasets</title><p>One of the problems regarding sequential corrections is knowing how many tests have been made using the dataset. This issue was partially addressed above with suggestions for estimating the number of preceding tests. Additionally, repositories could provide information about all known previous uses of the data. Thus if data repositories were able to track summaries of tests performed and which variables involved in the tests, this would, at the very least, help guide future users with rough estimates. In order for this number to be precise, it would, however, require limiting the access to the dataset (compromising the open access desideratum).</p></sec><sec id="s9-5"><title>Held out data on repositories</title><p>A way to allow hypothesis testing or predictive frameworks (see below) to reuse the data is if the infrastructure exists that prevents the researcher from ever seeing some portion of the data. Dataset repositories could hold out data which data re-users can query their results against to either replicate their findings or test their predictive models. This perspective has seen success in machine learning competitions which hold out test data. Additional requirements could be added to this perspective, such as requiring preregistrations in order to query the held out data. However, there have been concerns that held out data can lead to overfitting (e.g. by copying the best fitting model) (<xref ref-type="bibr" rid="bib28">Neto et al., 2016</xref>) although others have argued this does not generally appear to be the case when evaluating overfitting (<xref ref-type="bibr" rid="bib33">Roelofs et al., 2019</xref>). However, <xref ref-type="bibr" rid="bib33">Roelofs et al., 2019</xref> noted that overfitting appears to occur on smaller datasets, which might prevent it from being a general solution for all disciplines.</p></sec><sec id="s9-6"><title>Narrow hypotheses and minimal statistical families</title><p>One way to avoid the sequential testing problem is to ensure small family sizes. If we can justify that there should be inherently small family sizes, then there is no need to worry about the sequential problems outlined here. This solution would also entail that each researcher does not need to justify their own particular family choice (as suggested above), but rather a specific consensus of what the contested concept <italic>family</italic> actually means is achieved. This would require: (1) confirmatory hypothesis testing on open data is possible, (2) encouraging narrow (i.e. very specific) hypotheses that will help maintain minimal family sizes, as the specificity of the hypothesis will limit the overlap with any other statistical test. Narrow hypotheses for confirmatory analyses can lead to families which are small, and can avoid correcting for multiple statistical tests (both simultaneous and sequential). This strategy is a possible solution to the problem. However, science does not merely consist of narrow hypotheses. Broader hypotheses can still be used in confirmatory studies (for example, genetic or neuroimaging datasets often ask broader questions not knowing which specific gene or brain area is involved, but know that a gene or brain region should be involved to confirm a hypothesis about a larger mechanism). Thus, while possibly solving a portion of the problem, this solution is unlikely to be a general solution for all fields, datasets, and types of hypotheses.</p></sec></sec><sec id="s10"><title>Different perspective-specific solutions regarding sequential testing</title><p>The solutions above focused on possible solutions compatible within the null hypothesis testing framework to deal with sequential statistical tests, although many are compatible with other perspectives as well. There are a few other perspectives about data analysis and statistical inferences that are worth considering, three of which we discuss here. Each provide some perspective-specific solution to the sequential testing problem. Any of these possible avenues may be superior to the ones we have considered in this article, but none appear to readily applicable in all situations without some additional considerations.</p><p>The first alternative is Bayesian statistics. Multiple comparisons in Bayesian frameworks are often circumnavigated by partial pooling and regularizing priors (<xref ref-type="bibr" rid="bib14">Gelman et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Kruschke and Liddell, 2017</xref>). While Bayesian statistics can suffer from similar problems as NHST if misapplied (<xref ref-type="bibr" rid="bib15">Gigerenzer and Marewski, 2014</xref>), it often deals with multiple tests without explicitly correcting for them, and may provide an avenue for sequential correction to be avoided. These techniques should allow for the sequential evaluation of different independent variables against a single dependent variable when using regularizing priors, especially as these different models could also be contrasted explicitly to see which model fits the data best. However, sequential tests could be problematic when the dependent variable changes and the false positive rate should be maintained across models. If uncorrected, this could create a similar sequential problem as outlined in the empirical example in the article. Nevertheless, there are multiple avenues where this could be fixed (e.g. sequentially adjusting the prior odds in Bayes-factor inferences). The extent of sequential analysis on open dataset within the Bayesian hypothesis testing frameworks, and possible solutions, is an avenue of future investigation.</p><p>The second alternative is using held-out data within prediction frameworks. Instead of using statistical inference, this framework evaluates a model by how well it performs on predicting unseen test data (<xref ref-type="bibr" rid="bib48">Yarkoni and Westfall, 2017</xref>). However, a well-known problem when creating models to predict on test datasets is overfitting. This phenomenon occurs, for example, if a researcher queries the test dataset multiple times. Reusing test data will occur when sequentially reusing open data. Held-out data on data repositories, as discussed above, is one potential solution here. Further, within machine learning, there have been advances towards having reusable held-out data that can be queried multiple times (<xref ref-type="bibr" rid="bib10">Dwork et al., 2015</xref>; <xref ref-type="bibr" rid="bib11">Dwork et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Rogers et al., 2019</xref>). This avenue is promising, but there appear to be some drawbacks for sequential reuse. First, this line of work within 'adaptive data analysis' generally considers a single user querying the holdout test data multiple times while optimizing their model/analysis. Second, this is ultimately a cross-validation technique which is not necessarily the best tool in datasets where sample sizes are small, (<xref ref-type="bibr" rid="bib44">Varoquaux, 2018</xref>) which is often the case with open data and thus not a general solution to this problem. Third, additional assumptions exist in these methods (e.g., there is still a 'budget limit' in <xref ref-type="bibr" rid="bib10">Dwork et al., 2015</xref>, and 'mostly guessing correctly' is required in <xref ref-type="bibr" rid="bib34">Rogers et al., 2019</xref>). However, this avenue of research has the potential to provide a better solution than what we have proposed here.</p><p>The third and perhaps most radical alternative is to consider all open data analysis to be exploratory data analysis (EDA). In EDA, the primary utility becomes generating hypotheses and testing assumptions of methods (<xref ref-type="bibr" rid="bib9">Donoho, 2017</xref>; <xref ref-type="bibr" rid="bib19">Jebb et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Thompson et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Tukey, 1980</xref>). Some may still consider this reframing problematic, as it could make findings based on open data seem less important. However, accepting that all analyses on open data is EDA would involve less focus on statistical inference — the sequential testing problem disappears. An increase of EDA on exploratory analyses would lead to an increase of EDA results which may not replicate. However, this is not necessarily problematic. There would be no increase of false positives within <italic>confirmatory studies</italic> in the scientific literature and the increase EDA studies will provide a fruitful guide about which confirmatory studies to undertake. Implementing this suggestion would require little infrastructural or methodological change; however, it would require an institutional shift in how researchers interpret open data results. This suggestions of EDA on open data also fits with recent proposals calling for exploration to be conducted openly (<xref ref-type="bibr" rid="bib38">Thompson et al., 2020</xref>).</p></sec><sec id="s11"><title>Conclusion</title><p>One of the benefits of open data is that it allows multiple perspectives to approach a question, given a particular sample. The trade-off of this benefit is that more false positives will enter the scientific literature. We remain strong advocates of open data and data sharing. We are not advocating that every single reuse of a dataset must necessarily correct for sequential tests and we have outlined multiple circumstances throughout this article where this is the case. However, researchers using openly shared data should be sensitive to the possibility of accumulating false positives and ensuing dataset decay that will occur with repeated reuse. Ensuring findings are replicated using independent samples will greatly decrease the false positive rate, since the chance of two identical false positives relationships occurring, even on well-explored datasets, is small.</p></sec><sec id="s12" sec-type="methods"><title>Methods</title><sec id="s12-1"><title>Preliminary assumptions</title><p>In this article, we put forward the argument that sequential statistical tests on open data could lead to an increase in the number of false positives. This argument requires several assumptions regarding (1) the type of datasets analyzed; (2) what kind of statistical inferences are performed; (3) the types of sequential analyses considered.</p><sec id="s12-1-1"><title>The type of dataset</title><p>we consider a dataset to be a fixed static snapshot of data collected at a specific time point. There are other cases of combining datasets or datasets that grow over time, but we will not consider those here. Second, we assume a dataset to be a random sample of a population and not a dataset that contains information about a full population.</p></sec><sec id="s12-1-2"><title>The type of statistical testing</title><p>We have framed our discussion of statistical inference using null hypothesis statistical testing (NHST). This assumption entails that we will use thresholded p-values to infer whether a finding differs from the null hypothesis. Our decision for this choice is motivated by a belief that the NHST framework being the most established framework for dealing with multiple statistical tests. There have been multiple valid critiques and suggestions to improve upon this statistical practice by moving away from thresholded p-values to evaluate hypotheses (<xref ref-type="bibr" rid="bib8">Cumming, 2014</xref>; <xref ref-type="bibr" rid="bib18">Ioannidis, 2019</xref>; <xref ref-type="bibr" rid="bib21">Lee, 2016</xref>; <xref ref-type="bibr" rid="bib25">McShane et al., 2019</xref>; <xref ref-type="bibr" rid="bib46">Wasserstein et al., 2019</xref>). Crucially, however, many proposed alternative approaches within statistical inference do not circumnavigate the problem of multiple statistical testing. For example, if confidence intervals are reported and used for inference regarding hypotheses, these should also be adjusted for multiple statistical tests (see, e.g. <xref ref-type="bibr" rid="bib41">Tukey, 1991</xref>). Thus, any alternative statistical frameworks that still must correct for multiple simultaneous statistical testing will have the same sequential statistical testing problem that we outline here. Thus, while we have chosen NHST for simplicity and familiarity, this does not entail that the problem is isolated to NHST. Solutions for different frameworks may however differ (see the discussion section for Bayesian approaches and prediction-based inference perspectives).</p></sec><sec id="s12-1-3"><title>The types of analyses</title><p>Sequential analyses involve statistical tests on the same data. Here, we consider sequential analyses that reuse the same data and analyses to be a part of the same statistical family (see section on statistical families for more details). Briefly, this involves either the statistical inferences being classed as exploratory or answering the same confirmatory hypothesis or research question. Further, we only consider analyses that are not supersets of previous analyses. This assumption entails that we are excluding analyses where a statistical model may improve upon a previous statistical model by, for example, adding an additional layer in a hierarchical model. Other types of data reuse may not be appropriate for sequential correction methods and are not considered here.</p><p>While we have restrained our analysis with these assumptions and definitions, it is done primarily to simplify the argument regarding the problem we are identifying. The degree to which sequential tests are problematic in more advanced cases remains outside the scope of this paper.</p></sec></sec><sec id="s12-2"><title>Simulations</title><p>The first simulation sampled data for one dependent variable and 100 independent variables from a multivariate Gaussian distribution (mean: 0, standard deviation: 1, covariance: 0). We conducted 100 different pairwise sequential analyses in a random order. For each analysis, we quantified the relationship between an independent and the dependent variable using a Pearson correlation. If the correlation had a two-tailed p-value less than 0.05, we considered it to be a false positive. The simulation was repeated for 1000 iterations.</p><p>The second simulation had three additional variables. First, a variable that controlled the number of true positives in the data. This variable varied between 1-10. Second, the selected true positive variables, along with the dependent variable, had their covariance assigned as <inline-formula><mml:math id="inf90"><mml:mi>p</mml:mi></mml:math></inline-formula>. <inline-formula><mml:math id="inf91"><mml:mi>p</mml:mi></mml:math></inline-formula> varied between 0 and 1 in steps of 0.025. Finally, we wanted to test the effect of the analysis order to identify when the true positive were included in the statistical tests. Each sequential analysis, (<inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> …), could be assigned to be a 'true positive' (i.e., covariance of <inline-formula><mml:math id="inf95"><mml:mi>p</mml:mi></mml:math></inline-formula> with the dependent variable) or a 'true negative' (covariance of 0 with dependent variable). First, <inline-formula><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> would be assigned one of the trials, then <inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and so forth. This procedure continued until there were only true positives or true negatives remaining. The procedure assigns the ith analysis to be randomly assigned, weighted by <inline-formula><mml:math id="inf98"><mml:mi>λ</mml:mi></mml:math></inline-formula>. If <inline-formula><mml:math id="inf99"><mml:mi>λ</mml:mi></mml:math></inline-formula> was 0, then there was a 50% chance that <inline-formula><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> would be a true positive or true negative. If <inline-formula><mml:math id="inf101"><mml:mi>λ</mml:mi></mml:math></inline-formula> was 1, a true positive was 100% more likely to be assigned to <inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (i.e. an odds ratio of 1+<inline-formula><mml:math id="inf103"><mml:mi>λ</mml:mi></mml:math></inline-formula>:1), The reverse occurred if <inline-formula><mml:math id="inf104"><mml:mi>λ</mml:mi></mml:math></inline-formula> was negative (i.e. -1 meant a true negative was 100% more likely at <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p></sec><sec id="s12-3"><title>Empirical example</title><p>Data from the Human Connectome Project (HCP) 1200 subject release was used (<xref ref-type="bibr" rid="bib42">Van Essen et al., 2012</xref>). We selected 68 estimates of cortical thickness to be the independent variables for 182 continuous behavioral and psychological variables dependent variables. Whenever possible, the age-adjusted values were used. <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> shows the variables selected in the analysis.</p><p>For each analysis, we fit an ordinary least squares model using <italic>Statsmodels</italic> (0.10.0-dev0+1579, <ext-link ext-link-type="uri" xlink:href="https://github.com/statsmodels/statsmodels/">https://github.com/statsmodels/statsmodels/</ext-link>). For all statistical models, we first standardized all variables to have a mean of 0 and a standard deviation of 1. We dropped any missing values for a subject for that specific analysis. Significance was considered for any independent variable if it had a p-value &lt; 0.05, two-tailed for the different correction methods.</p><p>We then quantified the number of findings and the number of potential published studies with positive results that the different correction methods would present. The number of findings is the sum of independent variables that were considered positive findings (i.e. p &lt; 0.05, two-tailed). The number of potential studies that identify positive results is the number of dependent variables that had at least one positive finding. The rationale for the second metric is to consider how many potential non null-finding publications would exist in the literature if a separate group conducted each analysis.</p><p>For the sequential correction procedures, we used two different orderings of the tests. The first was with a uniformly random order. The second was an informed order that pretends we somehow a priori knew which variables will be correlated. The motivation behind an informed order is because it may be unrealistic that scientists ask sequential questions in a random order. The 'informed' order was created by identifying the significant statistical tests when using simultaneous correction procedures (see below). With the baseline results, we identified analyses which were <italic>baseline positives</italic> (i.e. significant with any of the simultaneous baseline procedures. There were two analyses) and the other analyses that were <italic>baseline negatives</italic>. Then, as in simulation 2, the first analysis, <inline-formula><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was randomly assigned to be a baseline positive or negative with equal probability. This informed ordering means that the baseline positives would usually appear in an earlier in the sequence order. All sequential correction procedures were applied 100 times.</p></sec><sec id="s12-4"><title>Simultaneous correction procedures</title><p>We used the Bonferroni method and the Benjamini and Hochberg FDR method for simultaneous correction procedures (<xref ref-type="bibr" rid="bib2">Benjamini and Hochberg, 1995</xref>). Both correction methods were run using multipy (v0.16, <ext-link ext-link-type="uri" xlink:href="https://github.com/puolival/multipy">https://github.com/puolival/multipy</ext-link>). The FDR correction procedure intends to limit the proportion of type I errors by keeping in below a certain level. In contrast, Bonferroni error intends to limit the probability of at least one type-I error. Despite ideological criticisms and objections to both these methods (Bonferroni: <xref ref-type="bibr" rid="bib31">Perneger, 1998</xref>; FDR: <xref ref-type="bibr" rid="bib23">Mayo, 2017</xref>), the Bonferroni correction is a conservative procedure that allows for more type II errors to occur and the FDR is a liberal method (i.e. allows for more type I errors). Together they offer a baseline range that allows us to contrast how the sequential correction procedures perform together.</p><p>In the second simulation, the false discovery rate was also calculated to evaluate different correction methods. To calculate this metric, the average number of true positives was divided by the average number of discoveries (average false positives + average true positives).</p></sec><sec id="s12-5"><title>Sequential correction procedures</title><p><italic>Uncorrected. </italic>This procedure is to not correct for any sequential analyses. This analogous to reusing open data with no consideration for any sequential tests that occur due to data reuse. For all sequential hypothesis tests, p&lt;0.05 was considered a significant or positive finding.</p><p><inline-formula><mml:math id="inf107"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-debt</italic>. A sequential correction procedure that, to our knowledge, has not previously been proposed. At the first hypothesis tested, <inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> sets the statistical significance threshold (here 0.05). At the ith hypothesis tested the statistical threshold is <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>. The rationale here is that, at the ith test, a Bonferroni correction is applied that considers there to be <inline-formula><mml:math id="inf110"><mml:mi>i</mml:mi></mml:math></inline-formula> number of tests performed. This method lets the false positive rate increase (i.e. the debt of reusing the dataset) as each test corrects for the overall number of tests, but all earlier tests have a more liberal threshold. The total possible 'debt' incurred for <inline-formula><mml:math id="inf111"><mml:mi>m</mml:mi></mml:math></inline-formula> number of sequential tests can be calculated by <inline-formula><mml:math id="inf112"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and will determine the actual false positive rate.</p><p><inline-formula><mml:math id="inf113"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-spending</italic>. A predefined <inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is selected, which is called the <inline-formula><mml:math id="inf115"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth. At the ith test the statistical threshold, <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, a value is selected to meet the condition that <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The ith test selects <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that spends part of the remaining '<inline-formula><mml:math id="inf119"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth'. The remaining <inline-formula><mml:math id="inf120"><mml:mi>α</mml:mi></mml:math></inline-formula>-wealth at test <inline-formula><mml:math id="inf121"><mml:mi>i</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Like, <inline-formula><mml:math id="inf123"><mml:mi>α</mml:mi></mml:math></inline-formula>-debt, this method effectively decreases the p-value threshold of statistical significance at each test. However, it can also ensure that the false positive rate of all statistical tests is never higher than <inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Here, at test <inline-formula><mml:math id="inf125"><mml:mi>i</mml:mi></mml:math></inline-formula> we always spend 50% of <inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is set to 0.05. See <xref ref-type="bibr" rid="bib12">Foster and Stine, 2008</xref> for more details.</p><p><inline-formula><mml:math id="inf128"><mml:mi>α</mml:mi></mml:math></inline-formula><italic>-investing</italic>. The two previous methods only allow for the statistical threshold to decrease over time and are more akin to familywise error correction procedures. An alternative approach, which is closer to false discovery rate procedures, is to ensure the false discovery rate remains below a predefined wealth value (<inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib12">Foster and Stine, 2008</xref>). At each test, <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is selected from the remaining wealth at <inline-formula><mml:math id="inf131"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. If the sequentially indexed test <inline-formula><mml:math id="inf132"><mml:mi>i</mml:mi></mml:math></inline-formula> was considered statistically significant (i.e. rejecting the null hypothesis), then <inline-formula><mml:math id="inf133"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> increases: <inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:math></inline-formula>. Alternatively, if the null hypothesis cannot be rejected at <inline-formula><mml:math id="inf135"><mml:mi>i</mml:mi></mml:math></inline-formula>, then the wealth decreases: <inline-formula><mml:math id="inf136"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula>. We set <inline-formula><mml:math id="inf137"><mml:mi>ω</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, which is the convention, <inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to 0.05, and <inline-formula><mml:math id="inf140"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is set to 50% of the remaining wealth. See <xref ref-type="bibr" rid="bib12">Foster and Stine, 2008</xref> for more details.</p><p>When combining the simultaneous and sequential correction procedures in the empirical example, we used the sequential correction procedure to derive <inline-formula><mml:math id="inf141"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which we then used as the threshold in the simultaneous correction.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Pontus Plavén-Sigray, Lieke de Boer, Nina Becker, Granville Matheson, Björn Schiffler, and Gitanjali Bhattacharjee for helpful discussions and feedback.</p></ack><sec id="s13" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Conceptualization</p></fn><fn fn-type="con" id="con3"><p>Conceptualization</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision</p></fn></fn-group></sec><sec id="s14" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>The variables selected for analysis in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53498-supp1-v1.csv"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53498-transrepform-v1.docx"/></supplementary-material></sec><sec id="s15" sec-type="data-availability"><title>Data availability</title><p>All empirical data used in Figure 3 originates from the Human Connectome Project (<ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org/">https://www.humanconnectome.org/</ext-link>) from the 1200 healthy subject release. Code for the simulations and analyses is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/wiheto/datasetdecay">https://github.com/wiheto/datasetdecay</ext-link>.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharoni</surname> <given-names>E</given-names></name><name><surname>Rosset</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generalized <italic>α</italic> -investing: definitions, optimality results and application to public databases</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>76</volume><fpage>771</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1111/rssb.12048</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.2307/2346101</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bennett</surname> <given-names>CM</given-names></name><name><surname>Wolford</surname> <given-names>GL</given-names></name><name><surname>Miller</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The principled control of false positives in neuroimaging</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>4</volume><fpage>417</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1093/scan/nsp053</pub-id><pub-id pub-id-type="pmid">20042432</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bennett</surname> <given-names>CM</given-names></name><name><surname>Baird</surname> <given-names>AA</given-names></name><name><surname>Miller</surname> <given-names>MB</given-names></name><name><surname>George</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: an argument for proper multiple comparisons correction</article-title><source>Journal of Serendipitous and Unexpected Results</source><volume>1</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(09)71202-9</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Double-dipping revisited</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>688</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0398-z</pub-id><pub-id pub-id-type="pmid">31011228</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>A remark on multiple comparison methods</article-title><source>Technometrics</source><volume>7</volume><fpage>223</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1080/00401706.1965.10490250</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cramer</surname> <given-names>AO</given-names></name><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name><name><surname>Steingroever</surname> <given-names>H</given-names></name><name><surname>Wetzels</surname> <given-names>R</given-names></name><name><surname>Grasman</surname> <given-names>RP</given-names></name><name><surname>Waldorp</surname> <given-names>LJ</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hidden multiplicity in exploratory multiway ANOVA: prevalence and remedies</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>640</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0913-5</pub-id><pub-id pub-id-type="pmid">26374437</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cumming</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The new statistics: why and how</article-title><source>Psychological Science</source><volume>25</volume><fpage>7</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1177/0956797613504966</pub-id><pub-id pub-id-type="pmid">24220629</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoho</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>50 years of data science</article-title><source>Journal of Computational and Graphical Statistics</source><volume>26</volume><fpage>745</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1080/10618600.2017.1384734</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dwork</surname> <given-names>C</given-names></name><name><surname>Feldman</surname> <given-names>V</given-names></name><name><surname>Hardt</surname> <given-names>M</given-names></name><name><surname>Pitassi</surname> <given-names>T</given-names></name><name><surname>Reingold</surname> <given-names>O</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Preserving statistical validity in adaptive data analysis</article-title><conf-name>Proceedings of the Annual ACM Symposium on Theory of Computing</conf-name><fpage>14</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1145/2746539.2746580</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dwork</surname> <given-names>C</given-names></name><name><surname>Feldman</surname> <given-names>V</given-names></name><name><surname>Hardt</surname> <given-names>M</given-names></name><name><surname>Pitassi</surname> <given-names>T</given-names></name><name><surname>Reingold</surname> <given-names>O</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Guilt-free data reuse</article-title><source>Communications of the ACM</source><volume>60</volume><fpage>86</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1145/3051088</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>DP</given-names></name><name><surname>Stine</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>α-investing: a procedure for sequential control of expected false discoveries</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>70</volume><fpage>429</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2007.00643.x</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Games</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Multiple comparisons of means</article-title><source>American Educational Research Journal</source><volume>8</volume><fpage>531</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.3102/00028312008003531</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Carlin</surname> <given-names>JB</given-names></name><name><surname>Stern</surname> <given-names>HS</given-names></name><name><surname>Dunson</surname> <given-names>DB</given-names></name><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Bayesian Data Analysis</source><publisher-name>Chapman Hall/CRC</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gigerenzer</surname> <given-names>G</given-names></name><name><surname>Marewski</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Surrogate science: the idol of a universal method for scientific inference</article-title><source>Journal of Management</source><volume>41</volume><fpage>421</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1177/0149206314547522</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname> <given-names>GR</given-names></name><name><surname>Klockars</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The quest for α: developments in multiple comparison procedures in the quarter century since games (1971)</article-title><source>Review of Educational Research</source><volume>66</volume><fpage>269</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.2307/1170524</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hochberg</surname> <given-names>J</given-names></name><name><surname>Tamhane</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="1987">1987</year><chapter-title>Introduction</chapter-title><person-group person-group-type="editor"><name><surname>Hochberg</surname> <given-names>J</given-names></name><name><surname>Tamhane</surname> <given-names>A. C</given-names></name></person-group><source>Multiple Comparison Procedures</source><publisher-name>John Wiley &amp; Sons</publisher-name><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1002/9780470316672</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Options for publishing research without any P-values</article-title><source>European Heart Journal</source><volume>40</volume><fpage>2555</fpage><lpage>2556</lpage><pub-id pub-id-type="doi">10.1093/eurheartj/ehz556</pub-id><pub-id pub-id-type="pmid">31411717</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jebb</surname> <given-names>AT</given-names></name><name><surname>Parrigon</surname> <given-names>S</given-names></name><name><surname>Woo</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Exploratory data analysis as a foundation of inductive research</article-title><source>Human Resource Management Review</source><volume>27</volume><fpage>265</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1016/j.hrmr.2016.08.003</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruschke</surname> <given-names>JK</given-names></name><name><surname>Liddell</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The bayesian new statistics: from a Bayesian perspective</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>178</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1221-4</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>DK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Alternatives to P value: confidence interval and effect size</article-title><source>Korean Journal of Anesthesiology</source><volume>69</volume><fpage>555</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.4097/kjae.2016.69.6.555</pub-id><pub-id pub-id-type="pmid">27924194</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Maxwell</surname> <given-names>SE</given-names></name><name><surname>Delaney</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>Designing experiments and analyzing data: A model comparison perspective</chapter-title><source>Mixed Models</source><edition>2nd Ed</edition><publisher-name>Lawrence Erlbaum Associates, Inc, Publishers</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mayo</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A poor prognosis for the diagnostic screening critique of statistical tests</article-title><source>OSF Preprints</source><pub-id pub-id-type="doi">10.17605/OSF.IO/PS38B</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKiernan</surname> <given-names>EC</given-names></name><name><surname>Bourne</surname> <given-names>PE</given-names></name><name><surname>Brown</surname> <given-names>CT</given-names></name><name><surname>Buck</surname> <given-names>S</given-names></name><name><surname>Kenall</surname> <given-names>A</given-names></name><name><surname>Lin</surname> <given-names>J</given-names></name><name><surname>McDougall</surname> <given-names>D</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Ram</surname> <given-names>K</given-names></name><name><surname>Soderberg</surname> <given-names>CK</given-names></name><name><surname>Spies</surname> <given-names>JR</given-names></name><name><surname>Thaney</surname> <given-names>K</given-names></name><name><surname>Updegrove</surname> <given-names>A</given-names></name><name><surname>Woo</surname> <given-names>KH</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How open science helps researchers succeed</article-title><source>eLife</source><volume>5</volume><elocation-id>e16800</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16800</pub-id><pub-id pub-id-type="pmid">27387362</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McShane</surname> <given-names>BB</given-names></name><name><surname>Gal</surname> <given-names>D</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Robert</surname> <given-names>C</given-names></name><name><surname>Tackett</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Abandon statistical significance</article-title><source>The American Statistician</source><volume>73</volume><fpage>235</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1080/00031305.2018.1527253</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="1981">1981</year><source>Simultaneous Statistical Inference</source><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-45182-9</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molloy</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The Open Knowledge Foundation: open data means better science</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1001195</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001195</pub-id><pub-id pub-id-type="pmid">22162946</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Neto</surname> <given-names>EC</given-names></name><name><surname>Hoff</surname> <given-names>BR</given-names></name><name><surname>Bare</surname> <given-names>C</given-names></name><name><surname>Bot</surname> <given-names>BM</given-names></name><name><surname>Yu</surname> <given-names>T</given-names></name><name><surname>Magravite</surname> <given-names>L</given-names></name><name><surname>Stolovitzky</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reducing overfitting in challenge-based competitions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.00091">http://arxiv.org/abs/1607.00091</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Alter</surname> <given-names>G</given-names></name><name><surname>Banks</surname> <given-names>GC</given-names></name><name><surname>Borsboom</surname> <given-names>D</given-names></name><name><surname>Bowman</surname> <given-names>SD</given-names></name><name><surname>Breckler</surname> <given-names>SJ</given-names></name><name><surname>Buck</surname> <given-names>S</given-names></name><name><surname>Chambers</surname> <given-names>CD</given-names></name><name><surname>Chin</surname> <given-names>G</given-names></name><name><surname>Christensen</surname> <given-names>G</given-names></name><name><surname>Contestabile</surname> <given-names>M</given-names></name><name><surname>Dafoe</surname> <given-names>A</given-names></name><name><surname>Eich</surname> <given-names>E</given-names></name><name><surname>Freese</surname> <given-names>J</given-names></name><name><surname>Glennerster</surname> <given-names>R</given-names></name><name><surname>Goroff</surname> <given-names>D</given-names></name><name><surname>Green</surname> <given-names>DP</given-names></name><name><surname>Hesse</surname> <given-names>B</given-names></name><name><surname>Humphreys</surname> <given-names>M</given-names></name><name><surname>Ishiyama</surname> <given-names>J</given-names></name><name><surname>Karlan</surname> <given-names>D</given-names></name><name><surname>Kraut</surname> <given-names>A</given-names></name><name><surname>Lupia</surname> <given-names>A</given-names></name><name><surname>Mabry</surname> <given-names>P</given-names></name><name><surname>Madon</surname> <given-names>T</given-names></name><name><surname>Malhotra</surname> <given-names>N</given-names></name><name><surname>Mayo-Wilson</surname> <given-names>E</given-names></name><name><surname>McNutt</surname> <given-names>M</given-names></name><name><surname>Miguel</surname> <given-names>E</given-names></name><name><surname>Paluck</surname> <given-names>EL</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Soderberg</surname> <given-names>C</given-names></name><name><surname>Spellman</surname> <given-names>BA</given-names></name><name><surname>Turitto</surname> <given-names>J</given-names></name><name><surname>VandenBos</surname> <given-names>G</given-names></name><name><surname>Vazire</surname> <given-names>S</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perneger</surname> <given-names>TV</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>What's wrong with Bonferroni adjustments</article-title><source>BMJ</source><volume>316</volume><fpage>1236</fpage><lpage>1238</lpage><pub-id pub-id-type="doi">10.1136/bmj.316.7139.1236</pub-id><pub-id pub-id-type="pmid">9553006</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisani</surname> <given-names>E</given-names></name><name><surname>Aaby</surname> <given-names>P</given-names></name><name><surname>Breugelmans</surname> <given-names>JG</given-names></name><name><surname>Carr</surname> <given-names>D</given-names></name><name><surname>Groves</surname> <given-names>T</given-names></name><name><surname>Helinski</surname> <given-names>M</given-names></name><name><surname>Kamuya</surname> <given-names>D</given-names></name><name><surname>Kern</surname> <given-names>S</given-names></name><name><surname>Littler</surname> <given-names>K</given-names></name><name><surname>Marsh</surname> <given-names>V</given-names></name><name><surname>Mboup</surname> <given-names>S</given-names></name><name><surname>Merson</surname> <given-names>L</given-names></name><name><surname>Sankoh</surname> <given-names>O</given-names></name><name><surname>Serafini</surname> <given-names>M</given-names></name><name><surname>Schneider</surname> <given-names>M</given-names></name><name><surname>Schoenenberger</surname> <given-names>V</given-names></name><name><surname>Guerin</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Beyond open data: realising the health benefits of sharing data</article-title><source>BMJ</source><volume>355</volume><elocation-id>i5295</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.i5295</pub-id><pub-id pub-id-type="pmid">27758792</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Roelofs</surname> <given-names>R</given-names></name><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Hardt</surname> <given-names>M</given-names></name><name><surname>Fridovich-keil</surname> <given-names>S</given-names></name><name><surname>Schmidt</surname> <given-names>L</given-names></name><name><surname>Recht</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A meta-analysis of overfitting in machine learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><ext-link ext-link-type="uri" xlink:href="http://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning">http://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rogers</surname> <given-names>R</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>A</given-names></name><name><surname>Srebro</surname> <given-names>N</given-names></name><name><surname>Thakkar</surname> <given-names>O</given-names></name><name><surname>Woodworth</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Guaranteed validity for empirical approaches to adaptive data analysis</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1906.09231.pdf">https://arxiv.org/pdf/1906.09231.pdf</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>The file drawer problem and tolerance for null results</article-title><source>Psychological Bulletin</source><volume>86</volume><fpage>638</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.86.3.638</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Samartsidis</surname> <given-names>P</given-names></name><name><surname>Montagna</surname> <given-names>S</given-names></name><name><surname>Laird</surname> <given-names>AR</given-names></name><name><surname>Fox</surname> <given-names>PT</given-names></name><name><surname>Johnson</surname> <given-names>TD</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Estimating the number of missing experiments in a neuroimaging meta-analysis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/225425</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simmons</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>P-curve: a key to the file drawer</article-title><source>Journal of Experimental Psychology: General</source><volume>143</volume><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1037/a0033242</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname> <given-names>WH</given-names></name><name><surname>Wright</surname> <given-names>J</given-names></name><name><surname>Bissett</surname> <given-names>PG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Open exploration</article-title><source>eLife</source><volume>9</volume><elocation-id>e52157</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.52157</pub-id><pub-id pub-id-type="pmid">31916934</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thompson</surname> <given-names>WH</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>datasetdecay</data-title><source>GitHub</source><version designator="c06a705">c06a705</version><ext-link ext-link-type="uri" xlink:href="https://github.com/wiheto/datasetdecay">https://github.com/wiheto/datasetdecay</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tukey</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>We need both exploratory and confirmatory</article-title><source>American Statistician</source><volume>34</volume><fpage>23</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/00031305.1980.10482706</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tukey</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The philosophy of multiple comparisons</article-title><source>Statistical Science</source><volume>6</volume><fpage>100</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011945</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Auerbach</surname> <given-names>E</given-names></name><name><surname>Barch</surname> <given-names>D</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Bucholz</surname> <given-names>R</given-names></name><name><surname>Chang</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Curtiss</surname> <given-names>SW</given-names></name><name><surname>Della Penna</surname> <given-names>S</given-names></name><name><surname>Feinberg</surname> <given-names>D</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Harel</surname> <given-names>N</given-names></name><name><surname>Heath</surname> <given-names>AC</given-names></name><name><surname>Larson-Prior</surname> <given-names>L</given-names></name><name><surname>Marcus</surname> <given-names>D</given-names></name><name><surname>Michalareas</surname> <given-names>G</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name><name><surname>Prior</surname> <given-names>F</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2012">2012</year><article-title>The Human Connectome Project: a data acquisition perspective</article-title><source>NeuroImage</source><volume>62</volume><fpage>2222</fpage><lpage>2231</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id><pub-id pub-id-type="pmid">22366334</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Barch</surname> <given-names>DM</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The WU-Minn Human Connectome Project: an overview</article-title><source>NeuroImage</source><volume>80</volume><fpage>62</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id><pub-id pub-id-type="pmid">23684880</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varoquaux</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cross-validation failure: small sample sizes lead to large error bars</article-title><source>NeuroImage</source><volume>180</volume><fpage>68</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.061</pub-id><pub-id pub-id-type="pmid">28655633</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wetzels</surname> <given-names>R</given-names></name><name><surname>Borsboom</surname> <given-names>D</given-names></name><name><surname>van der Maas</surname> <given-names>HL</given-names></name><name><surname>Kievit</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An agenda for purely confirmatory research</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>632</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1177/1745691612463078</pub-id><pub-id pub-id-type="pmid">26168122</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wasserstein</surname> <given-names>RL</given-names></name><name><surname>Schirm</surname> <given-names>AL</given-names></name><name><surname>Lazar</surname> <given-names>NA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Moving to a world beyond “p&lt;0.05&quot;</article-title><source>American Statistician</source><volume>73</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1080/00031305.2019.1583913</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weston</surname> <given-names>SJ</given-names></name><name><surname>Ritchie</surname> <given-names>SJ</given-names></name><name><surname>Rohrer</surname> <given-names>JM</given-names></name><name><surname>Przybylski</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recommendations for increasing the transparency of analysis of preexisting data sets</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>2</volume><fpage>214</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1177/2515245919848684</pub-id><pub-id pub-id-type="pmid">32190814</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname> <given-names>T</given-names></name><name><surname>Westfall</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Choosing prediction over explanation in psychology: lessons from machine learning</article-title><source>Perspectives on Psychological Science</source><volume>12</volume><fpage>1100</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1177/1745691617693393</pub-id><pub-id pub-id-type="pmid">28841086</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53498.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Holmes</surname><given-names>Nick</given-names></name><role>Reviewer</role><aff><institution>University of Glasgow</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Dataset decay and the problem of sequential analyses on open datasets&quot; for consideration by <italic>eLife</italic>. Please note that following a discussion among the relevant editors, your article was considered as a Feature Article rather than as a Research Article.</p><p>Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor (Chris I Baker) and the <italic>eLife</italic> Features Editor (Peter Rodgers). The following individuals involved in review of your submission have agreed to reveal their identity: Chris I Baker (Reviewer #1); Nick Holmes (Reviewer #2); Guillaume A Rousselet (Reviewer #3).</p><p>Summary:</p><p>The reviewers all agreed that the manuscript focused on an important topic, and they all appreciated the simulations and analyses included it. However, the manuscript would benefit from clarifying a number of points - please see below. In particular, some passages require more in-depth discussion, and the passage that discuss potential solutions need to be fleshed out.</p><p>Essential revisions:</p><p>1) The notion of exploratory versus confirmatory analyses is ultimately a key issue in this manuscript. Indeed the authors propose that one solution to the problem of sequential analyses is to treat all studies using open data as exploratory. However, the authors do not clearly define or discuss these terms or the implications of labelling analyses as one or the other. I think the manuscript would benefit from more explicitly describing and discussing the distinction between exploratory and confirmatory, rather than assuming that everyone is already on the same page.</p><p>2) Another important issue is the ability to determine how many prior tests have been performed on a dataset. As the authors note several times in the discussion, the &quot;file drawer&quot; problem is of major concern here. But the authors do not really consider or discuss how this problem could possibly be dealt with. For example, should authors be required to preregister the analyses they plan to perform before being given access to a dataset? I think this is such an important issue in the context of the current manuscript that it deserves more in depth discussion - even if the field decides on an appropriate method of correction, that will only prove useful if there is a way to track the number of tests performed and not just those that resulted in a significant effect and hence publication.</p><p>3) In general, while the manuscript does a good job of highlighting the problem of sequential analyses on open datasets and discusses some possible solutions, it does not really suggest any recommendations for the path forward. How should the field grapple with these issues? Which of the possible solutions should be favored, if any? How should the field decide what is the best solution? How should we keep track of analyses on open datasets.</p><p>4) In the abstract, the authors state that &quot;we should expect a dataset's utility for discovering new true relations between variables to decay'. I don't quite follow this. The alpha level is about controlling the false-positive rates. I do not see a clear link between this and the likelihood of new true-positive discoveries (which would require consideration of likely effect sizes, power, etc). If a researcher has a (true) hypothesis which is clearly-predicted by theory, convergent with other datasets, supported by a small experiment, and comes with a precise effect-size, I do not see why *any* number of prior tests of an open dataset should affect that open dataset's ability to support the researcher's well-defined hypothesis. These researchers could, indeed, simply abandon the null-hypothesis significance-testing approach for the open dataset, and simply ask: what are the likely values for the effect that I am quite sure exists? Perhaps by 'new true relations' the authors here mean 'unpredicted' or 'novel' or even 'random' and only within the NHST approach? So, my general comment here is that I am uncomfortable with the idea that open data sets decay in usefulness with time, and I would ask the authors to consider this, and soften or qualify the description.</p><p>5) I can see that the definition of what is a 'family' of tests is fundamental here. What I find a bit odd about this sequential testing approach is that, at the outset, the size of the family is not known. But it could be predicted based on prior or concurrent datasets, or could be set at an arbitrary level. Have the authors considered, for example: the first X hypothesis tests can use a certain alpha level, then the next X tests, etc. This stratified sequential approach would set the size of each family from the outset, and allow everyone in the first X groups of researchers to work on a level playing field (there would then, hopefully, be no great rush to be the first researchers to test their dubious hypotheses without thought, thus wasting their scoop of alpha).</p><p>6) The sequential correction methods all punish late-comers to the data party. Perhaps a particular dataset is perfect to test an idea which has not yet been developed - the data comes before the idea's time. It seems wrong that good researchers or good ideas who happen to arrive at the dataset late relative to other (worse) researchers or ideas should be 'punished' with higher alphas just for being late. (Not wishing to increase the admin-burden, ) perhaps some of the alpha can be saved up for a rainy day? Perhaps some of the alpha can be won or awarded through a competitive merit-based process? Perhaps researchers who meet a certain level of good research practice (e.g., pre-registration, ethical, open, all the necessary review, meta-analysis, and experimental work already in place, etc), should be allowed to use standard alpha levels, and it is only the disorganised vultures feeding on the data carcass who should be discouraged with alpha-punishment?</p><p>7) If the dataset comprises *all* the available data on a particular topic (e.g., brains affected by 'mad cow disease' in Britain in the late 1990s) - i.e., it is the population, and not just a sample - does this change the assumptions or outcomes of the authors' approach at all? It feels like it should be a special case, but maybe not...</p><p>8) Relatedly, if a dataset is large, one solution could be simply to restrict researchers to a random sample of the dataset (say, 10% of the available data), and allow them to keep alpha at standard levels. Because exactly the same data will almost never be tested twice on the same or a different hypothesis, does this completely remove the problems of inflated false-positives? It feels to me like it should. Should alpha correction only apply to researchers who use exactly the same subset and/or all the dataset?</p><p>9) In the authors' simulations, to estimate the likely number of publications resulting from false-positive findings, they assume that *every single 'significant' finding* will lead directly to a single publication - in effect, that publication bias is absolute, complete, and is itself unbiased. I find this assumption very hard to stomach. Researchers may tend to hold back significant results which don't support their, or their supervisors' or group's prior, current, or proposed research. Publication bias is not simply the immediate publication of (false) positive results, but also the delayed or suppressed publication of (true) negative or opposite-direction (false positive) results. Further, many (good) labs would replicate a result before going to press, or at least report multiple results, true and false, in the same paper. The authors may have stated this in other ways, but I think this strong assumption leads only to a very upper bound on the likely number of resulting (false positive) papers. Perhaps this can be stressed more clearly?</p><p>10) The authors used a real dataset to test a series of psychological hypotheses. They seem to have assumed that none of these hypotheses would pick up on any real effects in the data. Can they comment on the likelihood that their tests are establishing the true null distribution of effects, rather than actually being skewed by real effects? One solution would be to scramble the raw data in some way to ensure that even if there was a true effect, randomised and then processed voxels would not show this effect.</p><p>11) The introduction and discussion could do a better job at contrasting different empirical traditions and statistical approaches. The introduction could make clearer that the current project assumes that most researchers are engaged in a particular (though dominant) type of research involving confirmatory hypothesis testing in which the goal is to explain rather than predict. Do you think the problem would be different if the focus was on prediction?</p><p>The discussion mentions cross-validation and the problem with small sample sizes, but doesn't acknowledge explicitly the tension between explanatory and predictive research - Yarkoni &amp; Westfall (2017) is a great reference on that topic:</p><p>https://doi.org/10.1177/1745691617693393</p><p>12) FDR is not clearly defined and would need to be better justified given the strong limitations of such a concept, which Richard Morey and Deborah Mayo described as completely flawed:</p><p>https://osf.io/ps38b/</p><p>https://medium.com/@richarddmorey/redefining-statistical-significance-the-statistical-arguments-ae9007bc1f91</p><p>13) What if we are in a field in which inappropriate statistical methods are the norm: should future researcher using appropriate tools be penalised for analysing a dataset after many doomed attempts? You touch indirectly on the subject in the section &quot;Gray-area when families are not clear&quot;. For instance, in a field dominated by fixed effect analyses of means, I would argue that researchers attempting to fit carefully justified generalised hierarchical models should be allowed to reset their alpha counter.</p><p>14) The discussion mentions Bayesian statistics as a potential solution, but with the current trend in adopting arbitrary thresholds for Bayes factors, the same problems encountered in mindless frequentist practices will also apply to Bayesian/Laplacian ones:</p><p>Gigerenzer, G. &amp; Marewski, J.N. (2015) Surrogate Science: The Idol of a Universal Method for Scientific Inference. Journal of Management, 41, 421-440.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53498.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>The reviewers comments were very helpful. We have three new subsections to the manuscript. One at the start, one in the methods, and one new section in the discussion. These address points raised by the reviewers, so we have decided to provide them in full here and motivate them generally first, before addressing each Essential revision point individually.</p><p><italic># General response 1. New section: “Preliminary assumptions” (Methods section)</italic></p><p>First, the reviewers have raised, on multiple occasions, issues regarding NHST, proposed alternatives to p-values, types of datasets, and asked about certain types of inferences that could be made. Thus, we have written an “assumptions” section to motivate certain choices in our analysis and discuss the implications of these choices. This section reads:</p><p>“Preliminary assumptions</p><p>In this article, we put forward the argument that sequential statistical tests on open data could lead to an increase in the number of false positives. This argument requires several assumptions regarding (1) the type of datasets analysed; (2) what kind of statistical inferences are performed; (3) the types of sequential analyses considered.</p><p><italic>The type of dataset:</italic> we consider a dataset to be a fixed static snapshot of data collected at a specific time point. There are other cases of combining datasets or datasets that grow over time, but we will not consider those here. Second, we assume a dataset to be a random sample of a population and not a dataset that contains information about a full population.</p><p><italic>The type of statistical testing:</italic> We have framed our discussion of statistical inference using null hypothesis statistical testing (NHST). This assumption entails that we will use thresholded p-values to infer whether a finding differs from the null hypothesis. Our decision for this choice is motivated by a belief that the NHST framework being the most established framework for dealing with multiple statistical tests. There have been multiple valid critiques and suggestions to improve upon this statistical practice by moving away from thresholded p-values to evaluate hypotheses (Cumming, 2014; Ioannidis, 2019; Lee, 2016; McShane, Gal, Gelman, Robert, &amp; Tackett, 2019; Wasserstein, Schirm, &amp; Lazar, 2019). Crucially, however, many proposed alternative approaches within statistical inference do not circumnavigate the problem of multiple statistical testing. For example, if confidence intervals are reported and used for inference regarding hypotheses, these should also be adjusted for multiple statistical tests (see, e.g. Tukey (1991)). Thus, any alternative statistical frameworks that still must correct for multiple simultaneous statistical testing will have the same sequential statistical testing problem that we outline here. Thus, while we have chosen NHST for simplicity and familiarity, this does not entail that the problem is isolated to NHST. Solutions for different frameworks may however differ (see the discussion section for Bayesian approaches and prediction-based inference perspectives).</p><p><italic>The types of analyses:</italic> Sequential analyses involve statistical tests on the same data. Here, we consider sequential analyses that reuse the same data and analyses to be a part of the same statistical family (see section on statistical families for more details). Briefly, this involves either the statistical inferences being classed as exploratory or answering the same confirmatory hypothesis or research question. Further, we only consider analyses that are not supersets of previous analyses. This assumption entails that we are excluding analyses where a statistical model may improve upon a previous statistical model by, for example, adding an additional layer in a hierarchical model. Other types of data reuse may not be appropriate for sequential correction methods and are not considered here.</p><p>While we have restrained our analysis with these assumptions and definitions, it is done primarily to simplify the argument regarding the problem we are identifying. The degree to which sequential tests are problematic in more advanced cases remains outside the scope of this paper.”</p><p>We believe this section addresses many of the concerns raised by the reviewers. It clarifies our argument but does not reduce the importance of our results.</p><p><italic>#General response 2. New section: “An intuitive example of the problem”</italic></p><p>We feared that our argument was getting interpreted as a conceptual debate about what should be included in a “family” of tests where the simple solution is to say “we decided to have small statistical families”. While important (which we address later), we feared this was making the manuscript lose its focus. Thus, before the discussion of statistical families, the simulations, and empirical results, we present a simple example of the problem:</p><p><bold>“</bold>An intuitive example of the problem</p><p>Before proceeding with technical details of the problem, we outline an intuitive problem regarding sequential statistical testing and open data. Imagine there is a dataset which contains the variables (𝑣1, 𝑣2, 𝑣3). Let us now imagine that one researcher performs the statistical tests to analyze the relationship between 𝑣1 ∼ 𝑣2 and 𝑣1 ∼ 𝑣3 and decides that a 𝑝&lt;0.05 is treated as a positive finding (i.e. null hypothesis rejected). The analysis yields p-values of 𝑝=0.001 and 𝑝=0.04 respectively. In many cases, we expect the researcher to correct for the fact that two statistical tests are being performed. Thus, the researcher chooses to apply a Bonferroni correction such that p &lt; 0.025 is the adjusted threshold for statistical significance. In this case, both tests are published, but only one of the findings is treated as a positive finding.</p><p>Alternatively, let us consider a different scenario with sequential analyses and open data. Instead, the researcher only performs one statistical test (𝑣1 ∼ 𝑣2, p=0.001). No correction is performed, and it is considered a positive finding (i.e. null hypothesis rejected). The dataset is then published online. A second researcher now performs the second test (𝑣1 ∼ 𝑣3, p=0.04) and deems this a positive finding too because it is under a 𝑝&lt;0.05 threshold and they have only performed one statistical test. In this scenario, with the same data, we have two published positive findings compared to the single positive finding in the previous scenario. Unless a reasonable justification exists for this difference between the two scenarios, this is troubling.</p><p>What are the consequences of these two different scenarios? A famous example of the consequences of uncorrected multiple simultaneous statistical tests is the finding of fMRI BOLD activation in a dead salmon when appropriate corrections for multiple tests were not performed (Bennett, Baird, Miller, &amp; George, 2010; Bennett, Wolford, &amp; Miller, 2009). Now let us imagine this dead salmon dataset is published online but, in the original analysis, only one part of the salmon was analyzed, and no evidence was found supporting the hypothesis of neural activity in a dead salmon. Subsequent researchers could access this dataset, test different regions of the salmon and report their uncorrected findings. Eventually, we would see reports of dead salmon activations if no sequential correction strategy is applied, but each of these individual findings would appear completely legitimate by current correction standards.</p><p>We will now explore the idea of sequential tests in more detail, but this example highlights some crucial arguments that need to be discussed. Can we justify the sequential analysis without correcting for sequential tests? If not, what methods could sequentially correct for the multiple statistical tests? In order to fully grapple with these questions, we first need to discuss the notion of a statistical family and whether sequential analyses create new families.”</p><p>We hope the reviewers agree this section hopes makes the argument clearer by providing this example.</p><p>#<italic>General response 3. New section: “Towards a solution”</italic></p><p>A considerable number of the suggestions made by the reviewers were regarding possible solutions to the problem. Many were good suggestions, but many do compromise one of the three desiderata we had outlined in the introduction (that the reviewers did not challenge) and one of our conclusions is that we will have to compromise on one of these desiderata. However, we agree that it is worthwhile spending some time discussing possible solutions. Thus, we have listed some of the possible solutions that could be implemented in the Discussion section:</p><p>“Towards a solution</p><p>Statistics is a multifaceted tool for experimental researchers to use, but it (rarely) aims to provide universal solutions for all problems and use cases. Thus, it may be hard to expect a one size fits all solution to the problem of sequential tests on open data. Indeed, the idiosyncrasies within different disciplines regarding the size of data, open data infrastructure, and how often new data is collected, may necessitate that they adopt different solutions. Thus, any prescription we offer now is, at best, tentative. Further, the solutions also often compromise the desiderata in some way. That being said, there are some suggestions which should assist in mitigating the problem to different degrees. Some of these suggestions only require the individual researcher to adapt their practices, others require entire disciplines to form a consensus, and others require infrastructural changes. This section deals with solutions compatible with the null hypothesis testing framework, the next section considers solutions specific to other perspectives.</p><p><italic>Preregistration grace period of analyses prior to open data release.</italic> To increase the number of confirmatory analyses that can be performed on an open dataset, one possible solution is to have a &quot;preregistration grace period&quot;. Here a description of the data can be provided, and data re-users will have the opportunity to write a preregistration prior to the data being released. This solution allows for confirmatory analyses to be performed on open data while simultaneously being part of different statistical families. This idea follows Wagenmakers et al. (2012) definition of confirmatory analysis. Consequently, once the dataset or the first study using the dataset are published, the problems outlined in this paper will remain for all subsequent (non pre-registered) analyses reusing the data.</p><p><italic>Increased justification of the statistical family.</italic> One of the recurring problems regarding statistical testing is that, given the Wagenmakers et al. (2012) definition, it is hard to class open data reuse as confirmatory after data release. However, if disciplines decide that confirmatory analyses on open data (post-publication) are possible, one of our main arguments above is that a new paper does not automatically create a new statistical family. If researchers can, for other reasons, justify why their statistical family is separate in their analysis and state how it is different from previous statistical tests performed on the data, there is no necessity to sequentially correct. Thus providing sufficient justification for new a family in a paper can effectively reset the alpha wealth.</p><p><italic>Restrained or coordinated alpha-levels.</italic> One of the reasons the $\ One of the reasons the $\alpha$-values decays quickly in $\alpha$-invest and $\alpha$-spend is the 50% invest/spend rate that we chose in this article uses a large portion of the total $alpha$ in the initial statistical tests. For example, the first two tests in $\alpha$-spend, use 75% of the overall $\alpha$-wealth. Different spending or investing strategies are possible, which could restrain the decay of the remaining $\alpha-wealth$, allowing for more discoveries in later statistical tests. For example, a discipline could decide that the first ten statistical tests spend 5% of the $\alpha$ wealth, then the next ten spends 2.5% of the overall wealth. Such a strategy would still always remain under the overall wealth, but allow more people to utilize the dataset. However, imposing this restrained or fair-use of $\alpha$ spending would either require consensus from all researchers (however, this strategy would be in vain if just one researcher fails to comply) or restricting data access (compromising the open access desideratum). Importantly, this solution does not mitigate the decay of the alpha threshold; it just reduces the decay.</p><p>One of the reasons the 𝛼-values decays quickly in 𝛼-invest and 𝛼-spend is the 50% invest/spend rate that we chose in this article uses a large portion of the total 𝑎𝑙𝑝ℎ𝑎 in the initial statistical tests. For example, the first two tests in 𝛼-spend, use 75% of the overall 𝛼-wealth. Different spending or investing strategies are possible, which could restrain the decay of the remaining 𝛼 − 𝑤𝑒𝑎𝑙𝑡ℎ, allowing for more discoveries in later statistical tests. For example, a discipline could decide that the first ten statistical tests spend 5% of the 𝛼 wealth, then the next ten spends 2.5% of the overall wealth. Such a strategy would still always remain under the overall wealth, but allow more people to utilize the dataset. However, imposing this restrained or fair-use of 𝛼 spending would either require consensus from all researchers (however, this strategy would be in vain if just one researcher fails to comply) or restricting data access (compromising the open access desideratum). Importantly, this solution does not mitigate the decay of the alpha threshold; it just reduces the decay.</p><p><italic>Metadata about reuse coupled to datasets.</italic> One of the problems regarding sequential corrections is knowing how many tests have been made using the dataset. This issue was partially addressed above with suggestions for estimating the number of preceding tests. Additionally, repositories could provide information about all known previous uses of the data. Thus if data repositories were able to track summaries of tests performed and which variables involved in the tests, this would, at the very least, help guide future users with rough estimates. In order for this number to be precise, it would, however, require limiting the access to the dataset (compromising the open access desideratum).</p><p>qHeld out data on repositories. A way to allow hypothesis testing or predictive frameworks (see below) to reuse the data is if the infrastructure exists that prevents the researcher from ever seeing some portion of the data. Dataset repositories could hold out data which data re-users can query their results against to either replicate their findings or test their predictive models. This perspective has seen success in machine learning competitions which hold out test data. Additional requirements could be added to this perspective, such as requiring preregistrations in order to query the held out data. However, there have been concerns that held out data can lead to overfitting (e.g. by copying the best fitting model) (Neto et al., 2016) although others have argued this does not generally appear to be the case when evaluating overfitting (Roelofs et al., 2019). However, Roelofs et al. (2019) noted that overfitting appears to occur on smaller datasets, which might prevent it from being a general solution for all disciplines.</p><p><italic>Narrow hypotheses and minimal statistical families.</italic> One way to avoid the sequential testing problem is to ensure small family sizes. If we can justify that there should be inherently small family sizes, then there is no need to worry about the sequential problems outlined here. This solution would also entail that each researcher does not need to justify their own particular family choice (as suggested above), but rather a specific consensus of what the contested concept family actually means is achieved. This would require: (1) confirmatory hypothesis testing on open data is possible, (2) encouraging narrow (i.e. very specific) hypotheses that will help maintain minimal family sizes, as the specificity of the hypothesis will limit the overlap with any other statistical test. Narrow hypotheses for confirmatory analyses can lead to families which are small, and can avoid correcting for multiple statistical tests (both simultaneous and sequential). This strategy is a possible solution to the problem. However, science does not merely consist of narrow hypotheses. Broader hypotheses can still be used in confirmatory studies (for example, genetic or neuroimaging datasets often ask broader questions not knowing which specific gene or brain area is involved, but know that a gene or brain region should be involved to confirm a hypothesis about a larger mechanism). Thus, while possibly solving a portion of the problem, this solution is unlikely to be a general solution for all fields, datasets, and types of hypotheses.”</p><p>[We repeat the reviewers’ points here in italic, followed by our reply and a description of the changes made].</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The notion of exploratory versus confirmatory analyses is ultimately a key issue in this manuscript. Indeed the authors propose that one solution to the problem of sequential analyses is to treat all studies using open data as exploratory. However, the authors do not clearly define or discuss these terms or the implications of labelling analyses as one or the other. I think the manuscript would benefit from more explicitly describing and discussing the distinction between exploratory and confirmatory, rather than assuming that everyone is already on the same page.</p></disp-quote><p>This is indeed a distinction that is lurking in the background of the text and we agree with the reviewers that this could be made explicit. We also had not explicitly highlighted the consequences of the more stringent definitions of confirmatory research and how this impacts statistical families (especially in sequential analyses). We have thus added two paragraphs to the section “Statistical families” [emphasis added on new text]:</p><p>“A family is a set of tests which we relate the same error rate to (familywise error). What constitutes a family has been challenging to precisely define, and the existing guidelines often contain additional imprecise terminology (e.g. Cox, 1965; Games, 1971; Hancock &amp; Klockars, 1996; Hochberg &amp; Tamhane, 1987; Miller, 1981). Generally, tests are considered part of a family when: (i) multiple variables are being tested with no predefined hypothesis (i.e. exploration or data-dredging), or (ii) multiple pre-specified tests together help support the same or associated research questions (Hancock &amp; Klockars, 1996; Hochberg &amp; Tamhane, 1987). Even if following these guidelines, there can still be considerable disagreements about what constituents a statistical family, which can include both very liberal and very conservative inclusion criteria. <italic>An example of this discrepancy is seen in using a factorial ANOVA. Some have argued that the main effect and interaction are separate families as they answer 'conceptually distinct questions' (e.g. page 291 of Maxwell &amp; Delaney, 2004), while others would argue the opposite and state they are the same family (e.g. Cramer et al., 2016; 136 Hancock &amp; Klockars, 1996). Given the substantial leeway regarding the definition of family, recommendations have directed researchers to define and justify their family of tests a priori (Hancock &amp; Klockars, 1996; Miller, 1981).</italic></p><p><italic>A crucial distinction in the definition of a family is whether the analysis is confirmatory (i.e. hypothesis driven) or exploratory. Given issues regarding replication in recent years (Open Science Collaboration, 2015), there has been considerable effort placed into clearly demarcating what is exploratory and what is confirmatory. One prominent definition is that confirmatory research requires preregistration before seeing the data (Wagenmakers, Wetzels, Borsboom, Maas, &amp; Kievit, 2012). However, current practice often involves releasing open data with the original research article. Thus, all data reuse may be guided by the original or subsequent analyses (a HARKing-like problem where methods are formulated after some results are known (Button, 2019)). Therefore, if adopting this prominent definition of confirmatory research (Wagenmakers et al., 2012), it follows that any reuse of open data after publication must be exploratory unless the analysis is preregistered before the data release.</italic></p><p><italic>Some may find Wagenmakers et al. (2012) definition to be too stringent and instead would rather allow that confirmatory hypotheses can be stated at later dates despite the researchers having some information about the data from previous use. Others have said confirmatory analyses may not require preregistrations (Jebb, Parrigon, &amp; Woo, 2017) and have argued that confirmatory analyses on open data are possible (Weston et al., 2019). If analyses on open data can be considered confirmatory, then we need to consider the second guideline about whether statistical tests are answering similar or the same research questions. The answer to this question is not always obvious, as was highlighted above regarding factorial ANOVA. However, if a study reusing data can justify itself as confirmatory, then it must also justify that it is asking a 'conceptually distinct question' from previous instances that used the data. We are not claiming that this is not possible to justify, but the justification ought to be done if no sequential correction is applied as new families are not created just because the data is being reused (see next section).”</italic></p><p>We believe that all this new text to help clarify what statistical families are made the previous supplementary information that tried to give examples has become less useful. Thus, it has been removed.</p><disp-quote content-type="editor-comment"><p>2) Another important issue is the ability to determine how many prior tests have been performed on a dataset. As the authors note several times in the discussion, the &quot;file drawer&quot; problem is of major concern here. But the authors do not really consider or discuss how this problem could possibly be dealt with. For example, should authors be required to preregister the analyses they plan to perform before being given access to a dataset? I think this is such an important issue in the context of the current manuscript that it deserves more in depth discussion - even if the field decides on an appropriate method of correction, that will only prove useful if there is a way to track the number of tests performed and not just those that resulted in a significant effect and hence publication.</p></disp-quote><p>The file drawer problem is indeed a problem that is lurking in the background. We have made two different changes to address this. Firstly we have edited the manuscript substantially to remove the implication that only statistically significant findings are publishable. Instead we now talk about studies that have either positive findings or null findings, to prevent us from unintentionally promoting file drawer scenarios or behaviour.</p><p>Second, in relation to the file drawer problem, it is also relevant with regards to estimating the number of sequential tests that have been performed. We discuss the final paragraph of the subsection “Consequence for sequential tests and open data” in the Discussion:</p><p>“Finally, a practical issue that must be taken into consideration with all sequential correction procedures is whether it is ever possible to know the actual number of tests performed on an unrestricted dataset. This issue relates to the file drawer problem where there is a bias towards the publication of positive findings compared to null findings (Rosenthal, 1979). Until this is resolved, to fully sequentially correct for the number of previous tests corrected, an estimation of the number of tests may be required (e.g. by identifying publication biases (Samartsidis et al., 2017; Simonsohn, Nelson, &amp; Simmons, 2013)). Using such estimations is less problematic with 𝛼-debt because this only requires the number of tests to be known. Comparatively, 𝛼-investing requires the entire results chain of statistical tests to be known and 𝛼-spending requires knowing every 𝛼 value that has been used, both of which would require additional assumptions to estimate. However, even if 𝛼-debt correction underestimates the number of previous statistical tests, the number of false positives will be reduced compared to no sequential correction.”</p><disp-quote content-type="editor-comment"><p>3) In general, while the manuscript does a good job of highlighting the problem of sequential analyses on open datasets and discusses some possible solutions, it does not really suggest any recommendations for the path forward. How should the field grapple with these issues? Which of the possible solutions should be favored, if any? How should the field decide what is the best solution? How should we keep track of analyses on open datasets.</p></disp-quote><p>We have now included an entire section about possible solutions going forward. See General response 3 above for the text that we have added. It also discusses how we will arrive at the different solutions (i.e. through discipline consensus or infrastructural changes).</p><disp-quote content-type="editor-comment"><p>4) In the abstract, the authors state that &quot;we should expect a dataset's utility for discovering new true relations between variables to decay'. I don't quite follow this. The alpha level is about controlling the false-positive rates. I do not see a clear link between this and the likelihood of new true-positive discoveries (which would require consideration of likely effect sizes, power, etc). If a researcher has a (true) hypothesis which is clearly-predicted by theory, convergent with other datasets, supported by a small experiment, and comes with a precise effect-size, I do not see why *any* number of prior tests of an open dataset should affect that open dataset's ability to support the researcher's well-defined hypothesis. These researchers could, indeed, simply abandon the null-hypothesis significance-testing approach for the open dataset, and simply ask: what are the likely values for the effect that I am quite sure exists? Perhaps by 'new true relations' the authors here mean 'unpredicted' or 'novel' or even 'random' and only within the NHST approach? So, my general comment here is that I am uncomfortable with the idea that open data sets decay in usefulness with time, and I would ask the authors to consider this, and soften or qualify the description.</p></disp-quote><p>The logic of our argument is as follows: <italic>If</italic> we decide to control for the increasingly likelihood of false positives, <italic>then</italic> it will become harder to identify true positives. This appears true for all types of hypothesis testing, not just NHST (see General response 1). Obviously, if there is no wish to control for the increase in false positives, then the ability to identify true positives becomes unimpeded. Thus for increased precision in the text, we have amended the Abstract to say (emphasis added to show additions):</p><p>&quot;<italic>Thus, if correcting for this increase in hypothesis testing,</italic> we should expect a dataset’s utility for discovering new true relations between variables to decay.&quot;</p><p>And have changed the Discussion to say (emphasis added to show additions):</p><p>&quot;Broadly, we conclude that a dataset’s potential to identify new statistically significant relationships will decay over time as the number of sequential statistical tests increases <italic>when controlling for sequential tests.&quot;</italic></p><p>These additions qualify that the “decay” for true positives that we discuss is contingent on controlling for the increase in false positives.</p><disp-quote content-type="editor-comment"><p>5) I can see that the definition of what is a 'family' of tests is fundamental here. What I find a bit odd about this sequential testing approach is that, at the outset, the size of the family is not known. But it could be predicted based on prior or concurrent datasets, or could be set at an arbitrary level. Have the authors considered, for example: the first X hypothesis tests can use a certain alpha level, then the next X tests, etc. This stratified sequential approach would set the size of each family from the outset, and allow everyone in the first X groups of researchers to work on a level playing field (there would then, hopefully, be no great rush to be the first researchers to test their dubious hypotheses without thought, thus wasting their scoop of alpha).</p></disp-quote><p>Firstly, see the reply to Essential revision 1 about changes made to the section on statistical families. See also General response 2 for a intuitive problem about sequential testing as this may help assist why it is a problem despite the size of the family being unknown. Also, in General response 3, we have included this stratified approach as a possible solution. However, we note that this does not stop the decay of alpha, it just reduces the decay by starting lower.</p><p>To address this comment in a little more detail, the investing and spending correction procedures allow for infinite size families (as alpha will just get infinitesimally small, and never exceeds the set amount).</p><p>It is indeed possible for the reviewer’s approach to be an alternative way of doing sequential correction. In alpha-spending (the simplest of the correction procedures) we, throughout the paper, spent 50% of the remaining alpha-wealth. We could have changed the spending rate (e.g. only spend 10%) or varied the spend rate but this has little effect on the conclusions of the paper. It is indeed possible for different spending procedures to occur. Thus it is possible for the first ten to spend 5% of the overall alpha-wealth (coming to 50%). Then the next ten researchers spend 50% of the remaining wealth (coming to 75%). In practice this would mean alpha threshold (assuming 0.05 is the wealth of): 0.0025 for the first ten tests, 0.0013 for the next ten tests. This strategy will however always have a lower alpha than the alpha-debt approach and does not change any of the conclusions of the paper. So changing the spending or investing rates will change the thresholds, it does not get around the problem – it would just reduce the rate of decay.</p><disp-quote content-type="editor-comment"><p>6) The sequential correction methods all punish late-comers to the data party. Perhaps a particular dataset is perfect to test an idea which has not yet been developed - the data comes before the idea's time. It seems wrong that good researchers or good ideas who happen to arrive at the dataset late relative to other (worse) researchers or ideas should be 'punished' with higher alphas just for being late. (Not wishing to increase the admin-burden, ) perhaps some of the alpha can be saved up for a rainy day? Perhaps some of the alpha can be won or awarded through a competitive merit-based process? Perhaps researchers who meet a certain level of good research practice (e.g., pre-registration, ethical, open, all the necessary review, meta-analysis, and experimental work already in place, etc), should be allowed to use standard alpha levels, and it is only the disorganised vultures feeding on the data carcass who should be discouraged with alpha-punishment?</p></disp-quote><p>The idea behind “dataset decay” is that it will ultimately punish late comers <italic>if</italic> they are considered the same type of statistical family. General response 3 discusses some possible solutions more forward that incorporates some of the suggestions the reviewer has here. However, as we have discussed in Essential revision 1, we have further clarified our discussion of statistical families in order to make it clear that it is possible to have “standard alpha levels” if (1) the analysis is classed as confirmatory, and (2) that any data reuser can justify that their new analysis is a new hypothesis from previous use-cases.</p><disp-quote content-type="editor-comment"><p>7) If the dataset comprises *all* the available data on a particular topic (e.g., brains affected by 'mad cow disease' in Britain in the late 1990s) - i.e., it is the population, and not just a sample - does this change the assumptions or outcomes of the authors' approach at all? It feels like it should be a special case, but maybe not…</p></disp-quote><p>This is an excellent point. However, our intention in this article is to raise the issue: there is a subset of analyses that can be conducted on open data which could increase the number of false positives. It is not our intention to say that all analyses on open data are problematic. We have tried to explicitly define the types of analyses and datasets we are considered in General response 1 (preliminary assumptions). This should hopefully not leave a reader confused about what type of datasets we are talking about (the types of analyses where one has access to the full dataset means that there is no longer the same type of uncertainty as when taking a random sample). The new section in General response 1 explicitly says that we are not considering circumstances when the dataset is a whole population to avoid this confusion.</p><p>We have also raised in General response 3 (towards a solution) the point that different fields may require different solutions to the sequential problem depending on the type of datasets they have. Thus, with this disclaimer, we hope the reviewers agree that the reader understands which type of datasets we are addressing.</p><disp-quote content-type="editor-comment"><p>8) Relatedly, if a dataset is large, one solution could be simply to restrict researchers to a random sample of the dataset (say, 10% of the available data), and allow them to keep alpha at standard levels. Because exactly the same data will almost never be tested twice on the same or a different hypothesis, does this completely remove the problems of inflated false-positives? It feels to me like it should. Should alpha correction only apply to researchers who use exactly the same subset and/or all the dataset?</p></disp-quote><p>We do not see this as a potential solution, unfortunately. We had already discussed the reuseable hold out approach as a possible solution. The reviewer here is advocating for each data-user to create their out hold out data. This approach is still not ideal because any knowledge gained about the dataset in the previous analyses could bias the methods (see Essential revision 1). This will ultimately lead to overfitting as, results from previous uses of the data will guide the analyses. Thus, any knowledge about previous uses of the data means that there is knowledge about the test dataset, which increases the chance of overestimating the prediction. This is a well documented problem in machine learning.</p><p>Further, this approach requires for datasets to be sufficiently powered to achieve not just apt training models but sufficient data to assess predictive generalizability (we already touch upon this in our discussion about the reuseable hold out dataset), which may not be a universal solution for sequential tests on open data.</p><p>When we discuss the reuseable held out data, we are now more explicit that this is within a predictive framework. Further in the new subsection “Towards a solution” (General response 3) we discuss the possibilities of having a collective held out dataset on repositories, which does not seem to lead to have led to extensive overfitting in machine learning yet. Indeed, this does seem like a good solution (although some still fear this will lead to overfitting) which will however require infrastructural change within many fields.</p><p>Finally, as discussed above, in the new section “Preliminary assumptions” (General response 1) we discuss that we are only treating analyses on complete datasets. We understand (and sympathize) with the reviewers wish for asking about whether the point still holds under certain assumptions (whole dataset or subset). We however feel the point of the paper is to establish the problem that can exist within many reuse cases (the simplest types), we do not think it should be our priority to show the extent of the problem or present a solution with regards to every possible type of statistical test and dataset type.</p><disp-quote content-type="editor-comment"><p>9) In the authors' simulations, to estimate the likely number of publications resulting from false-positive findings, they assume that *every single 'significant' finding* will lead directly to a single publication - in effect, that publication bias is absolute, complete, and is itself unbiased. I find this assumption very hard to stomach. Researchers may tend to hold back significant results which don't support their, or their supervisors' or group's prior, current, or proposed research. Publication bias is not simply the immediate publication of (false) positive results, but also the delayed or suppressed publication of (true) negative or opposite-direction (false positive) results. Further, many (good) labs would replicate a result before going to press, or at least report multiple results, true and false, in the same paper. The authors may have stated this in other ways, but I think this strong assumption leads only to a very upper bound on the likely number of resulting (false positive) papers. Perhaps this can be stressed more clearly?</p></disp-quote><p>The reviewers are correct that we may have been somewhat clumsy in our formulation. It was indeed unwise in our former formulation to present the results with a publication bias by discussing “number of publications”, where we should state the general consequences (with or without publication bias) by discussing the number of “positive findings”. Thus have rephrased the text in many places to state “number publications with positive findings (i.e. null hypothesis rejected)” instead of “number of publications”.</p><p>Several examples of the new formulation are listed below.</p><p>[In section: “Uncorrected sequential tests will flood the scientific literature with false positives”. Most of the text has been revised in someway, so no emphasis added.].</p><p>&quot;We asked two questions with these models. First, we identified the number of positive findings that would be reported (a positive finding is considered to be when the null hypothesis is rejected at p &lt; 0.05, two tail) for the different correction methods. Second, we asked how many additional scientific articles would be published claiming to have identified a positive result (i.e. a null hypothesis has been rejected) for the different correction methods. Importantly, in this evaluation of empirical data, we are not necessarily concerned with the number of true relationships with this analysis. Primarily, we consider the differences in the inferred statistical relationships when comparing the different sequential correction procedures to a baseline of the simultaneous correction procedures. These simultaneous procedures allow us to contrast the sequential approaches with current practices (Bonferroni, a conservative procedure, and FDR, a more liberal measure). Thus any procedure that is more stringent than the Bonferroni baseline will be too conservative (more type II errors). Any procedure that is less stringent than FDR will have an increased false discovery rate, implying more false positives (relative to the true positives). Note that, we are tackling only issues regarding correction procedures to multiple hypothesis tests; determining the truth of any particular outcome would require additional replication.</p><p>Figure 3 shows the results for all correction procedures. Using sequentially uncorrected tests leads to an increase in positive findings (30/44 Bonferroni/FDR), compared to a baseline of 2 findings when correcting for all tests simultaneously (for both Bonferroni and FDR procedures). The sequentially uncorrected procedures would also result in 29/30 (Bonferroni/FDR) publications that claim to identify at least one positive result instead of the simultaneous baseline of two publications (Bonferroni and FDR), reflecting a 1,400% increase in publications claiming positive results. If we accept that the two baseline estimates are a good trade-off between error rates, then we have good reason to believe this increase reflects false positives.</p><p>The sequential correction procedures were closer to baseline but saw divergence based on the order of the statistical tests. If the order was completely random, then 𝛼-debt found, on average, 2.77 positive findings (min/max: 2/6) and 2.53 publications claiming positive results (min/max: 2/4) would be published. The random order leads to an increase in the number of false positives compared to baseline but considerably less than the sequentially uncorrected procedure. In contrast, 𝛼-spending found 0.33 positive findings (min/max: 0/5) resulting in 0.22 studies with positive findings (min/max: 0/2) and 𝛼-investing found 0.48 (min/max: 0/8) positive findings and 0.37 (min/max 0/4) studies with positive findings; both of which are below the conservative baseline of 2. When the order is informed by the baseline findings, the sequential corrections procedures increase in the number of findings (findings [min/max]: 𝛼-debt: 3.49 [2/7], 𝛼-spending: 2.58 [1/4], 𝛼-investing: 3.54 [1/10]; and publications with positive findings [min/max]: 𝛼-debt: 2.38 [2/4], 𝛼-spending: 1.97 [1/3], 𝛼-investing: 2.54 [1/5]). All procedures now increase their number of findings above baseline. On average 𝛼-debt with a random order has a 19% increase in the number of published studies with positive findings, substantially less than the increase in the number of uncorrected studies. Two conclusions emerge. First, 𝛼-debt remains sensitive to the number of findings found regardless of the sequence of tests (fewer type II errors) and can never fall above the Bonferroni in regards to type II errors. At the same time, the other two sequential procedures can be more conservative than Bonferroni. Second, while 𝛼- debt does not ensure the false positive rate remains under a specific level (more type I errors), it dramatically closes the gap between the uncorrected and simultaneous number of findings.”</p><p>[In Methods section, emphasis added at relevant places]</p><p>&quot;We then quantified the number of findings and the number of potential published studies with positive results that the different correction methods would present. <italic>The number of findings is the sum of independent variables that were considered positive findings (i.e. p &lt; 0.05, two-tailed). The number of potential studies that identify positive results is the number of dependent variables that had at least one positive finding. The rationale for the second metric is to consider how many potential non null-finding publications would exist in the literature if a separate group conducted each analysis.&quot;</italic></p><disp-quote content-type="editor-comment"><p>10) The authors used a real dataset to test a series of psychological hypotheses. They seem to have assumed that none of these hypotheses would pick up on any real effects in the data. Can they comment on the likelihood that their tests are establishing the true null distribution of effects, rather than actually being skewed by real effects? One solution would be to scramble the raw data in some way to ensure that even if there was a true effect, randomised and then processed voxels would not show this effect.</p></disp-quote><p>We do not believe we have made this assumption that there are no real effects in the data – we were quite agnostic about this. We do not make any judgment on what is a “real effect” but we contrast the consequences of different approaches and consider simultaneous correction to be a baseline (as that is an accepted consensus). We have however modified the statement below, when discussing the empirical examples, to make this even clearer:</p><p>&quot;Importantly, in this evaluation of empirical data, we are not necessarily concerned with the number of true relationships with this analysis. Primarily, we consider the differences in the inferred statistical relationships when comparing the different sequential correction procedures to a baseline of the simultaneous correction procedures. These simultaneous procedures allow us to contrast the sequential approaches with current practices (Bonferroni, a conservative procedure, and FDR, a more liberal measure). Thus any procedure that is more stringent than the Bonferroni baseline will be too conservative (more type II errors). Any procedure that is less stringent than FDR will have an increased false discovery rate, implying more false positives (relative to the true positives).”</p><p>This shows (1) what convention (simultaneous correction) would produce as a result and we compare the consequences of no sequential correction and sequential correction.</p><p>The reviewer suggests scrambling the data, but this would be identical to the simulations, so we do not see any reason for adding them. Our logic of the argument is quite simple: (1) simulations, (2) an empirical example to show that the results are consistent with the simulations. The empirical example shows that our simulated effect actually has real world value. So we do not see a justification for adding the scrambled data analyses. However, we have added the following text when introducing the empirical example to clarify its purpose:</p><p>“We have demonstrated a possible problem with sequential tests on simulations. These results show that sequential correction strategies are more liberal than their simultaneous counterparts. Therefore we should expect more false positives if sequential correction methods were performed on a dataset. We now turn our attention to empirical data from a well-known shared dataset in neuroscience to examine the effect of multiple reuses of the dataset. This empirical example is to confirm the simulations and show that more positive findings (i.e. null hypothesis rejected) will be identified with sequential correction. We used 68 cortical thickness estimates from the 1200 subject release of the HCP dataset (Van Essen et al., 2012). All subjects belonging to this dataset gave informed consent (see Van Essen et al., 2013 for more details). IRB protocol #31848 approved by the Stanford IRB approves the analysis of shared data. We then used 182 behavioral measures ranging from task performance to survey responses (see Supplementary File 1). For simplicity, we ignore all previous publications using the HCP dataset (of which there are now several hundred) for our p-value correction calculation.”</p><disp-quote content-type="editor-comment"><p>11) The introduction and discussion could do a better job at contrasting different empirical traditions and statistical approaches. The introduction could make clearer that the current project assumes that most researchers are engaged in a particular (though dominant) type of research involving confirmatory hypothesis testing in which the goal is to explain rather than predict. Do you think the problem would be different if the focus was on prediction?</p><p>The discussion mentions cross-validation and the problem with small sample sizes, but doesn't acknowledge explicitly the tension between explanatory and predictive research - Yarkoni &amp; Westfall (2017) is a great reference on that topic: https://doi.org/10.1177/1745691617693393</p></disp-quote><p>We have added two new sections to help improve the focus of the article. These sections are: “Preliminary Assumption” (General response 1) in the Methods section and “An intuitive example of the problem” (General response 2) at the start of the article. This help contrast and hone in on the different statistical approaches. Further, we’ve made it clearer in the discussion section when we are talking about NHST and when we are talking about Bayesian statistics or prediction. For example, in the section entitled “Different perspective-specific solutions regarding sequential testing”, we have been more explicit that the reuseable held out data is a solution within the predictive framework and offered a longer introduction to predictive frameworks when introducing the problem. This paragraph now starts with:</p><p>&quot;The second alternative is using held-out data within prediction frameworks. Instead of using statistical inference, this framework evaluates a model by how well it performs on predicting unseen test data (Yarkoni &amp; Westfall, 2017). However, a well-known problem when creating models to predict on test datasets is overfitting. This phenomenon occurs, for example, if a researcher queries the test dataset multiple times. Reusing test data will occur when sequentially reusing open data. Held-out data on data repositories, as discussed above, is one potential solution here.&quot;</p><p>We have also added a solution about held out data on data repositories in the possible solution category.</p><disp-quote content-type="editor-comment"><p>12) FDR is not clearly defined and would need to be better justified given the strong limitations of such a concept, which Richard Morey and Deborah Mayo described as completely flawed:</p><p>https://osf.io/ps38b/</p><p>https://medium.com/@richarddmorey/redefining-statistical-significance-the-statistical-arguments-ae9007bc1f91</p></disp-quote><p>We have extended our the Methods section of the simultaneous correction strategies to include:</p><p>“We used the Bonferroni method and the Benjamini &amp; Hochberg FDR method for simultaneous correction procedures (Benjamini &amp; Hochberg, 1995). Both correction methods were run using multipy (v0.16, https://github.com/puolival/multipy). The FDR correction procedure intends to limit the proportion of type I errors by keeping in below a certain level. In contrast, Bonferroni error intends to limit the probability of at least one type-I error. Despite ideological criticisms and objections to both these methods (Bonferroni: (Perneger, 1998); FDR: (Mayo &amp; Morey, 2017)), the Bonferroni correction is a conservative procedure that allows for more type II errors to occur and the FDR is a liberal method (i.e. allows for more type I errors). Together they offer a baseline range that allows us to contrast how the sequential correction procedures perform together.”</p><disp-quote content-type="editor-comment"><p>13) What if we are in a field in which inappropriate statistical methods are the norm: should future researcher using appropriate tools be penalised for analysing a dataset after many doomed attempts? You touch indirectly on the subject in the section &quot;Gray-area when families are not clear&quot;. For instance, in a field dominated by fixed effect analyses of means, I would argue that researchers attempting to fit carefully justified generalised hierarchical models should be allowed to reset their alpha counter.</p></disp-quote><p>We believe we have addressed this point in General Response 1 (paragraph starting “types of analyses”) where we have highlighted what types of analyses we are considering with regards to data reuse that will require sequential correction. The example the reviewer gives here, which is a very good example about when sequential correction seems inappropriate which we have incorporated into the text.</p><disp-quote content-type="editor-comment"><p>14) The discussion mentions Bayesian statistics as a potential solution, but with the current trend in adopting arbitrary thresholds for Bayes factors, the same problems encountered in mindless frequentist practices will also apply to Bayesian/Laplacian ones:</p><p>Gigerenzer, G. &amp; Marewski, J.N. (2015) Surrogate Science: The Idol of a Universal Method for Scientific Inference. Journal of Management, 41, 421-440.</p></disp-quote><p>The reviewers are indeed correct that Bayes, applied poorly, is a problem. We have expanded the paragraph discussing Bayes statistics in a little more detail to explain why we have discussed it as an alternative framework, and included the reference suggested by the reviewers. The text now reads:</p><p>“The first alternative is Bayesian statistics. Multiple comparisons in Bayesian frameworks are often circumnavigated by partial pooling and regularizing priors (Gelman et al., 2013; Kruschke &amp; Liddell, 2017). While Bayesian statistics can suffer from similar problems as NHST if misapplied (Gigerenzer &amp; Marewski, 2014), it often deals with multiple tests without explicitly correcting for them, and may provide an avenue for sequential correction to be avoided.”</p></body></sub-article></article>