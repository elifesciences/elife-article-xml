<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79450</article-id><article-id pub-id-type="doi">10.7554/eLife.79450</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Rate-distortion theory of neural coding and its implications for working memory</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-276512"><name><surname>Jakob</surname><given-names>Anthony MV</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0996-1356</contrib-id><email>anthony.jakob@outlook.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-42557"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6546-3298</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Section of Life Sciences Engineering, École Polytechnique Fédérale de Lausanne</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Neurobiology, Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Psychology and Center for Brain Science, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Center for Brains, Minds, and Machines, MIT</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bays</surname><given-names>Paul</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>07</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e79450</elocation-id><history><date date-type="received" iso-8601-date="2022-04-13"><day>13</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-06-22"><day>22</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-03-02"><day>02</day><month>03</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.02.28.482269"/></event></pub-history><permissions><copyright-statement>© 2023, Jakob and Gershman</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Jakob and Gershman</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79450-v2.pdf"/><abstract><p>Rate-distortion theory provides a powerful framework for understanding the nature of human memory by formalizing the relationship between information rate (the average number of bits per stimulus transmitted across the memory channel) and distortion (the cost of memory errors). Here, we show how this abstract computational-level framework can be realized by a model of neural population coding. The model reproduces key regularities of visual working memory, including some that were not previously explained by population coding models. We verify a novel prediction of the model by reanalyzing recordings of monkey prefrontal neurons during an oculomotor delayed response task.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>working memory</kwd><kwd>population coding</kwd><kwd>information theory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009152</institution-id><institution>Fondation Bertarelli</institution></institution-wrap></funding-source><award-id>Bertarelli Fellowship</award-id><principal-award-recipient><name><surname>Jakob</surname><given-names>Anthony MV</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF STC award CCF-1231216</award-id><principal-award-recipient><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The abstract framework of rate-distortion theory can be realized by a neural population coding model to reproduce key and previously unexplained regularities of human visual working memory.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>All memory systems are capacity limited in the sense that a finite amount of information about the past can be stored and retrieved without error. Most digital storage systems are designed to work without error. Memory in the brain, by contrast, is error-prone. In the domain of working memory, these errors follow well-behaved functions of set size, variability, attention, among other factors. An important insight into the nature of such regularities was the recognition that they may emerge from maximization of memory performance subject to a capacity limit or encoding cost (<xref ref-type="bibr" rid="bib73">Sims et al., 2012</xref>; <xref ref-type="bibr" rid="bib74">Sims, 2015</xref>; <xref ref-type="bibr" rid="bib82">van den Berg and Ma, 2018</xref>; <xref ref-type="bibr" rid="bib9">Bates et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Bates and Jacobs, 2020</xref>; <xref ref-type="bibr" rid="bib20">Brady et al., 2009</xref>; <xref ref-type="bibr" rid="bib58">Nassar et al., 2018</xref>).</p><p>Rate-distortion theory (<xref ref-type="bibr" rid="bib70">Shannon, 1959</xref>) provides a general formalization of the memory optimization problem (reviewed in <xref ref-type="bibr" rid="bib75">Sims, 2016</xref>). The costs of memory errors are specified by a <italic>distortion function</italic>; the capacity of memory is specified by an upper bound on the mutual information between the inputs (memoranda) and outputs (reconstructions) of the memory system. Systems with higher capacity can achieve lower expected distortion, tracing out an optimal trade-off curve in the rate-distortion plane. The hypothesis that human memory operates near the optimal trade-off curve allows one to deduce several known regularities of working memory errors, some of which we describe below. Past work has studied rate-distortion trade-offs in human memory (<xref ref-type="bibr" rid="bib73">Sims et al., 2012</xref>; <xref ref-type="bibr" rid="bib74">Sims, 2015</xref>; <xref ref-type="bibr" rid="bib57">Nagy et al., 2020</xref>), as well as in other domains such as category learning (<xref ref-type="bibr" rid="bib9">Bates et al., 2019</xref>), perceptual identification (<xref ref-type="bibr" rid="bib76">Sims, 2018</xref>), visual search (<xref ref-type="bibr" rid="bib10">Bates and Jacobs, 2020</xref>), linguistic communication (<xref ref-type="bibr" rid="bib88">Zaslavsky et al., 2018</xref>), and decision making (<xref ref-type="bibr" rid="bib38">Gershman, 2020</xref>; <xref ref-type="bibr" rid="bib48">Lai and Gershman, 2021</xref>).</p><p>Our goal is to show how the abstract rate-distortion framework can be realized in a neural circuit using population coding. As exemplified by the work of Bays and his colleagues, population coding offers a systematic account of working memory performance (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib14">Bays, 2015</xref>; <xref ref-type="bibr" rid="bib15">Bays, 2016</xref>; <xref ref-type="bibr" rid="bib67">Schneegans and Bays, 2018</xref>; <xref ref-type="bibr" rid="bib68">Schneegans et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Taylor and Bays, 2018</xref>; <xref ref-type="bibr" rid="bib80">Tomić and Bays, 2018</xref>), according to which errors arise from the readout of a noisy spiking population that encodes memoranda. We show that a modified version of the population coding model implements the celebrated Blahut–Arimoto algorithm for rate-distortion optimization (<xref ref-type="bibr" rid="bib17">Blahut, 1972</xref>; <xref ref-type="bibr" rid="bib3">Arimoto, 1972</xref>). The modified version can explain a number of phenomena that were puzzling under previous population coding accounts, such as <italic>serial dependence</italic> (the influence of previous trials on performance; <xref ref-type="bibr" rid="bib45">Kiyonaga et al., 2017</xref>).</p><p>The Blahut–Arimoto algorithm is parametrized by a coefficient that specifies the trade-off between rate and distortion. In our circuit implementation, this coefficient controls the precision of the population code. We derive a homeostatic learning rule that adapts the coefficient to maintain performance at the capacity limit. This learning rule explains the dependence of memory performance on the intertrial and retention intervals (RIs) (<xref ref-type="bibr" rid="bib72">Shipstead and Engle, 2013</xref>; <xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>; <xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>). It also makes the prediction that performance should adapt across trials to maintain a set point close to the channel capacity. We confirm these performance adjustments empirically. Finally, we show that variations in performance track changes in neural gain, consistent with our theory.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The channel design problem</title><p>We begin with an abstract characterization of the channel design problem, before specializing it to the case of neural population coding. A communication channel (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) is a probabilistic mapping, <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, from input <inline-formula><mml:math id="inf2"><mml:mi>θ</mml:mi></mml:math></inline-formula> to a reconstruction <inline-formula><mml:math id="inf3"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. The input and output spaces are assumed to be discrete in our treatment (for continuous variables like color and orientation, we use discretization into a finite number of bins; see also <xref ref-type="bibr" rid="bib74">Sims, 2015</xref>). We also assume that there is some capacity limit <inline-formula><mml:math id="inf4"><mml:mi>C</mml:mi></mml:math></inline-formula> on the amount of information that this channel can communicate about <inline-formula><mml:math id="inf5"><mml:mi>θ</mml:mi></mml:math></inline-formula>, as quantified by the mutual information <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between <inline-formula><mml:math id="inf7"><mml:mi>θ</mml:mi></mml:math></inline-formula> and the stimulus estimate <inline-formula><mml:math id="inf8"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> decoded from the population activity. We will refer to <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>R</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> as the channel’s <italic>information rate</italic>. To derive the optimal channel design, we also need to specify what <italic>distortion function</italic><inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the channel is optimizing—that is, how errors are quantified. Details on our choice of distortion function can be found below.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Model illustration.</title><p>(<bold>A</bold>) Top: Abstract characterization of a communication channel. A stimulus <inline-formula><mml:math id="inf11"><mml:mi>θ</mml:mi></mml:math></inline-formula> is sampled from an information source <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and passed through a noisy communication channel <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which outputs a stimulus reconstruction <inline-formula><mml:math id="inf14"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. The reconstruction error is quantified by a distortion function, <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Bottom: Circuit architecture implementing the communication channel. Input neurons encoding the negative distortion function provide the driving input to output neurons with excitatory input <italic>u</italic><sub><italic>i</italic></sub> and global feedback inhibition <inline-formula><mml:math id="inf16"><mml:mi>b</mml:mi></mml:math></inline-formula>. Each circuit codes a single stimulus at a fixed retinotopic location. When multiple stimuli are presented, the circuits operate in parallel, interacting only through a common gain parameter, <inline-formula><mml:math id="inf17"><mml:mi>β</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Tuning curves of input neurons encoding the negative cosine distortion function over a circular stimulus space. (<bold>C</bold>) Rate-distortion curves for two different set sizes (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>). The optimal gain parameter <inline-formula><mml:math id="inf20"><mml:mi>β</mml:mi></mml:math></inline-formula> is shown for each curve, corresponding to the point at which each curve intersects the channel capacity (horizontal dashed line). Expected distortion decreases with the information rate of the channel, but the channel capacity imposes a lower bound on expected distortion. (<bold>D</bold>) Example spike counts for output neurons in response to a stimulus (<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, vertical line). The output neurons are color coded by their corresponding input neuron (arranged horizontally by their preferred stimulus, <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> for neuron <inline-formula><mml:math id="inf23"><mml:mi>i</mml:mi></mml:math></inline-formula>; full tuning curves are shown in panel B). When only a single stimulus is presented (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), the gain is high and the output neurons report the true stimulus with high precision. (<bold>E</bold>) When multiple stimuli are presented <inline-formula><mml:math id="inf25"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, the gain is lower and the output has reduced precision (i.e., sometimes the wrong output neuron fires).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig1-v2.tif"/></fig><p>With these elements in hand, we can define the channel design problem as finding the channel <inline-formula><mml:math id="inf26"><mml:msup><mml:mi>Q</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:math></inline-formula> that minimizes expected distortion <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> subject to the constraint that the information rate <inline-formula><mml:math id="inf28"><mml:mi>R</mml:mi></mml:math></inline-formula> cannot exceed the capacity limit <inline-formula><mml:math id="inf29"><mml:mi>C</mml:mi></mml:math></inline-formula>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mo>≤</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>D</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For computational convenience, we can equivalently formulate this as an unconstrained optimization problem using a Lagrangian:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:mrow><mml:mi>Q</mml:mi></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf30"><mml:mi>β</mml:mi></mml:math></inline-formula> is a Lagrange multiplier equal to the negative slope of the rate-distortion function at the capacity limit:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Intuitively, the Lagrangian can be understood as expressing a cost function that captures the need to both minimize distortion (i.e., memory should be accurate) and minimize the information rate (i.e., memory resources should economized). The Lagrange parameter <inline-formula><mml:math id="inf31"><mml:mi>β</mml:mi></mml:math></inline-formula> determines the trade-off between these two terms. Note that because the optimal trade-off function is always monotonically non-increasing and convex, the value of <inline-formula><mml:math id="inf32"><mml:mi>β</mml:mi></mml:math></inline-formula> is always positive and non-increasing in <inline-formula><mml:math id="inf33"><mml:mi>D</mml:mi></mml:math></inline-formula>.</p><p>By integrating the ordinary differential equation defined in <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref> and using the Lagrangian formulation, one can show that the optimal channel for a discrete stimulus takes the following form:<disp-formula id="equ4"> <label>(4)</label><mml:math id="m4"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the marginal probability <inline-formula><mml:math id="inf34"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined by:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>These two equations are coupled. One can obtain the optimal channel by initializing them to uniform distributions and iterating them until convergence. This is known as the Blahut–Arimoto algorithm (<xref ref-type="bibr" rid="bib17">Blahut, 1972</xref>; <xref ref-type="bibr" rid="bib3">Arimoto, 1972</xref>).</p><p>For a channel with a fixed capacity <inline-formula><mml:math id="inf35"><mml:mi>C</mml:mi></mml:math></inline-formula> but variable <inline-formula><mml:math id="inf36"><mml:mi>D</mml:mi></mml:math></inline-formula> across contexts, the Lagrange multiplier <inline-formula><mml:math id="inf37"><mml:mi>β</mml:mi></mml:math></inline-formula> will need to be adjusted for each context so that <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>. We can accomplish this by computing <inline-formula><mml:math id="inf39"><mml:mi>R</mml:mi></mml:math></inline-formula> for a range of <inline-formula><mml:math id="inf40"><mml:mi>β</mml:mi></mml:math></inline-formula> values and choosing the value that gets closest to the constraint <inline-formula><mml:math id="inf41"><mml:mi>C</mml:mi></mml:math></inline-formula> (later we will propose a more biologically plausible algorithm). Intuitively, <inline-formula><mml:math id="inf42"><mml:mi>β</mml:mi></mml:math></inline-formula> characterizes the sensitivity of the channel to the stimulus. When stimulus sensitivity is lower, the information rate is lower and hence the expected distortion is higher.</p><p>In general, we will be interested in communicating a collection of <inline-formula><mml:math id="inf43"><mml:mi>M</mml:mi></mml:math></inline-formula> stimuli, <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with associated probing probabilities <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>π</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the probability that stimulus <inline-formula><mml:math id="inf47"><mml:mi>m</mml:mi></mml:math></inline-formula> will be probed (<xref ref-type="bibr" rid="bib82">van den Berg and Ma, 2018</xref>). The resulting distortion function is obtained by marginalizing over the probe stimulus:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s2-2"><title>Optimal population coding</title><p>We now consider how to realize the optimal channel with a population of spiking neurons, each tuned to a particular stimulus (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The firing rate of neuron <inline-formula><mml:math id="inf48"><mml:mi>i</mml:mi></mml:math></inline-formula> is determined by a simple Spike Response Model (<xref ref-type="bibr" rid="bib39">Gerstner and Kistler, 2002</xref>) in which the membrane potential is the difference between the excitatory input, <italic>u</italic><sub><italic>i</italic></sub>, and the inhibitory input, <inline-formula><mml:math id="inf49"><mml:mi>b</mml:mi></mml:math></inline-formula>, which we model as common across neurons (to keep notation simple, we will suppress the time index for all variables). Spiking is generated by a Poisson process, with firing rate modeled as an exponential function of the membrane potential (<xref ref-type="bibr" rid="bib43">Jolivet et al., 2006</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We assume that inhibition is given by <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, in which case the firing rate is driven by the excitatory input with divisive normalization (<xref ref-type="bibr" rid="bib22">Carandini and Heeger, 2011</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The resulting population dynamics is a form of ‘winner-take-all’ circuit (<xref ref-type="bibr" rid="bib59">Nessler et al., 2013</xref>). If each neuron has a preferred stimulus <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, then the winner can be understood as the momentary channel output, <inline-formula><mml:math id="inf52"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> whenever neuron <inline-formula><mml:math id="inf53"><mml:mi>i</mml:mi></mml:math></inline-formula> spikes (denoted <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). The probability that neuron <inline-formula><mml:math id="inf55"><mml:mi>i</mml:mi></mml:math></inline-formula> is the winner within a given infinitesimal time window is:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Importantly, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> has the same functional form as <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, and the two are equivalent if the excitatory input is given by:<disp-formula id="equ10"> <label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>is the log marginal probability of neuron <inline-formula><mml:math id="inf56"><mml:mi>i</mml:mi></mml:math></inline-formula> being selected as the winner. We can see from this expression that the first term in <xref ref-type="disp-formula" rid="equ1">Equation 10</xref> corresponds to the neuron’s stimulus-driven excitatory input and the second term corresponds to the neuron’s excitability. The Lagrange multiplier <inline-formula><mml:math id="inf57"><mml:mi>β</mml:mi></mml:math></inline-formula> plays the role of a gain modulation factor.</p><p>The excitability term can be learned through a form of intrinsic plasticity (<xref ref-type="bibr" rid="bib59">Nessler et al., 2013</xref>), using the following spike-triggered update rule:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf58"><mml:mi>η</mml:mi></mml:math></inline-formula> is a learning rate and <inline-formula><mml:math id="inf59"><mml:mi>c</mml:mi></mml:math></inline-formula> a gain parameter. After a spike (<inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), the excitability is increased proportionally to the inverse exponential of current excitability. In the absence of a spike, the excitability is decreased by a constant. This learning rule is broadly in agreement with experimental studies (<xref ref-type="bibr" rid="bib28">Daoudal and Debanne, 2003</xref>; <xref ref-type="bibr" rid="bib27">Cudmore and Turrigiano, 2004</xref>).</p></sec><sec id="s2-3"><title>Gain adaptation</title><p>We now address how to optimize the gain parameter <inline-formula><mml:math id="inf61"><mml:mi>β</mml:mi></mml:math></inline-formula>. We want the circuit to operate at the set point <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>, where the channel capacity <inline-formula><mml:math id="inf63"><mml:mi>C</mml:mi></mml:math></inline-formula> is understood as some fixed property of the circuit, whereas the information rate <inline-formula><mml:math id="inf64"><mml:mi>R</mml:mi></mml:math></inline-formula> can vary based on the parameters and input distribution, but cannot persistently exceed <inline-formula><mml:math id="inf65"><mml:mi>C</mml:mi></mml:math></inline-formula>. Assuming the total firing rate of the population is approximately constant across time, we can express the information rate as follows:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf66"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of neurons. This expression reveals that channel capacity corresponds to a constraint on stimulus-driven deviations in firing rate from the marginal firing rate. When the stimulus-driven firing rate is persistently greater than the marginal firing rate, the population may incur an unsustainably large metabolic cost (<xref ref-type="bibr" rid="bib54">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="bib50">Laughlin et al., 1998</xref>). When the stimulus-driven firing rate is lower than the marginal firing rate, the population is underutilizing its information transmission resources. We can adapt the deviation through a form of homeostatic plasticity, by increasing <inline-formula><mml:math id="inf67"><mml:mi>β</mml:mi></mml:math></inline-formula> when the deviation is below the channel capacity, and decreasing <inline-formula><mml:math id="inf68"><mml:mi>β</mml:mi></mml:math></inline-formula> when the deviation is above the channel capacity. Concretely, a simple update rule implements this idea:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf69"><mml:mi>α</mml:mi></mml:math></inline-formula> is a learning rate parameter. We assume that this update is applied continuously. A similar adaptive gain modulation has been observed in neural circuits (<xref ref-type="bibr" rid="bib29">Desai et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Hengen et al., 2016</xref>). Mechanistically, this could be implemented by changes in background activity: when stimulus-driven excitation is high, the inhibition will also be high (the network is balanced), and the ensuing noise will effectively decrease the gain (<xref ref-type="bibr" rid="bib24">Chance et al., 2002</xref>).</p><p>In this paper, we do not directly model how the information rate <inline-formula><mml:math id="inf70"><mml:mi>R</mml:mi></mml:math></inline-formula> is estimated in a biologically plausible way. One possibility is that this is implemented with slowly changing extracellular calcium levels, which decrease when cells are stimulated and then slowly recover. This mirrors (inversely) the qualitative behavior of the information rate. More quantitatively, it has been posited that the relationship between firing rate and extracellular calcium level is logarithmic (<xref ref-type="bibr" rid="bib44">King et al., 2001</xref>), consistent with the mathematical definition in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>. Thus, in this model, capacity <inline-formula><mml:math id="inf71"><mml:mi>C</mml:mi></mml:math></inline-formula> corresponds to a calcium set point, and the gain parameter adapts to maintain this set point. A related mechanism has been proposed to control intrinsic excitability via calcium-driven changes in ion channel conductance (<xref ref-type="bibr" rid="bib52">LeMasson et al., 1993</xref>; <xref ref-type="bibr" rid="bib1">Abbott and LeMasson, 1993</xref>).</p></sec><sec id="s2-4"><title>Multiple stimuli</title><p>In the case where there are multiple stimuli, the same logic applies, but now we calculate the information rate over all the subpopulations of neurons (each coding a different stimulus). Specifically, the excitatory input becomes:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf72"><mml:mi>m</mml:mi></mml:math></inline-formula> indexes both stimuli and separate subpopulations of neurons tuned to each stimulus location (or other stimulus feature that individuates the stimuli). As a consequence, <inline-formula><mml:math id="inf73"><mml:mi>β</mml:mi></mml:math></inline-formula> will tend to be smaller when more stimuli are encoded, because the same capacity constraint will be divided across more neurons.</p></sec><sec id="s2-5"><title>Memory maintenance</title><p>In delayed response tasks, the stimulus is presented transiently, and then probed after a delay. The channel thus needs to maintain stimulus information across the delay. Our model assumes that the excitatory input <italic>u</italic><sub><italic>i</italic></sub> maintains a trace of the stimulus across the delay. The persistence of this trace is determined by the gain parameter <inline-formula><mml:math id="inf74"><mml:mi>β</mml:mi></mml:math></inline-formula>. Because persistently high levels of stimulus-evoked activity may, according to <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, increase the information rate above the channel capacity, the learning rule in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> will reduce <inline-formula><mml:math id="inf75"><mml:mi>β</mml:mi></mml:math></inline-formula> and thereby functionally decay the memory trace.</p><p>The circuit model does not commit to a particular mechanism for maintaining the stimulus trace. A number of suitable mechanisms have been proposed (<xref ref-type="bibr" rid="bib90">Zylberberg and Strowbridge, 2017</xref>). One prominent model posits that recurrent connections between stimulus-tuned neurons can implement an attractor network that maintains the stimulus trace as a bump of activity (<xref ref-type="bibr" rid="bib83">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib2">Amit and Brunel, 1997</xref>). Other models propose cell-intrinsic mechanisms (<xref ref-type="bibr" rid="bib32">Egorov et al., 2002</xref>; <xref ref-type="bibr" rid="bib31">Durstewitz and Seamans, 2006</xref>) or short-term synaptic modifications (<xref ref-type="bibr" rid="bib56">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Bliss and D’Esposito, 2017</xref>). All of these model classes are potentially compatible with the theory that population codes are optimizing a rate-distortion trade-off, provided that the dynamics of the memory trace conform to the equations given above.</p><p>During time periods when no memory trace needs to be maintained, such as the intertrial interval (ITI) in delayed response tasks, we assume that the information rate is 0. Because the information rate is the <italic>average</italic> number of bits communicated across the channel, these ‘silent’ periods effectively increase the achievable information rate during ‘active’ periods (which we denote by <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>). Specifically, if <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> is the active time (delay period length), and <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>T</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> is the silent time (ITI length), then the channel’s rate is given by:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Equivalently, we can ignore the intervals in our model and simply rescale the channel capacity by <inline-formula><mml:math id="inf79"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This will allow us to model the effects of delay and ITI on performance in working memory tasks.</p></sec><sec id="s2-6"><title>Implications for working memory</title><sec id="s2-6-1"><title>Continuous report with circular stimuli</title><p>We apply the framework described above to the setting in which each stimulus is drawn from a circular space (e.g., color or orientation), <inline-formula><mml:math id="inf80"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which we discretize. Reconstruction errors are evaluated using a cosine distortion function:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is a scaling parameter. This implies that the input neurons have cosine tuning curves (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), and the output neurons have Von Mises tuning curves, as assumed in previous population coding models of visual working memory for circular stimulus spaces (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib67">Schneegans and Bays, 2018</xref>; <xref ref-type="bibr" rid="bib79">Taylor and Bays, 2018</xref>; <xref ref-type="bibr" rid="bib80">Tomić and Bays, 2018</xref>). All of our subsequent simulations use the same tuning curves.</p><p>As an illustration of the model behavior in the continuous report task, we compare performance for set sizes 1 and 4. The optimal trade-off curves are shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. For every point on the curve, the same information rate achieves a lower distortion for set size 1, due to the fact that all of the channel capacity can be devoted to a single stimulus (a hypothetical capacity limit is shown by the dashed horizontal line). In the circuit model, this higher performance is achieved by a narrow bump of population activity around the true stimulus (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), compared to a broader bump when multiple stimuli are presented (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p>In the following sections, we compare the full rate-distortion model (as described above) with two variants. The ‘fixed gain’ variant assumes that <inline-formula><mml:math id="inf82"><mml:mi>β</mml:mi></mml:math></inline-formula> is held fixed to a constant (fit as a free parameter) rather than adjusted dynamically. The ‘no plasticity’model holds the neural excitability to a fixed value (fit as a free parameter). These two variants remove features of the full rate-distortion model which critically distinguish it from the population coding model of working memory (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>). As a strong test of our model, we fit only to data from Experiment 1 in <xref ref-type="bibr" rid="bib13">Bays, 2014</xref>, and then evaluated the model on the other datasets without fitting any free parameters.</p></sec><sec id="s2-6-2"><title>Set size</title><p>One of the most fundamental findings in the visual working memory literature is that memory precision decreases with set size (<xref ref-type="bibr" rid="bib12">Bays et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib84">Wilken and Ma, 2004</xref>). Our model asserts that this is the case because the capacity constraint of the system is divided across more neurons as the number of stimuli to be remembered increases, thus reducing the recall accuracy for any one stimulus. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the distribution of recall error for different set sizes as published in previous work (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>). <xref ref-type="fig" rid="fig2">Figure 2D</xref> shows simulation results replicating these findings.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Set size effects and prioritization.</title><p>(<bold>A</bold>) Error distributions for different set sizes, as reported in <xref ref-type="bibr" rid="bib13">Bays, 2014</xref>. Error variability increases with set size. (<bold>B</bold>) Error variance as a function of set size for cued and uncued stimuli. Reports for cued stimuli have lower error variance. (<bold>C</bold>) Kurtosis as a function of set size for cued and uncued stimuli. Simulation results for the full model (<bold>D–F</bold>), model with fixed gain parameter <inline-formula><mml:math id="inf83"><mml:mi>β</mml:mi></mml:math></inline-formula> (<bold>G–I</bold>), and model without plasticity term <inline-formula><mml:math id="inf84"><mml:mi>w</mml:mi></mml:math></inline-formula> (<bold>J–L</bold>). Error bars represent standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig2-v2.tif"/></fig></sec><sec id="s2-6-3"><title>Prioritization</title><p>Stimuli that are attentionally prioritized are recalled more accurately. For example, error variance is reduced by a cue that probabilistically predicts the location of the probed stimulus (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib87">Yoo et al., 2018</xref>). In our model, the cue is encoded by the probing probability <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>π</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>, which alters the expected distortion. This results in greater allocation of the capacity budget to cued stimuli than to uncued stimuli. <xref ref-type="fig" rid="fig2">Figure 2B, C</xref> shows empirical findings (variance and kurtosis), which are reproduced by our simulations shown in <xref ref-type="fig" rid="fig2">Figure 2E, F</xref>. Kurtosis is one way of quantifying deviation from normality: values greater than 0 indicate tails of the error distribution that are heavier than expected under a normal distribution. The ‘excess’ kurtosis observed in our model is comparable to that observed by Bays in his population coding model (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>) when gain is sufficiently low. This is not surprising, given the similarity of the models.</p></sec><sec id="s2-6-4"><title>Timing</title><p>It is well established that memory performance typically degrades with the RI (<xref ref-type="bibr" rid="bib64">Pertzov et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Schneegans and Bays, 2018</xref>; <xref ref-type="bibr" rid="bib89">Zhang and Luck, 2009</xref>), although the causes of this degradation are controversial (<xref ref-type="bibr" rid="bib60">Oberauer et al., 2016</xref>), and in some cases the effect is unreliable (<xref ref-type="bibr" rid="bib71">Shin et al., 2017</xref>). According to our model, this occurs because long RIs tax the information rate of the neural circuit. In order to stay within the channel capacity, the circuit reduces the gain parameter <inline-formula><mml:math id="inf86"><mml:mi>β</mml:mi></mml:math></inline-formula> for long RIs, thereby reducing the information rate and degrading memory performance.</p><p>Memory performance also depends on the ITI, but in the opposite direction: longer ITIs improve performance (<xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>; <xref ref-type="bibr" rid="bib72">Shipstead and Engle, 2013</xref>). The critical determinant of performance is in fact the ratio between the ITI and RI. <xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref> found that performance in a color working memory task was similar when both intervals were short or both intervals were long. They also reported that a <italic>longer</italic> RI could produce <italic>better</italic> memory performance when it is paired with a longer ITI. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows a simulation of the same experimental paradigm, reproducing the key results. This timescale invariance, which is also seen in studies of associative learning (<xref ref-type="bibr" rid="bib6">Balsam and Gallistel, 2009</xref>), arises as a direct consequence of <xref ref-type="disp-formula" rid="equ16">Equation 16</xref>. Increasing the ITI reduces the information rate, since no stimuli are being communicated during that time period, and can therefore compensate for longer RIs.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Timing effects.</title><p>(<bold>A</bold>) Error distributions for different intertrial intervals (ITIs) and retention intervals (RIs), as reported in <xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>. ‘S’ denotes a short interval, and ‘L’ denotes a long interval. (<bold>B</bold>) Error variance as a function of timing parameters. Longer ITIs are associated with lower error variance, whereas longer RIs are associated with larger error variance. Simulation results for the full model (<bold>C, D</bold>), model with fixed gain parameter <inline-formula><mml:math id="inf87"><mml:mi>β</mml:mi></mml:math></inline-formula> (<bold>E, F</bold>), and model without plasticity term <inline-formula><mml:math id="inf88"><mml:mi>w</mml:mi></mml:math></inline-formula> (<bold>G, H</bold>). Error bars represent standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig3-v2.tif"/></fig></sec><sec id="s2-6-5"><title>Serial dependence</title><p>Working memory recall is biased by recent stimuli, a phenomenon known as <italic>serial dependence</italic> (<xref ref-type="bibr" rid="bib33">Fischer and Whitney, 2014</xref>; <xref ref-type="bibr" rid="bib36">Fritsche et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Papadimitriou et al., 2015</xref>). Recall is generally attracted toward recent stimuli, though some studies have reported repulsive effects when the most recent and current stimulus differ by a large amount (<xref ref-type="bibr" rid="bib8">Barbosa et al., 2020</xref>; <xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>). Our theory explains serial dependence as a consequence of the marginal firing rate of the output cells, which biases the excitatory input <italic>u</italic><sub><italic>i</italic></sub> (see <xref ref-type="disp-formula" rid="equ1">Equation 10</xref>). Because the marginal firing rate is updated incrementally, it will reflect recent stimulus history.</p><p>An important benchmark for theories of serial dependence is the finding that it increases with the RI and decreases with ITI (<xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>). These twin dependencies are reproduced by our model (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Our explanation of serial dependence is closely related to our explanation of timing effects on recall error: the strength of serial dependence varies inversely with the information rate, which in turn increases with the ITI and decreases with the RI. Mechanistically, this effect is mediated by adjustments of the gain parameter <inline-formula><mml:math id="inf89"><mml:mi>β</mml:mi></mml:math></inline-formula> in order to keep the information rate near the channel capacity.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Serial dependence as a function of retention interval and intertrial interval.</title><p>(<bold>A</bold>) Serial dependence increases with the retention interval until eventually reaching an asymptote, as reported in <xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>. Serial dependence is quantified as the peak-to-peak amplitude of a derivative of Gaussian (DoG) tuning function fitted to the data using least squares (see Methods). (<bold>B</bold>) Serial dependence decreases with intertrial interval. Simulation results for the full model (<bold>C, D</bold>), model with fixed gain parameter <inline-formula><mml:math id="inf90"><mml:mi>β</mml:mi></mml:math></inline-formula> (<bold>E, F</bold>), and model without plasticity term <inline-formula><mml:math id="inf91"><mml:mi>w</mml:mi></mml:math></inline-formula> (<bold>G, H</bold>). Shaded area corresponds to standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig4-v2.tif"/></fig><p>Serial dependence has also been shown to build up over the course of an experimental session (<xref ref-type="bibr" rid="bib7">Barbosa and Compte, 2020</xref>). This is hard to explain in terms of theories based on purely short-term effects, but it is consistent with our account in terms of the bias induced by the marginal firing rate. Because this bias reflects continuous incremental adjustments, it integrates over the entire stimulus history, thereby building up over the course of an experimental session (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Serial dependence builds up during an experiment.</title><p>(<bold>A</bold>) Serial dependence computed using first third (blue) and last third (orange) of the trials within a session, as reported in <xref ref-type="bibr" rid="bib7">Barbosa and Compte, 2020</xref>. Data shown here were originally reported in <xref ref-type="bibr" rid="bib34">Foster et al., 2017</xref>. To obtain a trial-by-trial measure of serial dependence, we calculated the folded error as described in <xref ref-type="bibr" rid="bib7">Barbosa and Compte, 2020</xref> (see Methods). Positive values indicate attraction to the last stimulus, while negative values indicate repulsion. Serial dependence is stronger in the last third of the trials in the experiment compared to the first third. (<bold>B</bold>) Serial dependence increases over the course of the experimental session, computed here with a sliding window of 200 trials. Simulation results for the full model (<bold>C, D</bold>), model with fixed gain parameter <inline-formula><mml:math id="inf92"><mml:mi>β</mml:mi></mml:math></inline-formula> (<bold>E, F</bold>), and model without plasticity term <inline-formula><mml:math id="inf93"><mml:mi>w</mml:mi></mml:math></inline-formula> (<bold>G, H</bold>). Shaded area corresponds to standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig5-v2.tif"/></fig><p>If, as we hypothesize, serial dependence reflects a capacity limit, then we should expect it to increase with set size, since <inline-formula><mml:math id="inf94"><mml:mi>β</mml:mi></mml:math></inline-formula> must decrease to stay within the capacity limit. To the best of our knowledge, this prediction has not been tested. We confirmed this prediction for color working memory using a large dataset reported in <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows that the attractive bias for similar stimuli on consecutive trials is stronger when the set size is larger (p &lt; 0.05, group permutation test).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Serial dependence increases with set size.</title><p>(<bold>A</bold>) Serial dependence (quantified using folded error) for set sizes <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (blue) and <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> (orange), using data originally reported in <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>. Serial dependence computed as the peak amplitude of a derivative of Gaussian (DoG) tuning function fitted to the data using least squares is stronger for larger set sizes (see Methods). On the <italic>x</italic>-axis, ‘color of previous trial’ refers to the color of the single stimulus probed on the previous trial. (<bold>B–D</bold>) Simulation results for the full model, model with fixed gain parameter <inline-formula><mml:math id="inf97"><mml:mi>β</mml:mi></mml:math></inline-formula>, and model without plasticity term <inline-formula><mml:math id="inf98"><mml:mi>w</mml:mi></mml:math></inline-formula>. Shaded area corresponds to standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig6-v2.tif"/></fig></sec><sec id="s2-6-6"><title>Systematic biases</title><p>Working memory exhibits systematic biases toward stimuli that are shown more frequently than others (<xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>). Moreover, these biases increase with the RI, and build up over the course of an experimental session. Our interpretation of serial dependence, which also builds up over the course of a session, suggests that these two phenomena may be linked (see also <xref ref-type="bibr" rid="bib81">Tong and Dubé, 2022</xref>).</p><p>Our theory posits that, over the course of the experiment, the marginal firing rate asymptotically approaches the distribution of presented stimuli (assuming there are no inhomogeneities in the distortion function). Thus, the neurons corresponding to high-frequency stimuli become more excitable than others and bias recall toward their preferred stimuli. This bias is amplified by lower effective capacities brought about by longer RIs. <xref ref-type="fig" rid="fig7">Figure 7</xref> shows simulation results replicating these effects.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Continuous reports are biased toward high-frequency colors.</title><p>(<bold>A, B</bold>) Bias for targets around common colors during the first (Panel A) and last (Panel B) third of the session, as reported in <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>. Bias refers to the difference between the stimulus and the mean reported color. <italic>x</italic>-Axis is centered around high-frequency colors. Bias increases with RI length (blue = short RI, orange = long RI). Bias also increases as the experiment progresses. Simulation results for the full model (<bold>C, D</bold>), model with fixed gain parameter <inline-formula><mml:math id="inf99"><mml:mi>β</mml:mi></mml:math></inline-formula> (<bold>E, F</bold>), and model without plasticity term <inline-formula><mml:math id="inf100"><mml:mi>w</mml:mi></mml:math></inline-formula> (<bold>G, H</bold>). Shaded area corresponds to standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig7-v2.tif"/></fig></sec><sec id="s2-6-7"><title>Quantitative model comparison</title><p>To systematically compare the performance of the different models, we carried out random-effects Bayesian model comparison (<xref ref-type="bibr" rid="bib65">Rigoux et al., 2014</xref>) for each dataset (see Methods). This method estimates a population distribution over models from which each subject’s data are assumed to be sampled. The protected exceedance probabilities, shown in <xref ref-type="table" rid="table1">Table 1</xref>, quantify the posterior probability that each model is the most frequent in the population, taking into account the possibility of the null hypothesis where model probabilities are uniform.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Bayesian model comparison between the population coding (PC) model (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>), the full rate-distortion (RD), and two variants of the RD model (fixed gain and no plasticity).</title><p>Each model is assigned a protected exceedance probability.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Experiment</th><th align="left" valign="bottom">Figure</th><th align="left" valign="bottom">PC model</th><th align="left" valign="bottom">RD model(full)</th><th align="left" valign="bottom">RD model(fixed gain)</th><th align="left" valign="bottom">RD model(no plasticity)</th></tr></thead><tbody><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib13">Bays, 2014</xref>, Experiment 1</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">0.2141</td><td align="char" char="." valign="bottom">0.2286</td><td align="char" char="." valign="bottom">0.4128</td><td align="char" char="." valign="bottom">0.1445</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib13">Bays, 2014</xref>, Experiment 2</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">0.1853</td><td align="char" char="." valign="bottom">0.7175</td><td align="char" char="." valign="bottom">0.0487</td><td align="char" char="." valign="bottom">0.0485</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref></td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">0.0115</td><td align="char" char="." valign="bottom">0.9785</td><td align="char" char="." valign="bottom">0.0093</td><td align="char" char="." valign="bottom">0.0007</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib18">Bliss and D’Esposito, 2017</xref>, Experiment 1</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">0.0000</td><td align="char" char="." valign="bottom">1.0000</td><td align="char" char="." valign="bottom">0.0000</td><td align="char" char="." valign="bottom">0.0000</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>, Experiment 2</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">0.0029</td><td align="char" char="." valign="bottom">0.7689</td><td align="char" char="." valign="bottom">0.2264</td><td align="char" char="." valign="bottom">0.0018</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib34">Foster et al., 2017</xref></td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0.3185</td><td align="char" char="." valign="bottom">0.6638</td><td align="char" char="." valign="bottom">0.0089</td><td align="char" char="." valign="bottom">0.0088</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>, Experiment 1a</td><td align="char" char="." valign="bottom">6</td><td align="char" char="." valign="bottom">0.2613</td><td align="char" char="." valign="bottom">0.7387</td><td align="char" char="." valign="bottom">0.0000</td><td align="char" char="." valign="bottom">0.0000</td></tr><tr><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>, Experiment 2</td><td align="char" char="." valign="bottom">7</td><td align="char" char="." valign="bottom">0.0544</td><td align="char" char="." valign="bottom">0.9456</td><td align="char" char="." valign="bottom">0.0000</td><td align="char" char="." valign="bottom">0.0000</td></tr></tbody></table></table-wrap><p>Experiment 1 from <xref ref-type="bibr" rid="bib13">Bays, 2014</xref> did not discriminate strongly between models. All the other datasets provided moderate to strong evidence in favor of the full rate-distortion model, with an average protected exceedance probability of 0.76.</p></sec><sec id="s2-6-8"><title>Variations in gain</title><p><xref ref-type="disp-formula" rid="equ14">Equation 14</xref> predicts that operating below the channel capacity will lead to an increase in the gain term <inline-formula><mml:math id="inf101"><mml:mi>β</mml:mi></mml:math></inline-formula>, which, in turn, leads to a higher information rate and better memory performance. Therefore, our model predicts that recall accuracy should improve after a period of poor memory performance, and degrade after a period of good memory performance. At the neural level, the model predicts that error will tend to be lower when gain (<inline-formula><mml:math id="inf102"><mml:mi>β</mml:mi></mml:math></inline-formula>) is higher.</p><p>We tested these predictions by reanalyzing the monkey neural and behavioral data reported in <xref ref-type="bibr" rid="bib8">Barbosa et al., 2020</xref> (<inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>). The neural data were collected from the dorsolateral prefrontal cortex, a region classically associated with maintenance of information in working memory (<xref ref-type="bibr" rid="bib55">Levy and Goldman-Rakic, 2000</xref>; <xref ref-type="bibr" rid="bib37">Funahashi, 2006</xref>; <xref ref-type="bibr" rid="bib85">Wimmer et al., 2014</xref>).</p><p>Behavioraly, squared error was significantly lower following higher-than-average error than following lower-than-average error (linear mixed model, p &lt; 0.001; <xref ref-type="fig" rid="fig8">Figure 8A</xref>), consistent with the hypothesis that gain tends to increase after poor performance and decrease after good performance.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Dynamic variation in memory precision and neural gain.</title><p>(<bold>A</bold>) Mean squared error on current trial, classified by quantiles of squared error on previous trial. Squared error tends to be above average (dashed black line) following low squared error on the previous trial, and tends to be below average following large squared error on the previous trial. (<bold>B</bold>) Angular location tuning curve (orange) fitted to mean spike count (blue) during the retention interval, shown for one example neuron. The neuron’s preferred stimulus (dashed black line) corresponds to the peak of the tuning curve. Shaded region corresponds to standard error of the mean. (<bold>C</bold>) Mean squared error for different sessions plotted against mean fitted <inline-formula><mml:math id="inf104"><mml:mi>β</mml:mi></mml:math></inline-formula>. According to our theory, <inline-formula><mml:math id="inf105"><mml:mi>β</mml:mi></mml:math></inline-formula> plays the role of a gain control on the stimulus. Consistent with this hypothesis, memory error decreases with <inline-formula><mml:math id="inf106"><mml:mi>β</mml:mi></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79450-fig8-v2.tif"/></fig><p>In order to estimate the neural gain, we first inferred the preferred stimulus of each neuron by fitting a bell-shaped tuning function to its spiking behavior (<xref ref-type="disp-formula" rid="equ23">Equation 23</xref>, <xref ref-type="fig" rid="fig8">Figure 8B</xref>). We then performed Poisson regression to fit a <inline-formula><mml:math id="inf107"><mml:mi>β</mml:mi></mml:math></inline-formula> for each neuron (<xref ref-type="disp-formula" rid="equ24">Equation 24</xref>). Model comparison using the Bayesian information criterion (BIC) established that both the distortion function (which captures driving input) and spiking history were significant predictors of spiking behavior (full model: 54,545; no history: 59,163; neither distortion nor history: 67,903). We then examined the relationship between neural gain and memory precision across sessions, finding that session-specific mean squared error was negatively correlated with the average <inline-formula><mml:math id="inf108"><mml:mi>β</mml:mi></mml:math></inline-formula> estimate (<inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = –0.32, p &lt; 0.02; <xref ref-type="fig" rid="fig8">Figure 8C</xref>). This result is consistent with the hypothesis that dynamic changes in memory performance are associated with changes in neural gain.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have shown that a simple population coding model with spiking neurons can solve the channel design problem: signals passed through the spiking network are transmitted with close to the minimum achievable distortion under the network’s capacity limit. We focused on applying this general model to the domain of working memory, unifying several seemingly disparate aspects of working memory performance: set size effects, stimulus prioritization, serial dependence, approximate timescale invariance, and systematic bias. Our approach builds a bridge between biologically plausible population coding and prior applications of rate-distortion theory to human memory (<xref ref-type="bibr" rid="bib73">Sims et al., 2012</xref>; <xref ref-type="bibr" rid="bib74">Sims, 2015</xref>; <xref ref-type="bibr" rid="bib75">Sims, 2016</xref>; <xref ref-type="bibr" rid="bib9">Bates et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Bates and Jacobs, 2020</xref>; <xref ref-type="bibr" rid="bib57">Nagy et al., 2020</xref>).</p><sec id="s3-1"><title>Relationship to other models</title><p>The hypothesis that neural systems are designed to optimize a rate-distortion trade-off has been previously studied through the lens of the information bottleneck method (<xref ref-type="bibr" rid="bib16">Bialek et al., 2006</xref>; <xref ref-type="bibr" rid="bib46">Klampfl et al., 2009</xref>; <xref ref-type="bibr" rid="bib21">Buesing and Maass, 2010</xref>; <xref ref-type="bibr" rid="bib61">Palmer et al., 2015</xref>), a special case of rate-distortion theory in which the distortion function is derived from a compression principle. Specifically, the distortion function is defined as the Kullback–Leibler divergence between <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf112"><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> denotes the probed stimulus. This distortion function applies a ‘soft’ penalty to errors based on how much probability mass the channel places on each stimulus. The expected distortion is equal to the mutual information between <inline-formula><mml:math id="inf113"><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. Thus, the information bottleneck method seeks a channel that maps the input <inline-formula><mml:math id="inf115"><mml:mi>θ</mml:mi></mml:math></inline-formula> into a compressed representation <inline-formula><mml:math id="inf116"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> satisfying the capacity limit, while preserving information necessary to predict the probe <inline-formula><mml:math id="inf117"><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>.</p><p>As pointed out by <xref ref-type="bibr" rid="bib51">Leibfried and Braun, 2015</xref>, using the Kullback–Leibler divergence as the distortion function leads to a harder optimization compared to classical rate-distortion theory because <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depends on the channel distribution, which is the thing being optimized. One consequence of this dependency is that minimizing the rate-distortion objective using alternating optimization (in the style of the Blahut–Arimoto algorithm) is not guaranteed to find the globally optimal channel. It is possible to break the dependency by replacing <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a reference distribution that does not depend on the channel. This turns out to strictly generalize rate-distortion theory, because an arbitrary choice of the reference distribution allows one to recover any lower-bounded distortion function up to a constant offset (<xref ref-type="bibr" rid="bib51">Leibfried and Braun, 2015</xref>). However, existing spiking neuron implementations of the information bottleneck method (<xref ref-type="bibr" rid="bib46">Klampfl et al., 2009</xref>; <xref ref-type="bibr" rid="bib21">Buesing and Maass, 2010</xref>) do not make use of such a reference distribution, and hence do not attain the same level of generality.</p><p><xref ref-type="bibr" rid="bib51">Leibfried and Braun, 2015</xref> propose a spiking neuron model that explicitly optimizes the rate-distortion objective function for arbitrary distortion functions. Their approach differs from ours in several ways. First, they model a single neuron, rather than a population. Second, they posit that the channel optimization is realized through synaptic plasticity, in contrast to the intrinsic plasticity rule that we study here. Third, they treat the gain parameter <inline-formula><mml:math id="inf120"><mml:mi>β</mml:mi></mml:math></inline-formula> as fixed, whereas we propose an algorithm for optimizing <inline-formula><mml:math id="inf121"><mml:mi>β</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s3-2"><title>Open questions</title><p>A cornerstone of our approach is the assumption that the neural circuit responsible for working memory dynamically modifies its output to stay within a capacity limit. What, at a biological level, is the nature of this capacity limit? Spiking activity accounts for a large fraction of cortical energy expenditure (<xref ref-type="bibr" rid="bib4">Attwell and Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib53">Lennie, 2003</xref>). Thus, a limit on the overall firing rate of a neural population is a natural transmission bottleneck. Previous work on energy-efficient coding has similarly used the cost of spiking as a constraint (<xref ref-type="bibr" rid="bib54">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="bib78">Stemmler and Koch, 1999</xref>; <xref ref-type="bibr" rid="bib5">Balasubramanian et al., 2001</xref>). One subtlety is that the capacity limit in our framework is an upper bound on the stimulus-driven firing rate <italic>relative</italic> to the average firing rate (on a log scale). This means that the average firing rate can be high provided the stimulus-evoked transients are small, consistent with the observation that firing rate tends to be maintained around a set point rather than minimized (<xref ref-type="bibr" rid="bib29">Desai et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Hengen et al., 2016</xref>). The set point should correspond to the capacity limit.</p><p>The next question is how a neural circuit can control its sensitivity to inputs in such a way that the information rate is maintained around the capacity limit. At the single neuron level, this might be realized by adaptation of voltage conductances (<xref ref-type="bibr" rid="bib78">Stemmler and Koch, 1999</xref>). At the population level, neuromodulators could act as a global gain control. Catecholamines (e.g., dopamine and norepinephrine), in particular, have been thought to play this role (<xref ref-type="bibr" rid="bib69">Servan-Schreiber et al., 1990</xref>; <xref ref-type="bibr" rid="bib30">Durstewitz et al., 1999</xref>). Directly relevant to this hypothesis are experiments showing that local injection of dopamine D1 receptor antagonists into the prefrontal cortex impaired performance in an oculomotor delayed response task (<xref ref-type="bibr" rid="bib66">Sawaguchi and Goldman-Rakic, 1991</xref>), whereas D1 agonists can improve performance (<xref ref-type="bibr" rid="bib23">Castner et al., 2000</xref>).</p><p>In experiments with humans, it has been reported that pharmacological manipulations of dopamine can have non-monotonic effects on cognitive performance, with the direction of the effect depending on baseline dopamine levels (see <xref ref-type="bibr" rid="bib26">Cools and D’Esposito, 2011</xref> for a review). The baseline level (particularly in the striatum) correlates with working memory performance (<xref ref-type="bibr" rid="bib25">Cools et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Landau et al., 2009</xref>). Taken together, these findings suggest that dopaminergic neuromodulation controls the capacity limit (possibly through a gain control mechanism), and that pushing dopamine levels beyond the system’s capacity provokes a compensatory decrease in gain, as predicted by our homeostatic model of gain adaptation. A more direct test of our model would use continuous report tasks to quantify memory precision, bias, and serial dependence under different levels of dopamine.</p><p>We have considered a relatively restricted range of visual working memory tasks for which extensive data are available. An important open question concerns the generality of our model beyond these tasks. For example, serial order, AX-CPT, and N-back tasks are widely used but outside the scope of our model. With appropriate modification, the rate-distortion framework can be applied more broadly. For example, one could construct channels for sequences rather than individual items, analogous to how we have handled multiple simultaneously presented stimuli. One could also incorporate a capacity-limited attention mechanism for selecting previously presented information for high fidelity representation, rather than storing everything from a fixed temporal window with relatively low fidelity. This could lead to a new information-theoretic perspective on attentional gating in working memory.</p><p>Our model can be extended in several other ways. One, as already mentioned, is to develop a biologically plausible implementation of gain adaptation, either through intrinsic or neuromodulatory mechanisms. A second direction is to consider channels that transmit a compressed representation of the input. Previous work has suggested that working memory representations are efficient codes that encode some stimuli with higher precision than others (<xref ref-type="bibr" rid="bib47">Koyluoglu et al., 2017</xref>; <xref ref-type="bibr" rid="bib79">Taylor and Bays, 2018</xref>). Finally, an important direction is to enable the model to handle more complex memoranda, such as natural images. Recent applications of large-scale neural networks, such as the variational autoencoder, to modeling human memory hold promise (<xref ref-type="bibr" rid="bib57">Nagy et al., 2020</xref>; <xref ref-type="bibr" rid="bib10">Bates and Jacobs, 2020</xref>; <xref ref-type="bibr" rid="bib35">Franklin et al., 2020</xref>; <xref ref-type="bibr" rid="bib11">Bates et al., 2023</xref>; <xref ref-type="bibr" rid="bib86">Xie et al., 2023</xref>), though linking these to more realistic neural circuits remains a challenge.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>We reanalyzed five datasets with human subjects and one dataset with monkey subjects performing delayed response tasks. The detailed experimental procedures can be found in the original reports (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>; <xref ref-type="bibr" rid="bib8">Barbosa et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Barbosa and Compte, 2020</xref>; <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Bliss and D’Esposito, 2017</xref>). In three of the six datasets, one or multiple colors were presented on a screen at equally spaced locations. After an RI, during which the cues were no longer visible, subjects had to report the color at a particular cued location, measured as angles on a color wheel. In one dataset, angled color bars were presented, and the angle of the bar associated with a cued color had to be reported (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>). In the two last datasets, only the location of a black cue on a circle had to be remembered and reported (<xref ref-type="bibr" rid="bib8">Barbosa et al., 2020</xref>; <xref ref-type="bibr" rid="bib18">Bliss and D’Esposito, 2017</xref>).</p><sec id="s4-1"><title>Set size and stimulus prioritization</title><p>Human subjects (<inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>) were presented with 2, 4, or 8 color stimuli at the same time. On each trial, one of the locations was cued before the appearance of the stimuli. Cued locations were 3 times as likely to be probed (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>).</p><p>We computed trial-wise error as the circular distance between the reported angle and the target angle, separately for each set size and cuing condition. We then calculated circular variance (<inline-formula><mml:math id="inf123"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) and kurtosis (<inline-formula><mml:math id="inf124"><mml:mi>k</mml:mi></mml:math></inline-formula>) as presented in the original paper, using the following equations:<disp-formula id="equ18"> <label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext>Arg</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mtext>Arg</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf125"><mml:msub><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf126"><mml:mi>n</mml:mi></mml:math></inline-formula> th uncentered trigonometric moment. A histogram with <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>31</mml:mn></mml:mrow></mml:math></inline-formula> bins was used to visualize the error distribution in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></sec><sec id="s4-2"><title>Timing effects</title><p>Human subjects (<inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>36</mml:mn></mml:mrow></mml:math></inline-formula>) were presented with 6 simultaneous color stimuli and had to report the color at a probed location as an angle on a color wheel. The RI and ITI lengths varied across sessions (RI: 1 or 3 s, ITI: 1 or 7.5 s) (<xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>). A histogram with <inline-formula><mml:math id="inf129"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>31</mml:mn></mml:mrow></mml:math></inline-formula> bins was used to visualize the error distribution in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></sec><sec id="s4-3"><title>Serial dependence increases with RI and decreases with ITI</title><p>Human subjects (<inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:math></inline-formula>) were presented with a black square at a random position on a circle and had to report the location of the cue (<xref ref-type="bibr" rid="bib18">Bliss and D’Esposito, 2017</xref>). The RI and ITI were varied across blocks of trials (RI: 0, 1, 3, 6, or 10 s, ITI: 1, 3, 6, or 10 s). For each block and subject, we computed serial dependence as the peak-to-peak amplitude of a derivative of Gaussian (DoG) function fit to the data. The DoG function is defined as follows:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>x</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf131"><mml:mi>y</mml:mi></mml:math></inline-formula> is the trial-wise error, <inline-formula><mml:math id="inf132"><mml:mi>x</mml:mi></mml:math></inline-formula> is the relative circular distance to the target angle of the previous trial, <inline-formula><mml:math id="inf133"><mml:mi>a</mml:mi></mml:math></inline-formula> is the amplitude of the DoG peak, <inline-formula><mml:math id="inf134"><mml:mi>w</mml:mi></mml:math></inline-formula> is the width of the curve, and <inline-formula><mml:math id="inf135"><mml:mi>c</mml:mi></mml:math></inline-formula> is the constant <inline-formula><mml:math id="inf136"><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msqrt></mml:math></inline-formula>, chosen such that the peak-to-peak amplitude of the DoG fit—the measure of serial dependence in <xref ref-type="bibr" rid="bib18">Bliss and D’Esposito, 2017</xref>—is exactly <inline-formula><mml:math id="inf137"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-4"><title>Build-up of serial dependence</title><p>Human subjects (<inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>) performed a delayed continuous report task with one item (<xref ref-type="bibr" rid="bib34">Foster et al., 2017</xref>). Following <xref ref-type="bibr" rid="bib7">Barbosa and Compte, 2020</xref>, we obtained a trial-by-trial measure of serial dependence using their definition of folded error.</p><p>Let <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>θ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> denotes the circular distance between the angle reported on the previous trial and the target angle on the current trial. In order to aggregate trials with negative <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>θ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> (preceding target is located clockwise to current target) and trials with positive <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>θ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> (preceding target is located counter-clockwise to current target), we computed the folded error as <inline-formula><mml:math id="inf142"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>e</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mtext>sign</mml:mtext></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf143"><mml:msub><mml:mi>θ</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> is the circular distance between the reported angle and the target angle. Positive <inline-formula><mml:math id="inf144"><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>e</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula> corresponds to attraction to the previous stimulus, whereas negative <inline-formula><mml:math id="inf145"><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>e</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula> corresponds to repulsion.</p><p>We excluded trials with absolute errors larger than <inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. We then computed serial bias as the average folded error in sliding windows of width <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> rad and steps of <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> rad. We repeated this procedure separately for the trials contained in the first and last third of all sessions. Finally, we computed the increase in serial dependence over the course of a session using a sliding window of 200 trials on the folded error.</p></sec><sec id="s4-5"><title>Serial dependence increases with set size</title><p>We reanalyzed the dataset collected by <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>, experiment 1a, in which human subjects (<inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>) performed a delayed response task with one or three items.</p><p>We calculated folded error using the procedure mentioned above. We excluded trials with absolute errors larger than <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. We then computed serial bias as the average folded error in sliding windows of width <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> rad and steps of <inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> rad. We repeated this procedure separately for the trials with <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> items. In order to test whether serial dependence was stronger for one of the set size conditions, we performed a permutation test: We shuffled the entire dataset and partitioned it into two groups of size <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf157"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the number of trials recorded for the set size condition <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>. We fitted a DoG curve (<xref ref-type="disp-formula" rid="equ20">Equation 20</xref>) to each partition using least squares and computed the difference between the peak amplitude of the two fits. We repeated this process 20,000 times. We then calculated the p-value as the proportion of shuffles for which the difference between the peak amplitudes was equal to or larger than the one computed using the unshuffled dataset.</p></sec><sec id="s4-6"><title>Continuous reports are biased toward high-frequency colors</title><p>Human subjects (<inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula>) performed a delayed continuous report task with a set size of 2 (<xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>). On each trial, the RI was either 0.5 or 4 s. The stimuli were either drawn from a uniform distribution or from a set of four equally spaced bumps of width <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> rad with equal probability. The centers of each bump were held constant for each subject.</p><p>We defined systematic bias as mean error versus distance to the closest bump center and computed it in sliding windows of width <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:math></inline-formula> rad and steps of <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> rad, as done in the original study. We repeated this procedure separately for the trials with <inline-formula><mml:math id="inf163"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and for the first and last third of trials within a session.</p></sec><sec id="s4-7"><title>Simulations and model fitting</title><p>For each dataset described above, we performed simulations with three different models: the <italic>full model</italic>, a model with <italic>fixed</italic> β (<inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), and a model with <italic>no plasticity</italic> (<inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). The following parameters were held fixed for all simulations, unless stated otherwise: <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf168"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf172"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> s. Weights <inline-formula><mml:math id="inf173"><mml:mi>w</mml:mi></mml:math></inline-formula> were clipped to be in the range <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. <italic>β</italic> was initialized at <inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> and clipped to be in the range <inline-formula><mml:math id="inf176"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>In order to account for the higher probing probability of the cued stimulus in <xref ref-type="bibr" rid="bib13">Bays, 2014</xref>, we used<disp-formula id="equ21"> <label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf177"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mtext>priority</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> otherwise, as given by the base rates.</p><p>Simulations were run on the same trials as given in the dataset. When multiple stimuli were presented simultaneously (<inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the values of non-probed stimuli were not included in the dataset, we used stimuli sampled at random in the range <inline-formula><mml:math id="inf180"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to replace the missing values.</p><p>When running a simulation, time was discretized into steps of length <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. The simulation time step <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> was manually set to provide a good trade-off between simulation resolution and run time. The learning rates <inline-formula><mml:math id="inf183"><mml:mi>η</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf184"><mml:mi>α</mml:mi></mml:math></inline-formula> were scaled by <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> to make the simulation results largely independent of the precise choice of <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. At each step, spikes <italic>z</italic><sub><italic>i</italic></sub> were generated by sampling from a Poisson distribution with parameter <inline-formula><mml:math id="inf187"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Subsequently, <italic>w</italic><sub><italic>i</italic></sub>, <italic>u</italic><sub><italic>i</italic></sub>, <italic>r</italic><sub><italic>i</italic></sub>, <inline-formula><mml:math id="inf188"><mml:mi>R</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf189"><mml:mi>β</mml:mi></mml:math></inline-formula> were computed using the equations given in the main text. At the end of the RI, model predictions were performed by decoding samples generated during a window of <inline-formula><mml:math id="inf190"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> s using maximum likelihood estimation.</p><p>The capacity <inline-formula><mml:math id="inf191"><mml:mi>C</mml:mi></mml:math></inline-formula>, the population gain <inline-formula><mml:math id="inf192"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>, and the plasticity gain parameter <inline-formula><mml:math id="inf193"><mml:mi>c</mml:mi></mml:math></inline-formula> were independently fitted for each subject to maximize the likelihood of the observed errors. To demonstrate the generalizability of these parameter estimates, the parameters were fitted for the dataset from <xref ref-type="bibr" rid="bib13">Bays, 2014</xref> only, and then applied without modification to the other datasets. We used the subject-averaged <inline-formula><mml:math id="inf194"><mml:mi>C</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf195"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>, and <inline-formula><mml:math id="inf196"><mml:mi>c</mml:mi></mml:math></inline-formula> to run simulations on the remaining datasets. The one exception was for <xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>, where responses appeared to be unusually noisy responses; for this dataset, we fixed <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>In order to compare model performance quantitatively, we fitted the model presented in <xref ref-type="bibr" rid="bib13">Bays, 2014</xref> on the dataset presented in the same paper. This model depends on two free parameters: <inline-formula><mml:math id="inf198"><mml:mi>ω</mml:mi></mml:math></inline-formula>, which controls the tuning width of the neurons, and <inline-formula><mml:math id="inf199"><mml:mi>γ</mml:mi></mml:math></inline-formula>, which controls the population gain and corresponds to <inline-formula><mml:math id="inf200"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> in our text. These parameters were fit to maximize the likelihood of the observed errors; the detailed model fitting procedure can be found in the original report. As outlined above, averaged parameter estimates were used to run simulations on the remaining datasets. Models were subsequently compared by computing the BIC, defined as:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>BIC</mml:mtext><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf201"><mml:mi>k</mml:mi></mml:math></inline-formula> is the number of parameters estimated by the model, <inline-formula><mml:math id="inf202"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of data points, and <inline-formula><mml:math id="inf203"><mml:msup><mml:mi>L</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:math></inline-formula> is the likelihood of the model. For the fitted data, the BIC was used to approximate the marginal likelihood, <inline-formula><mml:math id="inf204"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>data</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mtext>BIC</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which was then submitted to the Bayesian model selection algorithm described in <xref ref-type="bibr" rid="bib65">Rigoux et al., 2014</xref>. Since the same parameters were applied to all the other datasets (i.e., these were generalization tests of the model fit), we instead submitted the log-likelihood directly to Bayesian model selection.</p></sec><sec id="s4-8"><title>Dynamics of memory precision and neural gain</title><p>We reanalyzed the behavioral and neural dataset collected in <xref ref-type="bibr" rid="bib8">Barbosa et al., 2020</xref>. In this dataset, four adult male rhesus monkeys (<italic>Macaca mulatta</italic>) were trained in an oculomotor delayed response task that involved fixing their gaze on a central point and subsequently making a saccadic eye movement to the stimulus location after a delay period. While performing the task, firing of neurons in the dorsolateral prefrontal cortex was recorded. Since recordings were not available for all trials within a session, we ignored sessions in which only a subset of the eight potential cues were displayed.</p><p>We sorted the squared error on trial <inline-formula><mml:math id="inf205"><mml:mi>t</mml:mi></mml:math></inline-formula> (denoted by <inline-formula><mml:math id="inf206"><mml:msubsup><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>) based on six quantiles of the squared error on the previous trial. We then defined the indicator variable <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, taking the value +1 if the squared error on the previous trial was larger than the mean squared error, and −1 otherwise. We then fit the linear mixed model <inline-formula><mml:math id="inf208"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>session</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In order to infer the preferred stimulus of each recorded neuron, we used a least squares approach to fit the mean spike count for each presented stimulus and neuron to a bell-shaped tuning function:<disp-formula id="equ23"> <label>(23)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf209"><mml:mi>θ</mml:mi></mml:math></inline-formula> is the presented stimulus, <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <italic>w</italic><sub><italic>i</italic></sub> control the amplitude and width of the tuning function, respectively, and <inline-formula><mml:math id="inf211"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the preferred stimulus of neuron <inline-formula><mml:math id="inf212"><mml:mi>i</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib13">Bays, 2014</xref>).</p><p>We then fitted the neural data by performing Poisson regression for each neuron using the following model:<disp-formula id="equ24"> <label>(24)</label><mml:math id="m24"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>s</italic><sub><italic>j</italic></sub> is the number of spikes emitted by the neuron on trial <inline-formula><mml:math id="inf213"><mml:mi>j</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf214"><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the expected distortion between the stimulus <inline-formula><mml:math id="inf215"><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and the neuron’s preferred stimulus, and <inline-formula><mml:math id="inf216"><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is an exponential moving average of the neuron’s spike history with decay rate 0.8. We discarded three neurons for which the fitted <inline-formula><mml:math id="inf217"><mml:mi>β</mml:mi></mml:math></inline-formula> was negative and one neuron for which the fitted <inline-formula><mml:math id="inf218"><mml:mi>β</mml:mi></mml:math></inline-formula> was larger than 5 standard deviations above the mean of the fitted values.</p><p>In order to ascertain the utility of the different regressors, we fitted another model without the history term, and another without both the distortion and history terms, and compared them based on their BIC values.</p></sec><sec id="s4-9"><title>Source code</title><p>All simulations and analyses were performed using Julia, version 1.6.2. Source code can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/amvjakob/wm-rate-distortion">https://github.com/amvjakob/wm-rate-distortion</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib42">Jakob, 2023</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79450-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Source code can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/amvjakob/wm-rate-distortion">https://github.com/amvjakob/wm-rate-distortion</ext-link> (copy archived at <xref ref-type="bibr" rid="bib42">Jakob, 2023</xref>). The previously published datasets are available upon request from the corresponding authors of the published papers, <xref ref-type="bibr" rid="bib77">Souza and Oberauer, 2015</xref>, <xref ref-type="bibr" rid="bib19">Bliss et al., 2017</xref>, and <xref ref-type="bibr" rid="bib62">Panichello et al., 2019</xref>. A minimally processed dataset from <xref ref-type="bibr" rid="bib8">Barbosa et al., 2020</xref> is available online (<ext-link ext-link-type="uri" xlink:href="https://github.com/comptelab/interplayPFC">https://github.com/comptelab/interplayPFC</ext-link>), with the raw data available upon request from the corresponding author of the published paper (raw monkey data available upon request to Christos Constantinidis cconstan@wakehealth.edu, and raw EEG data available upon request to Heike Stein, heike.c.stein@gmail.com). There are no specific application or approval processes involved in requesting these datasets.</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>Noise in Neural Populations Accounts for Errors in Working Memory</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/s7dhn/">s7dhn</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Alpha-band activity reveals spontaneous representations of spatial position in visual working memory</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/vw4uc/">vw4uc</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset3"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Interplay between persistent activity and activity-silent dynamics in the prefrontal cortex underlies serial biases in working memory</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/comptelab/interplayPFC">comptelab/interplayPFC</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Johannes Bill, Wulfram Gerstner, and Chris Bates generously provided constructive feedback and discussion. This research was supported by a Bertarelli Fellowship and by the Center for Brains, Minds, and Machines (funded by NSF STC award CCF-1231216).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>LeMasson</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Analysis of neuron models with dynamically regulated conductances</article-title><source>Neural Computation</source><volume>5</volume><fpage>823</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.6.823</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>237</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.3.237</pub-id><pub-id pub-id-type="pmid">9143444</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arimoto</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>An algorithm for computing the capacity of arbitrary discrete memoryless channels</article-title><source>IEEE Transactions on Information Theory</source><volume>18</volume><fpage>14</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TIT.1972.1054753</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname><given-names>D</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>Journal of Cerebral Blood Flow and Metabolism</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id><pub-id pub-id-type="pmid">11598490</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Kimber</surname><given-names>D</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Metabolically efficient information processing</article-title><source>Neural Computation</source><volume>13</volume><fpage>799</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1162/089976601300014358</pub-id><pub-id pub-id-type="pmid">11255570</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balsam</surname><given-names>PD</given-names></name><name><surname>Gallistel</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal maps and informativeness in associative learning</article-title><source>Trends in Neurosciences</source><volume>32</volume><fpage>73</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.10.004</pub-id><pub-id pub-id-type="pmid">19136158</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Build-up of serial dependence in color working memory</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>10959</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-67861-2</pub-id><pub-id pub-id-type="pmid">32616792</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Stein</surname><given-names>H</given-names></name><name><surname>Martinez</surname><given-names>RL</given-names></name><name><surname>Galan-Gadea</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Dalmau</surname><given-names>J</given-names></name><name><surname>Adam</surname><given-names>KCS</given-names></name><name><surname>Valls-Solé</surname><given-names>J</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Interplay between persistent activity and activity-silent dynamics in the prefrontal cortex underlies serial biases in working memory</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1016</fpage><lpage>1024</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0644-4</pub-id><pub-id pub-id-type="pmid">32572236</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>CJ</given-names></name><name><surname>Lerch</surname><given-names>RA</given-names></name><name><surname>Sims</surname><given-names>CR</given-names></name><name><surname>Jacobs</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptive allocation of human visual working memory capacity during statistical and categorical learning</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1167/19.2.11</pub-id><pub-id pub-id-type="pmid">30802280</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>CJ</given-names></name><name><surname>Jacobs</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient data compression in perception and perceptual memory</article-title><source>Psychological Review</source><volume>127</volume><fpage>891</fpage><lpage>917</lpage><pub-id pub-id-type="doi">10.1037/rev0000197</pub-id><pub-id pub-id-type="pmid">32324016</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>CJ</given-names></name><name><surname>Alvarez</surname><given-names>G</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Scaling models of visual working memory to natural images</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.17.533050</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>PM</given-names></name><name><surname>Catalao</surname><given-names>RFG</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The precision of visual working memory is set by allocation of a shared resource</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/9.10.7</pub-id><pub-id pub-id-type="pmid">19810788</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noise in neural populations accounts for errors in working memory</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>3632</fpage><lpage>3645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3204-13.2014</pub-id><pub-id pub-id-type="pmid">24599462</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spikes not slots: noise in neural populations limits working memory</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>431</fpage><lpage>438</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.06.004</pub-id><pub-id pub-id-type="pmid">26160026</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A signature of neural coding at human perceptual limits</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/16.11.4</pub-id><pub-id pub-id-type="pmid">27604067</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>De Ruyter Van Steveninck</surname><given-names>RR</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient representation as a design principle for neural coding and computation</article-title><conf-name>2006 IEEE International Symposium on Information Theory</conf-name><pub-id pub-id-type="doi">10.1109/ISIT.2006.261867</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blahut</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Computation of channel capacity and rate-distortion functions</article-title><source>IEEE Transactions on Information Theory</source><volume>18</volume><fpage>460</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1109/TIT.1972.1054855</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bliss</surname><given-names>DP</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Synaptic augmentation in a cortical circuit model reproduces serial dependence in visual working memory</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0188927</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0188927</pub-id><pub-id pub-id-type="pmid">29244810</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bliss</surname><given-names>DP</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Serial dependence is absent at the time of perception but increases in visual working memory</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>14739</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-15199-7</pub-id><pub-id pub-id-type="pmid">29116132</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Compression in visual working memory: using statistical regularities to form more efficient memory representations</article-title><source>Journal of Experimental Psychology. General</source><volume>138</volume><fpage>487</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1037/a0016797</pub-id><pub-id pub-id-type="pmid">19883132</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A spiking neuron as information bottleneck</article-title><source>Neural Computation</source><volume>22</volume><fpage>1961</fpage><lpage>1992</lpage><pub-id pub-id-type="doi">10.1162/neco.2010.08-09-1084</pub-id><pub-id pub-id-type="pmid">20337537</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castner</surname><given-names>SA</given-names></name><name><surname>Williams</surname><given-names>GV</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Reversal of antipsychotic-induced working memory deficits by short-term dopamine D1 receptor stimulation</article-title><source>Science</source><volume>287</volume><fpage>2020</fpage><lpage>2022</lpage><pub-id pub-id-type="doi">10.1126/science.287.5460.2020</pub-id><pub-id pub-id-type="pmid">10720329</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chance</surname><given-names>FS</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Reyes</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Gain modulation from background synaptic input</article-title><source>Neuron</source><volume>35</volume><fpage>773</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00820-6</pub-id><pub-id pub-id-type="pmid">12194875</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>Gibbs</surname><given-names>SE</given-names></name><name><surname>Miyakawa</surname><given-names>A</given-names></name><name><surname>Jagust</surname><given-names>W</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Working memory capacity predicts dopamine synthesis capacity in the human striatum</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>1208</fpage><lpage>1212</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4475-07.2008</pub-id><pub-id pub-id-type="pmid">18234898</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inverted-U-shaped dopamine actions on human working memory and cognitive control</article-title><source>Biological Psychiatry</source><volume>69</volume><fpage>e113</fpage><lpage>e125</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2011.03.028</pub-id><pub-id pub-id-type="pmid">21531388</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cudmore</surname><given-names>RH</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Long-term potentiation of intrinsic excitability in LV visual cortical neurons</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>341</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1152/jn.01059.2003</pub-id><pub-id pub-id-type="pmid">14973317</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daoudal</surname><given-names>G</given-names></name><name><surname>Debanne</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Long-term plasticity of intrinsic excitability: learning rules and mechanisms</article-title><source>Learning &amp; Memory</source><volume>10</volume><fpage>456</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1101/lm.64103</pub-id><pub-id pub-id-type="pmid">14657257</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desai</surname><given-names>NS</given-names></name><name><surname>Rutherford</surname><given-names>LC</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Plasticity in the intrinsic excitability of cortical pyramidal neurons</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>515</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1038/9165</pub-id><pub-id pub-id-type="pmid">10448215</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Kelc</surname><given-names>M</given-names></name><name><surname>Güntürkün</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A neurocomputational theory of the dopaminergic modulation of working memory functions</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>2807</fpage><lpage>2822</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-07-02807.1999</pub-id><pub-id pub-id-type="pmid">10087092</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Beyond bistability: biophysics and temporal dynamics of working memory</article-title><source>Neuroscience</source><volume>139</volume><fpage>119</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2005.06.094</pub-id><pub-id pub-id-type="pmid">16326020</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egorov</surname><given-names>AV</given-names></name><name><surname>Hamam</surname><given-names>BN</given-names></name><name><surname>Fransén</surname><given-names>E</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Alonso</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Graded persistent activity in entorhinal cortex neurons</article-title><source>Nature</source><volume>420</volume><fpage>173</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature01171</pub-id><pub-id pub-id-type="pmid">12432392</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Serial dependence in visual perception</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>738</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1038/nn.3689</pub-id><pub-id pub-id-type="pmid">24686785</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>JJ</given-names></name><name><surname>Bsales</surname><given-names>EM</given-names></name><name><surname>Jaffe</surname><given-names>RJ</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Alpha-band activity reveals spontaneous representations of spatial position in visual working memory</article-title><source>Current Biology</source><volume>27</volume><fpage>3216</fpage><lpage>3223</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.09.031</pub-id><pub-id pub-id-type="pmid">29033335</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franklin</surname><given-names>NT</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structured Event Memory: A neuro-symbolic model of event cognition</article-title><source>Psychological Review</source><volume>127</volume><fpage>327</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1037/rev0000177</pub-id><pub-id pub-id-type="pmid">32223284</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritsche</surname><given-names>M</given-names></name><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Opposite effects of recent history on perception and decision</article-title><source>Current Biology</source><volume>27</volume><fpage>590</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.01.006</pub-id><pub-id pub-id-type="pmid">28162897</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Prefrontal cortex and working memory processes</article-title><source>Neuroscience</source><volume>139</volume><fpage>251</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2005.07.003</pub-id><pub-id pub-id-type="pmid">16325345</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Origin of perseveration in the trade-off between reward and complexity</article-title><source>Cognition</source><volume>204</volume><elocation-id>104394</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104394</pub-id><pub-id pub-id-type="pmid">32679270</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Kistler</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Spiking Neuron Models: Single Neurons, Populations, Plasticity.</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511815706</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hengen</surname><given-names>KB</given-names></name><name><surname>Lambo</surname><given-names>ME</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Firing rate homeostasis in visual cortex of freely behaving rodents</article-title><source>Neuron</source><volume>80</volume><fpage>335</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.038</pub-id><pub-id pub-id-type="pmid">24139038</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hengen</surname><given-names>KB</given-names></name><name><surname>Torrado Pacheco</surname><given-names>A</given-names></name><name><surname>McGregor</surname><given-names>JN</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neuronal firing rate homeostasis is inhibited by sleep and promoted by wake</article-title><source>Cell</source><volume>165</volume><fpage>180</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.01.046</pub-id><pub-id pub-id-type="pmid">26997481</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jakob</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Wm-rate-distortion</data-title><version designator="swh:1:rev:ac3210ae90fb28ef9edc97f0651b3ff3b136eef2">swh:1:rev:ac3210ae90fb28ef9edc97f0651b3ff3b136eef2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ebc2af8218f6599acf30732c7ad515f5f80d1395;origin=https://github.com/amvjakob/wm-rate-distortion;visit=swh:1:snp:bdcf5343b5f4bf0a42ecd8a701b343673797ff9a;anchor=swh:1:rev:ac3210ae90fb28ef9edc97f0651b3ff3b136eef2">https://archive.softwareheritage.org/swh:1:dir:ebc2af8218f6599acf30732c7ad515f5f80d1395;origin=https://github.com/amvjakob/wm-rate-distortion;visit=swh:1:snp:bdcf5343b5f4bf0a42ecd8a701b343673797ff9a;anchor=swh:1:rev:ac3210ae90fb28ef9edc97f0651b3ff3b136eef2</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolivet</surname><given-names>R</given-names></name><name><surname>Rauch</surname><given-names>A</given-names></name><name><surname>Lüscher</surname><given-names>HR</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models</article-title><source>Journal of Computational Neuroscience</source><volume>21</volume><fpage>35</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1007/s10827-006-7074-5</pub-id><pub-id pub-id-type="pmid">16633938</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>RD</given-names></name><name><surname>Wiest</surname><given-names>MC</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Extracellular calcium depletion as a mechanism of short-term synaptic depression</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>1952</fpage><lpage>1959</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.85.5.1952</pub-id><pub-id pub-id-type="pmid">11353012</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiyonaga</surname><given-names>A</given-names></name><name><surname>Scimeca</surname><given-names>JM</given-names></name><name><surname>Bliss</surname><given-names>DP</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Serial dependence across perception, attention, and memory</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>493</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.04.011</pub-id><pub-id pub-id-type="pmid">28549826</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klampfl</surname><given-names>S</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spiking neurons can learn to solve information bottleneck problems and extract independent components</article-title><source>Neural Computation</source><volume>21</volume><fpage>911</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.01-07-432</pub-id><pub-id pub-id-type="pmid">19018708</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koyluoglu</surname><given-names>OO</given-names></name><name><surname>Pertzov</surname><given-names>Y</given-names></name><name><surname>Manohar</surname><given-names>S</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fundamental bound on the persistence and capacity of short-term memory stored as graded persistent activity</article-title><source>eLife</source><volume>6</volume><elocation-id>e22225</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22225</pub-id><pub-id pub-id-type="pmid">28879851</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>L</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Policy compression: An information bottleneck in action selection</article-title><source>Psychology of Learning and Motivation</source><volume>74</volume><fpage>195</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/bs.plm.2021.02.004</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>SM</given-names></name><name><surname>Lal</surname><given-names>R</given-names></name><name><surname>O’Neil</surname><given-names>JP</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name><name><surname>Jagust</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Striatal dopamine and working memory</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>445</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn095</pub-id><pub-id pub-id-type="pmid">18550595</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Anderson</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The metabolic cost of neural information</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>36</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1038/236</pub-id><pub-id pub-id-type="pmid">10195106</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leibfried</surname><given-names>F</given-names></name><name><surname>Braun</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A reward-maximizing spiking neuron as a bounded rational decision maker</article-title><source>Neural Computation</source><volume>27</volume><fpage>1686</fpage><lpage>1720</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00758</pub-id><pub-id pub-id-type="pmid">26079747</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeMasson</surname><given-names>G</given-names></name><name><surname>Marder</surname><given-names>E</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Activity-dependent regulation of conductances in model neurons</article-title><source>Science</source><volume>259</volume><fpage>1915</fpage><lpage>1917</lpage><pub-id pub-id-type="doi">10.1126/science.8456317</pub-id><pub-id pub-id-type="pmid">8456317</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The cost of cortical computation</article-title><source>Current Biology</source><volume>13</volume><fpage>493</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(03)00135-0</pub-id><pub-id pub-id-type="pmid">12646132</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>WB</given-names></name><name><surname>Baxter</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Energy efficient neural codes</article-title><source>Neural Computation</source><volume>8</volume><fpage>531</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.3.531</pub-id><pub-id pub-id-type="pmid">8868566</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Segregation of working memory functions within the dorsolateral prefrontal cortex</article-title><source>Experimental Brain Research</source><volume>133</volume><fpage>23</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1007/s002210000397</pub-id><pub-id pub-id-type="pmid">10933207</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagy</surname><given-names>DG</given-names></name><name><surname>Török</surname><given-names>B</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Optimal forgetting: Semantic compression of episodic memories</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008367</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008367</pub-id><pub-id pub-id-type="pmid">33057380</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Helmers</surname><given-names>JC</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Chunking as a rational strategy for lossy data compression in visual working memory</article-title><source>Psychological Review</source><volume>125</volume><fpage>486</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1037/rev0000101</pub-id><pub-id pub-id-type="pmid">29952621</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Pfeiffer</surname><given-names>M</given-names></name><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003037</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003037</pub-id><pub-id pub-id-type="pmid">23633941</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberauer</surname><given-names>K</given-names></name><name><surname>Farrell</surname><given-names>S</given-names></name><name><surname>Jarrold</surname><given-names>C</given-names></name><name><surname>Lewandowsky</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What limits working memory capacity?</article-title><source>Psychological Bulletin</source><volume>142</volume><fpage>758</fpage><lpage>799</lpage><pub-id pub-id-type="doi">10.1037/bul0000046</pub-id><pub-id pub-id-type="pmid">26950009</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>SE</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predictive information in a sensory population</article-title><source>PNAS</source><volume>112</volume><fpage>6908</fpage><lpage>6913</lpage><pub-id pub-id-type="doi">10.1073/pnas.1506855112</pub-id><pub-id pub-id-type="pmid">26038544</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Error-correcting dynamics in visual working memory</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3366</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11298-3</pub-id><pub-id pub-id-type="pmid">31358740</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papadimitriou</surname><given-names>C</given-names></name><name><surname>Ferdoash</surname><given-names>A</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ghosts in the machine: memory interference from the previous trial</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>567</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1152/jn.00402.2014</pub-id><pub-id pub-id-type="pmid">25376781</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pertzov</surname><given-names>Y</given-names></name><name><surname>Manohar</surname><given-names>S</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rapid forgetting results from competition over time between items in visual working memory</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>43</volume><fpage>528</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1037/xlm0000328</pub-id><pub-id pub-id-type="pmid">27668485</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoux</surname><given-names>L</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian model selection for group studies - revisited</article-title><source>NeuroImage</source><volume>84</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id><pub-id pub-id-type="pmid">24018303</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawaguchi</surname><given-names>T</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>D1 dopamine receptors in prefrontal cortex: involvement in working memory</article-title><source>Science</source><volume>251</volume><fpage>947</fpage><lpage>950</lpage><pub-id pub-id-type="doi">10.1126/science.1825731</pub-id><pub-id pub-id-type="pmid">1825731</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneegans</surname><given-names>S</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Drift in neural population activity causes working memory to deteriorate over time</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>4859</fpage><lpage>4869</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3440-17.2018</pub-id><pub-id pub-id-type="pmid">29703786</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneegans</surname><given-names>S</given-names></name><name><surname>Taylor</surname><given-names>R</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stochastic sampling provides a unifying account of visual working memory limits</article-title><source>PNAS</source><volume>117</volume><fpage>20959</fpage><lpage>20968</lpage><pub-id pub-id-type="doi">10.1073/pnas.2004306117</pub-id><pub-id pub-id-type="pmid">32788373</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Servan-Schreiber</surname><given-names>D</given-names></name><name><surname>Printz</surname><given-names>H</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>A network model of catecholamine effects: gain, signal-to-noise ratio, and behavior</article-title><source>Science</source><volume>249</volume><fpage>892</fpage><lpage>895</lpage><pub-id pub-id-type="doi">10.1126/science.2392679</pub-id><pub-id pub-id-type="pmid">2392679</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Coding theorems for a discrete source with a fidelity criterion</article-title><conf-name>Institute of Radio Engineers, International Convention Record, vol. 7</conf-name><fpage>325</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1109/9780470544242.ch21</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>H</given-names></name><name><surname>Zou</surname><given-names>Q</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The effects of delay duration on visual working memory for orientation</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/17.14.10</pub-id><pub-id pub-id-type="pmid">29234786</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shipstead</surname><given-names>Z</given-names></name><name><surname>Engle</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Interference within the focus of attention: working memory tasks reflect more than temporary maintenance</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>39</volume><fpage>277</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1037/a0028467</pub-id><pub-id pub-id-type="pmid">22612165</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname><given-names>CR</given-names></name><name><surname>Jacobs</surname><given-names>RA</given-names></name><name><surname>Knill</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An ideal observer analysis of visual working memory</article-title><source>Psychological Review</source><volume>119</volume><fpage>807</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1037/a0029856</pub-id><pub-id pub-id-type="pmid">22946744</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cost of misremembering: Inferring the loss function in visual working memory</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/15.3.2</pub-id><pub-id pub-id-type="pmid">25740875</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rate-distortion theory and human perception</article-title><source>Cognition</source><volume>152</volume><fpage>181</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.03.020</pub-id><pub-id pub-id-type="pmid">27107330</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient coding explains the universal law of generalization in human perception</article-title><source>Science</source><volume>360</volume><fpage>652</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1126/science.aaq1118</pub-id><pub-id pub-id-type="pmid">29748284</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souza</surname><given-names>AS</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Time-based forgetting in visual working memory reflects temporal distinctiveness, not decay</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>22</volume><fpage>156</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.3758/s13423-014-0652-z</pub-id><pub-id pub-id-type="pmid">24825306</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stemmler</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>521</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1038/9173</pub-id><pub-id pub-id-type="pmid">10448216</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>R</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient coding in visual working memory accounts for stimulus-specific variations in recall</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7132</fpage><lpage>7142</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1018-18.2018</pub-id><pub-id pub-id-type="pmid">30006363</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomić</surname><given-names>I</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Internal but not external noise frees working memory resources</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006488</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006488</pub-id><pub-id pub-id-type="pmid">30321172</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>K</given-names></name><name><surname>Dubé</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A tale of two literatures: A fidelity-based integration account of central tendency bias and serial dependency</article-title><source>Computational Brain &amp; Behavior</source><volume>5</volume><fpage>103</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1007/s42113-021-00123-0</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname><given-names>R</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A resource-rational theory of set size effects in human visual working memory</article-title><source>eLife</source><volume>7</volume><elocation-id>e34963</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34963</pub-id><pub-id pub-id-type="pmid">30084356</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilken</surname><given-names>P</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A detection theory account of change detection</article-title><source>Journal of Vision</source><volume>4</volume><fpage>1120</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1167/4.12.11</pub-id><pub-id pub-id-type="pmid">15669916</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>K</given-names></name><name><surname>Nykamp</surname><given-names>DQ</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1038/nn.3645</pub-id><pub-id pub-id-type="pmid">24487232</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Duan</surname><given-names>Y</given-names></name><name><surname>Cheng</surname><given-names>A</given-names></name><name><surname>Jiang</surname><given-names>P</given-names></name><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Natural constraints explain working memory capacity limitations in sensory-cognitive models</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.30.534982</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>AH</given-names></name><name><surname>Klyszejko</surname><given-names>Z</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Strategic allocation of working memory resource</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>16162</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-34282-1</pub-id><pub-id pub-id-type="pmid">30385803</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaslavsky</surname><given-names>N</given-names></name><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Regier</surname><given-names>T</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient compression in color naming and its evolution</article-title><source>PNAS</source><volume>115</volume><fpage>7937</fpage><lpage>7942</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800521115</pub-id><pub-id pub-id-type="pmid">30021851</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sudden death and gradual decay in visual working memory</article-title><source>Psychological Science</source><volume>20</volume><fpage>423</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02322.x</pub-id><pub-id pub-id-type="pmid">19320861</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Strowbridge</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mechanisms of persistent activity in cortical circuits: Possible neural substrates for working memory</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>603</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-014006</pub-id><pub-id pub-id-type="pmid">28772102</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79450.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bays</surname><given-names>Paul</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.02.28.482269" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.02.28.482269"/></front-stub><body><p>This important study describes a model neural circuit that learns to optimally represent its inputs subject to an information capacity limit. This novel hypothesis provides a bridge between the theoretical frameworks of rate-distortion theory and neural population coding. Convincing evidence is presented that this model can account for a range of empirical phenomena in the visual working memory literature.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79450.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bays</surname><given-names>Paul</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bays</surname><given-names>Paul</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.02.28.482269">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.02.28.482269v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Rate-distortion theory of neural coding and its implications for working memory&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Paul Bays as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Expand and clarify the description of the model and how and why it makes the highlighted predictions, to fill in missing steps and make it more comprehensible to readers from a range of backgrounds.</p><p>2) Explain to what extent the neural model and its predictions are compatible with neurophysiological observations of stable tuning functions, and the recurrent excitation believed to maintain activity during a delay.</p><p>3) Expand the manuscript to better situate the current model in relation to other mechanistic WM models in the literature, and perform quantitative fitting to more concretely evaluate the model's ability to reproduce empirical data. While formal model comparison (e.g. with AIC) may not be necessary, it is important to evaluate the present model's match to data against other models' to understand its relative strengths and weaknesses. As the model is designed to implement an information capacity limit, its ability to reproduce set size effects (on error variability and error distribution) seems worthy of particular attention.</p><p>4) Consider and develop the implications of the model's basic principles (e.g. the idea that periods when WM requirement is low can be traded for a boost in information capacity at a later time) for a broader range of WM tasks and observations. This doesn't have to be quantitative fitting but needs to counter the argument that the empirical data currently presented has been selected to fit the model.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Some more technical points:</p><p>The &quot;simple update rule&quot; of Equation 14 requires knowledge of the information rate R (Equation 13) – is there a plausible method by which this could be computed in the circuit?</p><p>WTA, as utilized in the model, is generally a suboptimal decoding method – would predictions change for population vector or ML decoding?</p><p>Cosine tuning curves for input neurons lead to von Mises tuning curves for the output neurons – if that's correct it might be worth spelling out as it is the assumed tuning in previous population models of WM.</p><p>Excess kurtosis in error distributions has been an important point of debate in WM modelling – it appears to be present in the simulated data, but how does it arise in the model?</p><p>How are the smooth &quot;Data&quot; distribution plots generated? Smoothing may be misleading if the shape of the distribution is a question of interest, as it is for set size effects. The smoothing does not appear to take into account the circularity of the response space, based on what happens at π and -pi.</p><p>The description of how model predictions are generated needs to be substantially expanded, to explain step-by-step how the data that appears in the &quot;Simulation&quot; panels were produced. How were estimates obtained? How many repetitions/simulated trials were used (the plots are quite noisy)? To what extent were the simulated trials matched to the real ones, in terms of individual stimuli, etc?</p><p>What does it mean for the model that the parameter C had to be made ten times smaller for one experiment than the others?</p><p>&quot;Spikes contributed to intrinsic synaptic plasticity for 10 timesteps&quot; – what is the implication of this? Is it plausible? Do the results depend on it?</p><p>The section on primate data includes results of a model comparison – the methods need to be explained.</p><p>Figure 6A axis-label &quot;Relative color of previous trial&quot; – what does this refer to when the set size is three?</p><p>The scales in Figure 3B and D don't match.</p><p>The variable u_i is in several places (legend of Figure 1, before Equation 15, maybe others) described as the membrane potential instead of excitatory input (which I think is the intended meaning).</p><p>Legend of Figure 8B refers to an orientation tuning curve, but I don't believe the task involved oriented stimuli: angular location is probably what is intended.</p><p>Figure 1 and elsewhere – using K to indicate set size is going to be very confusing for WM researchers as it has long been used to refer to WM capacity. SS or even N would be preferable.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Equation (2): More explanation is needed when moving from the constrained optimization problem to the unconstrained optimization by forming a Lagrangian, especially for some readers of <italic>eLife</italic>. Add a couple of sentences justifying this step here.</p><p>p.3: OK, I'm going to be a bit nitpicky here. &quot;The rate-distortion function is monotonically decreasing&quot; – I'm not sure you can be sure of this. Increasing the capacity could yield no decrease in distortion in some cases (like if you're already at D=0), so technically you should say &quot;monotonically non-increasing&quot; I think. You also say this a couple of paragraphs later to justify \β being decreased, but I think you can only ensure it's non-increasing, I think, but maybe you can convince me otherwise.</p><p>Equation (4): Are you just integrating the ODE defined by (2) and (3) to get this result? If so, say that, or give a bit more info on how you arrive at this formula.</p><p>&quot;We assume that inhibition is strong enough such that only one neuron in the population will be active within an infinitesimally small time window.&quot; – I think this statement is unnecessary. As long as you make a time window small enough, the probability of co-occurring spikes in a window will be zero.</p><p>p.6: When you talk about recurrent models, I don't really see any semblance of recurrence in your models. They're just feedforward, right? The weights w_i seem to really just to be inputs. r_i does not feedback into all the other u_j's. Admittedly, this would be a trickier optimization problem to solve using a simple learning rule. Do you have an idea how you would do this if you introduced recurrence into the formulation Equation. (10) and (11)? You could punt this to the Discussion or get into more detail here, but not worrying enough about the mechanism of maintenance of the population spiking means you have to artificially dial down the capacity as the delay time is increased when you get to the &quot;Timing&quot; section on p.9.</p><p>Figure 2: You're qualitatively replicating the data, but admittedly missing some details. Variance scales up slower in the model and kurtosis drops off slower. Why? Is this just a case of people satisficing? Also, I think if you follow the analysis in the supplement of Sims et al. (2012), you should be able to get a good analytic approximation of 2D assuming an unbounded (rather than periodic) domain. Even if you place the problem on a periodic domain, I think you can solve the rate-distortion problem by hand to find the correctly parameterized von Mises distribution.</p><p>&quot;Increasing the intertrial interval reduces the information rate, since no stimuli are being communicated during that time period, and can therefore compensate for longer retention intervals.&quot; – I don't agree with this interpretation. I would think that longer ITIs might lead to less serial bias. Mechanistically, I don't see what else would disrupt persistent activity in a neural circuit as a function of ITI.</p><p>p.10: Neural firing rate activity from the previous trial seems an unlikely candidate for the primary mechanism for serial dependence given the persistent activity is typically gone between trials (see Barbosa et al. 2020) – e.g., short-term plasticity or something else seems more likely. I know you're aiming at a parsimonious model, but I think this point warrants discussion.</p><p>For all your Simulation/Data comparisons, it's not clear exactly how you chose model parameters. Did you do something systematic to try and best replicate the trends in data? Did you pick an arbitrary \β and stick with this throughout? More and clearer information about this would be helpful.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>It would be informative to formally compare the current model against alternatives [as one example, Matthey, L., Bays, P. M., and Dayan, P. (2015). A probabilistic palimpsest model of visual short-term memory. PLoS computational biology, 11(1), e1004003.]. As it stands, I am not sure if the model is &quot;yet another model&quot; in a fairly large space, or whether it makes unique predictions or outperforms (or perhaps underperforms) existing models.</p><p>Absent formal model comparisons, an extended discussion of the current model's strengths, and especially weaknesses, would greatly improve the manuscript.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Rate-distortion theory of neural coding and its implications for working memory&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Joshua Gold (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved, and two out of three reviewers are now satisfied, but there are a small number of remaining issues that need to be addressed, as outlined below:</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This revision resolves a number of previous concerns, and the clarity of presentation of the model is much improved. There are two issues remaining that I hope can be dealt with relatively straightforwardly:</p><p>1. The revision makes clearer where the empirical data, to which the models are compared and fit, comes from, including that the data in Figure 2A corresponds to Exp 1 in ref [15]. However, if that is the case, why is the data from set size 1 missing? Was it also omitted from model fitting? Please remedy this, as predicting recall performance with a single item is pretty crucial for a working memory model.</p><p>2. The problems I noted with data smoothing haven't been dealt with. I checked the plots in the original papers corresponding to Figures2 and 3, and its clear the smoothing is strongly distorting the distribution shapes (which form a key part of the evidence the models are intended to reproduce). Is there a reason you can't just plot histograms, like the original papers did for the data?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79450.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Expand and clarify the description of the model and how and why it makes the highlighted predictions, to fill in missing steps and make it more comprehensible to readers from a range of backgrounds.</p></disp-quote><p>We’ve now expanded the methods and Results sections to make the model description clearer. Please see responses to the comments below.</p><disp-quote content-type="editor-comment"><p>2) Explain to what extent the neural model and its predictions are compatible with neurophysiological observations of stable tuning functions, and the recurrent excitation believed to maintain activity during a delay.</p></disp-quote><p>As we explain below and on p. 7, we deliberately chose to be agnostic about the memory maintenance mechanism. Several have been proposed in the literature, all of which might be compatible with our framework. The essential constraint imposed by our framework is the capacity limit. Any maintenance mechanism that adheres to this capacity limit is compatible.</p><p>We’re not entirely sure what aspect of stable tuning is being referenced here. Some studies suggest representational drift in visual working memory (e.g., Murray et al., 2017; Wolff et al., 2020), so it’s a bit unclear how stable the tuning functions really are. In our setup, the input and output neurons have stable tuning functions; what changes across time is only the gain and excitability.</p><disp-quote content-type="editor-comment"><p>3) Expand the manuscript to better situate the current model in relation to other mechanistic WM models in the literature, and perform quantitative fitting to more concretely evaluate the model's ability to reproduce empirical data. While formal model comparison (e.g. with AIC) may not be necessary, it is important to evaluate the present model's match to data against other models' to understand its relative strengths and weaknesses. As the model is designed to implement an information capacity limit, its ability to reproduce set size effects (on error variability and error distribution) seems worthy of particular attention.</p></disp-quote><p>We now include quantitative model fitting and comparison (summarized in Table 1), both to an existing model (Bays, 2014) and to several variants of the rate-distortion model that lack key features (gain adaptation and intrinsic plasticity).</p><disp-quote content-type="editor-comment"><p>4) Consider and develop the implications of the model's basic principles (e.g. the idea that periods when WM requirement is low can be traded for a boost in information capacity at a later time) for a broader range of WM tasks and observations. This doesn't have to be quantitative fitting but needs to counter the argument that the empirical data currently presented has been selected to fit the model.</p></disp-quote><p>We believe that the standard for breadth should be based on comparable papers in the literature. The most closely related papers to our own are Bays (2014) and Sims (2015). Both of those papers analyzed similar visual working memory tasks (indeed, we analyze some of the same datasets), using similar visual and quantitative metrics. Neither paper attempted to model neurophysiological data, serial dependence, stimulus inhomogeneities, or the effect of intertrial interval timing (some of these were addressed in later papers, though). Given the breadth of findings that are already modeled in our paper, we think that it’s not really a fair assessment to say that we selected data to fit the model. We are in fact trying to explain data that were ignored by many previous models.</p><p>We agree that expanding the broader implications of our research is a useful addition. We have done this in a few places. In the “open questions” section of the Discussion (p. 19), we have added the following paragraph:</p><p>“In experiments with humans, it has been reported that pharmacological manipulations of dopamine can have non-monotonic effects on cognitive performance, with the direction of the effect depending on baseline dopamine levels (see [83] for a review). The baseline level (particularly in the striatum) correlates with working memory performance [84, 85]. Taken together, these findings suggest that dopaminergic neuromodulation controls the capacity limit (possibly through a gain control mechanism), and that pushing dopamine levels beyond the system’s capacity provokes a compensatory decrease in gain, as predicted by our homeostatic model of gain adaptation. A more direct test of our model would use continuous report tasks to quantify memory precision, bias, and serial dependence under different levels of dopamine.”</p><p>Immediately below this paragraph, we have added another new paragraph addressing potential extensions that can address a wider range of tasks:</p><p>“We have considered a relatively restricted range of visual working memory tasks for which extensive data are available. An important open question concerns the generality of our model beyond these tasks. For example, serial order, AX-CPT, and N-back tasks are widely used but outside the scope of our model. With appropriate modification, the rate-distortion framework can be applied more broadly. For example, one could construct channels for sequences rather than individual items, analogous to how we have handled multiple simultaneously presented stimuli. One could also incorporate a capacity-limited attention mechanism for selecting previously presented information for high fidelity representation, rather than storing everything from a fixed temporal window with relatively low fidelity. This could lead to a new information-theoretic perspective on attentional gating in working memory.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Some more technical points:</p><p>The &quot;simple update rule&quot; of Equation 14 requires knowledge of the information rate R (Equation 13) – is there a plausible method by which this could be computed in the circuit?</p></disp-quote><p>This is a great question. We have sketched an answer on p. 6:</p><p>“In this paper, we do not directly model how the information rate R is estimated in a biologically plausible way. One possibility is that this is implemented with slowly changing extracellular calcium levels, which decrease when cells are stimulated and then slowly recover. This mirrors (inversely) the qualitative behavior of the information rate. More quantitatively, it has been posited that the relationship between firing rate and extracellular calcium level is logarithmic [40], consistent with the mathematical definition in Equation. 13. Thus, in this model, capacity C corresponds to a calcium set point, and the gain parameter adapts to maintain this set point. A related mechanism has been proposed to control intrinsic excitability via calcium-driven changes in ion channel conductance [41, 42].”</p><disp-quote content-type="editor-comment"><p>WTA, as utilized in the model, is generally a suboptimal decoding method – would predictions change for population vector or ML decoding?</p></disp-quote><p>This is an interesting question, but note that in this case the WTA mechanism is in fact optimal under our specified objective function. We think comparing different decoders would take us too far afield, since our goal was to show how to implement an optimal channel under rate-distortion theory.</p><disp-quote content-type="editor-comment"><p>Cosine tuning curves for input neurons lead to von Mises tuning curves for the output neurons – if that's correct it might be worth spelling out as it is the assumed tuning in previous population models of WM.</p></disp-quote><p>Thanks for pointing this out. We now note this explicitly on p. 7.</p><disp-quote content-type="editor-comment"><p>Excess kurtosis in error distributions has been an important point of debate in WM modelling – it appears to be present in the simulated data, but how does it arise in the model?</p></disp-quote><p>Excess kurtosis was noted by Bays (2014) in his population coding model, but as far as we know there isn’t a naturally intuitive explanation for why this arises from the model. Although we don’t have an intuitive explanation, we’ve elaborated our description of the phenomenon on p. 9:</p><p>“Kurtosis is one way of quantifying deviation from normality: values greater than 0 indicate tails of the error distribution that are heavier than expected under a normal distribution. The ‘excess’ kurtosis observed in our model is comparable to that observed by Bays in his population coding model [15] when gain is sufficiently low. This is not surprising, given the similarity of the models.”</p><disp-quote content-type="editor-comment"><p>How are the smooth &quot;Data&quot; distribution plots generated? Smoothing may be misleading if the shape of the distribution is a question of interest, as it is for set size effects. The smoothing does not appear to take into account the circularity of the response space, based on what happens at π and -pi.</p></disp-quote><p>For Figures 2 and 3, we used a kernel density estimate plot to visualize the smoothed data distribution. We used the Python function `kdeplot` from the `seaborn` package with the default parameters.</p><p>For Figures 5, 6, 7, we obtained the smoothed data distribution plots by moving window averaging. The precise smoothing parameters we used are given in the Methods. While smoothing the data distribution might introduce artifacts on the edges of the plot domain, the rest of the plot is unaffected for sufficiently light smoothing.</p><disp-quote content-type="editor-comment"><p>The description of how model predictions are generated needs to be substantially expanded, to explain step-by-step how the data that appears in the &quot;Simulation&quot; panels were produced. How were estimates obtained? How many repetitions/simulated trials were used (the plots are quite noisy)? To what extent were the simulated trials matched to the real ones, in terms of individual stimuli, etc?</p></disp-quote><p>We have clarified the model prediction procedure in a step-by-step fashion on p. 22. Furthermore, in the revised version of this manuscript, the simulated trials exactly correspond to the trials in the dataset. While the investigated datasets always contained information on the probed stimulus, in the case of multiple simultaneous stimuli the value of non-probed stimuli were sometimes omitted. In such cases, the missing values were replaced with stimuli sampled at random in the range [-pi, pi].</p><disp-quote content-type="editor-comment"><p>What does it mean for the model that the parameter C had to be made ten times smaller for one experiment than the others?</p></disp-quote><p>As we now clarify in the paper, this particular dataset appears to contain unusually noisy responses, which might be due to several factors, such as the task being more challenging or the subjects being less attentive. Therefore, in order to reasonably fit the dataset, we lowered the model gain. This modeling choice is obviously ad hoc, but hopefully it is justified based on the adequacy of the model fit.</p><disp-quote content-type="editor-comment"><p>&quot;Spikes contributed to intrinsic synaptic plasticity for 10 timesteps&quot; – what is the implication of this? Is it plausible? Do the results depend on it?</p></disp-quote><p>Our results don’t depend strongly on this assumption, and it has been removed from the manuscript. In the revised model, a spike contributes to intrinsic plasticity for one timestep only, the timestep during which it was generated.</p><disp-quote content-type="editor-comment"><p>The section on primate data includes results of a model comparison – the methods need to be explained.</p></disp-quote><p>We have expanded the methods section on primate data analysis, including information on the source of the neural recordings in addition to details of model comparison.</p><disp-quote content-type="editor-comment"><p>Figure 6A axis-label &quot;Relative color of previous trial&quot; – what does this refer to when the set size is three?</p></disp-quote><p>Despite showing three cues, only one is probed during each trial. In this context, “relative color of previous trial” refers to the color of the cue that was probed on the previous trial. We now state this explicitly in the caption.</p><disp-quote content-type="editor-comment"><p>The scales in Figure 3B and D don't match.</p></disp-quote><p>Thank you for pointing this out; it has been corrected.</p><disp-quote content-type="editor-comment"><p>The variable u_i is in several places (legend of Figure 1, before Equation 15, maybe others) described as the membrane potential instead of excitatory input (which I think is the intended meaning).</p></disp-quote><p>Thanks for catching that error; we’ve corrected it throughout.</p><disp-quote content-type="editor-comment"><p>Legend of Figure 8B refers to an orientation tuning curve, but I don't believe the task involved oriented stimuli: angular location is probably what is intended.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>Figure 1 and elsewhere – using K to indicate set size is going to be very confusing for WM researchers as it has long been used to refer to WM capacity. SS or even N would be preferable.</p></disp-quote><p>We’ve changed “K” to “M” throughout.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Equation (2): More explanation is needed when moving from the constrained optimization problem to the unconstrained optimization by forming a Lagrangian, especially for some readers of eLife. Add a couple of sentences justifying this step here.</p></disp-quote><p>We’ve added several sentences around Equation. 2 to hopefully make this more accessible.</p><disp-quote content-type="editor-comment"><p>p.3: OK, I'm going to be a bit nitpicky here. &quot;The rate-distortion function is monotonically decreasing&quot; – I'm not sure you can be sure of this. Increasing the capacity could yield no decrease in distortion in some cases (like if you're already at D=0), so technically you should say &quot;monotonically non-increasing&quot; I think. You also say this a couple of paragraphs later to justify \β being decreased, but I think you can only ensure it's non-increasing, I think, but maybe you can convince me otherwise.</p></disp-quote><p>We’ve changed “decreasing” to “non-increasing” in both places.</p><disp-quote content-type="editor-comment"><p>Equation (4): Are you just integrating the ODE defined by (2) and (3) to get this result? If so, say that, or give a bit more info on how you arrive at this formula.</p></disp-quote><p>We are indeed integrating the ODE defined by Eqs. 2 and 3 to obtain Equation. 4. We have clarified this on p. 22.</p><disp-quote content-type="editor-comment"><p>&quot;We assume that inhibition is strong enough such that only one neuron in the population will be active within an infinitesimally small time window.&quot; – I think this statement is unnecessary. As long as you make a time window small enough, the probability of co-occurring spikes in a window will be zero.</p></disp-quote><p>We have removed this sentence.</p><disp-quote content-type="editor-comment"><p>p.6: When you talk about recurrent models, I don't really see any semblance of recurrence in your models. They're just feedforward, right? The weights w_i seem to really just to be inputs. r_i does not feedback into all the other u_j's. Admittedly, this would be a trickier optimization problem to solve using a simple learning rule. Do you have an idea how you would do this if you introduced recurrence into the formulation Equation. (10) and (11)? You could punt this to the Discussion or get into more detail here, but not worrying enough about the mechanism of maintenance of the population spiking means you have to artificially dial down the capacity as the delay time is increased when you get to the &quot;Timing&quot; section on p.9.</p></disp-quote><p>We only mention recurrent networks as one possible model that realizes persistent activation (on p. 7 we discuss other possibilities). We didn’t want to commit to a particular implementation because our theoretical framework is largely agnostic with respect to this choice (this is arguably true of the Bays 2014 population coding model as well).</p><p>Note that we did not artificially dial down the capacity as the delay time increases. The gain parameter is endogenously set by Equation. 14. Critically, capacity is assumed to stay fixed across time (we treat this as a physical/structural property of the memory system). As explained in the “memory maintenance” section, interval dependence arises from the fact that a fixed capacity is allocated (typically evenly) across a given interval.</p><disp-quote content-type="editor-comment"><p>Figure 2: You're qualitatively replicating the data, but admittedly missing some details. Variance scales up slower in the model and kurtosis drops off slower. Why? Is this just a case of people satisficing? Also, I think if you follow the analysis in the supplement of Sims et al. (2012), you should be able to get a good analytic approximation of 2D assuming an unbounded (rather than periodic) domain. Even if you place the problem on a periodic domain, I think you can solve the rate-distortion problem by hand to find the correctly parameterized von Mises distribution.</p></disp-quote><p>With respect to the figure, it’s true that we are missing some details, but in our view the differences between model and data are rather subtle.</p><p>The analysis suggestion is intriguing, but we felt that it is outside the scope of our paper, which tries to follow closely the setup of Bays (2014), which assumed a periodic domain.</p><disp-quote content-type="editor-comment"><p>&quot;Increasing the intertrial interval reduces the information rate, since no stimuli are being communicated during that time period, and can therefore compensate for longer retention intervals.&quot; – I don't agree with this interpretation. I would think that longer ITIs might lead to less serial bias. Mechanistically, I don't see what else would disrupt persistent activity in a neural circuit as a function of ITI.</p></disp-quote><p>Maybe there is some confusion here? The data show that longer ITIs reduce serial dependence, and we reproduce this with our model. So we’re not sure what the point of disagreement is.</p><disp-quote content-type="editor-comment"><p>p.10: Neural firing rate activity from the previous trial seems an unlikely candidate for the primary mechanism for serial dependence given the persistent activity is typically gone between trials (see Barbosa et al. 2020) – e.g., short-term plasticity or something else seems more likely. I know you're aiming at a parsimonious model, but I think this point warrants discussion.</p></disp-quote><p>To be clear, we are not suggesting that the model is updating the gain parameter on a trial-by-trial basis. Rather, Equation. 14 is being applied continuously (which we now state explicitly). So there is no need to assume that persistent activity is maintained between trials.</p><disp-quote content-type="editor-comment"><p>For all your Simulation/Data comparisons, it's not clear exactly how you chose model parameters. Did you do something systematic to try and best replicate the trends in data? Did you pick an arbitrary \β and stick with this throughout? More and clearer information about this would be helpful.</p></disp-quote><p>Thanks for this suggestion. We have now completely redone the modeling using fitted parameters. Our model fitting procedures are described in the Methods (p. 22).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>It would be informative to formally compare the current model against alternatives [as one example, Matthey, L., Bays, P. M., and Dayan, P. (2015). A probabilistic palimpsest model of visual short-term memory. PLoS computational biology, 11(1), e1004003.]. As it stands, I am not sure if the model is &quot;yet another model&quot; in a fairly large space, or whether it makes unique predictions or outperforms (or perhaps underperforms) existing models.</p></disp-quote><p>This is a very interesting model of visual working memory. However, it is primarily focused on memory for multi-feature items, a topic that we don’t address in this paper. While the binding problem in memory is extremely important, we felt that it was outside the scope of this paper, especially given that we are comparing our model to several others already.</p><disp-quote content-type="editor-comment"><p>Absent formal model comparisons, an extended discussion of the current model's strengths, and especially weaknesses, would greatly improve the manuscript.</p></disp-quote><p>Agreed, which is why now we include formal model comparison, summarized in Table 1 and discussed on p. 14.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved, and two out of three reviewers are now satisfied, but there are a small number of remaining issues that need to be addressed, as outlined below:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>This revision resolves a number of previous concerns, and the clarity of presentation of the model is much improved. There are two issues remaining that I hope can be dealt with relatively straightforwardly:</p><p>1. The revision makes clearer where the empirical data, to which the models are compared and fit, comes from, including that the data in Figure 2A corresponds to Exp 1 in ref [15]. However, if that is the case, why is the data from set size 1 missing? Was it also omitted from model fitting? Please remedy this, as predicting recall performance with a single item is pretty crucial for a working memory model.</p></disp-quote><p>Thank you for raising this point. We included the data from set size 1 in the model fitting; we merely omitted plotting it in Figure 2A for visual consistency with Figure 2BC, which uses the dataset from Exp 2 in ref [15], for which only set sizes 2, 4 and 8 are available.</p><p>We have now additionally plotted the data and simulation results pertaining to set size 1 in Figure 2A,D,G,J.</p><disp-quote content-type="editor-comment"><p>2. The problems I noted with data smoothing haven't been dealt with. I checked the plots in the original papers corresponding to Figures2 and 3, and its clear the smoothing is strongly distorting the distribution shapes (which form a key part of the evidence the models are intended to reproduce). Is there a reason you can't just plot histograms, like the original papers did for the data?</p></disp-quote><p>We initially smoothed the error distribution plots to remove sampling noise from the simulation results. However, as rightfully noted, this was distorting the distribution shape, especially at the boundaries of the function domain.</p><p>We have remedied this matter by plotting histograms instead for both the original data and the simulation results in Figure 2 and Figure 3.</p></body></sub-article></article>