<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="editorial" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75830</article-id><article-id pub-id-type="doi">10.7554/eLife.75830</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Editorial</subject></subj-group><subj-group subj-group-type="heading"><subject>Cancer Biology</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Reproducibility in Cancer Biology</subject></subj-group></article-categories><title-group><article-title>What have we learned?</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-1390"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8332-936X</contrib-id><email>p.rodgers@elifesciences.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><bio><p><bold>Peter Rodgers</bold> is the Features Editor of eLife</p></bio></contrib><contrib contrib-type="author" id="author-1001"><name><surname>Collings</surname><given-names>Andy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9570-2061</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><bio><p><bold>Andy Collings</bold> is the Executive Editor of eLife</p></bio></contrib><aff id="aff1"><label>1</label><institution>eLife</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>12</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e75830</elocation-id><history><date date-type="received" iso-8601-date="2021-11-25"><day>25</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-11-25"><day>25</day><month>11</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Rodgers and Collings</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Rodgers and Collings</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75830-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75830-figures-v2.pdf"/><abstract><p>As the final outputs of the Reproducibility Project: Cancer Biology are published, it is clear that preclinical research in cancer biology is not as reproducible as it should be.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Reproducibility Project: Cancer Biology</kwd><kwd>replication</kwd><kwd>reproducibility</kwd><kwd>open science</kwd><kwd>meta-analysis</kwd><kwd>reporting standards</kwd></kwd-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>As the final outputs of the Reproducibility Project: Cancer Biology are published, it is clear that preclinical research in cancer biology is not as reproducible as it should be.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>Back in 2014, when the first articles from the Reproducibility Project: Cancer Biology (<ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">RPCB</ext-link>) were published in eLife, there were widespread concerns about what seemed to be low levels of replicability and reproducibility in some areas of research. Researchers at two drug companies – Bayer and Amgen – had reported that they had not been able to replicate many published findings in cancer biology and other areas of preclinical research (<xref ref-type="bibr" rid="bib17">Prinz et al., 2011</xref>; <xref ref-type="bibr" rid="bib5">Begley and Ellis, 2012</xref>). Since then large-scale studies of replicability and reproducibility in psychology, economics and other areas of research (<xref ref-type="bibr" rid="bib16">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="bib8">Camerer et al., 2016</xref>), reports from learned societies (<xref ref-type="bibr" rid="bib1">Academy of Medical Sciences, 2015</xref>; <xref ref-type="bibr" rid="bib14">NAS, 2019</xref>), surveys of researchers (<xref ref-type="bibr" rid="bib4">Baker, 2016</xref>; <xref ref-type="bibr" rid="bib7">Boulbes et al., 2018</xref>), and popular books (<xref ref-type="bibr" rid="bib12">Harris, 2017</xref>; <xref ref-type="bibr" rid="bib18">Ritchie, 2020</xref>) have ensured that concerns about the 'reproducibility crisis' have maintained a high profile ever since.</p><p>The RPCB had two main aims: to provide evidence about replicability in preclinical cancer research, and to identify the factors that influence replicability more generally. Now, seven years later, the final three articles from the project have just been published, and they confirm that there is still considerable scope for improving the reproducibility of preclinical research in cancer biology (<xref ref-type="bibr" rid="bib9">Errington et al., 2021a</xref>; <xref ref-type="bibr" rid="bib10">Errington et al., 2021b</xref>; <xref ref-type="bibr" rid="bib11">Errington et al., 2021c</xref>).</p><p>The RPCB was a collaboration between the <ext-link ext-link-type="uri" xlink:href="https://www.cos.io/">Center for Open Science</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://www.scienceexchange.com/">Science Exchange</ext-link>, and the project was funded by a grant from a private foundation (now called Arnold Ventures). To achieve its aims the project team planned to repeat selected experiments from 53 <ext-link ext-link-type="uri" xlink:href="https://osf.io/e81xl/wiki/studies">high-profile papers</ext-link> in the field of cancer biology that had been published in the period 2010–2012. eLife agreed to be the publishing partner for the project, and to use what was then a new approach to peer review to assess the outputs of the project.</p><p>Under this approach, for each paper selected, the project team would prepare a Registered Report that described in detail how the experiments would be carried out and how the data would be analyzed. Each Registered Report would be peer reviewed, and experiments could not begin until it had been accepted for publication. The results of the experiments would then be written up as a Replication Study, which would be peer reviewed to ensure that the experiments and data analysis had been performed in accordance with the Registered Report. Where possible one of the authors of the original paper would be involved in the peer review of both the Registered Report and the Replication Study.</p><p>A total of 193 experiments from 53 papers were selected for replication, and the project team set about preparing Registered Reports for each paper. However, as recounted in detail in 'Challenges for assessing replicability in preclinical cancer biology' (<xref ref-type="bibr" rid="bib9">Errington et al., 2021a</xref>), the team encountered problems almost immediately. For example, many of the original papers failed to report key descriptive and inferential statistics, and despite contacting the original authors the project team was unable to obtain these data for 68% of the experiments. Similarly, none of the 193 experiments were described in sufficient detail for the project team to design protocols to repeat them. And although the original authors were often helpful when asked for such details, they were 'not at all helpful' (or did not respond to the project team) for 32% of the experiments. These problems meant that the early stages of the project took longer than expected and went over budget: the end result was that it was only possible to publish 29 Registered Reports.</p><p>Once experimental work started, two-thirds of the protocols needed to be modified to allow the experiments to be completed. Again this stage of the project took longer and cost more than expected, and in the end the project team was only able to repeat 50 experiments from 23 papers: the results of these experiments are reported in 17 Replication Studies and an aggregate paper (<xref ref-type="bibr" rid="bib11">Errington et al., 2021c</xref>). The clear message to emerge here is that the reporting of both methods and results needs to be improved.</p><p>So how replicable were the 50 experiments that the team managed to repeat? As explained in a meta-analysis that combines the data from all the replications (<xref ref-type="bibr" rid="bib10">Errington et al., 2021b</xref>), there are a number of different answers to this question. One reason for this is that many of the experiments involved measuring more than one effect (such as measuring the influence of an intervention on both the tumor burden and overall survival). Indeed, the 50 experiments involved a total of 158 effects. Moreover, these effects could be positive effects or null effects. Furthermore, some of the original papers reported effects in terms of numerical values, whereas others relied on images.</p><p>The team used seven criteria to assess replicability, although some were not suitable for assessing all effects (e.g., some only worked for positive effects, or when numerical values were available). One criterion compared effect sizes for positive effects: this revealed the median effect size in the replications was 85% smaller than in the original experiments; moreover, the effect size in the replication was smaller than the original in 92% of cases. The other criteria were binary – the replication was either a success or a failure – and five of these could be used for both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40% of replications succeeded according to three or more of these criteria, and this figure increased to 80% for null effects.</p><p>In a separate article, Patrick Kane and Jonathan Kimmelman (who were not part of the RPCB) take a step back and discuss some of the scientific, ethical and policy implications of the project (<xref ref-type="bibr" rid="bib13">Kane and Kimmelman, 2021</xref>). They liken basic and preclinical research in cancer biology to a 'diagnostic machine' that is used to decide which clinical hypotheses should be progressed (including which should go forward to clinical trials). While the results of the RPCB may be 'concerning', Kane and Kimmelman argue that further work is needed to better understand the performance of the diagnostic machine.</p><p>And further work is being done on many fronts. National projects to explore various aspects of reproducibility are under way in several countries, including Brazil (<xref ref-type="bibr" rid="bib2">Amaral et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Amaral and Neves, 2021</xref>), Germany (<xref ref-type="bibr" rid="bib6">BMBF, 2018</xref>) and the Netherlands (<xref ref-type="bibr" rid="bib15">NWO, 2020</xref>). National reproducibility networks have also been set up in <ext-link ext-link-type="uri" xlink:href="https://reproducibilitynetwork.de/">Germany</ext-link> and the <ext-link ext-link-type="uri" xlink:href="https://www.ukrn.org/">UK</ext-link>.</p><p>The aim of the RPCB was not to find papers that were flawed or faulty, and a failure of the team to replicate an experiment does not mean that the original was wrong (and, likewise, a successful replication does not guarantee that the original was correct – both the original and the replication may be wrong). However, the results of the project should give the biomedical research enterprise pause for thought. Journals have encouraged more complete reporting of methods and results in recent years, but there is still scope for improvement, especially when it comes to making data and code openly available. Many studies would benefit from greater input from experts in statistics, ideally before data are collected, and preregistration should help to reduce bias and increase rigor in certain types of studies. Increased preprinting will also help for most papers by increasing both readership and scrutiny, and by making new results available sooner. Lastly, a greater emphasis on science that is rigorous, as opposed to eye-catching, from researchers, institutions, funders and journals would benefit everyone.</p><sec id="s1"><title>Note</title><p>All <italic>eLife</italic> content related to the Reproducibility Project: Cancer Biology is available at: <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology</ext-link>.</p><p>All underlying data, code, and digital materials for the project is available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/collections/rpcb/">https://osf.io/collections/rpcb/</ext-link>.</p></sec></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ack id="ack"><title>Acknowledgements</title><p>The authors thank all the editors and reviewers who were involved in the peer review of RPCB articles.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Academy of Medical Sciences</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Reproducibility and Reliability of Biomedical Research: Improving Research Practice</article-title><ext-link ext-link-type="uri" xlink:href="https://acmedsci.ac.uk/policy/policy-projects/reproducibility-and-reliability-of-biomedical-research">https://acmedsci.ac.uk/policy/policy-projects/reproducibility-and-reliability-of-biomedical-research</ext-link><date-in-citation iso-8601-date="2021-11-16">November 16, 2021</date-in-citation></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amaral</surname><given-names>OB</given-names></name><name><surname>Neves</surname><given-names>K</given-names></name><name><surname>Wasilewska-Sampaio</surname><given-names>AP</given-names></name><name><surname>Carneiro</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Brazilian Reproducibility Initiative</article-title><source>eLife</source><volume>8</volume><elocation-id>e41602</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41602</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amaral</surname><given-names>OB</given-names></name><name><surname>Neves</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Reproducibility: Expect less of the scientific paper</article-title><source>Nature</source><volume>597</volume><fpage>329</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1038/d41586-021-02486-7</pub-id><pub-id pub-id-type="pmid">34526702</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>1,500 scientists lift the lid on reproducibility</article-title><source>Nature</source><volume>533</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1038/533452a</pub-id><pub-id pub-id-type="pmid">27225100</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begley</surname><given-names>CG</given-names></name><name><surname>Ellis</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Raise standards for preclinical cancer research</article-title><source>Nature</source><volume>483</volume><fpage>531</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/483531a</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="web"><person-group person-group-type="author"><collab>BMBF</collab></person-group><year iso-8601-date="2018">2018</year><article-title>Guideline for the Promotion of Confirmatory Preclinical Studies – Quality in Health Research</article-title><ext-link ext-link-type="uri" xlink:href="https://www.gesundheitsforschung-bmbf.de/de/8344.php">https://www.gesundheitsforschung-bmbf.de/de/8344.php</ext-link><date-in-citation iso-8601-date="2021-11-18">November 18, 2021</date-in-citation></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boulbes</surname><given-names>DR</given-names></name><name><surname>Costello</surname><given-names>T</given-names></name><name><surname>Baggerly</surname><given-names>K</given-names></name><name><surname>Fan</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Bhattacharya</surname><given-names>R</given-names></name><name><surname>Ye</surname><given-names>X</given-names></name><name><surname>Ellis</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A survey on data reproducibility and the effect of publication process on the ethical reporting of laboratory research</article-title><source>Clinical Cancer Research</source><volume>24</volume><fpage>3447</fpage><lpage>3455</lpage><pub-id pub-id-type="doi">10.1158/1078-0432.CCR-18-0227</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Forsell</surname><given-names>E</given-names></name><name><surname>Ho</surname><given-names>T-H</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Almenberg</surname><given-names>J</given-names></name><name><surname>Altmejd</surname><given-names>A</given-names></name><name><surname>Chan</surname><given-names>T</given-names></name><name><surname>Heikensten</surname><given-names>E</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Isaksson</surname><given-names>S</given-names></name><name><surname>Nave</surname><given-names>G</given-names></name><name><surname>Pfeiffer</surname><given-names>T</given-names></name><name><surname>Razen</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evaluating replicability of laboratory experiments in economics</article-title><source>Science</source><volume>351</volume><fpage>1433</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0918</pub-id><pub-id pub-id-type="pmid">26940865</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Challenges for assessing replicability in preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e67995</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67995</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Mathur</surname><given-names>MB</given-names></name><name><surname>Soderberg</surname><given-names>CK</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Investigating the replicability of preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e71601</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71601</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Allison</surname><given-names>AB</given-names></name><name><surname>Araiza</surname><given-names>R</given-names></name><name><surname>Aza-Blanc</surname><given-names>P</given-names></name><name><surname>Bower</surname><given-names>LR</given-names></name><name><surname>Campos</surname><given-names>J</given-names></name><name><surname>Chu</surname><given-names>H</given-names></name><name><surname>Denson</surname><given-names>S</given-names></name><name><surname>Donham</surname><given-names>C</given-names></name><name><surname>Harr</surname><given-names>K</given-names></name><name><surname>Haven</surname><given-names>B</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Kwok</surname><given-names>J</given-names></name><name><surname>McDonald</surname><given-names>E</given-names></name><name><surname>Pelech</surname><given-names>S</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Pike</surname><given-names>A</given-names></name><name><surname>Sampey</surname><given-names>D</given-names></name><name><surname>Settles</surname><given-names>M</given-names></name><name><surname>Scott</surname><given-names>DA</given-names></name><name><surname>Sharma</surname><given-names>V</given-names></name><name><surname>Tolentino</surname><given-names>T</given-names></name><name><surname>Trinh</surname><given-names>A</given-names></name><name><surname>Tsui</surname><given-names>R</given-names></name><name><surname>Willis</surname><given-names>B</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Young</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021c</year><article-title>Experiments from unfinished Registered Reports in the Reproducibility Project: Cancer Biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e73430</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.73430</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Rigor Mortis: How Sloppy Science Creates Worthless Cures, Crushes Hope, and Wastes Billions</source><publisher-loc>New York</publisher-loc><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>P</given-names></name><name><surname>Kimmelman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Is preclinical research in cancer biology reproducible enough?</article-title><source>eLife</source><volume>10</volume><elocation-id>e67527</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67527</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><collab>NAS</collab></person-group><year iso-8601-date="2019">2019</year><source>Reproducibility and Replicability in Science</source><publisher-loc>Washington, D.C</publisher-loc><publisher-name>National Academies Press</publisher-name><pub-id pub-id-type="doi">10.17226/25303</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="web"><person-group person-group-type="author"><collab>NWO</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Replication Studies third round: Repetition of important research</article-title><ext-link ext-link-type="uri" xlink:href="https://www.nwo.nl/en/news/replication-studies-third-round-repetition-important-research">https://www.nwo.nl/en/news/replication-studies-third-round-repetition-important-research</ext-link><date-in-citation iso-8601-date="2021-11-18">November 18, 2021</date-in-citation></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname><given-names>F</given-names></name><name><surname>Schlange</surname><given-names>T</given-names></name><name><surname>Asadullah</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Believe it or not: How much can we rely on published data on potential drug targets?</article-title><source>Nature Reviews Drug Discovery</source><volume>10</volume><elocation-id>712</elocation-id><pub-id pub-id-type="doi">10.1038/nrd3439-c1</pub-id><pub-id pub-id-type="pmid">21892149</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Science Fictions: Exposing Fraud, Bias, Negligence and Hype in Science</source><publisher-loc>London</publisher-loc><publisher-name>Bodley Head</publisher-name></element-citation></ref></ref-list></back></article>