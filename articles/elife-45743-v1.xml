<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">45743</article-id><article-id pub-id-type="doi">10.7554/eLife.45743</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Inference of nonlinear receptive field subunits with spike-triggered clustering</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-127402"><name><surname>Shah</surname><given-names>Nishal P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1275-0381</contrib-id><email>bhaishahster@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-131602"><name><surname>Brackbill</surname><given-names>Nora</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0308-1382</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-131603"><name><surname>Rhoades</surname><given-names>Colleen</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-131604"><name><surname>Kling</surname><given-names>Alexandra</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-131605"><name><surname>Goetz</surname><given-names>Georges</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-21116"><name><surname>Litke</surname><given-names>Alan M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3973-3642</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-131606"><name><surname>Sher</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-15874"><name><surname>Simoncelli</surname><given-names>Eero</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1206-527X</contrib-id><email>eero.simoncelli@nyu.edu</email><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-21117"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5613-0248</contrib-id><email>ej@stanford.edu</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Electrical Engineering</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Physics</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Bioengineering</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Neurosurgery</institution><institution>Stanford School of Medicine</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Ophthalmology</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Hansen Experimental Physics Laboratory</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution content-type="dept">Institute for Particle Physics</institution><institution>University of California, Santa Cruz</institution><addr-line><named-content content-type="city">Santa Cruz</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution content-type="dept">Santa Cruz Institute for Particle Physics</institution><institution>University of California, Santa Cruz</institution><addr-line><named-content content-type="city">Santa Cruz</named-content></addr-line><country>United States</country></aff><aff id="aff9"><label>9</label><institution content-type="dept">Center for Neural Science</institution><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff10"><label>10</label><institution>Howard Hughes Medical Institute</institution><addr-line><named-content content-type="city">Chevy Chase</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution>Salk Institute for Biological Studies</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>09</day><month>03</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e45743</elocation-id><history><date date-type="received" iso-8601-date="2019-02-03"><day>03</day><month>02</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-10-29"><day>29</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>© 2020, Shah et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Shah et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-45743-v1.pdf"/><abstract><p>Responses of sensory neurons are often modeled using a weighted combination of rectified linear subunits. Since these subunits often cannot be measured directly, a flexible method is needed to infer their properties from the responses of downstream neurons. We present a method for maximum likelihood estimation of subunits by soft-clustering spike-triggered stimuli, and demonstrate its effectiveness in visual neurons. For parasol retinal ganglion cells in macaque retina, estimated subunits partitioned the receptive field into compact regions, likely representing aggregated bipolar cell inputs. Joint clustering revealed shared subunits between neighboring cells, producing a parsimonious population model. Closed-loop validation, using stimuli lying in the null space of the linear receptive field, revealed stronger nonlinearities in OFF cells than ON cells. Responses to natural images, jittered to emulate fixational eye movements, were accurately predicted by the subunit model. Finally, the generality of the approach was demonstrated in macaque V1 neurons.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>subunits</kwd><kwd>spike triggered analysis</kwd><kwd>LNLN model</kwd><kwd>natural scenes</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF IGERT Grant 0801700</award-id><principal-award-recipient><name><surname>Brackbill</surname><given-names>Nora Jane</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>NIH NEI R01-EY021271</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Simoncelli</surname><given-names>Eero</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000875</institution-id><institution>Pew Charitable Trusts</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Sher</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>NIH NEI F31EY027166</award-id><principal-award-recipient><name><surname>Rhoades</surname><given-names>Colleen</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF GRFP DGE-114747</award-id><principal-award-recipient><name><surname>Rhoades</surname><given-names>Colleen</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>NIH NEI P30-EY019005</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>E J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Nonlinear receptive field subunits in retinal ganglion cells are isolated and characterized by clustering spike-triggered stimuli, and validated on population responses to naturalistic and novel closed loop stimuli.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The functional properties of sensory neurons are often probed by presenting stimuli and then inferring the rules by which inputs from presynaptic neurons are combined to produce receptive field selectivity. The most common simplifying assumption in these models is that the combination rule is <italic>linear</italic>, but it is well-known that many aspects of neural response are highly nonlinear. For example, retinal ganglion cells (RGCs) are known to be driven by nonlinear 'subunits' (<xref ref-type="bibr" rid="bib30">Hochstein and Shapley, 1976</xref>), which reflect signals from presynaptic bipolar cells that are rectified at the synapse onto RGCs (<xref ref-type="bibr" rid="bib16">Demb et al., 1999</xref>; <xref ref-type="bibr" rid="bib17">Demb et al., 2001</xref>; <xref ref-type="bibr" rid="bib6">Borghuis et al., 2013</xref>). This subunit computation is fundamentally nonlinear because hyperpolarization of one bipolar cell input does not cancel depolarization of another. The subunit architecture endows RGCs with sensitivity to finer spatial detail than would arise from a linear receptive field (<xref ref-type="bibr" rid="bib30">Hochstein and Shapley, 1976</xref>; <xref ref-type="bibr" rid="bib17">Demb et al., 2001</xref>;; <xref ref-type="bibr" rid="bib3">Baccus et al., 2008</xref>; <xref ref-type="bibr" rid="bib13">Crook et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">Schwartz et al., 2012</xref>) and has been implicated in the processing of visual features like object motion and looming (<xref ref-type="bibr" rid="bib44">Olveczky et al., 2007</xref>; <xref ref-type="bibr" rid="bib43">Münch et al., 2009</xref>). As another example, complex cells in primary visual cortex are thought to perform subunit computations on simple cell inputs, producing invariance to the spatial location of a stimulus. Indeed, subunits appear to be a common computational motif in the brain, and inferring their functional properties from neural recordings requires the development of effective estimation methods.</p><p>To this end, simple techniques have been used to reveal the presence and typical sizes of subunits (<xref ref-type="bibr" rid="bib30">Hochstein and Shapley, 1976</xref>), and more elaborate techniques have been used in specific experimental settings (<xref ref-type="bibr" rid="bib20">Emerson et al., 1992</xref>; <xref ref-type="bibr" rid="bib45">Paninski, 2003</xref>; <xref ref-type="bibr" rid="bib59">Sharpee et al., 2004</xref>; <xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref>; <xref ref-type="bibr" rid="bib56">Schwartz et al., 2006</xref>; <xref ref-type="bibr" rid="bib49">Pillow and Simoncelli, 2006</xref>; <xref ref-type="bibr" rid="bib50">Rajan and Bialek, 2013</xref>; <xref ref-type="bibr" rid="bib46">Park et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Theis et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Kaardal et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Freeman et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Maheswaranathan et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Shi et al., 2019</xref>). However, no widely accepted general computational tools exist for inferring the structure of nonlinear subunits providing inputs to a neuron, such as their individual sizes, shapes, and spatial arrangement. Such a tool must be robust, efficient, and applicable, possibly with simple modifications, in a variety of different experimental contexts.</p><p>Here, we present a novel method for estimating the properties of subunits providing inputs to a recorded neuron. The method was derived as a maximum likelihood procedure for estimating subunit model parameters and took the form of a spike-triggered clustering algorithm. As such, it provided a natural generalization of spike-triggered averaging, which is widely used to estimate linear receptive fields because of its conceptual simplicity, robustness, and straightforward interpretation. In populations of recorded parasol RGCs in macaque retina, the new method revealed a gradual partitioning of receptive fields with a hierarchical organization of spatially localized and regularly spaced subunits, consistent with the input from a mosaic of bipolar cells, and generalized naturally and parsimoniously to a population of cells. Unlike other approaches, the technique optimized a simple and explicit model of neural response, required no assumptions about the spatio-temporal properties of subunits, and was validated on an extensive data set. The structure of the model permitted prior information, such as spatial locality, to be incorporated naturally in cases of limited data. A novel closed-loop 'null' stimulus revealed that the subunit model was substantially more accurate than a linear model in explaining RGC responses, a finding that also extended to natural stimuli. Responses to null stimuli were stronger in OFF parasol cells than ON parasol cells, consistent with their stronger nonlinear responses (<xref ref-type="bibr" rid="bib17">Demb et al., 2001</xref>; <xref ref-type="bibr" rid="bib10">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib64">Turner and Rieke, 2016</xref>). Application of the estimation method to complex cells in primary visual cortex revealed subunits with expected spatiotemporal structure, demonstrating that the approach generalizes to other experimental contexts and neural circuits.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The goal is to develop a method to estimate the nonlinear subunits of recorded neurons, examine the spatial and temporal properties of the estimated subunits, and verify their predictions for a wide range of stimuli.</p><sec id="s2-1"><title>Subunit response model and parameter estimation</title><p>The method is developed as an optimal estimation procedure for a generic subunit model in which responses are described as an alternating cascade of two linear-nonlinear (LN) stages (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the first LN stage, visual inputs over space and recent time are weighted and summed by linear subunit filters, followed by an exponential nonlinearity. In the second LN stage, the subunit outputs are weighted by non-negative scalars, summed, and passed through a final output nonlinearity. This yields the neuron's firing rate, which drives a Poisson spike generator.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Spiking response model, and estimation through spike-triggered stimulus clustering.</title><p>(<bold>A</bold>) The model is constructed as a cascade of two linear-nonlinear (LN) stages. In the first stage, subunit activations are computed by linearly filtering the stimulus (<inline-formula><mml:math id="inf1"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>) with kernels (<inline-formula><mml:math id="inf2"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>) followed by an exponential nonlinearity. In the second stage, a sum of subunit activations, weighted by (<inline-formula><mml:math id="inf3"><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>), is passed through a saturating output nonlinearity (<inline-formula><mml:math id="inf4"><mml:mi>g</mml:mi></mml:math></inline-formula>), yielding a firing rate that drives an inhomogeneous Poisson spike generator. (<bold>B</bold>) Estimation of subunits in simulated three-subunit model cell. Responses are generated to two-dimensional Gaussian stimuli (iso-probability circular contours shown) with three subunits (magenta lines), generating a spike-triggered stimulus ensemble (colored dots). Weights for different subunits are assigned to each spike triggered stimulus, with colors indicating the relative weights for different subunits. Subunits are then estimated by weighted summation of spike triggered stimulus ensemble (black lines). Soft-clustering of spike triggered stimuli with different number of clusters results in progressive estimation of the underlying subunits. Clustering with correct number of subunits (right panel) results in estimated subunits (black lines) aligned with the true subunits.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Validation of the subunit fitting algorithm on simulated RGC data.</title><p>(<bold>A</bold>) Receptive field obtained via spike-triggered average of a simulated RGC with cascaded linear nonlinear units. The stimulus is temporally filtered by 64 photoreceptors organized on a jittered hexagonal grid. Inputs from 4 to 8 photoreceptors are summed by each of 12 bipolar cells. Bipolar activations are exponentiated and added to produce the firing rate, and spikes are generated by sampling from a Poisson process. Dots indicate photoreceptor locations, with color indicating photoreceptors connecting to a common bipolar cell. See Materials and methods for further details. (<bold>B</bold>) Subunits estimated from simulated RGC responses to coarsely-pixelated white noise reveals fewer subunits (8) than the actual number of bipolar cells (12). The estimated subunits are spatially localized, and partially overlapping (corresponding to weighted aggregates of adjacent underlying bipolar cells), and together cover the receptive field. Inset: Blue bar height indicates the relative strength (average contribution to response over the stimulus ensemble) of each subunit (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in Materials and methods). (<bold>C</bold>) Subunits estimated from simulated RGC responses to a white noise stimulus in which the individual elements of the stimulus target individual cones. The simulation correctly recovers 12 subunits (cross-validated), in which the groups of cones correspond to those that feed each of the simulated bipolar cells. Sizes of dots indicate relative weights of different photoreceptors to the estimated subunits. Inset: Same as B.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig1-figsupp1-v1.tif"/></fig></fig-group><p>To predict neural responses using the model, the model parameters (subunits, weights, and nonlinearity) must be estimated from recorded data. The estimation problem is difficult because the negative log-likelihood is not a convex function of the parameters. An efficient solution is presented with two steps (see Materials and methods): (1) obtain a maximum likelihood estimate of the subunits by identifying clusters in the space of spike-triggered stimuli (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), (2) estimate the parameters of the output nonlinearity using standard optimization methods. This <italic>spike-triggered clustering</italic> solution offers a simple and robust generalization of the commonly used spike-triggered averaging method for receptive field estimation (see <xref ref-type="bibr" rid="bib9">Chichilnisky, 2001</xref>). Intuitively, clustering is effective because stimuli that produce spikes will typically fall into distinct categories based on which subunit(s) they activate. Simulations with a RGC model show that this procedure yields accurate estimates of model parameters when sufficient data are available (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>The remaining model parameter, the number of subunits, is discrete and is therefore typically selected by evaluating model predictions on held-out data (i.e. cross-validation) (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). However, in some cases, it is desirable to either compare the results obtained with different numbers of estimated subunits (in order to study the strongest sources of nonlinearity; <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>, 4, 5–8), or to use a fixed number of subunits (to compare variants of the algorithm; <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Estimated subunit properties.</title><p>(<bold>A</bold>) Subunits, shown as grayscale images, estimated from OFF parasol cell responses to 24 min of white noise. Each pixel was temporally prefiltered with a kernel derived from the spike-triggered average (STA). Rows show estimated spatial subunits for successive values of N. Subunit locations are indicated with ellipses (green for same subunit, red for other subunits from the same fit), corresponding to the contour of a fitted two-dimensional Gaussian with standard deviation equal to the average nearest neighbor separation between subunits. As N increases, each successive set of subunits may be (approximately) described as resulting from splitting one subunit into two (indicated by lines). Large N (e.g. last row) yields some subunits that are noisy or overlap substantially with each other. Height of vertical blue bars indicate the relative strength (average contribution to the cell's firing rate over stimulus ensemble, ignoring the output nonlinearity) of each subunit (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in Materials and methods). Horizontal black bars indicate spatial scale (150μm). (<bold>B</bold>) Log-likelihood as a function of number of subunits (relative to single subunit model) for 91 OFF parasol cells (black) on 3 min of held-out test data, averaged across 10 random initializations of the model, from a distinct randomly sampled training data (24 min from remaining 27 min of data). Population average is shown in blue and the example cell from (<bold>A</bold>) is shown in red. (<bold>C</bold>) Distribution of optimal number of subunits across different cells, as determined by cross-validated log-likelihood on a held-out test set for OFF parasol cells. (<bold>D, E</bold>) Spatial locality of OFF parasol subunits, as measured by mean-squared error of 2D gaussian fits to subunits after normalizing with the maximum weight over space. Control subunits are generated by randomly permuting pixel weights for different subunits within a cell. For this analysis, the optimal number of subunits was chosen for each cell. (<bold>D</bold>) Distribution of MSE values for randomly permuted subunits for the cell shown in (A). MSE of six (optimal N) estimated subunits indicated with black arrows. (<bold>E</bold>) Distribution of quantiles of estimated OFF parasol subunits, relative to the distribution of MSE values for permuted subunits, across all cells and subunits. Null hypothesis has uniform distribution between 0–1. (<bold>F</bold>) Distribution of distances to nearest neighboring subunit within each OFF parasol cell. Distances are normalized by geometric mean of standard deviation of the gaussian fits along the line joining center of subunits. For this analysis, each cell is fit with five subunits (most frequent optimum from (<bold>C</bold>)).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Gradual partitioning of the receptive field into subunits by hierarchical clustering.</title><p>Different number of subunits (rows) estimated by splitting one parent subunit into two subunits at each step. Children subunits estimated by soft-clustering the simulated spikes of the parent subunit, with the simulated spikes for parent subunit computed as the spiking activity of the cell, weighed by its soft-max subunit activation (see Materials and methods). The parent subunit that gives maximum decrease in log-likelihood on training data is chosen for splitting. The choice of training data, preprocessing and figure details same as <xref ref-type="fig" rid="fig2">Figure 2</xref>. The achieved splitting of subunits is similar to the pattern of splitting in <xref ref-type="fig" rid="fig2">Figure 2</xref> for small number of subunits (1–5 subunits), but differs for larger number of subunits (6, 7 subunits). This suggests that enforcing the hierarchical constraint could lead to a more efficient estimation procedure.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig2-figsupp1-v1.tif"/></fig></fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Spatially localized subunit estimation.</title><p>Comparison of different regularizers for estimating subunits using limited data. Examples of OFF parasol cells are shown. (<bold>A</bold>) Five subunits (most frequent optimum across cells from <xref ref-type="fig" rid="fig2">Figure 2</xref>) estimated using all data (1 hr 37 min) for fine resolution white noise without regularization (top row). The first estimated subunit is identical to the full receptive field and the others are dominated by noise. Locally normalized L1 (second row) and L1 (third row) regularization both give spatially localized subunits, with L1 regularization leaving more noisy pixels in background. In both cases, optimal regularization strength was chosen (from 0 to 1.8, steps of 0.1) based on performance on held-out validation data. The contours reveal interdigitation of subunits (red lines). Subunits estimated using white noise with 2.5x coarser resolution (24 min) and no regularization are larger, but at similar locations as subunits with fine resolution (bottom row). Height of vertical blue bars indicate the relative strength (average contribution to response) for each subunit (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in Materials and methods). Scale bar: 75 µm (<bold>B</bold>) For the cell in <xref ref-type="fig" rid="fig1">Figures 1A</xref>, 5 subunits estimated using the 3 min (10% of recorded data) of coarse resolution white noise responses are noisy and non-localized (top row). Similar to the fine case, using locally normalized L1 (second row) and L1 (third row) regularization both give spatially localized subunits, with L1 regularization subunits having noisier background pixels. The regularization strength (between 0 and 2.1, steps of 0.1) was chosen to maximize log-likelihood on held out data (last 5 min of data). Subunits estimated using 24 min (81% of data) of data are spatially localized and partition the receptive field (bottom row). Vertical bars same as (<bold>A</bold>). Scale bar: 150 µm (<bold>C</bold>) Held out log-likelihood for a 5 subunit model estimated from varying durations of training data with L1 (green) and locally normalized L1 (blue) regularization. Results averaged over 91 OFF parasol cells from <xref ref-type="fig" rid="fig1">Figure 1A</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig3-v1.tif"/></fig><p>In what follows, results are presented using the core estimation approach, followed by several modifications appropriate for analyzing data from different experimental conditions. Specifically, consider two situations with data limitations: short recording durations, and biased stimulus statistics. For short durations, the core method is modified to include regularization that assumes localized spatial structure of subunits (see Materials and methods, <xref ref-type="fig" rid="fig3">Figures 3</xref>,<xref ref-type="fig" rid="fig4">4</xref>). For biased stimuli (e.g. natural scenes), subunit filters are estimated using responses from white noise, then the output nonlinearities are fine-tuned (Figures 6,7).</p></sec><sec id="s2-2"><title>Estimated subunits emerge hierarchically and are spatially localized and non-overlapping</title><p>To test the subunit model and estimation procedure on primate RGCs, light responses were obtained using large-scale multielectrode recordings from isolated macaque retina (<xref ref-type="bibr" rid="bib10">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib37">Litke et al., 2004</xref>; <xref ref-type="bibr" rid="bib25">Frechette et al., 2005</xref>). Spiking responses of hundreds of RGCs to a spatiotemporal white noise stimulus were used to classify distinct cell types, by examining the properties of the spike-triggered average (STA) stimulus (<xref ref-type="bibr" rid="bib25">Frechette et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Field et al., 2007</xref>). Complete populations of ON and OFF parasol RGCs covering the recorded region were examined.</p><p>Fitting a subunit model reliably to these data was aided by decoupling the spatial and temporal properties of subunits. Specifically, although the fitting approach in principle permits estimation of full spatiotemporal subunit filters, the high dimensionality of these filters requires substantial data (and thus, long recordings). The dimension of the parameter space was reduced by assuming that subunit responses are spatio-temporally separable, and that all subunits have the same temporal filter (consistent with known properties of retinal bipolar cells that are thought to form the RGC subunits, see <xref ref-type="bibr" rid="bib21">Enroth-Cugell et al., 1983</xref> and <xref ref-type="bibr" rid="bib12">Cowan et al., 2016</xref>). The common temporal filter was estimated from the STA (see Materials and methods). The stimulus was then convolved with this time course to produce an instantaneous spatial stimulus associated with each spike time. This temporally prefiltered spatial stimulus was then used to fit a model with purely spatial subunits.</p><p>The core spike triggered clustering method (no regularization) was applied to study the spatial properties of subunits with increasing numbers of subunits (N). As expected, setting N = 1 yielded a single subunit with a receptive field essentially identical to the STA, i.e. the spatial filter of a LN model. Interestingly, a model with two subunits partitioned this receptive field into spatially localized regions (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, second row). Fitting the model with additional subunits (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, subsequent rows) typically caused one of the subunits from the preceding fit to be partitioned further, while other subunits were largely unchanged. In principle this procedure could be repeated many times to capture all subunits in the receptive field. However, the number of model parameters increases with N, and thus the estimation accuracy decreases, with estimated subunits becoming noisier and exhibiting larger overlap (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, last row). The partitioning observation suggests the possibility of an estimation approach in which the hierarchical introduction of subunits is built in, with the potential for greater efficiency (see Materials and methods). Explicitly incorporating the hierarchical assumption produces a similar collection of subunits at each level (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), reinforcing the idea of subunit partitioning as the number of subunits increases. However, for clarity and brevity, the hierarchical estimation approach is not used for the remainder of this paper.</p><p>An optimal number of subunits was chosen for each cell to maximize the cross-validated likelihood (i.e. the likelihood measured on a held out test set - <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The typical optimum for OFF parasol cells in the recorded data was 4–6 subunits (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This optimal value is governed by the true number of subunits, the resolution of the stimulus (which determines the dimensionality of the parameter space, see <xref ref-type="fig" rid="fig3">Figure 3</xref>), and the amount of data (i.e. number of spikes). Since the ON parasol cells had a smaller optimum number of subunits (one or two subunits provided the optimum fit for 48% of ON parasol cells, while this held for only 3% of OFF parasol cells) and exhibited much smaller increases in likelihood of the subunit model compared to a LN model (not shown), subsequent analysis focuses only on the OFF parasol cells. Note, however, that a model with multiple subunits frequently explained ON parasol data more accurately than a model with one subunit (three or more subunits for 52% of cells), implying that ON parasol cells do have spatial nonlinearities.</p><p>The estimated subunits were larger and fewer in number than expected from the size and number of bipolar cells providing input to parasol RGCs at the eccentricities recorded (<xref ref-type="bibr" rid="bib32">Jacoby et al., 2000</xref>; <xref ref-type="bibr" rid="bib58">Schwartz and Rieke, 2011</xref>; <xref ref-type="bibr" rid="bib62">Tsukamoto and Omi, 2015</xref>). However, two other features of estimated subunits suggested a relationship to the collection of underlying bipolar cells contributing to the receptive field. First, estimated subunits were compact in space as shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. This was quantified by comparing each subunit with the collection of subunits derived by randomly permuting the filter values at each pixel location across different subunits of the same cell. Spatial locality for estimated and permuted subunits was then evaluated by the mean-squared error (MSE) of 2D Gaussian fits. Compared to the permuted subunits, the estimated subunits had substantially lower MSE (<xref ref-type="fig" rid="fig2">Figure 2D,E</xref>). Second, the subunits 'tiled' the RGC receptive field, in that they covered it with minimal gaps and modest overlap. This was quantified by noting that on average, the neighboring subunits for a given cell were separated by ∼ 1.5 times their width, with relatively little variation (<xref ref-type="fig" rid="fig2">Figure 2F</xref>).</p><p>Given these observations, a natural interpretation of subunits observed with coarse stimulation is that they correspond to aggregates of neighboring bipolar cells. To test this intuition, the algorithm was applied to white noise responses generated from a simulated RGC with a realistic size and number of subunits in the receptive field (<xref ref-type="bibr" rid="bib32">Jacoby et al., 2000</xref>; <xref ref-type="bibr" rid="bib58">Schwartz and Rieke, 2011</xref>). For data simulated using coarse pixelated stimuli matched to those used in the experiments, the estimated subunits were fewer and larger than the bipolar cells, with each subunit representing an aggregate of several bipolar cells (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The recovered subunits also exhibited spatial locality and tiling, as in the data.</p></sec><sec id="s2-3"><title>Regularization for spatially localized subunit estimation</title><p>To estimate subunits with a resolution approaching that of the underlying bipolar cell lattice would require much higher resolution stimuli. But higher resolution stimuli typically require more data. Specifically, to obtain a constant quality estimate of subunits as the stimulus resolution/dimensionality is increased requires a proportional increase in the number of spikes (e.g. see <xref ref-type="bibr" rid="bib15">Dayan and Abbott, 2001</xref>). Furthermore, higher resolution stimuli typically produce lower firing rates, because the effective stimulus contrast is lower. To illustrate the problem, the impact of increased resolution and smaller duration of data were examined separately. Higher resolution stimuli led to worse subunit estimates, even when all the recorded data (97 min) were used (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top row). Specifically, of the five estimated subunits, one resembled the receptive field, and the others were apparently dominated by noise and had low weights; thus, the algorithm effectively estimated a LN model. For coarse resolution stimuli, using only 10% of the recorded data led to noisy subunit estimates (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, top row) compared to the estimates obtained with 81% of the data (24 min, <xref ref-type="fig" rid="fig3">Figure 3B</xref>, last row). These observations suggest that it would be useful to modify the core clustering algorithm by incorporating prior knowledge about subunit structure, and thus potentially obtain more accurate estimation with limited data.</p><p>To accomplish this, a regularizer was introduced to encourage the spatial locality of estimated subunits (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Previous studies (<xref ref-type="bibr" rid="bib38">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Maheswaranathan et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">McFarland et al., 2013</xref>) have used L1 regularization (a common means of inducing sparsity), but L1 is agnostic to spatial locality, and indeed is invariant to spatial rearrangement. Thus, a novel locally normalized L1 (LNL1) regularizer was developed that penalizes large weights, but only if all of the neighboring weights are small: <inline-formula><mml:math id="inf5"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mtext>neighbor</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, a value smaller than typical non-zero weights which are on the order of unity. This penalty is designed to have a relatively modest influence on subunit size compared to L1, which induces a strong preference for smaller subunits. The LNL1 penalty was incorporated into the estimation procedure by introducing a proximal operator in the clustering loop (see Materials and methods). In this case, the appropriate projection operator is an extension of the soft-thresholding operator for the L1 regularizer, where the threshold for each pixel depends on the strength of neighboring pixels. The regularization strength is chosen to maximize cross-validated likelihood (see Materials and methods).</p><p>Introducing the LNL1 prior improved subunit estimates for both of the limited data situations (high resolution, or small duration) presented above. Since the optimal number of subunits can vary for different regularization methods, the subunits estimated from different regularization methods were compared after fixing the total number of subunits to the most likely optimum across cells (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In the case of the coarse resolution data, both L1 and LNL1 priors produced spatially localized subunits (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, middle rows) whose locations matched the location of the subunits estimated using more data without regularization (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, last row). This suggests that the proposed prior leads to efficient estimates without introducing significant biases. Relative to L1 regularization, LNL1 regularization yielded fewer apparent noise pixels in the background (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, middle rows) and a larger optimum number of subunits (not shown). This improvement in spatial locality is reflected in more accurate response prediction with LNL1 regularization (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>In the case of high dimensional stimuli (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top row), both L1 and LNL1 priors were successful in estimating compact subunits (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, middle rows) and matched the location of subunits obtained with a coarser stimulus (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom row). The estimated subunits tiled the receptive field, as would be expected from an underlying mosaic of bipolar cells. The LNL1 prior also led to subunit estimates with fewer spurious pixels compared to the L1 prior. Note that although these spurious pixels can also be suppressed by using a larger weight on the L1 prior, this also yielded smaller subunit size, which produced less accurate response predictions, while LNL1 had a much smaller effect on estimated subunit size (not shown). Hence, in both the conditions of limited data examined, the novel LNL1 prior yielded spatially localized subunits, fewer spurious pixels, and a net improvement in response prediction accuracy.</p></sec><sec id="s2-4"><title>Parsimonious modeling of population responses using shared subunits</title><p>To fully understand visual processing in retina it is necessary to understand how a population of RGCs coordinate to encode the visual stimulus in their joint activity. The core spike triggered clustering method extends naturally to joint identification of subunits in populations of neurons. Since neighboring OFF parasol cells have partially overlapping receptive fields (<xref ref-type="bibr" rid="bib27">Gauthier et al., 2009</xref>), and sample a mosaic of bipolar cells, neighboring cells would be expected to receive input from common subunits. Indeed, in mosaics (lattices) of recorded OFF parasol cells, estimated subunits of different cells were frequently closer than the typical nearest-neighbor subunit distances obtained from single-cell data. For example, in one data set, the fraction of subunits from different cells closer than 1SD of a Gaussian fit was 14%, substantially higher than the predicted value of 9% based on the separation of subunits within a cell (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Examination of the mosaic suggests that these closely-spaced subunits correspond to single subunits shared by neighboring RGCs (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), that is, that they reflect a shared biological substrate. If so, then joint estimation of subunits from the pooled data of neighboring RGCs may more correctly reveal their collective spatial structure.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Joint estimation of subunits across multiple nearby cells.</title><p>(<bold>A</bold>) Gaussian fits to the subunits estimated for an entire OFF parasol cell population (5 subunits per cell, with poorly estimated subunits removed). Lines connect center of each cell to its subunits. Subunits which are closer to their nearest neighbor than average (below 15 percentile) are indicated in red. (<bold>B</bold>) Detailed examination of four neighboring cells from the population (color-coded). Number of subunits for each cell is chosen (total 15 subunits) to give the highest summation of log-likelihood (on validation data) for four nearby cells. Gaussian fits to receptive field of the cells (red) and their subunits (gray). Connection strength from a cell (distinct color) to its subunits is indicated by thickness of line. (<bold>C</bold>) A common bank of 12 subunits estimated by jointly fitting responses for all four cells gives similar accuracy as (<bold>B</bold>). Sharing is indicated by subunits connected with lines of different colors (different cells). (<bold>D</bold>) A model with a common bank of 15 subunits (same number as B) gives better performance than estimating the subunits for each cell separately. (<bold>E</bold>) The total negative log-likelihood on test data for these four cells (y-axis) v/s total number of subunits (x-axis) for the two population models with subunits estimated jointly across cells (green) and a combination of separately estimated subunits chosen to maximize total log-likelihood on validation data (blue). Horizontal shift between curves indicates the reduction in total number of subunits by jointly estimating subunits for nearby cells for similar prediction accuracy. The vertical shift indicates better performance by sharing subunits, for a fixed total number of subunits. For both separate and joint fitting, the best value for locally normalized L1 regularization (in 0–3, step size 0.2) for a fixed number of subunits was chosen based on the performance on a separate validation dataset (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig4-v1.tif"/></fig><p>This hypothesis was tested by fitting a common 'bank' of subunits to simultaneously predict the responses for multiple nearby cells. Specifically, the firing rate for the <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>i</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> neuron was modeled as: <inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is the stimulus, <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> are the filters for the common population of subunits, <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the output nonlinearity for the <inline-formula><mml:math id="inf12"><mml:msup><mml:mi>i</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> neuron,and <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the non-negative weight of the <inline-formula><mml:math id="inf14"><mml:msup><mml:mi>n</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> subunit to the <inline-formula><mml:math id="inf15"><mml:msup><mml:mi>i</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> neuron, which is 0 if they are not connected. The model parameters were estimated by maximizing the sum of log-likelihood across cells, again in two steps. For the first step, the output nonlinearities were ignored for each cell and a common set of subunit filters was found that clustered the spike-triggered stimuli for all the cells simultaneously. Specifically, on each iteration, a) the relative weights for different subunits were computed for each spike-triggered stimulus and b) the cluster centers were updated using the weighted sum of spike-triggered stimuli across different cells (see Materials and methods). Since the subunit filters could span the receptive fields of all the cells, leading to higher dimensionality, the spatial locality (LNL1) prior was used to efficiently estimate the subunits (<xref ref-type="fig" rid="fig3">Figure 3</xref>). In the second step of the algorithm, the non-linearities were then estimated for each cell independently (similar to <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>To quantify the advantages of joint subunit estimation, the method was applied to a group of four nearby OFF parasol cells. As a baseline, models with different numbers of subunits were estimated for each cell separately, and the combination of 15 subunits that maximized the cross-validated log-likelihood when summed over all four cells was chosen (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Then, a joint model with 12 subunits was fitted. Similar test log-likelihood was observed using the joint model, which effectively reduced model complexity by three subunits (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, horizontal line). Examination of the subunit locations revealed that pairs of overlapping subunits from neighboring cells in separate fits were replaced with one shared subunit in the joint model (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Also, a joint model with 15 subunits showed a higher prediction accuracy than a model in which the 15 subunits were estimated separately (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, vertical line). In this case, sharing of subunits leads to larger number of subunits per cell (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>This trend was studied for different number of subunits, by comparing the total test negative log-likelihood associated with the joint model (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, green) and the best combination of subunits from separate fits to each cell based on validation data (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, blue). Joint fitting provided more accurate predictions than separate fits, when using a large total number of subunits (&gt;12). For a smaller number of subunits, a horizontal shift in the observed likelihood curves revealed the reduction in the number of subunits that was obtained with joint estimation, without a reduction in prediction accuracy. In sum, jointly estimating a common collection of subunits for nearby cells resulted in a more parsimonious explanation of population responses.</p></sec><sec id="s2-5"><title>Subunit model explains spatial nonlinearities revealed by null stimulus</title><p>To more directly expose the advantage of a subunit model over commonly used LN models of RGC light response, experiments were performed using a visual stimulus that elicits no response in a LN model (or other model that begins with linear integration of the visual stimulus over space, for example <xref ref-type="bibr" rid="bib48">Pillow et al., 2008</xref>). An example of such a stimulus is a contrast-reversing grating stimulus centered on the RF (<xref ref-type="bibr" rid="bib30">Hochstein and Shapley, 1976</xref>; <xref ref-type="bibr" rid="bib16">Demb et al., 1999</xref>). However, to avoid recruiting additional nonlinearities, a stimulus was developed that has spatio-temporal statistics very similar to the high-entropy white noise stimulus used for model characterization. This 'null' stimulus was obtained by manipulating white noise so as to eliminate responses of the linear RF.</p><p>Specifically, the computation performed by a hypothetical LN neuron can be represented in a stimulus space in which each axis represents stimulus contrast at a single pixel location (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In this representation, the LN response is determined by the projection of the stimulus vector onto the direction associated with the weights in the neuron's spatial RF. Thus, a null stimulus orthogonal to the RF can be computed by subtracting a vector with the same RF projection from the original stimulus (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). In addition to the orthogonality constraint, two other constraints are imposed in constructing the stimulus: (i) the range of pixel values is bounded for the stimulus display, and (ii) the variance of each pixel over time is held constant (as in white noise), to prevent gain changes in photoreceptors (see Materials and methods). Given these constraints, the resulting stimulus is the <italic>closest</italic> stimulus to the original white noise stimulus that yields zero response in a LN neuron. Note that although this approach is described purely in the spatial domain, it generalizes to the full spatiotemporal RF, and the procedure based on the spatial RF alone will produce a null stimulus for a spatio-temporal linear filter that is space-time separable (see Materials and methods).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Cells respond to stimulus in null space of receptive field.</title><p>(<bold>A</bold>) Construction of null stimulus, depicted in a two-dimensional stimulus space. Each dimension of stimulus space consists of intensity along a particular pixel. A stimulus frame is represented as a point in this space. Each stimulus frame can be represented geometrically as the sum of the component along the receptive field (RF) direction (the response component of a linear nonlinear model) and the component orthogonal to the RF direction (the null component). (<bold>B</bold>) The null stimulus is constructed by projecting out the RF contribution from each frame of white noise stimulus. (<bold>C</bold>) Rasters representing responses for an OFF parasol (top) and ON parasol (bottom) cell to 30 repeats of 10 s long white noise (red) and the corresponding null stimulus (black). (<bold>D</bold>) Response structure for ON parasol (blue) and OFF parasol (orange) populations for white noise (x-axis) and null stimulus (y-axis) across three preparations (different plots). The response structure was measured as variance of PSTH over time. Variance of PSTH converged with increasing number of trials (not shown), suggesting minimal contribution of inter-trial variability to response structure. Insets: Histogram of relative structure in white noise responses that is preserved in null stimulus for ON parasol (blue) and OFF parasol (orange). Relative structure is measured by ratio of response structure in the null stimulus and response structure in white noise stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig5-v1.tif"/></fig><p>The ability to silence RGC responses with null stimuli was tested as follows. A 10 s movie of white noise was projected into the intersection of the spatial null spaces corresponding to all cells in a region, of a single type (e.g. OFF parasol). Responses were recorded to 30 repeated presentations of both the white noise and the null stimulus. RGC firing rates showed a substantial modulation for both white noise and the null stimulus that was highly reproducible across repeats (<xref ref-type="fig" rid="fig5">Figure 5C,D</xref>), inconsistent with linear integration by the RGC over its RF. Note that the null stimulus modulated OFF parasol cells more strongly than ON parasol cells, consistent with previous results obtained with white noise and natural scene stimuli (<xref ref-type="bibr" rid="bib17">Demb et al., 2001</xref>; <xref ref-type="bibr" rid="bib10">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib64">Turner and Rieke, 2016</xref>); henceforth, analysis is focused on OFF parasol cells.</p><p>The subunit model substantially captured response nonlinearities revealed by the null stimulus. Because the biased statistics of the null stimulus induce errors in the estimated subunits, subunit filters were first estimated from responses to white noise (step 1 of model estimation, without regularization, <xref ref-type="fig" rid="fig1">Figure 1</xref>), and response nonlinearities were then estimated from responses to the null stimulus (step 2 of model estimation, <xref ref-type="fig" rid="fig1">Figure 1</xref>). As expected, the single subunit (LN) model failed to capture responses to the held-out null stimulus (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, second row). As the number of subunits was increased, model performance progressively increased (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, subsequent rows). Prediction accuracy, defined as the correlation between the recorded and predicted firing rate, was evaluated across entire populations of OFF parasol cells in three recordings (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The single subunit model failed, with prediction accuracy near 0 (mean accuracy : 0.02, 0.12, 0.03 for three retinas). The prediction accuracy gradually increased with the addition of more subunits, saturating at roughly 4–6 subunits (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, red). In contrast, the addition of subunits showed marginal improvements for white noise stimuli of the same duration (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, black), despite being measurable with longer stimuli (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Thus, the response nonlinearities captured by the subunit model are more efficiently exposed by the null stimulus.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Subunits improve prediction of responses to null stimuli.</title><p>(<bold>A</bold>) Rasters for recorded responses of an OFF parasol cell to 30 presentations of a 5 s long null stimulus (top row). Predictions of models with increasing (1 to 10) number of subunits (subsequent rows). (<bold>B</bold>) The change in correlation (relative to the correlation obtained with one subunit) between PSTH for recorded and predicted responses with different numbers of subunits across three preparations. Spatial kernels were estimated from 24 min of non-repeated white noise stimulus, with scales and output nonlinearity estimated from the first 5 s of the repeated stimulus. Performance on the last 5 s of the repeated stimulus was averaged over 10 fits, each with a random subsample of the non-repeated white noise stimulus. Individual OFF parasol cells (thin lines) and population average (thick lines) for the null stimulus (red) and white noise (black). The cell in (<bold>A</bold>) is indicated with dashed lines.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig6-v1.tif"/></fig></sec><sec id="s2-6"><title>Subunits enhance response prediction for naturalistic stimuli</title><p>A fuller understanding of the biological role of spatial nonlinearities requires examining how they influence responses elicited by natural stimuli. For this purpose, segments of images from the Van Hateren database were used, with a new image presented every second to emulate saccadic eye movements. Between simulated saccades, the image was jittered according to the eye movement trajectories recorded during fixation by awake macaque monkeys (ZM Hafed and RJ Krauzlis, personal communication, <xref ref-type="bibr" rid="bib65">van Hateren and van der Schaaf, 1998</xref>; <xref ref-type="bibr" rid="bib29">Heitman et al., 2016</xref>). Because the biased statistics of natural stimuli make it infeasible to directly estimate subunits, subunit filters and time courses were first estimated using white noise stimuli (1st step, without regularization <xref ref-type="fig" rid="fig1">Figure 1</xref>). Then, the non-linearities and subunit magnitudes were fitted using natural stimuli (2nd step, <xref ref-type="fig" rid="fig1">Figure 1</xref>) (see Materials and methods, similar to above). Subsequently, the ability of the subunit model to capture responses to natural stimuli was measured.</p><p>Responses to natural stimuli showed a distinct structure, with strong periods of firing or silence immediately following saccades and less frequent firing elicited by image jitter (<xref ref-type="bibr" rid="bib29">Heitman et al., 2016</xref>, <xref ref-type="fig" rid="fig7">Figure 7A</xref> black raster). This structure was progressively better captured by the model as the number of subunits increased, as shown by the rasters for an OFF parasol cell in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. This finding was replicated in OFF parasol populations in three retinas, and quantified by the correlation between the recorded and predicted PSTH (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). However, some of the firing events could not be captured even with a large number of subunits, suggesting that models with additional mechanisms such as gain control (<xref ref-type="bibr" rid="bib28">Heeger, 1992</xref>; <xref ref-type="bibr" rid="bib8">Carandini and Heeger, 2012</xref>) or nonlinear interactions between subunits (<xref ref-type="bibr" rid="bib36">Kuo et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Manookin et al., 2018</xref>) may be required to describe natural scene responses more completely.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Subunits improve response prediction accuracy for naturalistic stimuli.</title><p>(<bold>A</bold>) Top row: Rasters of responses for an OFF parasol cell from 40 presentations for 30 s long naturalistic stimuli. Natural scene images are presented and spatially jittered, with a new image every 1 s (black lines). Subsequent rows indicate model predictions using different number of subunits (1 to 10), respectively. (<bold>B</bold>) Change in correlation (relative to the correlation obtained with one subunit) between PSTH for recorded and predicted responses with different number of subunits across three preparations. Individual OFF parasol cells (thin lines) and population average (thick lines) for two conditions: 250 ms immediately following saccades (black) and inter-saccadic stimuli (red). Cell from (<bold>A</bold>) shown with dashed lines. (<bold>C</bold>) Distribution of angle between vectors representing the spatial receptive field and natural stimuli (temporally filtered using time course from white noise STA) at saccades (black) and between saccades (blue). Angles with inter-saccadic stimulus are more peaked around 90 degrees, with a standard deviation of 12 degrees, compared with 15 degrees for the saccadic stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig7-v1.tif"/></fig><p>Interestingly, further examination revealed that addition of subunits produced greater prediction improvements for responses elicited by image jitter compared to those elicited immediately following simulated saccades (&lt; 250 ms) (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). This could be explained by the fact that changes in luminance are usually more spatially uniform at saccades, resulting in more linear signaling by RGCs. Indeed, after temporally filtering the visual stimulus, there were fewer stimulus samples in the null space of the receptive field (90 degrees) during saccades than between saccades (<xref ref-type="fig" rid="fig7">Figure 7C</xref>), and the subunit model provided a more accurate explanation of responses to stimuli in the null space (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Thus, these results indicate that spatial non-linearities contribute to explaining responses to natural stimuli, especially those elicited by jittering images.</p></sec><sec id="s2-7"><title>Application to simple and complex cells in primary visual cortex</title><p>Identification of subunits using spike-triggered stimulus clustering ideally would generalize to other neural systems with similar cascades of linear and non-linear operations. As a test of this generality, spike triggered clustering (<xref ref-type="fig" rid="fig1">Figure 1</xref>) was applied to obtain estimates of space-time subunit filters from responses of V1 simple and complex cells to flickering bars (<xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref>). The estimated subunits showed oriented spatio-temporal structure similar to that observed with other methods that imposed more restrictions (<xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref>; <xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>) (because of this similarity, additional priors on the filters were not explored). Increasing the number of subunits typically caused one of the subunits from the preceding fits to be partitioned into two, while other subunits were largely unchanged (<xref ref-type="fig" rid="fig8">Figure 8</xref>); that is, a hierarchical structure similar to that observed in RGCs (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The number of cross-validated subunits for the complex cell (N = 8) was greater than for the simple cell (N = 4), consistent with previous work (<xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref>). All subunits for a given cell had similar structure in space and time, but were approximately translated relative to each other, consistent with the similar frequency spectrum (<xref ref-type="fig" rid="fig8">Figure 8A,B</xref>, bottom row). For the complex cell, this is consistent with the hypothesis that it receives inputs from multiple simple cells at different locations (<xref ref-type="bibr" rid="bib31">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>). Thus, the method presented here extends to V1 data, producing subunit estimates that are not constrained by assumptions of orthogonality (<xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref>) or convolutional structure (<xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>), yet broadly resembling the results of previous work (see Discussion and Figure 10 for a detailed comparison).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Application of subunit model to V1.</title><p>(<bold>A</bold>) Hierarchical relationship between subunits estimated using responses to flickering bar stimuli for the complex cell featured in <xref ref-type="bibr" rid="bib54">Rust et al. (2005)</xref>. Rows show estimated spatio-temporal filters for different number of subunits (N). As N increases, each successive set of subunits may be (approximately) described as resulting from splitting one subunit into two (indicated by arrows, see Materials and methods). Largest N represents the optimal number of subunits, determined by cross-validation. Fourier magnitude spectrum of the optimal subunits (bottom row) show translational invariance of estimated subunits. Inset : Relative contribution to cell's firing rate for each subunit indicated by height of blue bar (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in Materials and methods). (<bold>B</bold>) Same as (<bold>A</bold>) for the simple cell from <xref ref-type="bibr" rid="bib54">Rust et al. (2005)</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig8-v1.tif"/></fig></sec><sec id="s2-8"><title>Comparison to other methods</title><p>Comparisons with recent studies reveal advantages of the spike-triggered clustering approach in terms of efficiency of estimation, and interpretation of estimated subunits. A relevant comparison is the recently published subspace-based approach, spike-triggered non-negative matrix factorization (SNMF) (<xref ref-type="bibr" rid="bib38">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Jia et al., 2018</xref>). This method was applied to salamander RGCs, and estimated subunits were shown to be well-matched to independent physiological measurements of bipolar cell receptive fields, an impressive validation. This approach works by collecting the spike triggered stimuli in a matrix and then factorizing the matrix. Because this method differs in a variety of ways from the spike-triggered clustering approach, only the core aspects of the two methods were compared (see Discussion). Both methods were applied without any regularization, and were run without perturbing the parameters after convergence (contrary to the suggestion in <xref ref-type="bibr" rid="bib38">Liu et al., 2017</xref>). The number of subunits used was equal to the true number of subunits in the simulated cell. In these conditions, recovery of subunits revealed two trends. First, for a realistic amount of data, the SNMF approach often produced mixtures of underlying subunits, while the spike-triggered clustering approach produced unique and separate subunits (see <xref ref-type="fig" rid="fig9">Figure 9</xref>). Second, the spatial structure of extracted subunit was substantially more variable using SNMF across different initializations. These results indicate that, at least in these simulated conditions, the spike-triggered clustering method produces more consistent and accurate estimates of subunit spatial structure than SNMF. Note, however, that choices of initialization and regularization can alter the efficiency of both procedures. A summary of other differences with SNMF is presented in Discussion.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Comparison with spike-triggered non-negative matrix factorization.</title><p>(<bold>A</bold>) Spatial filters for five simulated subunits. Responses are generated by linearly projecting white noise stimulus onto these spatial filters, followed by exponential subunit nonlinearity, summation across subunits and saturating output nonlinearity. (<bold>B</bold>) Results from the core spike triggered clustering method (without regularization), applied with N = 5 subunits. Rows indicate fits with different initializations. (<bold>C</bold>) Results from spike-triggered non-negative matrix factorization (SNMF) applied to simulated data. Code provided by <xref ref-type="bibr" rid="bib38">Liu et al. (2017)</xref>. For comparison with to the present method, SNMF applied with no regularization, N = 5 subunits and one perturbation. Similar to (<bold>B</bold>), rows correspond to fits with different initializations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig9-v1.tif"/></fig><p>Subunit behaviors have also been captured using convolutional models that rely on the simplifying assumption that all subunit filters are spatially shifted copies of one another (<xref ref-type="bibr" rid="bib19">Eickenberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Wu et al., 2015</xref>). To perform the comparison, RGC responses were analyzed with a modification of the spike-triggered clustering approach that enforces convolutional subunit structure, with model parameters fitted by gradient descent. The method was applied with locally normalized L1 regularization, shown above to be effective for retinal data (<xref ref-type="fig" rid="fig10">Figure 10A,B</xref>). As expected, the convolutional model was substantially more efficient, exhibiting lower prediction error for limited training data. However, for longer durations of training data (within the range of typical experiments), the unconstrained model exhibited lower error (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). The spatial location and resulting layout of subunits obtained from the two methods were similar (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). However, spike-triggered clustering captured the deviations from a purely convolutional structure, leading to improved performance. Similar results were obtained using spike-triggered clustering without LNL1 regularization (not shown). Similar results were also obtained using data from a V1 complex cell (<xref ref-type="fig" rid="fig10">Figure 10C,D</xref>). Note that the nearly identical power spectra (<xref ref-type="fig" rid="fig8">Figure 8</xref>) of the subunits estimated using spike-triggered clustering suggested nearly convolutional structure, without explicitly enforcing this constraint. Note that further improvement in both approaches could potentially be achieved with modifications. For example in the V1 neurons, replacing the exponential nonlinearity with a quadratic nonlinearity (as in <xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>) provided more accurate fits for both models, but the performance of spike-triggered clustering was still superior when more data were available (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>).</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Comparison with convolutional subunit model.</title><p>Subunits estimated for the retinal ganglion cell from <xref ref-type="fig" rid="fig1">Figure 1A</xref> using a convolutional model, similar to <xref ref-type="bibr" rid="bib66">Vintch et al. (2015)</xref>. The visual stimulus is convolved with a 4 × 4 spatial filter, separately scaled for each location of filter, followed by exponentiation and weighted summation, resulting in Poisson firing rate. Parameters fit using gradient descent. (<bold>A</bold>) Comparison of neural response predictions for convolutional and spike-triggered clustering analysis. Negative log-likelihood loss on 3 min of test data (y-axis) is shown as a function of increasing duration of training data (x-axis), for the convolutional model (blue) and spike triggered clustering (with locally normalized L1 regularization,=0.1, no output nonlinearity, red). The convolutional model is more efficient (better performance with less training data), but spike triggered clustering performs better with more training data. (<bold>B</bold>) Estimated subunits for convolutional model (top row) and spike triggered clustering (bottom row, six subunits after cross-validation) for 24 min of training data. Six filters are presented for the convolutional model, translated at location of six strongest learned weights. Columns indicate overlapping subunits from the two methods, matched greedily based on the inner product of the spatio-temporal filters. (<bold>C</bold>) Similar to (<bold>A</bold>) for the complex cell from <xref ref-type="fig" rid="fig8">Figure 8A</xref>, evaluated on 5 min of test data (y-axis) with a convolutional model that has 8 × 8 spatio-temporal filter (same dimensions as <xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref> and the spike triggered clustering model without regularization. (<bold>D</bold>) Matched subunits (similar to B) for convolutional model and spike triggered clustering (eight filters selected after cross-validation, see <xref ref-type="fig" rid="fig8">Figure 8A</xref>), estimated using 45 min of training data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig10-v1.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>Comparison between spike triggered clustering and convolutional subunit model (<xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>) with quadratic nonlinearity.</title><p>Dashed lines correspond to models with exponential nonlinearity (same as <xref ref-type="fig" rid="fig10">Figure 10C</xref>); solid lines correspond to quadratic nonlinearity. For the convolutional model, the stimulus is passed through a common subunit filter, with location-specific scales and biases, followed by a quadratic nonlinearity. Parameters are estimated using gradient descent. For spike triggered clustering, the subunit filters are first estimated with spike-triggered clustering (assuming an exponential nonlinearity), and then fine-tuned for the quadratic nonlinearity using gradient descent.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig10-figsupp1-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We developed an efficient modeling and estimation approach to study a ubiquitous motif of neural computation in sensory circuits: summation of responses of rectified subunits. The approach allowed efficient subunit estimation by clustering of spike-triggered stimuli, and the inclusion of spatial localization regularizers to additionally constrain the fitting under conditions of limited data. The model and fitting procedure extended naturally to populations of cells, allowing joint fitting of shared subunits. The effectiveness of the method was demonstrated by its ability to capture subunit computations in macaque parasol RGCs, specifically in stimulus conditions in which linear models completely fail, as well in V1 neurons. Below, we discuss the properties of the approach compared to others, and the implications of the application to retinal data.</p><sec id="s3-1"><title>Modeling linear-nonlinear cascades</title><p>The subunit model is a natural generalization of linear-nonlinear (LN) cascade models, and the estimation procedure is a natural generalization of the spike-triggered estimation procedures that facilitated the widespread use of LN models. In the present model framework, a maximum-likelihood estimate of model parameters leads to a spike-triggered clustering algorithm for obtaining the early linear filters that represent model subunits. These linear components are the key, high-dimensional entities that define stimulus selectivity, and, in the absence of a robust computational framework, would be difficult to estimate.</p><p>The closest prior work in retina is spike-triggered non-negative matrix factorization (SNMF) (<xref ref-type="bibr" rid="bib38">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Jia et al., 2018</xref>). In addition to the greater efficiency of spike-triggered clustering (<xref ref-type="fig" rid="fig9">Figure 9</xref>), there are several technical differences worth noting. First, the assumptions of the SNMF algorithm are not directly matched to the structure of the subunit response model: the present model assumes nonnegative subunit outputs, while the SNMF factorization method assumes that the subunit filters are nonnegative. This assumption of nonnegative filters is inconsistent with suppressive surrounds previously reported in retinal bipolar cells (<xref ref-type="bibr" rid="bib14">Dacey et al., 2000</xref>; <xref ref-type="bibr" rid="bib23">Fahey and Burkhardt, 2003</xref>; <xref ref-type="bibr" rid="bib63">Turner et al., 2018</xref>), and limits the application of the method to other systems such as V1 (<xref ref-type="fig" rid="fig8">Figure 8</xref>). In contrast, the spike-triggered clustering method makes no assumptions about the structure of the subunit filers. Second, the SNMF method requires a sequence of additional steps to relate the estimated matrix factors to the parameters of an LNLN model. In contrast, in the present work the clustering of spike-triggered stimuli is shown to be equivalent to optimizing the likelihood of a LNLN model, eliminating extra steps in relating model parameters to biology. Third, the comparison to bipolar cell receptive fields using SNMF relied on a regularization parameter that trades off sparseness and reconstruction error. This regularization can influence the size of the estimated subunits, and thus the comparison to bipolar receptive fields. In contrast, the present method includes a novel regularizer that encourages spatial contiguity of receptive fields, while exerting minimal influence on the size of estimated subunits, and the strength of this regularizer is chosen using a cross-validation procedure.</p><p>Another study (<xref ref-type="bibr" rid="bib26">Freeman et al., 2015</xref>) presented an approach to estimating subunits from OFF midget RGCs with visual inputs presented at single-cone resolution, and was used to obtain a successful cellular resolution dissection of subunits. However, this method assumed that subunits receive inputs from non-overlapping subsets of cones, which is likely to be incorrect for other cell types, including parasol RGCs. Moreover, the cone to subunit assignments were learned using a greedy procedure, which was effective for subunits composed of 1–2 cone inputs, but is more likely to converge to incorrect local minima in more general conditions.</p><p>Recent studies fitted a large class of LNLN models to simulated RGC responses (<xref ref-type="bibr" rid="bib41">McFarland et al., 2013</xref>), to recorded ON-OFF mouse RGC responses (<xref ref-type="bibr" rid="bib60">Shi et al., 2019</xref>), and to flickering bar responses in salamander RGCs (<xref ref-type="bibr" rid="bib39">Maheswaranathan et al., 2018</xref>) using standard gradient descent procedures. These approaches have the advantage of flexibility and simplicity. In contrast, this work assumes a specific subunit nonlinearity that leads to a specialized efficient optimization algorithm, and applies it to recorded data from primate RGCs. The choice of an exponential subunit non-linearity enables likelihood maximization for Gaussian stimuli, which leads to more accurate estimates with limited data, and reduces the computational cost of each iteration during fitting because the loss depends only on the collection of stimuli that elicit a spike (<xref ref-type="bibr" rid="bib51">Ramirez and Paninski, 2014</xref>). Additionally, the soft-clustering algorithm requires far fewer iterations than stochastic gradient descent algorithms used in fitting deep neural networks (<xref ref-type="bibr" rid="bib35">Kingma and Ba, 2014</xref>; <xref ref-type="bibr" rid="bib18">Duchi et al., 2011</xref>). Depending on the experimental conditions (e.g. type of stimuli, duration of recording, number of subunits to be estimated), the present approach may provide more accurate estimates of subunits, with less data, and more quickly.</p><p>Other recent studies applied convolutional and recurrent neural network models, fitted to natural scene responses, to mimic a striking variety of retinal response properties (<xref ref-type="bibr" rid="bib42">McIntosh et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Batty et al., 2016</xref>). The general structure of the model – convolutions and rectifying nonlinearities – resembles the subunit model used here. However, the number of layers and the interactions between channels do not have a direct correspondence to the known retinal architecture, and to date the specific properties of the inferred subunits, and their contributions to the behaviors of the model, have not been elucidated. The complexity of the underlying models suggest that a comparison to the biology may prove difficult.</p><p>In the context of V1 neurons, the present approach has advantages and disadvantages compared to previous methods. Spike-triggered covariance (STC) methods (<xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref>) estimate an orthogonal basis for the stimulus subspace captured by the subunits. However, the underlying subunits are unlikely to be orthogonal and are not guaranteed to align with the basis elements. The method also requires substantial data to obtain accurate estimates. To reduce the data requirements, other methods (<xref ref-type="bibr" rid="bib19">Eickenberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Vintch et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Wu et al., 2015</xref>) imposed an assumption of convolutional structure on the subunits. This approach is efficient, because it effectively fits only one subunit filter that is then applied convolutionally. However, as shown above using recorded data from RGCs and V1 complex cells, the convolutional restriction may not be necessary when enough data are available, and under these conditions, the present method can more accurately reveal subunit structure in receptive fields that is not precisely convolutional (<xref ref-type="fig" rid="fig10">Figure 10</xref>).</p><p>A generalization of the STC approach (<xref ref-type="bibr" rid="bib59">Sharpee et al., 2004</xref>; <xref ref-type="bibr" rid="bib45">Paninski, 2003</xref>; <xref ref-type="bibr" rid="bib19">Eickenberg et al., 2012</xref>) involves finding a subspace of stimulus space which is most informative about a neuron's response. This approach also yields a basis for the space spanned by subunits, but the particular axes of the space may or may not correspond to the subunits. Unlike the method presented here, this approach requires minimal assumptions about stimulus statistics, making it very flexible. However, methods based on information-theoretic measures typically require substantially more data than are available in experimental recordings (<xref ref-type="bibr" rid="bib45">Paninski, 2003</xref>).</p><p>Although constraints on the structure of the subunit model (e.g. convolutional), and the estimation algorithm (e.g. spike-triggered covariance or clustering), both influence the efficiency of subunit estimation, the choice of priors used to estimate the subunits efficiently with limited data (i.e. regularization) is also important. In the present work, a regularizer was developed that imposes spatial contiguity, with minimal impact on the size of estimated subunits. A variety of other regularization schemes have been previously proposed for spatial contiguity for the estimation of V1 subunits (<xref ref-type="bibr" rid="bib47">Park and Pillow, 2011</xref>). These could be incorporated in the estimation of the present model using a corresponding projection operator.</p><p>As suggested by the results, another algorithmic modification that could improve the subunit estimation is the hierarchical variant of the clustering approach (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). By splitting one of the subunits into two subunits at each step, models with different number of subunits can be estimated with greater efficiency, and preliminary exploration indicated that this may offer improvements in data efficiency and speed. However, potential tradeoffs emerging from the additional assumptions will require further study.</p></sec><sec id="s3-2"><title>Revealing the impact of non-linear computations using null stimuli</title><p>The null stimulus methodology revealed the failure of LN models, and isolated the nonlinear behaviors arising from receptive field subunits. Previous studies have developed specialized stimuli to examine different aspects of the cascaded linear-nonlinear model. Contrast reversing gratings (CRGs) have been the most commonly used stimuli for identifying the existence of a spatial nonlinearity (<xref ref-type="bibr" rid="bib30">Hochstein and Shapley, 1976</xref>; <xref ref-type="bibr" rid="bib17">Demb et al., 2001</xref>). Recent studies (<xref ref-type="bibr" rid="bib57">Schwartz et al., 2012</xref>; <xref ref-type="bibr" rid="bib64">Turner and Rieke, 2016</xref>) extended this logic to more naturalistic stimuli, with changes in responses to translations and rotations of texture stimuli revealing the degree of spatial nonlinearity. Another (<xref ref-type="bibr" rid="bib5">Bölinger and Gollisch, 2012</xref>) developed a closed-loop experimental method to measure subunit nonlinearities, by tracing the intensities of oppositely signed stimuli on two halves of the receptive field to measure iso-response curves. More recent work (<xref ref-type="bibr" rid="bib26">Freeman et al., 2015</xref>) developed stimuli in closed loop that targeted pairs of distinct cones in the receptive field, and used these to identify linear and nonlinear summation in RGCs depending on whether the cones provided input to the same or different subunits.</p><p>The null stimulus approach introduced here offers a generalization of the preceding methods. Null stimuli can be generated starting from any desired stimulus ensemble (e.g. natural images), and the method does not assume any particular receptive field structure (e.g. radial symmetry). Construction of the null stimuli does, however, require fitting a linear (or LN) model to the data in closed loop. Projecting a diverse set of stimuli (e.g. white noise) onto the null space yields a stimulus ensemble with rich spatio-temporal structure that generates diverse neural responses. By construction, the null stimulus is the closest stimulus to the original stimulus that is orthogonal to the receptive field, and thus minimally alters other statistical properties of the stimulus that affect neural response. This property is useful for studying the effect of spatial nonlinearities in the context of specific stimuli, such as natural scenes. Whereas contrast reversing gratings do not reveal stronger nonlinearities in OFF parasol cells compared to ON parasol cells in responses with natural stimuli (<xref ref-type="bibr" rid="bib64">Turner and Rieke, 2016</xref>), the null stimulus captures these differences (<xref ref-type="fig" rid="fig6">Figure 6</xref>), highlighting the advantages of tailored stimulation with rich spatio-temporal responses.</p></sec><sec id="s3-3"><title>Further applications and extensions</title><p>The assumptions behind the proposed subunit model enable efficient fitting and interpretability, but also lead to certain limitations that could potentially be overcome. First, the choice of an exponential subunit nonlinearity improves efficiency by allowing the maximum expected likelihood approximation. However, other nonlinearities that allow this approximation, such as a rectified quadratic form (<xref ref-type="bibr" rid="bib5">Bölinger and Gollisch, 2012</xref>), could be explored. Second, in fitting RGCs, the assumption of space-time separable subunits significantly reduces the number of parameters that need to be estimated, but could be relaxed with more data. The space-time separability assumption can be verified by checking if the spatio-temporal STA had rank one. In the present data (not shown), the STA has rank one when only pixels corresponding to the receptive field center were considered but had higher rank when the surround was also taken into consideration (<xref ref-type="bibr" rid="bib22">Enroth-Cugell and Pinto, 1970</xref>; <xref ref-type="bibr" rid="bib12">Cowan et al., 2016</xref>). Thus, replacing the space-time separability assumption may be desirable. For example, a modification to the procedure that enforces subunits with rank two (not shown) could capture the center-surround structure of bipolar cell receptive fields (<xref ref-type="bibr" rid="bib14">Dacey et al., 2000</xref>; <xref ref-type="bibr" rid="bib63">Turner et al., 2018</xref>).</p><p>The methods developed here may also prove useful in tackling additional challenging problems in neural circuitry and nonlinear coding, in the retina and beyond. First, applying the model to high-resolution visual stimulation of the retina could be used to reveal how individual cones connect to bipolar cells, the likely cellular substrate of subunits, in multiple RGC types (see <xref ref-type="bibr" rid="bib26">Freeman et al., 2015</xref>). Second, the incorporation of noise in the model, along with estimation of shared subunits (<xref ref-type="fig" rid="fig4">Figure 4</xref>), could help to probe the origin of noise correlations in the firing of nearby RGCs (see <xref ref-type="bibr" rid="bib53">Rieke and Chichilnisky, 2014</xref>). Modeling the nonlinear interaction between subunits (<xref ref-type="bibr" rid="bib36">Kuo et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Manookin et al., 2018</xref>) could also improve prediction accuracy, especially for natural scenes. Third, extending the subunit model to include gain control, another ubiquitous nonlinear computation in the retina and throughout the visual system, could provide a more complete description of the responses in diverse stimulus conditions (<xref ref-type="bibr" rid="bib28">Heeger, 1992</xref>; <xref ref-type="bibr" rid="bib8">Carandini and Heeger, 2012</xref>). In addition to these potential scientific advances, a more accurate functional subunit model may be critical for the development of artificial retinas for treating blindness. Finally, the successful application of the method to V1 data (<xref ref-type="fig" rid="fig8">Figure 8</xref>) suggests that the method could be effective in capturing computations in other neural circuits that share the nonlinear subunit motif.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type <break/>(species) or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional <break/>information</th></tr></thead><tbody><tr><td>Biological Sample</td><td>Macaque retina</td><td>UC Davis Primate Research Center</td><td/><td/></tr><tr><td>Biological Sample</td><td>Macaque retina</td><td>Stanford University</td><td/><td/></tr><tr><td>Biological Sample</td><td>Macaque retina</td><td>University of California Berkeley</td><td/><td/></tr><tr><td>Biological Sample</td><td>Macaque retina</td><td>Salk Institue</td><td/><td/></tr><tr><td>Biological Sample</td><td>Macaque retina</td><td>The Scripps Research Institute</td><td/><td/></tr><tr><td>Chemical compound, drug</td><td>Ames' medium</td><td>Sigma-Aldrich</td><td>Cat #1420</td><td/></tr><tr><td>Software, algorithm</td><td>MGL</td><td>Gardner Lab</td><td><ext-link ext-link-type="uri" xlink:href="http://gru.stanford.edu/doku.php/mgl/overview">http://gru.stanford.edu/doku.php/mgl/overview</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>MATLAB</td><td>Mathworks</td><td/><td/></tr><tr><td>Software, algorithm</td><td>Python</td><td><ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link></td><td/><td/></tr><tr><td>Software, algorithm</td><td>Intaglio</td><td>Purgatory Design</td><td/><td/></tr><tr><td>Software, algorithm</td><td>Custom spike sorting software</td><td>Chichilnisky Lab</td><td/><td/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Recordings</title><p>Detailed preparation and recording methods are described elsewhere (<xref ref-type="bibr" rid="bib37">Litke et al., 2004</xref>; <xref ref-type="bibr" rid="bib25">Frechette et al., 2005</xref>; <xref ref-type="bibr" rid="bib10">Chichilnisky and Kalmar, 2002</xref>). Briefly, eyes were enucleated from seven terminally anesthetized macaque monkeys (Macaca sp.) used by other experimenters in accordance with institutional guidelines for the care and use of animals. Immediately after enucleation, the anterior portion of the eye and the vitreous were removed in room light. The eye was stored in darkness in oxygenated Ames' solution (Sigma, St Louis, MO) at 33°C, pH 7.4. Segments of isolated or RPE-attached peripheral retina (6–15 mm temporal equivalent eccentricity [<xref ref-type="bibr" rid="bib10">Chichilnisky and Kalmar, 2002</xref>], approximately 3 mm x 3 mm) were placed flat, RGC side down, on a planar array of extracellular microelectrodes. The array consisted of 512 electrodes in an isosceles triangular lattice, with 60 µm inter-electrode spacing in each row, covering a rectangular region measuring 1800 µm x 900 µm. While recording, the retina was perfused with Ames' solution (35°C for isolated recordings and 33°C for RPE-attached recordings), bubbled with 95% O<sub>2</sub>% and 5% CO<sub>2</sub>, pH 7.4. Voltage signals on each electrode were bandpass filtered, amplified, and digitized at 20 kHz.</p><p>A custom spike-sorting algorithm was used to identify spikes from different cells (<xref ref-type="bibr" rid="bib37">Litke et al., 2004</xref>). Briefly, candidate spike events were detected using a threshold on each electrode, and voltage waveforms on the electrode and nearby electrodes in the 5 ms period surrounding the time of the spike were extracted. Candidate neurons were identified by clustering the waveforms using a Gaussian mixture model. Candidate neurons were retained only if the assigned spikes exhibited a 1 ms refractory period and totaled more than 100 in 30 min of recording. Duplicate spike trains were identified by temporal cross-correlation and removed. Manual analysis was used to further select cells with a stable firing rate over the course of the experiment, and with a spatially localized receptive field.</p></sec><sec id="s4-2"><title>Visual stimuli</title><p>Visual stimuli were delivered using the optically reduced image of a CRT monitor refreshing at 120 Hz and focused on the photoreceptor outer segments. The optical path passed through the mostly transparent electrode array and the retina. The relative emission spectrum of each display primary was measured with a spectroradiometer (PR-701, PhotoResearch) after passing through the optical elements between the display and the retina. The total power of each display primary was measured with a calibrated photodiode (UDT Instruments). The mean photoisomerization rates for the L, M, and S cones were estimated by computing the inner product of the display primary power spectra with the spectral sensitivity of each cone type, and multiplying by the effective collecting area of primate cones (0.6 μm<sup>2</sup>) (<xref ref-type="bibr" rid="bib2">Angueyra and Rieke, 2013</xref>; <xref ref-type="bibr" rid="bib55">Schnapf et al., 1990</xref>). During white noise and null stimulus, the mean background illumination level resulted in photoisomerization rates of (800–2200, 800–2200, 400–900) for the (L, M, S) cones. The pixel size was either 41.6 microns (eight monitor pixels on a side) or 20.8 microns (four monitor pixels on a side). New black binary white noise frames were drawn at a rate of 60 Hz (<xref ref-type="fig" rid="fig2">Figure 2</xref>) or 30 Hz (<xref ref-type="fig" rid="fig5">Figures 5</xref>,<xref ref-type="fig" rid="fig6">6</xref>). The pixel contrast (difference between the maximum and minimum intensities divided by the sum) was either 48% or 96% for each display primary, with mean luminance at 50%.</p><p>The details of natural stimuli used are presented in <xref ref-type="bibr" rid="bib29">Heitman et al. (2016)</xref>. Briefly, the stimuli consisted of images from the Van Hateren database, shown for one second each, jittered according to eye movement trajectories recorded during fixation by awake macaque monkeys (Z.M. Hafed and R.J. Krauzlis, personal communication), (<xref ref-type="bibr" rid="bib29">Heitman et al., 2016</xref>; <xref ref-type="bibr" rid="bib65">van Hateren and van der Schaaf, 1998</xref>). Image intensities produced mean photoisomerization rates of (1900, 1800, 700) for the (L, M, S) cones, respectively. The pixel width was 10.4 microns (two monitor pixels on a side), and image frames refreshed at 120 Hz. The training data, consisting of 59 groups of 60 distinct natural scenes, was interleaved with identical repeats of testing data, consisting of 30 distinct natural scenes.</p></sec><sec id="s4-3"><title>Subunit estimation</title><p>The light responses of RGCs were modeled using a cascade of two linear-nonlinear (LN) stages, followed by a Poisson spike generator. Specifically, the instantaneous stimulus-conditional firing rate was modeled as <inline-formula><mml:math id="inf16"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is the visual stimulus at time <inline-formula><mml:math id="inf18"><mml:mi>t</mml:mi></mml:math></inline-formula> (expressed as a vector), <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> are the spatial subunit filters (also vectors), <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> are non-negative weights on different subunits, and <inline-formula><mml:math id="inf21"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output nonlinearity.</p><p>The parameters <inline-formula><mml:math id="inf23"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are estimated by minimizing their negative log-likelihood given observed spiking responses <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> to visual stimuli <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, in two steps. In the first step, the parameters <inline-formula><mml:math id="inf26"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are estimated using a clustering procedure while ignoring the nonlinearity (<inline-formula><mml:math id="inf27"><mml:mi>g</mml:mi></mml:math></inline-formula>). In the second step, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are estimated using gradient descent, with the <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> held fixed (up to a scale factor).</p><p>The clustering procedure is derived by minimizing an approximate convex upper bound of the negative log-likelihood based on current parameter estimates <inline-formula><mml:math id="inf30"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The convex upper-bound is derived as follows:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="21pc"/></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mspace width="2em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="19.5pc"/></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="12.25pc"/></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≤</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo/><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo> <mml:mo/><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo/><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo> <mml:mo/><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf31"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> does not depend on parameters <inline-formula><mml:math id="inf33"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. For simplicity, we present the equations for a standard Gaussian stimulus <inline-formula><mml:math id="inf34"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (modifications for arbitrary Gaussian stimuli are straightforward). Since the first term in the log-likelihood (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) does not depend on recorded responses (<inline-formula><mml:math id="inf35"><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>), it is replaced with its expected value across stimuli (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Because the projections of the input distribution onto filters <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> are assumed to be Gaussian, this expectation can be computed using the moment generating function of a Gaussian distribution (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). The second term only depends on the stimuli for which the cell generated spikes (<inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), reducing computational cost. Finally, to optimize the resulting non-convex function, a convex upper bound was minimized at each step. Specifically, the second term is replaced by a first-order Taylor approximation (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) (the first term is already convex). This upper bound is separable across parameters of different subunits, as can be seen by re-arranging the summation over time and subunits (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). Therefore, the parameters for each subunit can be updated separately by minimizing the corresponding term in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>.</p><p>The parameters for each subunit are optimized in three steps. First, the subunit activations, <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are updated based on the parameters <inline-formula><mml:math id="inf39"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> from the previous iteration. Then, setting to zero the partial derivatives of <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> with respect to <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> produces a system of two equations, which are rearranged to yield the remaining two steps:</p><list list-type="order"><list-item><p>Update subunit activations (cluster weights) for each stimulus : <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Update linear subunit filters (cluster centers) : <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Update relative weights for different subunits : <inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>Hence, applying the three steps repeatedly results in a clustering algorithm that iteratively minimizes a convex upper bound on the approximate negative log-likelihood function. Since the upper bound is tight at the current parameter estimates, the successive minimization of the convex upper bound reduces the approximate negative log-likelihood monotonically, leading to guaranteed convergence. However, due to non-convexity of the approximate negative log-likelihood function, the estimated parameters might not correspond to a global minimum. In the limit of large recording duration, the approximation in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is exact.</p><p>The memory requirement for each step of the algorithm is proportional to the number of spike triggered stimuli, which could be large for fine resolution stimuli and/or high firing rates. Computational runtime depends linearly on the number of spikes, the stimulus dimensions, the number of subunits, and the number of steps for convergence of the clustering procedure. For most of the results presented in this paper, each fit converged in ∼ 100 steps and required a few minutes on a single-core computer. Additional speed improvements would be possible by parallelizing each step of clustering across subunits, making runtime independent of the number of subunits. The resulting subunits can be interpreted as a decomposition of the spike triggered average (STA), since the sum of estimated subunits (<inline-formula><mml:math id="inf45"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>), weighted by their expected contribution to the response (<inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), is proportional to the spike-triggered average:<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(6)</mml:mtext></mml:mtd><mml:mtd><mml:mrow/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(7)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(8)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where (6) comes form (3) (at convergence), (7) comes from (2) (at convergence), and (8) arises because the <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> sum to one. Step (6) may be interpreted as replacing the average spike rate, multiplied by the contribution of each subunit. Step (7) may be interpreted as a weighted average of spike-triggered stimuli, where the weighting assigns responsibility for each spike across the subunits. The above formulae ignore the output non-linearity, justified by the observation that it is approximately linear after fitting.</p></sec><sec id="s4-4"><title>Incorporating priors</title><p>The log-likelihood can be augmented with a regularization term, to incorporate prior knowledge about the structure of subunits, thereby improving estimation in the case of limited data. Regularization is incorporated into the algorithm by projecting the subunit kernels (<inline-formula><mml:math id="inf48"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>) with a proximal operator (<inline-formula><mml:math id="inf49"><mml:msub><mml:mi>P</mml:mi><mml:mi>λ</mml:mi></mml:msub></mml:math></inline-formula>), derived from the regularization function, after each update of the cluster centers (<xref ref-type="fig" rid="fig11">Figure 11</xref>). For a regularization term of the form <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the proximal operator is defined as <inline-formula><mml:math id="inf51"><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mtext>argmin</mml:mtext><mml:mi>K</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Regularization was applied for estimating subunits for both single cell (<xref ref-type="fig" rid="fig3">Figure 3</xref>) as well as population (<xref ref-type="fig" rid="fig4">Figure 4</xref>) data and in each case, the regularization strength <inline-formula><mml:math id="inf52"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> was chosen by cross-validation (see below).</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Iterative fitting of subunits, partitioned into four steps.</title><p>The subunit kernels (<inline-formula><mml:math id="inf53"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>) and weights (<inline-formula><mml:math id="inf54"><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>) are randomly initialized, and used to compute soft cluster assignments (<inline-formula><mml:math id="inf55"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> upper left), followed by cluster centroid computation (<inline-formula><mml:math id="inf56"><mml:msub><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> - upper right), estimation of subunit kernels (<inline-formula><mml:math id="inf57"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> - lower right) and subunit weights (<inline-formula><mml:math id="inf58"><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> - lower left). The summations are only over times when the cell generated a spike (<inline-formula><mml:math id="inf59"><mml:msub><mml:mi>t</mml:mi><mml:mtext>sp</mml:mtext></mml:msub></mml:math></inline-formula>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-fig11-v1.tif"/></fig><p>For <inline-formula><mml:math id="inf60"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>-norm regularization (section 3), <inline-formula><mml:math id="inf61"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf62"><mml:msup><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula> is the value of <inline-formula><mml:math id="inf63"><mml:mi>i</mml:mi></mml:math></inline-formula>th pixel, and the proximal operator is a soft-thresholding operator <inline-formula><mml:math id="inf64"><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For the locally normalized <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>-norm regularization, the proximal operator of the Taylor approximation was used at each step. The Taylor approximation is a weighted <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>-norm with the weight for each pixel determined by the parameter value of the neighboring pixels. Hence, the proximal operator for the <inline-formula><mml:math id="inf67"><mml:mi>i</mml:mi></mml:math></inline-formula>th pixel is <inline-formula><mml:math id="inf68"><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf69"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mtext>Neighbors</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> black (smaller than typical non-zero values in <inline-formula><mml:math id="inf71"><mml:mi>K</mml:mi></mml:math></inline-formula>, which are on the order of unity). Simulations were used to verify that this regularizer induces a preference for spatially contiguous subunits, while being relatively insensitive to subunit area (compared with <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> regularization).</p><p>The entire procedure is summarized in <xref ref-type="fig" rid="fig11">Figure 11</xref>, with simplified notation illustrating the four key steps.</p></sec><sec id="s4-5"><title>Subunit estimation using hierarchical clustering</title><p>As shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, a hierarchical organization of subunits was observed in fits to RGC data, with one subunit broken down into two subunits each time the total number of subunits was increased by one. This hierarchical structure can be enforced explicitly to make the estimation procedure more efficient.</p><p>Since the softmax weights <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> (between 0 and 1) can be interpreted as the ‘activation probability’ of subunit <inline-formula><mml:math id="inf74"><mml:mi>m</mml:mi></mml:math></inline-formula>, hierarchical clustering is performed by estimating two 'child’ subunits <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with factorized activation probability : <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the conditional probability of child <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> given the activation of parent subunit with <inline-formula><mml:math id="inf79"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. The factorization is accurate if the activation of the parent subunit is the sum of activation of the child subunits (<inline-formula><mml:math id="inf80"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>.</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>.</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>The child subunits were initialized by adding independent random noise to parent subunits (<inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and equally dividing the subunit weight into two (<inline-formula><mml:math id="inf82"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:msup><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>). Hierarchical clustering was performed by iteratively updating the cluster assignments (<inline-formula><mml:math id="inf83"><mml:msub><mml:mi>α</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math></inline-formula>) and cluster centers (<inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo rspace="5.3pt">,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>) for the child subunits using the factored <inline-formula><mml:math id="inf85"><mml:mi>α</mml:mi></mml:math></inline-formula> as outlined above.</p><p>The parent subunit for splitting was chosen greedily, to yield maximum increase in log-likelihood at each step. Hierarchical subunit estimation provides a computationally efficient way to estimate the entire solution path with different <inline-formula><mml:math id="inf86"><mml:mi>N</mml:mi></mml:math></inline-formula>. The estimated subunits obtained with this procedure are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></sec><sec id="s4-6"><title>Population model</title><p>In Section 4, a method to estimate a common bank of subunits jointly using multiple nearby cells is described. For each cell <inline-formula><mml:math id="inf87"><mml:mi>c</mml:mi></mml:math></inline-formula>, the firing rate is given by <inline-formula><mml:math id="inf88"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>g</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the cell-specific subunit weights. The model parameters were estimated by maximizing the summation of log-likelihood across cells.</p><p>Similar to the single-cell case, the estimation was performed in two steps: 1) Ignoring the output nonlinearity <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>g</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, estimate <inline-formula><mml:math id="inf91"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> by clustering, and 2 ) Estimate <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>g</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the magnitude of <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> for each cell independently using gradient descent, with the vector direction of <inline-formula><mml:math id="inf95"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> fixed. Clustering in the first step can be interpreted as finding a common set of centroids to cluster the spike-triggered stimuli for multiple cells simultaneously. The following steps are repeated iteratively:</p><list list-type="order"><list-item><p>Update subunit activations (cluster weights) for each stimulus and cell: <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Update linear subunit filters (cluster centers) using population responses: <inline-formula><mml:math id="inf97"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="160%" minsize="160%">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo maxsize="160%" minsize="160%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Update subunit weights <inline-formula><mml:math id="inf98"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for each cell: <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list><p>As with subunit estimation using single-cell responses, prior knowledge about subunit structure can be incorporated with a proximal operator (<inline-formula><mml:math id="inf100"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) in the clustering loop. For RGC data, locally normalized L1 regularization was used (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></sec><sec id="s4-7"><title>Application to neural responses</title><p>The subunit estimation algorithm was applied to a rectangular segment of stimulus covering the receptive field (along with a one stimulus pixel boundary around it) and to the RGC spike counts over time (8.33 ms bin size). The binary white noise stimulus was temporally filtered and the resulting normally distributed stimulus (zero mean) was used for subunit estimation (<inline-formula><mml:math id="inf101"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>). The receptive field was estimated as a contiguous set of pixels black in the STA with magnitude larger than 2.5σ, where σ is the standard deviation of the background noise in the STA. The time course for filtering (125 ms length) was estimated by averaging the time course of significant pixels in the STA. The data were partitioned into three groups: testing data consisted of a contiguous segment of recording (last 10%), and the rest of the recording was randomly partitioned for training (90%) and validation (10%) data.</p><p>Models were fit with different numbers of subunits <inline-formula><mml:math id="inf102"><mml:mi>N</mml:mi></mml:math></inline-formula> (1-12) and regularization values <inline-formula><mml:math id="inf103"><mml:mi>λ</mml:mi></mml:math></inline-formula>. Selection of hyperparameters (<inline-formula><mml:math id="inf104"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mi>λ</mml:mi></mml:math></inline-formula>) was performed by averaging the performance of multiple fits with different training/validation partitions and random initializations (see Figure captions for details). For a given number of subunits, the regularization value was chosen by optimizing the performance on validation data. Similarly, the most accurate model was chosen by optimizing over the number of subunits as well as regularization strength. When data with repeated presentations of the same stimulus were available (e.g. <xref ref-type="fig" rid="fig6">Figures 6</xref>,<xref ref-type="fig" rid="fig7">7</xref> ), the prediction accuracy was evaluated by measuring the correlation between peri-stimulus time histogram (PSTH) of recorded data and the predicted spikes for the same number of repetitions. The PSTH was computed by averaging the discretized responses over repeats and gaussian smoothing with standard deviation 16.67 ms.</p><p>For null stimuli, responses from 30 repeats of a 10 s long stimulus were divided into training (first 5 s) and testing (last 5 s) data. Since the training data were limited, the subunit filters and weights (phase 1) were first estimated from responses to a long white noise stimulus and only the subunit scales and output nonlinearity (phase 2) were estimated using the null stimulus.</p><p>For natural scenes stimuli, the spatial kernel for subunits was first estimated from responses to the white noise stimulus (phase 1), and the scales of subunits and output nonlinearity were then fit using responses to a natural stimulus (phase 2). This procedure was used because subunits estimated directly from natural scenes did not show any improvement in prediction accuracy, and all the subunits were essentially identical to the RF, presumably because of the strong spatial correlations in natural images.</p><p>V1 neurons were fit to previously published responses to flickering bars (see <xref ref-type="bibr" rid="bib54">Rust et al., 2005</xref> for details). In contrast to the retinal data, the two dimensions of the stimulus were time and one dimension of space (orthogonal to the preferred orientation of the cell). The estimation procedure (without regularization) was applied for different numbers of subunits. For successive numbers of subunits (<inline-formula><mml:math id="inf106"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), the hierarchical relationship was evaluated by greedily matching <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> filters across the two fits using the inner product of their spatio-temporal filters and splitting the remaining subunit in the <inline-formula><mml:math id="inf109"><mml:mi>N</mml:mi></mml:math></inline-formula>-subunit fit into the two remaining subunits in the <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-subunit fit. The maximum number of subunits was chosen by cross validation as described above.</p></sec><sec id="s4-8"><title>Simulated RGC model</title><p>To test the estimation procedure in conditions where the answer is known, the procedure was applied to responses generated from a simulated RGC (Appendix I). In the simulation, each RGC received exponentiated inputs from bipolar cells, which in turn linearly summed inputs from photoreceptors. The spatial dimensions of the simulation and the number of cones and bipolar cells were approximately matched to parasol RF centers at the eccentricities of the recorded data (<xref ref-type="bibr" rid="bib58">Schwartz and Rieke, 2011</xref>; <xref ref-type="bibr" rid="bib32">Jacoby et al., 2000</xref>). Below, all the measurements are presented in grid units (g.u.), with 1 g.u. = 1 micron resulting in a biologically plausible simulation. The first layer consisted of 64 photoreceptors arranged on a jittered hexagonal lattice with nearest-neighbor distance 5 g.u., oriented at 60° with respect to the stimulus grid. The location of each cone was independently jittered with a radially symmetric Gaussian with standard deviation black 0.35 g.u. Stimuli were pooled by individual photoreceptors with a spatio-temporally separable filter, with time course derived from a typical parasol cell and a Gaussian spatial filter (standard deviation = black 1 g.u.).</p><p>The second layer of the cascade consisted of 12 model bipolar cells, each summing the inputs from a spatially localized set of photoreceptors, followed by an exponential nonlinearity. The photoreceptors were assigned to individual bipolars by k-means clustering (12 clusters) of their locations. Finally, the RGC firing rate is computed by a positively weighted summation of bipolar activations. The bipolar weights were drawn from a Gaussian distribution with standard deviation equal to 10% of the mean. The choice of photoreceptors and bipolar cell weights give a mean firing rate of ≈19 spikes/s.</p><p>First, the subunit model was fitted to 24 min of responses to a white noise stimulus, such that the receptive field was roughly covered by 6 × 6 stimulus pixels (Figure S1B), as in the recorded experimental data (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Subsequently, the subunit model was fitted to 6 hr of stimuli that excites each cone individually (Figure S1C). In this case, subunits are represented as weights on individual photoreceptors. In a retinal preparation, individual cone activation is possible using a fine resolution white noise such that the locations of individual photoreceptors can be identified (see <xref ref-type="bibr" rid="bib26">Freeman et al., 2015</xref>).</p></sec><sec id="s4-9"><title>Null stimulus</title><p>Null stimuli were constructed to experimentally verify the nonlinear influence of estimated subunits on light response. Below, a general approach is first presented to generate a spatio-temporal null stimulus which can be used for any neural system. Subsequently, it is shown that if the STA is space-time separable, the spatio-temporal null stimulus can be derived with a simpler and faster method that only uses spatial information . Since this separability assumption is approximately accurate for RGCs, the spatial null stimulus is used for the results presented in main paper (<xref ref-type="fig" rid="fig5">Figures 5</xref>,<xref ref-type="fig" rid="fig6">6</xref>).</p><p>The purpose of spatio-temporal nulling is to find the closest stimulus sequence such that convolution with a specific spatio-temporal linear filter gives 0 for all time points. For simplicity, the algorithm is described only for a single cell (the extension to multiple cells is straightforward). Let <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the spatio-temporal linear filter, estimated by computing the STA on white noise responses. The orthogonality constraint for a null stimulus <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is written as:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>τ</mml:mi></mml:munder><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo rspace="8.1pt">,</mml:mo><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The constraints for successive frames are not independent, but they become so when transformed to the temporal frequency domain. Writing <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the Fourier transform of <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> respectively, the constraints may be rewritten as:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo rspace="8.1pt">,</mml:mo><mml:mo>∀</mml:mo><mml:mpadded width="+2.8pt"><mml:mi>ω</mml:mi></mml:mpadded><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mfrac><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℤ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Hence for each temporal frequency, the spatial content can be projected onto the subspace orthogonal to the estimated spatial receptive field. Specifically, for a given temporal frequency <inline-formula><mml:math id="inf117"><mml:mi>ω</mml:mi></mml:math></inline-formula>, given a vectorized stimulus frame <inline-formula><mml:math id="inf118"><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> and a matrix <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>A</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:math></inline-formula> with columns corresponding to the vectorized receptive field, the null frame is obtained by solving the following optimization problem:<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>S</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The solution may be written in closed form:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>ω</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>ω</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the results presented in this paper, binary white noise stimuli (<inline-formula><mml:math id="inf120"><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula>) are projected onto the null space. However, in general, any visual stimulus can be used instead of white noise. The receptive field (linear filter) for each cell was estimated by computing a space-time separable approximation of the spike-triggered average (STA), with the support limited to the pixels with value significantly different from background noise (absolute value of pixel value &gt; 2.5 σ, with σ a robust estimate of standard deviation of background noise).</p><p>In addition to the orthogonality constraint, two additional constraints are imposed: a) each pixel must be in the range of monitor intensities (−0.5 to 0.5) and b) the variance of each pixel over time must be preserved, to avoid the possibility of contrast adaption in photoreceptors (<xref ref-type="bibr" rid="bib11">Clark et al., 2013</xref>, but see <xref ref-type="bibr" rid="bib52">Rieke, 2001</xref>). These two constraints were incorporated using Dykstra’s algorithm (<xref ref-type="bibr" rid="bib7">Boyle and Dykstra, 1986</xref>), which iteratively projects the estimated null stimulus into the subspace corresponding to the individual constraints until convergence. Setting the contrast of initial white noise black (<inline-formula><mml:math id="inf121"><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula>) to a value lower than 100% was necessary to satisfy these additional constraints. Finally, the pixel values of the resulting null stimuli were discretized to 8-bit integers, for display on a monitor with limited dynamic range.</p><p>For a space-time separable (rank one) STA (as is approximately true for RGCs), spatio-temporal nulling gives the same solution as spatial nulling. To see this, assume <inline-formula><mml:math id="inf122"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, a space-time separable linear filter. Hence, the null constraint can be rewritten as<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>τ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>τ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5.6pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In the frequency domain this is written as<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5.6pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="5.3pt">∀</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has limited support in time, it has infinite support in frequency. Hence, <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5.6pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> implying spatial-only nulling <inline-formula><mml:math id="inf126"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5.6pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal Analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Experimentation</p></fn><fn fn-type="con" id="con3"><p>Experimentation</p></fn><fn fn-type="con" id="con4"><p>Experimentation</p></fn><fn fn-type="con" id="con5"><p>Experimentation</p></fn><fn fn-type="con" id="con6"><p>Resources, Software</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Formal Analysis, Validation, Investigation, Methodology, Writing - original draft, Writing - review and editing, Supervision</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Formal Analysis, Validation, Investigation, Methodology, Writing - original draft, Writing - review and editing, Supervision</p></fn><fn fn-type="con" id="con9"><p>Experimentation</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: Eyes were removed from terminally anesthetized macaque monkeys (Macaca mulatta, Macaca fascicularis) used by other laboratories in the course of their experiments, in accordance with the Institutional Animal Care and Use Committee guidelines. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (#28860) of the Stanford University. The protocol was approved by the Administrative Panel on Laboratory Animal Care of the Stanford University (Assurance Number: A3213-01).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-45743-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data has been deposited with Dryad at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.dncjsxkvk">https://doi.org/10.5061/dryad.dncjsxkvk</ext-link>. Code available on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/Chichilnisky-Lab/shah-elife-2020">https://github.com/Chichilnisky-Lab/shah-elife-2020</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/shah-elife-2020">https://github.com/elifesciences-publications/shah-elife-2020</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Brackbill</surname><given-names>NJ</given-names></name><name><surname>Rhoades</surname><given-names>C</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Goetz</surname><given-names>G</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Simoncelli</surname><given-names>E</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Inference of nonlinear receptive field subunits with spike-triggered clustering</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.dncjsxkvk</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname> <given-names>EH</given-names></name><name><surname>Bergen</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America</source><volume>2</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angueyra</surname> <given-names>JM</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Origin and effect of phototransduction noise in primate cone photoreceptors</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1692</fpage><lpage>1700</lpage><pub-id pub-id-type="doi">10.1038/nn.3534</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baccus</surname> <given-names>SA</given-names></name><name><surname>Olveczky</surname> <given-names>BP</given-names></name><name><surname>Manu</surname> <given-names>M</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A retinal circuit that computes object motion</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>6807</fpage><lpage>6817</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4206-07.2008</pub-id><pub-id pub-id-type="pmid">18596156</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Merel</surname> <given-names>J</given-names></name><name><surname>Brackbill</surname> <given-names>NA</given-names></name><name><surname>Heitman</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>A</given-names></name><name><surname>Chichilnisky</surname> <given-names>E</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multilayer recurrent network models of primate retinal ganglion cell responses</article-title><source>Open Review</source><ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=HkEI22jeg">https://openreview.net/forum?id=HkEI22jeg</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bölinger</surname> <given-names>D</given-names></name><name><surname>Gollisch</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Closed-loop measurements of iso-response stimuli reveal dynamic nonlinear stimulus integration in the retina</article-title><source>Neuron</source><volume>73</volume><fpage>333</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.10.039</pub-id><pub-id pub-id-type="pmid">22284187</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghuis</surname> <given-names>BG</given-names></name><name><surname>Marvin</surname> <given-names>JS</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Demb</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Two-Photon imaging of nonlinear glutamate release dynamics at bipolar cell synapses in the mouse retina</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>10972</fpage><lpage>10985</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1241-13.2013</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boyle</surname> <given-names>JP</given-names></name><name><surname>Dykstra</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="1986">1986</year><chapter-title>A method for finding projections onto the intersection of convex sets in hilbert spaces</chapter-title><source>Advances in Order Restricted Statistical Inference</source><publisher-name>Springer</publisher-name><fpage>28</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1007/978-1-4613-9940-7_3</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Kalmar</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Functional asymmetries in ON and OFF ganglion cells of primate retina</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>2737</fpage><lpage>2747</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-07-02737.2002</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname> <given-names>DA</given-names></name><name><surname>Benichou</surname> <given-names>R</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name><name><surname>Azeredo da Silveira</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamical adaptation in photoreceptors</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003289</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003289</pub-id><pub-id pub-id-type="pmid">24244119</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname> <given-names>CS</given-names></name><name><surname>Sabharwal</surname> <given-names>J</given-names></name><name><surname>Wu</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Space-time codependence of retinal ganglion cells can be explained by novel and separable components of their receptive fields</article-title><source>Physiological Reports</source><volume>4</volume><elocation-id>e12952</elocation-id><pub-id pub-id-type="doi">10.14814/phy2.12952</pub-id><pub-id pub-id-type="pmid">27604400</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crook</surname> <given-names>JD</given-names></name><name><surname>Peterson</surname> <given-names>BB</given-names></name><name><surname>Packer</surname> <given-names>OS</given-names></name><name><surname>Robinson</surname> <given-names>FR</given-names></name><name><surname>Troy</surname> <given-names>JB</given-names></name><name><surname>Dacey</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Y-Cell receptive field and collicular projection of parasol ganglion cells in macaque monkey retina</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>11277</fpage><lpage>11291</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2982-08.2008</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname> <given-names>D</given-names></name><name><surname>Packer</surname> <given-names>OS</given-names></name><name><surname>Diller</surname> <given-names>L</given-names></name><name><surname>Brainard</surname> <given-names>D</given-names></name><name><surname>Peterson</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Center surround receptive field structure of cone bipolar cells in primate retina</article-title><source>Vision Research</source><volume>40</volume><fpage>1801</fpage><lpage>1811</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(00)00039-0</pub-id><pub-id pub-id-type="pmid">10837827</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demb</surname> <given-names>JB</given-names></name><name><surname>Haarsma</surname> <given-names>L</given-names></name><name><surname>Freed</surname> <given-names>MA</given-names></name><name><surname>Sterling</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Functional circuitry of the retinal ganglion cell's nonlinear receptive field</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>9756</fpage><lpage>9767</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-22-09756.1999</pub-id><pub-id pub-id-type="pmid">10559385</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demb</surname> <given-names>JB</given-names></name><name><surname>Zaghloul</surname> <given-names>K</given-names></name><name><surname>Haarsma</surname> <given-names>L</given-names></name><name><surname>Sterling</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Bipolar cells contribute to nonlinear spatial summation in the brisk-transient (Y) ganglion cell in mammalian retina</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>7447</fpage><lpage>7454</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-19-07447.2001</pub-id><pub-id pub-id-type="pmid">11567034</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchi</surname> <given-names>J</given-names></name><name><surname>Hazan</surname> <given-names>E</given-names></name><name><surname>Singer</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title><source>Journal of Machine Learning Research </source><volume>12</volume><fpage> 2121</fpage><lpage> 2159</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickenberg</surname> <given-names>M</given-names></name><name><surname>Rowekamp</surname> <given-names>RJ</given-names></name><name><surname>Kouh</surname> <given-names>M</given-names></name><name><surname>Sharpee</surname> <given-names>TO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Characterizing responses of translation-invariant neurons to natural stimuli: maximally informative invariant dimensions</article-title><source>Neural Computation</source><volume>24</volume><fpage>2384</fpage><lpage>2421</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00330</pub-id><pub-id pub-id-type="pmid">22734487</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emerson</surname> <given-names>RC</given-names></name><name><surname>Bergen</surname> <given-names>JR</given-names></name><name><surname>Adelson</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Directionally selective complex cells and the computation of motion energy in cat visual cortex</article-title><source>Vision Research</source><volume>32</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90130-B</pub-id><pub-id pub-id-type="pmid">1574836</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enroth-Cugell</surname> <given-names>C</given-names></name><name><surname>Robson</surname> <given-names>JG</given-names></name><name><surname>Schweitzer-Tong</surname> <given-names>DE</given-names></name><name><surname>Watson</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Spatio-temporal interactions in cat retinal ganglion cells showing linear spatial summation</article-title><source>The Journal of Physiology</source><volume>341</volume><fpage>279</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1983.sp014806</pub-id><pub-id pub-id-type="pmid">6620181</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enroth-Cugell</surname> <given-names>C</given-names></name><name><surname>Pinto</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Algebraic summation of centre and surround inputs to retinal ganglion cells of the cat</article-title><source>Nature</source><volume>226</volume><fpage>458</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1038/226458a0</pub-id><pub-id pub-id-type="pmid">5440052</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahey</surname> <given-names>PK</given-names></name><name><surname>Burkhardt</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Center-surround organization in bipolar cells: symmetry for opposing contrasts</article-title><source>Visual Neuroscience</source><volume>20</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1017/S0952523803201012</pub-id><pub-id pub-id-type="pmid">12699078</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatial properties and functional organization of small bistratified ganglion cells in primate retina</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>13261</fpage><lpage>13272</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3437-07.2007</pub-id><pub-id pub-id-type="pmid">18045920</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frechette</surname> <given-names>ES</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Grivich</surname> <given-names>MI</given-names></name><name><surname>Petrusca</surname> <given-names>D</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fidelity of the ensemble code for visual motion in primate retina</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>119</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1152/jn.01175.2004</pub-id><pub-id pub-id-type="pmid">15625091</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Li</surname> <given-names>PH</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Gunning</surname> <given-names>DE</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping nonlinear receptive field structure in primate retina at single cone resolution</article-title><source>eLife</source><volume>4</volume><elocation-id>e05241</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05241</pub-id><pub-id pub-id-type="pmid">26517879</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Receptive fields in primate retina are coordinated to sample visual space more uniformly</article-title><source>PLOS Biology</source><volume>7</volume><elocation-id>e1000063</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000063</pub-id><pub-id pub-id-type="pmid">19355787</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Normalization of cell responses in cat striate cortex</article-title><source>Visual Neuroscience</source><volume>9</volume><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1017/S0952523800009640</pub-id><pub-id pub-id-type="pmid">1504027</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heitman</surname> <given-names>A</given-names></name><name><surname>Brackbill</surname> <given-names>N</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Sher</surname> <given-names>AML</given-names></name><name><surname>Chichilnisky</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Testing pseudo-linear models of responses to natural scenes in primate retina</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/045336</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Shapley</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Linear and nonlinear spatial subunits in Y cat retinal ganglion cells</article-title><source>The Journal of Physiology</source><volume>262</volume><fpage>265</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1976.sp011595</pub-id><pub-id pub-id-type="pmid">994040</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname> <given-names>RA</given-names></name><name><surname>Wiechmann</surname> <given-names>AF</given-names></name><name><surname>Amara</surname> <given-names>SG</given-names></name><name><surname>Leighton</surname> <given-names>BH</given-names></name><name><surname>Marshak</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Diffuse bipolar cells provide input to OFF parasol ganglion cells in the macaque retina</article-title><source>The Journal of Comparative Neurology</source><volume>416</volume><fpage>6</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(20000103)416:1&lt;6::AID-CNE2&gt;3.0.CO;2-X</pub-id><pub-id pub-id-type="pmid">10578099</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jia</surname> <given-names>S</given-names></name><name><surname>Yu</surname> <given-names>Z</given-names></name><name><surname>Onken</surname> <given-names>A</given-names></name><name><surname>Tian</surname> <given-names>Y</given-names></name><name><surname>Liu</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Characterizing neuronal circuits with spike-triggered non-negative matrix factorization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1808.03958">https://arxiv.org/abs/1808.03958</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaardal</surname> <given-names>J</given-names></name><name><surname>Fitzgerald</surname> <given-names>JD</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Sharpee</surname> <given-names>TO</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Identifying functional bases for multidimensional neural computations</article-title><source>Neural Computation</source><volume>25</volume><fpage>1870</fpage><lpage>1890</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00465</pub-id><pub-id pub-id-type="pmid">23607565</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname> <given-names>SP</given-names></name><name><surname>Schwartz</surname> <given-names>GW</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Nonlinear spatiotemporal integration by electrical and chemical synapses in the retina</article-title><source>Neuron</source><volume>90</volume><fpage>320</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.012</pub-id><pub-id pub-id-type="pmid">27068789</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Bezayiff</surname> <given-names>N</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Cunningham</surname> <given-names>W</given-names></name><name><surname>Dabrowski</surname> <given-names>W</given-names></name><name><surname>Grillo</surname> <given-names>AA</given-names></name><name><surname>Grivich</surname> <given-names>M</given-names></name><name><surname>Grybos</surname> <given-names>P</given-names></name><name><surname>Hottowy</surname> <given-names>P</given-names></name><name><surname>Kachiguine</surname> <given-names>S</given-names></name><name><surname>Kalmar</surname> <given-names>RS</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Petrusca</surname> <given-names>D</given-names></name><name><surname>Rahman</surname> <given-names>M</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What does the eye tell the brain?: development of a system for the large-scale recording of retinal output activity</article-title><source>IEEE Transactions on Nuclear Science</source><volume>51</volume><fpage>1434</fpage><lpage>1440</lpage><pub-id pub-id-type="doi">10.1109/TNS.2004.832706</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>JK</given-names></name><name><surname>Schreyer</surname> <given-names>HM</given-names></name><name><surname>Onken</surname> <given-names>A</given-names></name><name><surname>Rozenblit</surname> <given-names>F</given-names></name><name><surname>Khani</surname> <given-names>MH</given-names></name><name><surname>Krishnamoorthy</surname> <given-names>V</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Gollisch</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inference of neuronal functional circuitry with spike-triggered non-negative matrix factorization</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>149</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00156-9</pub-id><pub-id pub-id-type="pmid">28747662</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname> <given-names>N</given-names></name><name><surname>Kastner</surname> <given-names>DB</given-names></name><name><surname>Baccus</surname> <given-names>SA</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring hidden structure in multilayered neural circuits</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006291</pub-id><pub-id pub-id-type="pmid">30138312</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manookin</surname> <given-names>MB</given-names></name><name><surname>Patterson</surname> <given-names>SS</given-names></name><name><surname>Linehan</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mechanisms mediating motion sensitivity in parasol ganglion cells of the primate retina</article-title><source>Neuron</source><volume>97</volume><fpage>1327</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.02.006</pub-id><pub-id pub-id-type="pmid">29503188</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McFarland</surname> <given-names>JM</given-names></name><name><surname>Cui</surname> <given-names>Y</given-names></name><name><surname>Butts</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003143</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003143</pub-id><pub-id pub-id-type="pmid">23874185</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McIntosh</surname> <given-names>L</given-names></name><name><surname>Maheswaranathan</surname> <given-names>N</given-names></name><name><surname>Nayebi</surname> <given-names>A</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Baccus</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning models of the retinal response to natural scenes</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1369</fpage><lpage>1377</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/6388-deep-learning-models-of-the-retinal-response-to-natural-scenes">https://papers.nips.cc/paper/6388-deep-learning-models-of-the-retinal-response-to-natural-scenes</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Münch</surname> <given-names>TA</given-names></name><name><surname>da Silveira</surname> <given-names>RA</given-names></name><name><surname>Siegert</surname> <given-names>S</given-names></name><name><surname>Viney</surname> <given-names>TJ</given-names></name><name><surname>Awatramani</surname> <given-names>GB</given-names></name><name><surname>Roska</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Approach sensitivity in the retina processed by a multifunctional neural circuit</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1308</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1038/nn.2389</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olveczky</surname> <given-names>BP</given-names></name><name><surname>Baccus</surname> <given-names>SA</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Retinal adaptation to object motion</article-title><source>Neuron</source><volume>56</volume><fpage>689</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.030</pub-id><pub-id pub-id-type="pmid">18031685</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Convergence properties of some spike-triggered analysis techniques</article-title><conf-name>Advances in Neural Information Processing Systems.</conf-name><fpage>189</fpage><lpage>196</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/2207-convergence-properties-of-some-spike-triggered-analysis-techniques">https://papers.nips.cc/paper/2207-convergence-properties-of-some-spike-triggered-analysis-techniques</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>IM</given-names></name><name><surname>Archer</surname> <given-names>EW</given-names></name><name><surname>Priebe</surname> <given-names>N</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spectral methods for neural characterization using generalized quadratic models.</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>2454</fpage><lpage>2462</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models">https://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>M</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Receptive field inference with localized priors</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002219</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002219</pub-id><pub-id pub-id-type="pmid">22046110</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dimensionality reduction in neural models: an information-theoretic generalization of spike-triggered average and covariance analysis</article-title><source>Journal of Vision</source><volume>6</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/6.4.9</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Maximally informative &quot;stimulus energies&quot; in the analysis of neural responses to natural signals</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e71959</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0071959</pub-id><pub-id pub-id-type="pmid">24250780</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramirez</surname> <given-names>AD</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fast inference in generalized linear models via expected log-likelihoods</article-title><source>Journal of Computational Neuroscience</source><volume>36</volume><fpage>215</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1007/s10827-013-0466-4</pub-id><pub-id pub-id-type="pmid">23832289</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporal contrast adaptation in salamander bipolar cells</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>9445</fpage><lpage>9454</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-23-09445.2001</pub-id><pub-id pub-id-type="pmid">11717378</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rieke</surname> <given-names>F</given-names></name><name><surname>Chichilnisky</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Correlated activity in the retina</chapter-title><source>The New Visual Neurosciences</source><volume>12</volume><publisher-name>MIT Press</publisher-name><fpage>145</fpage><lpage>162</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname> <given-names>NC</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title><source>Neuron</source><volume>46</volume><fpage>945</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.021</pub-id><pub-id pub-id-type="pmid">15953422</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnapf</surname> <given-names>JL</given-names></name><name><surname>Nunn</surname> <given-names>BJ</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name><name><surname>Baylor</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Visual transduction in cones of the monkey Macaca fascicularis</article-title><source>The Journal of Physiology</source><volume>427</volume><fpage>681</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1990.sp018193</pub-id><pub-id pub-id-type="pmid">2100987</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-triggered neural characterization</article-title><source>Journal of Vision</source><volume>6</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/6.4.13</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>GW</given-names></name><name><surname>Okawa</surname> <given-names>H</given-names></name><name><surname>Dunn</surname> <given-names>FA</given-names></name><name><surname>Morgan</surname> <given-names>JL</given-names></name><name><surname>Kerschensteiner</surname> <given-names>D</given-names></name><name><surname>Wong</surname> <given-names>RO</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The spatial structure of a nonlinear receptive field</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1572</fpage><lpage>1580</lpage><pub-id pub-id-type="doi">10.1038/nn.3225</pub-id><pub-id pub-id-type="pmid">23001060</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>G</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nonlinear spatial encoding by retinal ganglion cells: when 1 + 1 ≠ 2</article-title><source>The Journal of General Physiology</source><volume>138</volume><fpage>283</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1085/jgp.201110629</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname> <given-names>T</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title><source>Neural Computation</source><volume>16</volume><fpage>223</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1162/089976604322742010</pub-id><pub-id pub-id-type="pmid">15006095</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname> <given-names>Q</given-names></name><name><surname>Gupta</surname> <given-names>P</given-names></name><name><surname>Boukhvalova</surname> <given-names>AK</given-names></name><name><surname>Singer</surname> <given-names>JH</given-names></name><name><surname>Butts</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional characterization of retinal ganglion cells using tailored nonlinear modeling</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>8713</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-45048-8</pub-id><pub-id pub-id-type="pmid">31213620</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theis</surname> <given-names>L</given-names></name><name><surname>Chagas</surname> <given-names>AM</given-names></name><name><surname>Arnstein</surname> <given-names>D</given-names></name><name><surname>Schwarz</surname> <given-names>C</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Beyond GLMs: a generative mixture modeling approach to neural system identification</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003356</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003356</pub-id><pub-id pub-id-type="pmid">24278006</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsukamoto</surname> <given-names>Y</given-names></name><name><surname>Omi</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>OFF bipolar cells in Macaque retina: type-specific connectivity in the outer and inner synaptic layers</article-title><source>Frontiers in Neuroanatomy</source><volume>9</volume><elocation-id>122</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2015.00122</pub-id><pub-id pub-id-type="pmid">26500507</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>MH</given-names></name><name><surname>Schwartz</surname> <given-names>GW</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Receptive field center-surround interactions mediate context-dependent spatial contrast encoding in the retina</article-title><source>eLife</source><volume>7</volume><elocation-id>e38841</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38841</pub-id><pub-id pub-id-type="pmid">30188320</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>MH</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Synaptic rectification controls nonlinear spatial integration of natural visual inputs</article-title><source>Neuron</source><volume>90</volume><fpage>1257</fpage><lpage>1271</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.006</pub-id><pub-id pub-id-type="pmid">27263968</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname> <given-names>JH</given-names></name><name><surname>van der Schaaf</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B: Biological Sciences</source><volume>265</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0303</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vintch</surname> <given-names>B</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A convolutional subunit model for neuronal responses in macaque V1</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14829</fpage><lpage>14841</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2815-13.2015</pub-id><pub-id pub-id-type="pmid">26538653</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>A</given-names></name><name><surname>Park</surname> <given-names>IM</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Convolutional spike-triggered covariance analysis for neural subunit models</article-title><conf-name>Advances in Neural Information Processing Systems.</conf-name><fpage>793</fpage><lpage>801</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/5962-convolutional-spike-triggered-covariance-analysis-for-neural-subunit-models">https://papers.nips.cc/paper/5962-convolutional-spike-triggered-covariance-analysis-for-neural-subunit-models</ext-link></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.45743.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution>Salk Institute for Biological Studies</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Inference of nonlinear spatial subunits in primate retina with Spike-Triggered Clustering&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Joshua Gold as the Senior Editor, a Reviewing Editor, and three reviewers. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This manuscript introduces a new method for the estimation of nonlinear spatial subunits in neural data using an LNLN model (a cascade of two LN stages). The paper has clear merits by showing that it is beneficial to utilize subunit models for predicting parasol RGC responses and V1 neural responses to naturalistic stimuli. Parasol spike data has been previously mostly analyzed without taking this into account.</p><p>The main concern to address is that for a &quot;tools and resources&quot; papers needs to include clear benchmarking of the advantages of the current method against other existing and/or published approaches for subunit identification. The consensus among reviewers was that one would need to include comparison with at least one other method for both the retina and the V1 dataset. Some of the relevant methods that you could use are provided below, which offer different trade-offs between accuracy and ease of implementation and the amount of background work needed to set the hyperparameters. It would be helpful if the authors could include a discussion to justify their choice of the alternative method. The alternative method might be different for the V1 and the retina dataset. Including a more detailed discussion of the available methods and the authors' perspective on which would work best at each circumstance or generalize across the light/stimulus conditions would be very helpful to the field.</p><p>The second concern was about the apparent inconsistencies in how the steps of the proposed algorithm were chosen (as detailed below) and that more methodological details need to be provided.</p><p>Comparison with existing methods:</p><p>While the manuscript includes some discussion of the existing methods, it is still not clear which model is the best taken into account that the subunits of the current model do not correlate with the actual bipolar cell receptive fields but rather with their clusters in some way. For the reader to take up this method, one would need to clearly see the pros and cons in speed, resolution, easiness of implementation, neural correlates of subunits and/or other key features of this model against other nonlinear subunit models (e.g. Liu et al., 2017 and the simpler earlier version by Kaardal et al., 2013) and/or simple convolution neural network models (Eickenberg et al., 2013, Vintch et al., 2015). It is critical for the reader to be able to assess the value of the current model. Assuming this is done properly, and these clarifications are added, this paper would be significantly improved.</p><p>One would like to understand and document clearly performance of the subunit-identification method across stimulus conditions (white noise vs. naturalistic stimuli) against conventional currently used methods. This could perhaps be done by applying the new model to existing datasets of parasol RGCs in different experimental conditions. If such data are not available, then perhaps the method could be tested using simulated responses constructed to mimic properties of the retinal neurons.</p><p>Questions about the proposed methodology:</p><p>There are some inconsistencies in the design of the proposed methodology. For examples, the method is motivated with an example that involves hierarchical splitting of subunits, but this hierarchical version is actually not used further on. Then the bare method (without regularization and fairly coarse stimuli) is used to analyze populations of OFF parasol cells (Figure 2B-F), but Figure 3 shows that much better subunits are obtained with regularization and finer white noise. One wonders why this improved method wasn't used to obtain, for example, the distribution of subunit numbers. Further analyses are then again (partly) performed without regularization (e.g. joint fitting of multiple cells, V1 data). In particular when discussing the V1 data, where the subunits have very different structure than in the retina, one wonders whether regularization still works in the same way. Also, it is emphasized how the optimal number of subunits can be determined, but some of the subsequent analyses are performed for a fixed number of subunits (e.g. Figure 2 and Figure 3).</p><p>There are also several other parts where that information is difficult to extract or missing, which hinders understanding of the material.</p><p>The description of the subunit estimation approach via approximate maximum likelihood (subsection “Subunit estimation”) is central to the manuscript but quite difficult to follow. Some terms in the equation are not defined (K and w with superscript 0, c(K,w)), and the explanations for the derivations are cursory or absent. For example, when Equations 1-5 are derived, there is no mentioning of an iterative procedure (this only comes later; at this point, this seems to be only a reformulation of the log-likelihood), but some manipulations and terms here only make sense in the context of an iterative estimation procedure. This needs to be disentangled.</p><p>Relatedly, it seems important to spell out how the clustering algorithm (Equations 6-8) solves the maximum likelihood problem, in particular since this connection is central to the manuscript.</p><p>The application of the method to V1 data makes an important point regarding the general applicability of the method. This is nice, but only very little analysis is presented. Maybe the authors could at least show how the model performs for different numbers of subunits as in other analyses, to see how well the simple cell and the complex cell are separated in this respect. Also, the effect of regularization would be interesting here.</p><p>This assumption of space-time separability needs further discussion and justification. This assumption might be necessary to reduce the number of parameters to manageable levels, but it would be helpful if the authors could offer guidelines for when it might be appropriate.</p><p>About the response prediction to naturalistic stimuli. The use of white noise stimuli rather than natural stimuli might provide a good approximation for estimating of subunit filters, However, bipolar cells, which provide inputs to parasol RGCs, are strongly affected by stimulus correlation through gap-junction connectivity generating a nonlinear enhancement of bipolar activity, which could explain the differences found with real activity (Kuo et al., 2016, Manookin et al., 2018). These elements should be included in the Discussion section as one of the potential pitfalls of the approach.</p><p>The null-stimulus description indicated in the Results section needs further discussion and justification. This part should be rewritten extracting elements present in the methods. Also, the results are confusing. According to what is presented in Figure 5, the PSTH variance obtained from the white-noise response is larger than the variance of null-stimulus response, and at some extent, this variance would be reflecting inter-trial variability. At this level, this figure leads to confusion because it is not helping to understand the methodology. What is the role of the null-stimulus in this method paper? The method seems to predict the null-stimulus response better, but again, it is not clear how this clarifies the methodology neither provides insights to understand this behavior. It is counter-intuitive the fact that adding more units in the model subunits the response prediction for white-noise stimulus is not improving (Figure 6).</p><p>More detailed comments:</p><p>The manuscript was first submitted as original research before the submission as a method paper where essential changes in the structure and content were incorporated. Nevertheless, there is still information more related to retina physiology than the methodology description. Besides, if this is a method valid both for retina and V1 neurons, it is important to mention this in the title, where only the retina application is indicated. Therefore, please consider revising the article title.</p><p>In the Introduction, the authors claim there is no widely accepted general computation to infer the structure of nonlinear subunits inputs to a neuron. Nevertheless, relevant references have been published in the last years addressing the same problem noted by the authors. For instance, Liu et al., 2017, which appears in the discussion but not in the introduction, proposes a spike-triggered non-negative matrix factorization, to estimate the non-linear subunits conforming the RGC receptive field in salamander retina, where bipolar cells conforming RGCs can be also encountered.</p><p>It would be interesting to obtain and include some information about computational runtime.</p><p>The presentation of the analysis of the simulated ganglion cell under fine white noise stimulation (Figure 1—figure supplement 1C) is odd. Why are the subunits shown as sets of photoreceptors, not as pixel layouts? Does the method yield 12 subunits as the optimal number, or is this just the result of what came out when enforcing 12 subunits? A better discussion of how the method performs for simulated data could help better understand the strengths and limitations of the approach.</p><p>The figures show the &quot;relative strength&quot; of subunits by a blue scale bar. Is this the weight w? Or the norm of the subunit filter? Both seem to be related to &quot;strength&quot;. And both seem to be optimized in the second step of the fitting approach along with the output nonlinearity.</p><p>Subsection “Estimated subunits are spatially localized and non-overlapping: The text refers to the Materials and methods section for explaining how the temporal filter was obtained, but the Materials and methods section seem to lack information about receptive field and temporal filter estimation.</p><p>Subsection “Regularization for spatially localized subunit estimation”: The chosen value of epsilon in the LNL1 regularization should somehow be related to the typical range of weights. Are the latter of order unity so that a value of epsilon=0.01 is small compared to typical weights?</p><p>Figure 4E: It seems odd that the negative log-likelihood keeps decreasing with the number of subunits in the joint fitting procedure. Unlike for the separate fitting. Is there a reason for this? Relatedly: how was the total number of subunits constrained for the separate fitting?</p><p>Subsection “Regularization for spatially localized subunit estimation”: The text states that a joint model with 15 subunits gives higher prediction accuracy, referring to Figure 4D. Does this relate to predictions as analyzed in Figure 5? Or to the maximum likelihood of Figure 4E?</p><p>Subsection “Subunit model explains spatial nonlinearities revealed by null stimulus”: The text refers to &quot;spatial null spaces&quot;, but the Materials and methods section relates to spatiotemporal null spaces. Please clarify.</p><p>Subsection “Subunit model explains spatial nonlinearities revealed by null stimulus”, fourth paragraph: Maybe I missed it, but I wasn't sure which version of the subunit estimation is meant by &quot;the subunit model&quot;. With or without regularization? Fine or coarse stimuli? A specific instance of the different initial conditions? Same for Figure 7.</p><p>Figure 7: The figure and the text around it use the term saccade, but the stimulus seems just to be a switch to a new image without a real motion segment between fixations. This is a bit confusing. Also, how the stimulus is segmented into periods around and between image switches should be better explained and not only mentioned in the legend, in particular how this is used to compute the projection on the receptive field (Figure 7C; is this a spatiotemporal projection?). Furthermore, the difference in Figure 7C seems quite subtle. Is it significant?</p><p>In the Materials and methods section, maybe state that the spatiotemporal stimulus was binary white noise (not Gaussian) if this is the case and that the X<sub>t</sub> signal that goes into the subunit estimation is the contrast signal (zero mean).</p><p>Could the authors explain why the weights and the filter magnitudes need to be refitted in the second model fitting step?</p><p>Subsection “Application to neural responses”: What does the 5.2 µm boundary refer to?</p><p>For the simulated RGC model, the 180 µm (= 180 g.u.) spacing between cones seems large. Is that a typo? Also, in the description, it is not quite clear what the photoreceptor weights are (used for normalizing the firing rate) and how bipolar weights were varied around 10% of their mean (which mean?).</p><p>When describing the null stimulus, it would be good to explain where the original stimulus S<sub>w</sub> comes from and how the σ for computing the support of the receptive field is obtained.</p><p>The references Maherwaranathan et al., 2018, and Shi et al., 2018 are missing.</p><p>Figures are in general of poor quality and the color choice misleading. In Figure 2D is not possible to see the black arrows over the blue bars. In Figure 2B blue line is not well visualized.</p><p>Figure 5D, use the same bar width for the inset histograms.</p><p>&quot;… was highly reproducible across repeats (Figure 6C,D),&quot; -&gt; &quot;… was highly reproducible across repeats (Figure 5C,D),&quot;</p><p>Figure 6B: dashed are filled lines are not indicated in the caption.</p><p>Subsection &quot;Application to neural response&quot;. The last three paragraphs should be incorporated into the Results section.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Inference of Nonlinear Receptive Field Subunits with Spike-Triggered Clustering&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Joshua Gold (Senior Editor), a Reviewing Editor, and three reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The comparison to the convolutional subunit model is not completely convincing for the following reasons: (1) The comparison done on V1 data uses a restricted version of the convolutional subunit model (the subunit nonlinearity is forced to be an exponential, despite that Vintch et al., (2015) and others have showed that it is often quadratic when learned from data). (2) No comparison is made on the retinal dataset where the fixed exponential subunit nonlinearity probably is a more realistic assumption. The reader is thus left wondering whether the new method truly has better predictive performance than the simpler convolutional subunit model, and whether the main benefit with the new method is that it can fit multiple subunit filters easily. It would be beneficial at least to clarify these concerns by giving a clearer justification to the selected methodology regarding the above issues.</p><p>A description for how/why the clustering algorithm's three step procedure solves the likelihood problem is still missing. It would be helpful if the authors could briefly spell out why the three-step prescription subsection “Subunit Estimation” “minimizes the upper bound of the log-likelihood (Equation 5). Are the update rules derived from the partial derivatives of Equation 5? Is this guaranteed to converge and converge to the actual minimum?</p><p>Regarding the validation in V1 data: More than a validation, where there is no ground-truth to compare, the results should be focused to validate the model showing that it reproduces neural data better than previous approaches. For instance, Figure 10 compares the resulting subunits obtained by a convolutional model and the model proposed in this article. It is tough to compare both; it should be more useful to see how the two of them reproduce neural response.</p><p>The Discussion section requires a new organization. The comparison with the existing models, such as SNMF and V1 data, shouldn't be placed here. The paragraph describing the work of McIntosh et al., 2016 can be better articulated with the rest of the Discussion section.</p><p>- For instance, the sentence &quot;Note, however, that a model with two subunits frequently explained ON parasol data more accurately than a model with one subunit, implying that ON cells do have spatial nonlinearities&quot;. This is not related to the method validation, and moreover, there is no result to validate this affirmation.</p><p>- Subsection “Modeling linear-nonlinear cascades” states that the paper by Jia et al., (2018) applies a non-negativity constraint on the weights instead of the subunit filters. Looking at their methods and sample subunits in the figures, this doesn't seem to be the case.</p><p>- For the receptive field estimation in subsection “Application to neural responses”, the pixels with magnitude larger than 2.5 σ probably refer to magnitude **of the STA**; similarly, averaging the time course of pixels probably refers to averaging the pixels **in the STA**.</p><p>Figure 2 is still hard for me to follow. Is there a relationship between the grayscale images and the inner subunits? For instance, the subunits sizes go beyond the stimulus resolution, so, is their spatial location reliably estimated? Why level 3 only divides in two the left component, while the right one is still increasing the subunits number? In fact, the authors, in response to reviewers document and the article, indicate that the hierarchical partitioning is not used for the remainder in the paper. If this is not used, why is it introduced?</p><p>-Figure 5D. In the inset histograms, please use the same bin-width.</p><p>- Figure 6B. In the text, the authors talk about prediction accuracy. Nevertheless, the figure indicates Δ correlation. Same in Figure 7B.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.45743.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p></disp-quote><p>We thank the reviewers for a thorough evaluation of the manuscript. In response to their feedback, we have modified the paper, and have provided responses to their feedback below.</p><disp-quote content-type="editor-comment"><p>Summary:</p><p>[…]The main concern to address is that for a &quot;tools and resources&quot; papers needs to include clear benchmarking of the advantages of the current method against other existing and/or published approaches for subunit identification. The consensus among reviewers was that one would need to include comparison with at least one other method for both the retina and the V1 dataset. Some of the relevant methods that you could use are provided below, which offer different trade-offs between accuracy and ease of implementation and the amount of background work needed to set the hyperparameters. It would be helpful if the authors could include a discussion to justify their choice of the alternative method. The alternative method might be different for the V1 and the retina dataset. Including a more detailed discussion of the available methods and the authors' perspective on which would work best at each circumstance or generalize across the light/stimulus conditions would be very helpful to the field.</p><p>The second concern was about the apparent inconsistencies in how the steps of the proposed algorithm were chosen (as detailed below) and that more methodological details need to be provided.</p></disp-quote><p>We have substantially improved the manuscript regarding both of these concerns. For the first, in addition to a detailed discussion of relationships to previous methods, we have included new analyses comparing the performance of our method to two others: the semi non-negative matrix factorization method (for retinal data), and the convolutional method (for V1 data). For the second concern, we have added an overview of the different algorithmic variations of the basic subunit estimation approach at the end of subsection “Subunit response model and parameter estimation”, and improved the text transitions at various places throughout the Results section to make the modifications for different experimental conditions clearer and better motivated. Details are provided below in response to the specific critiques.</p><disp-quote content-type="editor-comment"><p>Comparison with existing methods:</p><p>While the manuscript includes some discussion of the existing methods, it is still not clear which model is the best taken into account that the subunits of the current model do not correlate with the actual bipolar cell receptive fields but rather with their clusters in some way. For the reader to take up this method, one would need to clearly see the pros and cons in speed, resolution, easiness of implementation, neural correlates of subunits and/or other key features of this model against other nonlinear subunit models (e.g. Liu et al., 2017 and the simpler earlier version by Kaardal et al., 2013) and/or simple convolution neural network models (Eickenberg et al., 2013, Vintch et al., 2015). It is critical for the reader to be able to assess the value of the current model. Assuming this is done properly, and these clarifications are added, this paper would be significantly improved.</p></disp-quote><p>We have added the following detailed comparisons:</p><p>Semi Non-negative Matrix Factorization (SNMF – similar to Liu et al., 2017): We have incorporated Figure 9 and an accompanying paragraph in the Discussion section comparing our method to SNMF in a simulated cell (for which ground truth is known). Briefly, we observed that our method is more efficient than SNMF at estimating subunits from limited data. Specifically, the fits using our method were more accurate and less variable.</p><p>A convolutional subunit model (similar to Vintch et al., 2015): We have incorporated Figure 10 and an accompanying paragraph in Discussion section comparing our method to an approach with convolutional (spatially-repeated, homogeneous) subunits. The convolutional method performs better for very small amounts of training data (as expected – it has far fewer parameters), but for larger amounts of data (comparable to what is available in typical experiments) our method performs better. The subunits estimated with our method roughly match both the locations and spatio-temporal selectivity of subunits estimated using the convolutional approach, but our method offers improved performance by capturing the distinct (non-convolutional) structure of individual subunits.</p><disp-quote content-type="editor-comment"><p>One would like to understand and document clearly performance of the subunit-identification method across stimulus conditions (white noise vs. naturalistic stimuli) against conventional currently used methods. This could perhaps be done by applying the new model to existing datasets of parasol RGCs in different experimental conditions. If such data are not available, then perhaps the method could be tested using simulated responses constructed to mimic properties of the retinal neurons.</p></disp-quote><p>If we understand correctly, the reviewer is asking us to (a) assess how well our method performs when applied to stimuli other than white noise (e.g., natural scenes), and (b) compare the performance of our method to the performance of previous methods (on at least two kinds of stimuli).</p><p>For (a), a direct comparison would involve obtaining and testing subunit model fits on natural scenes. We did not attempt to directly estimate subunits from natural scenes data, because natural scenes are biased toward large spatial scales, and are therefore ill-suited for estimating the fine structure of subunits in the RF. Indeed, even the linear estimate of the RF obtained using spike-triggered averaging is highly biased by natural scene structure. Because this problem is about the stimulus itself, it applies to all subunit estimation techniques. So, to test the effectiveness of the method for natural scenes, we instead estimated the subunits using white noise, then tested the success of these subunits in predicting the responses to natural scenes (Figure 7).</p><p>Regarding (b), we now provide a direct comparison of our method to the SNMF method (for RGCs) and the convolutional method (for V1), with white noise data. However, for technical reasons, we think comparison of natural scenes predictions would have little value. For both the SNMF approach and ours, extension to natural scenes data requires a modification to account for different response statistics in natural scenes. In both studies, this was handled with an ad hoc add-on procedure intended to demonstrate only that the subunits indeed help in response estimation (see Figure 7), but was <italic>not</italic> intended to fully capture responses to natural scenes (which likely entail much more complex nonlinearities than just subunits, e.g. various forms of gain control). We note that the authors of the SNMF study point this out explicitly. Our approach involves a modification of the output nonlinearity, and their approach involves an added response gain term, which are not easy to reconcile. In both studies, there is no exploration of alternatives or experimental test of the particular choice made. Furthermore, because the subunits themselves are estimated from white noise, a comparison of responses to natural scenes would hinge primarily on these ad hoc add-ons. For these reasons, we have not attempted this analysis -- regardless of the outcome, we don’t think it would do justice to either study.</p><disp-quote content-type="editor-comment"><p>Questions about the proposed methodology:</p><p>There are some inconsistencies in the design of the proposed methodology. For examples, the method is motivated with an example that involves hierarchical splitting of subunits, but this hierarchical version is actually not used further on.</p></disp-quote><p>The basic method is not intended to be hierarchical. The hierarchical analysis is intended to illustrate (1) how the method behaves when the number of estimated subunits is less than the number of true subunits; (2) that the solutions for different numbers of estimated subunits are “nested”, in the sense that each successive increase in the number of subunits leads (approximately) to a splitting of a subunit into two. This observation is formalized in an analysis where hierarchy is enforced, confirming the result (Figure 2—figure supplement 1). This hierarchical partitioning can also be used as the basis for an incremental algorithm that provides a more robust/efficient method of estimating the subunits (Figure 2—figure supplement 1). Despite this, in the interest of simplicity, this is not the approach we use in the remainder of the paper. This has been clarified in the text. If the reviewer thinks this is too confusing, Figure 2—figure supplement 1 and accompanying analysis could be omitted.</p><disp-quote content-type="editor-comment"><p>Then the bare method (without regularization and fairly coarse stimuli) is used to analyze populations of OFF parasol cells (Figure 2B-F), but Figure 3 shows that much better subunits are obtained with regularization and finer white noise.</p></disp-quote><p>Early in the manuscript, we use the most basic form of the method (without priors/regularization) to establish its properties, without introducing any inadvertent biases or relying on any features of the particular data used. This resulted in subunits that were spatially localized, despite the fact that the method incorporates no preferences or knowledge regarding the spatial location of the stimulus pixels. This motivated the development of a prior that favors spatial locality (locally normalized L1), which we demonstrate can regularize RGC subunit estimates more efficiently than the more commonly used sparsity (L1) prior when data are limited. We have tried to clarify the motivation for this sequenced presentation in the text.</p><disp-quote content-type="editor-comment"><p>One wonders why this improved method wasn't used to obtain, for example, the distribution of subunit numbers.</p></disp-quote><p>As shown early in the manuscript, when the pixel size is large, the estimated subunits are aggregates of true underlying bipolar cell subunits. For the data with finer pixels, even though the estimated number of subunits is slightly higher, the pixel size and estimated subunits are still too large to reveal bipolar cell inputs. As such, re-analyzing the true number of subunits with regularization did not seem sufficiently informative given the space it would take.</p><disp-quote content-type="editor-comment"><p>Further analyses are then again (partly) performed without regularization (e.g. joint fitting of multiple cells, V1 data). In particular when discussing the V1 data, where the subunits have very different structure than in the retina, one wonders whether regularization still works in the same way.</p></disp-quote><p>Joint fitting of multiple cells was actually performed using the locally normalized L1 regularization that we showed to be effective for single cells. This was originally indicated only in the caption of Figure 4; We now also mention it in the main text and Methods section.</p><p>V1 subunits estimated without any regularization already revealed interesting structure in relation to previous findings, so we did not think it was necessary to incorporate any regularization (either the localization, or a new V1-specific variation). We have tried to be clearer about the rationale in the text.</p><disp-quote content-type="editor-comment"><p>Also, it is emphasized how the optimal number of subunits can be determined, but some of the subsequent analyses are performed for a fixed number of subunits (e.g. Figure 2 and Figure 3).</p></disp-quote><p>The appropriate approach for setting this parameter (the number of subunits) depends on what the analysis is being used for. The most straightforward case is when the user seeks to determine the number and structure of subunits for a particular cell. For this, we advocate cross-validated optimization -- choose the number of subunits that best describes the data (while not overfitting). This is empirically the “best” model fit, and is used in Figure 2C and Figure 8.</p><p>However, Figure 2A and Figure 3 have different goals:</p><p>- The goal of Figure 2A is to identify strong sources of spatial nonlinearity. By comparing the fits for different numbers of subunits, we find that the subunits are related hierarchically, suggesting that weaker subunits are partitions of stronger subunits.</p><p>- The goal of Figure 3 is to compare different regularization methods for subunit estimation. To avoid comparing different optimal number of subunits for each regularization method, the number of subunits are fixed to a value that works best across multiple cells.</p><p>We have tried to clarify the motivation for these analyses in the text.</p><disp-quote content-type="editor-comment"><p>There are also several other parts where that information is difficult to extract or missing, which hinders understanding of the material.</p><p>The description of the subunit estimation approach via approximate maximum likelihood (subsection “Subunit estimation”) is central to the manuscript but quite difficult to follow. Some terms in the equation are not defined (K and w with superscript 0, c(K,w)), and the explanations for the derivations are cursory or absent. For example, when Equations 1-5 are derived, there is no mentioning of an iterative procedure (this only comes later; at this point, this seems to be only a reformulation of the log-likelihood), but some manipulations and terms here only make sense in the context of an iterative estimation procedure. This needs to be disentangled.</p><p>Relatedly, it seems important to spell out how the clustering algorithm (Equations 6-8) solves the maximum likelihood problem, in particular since this connection is central to the manuscript.</p></disp-quote><p>We appreciate these comments and corrections, and have rewritten subsection “Subunit estimation” to make it more clear. Briefly, each step of the clustering algorithm results from minimizing a convex upper bound based on current parameter estimates. Equations 1-5 derive this upper bound and Equations 6-8 present the steps to minimize this upper bound. Omission of term definitions has been corrected.</p><disp-quote content-type="editor-comment"><p>The application of the method to V1 data makes an important point regarding the general applicability of the method. This is nice, but only very little analysis is presented. Maybe the authors could at least show how the model performs for different numbers of subunits as in other analyses, to see how well the simple cell and the complex cell are separated in this respect. Also, the effect of regularization would be interesting here.</p></disp-quote><p>We have incorporated the reviewer’s comments by adding analysis with varying number of subunits for V1 data, showing similar hierarchical splitting as RGCs (Figure 8).</p><p>Although the suggestion about simple and complex cells is an interesting one, given that the paper is now essentially a methods paper, and that such an analysis would require more space (as well as substantially more data), we have chosen not to pursue this direction.</p><p>Although in principle we could explore and test various regularization methods on V1 data, our goal with this analysis (and limited data set) was simply to demonstrate that the basic method works as well as or better than previous methods for subunit identification.</p><disp-quote content-type="editor-comment"><p>This assumption of space-time separability needs further discussion and justification. This assumption might be necessary to reduce the number of parameters to manageable levels, but it would be helpful if the authors could offer guidelines for when it might be appropriate.</p></disp-quote><p>We have added text in Subsection “Further applications and extensions” on how to verify the space-time separability assumption empirically. We have performed this verification on our data, but it would substantially lengthen the manuscript to include these data.</p><p>Note that the separability assumption is also made in other work that focuses on spatial subunits (Liu et al., 2017) and implicitly or explicitly in much of the literature on RGCs (but see Enroth-Cugell and Pinto., 1970 for a deeper investigation of separability). Note also that separability is not required by our subunit model or fitting procedure -- for example, in the V1 cells, the method extracts spatio-temporally oriented (and thus, non-separable) receptive fields.</p><disp-quote content-type="editor-comment"><p>About the response prediction to naturalistic stimuli. The use of white noise stimuli rather than natural stimuli might provide a good approximation for estimating of subunit filters, However, bipolar cells, which provide inputs to parasol RGCs, are strongly affected by stimulus correlation through gap-junction connectivity generating a nonlinear enhancement of bipolar activity, which could explain the differences found with real activity (Kuo et al., 2016, Manookin et al., 2018). These elements should be included in the Discussion section as one of the potential pitfalls of the approach.</p></disp-quote><p>Thank you for pointing this out. We now explain the possible role of gap junctions in natural scenes responses in Results section and Discussion section.</p><disp-quote content-type="editor-comment"><p>The null-stimulus description indicated in the Results section needs further discussion and justification. This part should be rewritten extracting elements present in the methods.</p></disp-quote><p>We have rewritten this section with more specifics and detail as suggested.</p><disp-quote content-type="editor-comment"><p>Also, the results are confusing. According to what is presented in Figure 5, the PSTH variance obtained from the white-noise response is larger than the variance of null-stimulus response, and at some extent, this variance would be reflecting inter-trial variability.</p></disp-quote><p>We thank the reviewer for pointing this out. Indeed, the PSTH variance we measure has a component of response variation over time (the thing we are interested in) and inter-trial variability (which is of less interest here). On evaluating the effect of the number of trials, we find that PSTH variance converges with 30 trials, in 3 datasets (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>); thus, the inter-trial variability is not significant in the analysis performed in the paper. We have now indicated this in Results.</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Convergence of PSTH variance.</title><p>(<bold>A</bold>) PSTH variance (y-axis), averaged over ON and OFF populations for white noise (black) and null stimulus (red) as a function of the number of randomly sampled trials (x-axis). Line thickness corresponds to +/- 1 s.d. in estimation error. Same retina as in Figure 5, main paper. (<bold>B, C</bold>) Same as A, for two other retinas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-45743-resp-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>At this level, this figure leads to confusion because it is not helping to understand the methodology. What is the role of the null-stimulus in this method paper? The method seems to predict the null-stimulus response better, but again, it is not clear how this clarifies the methodology neither provides insights to understand this behavior. It is counter-intuitive the fact that adding more units in the model subunits the response prediction for white-noise stimulus is not improving (Figure 6).</p></disp-quote><p>Null stimuli provide a direct means of demonstrating the importance of the subunit nonlinearity. They expose spatial nonlinearities by specifically “zeroing out” the component of light response that can be explained with a linear model. In this sense, they are analogous to the commonly-used counterphase gratings and second harmonic analysis (Hochstein and Shapley, 1976), which have been used in dozens of studies to focus on bipolar cell nonlinearities. The null stimuli, however, have much richer stimulus statistics (contrast, temporal frequency, spatial scale) very similar to the white noise stimuli that are used for estimating the subunit model that is being tested. The counter-intuitive result that the reviewer points out is that the efficiency of the null stimuli at revealing spatial nonlinearities allows them to reveal a clear improvement of subunit models even for short duration stimuli (Figure 6), while white noise stimuli require longer stimulus presentations (Figure 2).</p><p>We have attempted to clarify these points in the text.</p><disp-quote content-type="editor-comment"><p>More detailed comments:</p><p>The manuscript was first submitted as original research before the submission as a method paper where essential changes in the structure and content were incorporated. Nevertheless, there is still information more related to retina physiology than the methodology description. Besides, if this is a method valid both for retina and V1 neurons, it is important to mention this in the title, where only the retina application is indicated. Therefore, please consider revising the article title.</p></disp-quote><p>We agree, and have revised the title accordingly, – “Inference of Nonlinear Receptive Field Subunits with Spike-Triggered Clustering”</p><disp-quote content-type="editor-comment"><p>In the Introduction, the authors claim there is no widely accepted general computation to infer the structure of nonlinear subunits inputs to a neuron. Nevertheless, relevant references have been published in the last years addressing the same problem noted by the authors. For instance, Liu et al., 2017, which appears in the discussion but not in the introduction, proposes a spike-triggered non-negative matrix factorization, to estimate the non-linear subunits conforming the RGC receptive field in salamander retina, where bipolar cells conforming RGCs can be also encountered.</p></disp-quote><p>We agree that the recent literature on this topic is active: various groups have realized that subunits are a common computational motif, and that new tools are required to study their role in neural computation. We have revised the introduction to position our contribution relative to these studies (although we note that the Liu et al., 2017 study was already mentioned in paragraph 2 of our original Introduction). In particular, although previous efforts to identify subunits have been advanced in various experimental conditions, none of the existing papers have developed a single method and applied it to multiple stimulus conditions, with different statistics, different systems, and large numbers of recorded neurons.</p><disp-quote content-type="editor-comment"><p>It would be interesting to obtain and include some information about computational runtime.</p></disp-quote><p>Thanks for this suggestion – we have added information about computational runtime in Materials and methods section.</p><disp-quote content-type="editor-comment"><p>The presentation of the analysis of the simulated ganglion cell under fine white noise stimulation (Figure 1—figure supplement 1C) is odd. Why are the subunits shown as sets of photoreceptors, not as pixel layouts? Does the method yield 12 subunits as the optimal number, or is this just the result of what came out when enforcing 12 subunits? A better discussion of how the method performs for simulated data could help better understand the strengths and limitations of the approach.</p></disp-quote><p>Unfortunately, the text in the caption was confusing. While the coarse pixel approach is described in panels A and B, in panel C we simulate what would happen if each cone were driven independently. Although this would require a closed-loop experiment identifying individual cone locations (Freeman, 2015), we show here that it would reveal the correct subunits with the estimation approach in this paper. We have attempted to explain the simulation better in the caption (including indication that the correct number of subunits (12) was identified), and motivate this choice of representation better in the text.</p><disp-quote content-type="editor-comment"><p>The figures show the &quot;relative strength&quot; of subunits by a blue scale bar. Is this the weight w? Or the norm of the subunit filter? Both seem to be related to &quot;strength&quot;. And both seem to be optimized in the second step of the fitting approach along with the output nonlinearity.</p></disp-quote><p>The blue bars correspond to the average contribution to cell response for different subunits given by: <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We now provide a pointer to Equation (6) in the methods. This formula ignores the learned output non-linearity, which saturates only for the strongest 0.5% of the stimuli. This information has been added in Results section and Materials and methods section.</p><disp-quote content-type="editor-comment"><p>Subsection “Estimated subunits are spatially localized and non-overlapping: The text refers to the Materials and methods section for explaining how the temporal filter was obtained, but the Materials and methods section seem to lack information about receptive field and temporal filter estimation.</p></disp-quote><p>Thank you for spotting this. We’ve added the explanation to the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>Subsection “Regularization for spatially localized subunit estimation”: The chosen value of epsilon in the LNL1 regularization should somehow be related to the typical range of weights. Are the latter of order unity so that a value of epsilon=0.01 is small compared to typical weights?</p></disp-quote><p>Yes, typically, the nonzero weights were ~2 and the maximum weights were ~4. This information has been added to the paper.</p><disp-quote content-type="editor-comment"><p>Figure 4E: It seems odd that the negative log-likelihood keeps decreasing with the number of subunits in the joint fitting procedure. Unlike for the separate fitting. Is there a reason for this?</p></disp-quote><p>Unlike other figures, the negative log-likelihood for the joint model is decreasing for the range of subunits plotted in Figure 4E. This can be attributed to the greater efficiency of joint estimation procedure compared to separate fitting. For separate fitting, a subunit connected to two nearby cells needs to be estimated by both the cells independently.</p><p>We believe that the negative log-likelihood for the joint model will indeed saturate, but for larger number of subunits than currently shown. For presenting a concise figure, we did not analyze the model for larger number of subunits.</p><disp-quote content-type="editor-comment"><p>Relatedly: how was the total number of subunits constrained for the separate fitting?</p></disp-quote><p>First, for each cell, models with different number of subunits are estimated. Then, for a fixed total number of subunits, these separate fits are used to find a combination of subunits (<italic>N<sub>i</sub></italic> for cell i) that gives maximum log-likelihood on held-out validation data and <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Performance is analyzed for different <italic>N</italic>. This has been clarified in the main text.</p><disp-quote content-type="editor-comment"><p>Subsection “Regularization for spatially localized subunit estimation”: The text states that a joint model with 15 subunits gives higher prediction accuracy, referring to Figure 4D. Does this relate to predictions as analyzed in Figure 5? Or to the maximum likelihood of Figure 4E?</p></disp-quote><p>Thank you for pointing out this typo. It refers to Figure 4E.</p><disp-quote content-type="editor-comment"><p>Subsection “Subunit model explains spatial nonlinearities revealed by null stimulus”: The text refers to &quot;spatial null spaces&quot;, but the Materials and methods section relates to spatiotemporal null spaces. Please clarify.</p></disp-quote><p>The methods section presents the steps involved in a more general, spatio-temporal null stimulus, as well as a method for spatial-only null stimulus, and the relationship between them. In case of RGCs, where the STA is space-time separable, the spatial-only nulling is equivalent to spatio-temporal nulling. However, for the V1 data, the more general spatio-temporal nulling method needs to be applied, and this is why we have included it in the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>Subsection “Subunit model explains spatial nonlinearities revealed by null stimulus”, fourth paragraph: Maybe I missed it, but I wasn't sure which version of the subunit estimation is meant by &quot;the subunit model&quot;. With or without regularization? Fine or coarse stimuli? A specific instance of the different initial conditions? Same for Figure 7.</p></disp-quote><p>For both Figure 6 and Figure 7, the unregularized method was applied on coarse stimuli; the first phase of fitting was applied on white noise and second phase was applied on null stimulus or natural scenes respectively. This has now been clarified in the text.</p><disp-quote content-type="editor-comment"><p>Figure 7: The figure and the text around it use the term saccade, but the stimulus seems just to be a switch to a new image without a real motion segment between fixations. This is a bit confusing.</p></disp-quote><p>This has been clarified in the main text. We have emphasized that this is only a crude approximation of visual input interrupted by saccades.</p><disp-quote content-type="editor-comment"><p>Also, how the stimulus is segmented into periods around and between image switches should be better explained and not only mentioned in the legend, in particular how this is used to compute the projection on the receptive field (Figure 7C; is this a spatiotemporal projection?).</p></disp-quote><p>Yes, it is spatio-temporal filtering. This has been clarified in the main text.</p><disp-quote content-type="editor-comment"><p>Furthermore, the difference in Figure 7C seems quite subtle. Is it significant?</p></disp-quote><p>We agree that the difference in the histograms seems subtle – one has to look for a moment to see that the black bars are lower in the center, and higher at the extremes. However, the two distributions are significantly different as evaluated by comparing their variances using a two-sided F-test. The variances for saccadic and inter-saccadic stimuli were 233 (N=5e6) and 155 (N=15e6), respectively.</p><disp-quote content-type="editor-comment"><p>In the Materials and methods section, maybe state that the spatiotemporal stimulus was binary white noise (not Gaussian) if this is the case and that the X<sub>t</sub> signal that goes into the subunit estimation is the contrast signal (zero mean).</p></disp-quote><p>This has been added.</p><disp-quote content-type="editor-comment"><p>Could the authors explain why the weights and the filter magnitudes need to be refitted in the second model fitting step?</p></disp-quote><p>Since the output nonlinearity changes from linear to a more saturating one in natural scenes, the overall firing rate of the model cell is lower, and the weights on subunits and/or their magnitudes must be increased to match the observed firing rate. Moreover, the saturating nonlinearity could change the relative weights for different subunits – subunits that drive the cell more strongly would be more affected by saturation. However, on applying to data, we observed that the final nonlinearity is nearly linear, confirming that the first model fitting step is sufficient in most cases.</p><disp-quote content-type="editor-comment"><p>Subsection “Application to neural responses”: What does the 5.2 µm boundary refer to?</p></disp-quote><p>This refers to the region of stimulus around the receptive field which is included for subunit estimation. This has been clarified in the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>For the simulated RGC model, the 180 µm (= 180 g.u.) spacing between cones seems large. Is that a typo? Also, in the description, it is not quite clear what the photoreceptor weights are (used for normalizing the firing rate) and how bipolar weights were varied around 10% of their mean (which mean?).</p></disp-quote><p>It is a typo – thanks for pointing it out. The spacing between photoreceptors is 5 µms. The details of photoreceptor and bipolar weights have been corrected. The bipolar weights were drawn from a gaussian distribution with standard deviation equal to 10% of the mean.</p><disp-quote content-type="editor-comment"><p>When describing the null stimulus, it would be good to explain where the original stimulus S<sub>w</sub> comes from and how the σ for computing the support of the receptive field is obtained.</p></disp-quote><p>The original stimulus S<italic><sub>w</sub></italic> is white noise used for computing the null stimulus and σ is a robust estimate of the standard deviation of background noise in STA. We have clarified in the text.</p><disp-quote content-type="editor-comment"><p>The references Maherwaranathan et al., 2018, and Shi et al., 2018 are missing.</p></disp-quote><p>These recent references have been added.</p><disp-quote content-type="editor-comment"><p>Figures are in general of poor quality and the color choice misleading.</p></disp-quote><p>We apologize for the low-resolution figures. They have been replaced with high resolution versions. We chose to focus on as few colors as possible and the color choices are consistent across figures. The color choices for white noise and null stimulus in Figure 5 and Figure 6 were interchanged. This is fixed now.</p><disp-quote content-type="editor-comment"><p>In Figure 2D is not possible to see the black arrows over the blue bars. In Figure 2B blue line is not well visualized.</p></disp-quote><p>For Figure 2D, we moved the black arrows to the top for clarity. The blue line is made continuous (instead of dotted) for better visualization.</p><disp-quote content-type="editor-comment"><p>Figure 5D, use the same bar width for the inset histograms.</p></disp-quote><p>We agree that having different bin widths makes the figure visually awkward. However, since the range of values for ON and OFF parasols is different, a common bin-width does not provide a good depiction of both distributions. Instead, we chose the same number of bins to describe both distributions equally accurately.</p><disp-quote content-type="editor-comment"><p>&quot;… was highly reproducible across repeats (Figure 6C,D),&quot; -&gt; &quot;… was highly reproducible across repeats (Figure 5C,D),&quot;</p></disp-quote><p>This has been fixed.</p><disp-quote content-type="editor-comment"><p>Figure 6B: dashed are filled lines are not indicated in the caption.</p></disp-quote><p>We had mistakenly referred to them as ‘dotted’ lines in caption. Now changed to ‘dashed’ lines.</p><disp-quote content-type="editor-comment"><p>Subsection &quot;Application to neural response&quot;. The last three paragraphs should be incorporated into the Results section.</p></disp-quote><p>The information from these paragraphs is now included in the main text in Results section and/or captions.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p></disp-quote><p>Thank you for your favourable review. Below, we address the remaining issues.</p><disp-quote content-type="editor-comment"><p>The comparison to the convolutional subunit model is not completely convincing for the following reasons: (1) The comparison done on V1 data uses a restricted version of the convolutional subunit model (the subunit nonlinearity is forced to be an exponential, despite that Vintch et al., (2015) and others have showed that it is often quadratic when learned from data). (2) No comparison is made on the retinal dataset where the fixed exponential subunit nonlinearity probably is a more realistic assumption. The reader is thus left wondering whether the new method truly has better predictive performance than the simpler convolutional subunit model, and whether the main benefit with the new method is that it can fit multiple subunit filters easily. It would be beneficial at least to clarify these concerns by giving a clearer justification to the selected methodology regarding the above issues.</p></disp-quote><p>(1) We agree with the reviewers, and have now added a relevant comparison (Figure 10—figure supplement 1). For the convolutional model, we estimate the common subunit filter, location-specific weights and biases for quadratic nonlinearity using standard gradient descent methods. For our model, we first estimate the subunit filters with spike-triggered clustering (which assumes an exponential nonlinearity), and fine-tune the subunits filters and bias for quadratic nonlinearity using gradient descent. We find that both methods perform better with a quadratic nonlinearity, consistent with previous studies (Vintch et al., 2015). But as with the exponential nonlinearity, the convolutional method outperforms the spike-triggered clustering method when trained on small amounts of data, but the opposite holds for larger amounts of data. (2) As requested, we have added a comparison on the retinal data, which produces similar results to the V1 data (Figure 10).</p><disp-quote content-type="editor-comment"><p>A description for how/why the clustering algorithm's three step procedure solves the likelihood problem is still missing. It would be helpful if the authors could briefly spell out why the three-step prescription subsection “Subunit estimation” “minimizes the upper bound of the log-likelihood (Equation 5). Are the update rules derived from the partial derivatives of Equation 5? Is this guaranteed to converge and converge to the actual minimum?</p></disp-quote><p>We have expanded the description and derivation of the algorithm in Methods. We now explain how the update rules are derived from partial derivatives of Equation 5. In brief: we are minimizing an approximation to the negative log-likelihood (Equation 2). As the amount of data grows, the approximation converges to the negative log-likelihood. The algorithm is guaranteed to converge since it successively minimizes a tight upper bound to the approximate log-likelihood, leading to a monotonic increase in the approximate log-likelihood. However, due to the non-convexity of the approximate negative log-likelihood, we cannot guarantee that the point of convergence will be a global optimum.</p><disp-quote content-type="editor-comment"><p>Regarding the validation in V1 data: More than a validation, where there is no ground-truth to compare, the results should be focused to validate the model showing that it reproduces neural data better than previous approaches. For instance, Figure 10 compares the resulting subunits obtained by a convolutional model and the model proposed in this article. It is tough to compare both; it should be more useful to see how the two of them reproduce neural response.</p></disp-quote><p>We agree with the reviewer that in addition to the qualitative comparison of the derived filters, it is valuable to compare predictions of neural response. This is done by plotting the log-likelihood of the recorded response as a function of the amount of training data in Figure 10C (and in the new Figure 10A as well for retinal data). We have attempted to clarify this in the figure caption.</p><disp-quote content-type="editor-comment"><p>The Discussion section requires a new organization. The comparison with the existing models, such as SNMF and V1 data, shouldn't be placed here. The paragraph describing the work of McIntosh et al., 2016 can be better articulated with the rest of the Discussion section.</p></disp-quote><p>As suggested, we have moved the comparison to SNMF and convolutional models to Results section and reorganized the relevant section of Discussion section.</p><disp-quote content-type="editor-comment"><p>- For instance, the sentence &quot;Note, however, that a model with two subunits frequently explained ON parasol data more accurately than a model with one subunit, implying that ON cells do have spatial nonlinearities&quot;. This is not related to the method validation, and moreover, there is no result to validate this affirmation.</p></disp-quote><p>This statement was intended to make the point that, even though the optimum number of subunits for ON parasol cells was lower than for OFF parasol cells, spatial nonlinearity was still present in ON parasol cells. To support this claim, we have now provided statistics in the text, and rephrased it slightly.</p><disp-quote content-type="editor-comment"><p>- Subsection “Modeling linear-nonlinear cascades” states that the paper by Jia et al., (2018) applies a non-negativity constraint on the weights instead of the subunit filters. Looking at their methods and sample subunits in the figures, this doesn't seem to be the case.</p></disp-quote><p>The reviewer is absolutely correct: we made a mistake in our interpretation of the 2018 paper. It has now been corrected in the manuscript.</p><disp-quote content-type="editor-comment"><p>- For the receptive field estimation in subsection “Application to neural responses”, the pixels with magnitude larger than 2.5 σ probably refer to magnitude **of the STA**; similarly, averaging the time course of pixels probably refers to averaging the pixels **in the STA**.</p></disp-quote><p>Thanks for pointing this out. It has been corrected.</p><disp-quote content-type="editor-comment"><p>Figure 2 is still hard for me to follow. Is there a relationship between the grayscale images and the inner subunits? For instance, the subunits sizes go beyond the stimulus resolution, so, is their spatial location reliably estimated? Why level 3 only divides in two the left component, while the right one is still increasing the subunits number? In fact, the authors, in response to reviewers document and the article, indicate that the hierarchical partitioning is not used for the remainder in the paper. If this is not used, why is it introduced?</p></disp-quote><p>We apologize for lack of clarity in this figure. We suspect the confusion is due to a poor graphics choice on our part that made it difficult for a reader to see the Gaussian fits to the subunits. We have modified the figure so that the Gaussian fit for the fitted subunit shown in the image is indicated with a green ellipse that is more visible on darker pixels, and used red ellipses to represent the other estimated subunits. We hope that this makes the partitioning of receptive field clearer.</p><p>As the reviewer points out, the subunit estimates are limited by stimulus pixel resolution. For this reason, we measure the separation between subunits using the peaks of their Gaussian fits which, assuming that the true subunits are roughly Gaussian, effectively interpolates their center locations.</p><p>Regarding the last comment, we thought it was important to mention that enforcing hierarchical partitioning (Figure 2—figure supplement 1) leads to similar subunits, because this reinforces the idea that subunit estimates effectively “split” as the number of subunits in the model increases. Also, enforcing this hierarchical organization could lead to improved estimates by providing a form of regularization. We have further clarified this in the text. If the reviewers still think that these points are not important, we could remove ‘Figure 2—figure supplement 1’.</p><disp-quote content-type="editor-comment"><p>-Figure 5D. In the inset histograms, please use the same bin-width.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>- Figure 6B. In the text, the authors talk about prediction accuracy. Nevertheless, the figure indicates Δ correlation. Same in Figure 7B.</p></disp-quote><p>We apologize for the confusion about the metric. Throughout the paper, we use negative log-likelihood for measuring accuracy for single-trial data, and correlation for raster data. For studying the variation of performance with increasing numbers of subunits, the <italic>change</italic> in correlation compared to one subunit is presented in the figures, because there is a comparatively large variation across cells in the correlation obtained with one subunit, which would obscure the trends seen in the figure. We have modified the figure caption to make this connection clearer.</p></body></sub-article></article>