<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82301</article-id><article-id pub-id-type="doi">10.7554/eLife.82301</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A model of hippocampal replay driven by experience and environmental structure facilitates spatial learning</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-287890"><name><surname>Diekmann</surname><given-names>Nicolas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3638-7617</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-109213"><name><surname>Cheng</surname><given-names>Sen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6719-8029</contrib-id><email>sen.cheng@rub.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04tsk2644</institution-id><institution>Institute for Neural Computation, Faculty of Computer Science, Ruhr University Bochum</institution></institution-wrap><addr-line><named-content content-type="city">Bochum</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04tsk2644</institution-id><institution>International Graduate School of Neuroscience, Ruhr University Bochum</institution></institution-wrap><addr-line><named-content content-type="city">Bochum</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Piray</surname><given-names>Payam</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03taz7m60</institution-id><institution>University of Southern California</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>03</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82301</elocation-id><history><date date-type="received" iso-8601-date="2022-07-29"><day>29</day><month>07</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-03-13"><day>13</day><month>03</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-07-28"><day>28</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.26.501588"/></event></pub-history><permissions><copyright-statement>© 2023, Diekmann and Cheng</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Diekmann and Cheng</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82301-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82301-figures-v2.pdf"/><abstract><p>Replay of neuronal sequences in the hippocampus during resting states and sleep play an important role in learning and memory consolidation. Consistent with these functions, replay sequences have been shown to obey current spatial constraints. Nevertheless, replay does not necessarily reflect previous behavior and can construct never-experienced sequences. Here, we propose a stochastic replay mechanism that prioritizes experiences based on three variables: 1. Experience strength, 2. experience similarity, and 3. inhibition of return. Using this prioritized replay mechanism to train reinforcement learning agents leads to far better performance than using random replay. Its performance is close to the state-of-the-art, but computationally intensive, algorithm by Mattar &amp; Daw (2018). Importantly, our model reproduces diverse types of replay because of the stochasticity of the replay mechanism and experience-dependent differences between the three variables. In conclusion, a unified replay mechanism generates diverse replay statistics and is efficient in driving spatial learning.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>hippocampus</kwd><kwd>replay</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>419037518 - FOR 2812 P2</award-id><principal-award-recipient><name><surname>Cheng</surname><given-names>Sen</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A model of hippocampal replay is proposed that gives a biologically plausible account of how the hippocampus could prioritize replay and produce a variety of different replay statistics, and is efficient in driving spatial learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans and other animals continuously make decisions that impact their well-being, be that shortly after emitting a choice or much later. To successfully optimize their behavior, animals must be able to correctly credit choices with resulting consequences, adapt their future behavior, and remember what they have learned. The hippocampus is known to play a critical role in the formation of new memories and retrieval of memories, as evidenced by the famous case of patient H.M. (<xref ref-type="bibr" rid="bib13">Corkin et al., 1997</xref>) and others (<xref ref-type="bibr" rid="bib59">Wilson et al., 1995</xref>; <xref ref-type="bibr" rid="bib47">Rosenbaum et al., 2004</xref>). In rats and mice damage to the hippocampus is known to impair spatial learning and memory (<xref ref-type="bibr" rid="bib37">Morris et al., 1982</xref>; <xref ref-type="bibr" rid="bib16">Deacon et al., 2002</xref>). An important phenomenon linked to learning and memory found in the hippocampus is that of ‘replay’ (<xref ref-type="bibr" rid="bib7">Buhry et al., 2011</xref>). As an animal navigates in an environment, so-called place cells (<xref ref-type="bibr" rid="bib38">O’Keefe and Dostrovsky, 1971</xref>) in the hippocampus are sequentially activated. Later, during awake resting states and during sleep, compressed reactivation of the aforementioned sequences can be observed within events of high-frequency neural activity which are known as sharp wave/ ripples (<xref ref-type="bibr" rid="bib8">Buzsáki, 1989</xref>; SWRs). These sequences preferentially start at the current position of the animal (<xref ref-type="bibr" rid="bib14">Davidson et al., 2009</xref>) and can occur in the order observed during behavior as well as in the reverse order (<xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>).</p><p>Consistent with the proposed function of replay in learning, <xref ref-type="bibr" rid="bib57">Widloski and Foster, 2022</xref> showed that in a goal-directed navigation task replay sequences obey spatial constraints when barriers in an environment change on a daily basis. By contrast, other studies suggest that replay is not limited to just reflecting the animal’s previous behavior. For instance, replay can represent shortcuts that the animals had never taken (<xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref>). Replay during sleep appeared to represent trajectories through reward-containing regions that the animals had seen, but never explored (<xref ref-type="bibr" rid="bib40">Ólafsdóttir et al., 2015</xref>). Following a foraging task, replay sequences resembled random walks, that is, their statistics was described by a Brownian diffusion process, even though the preceding behavioral trajectories did not (<xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref>). How can the hippocampus support the generation of such a variety of replay statistics and how do the sequences facilitate learning? Here, we address these questions by studying replay and its effect on learning using computational modeling.</p><p>We adopt the reinforcement learning framework (RL), which formulates the problem of crediting choices with resulting consequences of an agent interacting with its environment and trying to maximize the expected cumulative reward (<xref ref-type="bibr" rid="bib53">Sutton and Barto, 2018</xref>). A common way of solving this problem involves the learning of a so-called value function, which maps pairs of environmental states and actions to expected future rewards, and then choosing actions that yield the highest value. A popular algorithm that can learn such a function is Q-learning, in which the value function is referred to as the Q-function and expected future rewards are referred to as Q-values (<xref ref-type="bibr" rid="bib55">Watkins, 1989</xref>). The Q-function is updated using the so-called temporal-difference (TD) error, which is computed from an experience’s immediate reward and the Q-function estimates of the future reward (see Materials and methods section for more details). While RL has traditionally been used to solve technical control problems, it has recently been adopted to model animal (<xref ref-type="bibr" rid="bib4">Bathellier et al., 2013</xref>) and human (<xref ref-type="bibr" rid="bib46">Redish et al., 2007</xref>; <xref ref-type="bibr" rid="bib62">Zhang et al., 2018</xref>) behavior.</p><p>RL generally requires many interactions with the environment, which results in slow and inefficient learning. Interestingly, replay of stored experiences, that is, interactions with the environment, greatly improves the speed of learning (<xref ref-type="bibr" rid="bib31">Lin, 1992</xref>). The function of experience replay in RL has been linked to that of hippocampal replay in driving spatial learning (<xref ref-type="bibr" rid="bib27">Johnson and Redish, 2005</xref>). <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref> proposed a model of hippocampal replay as the replay of experiences which have the highest ‘utility’ from a reinforcement learning perspective, meaning that the experience which yields the highest behavioral improvement is reactivated. In their model, the agent’s environment is represented as a so-called grid world, which discretizes space into abstract states between which the agent can move using the four cardinal actions (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). During experience replay, experiences associated with the different transitions in the environment can be reactivated by the agent. Their model accounts for multiple observed replay phenomena. However, it is unclear how the brains of animals could compute or even approximate the computations required by Mattar and Daw’s model. The brain would have to perform hypothetical updates to its network for each experience stored in memory, compute and store the utility for each experience, and then reactivate the experience with the highest utility. Since biological learning rules operate on synapses based on neural activity, it appears unlikely that hypothetical updates can be computed and their outcome stored without altering the network.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The grid world.</title><p>(<bold>A</bold>) An example of a simple grid world environment. An agent can transition between the different states, that is, squares in the grid, by moving in the four cardinal directions depicted by arrowheads. (<bold>B</bold>) Example trajectories of an agent moving in the grid world. (<bold>C</bold>) The successor representation (SR) for one state (green frame). Note that the SR depends on the agent’s actual behavior. (<bold>D</bold>) The default representation (DR) for the same state as in <bold>C</bold>. In contrast to the SR, the DR does not depend on the agent’s actual behavior and is equivalent to the SR given random behavior.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig1-v2.tif"/></fig><p>Here, we introduce a model of hippocampal replay which is driven by experience and environmental structure, and therefore does not require computing the hypothetical update that a stored experience would lead to, if it were reactivated. Especially during early learning, the sequences generated by our proposed mechanism are often the optimal ones according to Mattar and Daw, and our replay mechanism facilitates learning in a series of spatial navigation tasks that comes close to the performance of Mattar and Daw’s model. Furthermore, we show that a variety of hippocampal replay statistics emerges from the variables that drive our model. Hence, our model could be seen as an approximation of Mattar and Daw’s model that avoids the computation of hypothetical updates at a small cost to learning performance.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Using structural knowledge and the statistics of experience to prioritize replay</title><p>We propose a model for the prioritization of experiences, which we call <italic>Spatial structure and Frequency-weighted Memory Access</italic>, or SFMA for short. The model was conceived with simplified grid world environments in mind (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each node in the grid corresponds to an environmental state. During behavior a reinforcement learning agent transitions between nodes and stores these transitions as experience tuples <inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <italic>s</italic><sub><italic>t</italic></sub> is the current state, <italic>a</italic><sub><italic>t</italic></sub> is the action executed by the agent, <italic>r</italic><sub><italic>t</italic></sub> is the reward or punishment received after transitioning, and <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the next state.</p><p>Reactivating stored experiences is viewed as analogous to the reactivation of place cells during replay events in the hippocampus. Assuming an experience <italic>e</italic><sub><italic>t</italic></sub> has just been reactivated, each stored experience <inline-formula><mml:math id="inf3"><mml:mi>e</mml:mi></mml:math></inline-formula> is assigned a priority rating <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> based on its strength <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, its similarity <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <italic>e</italic><sub><italic>t</italic></sub> and inhibition <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> applied to it (<xref ref-type="fig" rid="fig2">Figure 2A</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Illustration of the Spatial structure and Frequency-weighted Memory Access (SFMA) replay model.</title><p>(<bold>A</bold>) The interaction between the variables in our replay mechanism. Experience strength <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, experience similarity <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and inhibition of return <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are combined to form reactivation prioritization ratings. Reactivation probabilities are then derived from these ratings. (<bold>B</bold>) Experience tuples contain two states: the current state and next state. In the default mode, the current state of the currently reactivated experience (violet circle) is compared to the current states of all stored experiences (blue arrows) to compute the experience similarity <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the reverse mode, the current state of the currently reactivated experience is compared to the next state of all stored experiences (red arrows). (<bold>C</bold>) Example of the similarity of experiences to the currently reactivated experience (green arrow) in an open field for the default and reverse modes. Experience similarity is indicated by the colorbar. In the default mode, the most similar experiences are the current state or those nearby. In the reverse mode, the most similar experiences are those that lead to the current state.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Step by step example of a replay sequence generated by SFMA.</title><p>From left to right: the different variables of SFMA as well as the reactivated experiences. Experience strength <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> had the same value for all experiences and was therefore omitted. From top to bottom: the replay steps from start to finish. Replay was initiated at the center of the environment. Note that for replay initialization the prioritization variables do not convey information and were therefore omitted.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Reconciling non-local replays and preferential replay for current location in online replay.</title><p>(<bold>A</bold>) Histogram of starting locations of <italic>offline</italic> replay recorded in the virtual version of Gupta et al.’s experiment. Raw occurrences for each bin were divided by the maximum number of bin occurrences to account for uneven distribution of distance bins. Replays were recorded at either of the reward locations. Current locations are over-represented, but replay is also initiated at other locations (non-local replay). (<bold>B</bold>) Same as in A, but in an open field environment with no reward and homogeneous experience strengths. Replays were recorded at different locations in the environment. Non-local replays are prevalent, but there is no preference for the current location – contrary to observations of <italic>online</italic> replay. (<bold>C/D</bold>) Same as A/B, but the experience strengths close to current location were increased to model an initialization bias. In both environments, the current location is more strongly represented by replays and non-local replays occur. (<bold>C/D</bold>) Same as <bold>C/D</bold>, but with a stronger increase of experience strengths. The current location is more even more strongly represented by replays.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Possible replay modes and example replay trajectories.</title><p>(<bold>A</bold>) Experience similarities of the different replay modes, that is, default, reverse, forward and attractor mode, for a given experience (marked in green). (<bold>B</bold>) Example replay trajectories generated in an open field environment with the different replay modes (from left to right: default, reverse, forward, and attractor mode). Experience strengths were homogeneous and replay was initiated in the center of the environment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig2-figsupp3-v2.tif"/></fig></fig-group><p>The experience strength <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is modulated by the frequency of experience and reward.</p><p>The similarity <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between e and <italic>e</italic><sub><italic>t</italic></sub> reflects the spatial distance between states, taking into account structural elements of the environment, such as walls and other obstacles. This rules out the use of Euclidean distances. A possible candidate would be the successor representation (SR) (<xref ref-type="bibr" rid="bib15">Dayan, 1993</xref>). Given the current state state <italic>s</italic><sub><italic>j</italic></sub>, the SR represents as a vector the likelihood of visiting the other states <italic>s</italic><sub><italic>i</italic></sub> in the near future (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This likelihood is commonly referred to as the discounted future occupancy. Since states separated by a barrier are temporally more distant to each other than to those without a barrier, their expected occupancy is reduced more by discounting. However, the SR suffers from two disadvantages. First, the SR depends on the agent’s current behavior, i.e., the agent’s policy, which will distort the distance relationships between states unless that agent behaves randomly. Second, if the structure of the environment changes, the SR has to be relearned completely, and this may be complicated further by the first problem. These problems were recently addressed by <xref ref-type="bibr" rid="bib45">Piray and Daw, 2021</xref> in the form of the default representation (DR) (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Unlike the SR, the DR does not depend on the agent’s policy, but rather on a default policy, which is a uniform action policy, that is, random behavior, and the DR was shown to reflect the distance relationships between states even in the presence of barriers. Importantly, <xref ref-type="bibr" rid="bib45">Piray and Daw, 2021</xref> demonstrated that the DR can be efficiently updated if the environmental structure changes using a low-rank update matrix. We chose to base experience similarity on the DR due to its advantageous features.</p><p>Since experience tuples contain two states – the current state <italic>s</italic><sub><italic>t</italic></sub> and the next state <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> – we consider two ways of measuring the similarity between experiences. In the default mode, we compare the current states of two experiences (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In the reverse mode, the current state of the most recently reactivated experience is compared to the next states of the other experiences. We found that default and reverse modes tend to generate different kinds of sequences. Two more replay modes could be defined in our model, which we will not consider in this study. We explain this choice in the Discussion.</p><p>After an experience <inline-formula><mml:math id="inf16"><mml:mi>e</mml:mi></mml:math></inline-formula> has been reactivated, inhibition <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is applied to prevent the repeated reactivation of the same experience. Inhibition is applied to all experiences sharing the same starting state <italic>s</italic><sub><italic>t</italic></sub>, i.e., inhibition is set to <inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and decays in each time step by a factor of <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Inhibition values are maintained for one replay epoch and are reset when a new replay epoch is initiated.</p><p>The next experience to be reactivated is randomly chosen according to the activation probabilities <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which are computed by applying a customized softmax function on <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see Materials and methods). If the priority ratings for all experiences falls below a threshold <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> replay is stopped. The definition of one replay epoch is summarized in Algorithm 1. Sequences of replay are produced by iteratively activating and inhibiting individual experiences. Together, experience strengths and experience similarities guide replay while inhibition promotes the propagation of sequences (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>To initiate replay, we consider two cases: replay during awake resting states (online replay) is initiated at the agent’s current position, whereas replay during sleep (offline replay) is initiated with an experience randomly selected from memory based on the relative experience strengths. We chose different initialization schemes since awake replay has been reported to start predominantly at the animal’s current position (<xref ref-type="bibr" rid="bib14">Davidson et al., 2009</xref>). However, there are also non-local sequences in awake replay (<xref ref-type="bibr" rid="bib28">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref>; <xref ref-type="bibr" rid="bib41">Ólafsdóttir et al., 2017</xref>). Non-local replays are generated by our model in offline replay, albeit with a weaker bias for the current position (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A, B</xref>). To model awake replay, we could increase the bias in the random initialization by raising the experience strength for experiences associated with the current position (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–F</xref>), while preserving some non-local replays. However, for simplicity we opted to simply initiate awake replay at the current location.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Algorithm 1. Spatial structure and Frequency-weighted Memory Access (SFMA)</th></tr></thead><tbody><tr><td align="left" valign="top"><bold>Require:</bold>  <italic>e</italic><sub><italic>t</italic></sub> (Replay initiated)<break/>1:  <bold>for</bold> t=1:N <bold>do</bold><break/>2:   <bold>for</bold> experience <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold><break/>3:    Compute priority rating <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.<break/>4:   <bold>end for</bold><break/>5:   <bold>if</bold> <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>6:    Stop replay<break/>7:  <bold>end if</bold><break/>:   Compute reactivation probabilities <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.<break/>9:  Choose next experience <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to reactivate.<break/>10:  Reduce/decay inhibition for all stored experiences.<break/>11:  Inhibit experience <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.<break/>12: <bold>end for</bold></td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>SFMA facilitates spatial learning</title><p>We begin by asking what benefit a replay mechanism such as implemented in SFMA might have for spatial learning. To do so, we set up three goal-directed navigation tasks of increasing difficulty: a linear track, a square open field, and a labyrinth maze (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Simulations were run for the default and reverse replay modes and compared to an agent trained without experience replay, an agent trained with random experience replay, and the state-of-the-art Prioritized Memory Access model (PMA) by <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Statistics of replay has large impact on spatial learning.</title><p>(<bold>A</bold>) The three goal-directed navigation tasks that were used to test the effect of replay on learning: linear track, open field and maze. In each trial, the agent starts at a fixed starting location S and has to reach the fixed goal location G. (<bold>B</bold>) Performance for different agents measured as the escape latency over trials. Shown is the performance for an online agent without replay (black), an agent that was trained with random replay (blue), our SFMA model (green), and the Prioritized Memory Access model (PMA) by <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref> (red). The results of our SFMA model are further subdivided by the replay modes: default (solid), reverse (dash-dotted), and dynamic (dashed). Where the dashed and dash-dotted green lines are not visible they are overlapped by the red solid line. (<bold>C</bold>) Reverse and dynamic replay modes produce more optimal replays while the default replay mode yields pessimistic replays. Shown is the number of optimal updates in the replays generated on each trial for different replay modes: default (solid), reverse (dash-dotted), and dynamic (dashed). Note, that in later trials there is a lack of optimal updates because the learned policy is close to the optimal one and any further updates have little utility. (<bold>D</bold>) Directionality of replay produced by the default (solid) and reverse (dash-dotted) modes in the three environments. The reverse replay mode produces replays with strong reverse directionality irrespective of when replay was initiated. In contrast, the default mode produces replays with a small preference for forward directionality. After sufficient experience with the environment the directionality of replays is predominantly forward for replays initiated at the start and predominantly reverse for replays initiated at the end of a trial.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Directionality of consecutive replay pairs.</title><p>(<bold>A</bold>) Pairs produced on a linear track when using the default mode for replays generated at the start (top) and end (bottom) of a trial. A pair of consecutively reactivated experiences <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>e</italic><sub><italic>t</italic></sub> was considered forward when the next state of <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was the current state of <italic>e</italic><sub><italic>t</italic></sub>, reverse when the current state of <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was the next state of <italic>e</italic><sub><italic>t</italic></sub> and unordered otherwise. (<bold>B</bold>) Pairs produced on a linear track when using the reverse mode. (<bold>C/D</bold>) Same as A/B, but in an open field environment. (<bold>E/F</bold>) Same as A/B, but in a labyrinth environment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Example replay trajectories in different spatial navigation tasks.</title><p>(<bold>A</bold>) Replay trajectories generated at the start and end of trials in the linear track for different replay modes (default and reverse) and different stages of learning (early and late). Color indicates when in a replay event a transition was replayed. (<bold>B</bold>) Same as A, but in an open field. (<bold>C</bold>) Same as A, but in a labyrinth like in <xref ref-type="bibr" rid="bib57">Widloski and Foster, 2022</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Reward changes trigger higher probability of activating the reverse mode.</title><p>(<bold>A</bold>) The probability of replay being generated in the reverse mode for agents trained in open field (left) and T-maze (right) for 300 trials. Reward was changed once after 100 trials (+0.1) and again after another 100 trials (–0.1). The probability for reverse mode spikes once in the beginning when the reward was first encountered and when the reward changes. (<bold>B</bold>) The percentage change of reverse mode activation after reward change. Percentage change was computed from the five trial average before and after the reward change.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>The number of sequences decreases with experience.</title><p>The number of sequences per replay as a function of experienced trials for three different environments, that is, linear track, open field and labyrinth. The replay lengths were 10, 10 and 50, respectively. Early during learning replay contains multiple short sequences. By the end of learning replay tends to represent one long sequence in linear track and open field environments. For the labyrinth environment, the number of sequences is still decreasing due to the complexity of the environment and the much higher replay length.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig3-figsupp4-v2.tif"/></fig></fig-group><p>Using the reverse mode, our model clearly outperformed the agents trained without replay and with random replay (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Importantly, performance was close to, even if slightly below, that of PMA. Learning performance of SFMA critically depends on the replay mode. The default mode yielded a performance that was much lower than the reverse mode and only slightly better than random replay. Considering its low performance, the default mode may appear undesirable, however, it is important for two reasons. First, it generates different types of sequences from the reverse mode (<xref ref-type="fig" rid="fig3">Figure 3D</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) and these sequences are more consistent with some experimental observations. While the reverse mode mostly generates reverse sequences in all situations, the default mode generates forward replay sequences at the beginning of trials and reverse sequences at the end of trials. This pattern of changing replay directions has been observed in experiments in familiar environments (<xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>). Also, as we will show below the default mode generates shortcut replays like those found by <xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref>, whereas the reverse mode does not. Second, the recent discovery of so-called pessimistic replay, that is, replay of experiences which are not optimal from a reinforcement learning perspective, show that suboptimal replay sequences occur in human brains (<xref ref-type="bibr" rid="bib21">Eldar et al., 2020</xref>) for a good reason (<xref ref-type="bibr" rid="bib2">Antonov et al., 2022</xref>). Such suboptimal replays are better supported by the default mode (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>We therefore suggest that the reverse and default modes play distinct roles during the learning progress. Early in the learning session or following reward changes, when the environment is novel and TD errors are high, the reverse mode is preferentially used to acquire a successful behavior quickly (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Indeed, in our simulations the reverse mode produces more updates that were optimal, that is, they improved the agent’s policy the most, than did the default mode (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The preponderance of reverse replay sequences during early learning is consistent with experimental observations in novel environments (<xref ref-type="bibr" rid="bib22">Foster and Wilson, 2006</xref>) or after reward changes (<xref ref-type="bibr" rid="bib1">Ambrose et al., 2016</xref>). Later in the learning session, when the environment has become familiar and TD errors are low, the default mode is preferentially used to make the learned behavior more robust. The default mode then accounts for interspersed reverse and forward replay sequences in familiar environments. We provide a more detailed rationale for the default mode in the Discussion. We call this strategy of switching between the reverse and default modes the <italic>dynamic</italic> mode of SFMA. Put simply, in the dynamic mode the probability of generating replay using the reverse mode increases with the TD errors accumulated since the last trial (for more details see Materials and methods). It yields a learning performance (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and number of optimal updates (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) that are similar to the reverse mode.</p><p>In the following, we focus on the statistics of replay that SFMA generates in a range of different experimental settings.</p></sec><sec id="s2-3"><title>Near-homogeneous exploration of an open environment explains random walk replays</title><p>We first investigated the replay statistics that our model produces in a simple environment without navigational goals similar to the experiment of <xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref>. To this end, we created a virtual square grid world environment of size 100 × 100 without rewards. For simplicity, we first set experience strengths to the same value, <inline-formula><mml:math id="inf32"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, to reflect homogeneous exploration of the environment. All replays were generated using the default mode since the environment was familiar to the animal in the experiments, but using the reverse mode did not affect the results (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>With this setup the trajectories represented by the replayed experiences of the agent are visually similar to random walks (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The displacement distribution of replayed locations indicates that replay slowly diffuses from its starting location (<xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). To analyze the trajectories more systematically, we used the Brownian Diffusion Analysis also used by <xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref>. In this analysis, a random walk is described by a power law relationship between the average distance between two replayed positions <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and the time interval <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="inf35"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. Indeed, the simulated replays exhibit a linear relationship in the log-log-plot indicating a power law between the two variables (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) and the slope is close to the theoretical value for a random walk <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. This result is robust across a large range of model parameters, the most relevant are the DR’s discount factor <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and inhibition decay <inline-formula><mml:math id="inf39"><mml:mi>λ</mml:mi></mml:math></inline-formula>, and the range of values in our simulations, <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.467</mml:mn><mml:mo>,</mml:mo><mml:mn>0.574</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), is a good match to the values reported by <xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref>. (<inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.45</mml:mn><mml:mo>,</mml:mo><mml:mn>0.53</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). The values for the diffusion coefficient (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), which relate to the reactivation speed, are similarly robust and only affected when the decay factor is close to zero. Hence, our model robustly reproduces the experimental findings.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Replays resemble random walks across different parameter values for Default Representation (DR) discount factor and inhibition decay.</title><p>(<bold>A</bold>) Example replay sequences produced by our model. Reactivated locations are colored according to recency. (<bold>B</bold>) Displacement distributions for four time steps (generated with <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) A linear relationship in the log-log plot between average distance of replayed experiences and time-step interval indicates a power-law. Lines correspond to different values of the DR’s discount factor <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as indicated by the legend. (<bold>D</bold>) The anomaly parameters (exponent <inline-formula><mml:math id="inf44"><mml:mi>α</mml:mi></mml:math></inline-formula> of power law) for different parameter values of DR and inhibition decay. Faster decay of inhibition, which allows replay to return to the same location more quickly, yields anomaly parameters that more closely resemble a Brownian diffusion process, that is, closer to 0.5. (<bold>E</bold>) The diffusion coefficients for different parameter values of DR and inhibition decay. Slower decay of inhibition yields higher diffusion coefficients.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Replays generated using the reverse mode also resemble random walks across different parameter values.</title><p>Replays generated using the reverse mode also resemble random walks across different parameter values for Default Representation (DR) discount factor and inhibition decay. (<bold>A</bold>) Displacement distribution for the first four time steps. (<bold>B</bold>) Our model can reproduce the linear log-log relationship between time-step interval and average distance of replayed experiences. (<bold>C</bold>) The anomaly parameters for different parameter values of DR and inhibition decay. Faster decay of inhibition yields anomaly parameters which closer resemble a Brownian diffusion process (i.e. closer to 0.5). (<bold>D</bold>) The diffusion coefficients for different parameter values of DR and inhibition decay. Slower decay of inhibition yields higher diffusion coefficients.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Displacement distributions for different inverse temperature values.</title><p>Displacement distributions for different inverse temperature values (<inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0.9</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>λ</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0.9</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>A</bold>) Displacement distribution for the first four time steps with <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> given homogeneous experience strength. (<bold>B</bold>) The same as A, but with a higher inverse temperature.<inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> Note the lower local variance of the distribution compared to A. (<bold>C</bold>) The same as A, but given heterogeneous experience strengths. (<bold>D</bold>) The same as C, but with.<inline-formula><mml:math id="inf49"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> Note the negligible difference between homogeneous and heterogeneous experience strengths.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>For heterogeneous experience strengths replays also resemble random walks across different parameter values.</title><p>For heterogeneous experience strengths replays also resemble random walks across different parameter values for Default Representation (DR) discount factor and inhibition decay. (<bold>A</bold>) Homogeneous experience strength condition. (<bold>B</bold>) Heterogeneous experience strength condition. (<bold>C</bold>) Our model can reproduce the linear log-log relationship between time-step interval and average distance of replayed experiences. (<bold>D</bold>) The anomaly parameters for different parameter values of DR and inhibition decay. Faster decay of inhibition yields anomaly parameters which closer resemble a Brownian diffusion process (i.e., closer to 0.5). (<bold>E</bold>) The diffusion coefficients for different parameter values of DR and inhibition decay. Slower decay of inhibition yields higher diffusion coefficients.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>The starting positions of replay are randomly distributed across the environment.</title><p>The starting positions of replay are randomly distributed across the environment in a 2-d open field. (<bold>A</bold>) Distribution of replay starting positions given homogeneous experience strengths. Starting positions are distributed (&gt;90% randomness) evenly across the environment. Randomness of starting locations was measured like in <xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref>. (<bold>B</bold>) Initial direction of replays are evenly distributed given homogeneous experience strengths. The environment was divided into bins of size 20x20. (<bold>C/D</bold>) Same as A/B, but with heterogeneous experience strengths.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig4-figsupp4-v2.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Prioritized Memory Access’ (PMA) ability to produce sequences is severely disrupted when the gain calculation for n-step updates is adjusted.</title><p>(<bold>A</bold>) Sequences generated by PMA (<xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>) given uniform need and all-zero gain. Because utility is defined as a product of gain and need, the gain must have a nonzero value to prevent all-zero utility values. This is achieved by applying a (small) minimum gain value. For n-step updates, which update the Q-function for all n steps along the trajectory and are the main driver for forward replay sequences in PMA, the gains are summed across all n steps. <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref> apply the minimum gain value before summation, which artificially increases the gain of n-step updates. (<bold>B</bold>) Same as A, but the minimum gain value is enforced after summing gain along the steps of the n-step update. We argue that the minimum value should be applied after the summation of gain values, since otherwise the gain artificially increases with sequence length. If done so, PMA loses the ability to produce forward sequences and starts to randomly reactivate experiences.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig4-figsupp5-v2.tif"/></fig></fig-group><p>Since the results above were obtained assuming homogeneous exploration of the environment, we repeated our simulations with heterogeneous experience strengths (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B</xref>). While the relationships in the log-log-plot seems to slightly deviate from linear for very large time-step intervals, the statistics still largely resemble a random walk (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref>). The range of values for <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.410</mml:mn><mml:mo>,</mml:mo><mml:mn>0.539</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> still covers the experimental observations, albeit shifted toward smaller values.</p><p>Stella et al. further reported that the starting locations of replay were randomly distributed across the environment and replay exhibited no preferred direction. Our model reproduces similar results in the case of homogeneous experience strengths (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4A, B</xref>) and heterogeneous experience strengths (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4C, D</xref>). The results of our simulations suggest that replay resembling random walks can be accounted for given near-homogeneous exploration of an open-field environment. If exploration is non-homogeneous, the statistics of a random walk hold only for short to medium time-step intervals.</p></sec><sec id="s2-4"><title>Stochasticity results in shortcut replays following stereotypical behavior</title><p><xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref> provided further evidence that replay does not simply reactivate previously experienced sequences, by showing that replay sequences sometimes represent trajectories that animals were prevented from taking. These so-called shortcut sequences were synthesized from previously experienced trajectories. We constructed a simplified virtual version of the Gupta et al. experiment (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) to test whether, and under which conditions, our proposed mechanism can produce shortcut replays. In the experiment, animals exhibited very stereotypical behavior, that is, they ran laps in one direction and were prevented from running back. Therefore, in our model the agent was forced to run one of three predefined patterns: right laps, alternating laps, and left laps (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). This allowed us to focus on the effect of different specific behavioral statistics on replay. The virtual agent was made to run 20 trials in one session and replays were simulated after each trial.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Replay of shortcuts results from stochastic selection of experiences and the difference in relative experience strengths.</title><p>(<bold>A</bold>) Simplified virtual version of the maze used by <xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref>. The agent was provided with reward at specific locations on each lap (marked with an R). Trials started at bottom of the center corridor (marked with an S). At the decision point (marked with a D), the agent had to choose to turn left or right in our simulations. (<bold>B</bold>) Running patterns for the agent used to model different experimental conditions: left laps, right laps and alternating laps. Replays were recorded at the reward locations. (<bold>C</bold>) Examples of shortcut replays produced by our model. Reactivated locations are colored according to recency. (<bold>D</bold>) The number of shortcut replays pooled over trials for different running conditions and values of the inverse temperature <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula>. Conditions: alternating-alternating (AA), right-left (RL), right-alternating (RA), and alternating-left (AL). (<bold>E</bold>) Learning performance for different replay modes compared to random replay. The agent’s choice depended on the Q-values at the decision point. The agent was rewarded for turning right during the first 100 trials after which reward shifted to the left (red line marks the shift).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Number of shortcut replays on a trial by trial basis differs depending on behavioral statistics.</title><p>Shortcut replays in each trial for different experimental conditions: alternating-alternating (AA), right-left (RL), right-alternating (RA) and alternating-left (AL). For all conditions the number of shortcut replays is affected by the choice of the inverse temperature parameter.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Experience strengths resulting from different behavioral statistics in <xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref> experiment.</title><p>Experience strengths after the first (left panels) and second half (right panels) given the different behavioral statistics (rows). For simplicity, the experience strength shown in each state is the sum over the four potential actions. Conditions: alternating-alternating (AA), right-left (RL), right-alternating (RA), and alternating-left (AL).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Following strongly stereotypical behavior efficient learning occurs for the default mode after a change in goal location.</title><p>Top: he learning performance for different replay modes in a T-maze environment. The goal arm changes after 300 trials. For learning of the initial goal, the default mode is clearly outperformed by reverse and dynamic modes. However, there is no difference in learning performance between the different modes for learning the new goal location. Bottom: Same as for the top panel, but for an open field (Box), which allows for more variability in behavior (i.e. less stereotypical behavior). Learning performance is overall worse for all modes. The default mode is outperformed by the other two modes for both goal locations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig5-figsupp3-v2.tif"/></fig></fig-group><p>In the first 10 trials, the agents ran in one pattern, and in the next 10 trials, the agents used the same or a different pattern. To model Gupta et al.’s study, we ran simulations with the combinations right-left, right-alternating, and alternating-left. In addition, the combination alternating-alternating served as a baseline condition in which the left and right laps had roughly the same amount of experience throughout the simulation.</p><p>In this case, only the default mode could reproduce the experimental findings, while the reverse mode could not. The main cause lay within the large dissimilarity between the experiences at the decision point (D in <xref ref-type="fig" rid="fig5">Figure 5A</xref>) and the experiences to the left and right of it, which in turn drove prioritization values to zero.</p><p>We found that our model produces shortcut-like replays for each run pattern combination (<xref ref-type="fig" rid="fig5">Figure 5C and D</xref>). Shortcuts occurred in higher numbers for right-left and right-alternating combinations, and mainly in the last 10 trials, in trials when the agent was running a left lap, that is, every trial for right-left and every other trial for right-alternating (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Across the whole running session, for alternating-alternating and alternating-left combinations a lower number of shortcuts occurred. This difference between combinations results from the balance of experience strengths associated with either of the laps and with the center piece. For right-alternating and right-left combinations the experience strengths associated with right lap and center piece were similar and therefore replay, when initiated at the left lap’s reward location, was not biased to reactivate experiences along the center piece. The number of shortcut replays then decreased as the experience strengths associated with the center piece increased relative to those associated with either of the laps. Since for alternating-left and alternating-alternating combinations the experience strengths associated with the center piece already were higher relative to those associated with the laps an overall low but constant number of shortcut replays was produced.</p><p>Across all conditions the number of shortcut replays decreased with increasing inverse temperature <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula>, which transforms priority ratings to reactivation probabilities. The larger <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula>, the more deterministic replay becomes. This result therefore suggests that shortcut replay is driven by stochasticity in the prioritization process (<xref ref-type="fig" rid="fig5">Figure 5D</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) and differences in relative experience strengths (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) of the maze’s center piece and arms. These differences are in turn induced by the specific behavioral statistics.</p><p>Finally, we wanted to know if replay facilitates learning in this task. Since the agent cannot remember, which lap it ran in a previous trial, it would not be able to learn with alternating rewarded locations. Therefore, we chose the right-left condition as the learning task. To deal with problematic behavior of the simple RL agents mentioned above we adjusted our simulation to only consider the action selected at the decision point and let the agent run straight otherwise. We ran simulations in default and reverse mode. An agent trained with random experience replay was used as the baseline. All agents solved the task and successfully learned the new goal location after the rewarded locations changed from right to left (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). Both default and reverse mode yielded faster learning than random experience replay. However, there was no large difference between the replay modes during the first half. These small differences in learning performance suggest that replaying strictly in reverse mode may not be necessary in tasks with stereotypical behavior since the resulting experience strengths would naturally guide replays in forward or reverse depending on where replay started (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p></sec><sec id="s2-5"><title>Dynamic structural modulation allows replay to adapt to environmental changes</title><p>Recently, <xref ref-type="bibr" rid="bib57">Widloski and Foster, 2022</xref> reported that hippocampal replay dynamically adapts to daily changes in the structure of the environment. We argue that this type of replay can be accounted for by representing the environmental structure dynamically with the default representation (DR) (<xref ref-type="bibr" rid="bib45">Piray and Daw, 2021</xref>). While the SR (<xref ref-type="bibr" rid="bib15">Dayan, 1993</xref>) could also be used to represent the environment’s structure, it cannot be updated efficiently in the face of structural changes and depends on the agent’s policy. The DR does not suffer from these problems. Specifically, we employed the DR by separating the spatial representation into open 2D space and information about environmental barriers. Barrier information was provided when the environment changed (see Methods).</p><p>To test whether our proposed replay mechanism can adapt to changes of the environment’s structure, we simulated replays in three distinct consecutive environments (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and evaluated the occurrence of replay crossing environmental barriers. All replays were generated using the default mode, but using the reverse mode did not affect these results (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Our proposed replay mechanism successfully adapts to changes in the environment’s structure as illustrated by replays diffusing along barriers in the reactivation maps (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and shown by the absence of invalid transitions (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In contrast, when experience similarity is based on the Euclidean distance between locations, replay does not conform or adapt to the changing environmental structure (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The default representation (DR) allows replay to adapt to environmental changes.</title><p>(<bold>A</bold>) Reactivation maps, that is, the fraction of state reactivations, for all replays recorded in three environments while the agent is located at the white square. Experience similarity is based on the DR with a discount factor <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>. The colorbar indicates the fraction of reactivations. Note that replay obeys the current environmental boundaries. (<bold>B</bold>) Same as in A, but experience similarity is based on the Euclidean distance between states. Note that replay ignores the boundaries. (<bold>C</bold>) The fraction of invalid transitions during replay in different environments for the DR (blue) and Euclidean distance (red). While replay can adapt to environmental changes when using the DR, it does not when experience similarity is based on the Euclidean distance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Reverse mode replays adapt to environmental changes.</title><p>(<bold>A</bold>) Reactivation maps, that is, the fraction of state reactivations, for reverse mode replay recorded in three environments (<inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>). The color bar indicates the fraction of reactivations across all replays. Note that replay obeys the current environmental boundaries. (<bold>B</bold>) Same as in A, but experience similarity is based on the Euclidean distance between states. Note that replay ignores the boundaries. (<bold>C</bold>) The fraction of invalid transitions during reverse model replay in different environments for the DR (blue) and Euclidean distance (red). While replay can adapt to environmental changes when using the DR, it does not when experience similarity is based on the Euclidean distance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig6-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Preplay of unvisited locations requires structural modulation and visual exploration</title><p><xref ref-type="bibr" rid="bib40">Ólafsdóttir et al., 2015</xref> reported the preplay of trajectories through a part of a T-maze that the animal could see, but not physically explore, and that contained a visible reward. Our model can provide an intuitive explanation for the reported phenomenon. First, the use of DR explains, in principle, why the preplay of unvisited locations is possible. The DR allows for the representation of the environment’s structure provided it is observable. Still, these experiences would have zero experience strength in our model and therefore would not be reactivated. However, during the reward cueing phase, the animals were reported to have paid attention to the reward cue. We view this as a form of visual exploration that increases the strength of experiences along the attended arm and updates the reward information related to the experiences.</p><p>We set up a virtual version of the <xref ref-type="bibr" rid="bib41">Ólafsdóttir et al., 2017</xref> task (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) in which the agent first runs up and down the maze stem ten times with rewards provided at each end of the stem (run phase). In the cued phase, we increased experience strengths of the blocked arms by a small amount relative to actual exploration (attention strength), which was larger for the cued than the uncued arm (asymmetric allocation of attention). Then experiences were preplayed. Template matching was used to detect the preplay of cued and uncued arms, and replay of the stem. All preplays were generated using the default mode, but results were similar when using the reverse mode (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). We found that our model indeed produces preplays of the cued arm and only rarely of the uncued arm (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). As expected, the attention strength as well as the asymmetric allocation of attention are key to produce preplays (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). The higher the value of these two parameters, the more often the cued arm is preplayed.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Preplay of cued, but unvisited, locations can be explained by visual exploration.</title><p>(<bold>A</bold>) Schematic of the simplified virtual maze and task design used to model study by <xref ref-type="bibr" rid="bib40">Ólafsdóttir et al., 2015</xref>. First, in an initial run phase, the agent ran up and down the stem of the maze. Second, in the following visual exploration phase, the agent visually explores a cued part of the maze and increases the strength of the associated experiences. Third, experiences are reactivated according to their priority scores. Fourth, agents are given access to the entire T-maze. The fraction of choosing the two arms in the test phase are compared. (<bold>B</bold>) Fractions of reactivated arm locations. Experiences associated with the cued arm (right) are preferentially reactivated. (<bold>C</bold>) The fractions of choosing either arm before and after training the agent with preplay. Before preplay the agent shows no preference for one arm over the other. After preplay, the agent preferentially selects the cued arm over the uncued arm. (<bold>D</bold>) The percentage of cued-arm preplay (out of all arm preplays and stem replays) for different amounts of attention paid to the cued arm vs. the uncued arm (asymmetric allocation of attention) and experience strength increase relative to actual exploration (attention strength).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Preplay of yet unvisited cued locations can be explained by visual exploration (reverse mode).</title><p>(<bold>A</bold>) Fractions of reactivated arm locations. Experiences associated with the cued arm (right) are preferentially reactivated. (<bold>B</bold>) The fractions of choosing the cued arm and uncued arm before and after training the agent with preplay. The agent preferentially selects the cued arm after preplay. (<bold>C</bold>) The percentage of cued-arm preplay for different amounts of attention paid to the cued arm vs. the uncued arm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig7-figsupp1-v2.tif"/></fig></fig-group><p>Furthermore, <xref ref-type="bibr" rid="bib41">Ólafsdóttir et al., 2017</xref> reported that animals preferred the cued arm when the barrier blocking the arms was removed and they were given physical access to the maze. Our model shows the same preference for the cued arm after training an agent with simulated replays and testing it in the T-maze (<xref ref-type="fig" rid="fig7">Figure 7C</xref>).</p></sec><sec id="s2-7"><title>Reward modulation of experience strength leads to over-representation of rewarded locations</title><p>Replay is known to over-represent rewarded locations (<xref ref-type="bibr" rid="bib50">Singer and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib44">Pfeiffer and Foster, 2013</xref>). This might be accounted for in our model, since the experience strength <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is modulated not only by the frequency of experience but also by the frequency of reward. However, it is not intuitively clear how their relative contribution affect the statistics of replay. We therefore tested different values for reward modulation, that is, the increase of experience strength due to reward, in goal-directed navigation tasks in open field and T-maze environments. The default value used in other simulations was one. These simuluations were compared to those with a reward modulation of ten. Note, in our model changing the reward modulation is equivalent to changing the magnitude of reward.</p><p>Training lasted for 100 trials. Online replays were generated at the beginning and end of each trial. Additionally, offline replays were also generated after each trial. However, only online replays generated at the end of a trial were used to train the agent. All replays were generated using the default mode.</p><p>When the frequency of experience and reward modulate experience strength equally, replays mainly represent locations along the path leading to the rewarded location (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>). Replays in the open-field environment over-represent the starting location. The rewarded location shows stronger over-representation for online replays occurring at the end of trials partly due to initialization bias at the goal location. In the T-maze, these effects are less pronounced due to replay being constrained by the maze’s structure. Increasing the reward modulation to ten, drives replays to over-represent the rewarded location (<xref ref-type="fig" rid="fig8">Figure 8C and D</xref>). For online replays at the beginning of trials, this effect is not apparent due to the initialization bias at the starting location. For online replays at the end of trials and offline replay, the representation of the path leading to the rewarded location is heavily reduced and that of the goal location is enhanced.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Increasing the reward modulation of experience strengths leads to an over-representation of rewarded locations in replays.</title><p>(<bold>A</bold>) Reactivation maps for online replays generated in an open field environment at trial begin (left) and end (middle) as well as for offline replays (right). Reward modulation was one. Replay tends to over-represent the path leading to the reward location. The starting location is strongly over-represented for all replay conditions, while the the reward location is mainly over-represented for trial end replays. (<bold>B</bold>) Same as A, but in a T-maze. (<bold>C</bold>) Same as A, but due to a higher reward modulation of ten the rewarded location is strongly over-represented in replay. (<bold>D</bold>) Same as C, but in a T-maze.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig8-v2.tif"/></fig><p>To summarize, SFMA can account for the over-representation of rewarded locations reported in the literature and this over-representation depends on the balance of modulation due to experience and reward.</p></sec><sec id="s2-8"><title>Replay enters aversive locations given previous experience, driving the learning of avoidance</title><p>Replay was found to not only form reward-directed sequences, but also, rather counter-intuitively, to enter locations associated with aversive experiences (<xref ref-type="bibr" rid="bib61">Wu et al., 2017</xref>). The specific location and amount of replay reported by <xref ref-type="bibr" rid="bib61">Wu et al., 2017</xref> is not within the scope of our model. Nonetheless, SFMA can give a simple account for why replay would enter aversive locations. In their study, the animals had substantial experience with the whole environment, which consisted of a dark and light zone, over three phases (Run 1, Run 2, and Pre) before shock was introduced in the dark zone. Importantly, the animals initially displayed a strong bias for the dark zone.</p><p>For our simulation, we used a 20-states-long linear track where the left and right half represented dark and light zones, respectively. The left most two states represented the shock zone. We modeled the initial three phases by having an agent run 30 laps on the linear track environment and multiplied the experience strengths associated with the dark and light zones with the occupancy reported by <xref ref-type="bibr" rid="bib61">Wu et al., 2017</xref>. The remainder of the simulation consisted of a single shock trial in which the agent entered the shock zone and received shock of magnitude one, and nine trials in which the agent ran laps in the light zone. Twenty replays of length ten were generated after receiving the shock. While running laps in the light zone, replays were generated when the agent stopped in front of the dark zone. The generated replays preferentially entered the dark zone and reached the shock zone (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). This is due to the agent’s previous experience in the dark zone, which was over-represented relative to the light zone.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Replay will enter aversive zones given previous experience.</title><p>(<bold>A</bold>) The reactivation map indicates that the dark zone (left half), which contains the shock zone, was preferentially replayed even after the shock administered and the agent stopped in the middle. (<bold>B</bold>) Probability of the Q-function pointing away from the shock zone for each state. After the first five trials (Early), avoidance is apparent only for states in the dark zone (excluding the shock zone itself). In the following five trials (Late), the preference for avoidance is propagated to the light zone (right half).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82301-fig9-v2.tif"/></fig><p>Additionally, we investigated the effect of the generated replays on behavior by training the agent and analyzing the policy. The learned policy preferentially points away from the shock zone (<xref ref-type="fig" rid="fig9">Figure 9B</xref>), indicating that the agent learned to avoid the shock zone. Avoidance is stronger following more replay and spreads to the light zone (<xref ref-type="fig" rid="fig9">Figure 9B</xref>, bottom map). Our simulations suggest that previous experience is sufficient for replay to produce sequences that extend to aversive locations and that these sequences could further facilitate the learning of avoidance behavior.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We introduced a replay mechanism (SFMA) that accounts for a variety of different replay statistics reported in the literature. The core aspect of SFMA is the prioritization of experiences according to the frequency at which they occur, the likelihood with which they are rewarded, and the structure of the environment. Furthermore, reactivated experiences are inhibited in the prioritization process to drive the generation of sequential activations. The statistics of these sequences depend on whether SFMA operates in the default or reverse mode. Surprisingly, despite these differences both modes are consistent with diverse experimental results – with the notable exception of shortcut replays, which arise only in the default mode. However, the two modes have very different impact on spatial learning, which is much faster when using the reverse mode. In the following we discuss some details of our modeling assumptions and results.</p><sec id="s3-1"><title>The components of the spatial structure and frequency-weighted memory access (SFMA) model</title><p>The prioritization of experiences during replay is well supported by experimental evidence. Behavioral data suggests that animals use spatial representations to quickly adapt to changes in the environment (<xref ref-type="bibr" rid="bib17">de Cothi et al., 2020</xref>). Structural modulation of hippocampal replay is reported in the literature (<xref ref-type="bibr" rid="bib60">Wu and Foster, 2014</xref>; <xref ref-type="bibr" rid="bib40">Ólafsdóttir et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Widloski and Foster, 2022</xref>). The hippocampus and related structures, for example, entorhinal cortex (EC), are commonly known for containing cells that selectively fire for spatial features (<xref ref-type="bibr" rid="bib38">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib25">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">O’Keefe and Burgess, 1996</xref>). We chose to represent the environment’s structure with the default representation (DR) (<xref ref-type="bibr" rid="bib45">Piray and Daw, 2021</xref>), which is a special case of the successor representation (<xref ref-type="bibr" rid="bib15">Dayan, 1993</xref>). The latter has been proposed to explain firing properties of place and grid cells (<xref ref-type="bibr" rid="bib51">Stachenfeld et al., 2017</xref>), and the DR gives an account for boundary cells in EC (<xref ref-type="bibr" rid="bib45">Piray and Daw, 2021</xref>). Furthermore, we separated the representation of space into a general representation of 2D space and information about environmental barriers. It has been proposed that the hippocampal place cell code is formed by combining a learned representation of the task structure (e.g. space) with sensory information (<xref ref-type="bibr" rid="bib56">Whittington et al., 2020</xref>). We propose that SFMA’s similarity term is likely implemented through spatial representations found in the hippocampus and EC. Namely, place cell activity could provide the hippocampus access to the experience similarities or at least an estimate of them without the need for taxing computation.</p><p>The frequency of experience (<xref ref-type="bibr" rid="bib30">Kudrimoti et al., 1999</xref>; <xref ref-type="bibr" rid="bib42">O’Neill et al., 2008</xref>; <xref ref-type="bibr" rid="bib23">Gillespie et al., 2021</xref>) as well as reward <xref ref-type="bibr" rid="bib44">Pfeiffer and Foster, 2013</xref> have been shown to influence the content of replay. The experience strength term of SFMA is consistent with these experimental findings. Experience has also been incorporated in other models. For instance, the <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref> model uses the Successor Representation which depends on the agent’s previous behavior. However, integrating the frequency of an experience <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the frequency/size of reward <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into one variable, the experience strength, as SFMA does, renders replay less flexible and could negatively affect learning. For instance, extreme over-representation of the rewarded locations may slow down the propagation of reward information to locations far away from them. More flexibility could be introduced by separating the two parameters more clearly and computing experience strength as a weighted average <inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The weighting could then be flexibly adjusted depending on the situation. A potential way for a biological network to implement experience strength could be to increase the overall excitability of place cells after experience (<xref ref-type="bibr" rid="bib43">Pavlides and Winson, 1989</xref>) and especially when reward was encountered (<xref ref-type="bibr" rid="bib50">Singer and Frank, 2009</xref>).</p><p>Inhibition of return could easily be implemented via local inhibitory recurrent connections and experience strength through strengthening of connections between cell assemblies. While the main purpose of inhibition is that it enables the generation of sequences, it can be also viewed as a way of shifting priorities from experiences that were already used during learning to other experiences and, hence, ensuring the propagation of reward information. An interesting question pertains to whether a similar inhibition mechanism is utilized in the hippocampus. It was shown in modeling studies that the generation of sequences in a network does not strictly require inhibition (<xref ref-type="bibr" rid="bib5">Bayati et al., 2015</xref>). However, forms of inhibition have been used to account for replay as well as preplay sequences in biologically plausible models (<xref ref-type="bibr" rid="bib12">Chenkov et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Azizi et al., 2013</xref>). We propose that the existence of inhibition implemented in our model could be tested by measuring the excitability of place cells following their reactivation during replay events.</p><p>In summary, the variables that SFMA uses for prioritization could be realized in an efficient manner by the hippocampus.</p></sec><sec id="s3-2"><title>The role of different replay modes in our model</title><p>While the default mode accounts for all varieties of replay statistics considered, it performs considerably worse during learning than the reverse mode since it does not produce reverse sequences as reliably. Reverse sequences are important for learning because they can propagate rewards information back to preceding actions that led to the reward (<xref ref-type="bibr" rid="bib22">Foster and Wilson, 2006</xref>).</p><p>To benefit from the advantages of both replay modes, we propose that they could play different roles during learning. There is support in the literature for this view. <xref ref-type="bibr" rid="bib22">Foster and Wilson, 2006</xref> found that reverse replay can occur after even a single experience, which is predominantly generated by the reverse mode. Furthermore, the frequency of reverse replay has been observed to increase in response to positive reward changes (<xref ref-type="bibr" rid="bib1">Ambrose et al., 2016</xref>). Hence, the reverse mode might be used preferentially when the situation requires learning and the switch to the reverse mode may be triggered by neurons encoding reward prediction error (<xref ref-type="bibr" rid="bib49">Schultz et al., 1997</xref>). By contrast, <xref ref-type="bibr" rid="bib58">Wikenheiser and Redish, 2013</xref> reported both forward and reverse replay sequences during awake resting phases occurring in roughly equal proportions (for a whole session). In a familiar environment, <xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref> found that replay sequences in forward order at the beginning of a trial prior to running on a linear track and in reverse order at the end of a trial after running. These results are more consistent with the default mode and more balanced replay direction in familiar environments.</p><p>What could the function of the default mode be, if it does not speed up learning much compared to random replay? A possibility could be a supportive function in the RL process. For instance, in the context of deep RL it is well known that multi-layer networks trained with sequences suffer from learning instabilities (<xref ref-type="bibr" rid="bib35">Mnih et al., 2015</xref>). These instabilities are addressed by randomly replaying experiences to break the strong temporal correlations between sequential experiences. While not completely random, the default mode is more likely to break proper reverse and forward sequences. Another possible function that could be implemented by the default mode is the reactivation of experiences which otherwise would not (or only rarely) be reactivated in the reverse mode. Recent findings of pessimistic replay, i.e., the replay of sub-optimal experiences, in humans (<xref ref-type="bibr" rid="bib21">Eldar et al., 2020</xref>) supports this view. Interestingly, follow-up modeling work (<xref ref-type="bibr" rid="bib2">Antonov et al., 2022</xref>) suggests that optimal replay occurs predominantly during initial trials and then switches to pessimistic replay. However, this switch is dependent on the <italic>utility</italic> of experiences (<xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>). The default mode could be viewed as enabling these pessimistic replays, and the switch from optimal to pessimistic replay corresponds to the dynamic mode in our model.</p><p>Alternatively, the default mode might not be directly related to RL, but serve a different function altogether. For instance, the sequences generated in the default mode might be important for maintaining synaptic connections which otherwise would not be maintained due to the specific distribution of reverse replays (<xref ref-type="bibr" rid="bib26">Hebb, 1949</xref>; <xref ref-type="bibr" rid="bib9">Caporale and Dan, 2008</xref>). Another function often attributed to hippocampal replay is that of planning (<xref ref-type="bibr" rid="bib18">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib6">Buckner, 2010</xref>; <xref ref-type="bibr" rid="bib7">Buhry et al., 2011</xref>) by simulating possible future trajectories to inform subsequent behavior. The default mode could, in principle, support this function. In contrast, since the reverse mode almost exclusively produces reverse sequences, it could not support the prospective sequences needed for planning.</p><p>Two further replay modes can be defined in our model, which we did not consider in our study. An attractor mode, which compares the next state of experience <italic>e</italic><sub><italic>t</italic></sub> with the next state of all stored experiences e. Hence, this mode looks for experiences that end in the same state, so that neighboring experiences are activated in a ring-like fashion (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Since we are not aware of any experimental studies reporting such ring-like replay, we did not consider this mode in our study. Another possible replay mode, is the forward mode, which compares the next state of experience <italic>e</italic><sub><italic>t</italic></sub> with the current state of all stored experiences e. The forward mode exclusively produces forward replays, which are not efficient for learning, but could very well efficiently support planning. The latter is, however, outside the scope of the current study. Furthermore, the forward mode is not as flexible as the default mode in generating transitions between states that were never (or rarely) experienced, and replaying such transitions could be useful for discovering novel solutions.</p><p>How might default and reverse modes be implemented by a biological network? The default mode could be implemented as an attractor network where each experience’s current state is stored as an attractor. The currently reactivated experience/state would be represented by a bump of activity in the network. Due to self-inhibition the bump would be driven to leave the current attractor state and move to a close-by, i.e., similar, one.</p><p>The reverse mode requires two attractor networks, net0 and net1, to store the current and subsequent states of each experience, respectively. They are assumed to use the identical representations. In addition, the state transitions <inline-formula><mml:math id="inf60"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are encoded in the weight matrix <italic>w</italic><sub>10</sub> between net1 and net0. An experienced transition is stored by learning a connection from <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> in net1 to <italic>s</italic><sub><italic>t</italic></sub> in net0. When an experience is replayed in the reverse mode, the pattern representing <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is activated in net1, which in turn excites the state <italic>s</italic><sub><italic>t</italic></sub> in net0. The weights <italic>w</italic><sub>01</sub> from net0 to net1 are assumed to be one-to-one connections, so that the activity pattern in net0 activates the most similar pattern in net1, equivalent to our algorithm finding, in all experiences, the next state that is closest to the current state of the reactivated experience. And the process of reactivating experiences iterates from here, thereby resulting in reverse replay.</p><p>Since the reverse replay has to perform one more computational step than the default mode, that is, the transition from <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to <italic>s</italic><sub><italic>t</italic></sub>, this would suggest that generating sequences in the reverse mode is slower, which is consistent with experimental observations (<xref ref-type="bibr" rid="bib32">Liu et al., 2021</xref>).</p></sec><sec id="s3-3"><title>Current limitations of SFMA</title><p>SFMA accounts for the statistics of replay sequences and generates sequences that drive learning efficiently. However, in its current version, it addresses neither when replay events should occur nor the overall amount of replay. This limitation also extends to other similar models that implement a fixed number of update steps (<xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>) or initiate replay at predefined points in time (<xref ref-type="bibr" rid="bib29">Khamassi and Girard, 2020</xref>). This limitation leaves certain aspects of a variety of experimental findings outside of the scope of our model. For instance, <xref ref-type="bibr" rid="bib11">Cheng and Frank, 2008</xref> reported that replay decreases across sessions as the animals became more familiar with their environment, and <xref ref-type="bibr" rid="bib1">Ambrose et al., 2016</xref> found that the number of SWRs and replay increases or decreases with changing reward magnitude. Nonetheless, <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref> argue that their model accounts for both these findings by focusing on the number of ‘significant’ replays produced by their model, which decreases with familiarity with the environment and when reward decreases, and increases when reward increases. For SFMA, a similar effect w.r.t to the familiarity occurs in the default mode: As familiarity with the environment increases and the agent’s policy stabilizes, longer forward and reverse sequences are produced at the start and end of trials (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). Given the fixed number of replay steps, this results in a lower number sequences.</p><p>In PMA, the amount of reverse replay is driven by the gain term of the prioritization scheme. This is because gain is the main driver of reverse sequences and gain increases for positive reward changes. However, if gain is low, replay is dominated by the need term instead thereby resulting in the absence of reverse replays. While SFMA cannot account for the overall change in amount of replay due to changes in reward it can explain the shift in the statistics of occurring replay with the dynamic mode. The dynamic mode determines whether to produce replay using the reverse mode, which predominantly generates reverse sequences, given the recent history of prediction errors. Indeed, in open field and T-maze environments, we find that reward changes results in replays being generated in the reverse mode more often (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). However, this does not match the experimental findings which show a decrease of reverse replay for negative reward changes (<xref ref-type="bibr" rid="bib1">Ambrose et al., 2016</xref>). Unlike PMA, SFMA therefore cannot account for the differential effects of reward change in its current form.</p><p>Recent findings show that replay can be influenced by motivational shifts (<xref ref-type="bibr" rid="bib10">Carey et al., 2019</xref>). SFMA and other similar models may not be suitable to account for these findings, since they do not differentiate between different types of reward and do not consider different behavioral states like hunger or thirst. Changes in behavior due to motivational shifts could be explained by the learning of separate Q-functions for different sources of reward and then choosing the Q-function which corresponds to the current motivation. In <xref ref-type="bibr" rid="bib10">Carey et al., 2019</xref> two reward sources, that is, food and water, were available to the animals. When one was restricted behavior shifted towards it, but replay shifted toward the unrestricted one. While SFMA may produce replays representing the unrestricted reward source, it would not be able to account for the observed shifts in replay content due to shifts in motivation. SFMA could potentially account for the shift in replay content by including the recency of experience during prioritization. If more remote experiences have their priority increased, the content of replay could shift in a similar way to the one reported. Another account for the content of replay shifting to represent the unrestricted reward source is given by the model by <xref ref-type="bibr" rid="bib2">Antonov et al., 2022</xref>. In their model, which is partly based on PMA, the agent’s Q-function is subjected to forgetting, which eventually leads the utility of transitions not updated recently to increase. Since during behavior the restricted reward source is preferentially visited, the unrestricted reward source would be more affected by forgetting. This would lead to replay representing the unrestricted reward source more often. Both potential mechanisms would produce the shift of replay content as a result of the behavioral history rather than due to motivational shift directly.</p><p><xref ref-type="bibr" rid="bib23">Gillespie et al., 2021</xref> reported that awake replay preferentially represented previously rewarded goal locations rather than the current one. Since experience strength is modulated by both reward and previous behavior this finding could potentially be accounted for by our model. However, the experiment also found that locations that have not been visited recently are represented more often. This effect of behavioral recency can currently not be accounted for by SFMA. The two potential mechanisms discussed in the above paragraph could enable SFMA to also account for the effect of behavioral recency.</p></sec><sec id="s3-4"><title>Related work</title><p>The work closest to ours is the state-of-the-art PMA model by <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>. It proposes that replay prioritizes experiences according to their utility, which in turn is defined as the product of gain and need. Gain represents the policy improvement that would result hypothetically if an experience were reactivated and need represents the expected future occupancy of the experience’s current state. As mentioned in the introduction, it is not clear how a biological neural network could compute the utilities for stored experiences. Our model circumvents this problem by basing the probability of reactivation on behavioral and environmental variables and offers a potential account for how the utility of experiences could be approximated.</p><p>Furthermore, the implementation of PMA contains a few oddities. Because utility is defined as a product of gain and need, the gain must have a nonzero value to prevent all-zero utility values. This is achieved by applying a (small) minimum gain value. A problem arises for the special <inline-formula><mml:math id="inf64"><mml:mi>n</mml:mi></mml:math></inline-formula>-step update, which updates the Q-function for all n states along the replay trajectory, considered by their model for which gain is summed over <inline-formula><mml:math id="inf65"><mml:mi>n</mml:mi></mml:math></inline-formula> steps and which is the main driver of forward sequences (especially, for offline replay). The minimum gain value is applied before summation, so that the gain artificially increases with sequence length. Furthermore, Mattar &amp; Daw state that ‘EVB ties are broken in favor of shorter sequences’, where EVB stands for ‘expected value of backup’, that is, the utility. By contrast, due to their implementation, longer sequences are always favored in the case when gain is zero for all experiences, which can occur when the environment does not contain reward or the environment is highly familiar. This artifact could be avoided by applying the minimum value only once after the summation of gain values. In our hands, this reduces the model’s ability to produce sequences during offline replay, because the utility is dominated by the gain term (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>).</p><p>A different view of hippocampal replay was presented by <xref ref-type="bibr" rid="bib29">Khamassi and Girard, 2020</xref>. They argue that replay can be viewed as a form of bidirectional search in which learning is facilitated by alternating between phases of prioritized sweeping (<xref ref-type="bibr" rid="bib36">Moore and Atkeson, 1993</xref>), which prioritizes experiences according to how their reactivation affects behavior, and trajectory sampling (<xref ref-type="bibr" rid="bib53">Sutton and Barto, 2018</xref>), which explores potential routes by simulating trajectories through the environment. In simulations, they show that bidirectional search performs better than prioritized sweeping and trajectory sampling alone. Importantly, they also were able to reproduce the occurrence of shortcut replays. Their model is, in principle, able to generate random walk replays due to the fact that, in the absence of reward, replay would be dominated by the trajectory sampling phase under a uniform action policy. However, their model does not give an account for how replay could adapt to environmental changes.</p><p><xref ref-type="bibr" rid="bib34">McNamee et al., 2021</xref> proposed a generative model of replay which adapts replay statistics depending on the cognitive function that it supports. Unlike the models discussed above, this model focuses on states, that is, locations, reactivated and ignores the combination of state and action. Their model produces random walks and they show that these replay statistics facilitate the learning of a successor representation of the environment. However, this model lacks a proper account for why replay should be able to quickly adapt to changes in the environment, and assumes a generator for (random) sequences which is handcrafted for each simulation. Furthermore, their model does not seem to allow for differentiating forward from reverse sequences.</p></sec><sec id="s3-5"><title>Conclusion</title><p>Our model gives a biologically plausible account of how the hippocampus could prioritize replay and produce a variety of different replay statistics, and fits well with contemporary views on replay. Furthermore and most importantly, the replay mechanism facilitates learning in goal-directed tasks, and performs close to the current state-of-the-art model without relying on computations of hypothetical network updates.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Reinforcement learning</title><p>For our computational study we relied on reinforcement learning (RL) (<xref ref-type="bibr" rid="bib53">Sutton and Barto, 2018</xref>) which makes it possible to learn optimized sequences of choices based on sparse rewards. In RL, an agent interacts with an environment and tries to maximize the expected cumulative reward (<xref ref-type="bibr" rid="bib53">Sutton and Barto, 2018</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf66"><mml:mi>γ</mml:mi></mml:math></inline-formula> is the so-called discount factor that weighs the importance of rewards that will be received in the future relative to immediate ones, and <italic>r</italic><sub><italic>t</italic></sub> is a real-valued reward received at time <inline-formula><mml:math id="inf67"><mml:mi>t</mml:mi></mml:math></inline-formula>. The RL agent must learn to choose those actions that maximize the expected cumulative reward, the so-called optimal policy, through interactions with its environment. These interactions are generally formalized as experience tuples <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>s</italic><sub><italic>t</italic></sub> and <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> denote the current state and next state, respectively. The action taken by the agent in state <italic>s</italic><sub><italic>t</italic></sub> is represented by <italic>a</italic><sub><italic>t</italic></sub> and the scalar reward received is represented by <italic>r</italic><sub><italic>t</italic></sub>. The critical problem in RL is crediting the right choices leading to a reward that was received only (much) later.</p><p>A popular algorithm that solves this learning problem is Q-learning (<xref ref-type="bibr" rid="bib55">Watkins, 1989</xref>) which uses a state-action value function <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, also called the Q-function, to represent the cumulative future reward expected for taking action <inline-formula><mml:math id="inf71"><mml:mi>a</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf72"><mml:mi>s</mml:mi></mml:math></inline-formula>. The Q-function is updated from experiences in each time step <inline-formula><mml:math id="inf73"><mml:mi>t</mml:mi></mml:math></inline-formula> by:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mi>a</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf74"><mml:mi>η</mml:mi></mml:math></inline-formula> denotes the learning rate. For all simulations the learning rate was set to <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. The term in brackets is known as the temporal difference error, and represents by how much the last action and potentially collected reward has changed the expected discounted future reward associated with the state. RL generally requires many interactions with the environment, which results in slow learning. To address this problem <xref ref-type="bibr" rid="bib31">Lin, 1992</xref> introduced experience replay, where interactions with the environment are stored as experiences <italic>e</italic><sub><italic>t</italic></sub> and reactivated for updating the Q-function later. In its simplest form, experience replay samples stored experiences uniformly at random. Learning from replay can be further improved by prioritizing experiences to be replayed (<xref ref-type="bibr" rid="bib53">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib48">Schaul et al., 2016</xref>).</p></sec><sec id="s4-2"><title>Spatial structure and frequency-weighted memory access (SFMA)</title><p>For our simulations, we employed the Dyna-Q architecture in combination with grid world environments in a similar fashion to <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>. We view the reactivation of specific experiences as analogous to the reactivation of place cells in the hippocampus during awake and sleeping resting states. Like the original Dyna-Q architecture, our model stored experiences for all environmental transitions in a memory module. However, we stored additional variables for later prioritization of experiences during replay: The experience strength <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the experience similarity <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and inhibition of return <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The experience strength is a count of how often a certain environmental transition was experienced. Following an experienced transition <italic>e</italic><sub><italic>i</italic></sub> the experience strength was updated as follows:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Additionally, experience strength was modulated by rewards experienced during behavior. If reward was encountered, the strength of experiences <italic>e</italic><sub><italic>i</italic></sub> was increased weighted by their similarity to <italic>e</italic><sub><italic>r</italic></sub> according to the following update rule:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf79"><mml:mi>r</mml:mi></mml:math></inline-formula> is the scalar reward encountered and <italic>e</italic><sub><italic>r</italic></sub> is the corresponding experience. The similarity between two experiences <italic>e</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub> was encoded by <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and was based on the Default Representation (DR) (<xref ref-type="bibr" rid="bib45">Piray and Daw, 2021</xref>). Specifically, we used the DR to represent open space and barrier information separately. The DR representing open space was computed directly from the state-state transition model <inline-formula><mml:math id="inf81"><mml:mi>T</mml:mi></mml:math></inline-formula> which represents the default policy:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf82"><mml:mi>X</mml:mi></mml:math></inline-formula> is the identity matrix and <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the DR’s discount factor. The DR’s discount factor was set to <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> unless otherwise stated. Barrier information was provided during initialization as a list of invalid transitions and could be updated in case of environmental changes. The low rank matrix update of the DR was computed for these invalid transitions. Inhibition <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was used to prevent the repeated reactivation of the same experience and applied following reactivation of an experience to all experiences <italic>e</italic><sub><italic>i</italic></sub> that share the same starting state <italic>s</italic><sub><italic>t</italic></sub>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Inhibition terms decayed in each time step with a factor <inline-formula><mml:math id="inf86"><mml:mi>λ</mml:mi></mml:math></inline-formula>:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Inhibition was reset between replay epochs and the decay factor was set to <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> unless otherwise stated.</p><p>Replays were initiated in two different ways depending on whether they were simulated for awake resting states or sleep. Awake replay was initiated at the agent’s current state, sleep replay was initiated in a randomly chosen state based on the relative experience strengths:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Given the most recently reactivated experience <italic>e</italic><sub><italic>t</italic></sub>, an experience <inline-formula><mml:math id="inf88"><mml:mi>e</mml:mi></mml:math></inline-formula> was stochastically reactivated based on the priority ratings <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The activation probabilities <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were computed by applying a customized softmax function to the normalized priority ratings:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula> is the inverse temperature with <inline-formula><mml:math id="inf92"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula>, unless otherwise stated. Since in the usual softmax function priority ratings of zero would yield non-zero reactivation probabilities, we subtracted one from each exponential in our customized softmax function. The replay epoch ended, that is, replay stopped, when either a maximum number <inline-formula><mml:math id="inf93"><mml:mi>N</mml:mi></mml:math></inline-formula> of replayed experiences was reached or the priority ratings for all experiences were below a threshold of <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>Since each experience tuple contains two states, that is, the current and next state, that could be compared to one another to determine similarity of experiences <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we explored two different replay modes. The default mode, which computes similarity based on the two current states, and the reverse mode, which compares the next state of experience <inline-formula><mml:math id="inf96"><mml:mi>e</mml:mi></mml:math></inline-formula> to the current state of experience <italic>e</italic><sub><italic>t</italic></sub> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Additionally, we explored the possibility for the agent to dynamically decide which replay mode to use based on the cumulative temporal difference errors experienced since the last trial - we call this the dynamic mode. The dynamic mode defines the probability of generating replay in the reverse mode as:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf97"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula> is sum of absolute temporal difference errors that were encountered since the last trial.</p></sec><sec id="s4-3"><title>Model implementation</title><p>We implemented our model as well as all of our simulations and analysis code in Python 3 using the CoBeL-RL framework (<xref ref-type="bibr" rid="bib54">Walther et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Diekmann et al., 2022</xref>) (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5741291">https://doi.org/10.5281/zenodo.5741291</ext-link>). The code for all simulations has been made available on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay">https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay</ext-link> (copy archived at <xref ref-type="bibr" rid="bib20">Diekmann, 2023</xref>).</p></sec><sec id="s4-4"><title>Simulation: Navigation tasks</title><p>Spatial learning performance was tested in three grid world environments: a linear track of size 10 × 1 states, an open field of size 10 × 10 states and a labyrinth of size 10 × 10 states. Each environment had one specific goal, which yielded a reward of 1, and one starting state. We trained agents which use SFMA in default, reverse, and dynamic modes. The discount factor used for learning was <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula>. Simulations were repeated with different discount factors <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the DR and their performance was averaged. In the last mode, the agent dynamically chose either the default or reverse mode based on the cumulative temporal-difference error experienced in the current trial. The higher the cumulative temporal-difference error was, the higher was the probability of choosing the reverse replay mode. For comparison, we also trained agents that learned from online experience only, with random replay, and Prioritized Memory Access (<xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>) (PMA). To account for the different sizes and structure of the environments, the number of steps per trial as well as the length of replays were different for each environment (<xref ref-type="table" rid="table1">Table 1</xref>). Since the model by <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref> uses additional replay at the start of each trial, we split the replay length in half. Furthermore, during action selection we masked actions which would lead the agent to same state. Reasons for this were twofold: First, by masking action we sped up learning for all agents by removing pointless actions. Second, it mitigated the unfair advantage of PMA which in its prioritization ignores experiences that lead into the same state. To test the optimality of SFMA’s different replay modes we computed for each step in each replay whether the reactivated experience was optimal, that is, it had the highest utility as defined by <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>. For simplicity we assumed uniform need as in <xref ref-type="bibr" rid="bib2">Antonov et al., 2022</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Training settings for each task.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Environment</th><th align="left" valign="bottom">Trials</th><th align="left" valign="bottom">Steps per Trial</th><th align="left" valign="bottom">Replay Length</th></tr></thead><tbody><tr><td align="left" valign="bottom">Linear Track</td><td align="char" char="." valign="bottom">20</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom">Open Field</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom">Labyrinth</td><td align="char" char="." valign="bottom">100</td><td align="char" char="." valign="bottom">300</td><td align="char" char="." valign="bottom">50</td></tr></tbody></table></table-wrap><p>We further analyzed the directionality of replay in the default and reverse modes depending on when, in a trial, it was initiated, that is, at the start and end of a trial. Simulations were repeated with the same training settings listed above except that the number of trials for the linear track was extended to 100. Furthermore, additional replays were generated at the start and end of each trial but they did not contribute to learning. The directionality of these additional replays was measured by the difference between the number of forward and reverse pairs of consecutively reactivated experiences <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>e</italic><sub><italic>t</italic></sub>. A pair <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>e</italic><sub><italic>t</italic></sub> was considered to be forward when the next state of <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was the current state of <italic>e</italic><sub><italic>t</italic></sub>, reverse when the current state of <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was the next state of <italic>e</italic><sub><italic>t</italic></sub> and unordered otherwise.</p></sec><sec id="s4-5"><title>Simulation: Random walk replay</title><p>To replicate the experiment by <xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref> we chose an open-field grid world with 100 × 100 states. The large size of this environment allowed us to simulate replays of different lengths. We considered two cases: Homogeneous experience strengths and heterogeneous experience strengths, which were highest at the center of the environment and decreased with distance to half the maximum value at the borders. In each simulation a total of 50 replays of length 500 were generated. At such a large length, replays can easily reach the environment’s borders if initialized near the borders. We therefore initialized replays exclusively at the environment’s center. Simulations were repeated for different values of DR discount factor <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and inhibition decay <inline-formula><mml:math id="inf105"><mml:mi>λ</mml:mi></mml:math></inline-formula>:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.00</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We applied the Brownian Diffusion Analysis used by <xref ref-type="bibr" rid="bib52">Stella et al., 2019</xref> to the output of our simulations. Furthermore, we tested the randomness of replay starting positions as well as their directions. We did so by initializing 1000 replays and restricted replay length to five. Values for the DR discount factor and inhibition decay were fixed at <inline-formula><mml:math id="inf106"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>, respectively. The distribution of starting positions was computed by counting how often a given state occurred as the current state of initially replayed experiences. For the distribution of initial replay directions we first divided the environment into spatial bins of size 20 by 20 states. Direction was divided into 90° bins, i.e., bins centered at 0°, 90°, 180° and 270°. For each spatial bin we then counted how often replay direction fell into respective direction bins. Lastly, we computed displacement distributions at different time steps, <inline-formula><mml:math id="inf108"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, relative to initially replayed positions in a 25 × 25 states window. This was done by averaging the replayed position relative to the start of replay.</p></sec><sec id="s4-6"><title>Simulation: Shortcut replay</title><p>For this simulation, we simplified the maze used by <xref ref-type="bibr" rid="bib24">Gupta et al., 2010</xref> as an 11 × 7 states figure-eight-maze-like grid world. Reward locations on either side were reduced from two to one location. Since our main interest lay in the effect of specific stereotypical running behavior expressed by the animals on replay, basic running patterns were predefined:</p><list list-type="order"><list-item><p>From starting position to left side’s reward location.</p></list-item><list-item><p>From starting position to right side’s reward location.</p></list-item><list-item><p>From left side’s reward location to left side’s reward location.</p></list-item><list-item><p>From right side’s reward location to right side’s reward location.</p></list-item><list-item><p>From left side’s reward location to right side’s reward location.</p></list-item><list-item><p>From right side’s reward location to left side’s reward location.</p></list-item></list><p>From these running patterns, we created sessions consisting of 20 trials. Each session was characterized by one pattern during the first 10 trials, which could change to a different pattern for the remaining 10 trials, for example, 10 right laps followed by 10 left laps. Running patterns 1 and 2 were only used in the initial trial of a session. Patterns 3 and 4 were used when the agent was supposed to run laps on the same side, and patterns 5 and 6 were used when the agent was supposed to alternate between the two reward locations. We considered the following pattern combinations (used patterns are listed in parenthesis ordered numerically):</p><list list-type="bullet"><list-item><p>Right-Left (from patterns 2, 3, 4, and 6).</p></list-item><list-item><p>Right-Alternating (from patterns 2, 4, 5, and 6).</p></list-item><list-item><p>Alternating-Left (from patterns 2, 3, 5, and 6).</p></list-item><list-item><p>Alternating-Alternating (Control Condition; from patterns 2, 4, 5, and 6).</p></list-item></list><p>After each trial, 200 replays were generated and stored for later analysis. Since reactivation probabilities are controlled by the softmax function’s inverse temperature parameter <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></inline-formula>, we also repeated each simulation for a range of values <inline-formula><mml:math id="inf110"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To detect shortcut replays, we first generated state sequence templates for each corner of the maze (i.e. TL, TR, BL and BR) in forward and backward direction. A template was considered a match if the number of mismatches was at most two. If two corners that together would form a shortcut (e.g. TL in backward direction and TR in forward direction) were detected in a replay and were separated by <inline-formula><mml:math id="inf111"><mml:mrow><mml:mn>8</mml:mn><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> states they were considered to form a shortcut. The different possible shortcuts were:</p><list list-type="bullet"><list-item><p>From TL (backward) to TR (forward).</p></list-item><list-item><p>From TR (backward) to TL (forward).</p></list-item><list-item><p>From BL (forward) to BR (backward).</p></list-item><list-item><p>From BR (forward) to BL (backward).</p></list-item></list><p>For our additional learning simulations, an RL agent was trained in the environment. Since a simple RL agent, like the one employed, is not capable of expressing the same stereotypical running behavior, we restricted selectable actions such that the agent can only run forward. The only relevant choice was therefore limited to the maze’s decision point. If the agent picked the correct choice it received a reward of 1 for that trial. Furthermore, since the agent is not capable of learning in conditions with alternating reward arms for lack of working memory, we restricted simulations to the Right-Left condition. Simulations were repeated for agents trained with SFMA in default and reverse mode as well as random replay. To better differentiate the effect of replay type, we allowed but one replay after each trial and increased the number of trials per session to 200. The discount factor used for learning was <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-7"><title>Simulation: Adaptive replay</title><p>To evaluate our model’s ability to adapt to structural changes in an environment we set up three structurally different environments and presented them successively to our model. Replays were simulated in each environment and analyzed by computing the fraction of invalid transitions. Invalid transitions were defined as two consecutively replayed experiences whose current states were separated by a barrier. To evaluate the importance of the representation chosen for experience similarity, we performed all simulations using the Euclidean distance for the experience similarity, that is,<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-8"><title>Simulation: Preplay</title><p>We simplified the experimental setup from <xref ref-type="bibr" rid="bib40">Ólafsdóttir et al., 2015</xref> to a 5 × 7 T-maze-like grid world. A barrier was inserted to separate stem from arms. The simulation was divided into three phases. First, in the run phase the agent was made to run up and down the stem, and rewards were provided at the two ends of the stem. Second, in the cue phase we replicated the attention paid to the arms by the animals as an increase of experience strengths at the arms. Visual exploration was associated with a lower increase in experience strengths than physical exploration by a factor we call attention strength. The values focused on for the strength of attention was:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.02</mml:mn><mml:mo>,</mml:mo><mml:mn>0.04</mml:mn><mml:mo>,</mml:mo><mml:mn>0.06</mml:mn><mml:mo>,</mml:mo><mml:mn>0.08</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.12</mml:mn><mml:mo>,</mml:mo><mml:mn>0.14</mml:mn><mml:mo>,</mml:mo><mml:mn>0.16</mml:mn><mml:mo>,</mml:mo><mml:mn>0.18</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The right arm was chosen arbitrarily as the cued arm that contained a reward and therefore was allocated more attention than the other, uncued arm (asymmetric allocation of attention). For the asymmetric allocation of attention, we focused on values greater than 0.5 as they best reflect the attention reported:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.55</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.65</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.75</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>0.85</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The increase of experience strengths for cued was computed by multiplying attention strength and asymmetry with the increase that would be expected if the agent actually traversed the cued arm. The increase of experience strengths for the uncued arm was computed similarly by using inverse symmetry, i.e., <inline-formula><mml:math id="inf113"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For each combination of values for attention strength and asymmetric allocation of attention we simulated 5000 preplays of length six. Preplay sequences were identified with template matching. Templates were prepared for stem, cued, and uncued arms for forward and reverse sequences. In a final choice phase, the value combination (0.75 and 0.14 for asymmetric allocation of attention and attention strength, respectively) which yielded a preplay fraction closest to the one reported by <xref ref-type="bibr" rid="bib40">Ólafsdóttir et al., 2015</xref>, i.e., 7.37, was used to train the agent’s Q-function. The discount factor used for learning was <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula>. The choice at the decision point was compared between before and after training assuming a softmax action selection policy with an inverse temperature parameter of <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>. The action that leads into a wall and the action that leads back into the stem were ignored since we wanted to focus on the agent’s preference for the cued vs the uncued arm.</p></sec><sec id="s4-9"><title>Simulation: Reward modulation</title><p>Simulations were run in an open field environment of size 10 × 10 states, and a T-maze environment with a stem length of five states and an arm length three states. Reward of magnitude one was located in the left upper corner in the open field environment and the right arm in the T-maze environment. Agents were trained for 100 trials each of which lasted at most 100 steps. Online replays of length 10 were generated at the begin and end of trials. Additionally, offline replays with same length were also generated. However, only replays generated at the end of trials were used to train the agent. The reward modulation of experience strength, that is, the increase in experience strength due to reward, had a value of either 1 or 10. Simulations were repeated 100 times for each combination of replay mode (i.e. default, reverse, and dynamic) and value for reward modulation. The representation of replay locations was analyzed by computing their reactivation probability across all replays separately for trial begin, trial end, and offline replays.</p></sec><sec id="s4-10"><title>Simulation: Aversive shock zone</title><p>To replicate the findings from <xref ref-type="bibr" rid="bib61">Wu et al., 2017</xref> we used a linear track gridworld environment with a length of 20 states. The first two states from the left represented the shock zone. Dark and light zones of the environment were represented by the left and right halves, respectively. Experience with the environment preceding shock (i.e. during Run 1, Run 2 and Pre) was simulated by making the agent run 30 laps on the linear track and then multiplying the experience strengths of dark and light zones with their reported occupancy (i.e. 74% and 26%). Finally, ten trials were dedicated to Shock and Post phases. For the initial shock trial (Shock), the agent was made to run until the shock zone where it received a shock of magnitude 1. For the remaining nine trials (Post) the agent was running laps in the light zone of the environment. Twenty replays of length ten were generated for each trial in Shock and Post phases. For the shock phase, replays were generated following shock while for the Post phase replays were generated when the agent stopped in front of the dark zone. Simulations were repeated 100 times and the reactivation probability for each state was computed. Additionally, we analyzed how the generated replays could result in shock zone avoiding behavior by inspecting the agent’s Q-function. More precisely, for each state we computed the probability of the action pointing away from the shock zone being the better option (i.e. the action is associated with less shock). The learned Q-functions were analyzed separately for the first and second half of the Post phase. To account for the fact that the Q-learning update cannot propagate information about worse rewards due to the max operator we repurposed the Q-function to represent the (positive-valued) discounted cumulative punishment instead of reward. The Q-function was not updated during behavior to focus on the replays’ effect on behavior.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82301-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modelling code has been made publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay">https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay</ext-link> (copy archived at <xref ref-type="bibr" rid="bib20">Diekmann, 2023</xref>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrose</surname><given-names>RE</given-names></name><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reverse replay of hippocampal place cells is uniquely modulated by changing reward</article-title><source>Neuron</source><volume>91</volume><fpage>1124</fpage><lpage>1136</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.047</pub-id><pub-id pub-id-type="pmid">27568518</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antonov</surname><given-names>G</given-names></name><name><surname>Gagne</surname><given-names>C</given-names></name><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Optimism and pessimism in optimised replay</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009634</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009634</pub-id><pub-id pub-id-type="pmid">35020718</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azizi</surname><given-names>AH</given-names></name><name><surname>Wiskott</surname><given-names>L</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A computational model for preplay in the hippocampus</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>161</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00161</pub-id><pub-id pub-id-type="pmid">24282402</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bathellier</surname><given-names>B</given-names></name><name><surname>Tee</surname><given-names>SP</given-names></name><name><surname>Hrovat</surname><given-names>C</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A multiplicative reinforcement learning model capturing learning dynamics and interindividual variability in mice</article-title><source>PNAS</source><volume>110</volume><fpage>19950</fpage><lpage>19955</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312125110</pub-id><pub-id pub-id-type="pmid">24255115</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayati</surname><given-names>M</given-names></name><name><surname>Valizadeh</surname><given-names>A</given-names></name><name><surname>Abbassian</surname><given-names>A</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Self-Organization of synchronous activity propagation in neuronal networks driven by local excitation</article-title><source>Frontiers in Computational Neuroscience</source><volume>9</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2015.00069</pub-id><pub-id pub-id-type="pmid">26089794</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The role of the hippocampus in prediction and imagination</article-title><source>Annual Review of Psychology</source><volume>61</volume><fpage>27</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.60.110707.163508</pub-id><pub-id pub-id-type="pmid">19958178</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buhry</surname><given-names>L</given-names></name><name><surname>Azizi</surname><given-names>AH</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reactivation, replay, and preplay: how it might all fit together</article-title><source>Neural Plasticity</source><volume>2011</volume><elocation-id>203462</elocation-id><pub-id pub-id-type="doi">10.1155/2011/203462</pub-id><pub-id pub-id-type="pmid">21918724</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Two-Stage model of memory trace formation: a role for noisy brain states</article-title><source>Neuroscience</source><volume>31</volume><fpage>551</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(89)90423-5</pub-id><pub-id pub-id-type="pmid">2687720</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caporale</surname><given-names>N</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spike timing-dependent plasticity: a Hebbian learning rule</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>25</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125639</pub-id><pub-id pub-id-type="pmid">18275283</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carey</surname><given-names>AA</given-names></name><name><surname>Tanaka</surname><given-names>Y</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reward revaluation biases hippocampal replay content away from the preferred outcome</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1450</fpage><lpage>1459</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0464-6</pub-id><pub-id pub-id-type="pmid">31427771</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>New experiences enhance coordinated neural activity in the hippocampus</article-title><source>Neuron</source><volume>57</volume><fpage>303</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.11.035</pub-id><pub-id pub-id-type="pmid">18215626</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chenkov</surname><given-names>N</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Kempter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Memory replay in balanced recurrent networks</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005359</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005359</pub-id><pub-id pub-id-type="pmid">28135266</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corkin</surname><given-names>S</given-names></name><name><surname>Amaral</surname><given-names>DG</given-names></name><name><surname>González</surname><given-names>RG</given-names></name><name><surname>Johnson</surname><given-names>KA</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>H. M.’s medial temporal lobe lesion: findings from magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>3964</fpage><lpage>3979</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-10-03964.1997</pub-id><pub-id pub-id-type="pmid">9133414</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>TJ</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hippocampal replay of extended experience</article-title><source>Neuron</source><volume>63</volume><fpage>497</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id><pub-id pub-id-type="pmid">19709631</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Improving generalization for temporal difference learning: the successor representation</article-title><source>Neural Computation</source><volume>5</volume><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deacon</surname><given-names>RMJ</given-names></name><name><surname>Bannerman</surname><given-names>DM</given-names></name><name><surname>Kirby</surname><given-names>BP</given-names></name><name><surname>Croucher</surname><given-names>A</given-names></name><name><surname>Rawlins</surname><given-names>JNP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Effects of cytotoxic hippocampal lesions in mice on a cognitive test battery</article-title><source>Behavioural Brain Research</source><volume>133</volume><fpage>57</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/s0166-4328(01)00451-x</pub-id><pub-id pub-id-type="pmid">12048174</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Griesbauer</surname><given-names>E-M</given-names></name><name><surname>Ghanamé</surname><given-names>C</given-names></name><name><surname>Zisch</surname><given-names>F</given-names></name><name><surname>Lefort</surname><given-names>JM</given-names></name><name><surname>Fletcher</surname><given-names>L</given-names></name><name><surname>Newton</surname><given-names>C</given-names></name><name><surname>Renaudineau</surname><given-names>S</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Grieves</surname><given-names>R</given-names></name><name><surname>Duvelle</surname><given-names>É</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predictive Maps in Rats and Humans for Spatial Navigation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.09.26.314815</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname><given-names>K</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Diekmann</surname><given-names>N</given-names></name><name><surname>Vijayabaskaran</surname><given-names>S</given-names></name><name><surname>Zeng</surname><given-names>X</given-names></name><name><surname>Kappel</surname><given-names>D</given-names></name><name><surname>Menezes</surname><given-names>MC</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>CoBeL-RL: A Neuroscience-Oriented Simulation Framework for Complex Behavior and Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.12.27.521997</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Diekmann</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Sencheng/-mechanisms-and-functions-of-hippocampal-replay</data-title><version designator="swh:1:rev:aec14396ab710d4a34424c2ae3f5e20edb911743">swh:1:rev:aec14396ab710d4a34424c2ae3f5e20edb911743</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9e200494844aed0e48d65cacc7bac32a166b8f39;origin=https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay;visit=swh:1:snp:e68556a294b60163fbbaaf629cbcdd369c1ff29a;anchor=swh:1:rev:aec14396ab710d4a34424c2ae3f5e20edb911743">https://archive.softwareheritage.org/swh:1:dir:9e200494844aed0e48d65cacc7bac32a166b8f39;origin=https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay;visit=swh:1:snp:e68556a294b60163fbbaaf629cbcdd369c1ff29a;anchor=swh:1:rev:aec14396ab710d4a34424c2ae3f5e20edb911743</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Lièvre</surname><given-names>G</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The roles of online and offline replay in planning</article-title><source>eLife</source><volume>9</volume><elocation-id>e56911</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56911</pub-id><pub-id pub-id-type="pmid">32553110</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>AK</given-names></name><name><surname>Astudillo Maya</surname><given-names>DA</given-names></name><name><surname>Denovellis</surname><given-names>EL</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Kastner</surname><given-names>DB</given-names></name><name><surname>Coulter</surname><given-names>ME</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hippocampal replay reflects specific past experiences rather than a plan for subsequent choice</article-title><source>Neuron</source><volume>109</volume><fpage>3149</fpage><lpage>3163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.029</pub-id><pub-id pub-id-type="pmid">34450026</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>AS</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hippocampal replay is not a simple function of experience</article-title><source>Neuron</source><volume>65</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id><pub-id pub-id-type="pmid">20223204</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><source>The Organization of Behavior: A Neuropsychological Theory</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Hippocampal replay contributes to within session learning in a temporal difference reinforcement learning model</article-title><source>Neural Networks</source><volume>18</volume><fpage>1163</fpage><lpage>1171</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2005.08.009</pub-id><pub-id pub-id-type="pmid">16198539</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname><given-names>MP</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Awake replay of remote experiences in the hippocampus</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>913</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1038/nn.2344</pub-id><pub-id pub-id-type="pmid">19525943</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Girard</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling awake hippocampal reactivations with model-based bidirectional search</article-title><source>Biological Cybernetics</source><volume>114</volume><fpage>231</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1007/s00422-020-00817-x</pub-id><pub-id pub-id-type="pmid">32065253</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kudrimoti</surname><given-names>HS</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>4090</fpage><lpage>4101</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-10-04090.1999</pub-id><pub-id pub-id-type="pmid">10234037</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Self-improving reactive agents based on reinforcement learning, planning and teaching</article-title><source>Machine Learning</source><volume>8</volume><fpage>293</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1007/BF00992699</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Experience replay is associated with efficient nonlocal learning</article-title><source>Science</source><volume>372</volume><elocation-id>eabf1357</elocation-id><pub-id pub-id-type="doi">10.1126/science.abf1357</pub-id><pub-id pub-id-type="pmid">34016753</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prioritized memory access explains planning and hippocampal replay</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id><pub-id pub-id-type="pmid">30349103</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamee</surname><given-names>DC</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Flexible modulation of sequence generation in the entorhinal-hippocampal system</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>851</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00831-7</pub-id><pub-id pub-id-type="pmid">33846626</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Sadik</surname><given-names>A</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>H</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Legg</surname><given-names>S</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/nature14236</pub-id><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>AW</given-names></name><name><surname>Atkeson</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Prioritized sweeping: reinforcement learning with less data and less time</article-title><source>Machine Learning</source><volume>13</volume><fpage>103</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1007/BF00993104</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>RG</given-names></name><name><surname>Garrud</surname><given-names>P</given-names></name><name><surname>Rawlins</surname><given-names>JN</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Place navigation impaired in rats with hippocampal lesions</article-title><source>Nature</source><volume>297</volume><fpage>681</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/297681a0</pub-id><pub-id pub-id-type="pmid">7088155</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Geometric determinants of the place fields of hippocampal neurons</article-title><source>Nature</source><volume>381</volume><fpage>425</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/381425a0</pub-id><pub-id pub-id-type="pmid">8632799</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal place cells construct reward related sequences through unexplored space</article-title><source>eLife</source><volume>4</volume><elocation-id>e06063</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06063</pub-id><pub-id pub-id-type="pmid">26112828</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Carpenter</surname><given-names>F</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task demands predict a dynamic switch in the content of awake hippocampal replay</article-title><source>Neuron</source><volume>96</volume><fpage>925</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.035</pub-id><pub-id pub-id-type="pmid">29056296</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Senior</surname><given-names>TJ</given-names></name><name><surname>Allen</surname><given-names>K</given-names></name><name><surname>Huxter</surname><given-names>JR</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reactivation of experience-dependent cell assembly patterns in the hippocampus</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>209</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/nn2037</pub-id><pub-id pub-id-type="pmid">18193040</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pavlides</surname><given-names>C</given-names></name><name><surname>Winson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Influences of hippocampal place cell firing in the awake state on the activity of these cells during subsequent sleep episodes</article-title><source>The Journal of Neuroscience</source><volume>9</volume><fpage>2907</fpage><lpage>2918</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.09-08-02907.1989</pub-id><pub-id pub-id-type="pmid">2769370</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id><pub-id pub-id-type="pmid">23594744</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linear reinforcement learning in planning, grid fields, and cognitive control</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>4942</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25123-3</pub-id><pub-id pub-id-type="pmid">34400622</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Jensen</surname><given-names>S</given-names></name><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling</article-title><source>Psychological Review</source><volume>114</volume><fpage>784</fpage><lpage>805</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.3.784</pub-id><pub-id pub-id-type="pmid">17638506</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenbaum</surname><given-names>RS</given-names></name><name><surname>McKinnon</surname><given-names>MC</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual imagery deficits, impaired strategic retrieval, or memory loss: disentangling the nature of an amnesic person’s autobiographical memory deficit</article-title><source>Neuropsychologia</source><volume>42</volume><fpage>1619</fpage><lpage>1635</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2004.04.010</pub-id><pub-id pub-id-type="pmid">15327930</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prioritized Experience Replay</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.05952">https://arxiv.org/abs/1511.05952</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>AC</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rewarded outcomes enhance reactivation of experience in the hippocampus</article-title><source>Neuron</source><volume>64</volume><fpage>910</fpage><lpage>921</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.11.016</pub-id><pub-id pub-id-type="pmid">20064396</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stella</surname><given-names>F</given-names></name><name><surname>Baracskay</surname><given-names>P</given-names></name><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hippocampal reactivation of random trajectories resembling brownian diffusion</article-title><source>Neuron</source><volume>102</volume><fpage>450</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.052</pub-id><pub-id pub-id-type="pmid">30819547</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series</source><publisher-loc>Cambridge, Massachusetts</publisher-loc><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>T</given-names></name><name><surname>Diekmann</surname><given-names>N</given-names></name><name><surname>Vijayabaskaran</surname><given-names>S</given-names></name><name><surname>Donoso</surname><given-names>JR</given-names></name><name><surname>Manahan-Vaughan</surname><given-names>D</given-names></name><name><surname>Wiskott</surname><given-names>L</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Context-Dependent extinction learning emerging from raw sensory inputs: a reinforcement learning approach</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>2713</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-81157-z</pub-id><pub-id pub-id-type="pmid">33526840</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Learning from delayed rewards PhD Thesis</article-title><publisher-name>King’s College</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation</article-title><source>Cell</source><volume>183</volume><fpage>1249</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.10.024</pub-id><pub-id pub-id-type="pmid">33181068</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Widloski</surname><given-names>J</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible rerouting of hippocampal replay sequences around changing barriers in the absence of global place field remapping</article-title><source>Neuron</source><volume>110</volume><fpage>1547</fpage><lpage>1558</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.02.002</pub-id><pub-id pub-id-type="pmid">35180390</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikenheiser</surname><given-names>AM</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The balance of forward and backward hippocampal sequences shifts across behavioral states</article-title><source>Hippocampus</source><volume>23</volume><fpage>22</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1002/hipo.22049</pub-id><pub-id pub-id-type="pmid">22736562</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>BA</given-names></name><name><surname>Baddeley</surname><given-names>AD</given-names></name><name><surname>Kapur</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Dense amnesia in a professional musician following herpes simplex virus encephalitis</article-title><source>Journal of Clinical and Experimental Neuropsychology</source><volume>17</volume><fpage>668</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1080/01688639508405157</pub-id><pub-id pub-id-type="pmid">8557808</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal replay captures the unique topological structure of a novel environment</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>6459</fpage><lpage>6469</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3414-13.2014</pub-id><pub-id pub-id-type="pmid">24806672</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>CT</given-names></name><name><surname>Haggerty</surname><given-names>D</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Ji</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hippocampal awake replay in fear memory retrieval</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>571</fpage><lpage>580</lpage><pub-id pub-id-type="doi">10.1038/nn.4507</pub-id><pub-id pub-id-type="pmid">28218916</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Tong</surname><given-names>MH</given-names></name><name><surname>Cui</surname><given-names>Y</given-names></name><name><surname>Rothkopf</surname><given-names>CA</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Modeling sensory-motor decisions in natural behavior</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006518</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006518</pub-id><pub-id pub-id-type="pmid">30359364</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82301.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Piray</surname><given-names>Payam</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03taz7m60</institution-id><institution>University of Southern California</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.07.26.501588" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.26.501588"/></front-stub><body><p>This paper proposes a new, biologically realistic, computational model for the phenomenon of hippocampal replay. This is an important study with relevance for a broad audience in neuroscience. The proposed model convincingly simulates various aspects of experimental data discovered in the past.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82301.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Piray</surname><given-names>Payam</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03taz7m60</institution-id><institution>University of Southern California</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Ólafsdóttir</surname><given-names>H Freyja</given-names></name><role>Reviewer</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.26.501588">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.07.26.501588v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A Model of Hippocampal Replay Driven by Experience and Environmental Structure Facilitates Spatial Learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Guest Reviewing Editor and Laura Colgin as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: H Freyja Ólafsdóttir (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Reviewers agreed that this is an important study with relevance for a broad audience in neuroscience. Reviewers specifically commended the simplicity and elegance of the proposed computational model and the clarity of the manuscript. There were, however, also several concerns that we would ask you to address in a revision.</p><p>1) All reviewers agreed that there should be further simulation analyses to address several seminal empirical data in the field of replay. Of particular importance are those studies that are simulated in Mattar and Daw 2018 paper. Is the model able to recapitulate reward over-representation in the open field and T-maze (Mattar and Daw's Figure 4). Another crucial experiment (also simulated in the Mattar and Daw 2018) is the phenomenon of non-local replay. Reviewers agree that it's important that computational models of replay capture these characteristic aspects of replay.</p><p>2) Reviewers also agreed that it is important that authors perform further simulation analyses to address other experiments simulated in Mattar and Daw 2018. These include responsiveness of reverse replay to reward change, replay counterintuitively entering an aversive zone, and the effect of experience on replay frequency. Even if the proposed model cannot account for all of these findings, it is important that authors transparently report them (maybe with supplementary figures) and discuss them extensively.</p><p>3) The fact that the model has two modes (default and reverse) to account for empirical findings is also a major issue. Reviewers agree that the underlying mechanism regarding these two modes is not clear. Do they work in parallel? or is there a dynamic switch between the two? How does the dynamic switch work computationally? And how is this aspect of the model related to experimental data? Please see specific comments by Reviewers 2 and 3 about this issue.</p><p>4) In addition, individual reviewers made a number of suggestions to improve the manuscript. Please carefully consider these additional points when preparing the revised manuscript.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>My point is especially important in relation to the model by MandD. For example, does the current model explain evidence that replay over-represents reward locations (e.g. Figure 4 in MandD)? I suggest conducting additional simulation analysis, particularly for the experimental data explicitly presented in MandD and discussing any of those experiments that cannot be explained by the current model (if any).</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In their paper, Diekmann and Cheng describe a model for replay generation that is able to replicate multiple seminal findings in the field. Specifically, in their model experiences get selected for replay depending on their strength (which is in turn determined by their frequency of occurrence and reward), their similarity to other experiences and inhibition that prohibits experiences that match the current one too much from being reactivated (which leads to the generation of replay sequences). Further, they adopt two different methods for computing similarity which they show produces complementary results. With this rather simple and intuitive model Diekmann and Cheng are able to re-produce important, but divergent, findings in the field. For example, why replay switches directionality at different time points on a maze, how it can generate synthetic experiences, respects boundaries, etc. Being able to account for these different findings with a single mechanism is an important goal and no small feat.</p><p>However, I still have some reservations about how much the current model differentiates itself – and represents an improvement – of a recently published model (the so-called prioritized memory access (PMA) model for replay <xref ref-type="bibr" rid="bib33">Mattar and Daw, 2018</xref>). Further, although their model provides a relatively comprehensive account of multiple replay findings, it doesn't address many studies that show awake replay is non-local (even depicting experiences in remote environments). Finally, I would like the authors to elaborate on how their 'default' and 'reverse' policy could be implemented biologically and how these two modes relates to current thinking in the replay field. I elaborate on my points below:</p><p>– Benefit over PMA model: The authors maintain that their model represents a significant improvement from the PMA model as the latter is not biologically feasible. In the PMA model, replay gets selected based on the utility of a possible experience reactivation which is partly derived from its &quot;gain&quot; term. Here, the authors' critique is that if the calculation of gain requires access to all stored experiences, this becomes very computationally laborious and biologically unfeasible. Yet, it's not clear to me in the current model's implementation that it represents a clear advantage in this way. Namely, to compute similarity – which influences replay prioritization – the current experience state needs to be compared with all other experience states (or tuples). Thus, does their model not also require access to all stored experiences? If not, I think the authors need to spell out a bit more about how this still makes their model more efficient and biologically implementable than the PMA model. This is a particularly important point as one of the main 'selling points' of the paper is that it achieves what the PMA model achieves but in a physiologically realistic way.</p><p>– The authors show the model does well in replicating multiple replay findings in the field. However, they focus primarily on awake replay findings that show some relationship to current behaviour- e.g. depicting short cuts to goal locations, activating paths just taken, etc. However, many replay studies show that replay is very often non-local (e.g. Karlsson and Frank (2009); Olafsdottir et al. (2017)). How can their model accommodate these findings?</p><p>– In the current model there are two modes (policies) for computing experience similarity described; the default and reverse mode. The authors show you need both to effectively account for different replay phenomena (e.g. you need reverse for reward learning, particularly in environments where an animal's path is not prescribed). But what determines which policy is used? How would switching between the two be implemented in neural networks? Or do they propose they work in parallel?</p><p>– The default and reverse mode resonate with some contemporary theories of replay. Namely, a commonly held view is that awake replay may support planning/memory retrieval etc. whereas replay during rest periods supports memory consolidation. Perhaps the authors can elaborate on how their different modes, which resemble somewhat the planning vs consolidation mode, relate to these theories. Could it be that the default mode may be a sort of a planning mode – supporting on-going behaviour – whereas the reverse mode is more about learning?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>In multiple instances the authors refer to the biological plausibility of their model, and the lack thereof in the model by Mattar and Daw. Intuitively, I agree with the authors. However, to my knowledge we don't know what is biologically plausible and what is not. I suggest some rewording to emphasize that their model is simpler and less computationally taxing compared to Mattar and Daw. Further, I would appreciate a more explicit discussion of the biological plausibility of their model. They get into this to some degree in the discussion, particularly with regards to the return of inhibition term (i.e., line 401). However, the description of how experience strength or distance could be computed could certainly be expanded beyond just referencing other studies. Importantly, the model occurs in two modes. How might the hippocampus gate these two modes?</p><p>In general I found the methods to be exquisitely articulated and comprehensive. However, the description of how replay is generated in their model could be expanded/clarified in the main text:</p><p>– I understood that an experience consists of a transition between two states (in addition to reward/action)- and that the entire experience (including both states) is reactivated during replay. However I think a supplementary figure showing how exactly one gets from the last panel of figure 2A to an ordered set of reactivated states as in Figure 5c could be helpful.</p><p>– Most importantly- I think it would be very useful for the authors to show more examples of replay events generated by their model, particularly with respect to Figure 3. For example, they report the directionality of replays in the open field/labyrinth, but I would like to know where the replays actually started/ended, and how this changed over trials. This would help clarify how the replays are behaviorally useful to the agent.</p><p>– The authors could better describe the logic/motivation of the default and reverse states. Initially I assumed that the default mode simply drives forward replay, and the reverse mode reverse replay- but this is not entirely the case. Why did the authors use just 2 modes (the first comparing the similarity between experience 1-state 1 and experience 2 state-1, and the second comparing the similarity between experience 1-state 1 and experience 2 state-2)- why not, for example, have other modes comparing experience 1-state 2 and experience 2 state-1, or experience 1-state 2 and experience 2 state-2? Many readers would benefit from a bit more intuition in terms of linking these modes to biological phenomenon.</p><p>– Can a single replay consist of both forward and reverse transitions? As mentioned above, a panel of example replays upfront could be very useful.</p><p>– Please confirm that only one map was constructed for linear environments. What was the reason for this choice, and would it impact the results to use two maps (one for each running direction, as in Mattar and Daw, and consistent with hippocampal fields on a linear track)?</p><p>– A description of how the default versus reverse modes are selected in the dynamic mode is provided in the methods, but should also be described in the main text.</p><p>– The definition of a replay epoch is in the methods but might be moved to the main text.</p><p>Can this model account for a broader scope of replay phenomena including: the sensitivity of reverse replay to reward magnitude or reward prediction error, over-representation of reward locations, or the tendency of forward replays to counter-intuitively lead towards an aversive (negatively rewarding) locations? As Mattar and Daw demonstrated that their model produces patterns of replay that match these arguably fundamental empirical findings, examining the fully overlapping set of scenarios would allow the models to be compared head-to-head. It is unclear whether either model can account for the recent observations that replay is enriched for locations not visited recently (Gillespie et al., 2021) or trajectories directed away from the animal's preferred goal location (Carey et al., 2019). I do not mean to suggest that the authors needs to perform all of the above tests. However, it would be of interest to at least include discussion about which empirical findings the current model could likely not account for.</p><p>Line 160: 'in our simulations the reverse mode produces more updates that were optimal than did the default mode'. Please define an optimal (policy?) update.</p><p>Line 165: I'm not sure what the authors mean by 'mixed' replays. Are they describing replays that traverse in one direction and then switch to the other (i.e., reverse to forward), as in Wu and Foster 2014?. If not, a better word choice might be 'interspersed'.</p><p>Line 188: Are there any statistical tests to accompany the statement that these log-log plots are linear?</p><p>Line 223: It is not clear to me what the alternating-alternating condition controls for.</p><p>Line 234: The explanation about the observed differences in short-cut replays under the different behavioral paradigms was somewhat confusing. A figure showing the relative amount of experience in each segment of the maze in each paradigm could help clarify.</p><p>Line 248: While it is interesting to ask whether the replay generated by your model facilitates learning in the task, I think it would be of broader interest and more informative to show specifically which replays are useful. For example, what was the distribution of replays that the default and reverse mode yielded on that task? How does task performance change if you limit replay to a subset, such as only reverse replays starting at the goal, or, most relevant here- only-short cut replays?</p><p>Figure 6A: What are reactivation maps? I.e., is this a summary or average of all reactivations, or an example of an individual replay epoch (I assume it's the former, but that should be specified). Individual replay examples would also useful.</p><p>Line 329: 'General representations of task structure in EC have been proposed to be forming hippocampal place cell code together with sensory information'- I suspect there was a typo in this sentence; I was unable to decipher its meaning.</p><p>Line 409: 'We argue that the minimum value should be applied after the summation of gain values&quot; – what is the argument for applying the minimum value after summation? In general I wonder if the description of PMA's flaws (starting at line 404) should be moved to the related Supplemental Figure legend. I think more detail would be needed for most readers to understand the apparent error (for example, what is the 'special n-step update', what does it mean to apply before summation or after summation, and what makes one version better than the other)?</p><p>Figure 5e: Please state in the figure legend what happened at trial 100. Also, as noted elsewhere, I think it would be useful to compare performance with the dynamic mode (and possibly the PMA model).</p><p>Figure 5/Supplementary Figure 7: I'm confused as to why in Figure 5 there is little difference between the forward and reverse modes in the initial learning of the task, but a difference after the reward location switch, whereas in Supplementary Figure 7 the exact opposite pattern is shown on the T-Maze. In general, what explains the differences in the initial learning rate and the learning rate after the reward switch, especially on the T-maze? I don't see what information the agent learned in the initial phase that remained useful after the switch. Perhaps showing examples of the agents behavioral trajectory/decisions at different stages of the task would help.</p><p>Supplementary Figure 3: what is the take-home message of showing the displacement distributions with different inverse temperature parameters? Is the point that the results hold when using different temperatures? If so, why not provide a more quantitative metric, like the log-log plots using different inverse temperature parameters?</p><p>Supplementary Figure 8: The main text indicates that this figure was generated using the reverse mode, but the legend doesn't say so.</p><p>Methods line 463: 'The term in brackets is known as the temporal difference error'. This could perhaps be explained a bit more.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82301.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewers agreed that this is an important study with relevance for a broad audience in neuroscience. Reviewers specifically commended the simplicity and elegance of the proposed computational model and the clarity of the manuscript. There were, however, also several concerns that we would ask you to address in a revision.</p><p>1) All reviewers agreed that there should be further simulation analyses to address several seminal empirical data in the field of replay. Of particular importance are those studies that are simulated in Mattar and Daw 2018 paper. Is the model able to recapitulate reward over-representation in the open field and T-maze (Mattar and Daw's Figure 4).</p></disp-quote><p>We agree that the over-representation of rewarded locations is an important feature of replay that we did not sufficiently address in our previous simulations and manuscript. In fact, reward always was an important factor in our model in that it modulates the experience strength variable. To demonstrate the effect of reward explicitly, we performed simulations in open field and T-maze environments and analyzed the representation of environmental states during replay. A corresponding section detailing the simulation and discussing the results was included along with a new figure (Figure 8, “Reward Modulation of Experience Strength Leads to Over-Representation of Rewarded Locations”).</p><p>To summarize our results, replay produced by our model can account for the over-representation of rewarded locations in both environments. Over-representation depends on the relative influence of reward and experience as well as where and how replay is initiated. If experience strengths are more strongly modulated by reward and replay occurs at the end of a trial or in an offline manner, rewarded locations are strongly over-represented (Figure 8C, D).</p><disp-quote content-type="editor-comment"><p>Another crucial experiment (also simulated in the Mattar and Daw 2018) is the phenomenon of non-local replay. Reviewers agree that it's important that computational models of replay capture these characteristic aspects of replay.</p></disp-quote><p>In our previous simulations of awake replay we initialized replay at the agent’s current location as a very simple way of modeling the bias of replay to start at an animal’s current location (Davidson et al., 2009). This is, of course, rather artificial and also does not allow for non-local awake replay, as the reviewers rightly pointed out. By contrast, since offline replay during sleep appears to be less biased by the animal’s current location, we stochastically initialized sleep replay in our model. By applying the same stochastic initialization to online replay, our model also produces non-local replays. We tested the occurrence of non-local awake replays in our simulation of Gupta et al. (2010) in the figure-8 maze as well as in an open field environment with uniform experience strengths (Figure 2—figure supplement 2A, B). Replay exhibited non-local replays with a preference for rewarded and often visited locations in both environments, but a bias for the current location only emerged in the figure-8 maze (Figure 2—figure supplement 2A, B). That bias can be reintroduced by increasing priority ratings at the current location (Figure 2—figure supplement 2CF). We expanded the section of the main text which introduces the initialization of replay and added a supplementary figure (Figure 2—figure supplement 2) demonstrating how our model can also account for non-local awake replays.</p><disp-quote content-type="editor-comment"><p>2) Reviewers also agreed that it is important that authors perform further simulation analyses to address other experiments simulated in Mattar and Daw 2018. These include responsiveness of reverse replay to reward change, replay counterintuitively entering an aversive zone, and the effect of experience on replay frequency. Even if the proposed model cannot account for all of these findings, it is important that authors transparently report them (maybe with supplementary figures) and discuss them extensively.</p></disp-quote><p>Our model focuses on the statistics of replayed locations when replay occurs and does not have a mechanism for determining the frequency and duration of replay events. Hence, our model produces replay at fixed times and with a fixed number of replay steps. This places some aspects of the studies mentioned by the reviewers out of the scope of our model. In principle, this also holds for other similar models, including the one by Mattar and Daw (2018). Nonetheless, our model may still account for some aspects of the findings. We added a new section in the discussion which addresses the current limitations of our model in more detail (“Current Limitations of SFMA”).</p><p>The responsiveness of reverse replay to reward change (Ambrose et al., 2016) can in part be accounted for by SFMA’s dynamic mode. We ran simulations in open field and T-maze environments and induced positive and negative reward changes during learning (Figure 3—figure supplement 3). The dynamic mode determines the probability of activating the reverse mode based on the recent history of temporal difference errors. Therefore, our model produces more reverse replays whenever there are prediction errors regardless whether the reward changes are positive and negative. Ambrose et al. indeed find the increase in reverse replay when rewards are increased, but, in contrast to our modeling results, reverse replay decreases when rewards are decreased. Ambrose et al. find very few SWRs. Neither our model nor the one by Mattar and Daw can explain this result, since both use a fixed number of replay steps in their model. However, they analyze the number of “significant” replays and that number decreases with reduced reward due to the resulting absence of gain, and gain is the sole driver of reverse sequences in their model.</p><p>Our model can account for replays representing paths into an aversive zone. The only requirement is some previous exposure to the environment. We included an additional simulation of the experiments by Wu et al. (2017) in the main text demonstrating that the (biased) experience is sufficient to explain replay entering the aversive zone (Figure 9A). We further show that the generated replays can support the learning of shock-avoiding behavior (Figure 9B).</p><p>We consider the decrease in the amount of replay across sessions (Cheng and Frank, 2008) to be outside the scope of our model (see above). Mattar and Daw also use a fixed number of replay steps in their model. However, they analyze the number of “significant” replays and that number decreases with familiarity. We actually find a similar effect for replay generated with the default mode. During early learning stages our model produces multiple short sequences. With growing experience this number decreases but the sequence length increases (Figure 3—figure supplement 1, Figure 3—figure supplement 2, Figure 3—figure supplement 4).</p><disp-quote content-type="editor-comment"><p>3) The fact that the model has two modes (default and reverse) to account for empirical findings is also a major issue. Reviewers agree that the underlying mechanism regarding these two modes is not clear. Do they work in parallel? or is there a dynamic switch between the two? How does the dynamic switch work computationally? And how is this aspect of the model related to experimental data? Please see specific comments by Reviewers 2 and 3 about this issue.</p></disp-quote><p>We acknowledge that the need for and the implementation of the two modes of our model were not made clear enough in the manuscript. In SFMA’s dynamic mode, replay is generated in either default or reverse mode depending on the recent history of temporal difference errors. To be more precise, the more temporal difference errors were encountered, the higher the probability is for activating the reverse mode. This will be the case during learning, when the an environment is novel (Figure 3) or when the reward changes (Figure 3—figure supplement 3). This error-signal-induced switch is consistent with the increase in reverse replay due to positive reward changes (Ambrose et al., 2016). We clarified the description of the dynamic mode and its connection to experimental findings in the manuscript.</p><p>How might default and reverse modes be implemented by a biological network? The default mode could be implemented as an attractor network where each experience’s current state is stored as an attractor. The currently reactivated experience/state would be represented by a bump of activity in the network. Due to self-inhibition the bump would be driven to leave the current attractor state and move to a close-by (i.e., similar) one.</p><p>The reverse mode requires two attractor networks, net0 and net1, to store the current and subsequent states of each experience, respectively. They are assumed to use the identical representations. In addition, the state transitions (s<sub>t</sub>, s<sub>t+1</sub>) are encoded in the weight matrix w<sub>10</sub> between net1 and net0. An experienced transition is stored by learning a connection from s<sub>t+1</sub> in net1 to s<sub>t</sub> in net0. When an experience is replayed in the reverse mode, the pattern representing s<sub>t+1</sub> is activated in net1, which in turn excites the state s<sub>t</sub> in net0. The weights w<sub>01</sub> from net0 to net1 are assumed to be one-to-one connections, so that the activity pattern in net0 activates the most similar pattern in net1, equivalent to our algorithm finding, in all experiences, the next state that is closest to the current state of the reactivated experience. And the process of reactivating experiences iterates from here, thereby resulting in reverse replay.</p><p>Since the reverse replay has to perform one more computational step than the default mode, i.e., the transition from s<sub>t+1</sub> to s<sub>t</sub>, this would suggest that generating sequence in the reverse mode is slower, which is consistent with experimental observations (Liu et al., 2021).</p><p>We have added these clarifications to the manuscript.</p><disp-quote content-type="editor-comment"><p>4) In addition, individual reviewers made a number of suggestions to improve the manuscript. Please carefully consider these additional points when preparing the revised manuscript.</p></disp-quote><p>Further Manuscript Changes</p><p>We added a link to the Github repository with the simulation code in the Methods section:</p><p>“The code for all simulations has been made available on Github:</p><p>https://github.com/sencheng/-Mechanisms-and-Functions-of-Hippocampal-Replay.&quot;</p><p>We added details missing in the subsection “Simulation: Navigation Task”:</p><p>“Simulations were repeated with different discount factors = {0.1, 0.2, …, 0.9} for the DR and their performance was averaged.”</p><p>It was pointed out that our critique of Mattar and Daw (2018) could be interpreted as overtly aggressive/negative. This was not our intention. We therefore rephrased our points of critique in a more neutral manner. Furthermore, rather than opposing the Mattar and Daw model we position SFMA as a complementary model, which gives an account for how the utility-based prioritization could be approximated by hippocampus.</p><p>We shortened the legend text of Figure 3 since it was rather lengthy and the same information is conveyed in both the main text as well as legend text of Figure 3—figure supplement 1.</p><p>We added the author contributions.</p><p>The enumeration of supplementary figures changed because new ones were added:</p><list list-type="bullet"><list-item><p>Supplementary Figure 1 → Figure 2—figure supplement 1</p></list-item><list-item><p>Supplementary Figure 2 → Figure 2—figure supplement 2</p></list-item><list-item><p>Supplementary Figure 3 → Figure 3—figure supplement 1</p></list-item><list-item><p>Supplementary Figure 4 → Figure 3—figure supplement 2</p></list-item><list-item><p>Supplementary Figure 5 → Figure 3—figure supplement 3</p></list-item><list-item><p>Supplementary Figure 6 → Figure 4—figure supplement 1</p></list-item><list-item><p>Supplementary Figure 7 → Figure 4—figure supplement 2</p></list-item><list-item><p>Supplementary Figure 8 → Figure 4—figure supplement 3</p></list-item><list-item><p>Supplementary Figure 9 → Figure 4—figure supplement 4</p></list-item><list-item><p>Supplementary Figure 10 → Figure 4—figure supplement 5</p></list-item></list><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>My point is especially important in relation to the model by MandD. For example, does the current model explain evidence that replay over-represents reward locations (e.g. Figure 4 in MandD)? I suggest conducting additional simulation analysis, particularly for the experimental data explicitly presented in MandD and discussing any of those experiments that cannot be explained by the current model (if any).</p></disp-quote><p>The suggested simulations and analyses were added along with an additional section addressing our model’s current limitations in the discussion. We address the reviewer’s recommendation in more detail in our reply to the essential revisions 1 and 2.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In their paper, Diekmann and Cheng describe a model for replay generation that is able to replicate multiple seminal findings in the field. Specifically, in their model experiences get selected for replay depending on their strength (which is in turn determined by their frequency of occurrence and reward), their similarity to other experiences and inhibition that prohibits experiences that match the current one too much from being reactivated (which leads to the generation of replay sequences). Further, they adopt two different methods for computing similarity which they show produces complementary results. With this rather simple and intuitive model Diekmann and Cheng are able to re-produce important, but divergent, findings in the field. For example, why replay switches directionality at different time points on a maze, how it can generate synthetic experiences, respects boundaries, etc. Being able to account for these different findings with a single mechanism is an important goal and no small feat.</p><p>However, I still have some reservations about how much the current model differentiates itself – and represents an improvement – of a recently published model (the so-called prioritized memory access (PMA) model for replay Mattar and Daw (2018)). Further, although their model provides a relatively comprehensive account of multiple replay findings, it doesn't address many studies that show awake replay is non-local (even depicting experiences in remote environments). Finally, I would like the authors to elaborate on how their 'default' and 'reverse' policy could be implemented biologically and how these two modes relates to current thinking in the replay field. I elaborate on my points below:</p><p>– Benefit over PMA model: The authors maintain that their model represents a significant improvement from the PMA model as the latter is not biologically feasible. In the PMA model, replay gets selected based on the utility of a possible experience reactivation which is partly derived from its &quot;gain&quot; term. Here, the authors' critique is that if the calculation of gain requires access to all stored experiences, this becomes very computationally laborious and biologically unfeasible. Yet, it's not clear to me in the current model's implementation that it represents a clear advantage in this way. Namely, to compute similarity – which influences replay prioritization – the current experience state needs to be compared with all other experience states (or tuples). Thus, does their model not also require access to all stored experiences? If not, I think the authors need to spell out a bit more about how this still makes their model more efficient and biologically implementable than the PMA model. This is a particularly important point as one of the main 'selling points' of the paper is that it achieves what the PMA model achieves but in a physiologically realistic way.</p></disp-quote><p>The reviewer is correct in stating that our model requires access to all experiences stored in memory – just like PMA does. To us, this is a reasonable assumption. The issue we are concerned about is the computation of the gain in PMA, which requires computing the change in the Q-function that would result, if a particular experience were reactivated. So, the brain would have to perform hypothetical updates to its network for each experience in memory, compute and store the gain for each experience, and then reactivate the experience with the highest gain. Since biological learning rules operate on synapses based on neural activity, we believe that it is unlikely that hypothetical updates can be computed and their outcome stored without altering the network. We clarified this point in the manuscript.</p><p>We added to the main text:</p><p>“Their model accounts for multiple observed replay phenomena. However, it is unclear how brains could compute or even approximate the computations required by Mattar and Daw's model. The brain would have to perform hypothetical updates to its network for each experience stored in memory, compute and store the utility for each experience, and then reactivate the experience with the highest utility. Since biological learning rules operate on synapses based on neural activity, it appears unlikely that hypothetical updates”.</p><disp-quote content-type="editor-comment"><p>– The authors show the model does well in replicating multiple replay findings in the field. However, they focus primarily on awake replay findings that show some relationship to current behaviour- e.g. depicting short cuts to goal locations, activating paths just taken, etc. However, many replay studies show that replay is very often non-local (e.g. Karlsson and Frank (2009); Olafsdottir et al. (2017)). How can their model accommodate these findings?</p></disp-quote><p>Our model can produce non-local awake replays if online replays are initialized in the same way as offline replays. We demonstrate this in additional simulation analysis in a new supplementary figure (Figure 2—figure supplement 2). We address the reviewer’s recommendation in more detail as part of our reply to the essential revision 1.</p><disp-quote content-type="editor-comment"><p>– In the current model there are two modes (policies) for computing experience similarity described; the default and reverse mode. The authors show you need both to effectively account for different replay phenomena (e.g. you need reverse for reward learning, particularly in environments where an animal's path is not prescribed). But what determines which policy is used? How would switching between the two be implemented in neural networks? Or do they propose they work in parallel?</p></disp-quote><p>Our model decides which of the modes should be used based on the recent (i.e., since that last trial) temporal difference errors. The probability of replay being generated using the reverse mode increases with the amount of temporal difference errors encountered. Neurons encoding reward prediction error have been known for (Schultz et al., 1997) and could be a trigger for the “switch” between replay modes. We address the reviewer’s recommendation in more detail as part of our reply to the essential revision 3.</p><disp-quote content-type="editor-comment"><p>– The default and reverse mode resonate with some contemporary theories of replay. Namely, a commonly held view is that awake replay may support planning/memory retrieval etc. whereas replay during rest periods supports memory consolidation. Perhaps the authors can elaborate on how their different modes, which resemble somewhat the planning vs consolidation mode, relate to these theories. Could it be that the default mode may be a sort of a planning mode – supporting on-going behaviour – whereas the reverse mode is more about learning?</p></disp-quote><p>We agree with reviewer and already discussed the different potential functions (including planning) of both replay modes in the discussion (“The Role of the Reverse and Default Modes”). However, we acknowledge that our phrasing might not have been clear enough. Hence, we rephrased parts of the discussion.</p><p>“Another function often attributed to hippocampal replay is that of planning (Diba and Buzsáki, 2007; Buckner, 2010; Buhry et al., 2011) by simulating possible future trajectories to inform subsequent behavior. The default mode could, in principle, support this function.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>In multiple instances the authors refer to the biological plausibility of their model, and the lack thereof in the model by Mattar and Daw. Intuitively, I agree with the authors. However, to my knowledge we don't know what is biologically plausible and what is not. I suggest some rewording to emphasize that their model is simpler and less computationally taxing compared to Mattar and Daw. Further, I would appreciate a more explicit discussion of the biological plausibility of their model. They get into this to some degree in the discussion, particularly with regards to the return of inhibition term (i.e., line 401). However, the description of how experience strength or distance could be computed could certainly be expanded beyond just referencing other studies. Importantly, the model occurs in two modes. How might the hippocampus gate these two modes?</p></disp-quote><p>We agree with the reviewer that it is not exactly known what qualifies as biological plausible. Nonetheless, we would argue that the specific computations of PMA are unrealistic under the consideration of biological learning rules which operate on synapses based on neural activity. Namely, to compute the gain, the brain would have to perform hypothetical updates to its network for each experience in memory, compute and store the gain for each experience, and reactivate the experience with the highest gain. We find it unlikely that these computations can be performed without altering the network. We clarified this point in the manuscript and rephrased our criticism.</p><p>We extended the discussion of the biological plausibility of our model. Specifically, we more strongly relate experience similarity/distance to the activity of place cells which are theorized to encode just such a metric (Stachenfeld et al., 2017; Piray and Daw, 2021). Experience strength could be realized by place cell assemblies increasing their excitability in response to experience and reward. The replay mode switch could be triggered by neuron encoding reward prediction error (Schultz et al., 1997) and this is in part consistent with the increase of reverse replay following positive reward change (Ambrose et al., 2016, see our answer to Essential Revision 2). We address the reviewer’s recommendation in more detail as part of our reply to the essential revision 3.</p><disp-quote content-type="editor-comment"><p>In general I found the methods to be exquisitely articulated and comprehensive. However, the description of how replay is generated in their model could be expanded/clarified in the main text:</p><p>– I understood that an experience consists of a transition between two states (in addition to reward/action)- and that the entire experience (including both states) is reactivated during replay. However I think a supplementary figure showing how exactly one gets from the last panel of figure 2A to an ordered set of reactivated states as in Figure 5c could be helpful.</p></disp-quote><p>We acknowledge that the generation of replay sequences might not be very intuitive. We added Figure 2—figure supplement 1 to show how a replay sequence is generated step-by-step.</p><disp-quote content-type="editor-comment"><p>– Most importantly- I think it would be very useful for the authors to show more examples of replay events generated by their model, particularly with respect to Figure 3. For example, they report the directionality of replays in the open field/labyrinth, but I would like to know where the replays actually started/ended, and how this changed over trials. This would help clarify how the replays are behaviorally useful to the agent.</p></disp-quote><p>We added Figure 3—figure supplement 2 with example replays from the replays that were used to produce Figure 3.</p><disp-quote content-type="editor-comment"><p>– The authors could better describe the logic/motivation of the default and reverse states. Initially I assumed that the default mode simply drives forward replay, and the reverse mode reverse replay- but this is not entirely the case. Why did the authors use just 2 modes (the first comparing the similarity between experience 1-state 1 and experience 2 state-1, and the second comparing the similarity between experience 1-state 1 and experience 2 state-2)- why not, for example, have other modes comparing experience 1-state 2 and experience 2 state-1, or experience 1-state 2 and experience 2 state-2? Many readers would benefit from a bit more intuition in terms of linking these modes to biological phenomenon.</p></disp-quote><p>The reviewer is correct that technically we could have simply stored sequences of experiences and replayed them in forward or reverse order. However, we wanted to develop and study a model for a simple mechanism that generates replay sequences. The reviewer is also correct that formally two more replay modes could be defined in our model. Let us call the other two cases “forward” mode (comparing experience 1-state 2 and experience 2 state-1) and “attractor” mode (comparing experience 1-state 2 and experience 2 state-2). These were not consistent with the goals of our study. In our paper, we wanted to study replay in our model that 1. generate sequences similar to the ones observed during SWR in the rodent hippocampus and 2. contribute in a specific way to learning. The attractor mode, and only this one mode, violates condition 1. since it looks for experiences that end in the same state, so that neighboring experiences are activated in a ring-like fashion (Figure 2—figure supplement 3). A number of previous studies have shown that reverse sequences are particularly efficient at propagation reward information backwards to decision points. So, condition 2 favors those modes (default and reverse) that consistently generate reverse sequences. The forward mode predominantly produces forward replays, which are not efficient for learning, but could very efficiently support planning – which is outside the scope of the current study. Note, that even though the default mode generally produces fewer reverse sequences than the reverse mode, in some situations it might predominantly generate reverse sequences. In addition, the default mode is more flexible in generating transitions between states that were never experienced, something that could be useful for discovering novel solutions.</p><p>In addition to adding Figure 2—figure supplement 3, we added a the above explanation to the results near l. 160.</p><disp-quote content-type="editor-comment"><p>– Can a single replay consist of both forward and reverse transitions? As mentioned above, a panel of example replays upfront could be very useful.</p></disp-quote><p>Indeed, this can happen, but this is limited to the default mode. Figure 3—figure supplement 2 shows some replays (during early learning phases) that exhibit this.</p><disp-quote content-type="editor-comment"><p>– Please confirm that only one map was constructed for linear environments. What was the reason for this choice, and would it impact the results to use two maps (one for each running direction, as in Mattar and Daw, and consistent with hippocampal fields on a linear track)?</p></disp-quote><p>We used only one map. If two maps were used and they were independent, the results would not be affected. If one-way transitions between the maps (current and next map) were allowed, then replay produced with the default mode at the end of a trial (end of the current map) could either be reverse (in the current map) or forward (in the next map). This would change the proportion of forward and reverse replay. However, this does not change any of the claims in this paper. That is why we used only one map for simplicity.</p><disp-quote content-type="editor-comment"><p>– A description of how the default versus reverse modes are selected in the dynamic mode is provided in the methods, but should also be described in the main text.</p></disp-quote><p>We added the description to the main text:</p><p>“Put simply, in the dynamic mode the probability of generating replay using the reverse mode increases with the TD errors accumulated since the last trial (for more details see Methods).”</p><disp-quote content-type="editor-comment"><p>– The definition of a replay epoch is in the methods but might be moved to the main text.</p></disp-quote><p>We moved the pseudo code summary of the replay epoch to the main text.</p><disp-quote content-type="editor-comment"><p>Can this model account for a broader scope of replay phenomena including: the sensitivity of reverse replay to reward magnitude or reward prediction error, over-representation of reward locations, or the tendency of forward replays to counter-intuitively lead towards an aversive (negatively rewarding) locations? As Mattar and Daw demonstrated that their model produces patterns of replay that match these arguably fundamental empirical findings, examining the fully overlapping set of scenarios would allow the models to be compared head-to-head. It is unclear whether either model can account for the recent observations that replay is enriched for locations not visited recently (Gillespie et al., 2021) or trajectories directed away from the animal's preferred goal location (Carey et al., 2019). I do not mean to suggest that the authors needs to perform all of the above tests. However, it would be of interest to at least include discussion about which empirical findings the current model could likely not account for.</p></disp-quote><p>We address the reviewer’s recommendation regarding the listed replay phenomena in our reply to the essential revision 2. The two recent studies mentioned by the reviewer (Carey et al., 2019. Gillespie et al., 2021) are indeed intriguing. We would agree with the reviewer that neither our model nor Mattar and Daw’s in their current form could account for these findings. A recent model presented by Antonov et al. (2022) which is based partly on PMA and includes “forgetting” in their learning process might account for both of them. We added a discussion on the studies by Carey et al. (2019) and Gillespie et al. (2021) as part of the section “Current Limitations of SFMA”. Furthermore, we also discuss how SFMA could be extended with a sort of recency prioritization to account for these two studies.</p><disp-quote content-type="editor-comment"><p>Line 160: 'in our simulations the reverse mode produces more updates that were optimal than did the default mode'. Please define an optimal (policy?) update.</p></disp-quote><p>The optimal update was defined here as the update with the highest “gain”. We rephrased the sentence to clarify what is meant by optimal:</p><p>“in our simulations the reverse mode produces more updates that were optimal, i.e., they had the highest utility according to Mattar and Daw (2018)’s model, than the default mode did”.</p><disp-quote content-type="editor-comment"><p>Line 165: I'm not sure what the authors mean by 'mixed' replays. Are they describing replays that traverse in one direction and then switch to the other (i.e., reverse to forward), as in Wu and Foster 2014?. If not, a better word choice might be 'interspersed'.</p></disp-quote><p>We changed the wording to ‘interspersed’ as suggested.</p><disp-quote content-type="editor-comment"><p>Line 188: Are there any statistical tests to accompany the statement that these log-log plots are linear?</p></disp-quote><p>At the current stage of this field, we are still focusing on qualitatively accounting for experimental observations, therefore quantitative comparisons between model and experimental data, as well as quantitative analyses of model data, are currently not constructive. Thus, we did not perform statistical tests to check for the linearity in the log-log plots.</p><disp-quote content-type="editor-comment"><p>Line 223: It is not clear to me what the alternating-alternating condition controls for.</p></disp-quote><p>We considered the alternating-alternating condition a kind of baseline in which the experience strengths of both maze sides is roughly balanced throughout the simulation. We changed the wording from ‘control’ to ‘baseline’.</p><disp-quote content-type="editor-comment"><p>Line 234: The explanation about the observed differences in short-cut replays under the different behavioral paradigms was somewhat confusing. A figure showing the relative amount of experience in each segment of the maze in each paradigm could help clarify.</p></disp-quote><p>This is a very good suggestion that we gladly followed (Figure 5—figure supplement 2).</p><disp-quote content-type="editor-comment"><p>Line 248: While it is interesting to ask whether the replay generated by your model facilitates learning in the task, I think it would be of broader interest and more informative to show specifically which replays are useful. For example, what was the distribution of replays that the default and reverse mode yielded on that task? How does task performance change if you limit replay to a subset, such as only reverse replays starting at the goal, or, most relevant here- only-short cut replays?</p></disp-quote><p>We fully agree with the reviewer that these questions are very interesting and important to answer. Some of these aspects, we had already address in the manuscript, e.g., the distribution of replays in default and reverse mode (Figure 3—figure supplement 1). However, we feel that further analysis of which subtypes of replay might drive learning differentially opens new questions that need to be addressed comprehensively in a separate manuscript.</p><disp-quote content-type="editor-comment"><p>Figure 6A: What are reactivation maps? I.e., is this a summary or average of all reactivations, or an example of an individual replay epoch (I assume it's the former, but that should be specified). Individual replay examples would also useful.</p></disp-quote><p>Indeed, the reactivation map is the average of all reactivations. We made the figure description clearer. We have also included more examples of individual replay events in Figure 2—figure supplement 1 and Figure 3—figure supplement 2.</p><disp-quote content-type="editor-comment"><p>Line 329: 'General representations of task structure in EC have been proposed to be forming hippocampal place cell code together with sensory information'- I suspect there was a typo in this sentence; I was unable to decipher its meaning.</p></disp-quote><p>We rephrased the sentence as:</p><p>“Hippocampal place cell code has been proposed to be formed by combining a learned representation of the task structure (e.g., space) with sensory information (Whittington et al., 2020).”.</p><disp-quote content-type="editor-comment"><p>Line 409: 'We argue that the minimum value should be applied after the summation of gain values&quot; – what is the argument for applying the minimum value after summation? In general I wonder if the description of PMA's flaws (starting at line 404) should be moved to the related Supplemental Figure legend. I think more detail would be needed for most readers to understand the apparent error (for example, what is the 'special n-step update', what does it mean to apply before summation or after summation, and what makes one version better than the other)?</p></disp-quote><p>Our main criticism regarding the gain computation is that applying the minimum value artificially inflates the gain of sequences. We clarified the text regarding the n-step update and included parts of the text in the supplementary figure’s description (Figure 4—figure supplement 5).</p><disp-quote content-type="editor-comment"><p>Figure 5e: Please state in the figure legend what happened at trial 100. Also, as noted elsewhere, I think it would be useful to compare performance with the dynamic mode (and possibly the PMA model).</p></disp-quote><p>The reward shifted from left to right. We added clarification in the figure.</p><disp-quote content-type="editor-comment"><p>Figure 5/Supplementary Figure 7: I'm confused as to why in Figure 5 there is little difference between the forward and reverse modes in the initial learning of the task, but a difference after the reward location switch, whereas in Supplementary Figure 7 the exact opposite pattern is shown on the T-Maze. In general, what explains the differences in the initial learning rate and the learning rate after the reward switch, especially on the T-maze? I don't see what information the agent learned in the initial phase that remained useful after the switch. Perhaps showing examples of the agents behavioral trajectory/decisions at different stages of the task would help.</p></disp-quote><p>According to our modeling results two principles account for the difference pointed out by the reviewer: Reverse replay drives learning much more efficiently than random or forward replays and the default mode generates different fractions of forward and reverse replays depending on the statistics of the experiences. In Figure 5, choice was restricted to the decision point after which the agent would run straight forward until the next trial (running forward was imposed by the experimenter in Gupta et al. 2010). Hence, the task was quite simple and both replay modes lead to fast learning in the beginning. After the reward shift at trial 100 the agent has to unlearn the previously rewarded behavior. At the reward location, replay generated in the default mode can propagate either forward or in reverse order, whereas in the reverse mode sequences propagate only back towards the decision point. This makes the reverse mode perform better in unlearning the previously rewarded behavior in this specific task. For the simulations in the T maze shown in Figure S7 (now Figure 5—figure supplement 3), the agent can move freely and hence there is a difference in initial learning speeds between default and reverse mode, since the latter generates reverse sequences more reliably. However, when the rewarded side switches and the agent initially turns incorrectly, replay propagates backward in both replay modes because the agent is located at the end of the arm. So, the new reward information propagates with the same efficiency in both modes. Note, that in the open field (Figure 5—figure supplement 3, bottom) the default mode leads to lower learning speeds both during initial learning and after the reward is relocated, because behavior is less constraint and the default always generates fewer reverse sequences.</p><disp-quote content-type="editor-comment"><p>Supplementary Figure 3: what is the take-home message of showing the displacement distributions with different inverse temperature parameters? Is the point that the results hold when using different temperatures? If so, why not provide a more quantitative metric, like the log-log plots using different inverse temperature parameters?</p></disp-quote><p>The reviewer is partially correct in their assumption. The second motivation is to show that higher inverse temperature leads to less stochastic “diffusion” of replay sequences (Figure 4—figure supplement 2 B and D, as compared to A and C, respectively). The displacement distributions more intuitively demonstrates this than log-log plots would. We have clarified our motivation in the figure caption of Figure 4—figure supplement 2.</p><disp-quote content-type="editor-comment"><p>Supplementary Figure 8: The main text indicates that this figure was generated using the reverse mode, but the legend doesn't say so.</p></disp-quote><p>We changed the figure (Figure 6—figure supplement 1) to indicate that the reverse mode was used.</p><disp-quote content-type="editor-comment"><p>Methods line 463: 'The term in brackets is known as the temporal difference error'. This could perhaps be explained a bit more.</p></disp-quote><p>We added further explanation regarding the temporal difference error:</p><p>“The term in brackets is known as the temporal difference error, and represents by how much the last action and potentially collected reward has changed the expected discounted future reward associated with the state.”</p></body></sub-article></article>