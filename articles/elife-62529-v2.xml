<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">62529</article-id><article-id pub-id-type="doi">10.7554/eLife.62529</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Meta-Research</subject></subj-group></article-categories><title-group><article-title>Journal policies and editors’ opinions on peer review</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-204412"><name><surname>Hamilton</surname><given-names>Daniel G</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8104-474X</contrib-id><email>hamilton.d@unimelb.edu.au</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Daniel G Hamilton</bold> is in the Interdisciplinary Metaresearch Group, School of BioSciences, University of Melbourne, Victoria, Australia</p></bio></contrib><contrib contrib-type="author" id="author-206239"><name><surname>Fraser</surname><given-names>Hannah</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2443-4463</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Hannah Fraser</bold> is in the Interdisciplinary Metaresearch Group, School of BioSciences, University of Melbourne, Victoria, Australia</p></bio></contrib><contrib contrib-type="author" id="author-206240"><name><surname>Hoekstra</surname><given-names>Rink</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1588-7527</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Rink Hoekstra</bold> is in the Department of Educational Sciences, University of Groningen, Groningen, Netherlands</p></bio></contrib><contrib contrib-type="author" id="author-206238"><name><surname>Fidler</surname><given-names>Fiona</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Fiona Fidler</bold> is in the Interdisciplinary Metaresearch Group, School of BioSciences, and the School of Historical and Philosophical Studies, University of Melbourne, Victoria, Australia</p></bio></contrib><aff id="aff1"><label>1</label><institution>Interdisciplinary Metaresearch Group, School of BioSciences, University of Melbourne</institution><addr-line><named-content content-type="city">Melbourne</named-content></addr-line><country>Australia</country></aff><aff id="aff2"><label>2</label><institution>Department of Educational Sciences, University of Groningen</institution><addr-line><named-content content-type="city">Groningen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution>School of Historical and Philosophical Studies, University of Melbourne</institution><addr-line><named-content content-type="city">Melbourne</named-content></addr-line><country>Australia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>19</day><month>11</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e62529</elocation-id><history><date date-type="received" iso-8601-date="2020-08-27"><day>27</day><month>08</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-11-18"><day>18</day><month>11</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Hamilton et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Hamilton et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-62529-v2.pdf"/><abstract><p>Peer review practices differ substantially between journals and disciplines. This study presents the results of a survey of 322 editors of journals in ecology, economics, medicine, physics and psychology. We found that 49% of the journals surveyed checked all manuscripts for plagiarism, that 61% allowed authors to recommend both for and against specific reviewers, and that less than 6% used a form of open peer review. Most journals did not have an official policy on altering reports from reviewers, but 91% of editors identified at least one situation in which it was appropriate for an editor to alter a report. Editors were also asked for their views on five issues related to publication ethics. A majority expressed support for co-reviewing, reviewers requesting access to data, reviewers recommending citations to their work, editors publishing in their own journals, and replication studies. Our results provide a window into what is largely an opaque aspect of the scientific process. We hope the findings will inform the debate about the role and transparency of peer review in scholarly publishing.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>peer review</kwd><kwd>academic publishing</kwd><kwd>editorial policies</kwd><kwd>meta-research</kwd><kwd>publication ethics</kwd><kwd>data sharing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A survey of journals and editors in five areas of research - ecology, economics, medicine, physics and psychology - reveals a range of differences in their approach to peer review.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Almost all scientists who pursue publication of their research via academic journals will be familiar with the scrutiny of their work by their peers. This process of journal-organised, pre-publication evaluation to guide editorial decision-making, commonly referred to as ‘peer review’, is a standard procedure currently employed by over 65,000 English-language journals (<xref ref-type="bibr" rid="bib47">Ulrichsweb, 2020</xref>). However, beyond this broad definition, it is unclear exactly what peer review entails in practice.</p><p>Peer review comes in many forms, with each journal adopting their own unique set of policies governing aspects of the process such as: the amount and range of expertise solicited, the level of anonymity afforded, the availability of documentation to reviewers and the readership and the degree of interaction between stakeholders. Given the role peer review ostensibly plays in quality control, and the diversity of models currently in use, understanding what peer review is, and what it does, is considered a high priority (<xref ref-type="bibr" rid="bib45">Tennant and Ross-Hellauer, 2020</xref>).</p><p>Despite its diverse, and customarily closed nature, peer review continues to be widely perceived as a valuable endeavour that adds credibility to scholarship by both researchers (<xref ref-type="bibr" rid="bib26">Nicholas et al., 2015</xref>) and the public (<xref ref-type="bibr" rid="bib33">Pew Research Center, 2019</xref>). It is also widely viewed as a process that improves research quality and weeds out irrelevant and flawed work (<xref ref-type="bibr" rid="bib26">Nicholas et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Taylor and Francis, 2015</xref>; <xref ref-type="bibr" rid="bib52">Ware and Publishing Research Consortium, 2016</xref>). It is likely for these reasons that most researchers see peer review as part of their job, and consequently volunteer 68.5 million hours a year globally on reviewing (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>).</p><p>Most empirical research on peer review has centred on investigating criticisms of it being ‘an ineffective, slow, expensive, biased, inefficient, anti-innovatory, easily-abused lottery’ (<xref ref-type="bibr" rid="bib43">Smith, 2010</xref>). However, research efforts have done little to allay concerns, with studies reporting low error detection rates by reviewers, inconsistency among reviewers’ recommendations, and biases favouring prestigious institutions and established theories (<xref ref-type="bibr" rid="bib28">Nylenna et al., 1994</xref>; <xref ref-type="bibr" rid="bib38">Schroter et al., 2004</xref>; <xref ref-type="bibr" rid="bib39">Schroter et al., 2008</xref>; <xref ref-type="bibr" rid="bib19">Mahoney, 1977</xref>; <xref ref-type="bibr" rid="bib32">Peters and Ceci, 1982</xref>; <xref ref-type="bibr" rid="bib17">Kravitz et al., 2010</xref>). Scholars also point to examples of important scientific breakthroughs that were initially rejected following peer review, such as the development of the radioimmunoassay (<xref ref-type="bibr" rid="bib55">Yalow, 1978</xref>), as well as peer-reviewed articles that were later retracted due to significant quality concerns, such as the recent high-profile articles in <italic>The Lancet</italic> and <italic>The New England Journal of Medicine</italic> that reported on the utility of chloroquine treatments for COVID-19 (<xref ref-type="bibr" rid="bib23">Mehra et al., 2020a</xref>; <xref ref-type="bibr" rid="bib24">Mehra et al., 2020b</xref>). We also see instances of authors, reviewers and editors subverting and abusing the process, such as cases of authors rigging peer review (<xref ref-type="bibr" rid="bib8">Ferguson et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Hopp and Hoover, 2017</xref>), reviewers (and editors) manipulating performance metrics by coercing citations (<xref ref-type="bibr" rid="bib14">Hopp and Hoover, 2017</xref>; <xref ref-type="bibr" rid="bib46">Thombs et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Fong and Wilhite, 2017</xref>; <xref ref-type="bibr" rid="bib13">Ho et al., 2013</xref>) and editors preferentially publishing their research in the journals they edit (<xref ref-type="bibr" rid="bib18">Luty et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Mani et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Shelomi, 2014</xref>). Such examples, while perhaps isolated cases, are rhetorically powerful in creating distrust in the peer review process.</p><p>Criticism of the “black box” nature of peer review has also been a topic of interest since at least the 1990s and is associated with a large body of literature discussing the merits of “opening up” various aspects of the process. Some research has focussed on “open peer review”: most commonly referencing peer review policies that require full disclosure of the identities of authors and reviewers, increased interactions between all stakeholders and publication of review documentation (<xref ref-type="bibr" rid="bib36">Ross-Hellauer, 2017</xref>), while other research has focussed on improving the reporting of journal policies to the scientific community (<xref ref-type="bibr" rid="bib15">Horbach and Halffman, 2020</xref>; <xref ref-type="bibr" rid="bib16">Klebel et al., 2020</xref>). Three such ongoing initiatives include the Transparency in Scholarly Publishing for Open Scholarship Evolution (TRANSPOSE) project (<xref ref-type="bibr" rid="bib16">Klebel et al., 2020</xref>), the Transparency and Openness Promotion (TOP) Guidelines (<xref ref-type="bibr" rid="bib27">Nosek et al., 2015</xref>) and the European Commission's Open Science Monitor (<xref ref-type="bibr" rid="bib30">Parks and Gunashekar, 2017</xref>).</p><p>In the current study, we survey editors of ecology, economics, medicine, physics and psychology journals about how they employ peer review. Amongst other things, we investigated the degree of anonymity and interaction between stakeholders, the availability of review documentation, and the outsourcing of peer review. We also collected information about policies on recommending reviewers, voluntary disclosure of identities, editors altering reviewers’ reports, and the sharing of data, materials and code. In addition, we sought editors’ views on a number of issues in academic publishing: the appropriateness of reviewers co-writing reviews with colleagues or “co-reviewing” (<xref ref-type="bibr" rid="bib21">McDowell et al., 2019</xref>); reviewers suggesting citations to their work; and editors publishing research in the journals they edit. We also gauged editors’ views on reviewers requesting access to raw data during review, the value and role of replication studies, and innovation of their own peer review procedures.</p><sec id="s1-1"><title>Study participants</title><p>A total of 1490 unique editors representing 1500 journals (eight editors represented more than one journal) were invited to participate in this study, which involved two surveys. The first survey (Survey A) contained questions about peer review policies and practices; the second survey (Survey B) covered five issues related to publication ethics (see Materials and methods for further information). In the 13.5 weeks between invitation and deactivation, 336 unique editors entered Survey A, of which 332 consented to participate. Of the 332 journal editors that consented, 300 completed the survey, 22 started but didn’t finish, eight opened the survey but provided no responses and two withdrew consent following completion, giving an overall response rate of 21% (322/1500). Following completion of Survey A, 233 (78%) entered and finished Survey B.</p><p>Of the 322 editors who provided at least one response to Survey A, 293 (91%) identified themselves as the incoming or outgoing lead editor. The target response rate of 17% (N = 50) was achieved in the ecology (N = 90), psychology (N = 84) and economics (N = 80) groups, but not in the medicine (N = 40) or physics (N = 28) groups (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The distribution of impact factors among invited and participating journals by discipline is displayed in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>.</p></sec></sec><sec id="s2" sec-type="results"><title>Results: Peer review policies and practices</title><sec id="s2-1"><title>Pre-review policies</title><p>Just under half of editors reported that their journal routinely checks <italic>all</italic> incoming manuscripts for plagiarism (49%, 154, <xref ref-type="table" rid="table1">Table 1</xref>). The lowest rate of uniform plagiarism checks was reported in the economics group (31%, 24 of 78). The majority of journal editors reported that their journal allows authors to recommend both for and against specific reviewers (61%, 197), with just under a quarter (23%, 73) not providing any routine avenue for authors to influence who reviews their article. One medical journal and one physics journal also reported outsourcing peer review to a commercial third party.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Pre-review policies for all journals and by discipline.</title></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"/><th colspan="2">All journals</th><th colspan="2">Ecology</th><th colspan="2">Psychology</th><th colspan="2">Economics</th><th colspan="2">Medicine</th><th colspan="2">Physics</th></tr><tr><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th></tr></thead><tbody><tr><td colspan="13">Plagiarism software usage (N=317)</td></tr><tr><td> Never </td><td>7</td><td>2</td><td>4</td><td>4</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>4</td></tr><tr><td> Always </td><td>154</td><td>49</td><td>46</td><td>51</td><td>45</td><td>54</td><td>24</td><td>31</td><td>25</td><td>64</td><td>14</td><td>54</td></tr><tr><td> If suspicion has been raised </td><td>84</td><td>26</td><td>24</td><td>27</td><td>19</td><td>23</td><td>30</td><td>38</td><td>4</td><td>10</td><td>7</td><td>27</td></tr><tr><td> At editor's discretion </td><td>54</td><td>17</td><td>14</td><td>16</td><td>13</td><td>15</td><td>18</td><td>23</td><td>5</td><td>13</td><td>4</td><td>15</td></tr><tr><td> I don't know </td><td>5</td><td>2</td><td>0</td><td>0</td><td>3</td><td>4</td><td>2</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td> Other </td><td>13</td><td>4</td><td>2</td><td>2</td><td>3</td><td>4</td><td>3</td><td>4</td><td>5</td><td>13</td><td>0</td><td>0</td></tr><tr><td colspan="13">Recommending reviewers (N=321)</td></tr><tr><td> No </td><td>73</td><td>23</td><td>0</td><td>0</td><td>18</td><td>21</td><td>47</td><td>59</td><td>4</td><td>10</td><td>4</td><td>14</td></tr><tr><td> Yes - Recommend for only </td><td>27</td><td>8</td><td>11</td><td>12</td><td>6</td><td>7</td><td>2</td><td>3</td><td>4</td><td>10</td><td>4</td><td>14</td></tr><tr><td> Yes - Recommend against only </td><td>12</td><td>4</td><td>3</td><td>3</td><td>5</td><td>6</td><td>3</td><td>4</td><td>1</td><td>2</td><td>0</td><td>0</td></tr><tr><td> Yes - Recommend for and against </td><td>197</td><td>61</td><td>75</td><td>83</td><td>51</td><td>61</td><td>23</td><td>29</td><td>29</td><td>72</td><td>19</td><td>68</td></tr><tr><td> Other </td><td>12</td><td>4</td><td>1</td><td>1</td><td>4</td><td>5</td><td>4</td><td>5</td><td>2</td><td>5</td><td>1</td><td>4</td></tr><tr><td colspan="13">Outsourcing peer review (N=318)</td></tr><tr><td> No </td><td>315</td><td>99</td><td>90</td><td>100</td><td>82</td><td>100</td><td>78</td><td>100</td><td>38</td><td>95</td><td>27</td><td>96</td></tr><tr><td> Yes </td><td>2</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>2</td><td>1</td><td>4</td></tr><tr><td> Other </td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>2</td><td>0</td><td>0</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>Interactions and blinding systems</title><p>While most editors reported that their journals encourage interaction between reviewers and the handling editor (73%, 230), few reported encouraging dialogue between fellow reviewers (2%, 7) and authors and reviewers (6%, 20). As to blinding procedures (see <xref ref-type="table" rid="table2">Table 2</xref>), we note the predominance of single-blind systems (author identities are known to the reviewers, but the identities of the reviewers are not known to the authors). We also note that 16 editors (5%) reported using hybrid systems in which authors have the option to conceal their identity in a single-blind system, or reveal it in a double-blind system where authors’ and reviewers’ identities are hidden from each other. Similarly, 55 editors (18%) of blinded journals reported that reviewers are free to reveal their identities to authors if they wish.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Blinding policies for all journals and by discipline.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="left" colspan="2">All journals</th><th colspan="2">Ecology</th><th colspan="2">Psychology</th><th colspan="2">Economics</th><th colspan="2">Medicine</th><th colspan="2">Physics</th></tr><tr><th/><th align="left">N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th></tr></thead><tbody><tr><td>Open identities </td><td align="left">3</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>2</td><td>5</td><td>0</td><td>0</td></tr><tr><td>Single-blind </td><td align="left">176</td><td>57</td><td>68</td><td>78</td><td>16</td><td>20</td><td>33</td><td>43</td><td>33</td><td>87</td><td>26</td><td>100</td></tr><tr><td>Single-blind (hybrid) </td><td align="left">12</td><td>4</td><td>3</td><td>3</td><td>8</td><td>10</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Double-blind (hybrid) </td><td align="left">4</td><td>1</td><td>1</td><td>1</td><td>3</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Double-blind </td><td align="left">109</td><td>36</td><td>15</td><td>17</td><td>51</td><td>65</td><td>40</td><td>52</td><td>3</td><td>8</td><td>0</td><td>0</td></tr><tr><td>Triple-blind </td><td align="left">3</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>2</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></table-wrap><p>Lastly, 24 (8%) and 19 (6%) editors reported that their journal accepts Registered Reports or uses results-free review respectively i.e. uses ‘results-blind’ review (<xref ref-type="bibr" rid="bib3">Button et al., 2016</xref>). Registered Reports were most commonly offered by psychology journals (20%, 16 of 82) and results-free review most often used by economics journals (11%, 8 of 72).</p></sec><sec id="s2-3"><title>Peer review documentation</title><p>Only four editors (1%) reported publishing reviewer reports (signed or unsigned) and decision letters. However, while <italic>public</italic> sharing of peer review documentation remains rare, most editors did report that both reviewer reports (79%, 199) and editorial decision letters (82%, 233) were shared with all reviewers.</p><p>Editors were also asked whether an editor at their journal would ‘be permitted to edit a reviewer's report’ under certain circumstances, and the process they would follow in that case. While most editors stated that their journal didn’t have an official policy on editing reports (84%, 258), 276 (91%) identified at least one situation where editors at their journal would be permitted to alter a reviewer’s report with or without the reviewer’s permission (for an overview of possible situations, see <xref ref-type="table" rid="table3">Table 3</xref>). The two most common circumstances where editing was deemed acceptable were when the review contained offensive language (85%, 247) or discriminatory comments (83%, 242). For example, among medical journal editors, 100% of respondents reported it would be acceptable to edit the report under both of these circumstances. Further to this, 39% of responding editors reported that it would be acceptable to edit a review without the reviewer’s permission if they identified themselves (e.g. signed their review) in a blinded review system. Beyond removing offensive material, inappropriate references and identifying features, 55 editors (19%) reported that it would be acceptable to edit a reviewer’s report if they disagreed with the recommendation; 22 of whom reported this would be acceptable to do so without the reviewer’s permission.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Situations where an editor may edit a reviewer’s report.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">Never acceptable to edit the report</th><th colspan="2">Acceptable to edit without reviewer's permission</th><th colspan="2">Acceptable to edit, but only with reviewer's permission</th></tr><tr><th/><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th></tr></thead><tbody><tr><td>When a reviewer identifies themselves in a blinded peer review framework (N=276)</td><td>93</td><td>34</td><td>109</td><td>39</td><td>74</td><td>27</td></tr><tr><td>When a reviewer has used inappropriate or offensive language (N=291)</td><td>44</td><td>15</td><td>170</td><td>58</td><td>77</td><td>26</td></tr><tr><td>When the reviewer has made an inappropriate reference to an author's gender, age etc (N=290)</td><td>48</td><td>17</td><td>163</td><td>56</td><td>79</td><td>27</td></tr><tr><td>When there are spelling and/or grammatical errors in the review (N=294)</td><td>104</td><td>35</td><td>141</td><td>48</td><td>49</td><td>17</td></tr><tr><td>When the review has English language problems (N=292)</td><td>95</td><td>33</td><td>124</td><td>42</td><td>73</td><td>25</td></tr><tr><td>When the reviewer has left in their comments to the editor (N-290)</td><td>50</td><td>17</td><td>179</td><td>62</td><td>61</td><td>21</td></tr><tr><td>When the editor disagrees with the reviewer's recommendation (N=293)</td><td>238</td><td>81</td><td>22</td><td>8</td><td>33</td><td>11</td></tr></tbody></table></table-wrap></sec><sec id="s2-4"><title>Research output sharing</title><p>The last set of policies in our survey focussed on the sharing of data, research materials and analysis scripts. When asked ‘what is the journal’s current policy on the availability of research data, materials and code following publication’, the two most commonly observed policies were ‘Encourages sharing, but it is not required’ and ‘No policy’ (<xref ref-type="table" rid="table4">Table 4</xref>). Editors of 20%, 15% and 14% of surveyed ecology, medical and economics journals, respectively, declared having mandatory data sharing policies. In contrast, mandatory policies were noted at only one of the surveyed psychology journals, and none of the physics journals. While relatively few editors reported policies requiring data to be shared, 52 (18%) reported that they mandate including a data availability statement specifying whether any data will be shared, and if so, how to access it.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Journal policies on the sharing of research data, materials and code.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">Research data (N=294)</th><th colspan="2">Research materials (N=264)</th><th colspan="2">Research code (N=255)</th></tr><tr><th/><th>N</th><th>%</th><th>N</th><th>%</th><th>N</th><th>%</th></tr></thead><tbody><tr><td>Encourages sharing but it is not required</td><td>168</td><td>57</td><td>143</td><td>54</td><td>133</td><td>52</td></tr><tr><td>Must make available if requested</td><td>41</td><td>14</td><td>29</td><td>11</td><td>32</td><td>13</td></tr><tr><td>In-text statement required</td><td>52</td><td>18</td><td>34</td><td>13</td><td>29</td><td>11</td></tr><tr><td>Requires posting to a trusted repository</td><td>34</td><td>12</td><td>16</td><td>6</td><td>19</td><td>7</td></tr><tr><td>No policy</td><td>65</td><td>22</td><td>69</td><td>26</td><td>65</td><td>25</td></tr><tr><td>Not applicable</td><td>10</td><td>3</td><td>13</td><td>5</td><td>11</td><td>4</td></tr><tr><td>I don't know</td><td>4</td><td>1</td><td>4</td><td>2</td><td>6</td><td>2</td></tr><tr><td>Other</td><td>8</td><td>3</td><td>7</td><td>3</td><td>7</td><td>3</td></tr></tbody></table><table-wrap-foot><fn><p>Percentages do not add up to 100% due to multiple answers being possible</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s3" sec-type="results"><title>Results: Editors' views on publication ethics</title><p>Participating editors were also asked about their views on five issues related to publication ethics issues, and asked to describe what, if anything, they would change about how their journal conducts peer review (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Participating editors’ <italic>in principle</italic> stances on the six topics raised in Survey B.</title><p>The figures presented are limited to statements that provided a clear view for or against the topic of interest. An interactive version of this figure reporting results by discipline can be viewed at <ext-link ext-link-type="uri" xlink:href="https://plotly.com/~dghamilton/9/">https://plotly.com/~dghamilton/9/</ext-link> (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Source data for the figure can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/osf.io/cy2re">https://doi.org/10.17605/osf.io/cy2re</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62529-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Survey response rate by discipline.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62529-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Distribution of impact factors among invited and participating journals by discipline.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62529-fig1-figsupp2-v2.tif"/></fig></fig-group><sec id="s3-1"><title>Co-writing reviews with colleagues (co-reviewing)</title><p>Of the 203 editors that provided a codable position on the topic, 88 (43%) personally encouraged co-reviewing, 70 (34%) reported that it is an acceptable practice at their journal, and 4 (2%) responded that they would allow it despite personally discouraging the practice. Of the editors who expressed support for co-reviewing, one in three added that they think it is a good way to develop the next generation of reviewers. The belief that co-reviewing results in a better review and helps connect journals with future reviewers and editors were also cited as reasons to support the practice. In contrast, editors who discouraged the practice, expressed disinterest in the written opinions of reviewers they did not solicit themselves, as well as concerns about decreased review quality and efficiency.</p><p>While endorsement of co-reviewing was high, editors often qualified the need to disclose the practice to ensure co-reviewers are credited for their work, as well as to assess potential conflicts. Oversight over the process by the editor-invited reviewer, as well as ensuring adherence to confidentiality and conflicts of interest policies were also strongly emphasised. Concerns around breaches of these same themes were cited by editors as reasons to discourage the practice. The following statements capture common sentiments and caveats concisely:</p><p><italic>“We do it, and I think it is important training. We emphasize that the ultimate responsibility is with the mentor, that all COI policies apply to the mentor and the student, and that confidentiality extends no further than one student with the invited reviewer.”</italic> (Ecology journal editor).</p><p><italic>“I believe this is an incredibly valuable process, providing that it is true &quot;co-writing&quot; and not just getting people to do your work and taking credit for it. This is a great way to train learners in how to properly review, something that is not done enough.”</italic> (Medical journal editor).</p><p><italic>“… I feel this would violate confidentiality and open submitters to the risk of leaking their manuscript or parts of their submission that the authors might find problematic. Also, we ask for the reviewer's expertise purposefully. This is not intended to be a pedagogic exercise.”</italic> (Psychology journal editor).</p></sec><sec id="s3-2"><title>Reviewers suggesting citations of their work</title><p>Fewer than 5% of editors overall objected to reviewers suggesting citations of their own work. In fact, a common view was that this should be expected given invited reviewers have likely done relevant work. Despite few objections to this practice, respondents expressed the need for editors to stay vigilant and arbitrate cases. For example, editors were often described as responsible for ensuring that recommended citations are relevant, address significant gaps or mischaracterisations, and display balance or restraint (often to prevent unblinding of reviewers). A common view was that editors should ultimately leave it to authors to determine whether to cite the suggested paper or not. Interestingly, three editors also stated that gratuitous requests would be grounds for editing a report. The following statements touch upon many of these themes:</p><p><italic>“… We invite people to review because they are subject matter experts, and because they are experts, they have often done relevant work. However, when a reviewer does nothing but add citations of their own work, that is clearly inappropriate. We often state in the decision letter that it is not necessary to cite all requested references … or, because all our reviewers are aware that we may edit their comments, we will take requests for inappropriate citations out of decision letters when excessive. …”</italic> (Medical journal editor).</p><p><italic>“This is borderline, and acceptable only when this is relevant to the paper. This is a case for editing the review”</italic> (Physics journal editor).</p><p><italic>“Yes. But only if their work is so significant on the topic that to leave it out would be a significant indication of a large scholarly oversight. I do dislike this practice and often discontinue the use of the reviewer if the suggestions are not germane or if it happens consistently.”</italic> (Economics journal editor).</p></sec><sec id="s3-3"><title>Reviewers requesting access to raw data</title><p>Of the 210 editors that provided codable views, 200 (95%) stated that they would support and mediate a request from a reviewer to access a manuscript’s raw data if they felt it was needed. This high level of support was noted to be consistent across all disciplines. However, of these respondents, approximately one in six added that they would need a compelling reason to do so, such as if credible ethical or quality concerns were raised. Furthermore, despite the high levels of <italic>in principle</italic> support, some also conceded that respecting data protections would need to be prioritised. Some editors expressed that while they would attempt to circumnavigate confidentiality issues by offering investigation through an independent intermediary, disclosure would ultimately be left at the authors’ discretion. Consequently, few editors outside those of journals adopting mandatory data sharing policies stated that manuscript acceptance would be contingent on sharing. Some of these views were captured in the following responses:</p><p><italic>“Absolutely. But I've also indicated that this is at the authors' discretion and have also conveyed any problems with Human Ethics in sharing a data set when that has been an issue. Both parties must agree to confidentiality to avoid leaking if the author does not or cannot provide a full data set for public view.”</italic> (Psychology journal editor).</p><p><italic>“I would definitely support and mediate the request of reviewer to see raw data. The goal of an editor is to publish solid and reproducible science and we have to help reviewers provide a fair and unbiased review based on the scientific results.”</italic> (Medical journal editor).</p><p><italic>“I definitely would support a request from a peer reviewer to see a manuscript's raw data if they feel it's necessary. Acquisition, processing and (statistical) evaluation of data in ecology is often complex, and the reviewer can assess the validity of data interpretation sometimes only when seeing original data and experimental design.”</italic> (Ecology journal editor).</p></sec><sec id="s3-4"><title>Editors publishing in the journals they edit</title><p>When asked how often an editor should publish their own original articles in a journal they edit, 176 (79%) editors described a scenario where they thought it would be acceptable for any editor to do so. A smaller proportion of respondents stated that this would not be acceptable for the lead editor (8%, 17), or for any editor (13%, 29). Once again, support was often qualified with prerequisites, most commonly processes to protect against conflicts of interest. Suggested processes included: independent editing and reviewing, preventing the submitting editor knowing the identities of the handling editor and reviewers (and vice-versa), and providing a statement declaring the handling editor’s identity on the final article.</p><p>Interestingly, views on how often an editor should submit research were more polarised among respondents, with one group expressing that it should not be restricted, and another suggesting that manuscripts should only be submitted rarely. However, despite these opposing views on frequency, there was a strong sentiment around the lack of fairness associated with preventing editors publishing in their journals among both groups. Many respondents discussed the negative consequences, such as: shutting out potentially desirable content, restricting publication venues for collaborators and deterring researchers taking on editorial roles. In contrast, many editors stated that publishing in one’s journals demonstrates a commitment to it. These beliefs were captured well in the following statements:</p><p><italic>“As often as the work meets the standard. … Telling people that they cannot publish in it lowers the value of what is a thankless task for which we aren't adequately compensated anyway.”</italic> (Economics journal editor).</p><p><italic>“Whenever the subject matter is appropriate. It is crazy to shut off an excellent journal from an excellent researcher when they put a lot of their own time and effort into ensuring the journal succeeds. It is easy to set up a process whereby that editor never sees their own work, or reviews, or reviewer identity - or even handling editor identity, so that the process can be independent and not influenced by the author/editor.”</italic> (Ecology journal editor).</p><p><italic>“Not often. … I have been reluctant to send my work to the journal even if a former editor were to decision it. However, I may consider sending a paper or two in my [next] term. I have published many papers in my journal prior to becoming editor so I have proven that I can do it. Thus, perhaps it is not fair on myself to limit where I can publish. It can also be damaging to the journal to the extent that the work is very much suited for the [readership]. But in all cases, it must be done within reason and very carefully …”</italic> (Psychology journal editor).</p><p><italic>“Not very often. In [my country], though, there are not so many (recognized) outlets for economists, while there is a need for decent local work to be done. Thus, I do publish an article here and there. I have absolutely no access to the article while it is under review; so, at least in principle, I cannot influence its 'passage' through the system.”</italic> (Economics journal editor).</p></sec><sec id="s3-5"><title>Direct replications</title><p>When asked their views on direct replication studies (i.e. research that follows the methods of another study as closely as possible), of the 183 editors that provided a clear stance, 86% stated that they support the practice or allow replication studies to be submitted to their journals. However, while respondents often perceived direct replications as an important and informative line of research, many also stressed the need for replication studies to be accompanied by novel work or offer new insights as a condition for publication, with the discovery of findings contradicting or undermining the original study being preferable for some. Similarly, the importance of the original study, as well as the number of existing replication attempts were also highlighted as important factors influencing the likelihood of acceptance of direct replication studies.</p><p>In contrast, the few editors who discouraged, or were ambivalent towards replication studies tended not to explain why they held these beliefs. When reasons were given, they most often related to perceived lack of interest from the readership, concerns surrounding potential agendas by researchers, and non-compliance with their journal’s scope for novel material. Consequently, many editors encouraged submission of research deliberately altering elements of the original study to test hypotheses in a new way, or to test generalisability of results in other settings i.e. ‘conceptual’ replication. The range of difficult considerations contributing to editors’ perceptions of replication studies are illustrated in the following statements:</p><p><italic>“I am very conflicted about this. It is important to get to the truth, but there are so many reasons an experiment might not work, and many of them have nothing to do with truth. And, frankly, as an editor, I am completely uninterested in publishing replication studies. No one will read the journal. So this is a very tough issue. I guess I'm more in favor of people attacking the same question from different angles to get at the truth, rather than attempting to perform precise replication. It is also really critical that those performing the replication studies do so ingenuously, and not with an agenda to disprove the original, or any intellectual conflict of interest. I find this is rarely the case - agendas are rife in replication work. …”</italic> (Medical journal editor).</p><p><italic>“Replication studies using new data are extremely valuable. Whether it would be published in my journal would depend on the perceived value of the study being replicated, the scope and quality of the analysis and whether the replication supports the existing finding. …”</italic> (Economics journal editor).</p><p><italic>“A direct, close replication is interesting and worthwhile. A journal has no particular duty to publish these – I reject the 'Pottery Barn Rule'. But if they're interesting, informative and well-done, why wouldn't a journal want to publish them?”</italic> (Psychology journal editor).</p><p><italic>“Replication studies are essential as one of the points of scientific research is to be able replicate the results of other research. An inability to replicate a study should make us question the results of that study. Journals like [redacted] have a section devoted to Replication Studies with an Editor in charge of these studies. This is best practice as far as I am concerned.”</italic> (Economics journal editor).</p></sec><sec id="s3-6"><title>Changes to existing peer review practices</title><p>Lastly, when asked if they would change anything about how their journal conducts peer review, of the 196 editors who responded, only 35% indicated that they were satisfied with the current system. The remaining two-thirds discussed changes they were contemplating, implementing or had implemented recently. The most frequently mentioned changes included: modifying the blinding system (most often increasing anonymity); improving how the journal finds reviewers; and improving the overall efficiency of the process. Another common theme was incentives and rewards for reviewers: the ideas put forward included remuneration, acknowledgment via Publons, the waiving of article processing charges, and professional development credits.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>Innovation adoption is a complex and active process that regularly begins with the recognition of a need, or opportunities to improve processes (<xref ref-type="bibr" rid="bib15">Horbach and Halffman, 2020</xref>; <xref ref-type="bibr" rid="bib54">Wisdom et al., 2014</xref>). When considering innovation of peer review, the need to maintain, and the desire to expand and incentivise their journal’s pool of reviewers featured prominently among editors in the current study. These motivations were often explicitly cited as reasons to justify policy-making decisions and positions on the raised ethical issues.</p><p>At times, these needs and opportunities aligned. For example, co-reviewing was commonly endorsed because it presented as an opportunity to connect with and train the next generation of reviewers. However, at other times they were noted to be in conflict, stifling potential change. For example, of the editors who supported reviewers requesting access to data, many were also conflicted about making acceptance contingent upon sharing. Reluctance among editors to dictate such terms to authors may also help explain the low uptake of mandatory data sharing policies observed in both the current and previous research (<xref ref-type="bibr" rid="bib35">Resnik et al., 2019</xref>). However, setting aside issues of low adherence to journal and funder instructions (<xref ref-type="bibr" rid="bib1">Alsheikh-Ali et al., 2011</xref>; <xref ref-type="bibr" rid="bib37">Rowhani-Farid and Barnett, 2016</xref>; <xref ref-type="bibr" rid="bib7">Couture et al., 2018</xref>), it is likely that uptake of mandatory data sharing policies will increase in the future as major funding agencies like the National Institutes of Health (NIH) and the Medical Research Council continue to implement increasingly strict policies on data sharing (<xref ref-type="bibr" rid="bib25">National Institutes of Health (NIH), 2020</xref>; <xref ref-type="bibr" rid="bib22">Medical Research Council, 2016</xref>).</p><p>A further example included the conflict between an editor’s desire to increase transparency and the fear of alienating reviewers. With regards to increasing transparency through the adoption of open peer review policies, there is evidence to substantiate such fears (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>; <xref ref-type="bibr" rid="bib13">Ho et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">van Rooyen et al., 1999</xref>; <xref ref-type="bibr" rid="bib49">van Rooyen et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Ware, 2008</xref>; <xref ref-type="bibr" rid="bib50">Walsh et al., 2000</xref>). For example, findings from a survey of over 12,000 researchers by Publons in 2018 reported that 37% to 49% of participants would be less likely to accept an invitation to review if open policies were adopted (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>). However, there is also evidence to suggest these views may be shifting, especially among younger and non-academic scholars (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>; <xref ref-type="bibr" rid="bib2">Bravo et al., 2019</xref>).</p><p>Previous research has also reported that three quarters of editors consider finding reviewers the most difficult part of their job, a task which is projected to become more difficult in the future (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>). Given this, in addition to the estimate that 10% of reviewers account for 50% of performed reviews (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>), it is likely that the perceived impact of policy changes on an editor’s ability to recruit reviewers contributes heavily to journal policy-making decisions. These factors may further help to explain the low implementation of the three policies most often associated with open peer review: open identities, open reports and open interactions in the current study and previous research (<xref ref-type="bibr" rid="bib15">Horbach and Halffman, 2020</xref>; <xref ref-type="bibr" rid="bib16">Klebel et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Parks and Gunashekar, 2017</xref>).</p><p>Despite the low popularity of unblinded models, previous research has noted increasing adoption of policies that allow authors and reviewers greater flexibility in deciding whether to disclose their identity or not during peer review (<xref ref-type="bibr" rid="bib16">Klebel et al., 2020</xref>). For example, in our sample, 18% of editors reported that reviewer anonymity was left at the reviewers’ discretion. This estimate is consistent with findings from <xref ref-type="bibr" rid="bib16">Klebel et al., 2020</xref>, and almost four-times higher than the highest estimate from Publons data in 2017 (<xref ref-type="bibr" rid="bib30">Parks and Gunashekar, 2017</xref>). However, importantly we note this movement toward greater flexibility appears to be at odds with the 39% of other editors who reported that reviewers identifying themselves within a blinded framework would be grounds for editing the report without the reviewer’s permission.</p><p>The notion of editors altering reviews, as well as the belief that reviewers are aware of this practice, represented some of the most intriguing findings of the current survey. The high percentage of editors that identified at least one situation where they considered editing a review acceptable (91%) is also consistent with a survey of 146 editors and publishers recently released by the Committee on Publication Ethics (COPE) (86%) (<xref ref-type="bibr" rid="bib6">COPE Council, 2020</xref>). Many of the reasons the COPE survey participants provided also echo those explored in the current survey, including the observation that some respondents viewed removing a reviewer’s recommendation as a valid reason to edit a report. Unfortunately, we did not ask editors what edits they envisioned making in each of the proposed situations, so we are hesitant to draw any strong conclusions. For the case of an editor altering a reviewer’s report when they disagree with the recommendation, it is plausible that this might mean removing or changing a reviewer’s recommendation to accept or reject the paper. It is also plausible that editors may have envisaged more innocuous changes to temper the reviewer’s critique, or more extensive changes to the content of the reviewer’s commentary. Two members of the authorship team (FF, RH) have experienced an example of the latter situation, which was covered in a recent news article in <italic>Science</italic> (<xref ref-type="bibr" rid="bib29">O’Grady, 2020</xref>).</p><p>When considering the notion of editors altering reviews, one could argue that fixing spelling and grammatical errors is benign, or even a necessary copy-editing step prior to publication of reviews. Editors’ preference to tone down potentially offensive or discriminatory remarks is also comprehendible, particularly given current pressures to retain reviewers, the non-trivial rate of unprofessional reviews and their potential negative impact on authors’ productivity and wellbeing (<xref ref-type="bibr" rid="bib34">Publons, 2018</xref>; <xref ref-type="bibr" rid="bib13">Ho et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Gerwing et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Silbiger and Stubler, 2019</xref>). This course of action has also been specifically advised in cases discussed by the COPE Forum (<xref ref-type="bibr" rid="bib5">Committee on Publication Ethics, 2011</xref>). Removal of identifying information is also likely reflective of stricter enforcement of blinding policies (<xref ref-type="bibr" rid="bib6">COPE Council, 2020</xref>). However, what is concerning is the number of editors (8–62% depending on the situation encountered, see <xref ref-type="table" rid="table3">Table 3</xref>) in the current survey that reported it would be acceptable to make these changes without consulting reviewers.</p><p>It is also particularly concerning, given such practices would be undetectable in journals that do not publish reviews or share complete decision letters with reviewers. Of note, 15% of the editors surveyed in the current study and 50% of editors surveyed by Horbach and Halffman reported that their journals do not share decision letters with reviewers (<xref ref-type="bibr" rid="bib15">Horbach and Halffman, 2020</xref>). Ultimately, given the perception that editors and editorial boards are largely responsible for managing ethical issues (<xref ref-type="bibr" rid="bib44">Taylor and Francis, 2015</xref>), such practices, particularly if shown to change reviewer recommendations may act to further degrade trust in peer review and increase calls for greater transparency.</p><p>The current study explored a series of peer review policies across a range of scientific disciplines. The survey achieved a high overall response rate (21%) compared to past survey efforts (4–6%) (<xref ref-type="bibr" rid="bib14">Hopp and Hoover, 2017</xref>; <xref ref-type="bibr" rid="bib15">Horbach and Halffman, 2020</xref>), as well as similar distributions of invited and responding journals by impact factor. However, we note some weaknesses of the study. Despite similar distributions by impact factor, we cannot rule out systematic differences between responders and non-responders that may have resulted in an unrepresentative sample. We also did not explore how uptake and reporting of policies differ depending on the journal scope or language (e.g. dedicated review journals, foreign language journals), the journal publishing model (e.g. open access, hybrid or subscription model), or whether the journal is led by an academic or professional editor. Furthermore, the authors note that our study only provides superficial information on selected policies and practices. For example, investigation into <italic>how</italic> editors use author-recommended reviewers, as well as nuances in policies, such as mixed policies that mandate data sharing for some study designs (e.g. clinical trials) or research outputs (e.g. x-ray crystallography), but not others, were outside of the scope of the study. Lastly, the survey represents a snapshot of journal policies across a three-month period in 2019. Consequently, recognising that journal policies change with time and new leadership, the policy landscape could shift substantially through time.</p></sec><sec id="s5"><title>Conclusion</title><p>Due to its closed nature, peer review continues to be difficult to examine. Our study sheds light on what peer review policies and practices are currently being used by journals in ecology, economics, medicine, physics and psychology, and gauges editor attitudes on some topical publication ethics issues. It highlights the tension between maintaining and growing the reviewer pool, an integral part of an editor’s role, and engaging in more open practices, such as open peer review. It also draws attention to the ethics of editors altering reviewers’ reports. We hope that the data presented in this study will stimulate further discussion about the role of peer review in scholarly publishing, catalyse future research and contribute to guideline development efforts and policymaking, particularly those concerning transparency in editorial and publishing policy.</p></sec><sec id="s6" sec-type="materials|methods"><title>Materials and methods</title><sec id="s6-1"><title>Journal selection strategy</title><p>Clarivate Analytics' InCites Journal Citation Reports (“Browse by Journal” function) was used to identify journals within five broad fields of interest to members of the authorship team, specifically: ecology, economics, medicine, physics and psychology. Lists of journals sorted by 2017 Journal Impact Factor were generated for each discipline on April 9th, 2019. Journals flagged as duplicate entries across disciplinary lists were subjectively removed from the less subject-appropriate list (e.g. <italic>The Journal of Comparative Psychology</italic> was removed from the ecology list) and then replaced with the next entry. The list of categories used in InCites to define disciplines can be found in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. Foreign-language journals, book series and data repositories were excluded from this study.</p><p>Aiming for a minimum of 50 responses per discipline, the top 300 journals in each field (when ranked by impact factor) were invited to participate (assuming an expected response rate of between 15–20% [<xref ref-type="bibr" rid="bib4">Byrne, 2000</xref>]). Therefore, our target response rate was 17%. Impact factors were used to select journals for sampling as they can be reflective of the relative level of visibility of journals to researchers within a field (<xref ref-type="bibr" rid="bib31">Paulus et al., 2018</xref>).</p><p>The lead editor for each of the 1500 journals was identified via the journal’s website, and contact emails obtained via a reliable source e.g. journals’ and academic institutions’ websites or recent publications. In the event that the same individual was found to edit multiple journals (duplicate editor) a co-lead editor or deputy editor was invited if available, or the editor was contacted by email and asked whether they would complete the survey on behalf of each of the journals they edit. As indicated above, in the final sample eight editors represented more than one journal.</p></sec><sec id="s6-2"><title>Survey themes and questions</title><p>Two separate surveys were created for this study. The first (Survey A) aimed to characterise journals’ peer review policies and practices as understood and reported by a member of the journal’s editorial team. The second (Survey B) was designed to capture editors’ opinions on five current peer review and publication ethics issues. For Survey A, participants were informed prior to commencing that information on routine peer review policies and practices were going to be collected, and provided consent allowing the study team to link responses back to the journal. For Survey B, participants were informed that all statements would be kept anonymous. Participants provided consent prior to beginning each survey. All questions for both surveys can be found in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>. De-identified responses to Survey A and coding data for Survey B are publicly available (<xref ref-type="bibr" rid="bib12">Hamilton et al., 2020</xref>). The survey and data collection strategy was approved by the University of Melbourne’s Faculty of Science Human Ethics Advisory Group (Project ID: 1954490.1) prior to commencement.</p></sec><sec id="s6-3"><title>Survey platform and distribution</title><p>Qualtrics Solutions’ Online Survey Software (Qualtrics, Provo, UT) was used to create and distribute the two surveys. Email invitations housing a personalised link to Survey A were generated and sent to all editors between June 26th and July 18th, 2019. Participants who completed Survey A were automatically redirected to the anonymous Survey B. Throughout the course of the project, three reminder emails were sent in July, August and September 2019 to editors who had not started (and not opted-out of future email reminders), or not completed Survey A. Both surveys were deactivated, and all responses recorded on October 1 st, 2019.</p></sec><sec id="s6-4"><title>Statistical analysis and reporting</title><p>Descriptive statistics were used to analyse answers to multiple choice survey questions in R (R Foundation for Statistical Computing, Vienna, Austria, v3.6.0). Missing responses were omitted from the proportions reported in the results section. The full breakdown of Survey A responses by discipline can be seen in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>. All figures were created in R using the ggplot2 (v3.2.0), plotly (v4.9.2) and viridis packages (v0.5.1) (<xref ref-type="bibr" rid="bib53">Wickham, 2016</xref>; <xref ref-type="bibr" rid="bib41">Sievert, 2018</xref>; <xref ref-type="bibr" rid="bib10">Garnier, 2018</xref>).</p><p>Responses to the open-ended questions in Survey B were analysed in NVivo (QSR International Pty Ltd., v12.4.0). An inductive (data-driven) approach was used to develop analytic categories (codebooks) for qualitative analysis. Preliminary codebooks for each question in Survey B were drafted by one author (DGH) after carefully reading and re-reading all responses to identify emergent themes. This process was repeated until all emergent themes relevant to the research question were captured and analytic categories could not be further refined. All codebooks were then pilot tested on six randomly selected responses by two coders (DGH; HF) for calibration purposes. Following further discussion and refinement, codebooks were then repeatedly tested on 35 (15%) randomly selected responses by the same two coders (DGH; HF) until sufficient inter-coder reliability was obtained.</p><p>For the purposes of this study, a codebook was considered stable once an overall unweighted kappa coefficient greater than 0.70, and average percentage agreement greater than 95% was achieved. Codebooks that did not satisfy these criteria after a round of testing were discussed and refined prior to being re-tested. The summarised results of the reliability analysis can be found in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. Once determined to be stable, codebooks were then used by one coder (DGH) to structure the analysis of the full dataset of responses. Results from this analysis were reported in a mixed methods approach by survey question. This included quantitative methods (e.g. counts of editors coded as for or against the issue). In addition, qualitative methods were used such as: exploring the relationships between expressed themes (e.g. identifying reasons for supporting or discouraging a practice) and reporting of contrasting views and deviant cases.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>DGH is a PhD candidate supported by an Australian Commonwealth Government Research Training Program Scholarship. The authors thank Dr Eden Smith for advice on the qualitative analysis and feedback on the manuscript. The authors also thank Aurora Marquette, Oscar Marquette and Acacia Finch Buchanan for assistance with data extraction.</p></ack><sec id="s7" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Methodology, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: Both surveys were reviewed and approved by the University of Melbourne's Faculty of Science Human Ethics Advisory Group (Project ID: 1954490.1) prior to distribution. All participants were provided detailed information about the purpose of the study, specifically that information on routine peer review policies and practices at their journal, as well as their views on some publication ethics issues were going to be collected. Informed consent was obtained from participants prior to beginning each survey.</p></fn></fn-group></sec><sec id="s8" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Interactive version of <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-62529-supp1-v2.zip"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Categories used to define disciplines in InCites; codebook reliability testing.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-62529-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Survey questions (A and B).</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-62529-supp3-v2.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Responses to questions in survey A.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-62529-supp4-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-62529-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s9" sec-type="data-availability"><title>Data availability</title><p>De-identified responses to Survey A and coding data for Survey B are publicly available (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/CY2RE">https://doi.org/10.17605/OSF.IO/CY2RE</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>DG</given-names></name><name><surname>Fraser</surname><given-names>H</given-names></name><name><surname>Hoekstra</surname><given-names>R</given-names></name><name><surname>Fidler</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Policies and opinions on peer review: A cross-disciplinary survey.</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/osf.io/cy2re</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alsheikh-Ali</surname> <given-names>AA</given-names></name><name><surname>Qureshi</surname> <given-names>W</given-names></name><name><surname>Al-Mallah</surname> <given-names>MH</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Public availability of published research data in high-impact journals</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e24357</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0024357</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bravo</surname> <given-names>G</given-names></name><name><surname>Grimaldo</surname> <given-names>F</given-names></name><name><surname>López-Iñesta</surname> <given-names>E</given-names></name><name><surname>Mehmani</surname> <given-names>B</given-names></name><name><surname>Squazzoni</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The effect of publishing peer review reports on referee behavior in five scholarly journals</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>322</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08250-2</pub-id><pub-id pub-id-type="pmid">30659186</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname> <given-names>KS</given-names></name><name><surname>Bal</surname> <given-names>L</given-names></name><name><surname>Clark</surname> <given-names>A</given-names></name><name><surname>Shipley</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Preventing the ends from justifying the means: withholding results to address publication bias in peer review</article-title><source>BMC Psychology</source><volume>4</volume><elocation-id>59</elocation-id><pub-id pub-id-type="doi">10.1186/s40359-016-0167-7</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byrne</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Common reasons for rejecting manuscripts at medical journals: a survey of editors and peer reviewers</article-title><source>Science Editor</source><volume>23</volume><fpage>39</fpage><lpage>44</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Committee on Publication Ethics</collab></person-group><year iso-8601-date="2011">2011</year><article-title>Case number 11-12. transparency of peer review to co-authors</article-title><ext-link ext-link-type="uri" xlink:href="https://publicationethics.org/case/transparency-peer-review-co-authors">https://publicationethics.org/case/transparency-peer-review-co-authors</ext-link><date-in-citation iso-8601-date="2020-08-02">August 2, 2020</date-in-citation></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>COPE Council</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Editing of reviewer comments</article-title><source>Survey Results</source><volume>1</volume><elocation-id>gyZ99iXA</elocation-id><pub-id pub-id-type="doi">10.24318/gyZ99iXA</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couture</surname> <given-names>JL</given-names></name><name><surname>Blake</surname> <given-names>RE</given-names></name><name><surname>McDonald</surname> <given-names>G</given-names></name><name><surname>Ward</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A funder-imposed data publication requirement seldom inspired data sharing</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0199789</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0199789</pub-id><pub-id pub-id-type="pmid">29979709</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname> <given-names>C</given-names></name><name><surname>Marcus</surname> <given-names>A</given-names></name><name><surname>Oransky</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Publishing: The peer-review scam</article-title><source>Nature</source><volume>515</volume><fpage>480</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1038/515480a</pub-id><pub-id pub-id-type="pmid">25428481</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fong</surname> <given-names>EA</given-names></name><name><surname>Wilhite</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Authorship and citation manipulation in academic research</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0187394</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0187394</pub-id><pub-id pub-id-type="pmid">29211744</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Garnier</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Viridis: default color maps from 'matplotlib'</data-title><source>R Package</source><version designator="0.5.1">0.5.1</version><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=viridis">https://CRAN.R-project.org/package=viridis</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerwing</surname> <given-names>TG</given-names></name><name><surname>Allen Gerwing</surname> <given-names>AM</given-names></name><name><surname>Avery-Gomm</surname> <given-names>S</given-names></name><name><surname>Choi</surname> <given-names>CY</given-names></name><name><surname>Clements</surname> <given-names>JC</given-names></name><name><surname>Rash</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying professionalism in peer review</article-title><source>Research Integrity and Peer Review</source><volume>5</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1186/s41073-020-00096-x</pub-id><pub-id pub-id-type="pmid">32760597</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Hamilton</surname> <given-names>DG</given-names></name><name><surname>Fraser</surname> <given-names>H</given-names></name><name><surname>Hoeksta</surname> <given-names>R</given-names></name><name><surname>Fidler</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Data from &quot;Policies and opinions on peer review: a cross-disciplinary survey&quot;</article-title><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/osf.io/cy2re">https://doi.org/10.17605/osf.io/cy2re</ext-link><date-in-citation iso-8601-date="2020-12-01">December 1, 2020</date-in-citation></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname> <given-names>RC</given-names></name><name><surname>Mak</surname> <given-names>KK</given-names></name><name><surname>Tao</surname> <given-names>R</given-names></name><name><surname>Lu</surname> <given-names>Y</given-names></name><name><surname>Day</surname> <given-names>JR</given-names></name><name><surname>Pan</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Views on the peer review system of biomedical journals: an online survey of academics from high-ranking universities</article-title><source>BMC Medical Research Methodology</source><volume>13</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2288-13-74</pub-id><pub-id pub-id-type="pmid">23758823</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopp</surname> <given-names>C</given-names></name><name><surname>Hoover</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How prevalent is academic misconduct in management research?</article-title><source>Journal of Business Research</source><volume>80</volume><fpage>73</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.jbusres.2017.07.003</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horbach</surname> <given-names>SPJM</given-names></name><name><surname>Halffman</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Journal peer review and editorial evaluation: cautious innovator or sleepy giant?</article-title><source>Minerva</source><volume>58</volume><fpage>139</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1007/s11024-019-09388-z</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Klebel</surname> <given-names>T</given-names></name><name><surname>Reichmann</surname> <given-names>S</given-names></name><name><surname>Polka</surname> <given-names>J</given-names></name><name><surname>McDowell</surname> <given-names>G</given-names></name><name><surname>Penfold</surname> <given-names>N</given-names></name><name><surname>Hindle</surname> <given-names>S</given-names></name><name><surname>Ross-Hellauer</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Peer review and preprint policies are unclear at most major journals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.24.918995</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname> <given-names>RL</given-names></name><name><surname>Franks</surname> <given-names>P</given-names></name><name><surname>Feldman</surname> <given-names>MD</given-names></name><name><surname>Gerrity</surname> <given-names>M</given-names></name><name><surname>Byrne</surname> <given-names>C</given-names></name><name><surname>Tierney</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Editorial peer reviewers' recommendations at a general medical journal: are they reliable and do editors care?</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e10072</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0010072</pub-id><pub-id pub-id-type="pmid">20386704</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luty</surname> <given-names>J</given-names></name><name><surname>Arokiadass</surname> <given-names>SM</given-names></name><name><surname>Easow</surname> <given-names>JM</given-names></name><name><surname>Anapreddy</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Preferential publication of editorial board members in medical specialty journals</article-title><source>Journal of Medical Ethics</source><volume>35</volume><fpage>200</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1136/jme.2008.026740</pub-id><pub-id pub-id-type="pmid">19251974</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahoney</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Publication prejudices: an experimental study of confirmatory bias in the peer review system</article-title><source>Cognitive Therapy and Research</source><volume>1</volume><fpage>161</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1007/BF01173636</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mani</surname> <given-names>J</given-names></name><name><surname>Makarević</surname> <given-names>J</given-names></name><name><surname>Juengel</surname> <given-names>E</given-names></name><name><surname>Ackermann</surname> <given-names>H</given-names></name><name><surname>Nelson</surname> <given-names>K</given-names></name><name><surname>Bartsch</surname> <given-names>G</given-names></name><name><surname>Haferkamp</surname> <given-names>A</given-names></name><name><surname>Blaheta</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>I publish in I edit?--do editorial board members of urologic journals preferentially publish their own scientific work?</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e83709</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0083709</pub-id><pub-id pub-id-type="pmid">24386258</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDowell</surname> <given-names>GS</given-names></name><name><surname>Knutsen</surname> <given-names>JD</given-names></name><name><surname>Graham</surname> <given-names>JM</given-names></name><name><surname>Oelker</surname> <given-names>SK</given-names></name><name><surname>Lijek</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Co-reviewing and ghostwriting by early-career researchers in the peer review of manuscripts</article-title><source>eLife</source><volume>8</volume><elocation-id>e48425</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48425</pub-id><pub-id pub-id-type="pmid">31668163</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Medical Research Council</collab></person-group><year iso-8601-date="2016">2016</year><article-title>Data sharing policy v2.2</article-title><ext-link ext-link-type="uri" xlink:href="https://mrc.ukri.org/documents/pdf/mrc-data-sharing-policy/">https://mrc.ukri.org/documents/pdf/mrc-data-sharing-policy/</ext-link><date-in-citation iso-8601-date="2020-11-05">November 5, 2020</date-in-citation></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehra</surname> <given-names>MR</given-names></name><name><surname>Ruschitzka</surname> <given-names>F</given-names></name><name><surname>Patel</surname> <given-names>AN</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Retraction: Hydroxychloroquine or chloroquine with or without a macrolide for treatment of COVID-19: a multinational registry analysis</article-title><source>Lancet</source><volume>2020</volume><elocation-id>395</elocation-id><pub-id pub-id-type="doi">10.1016/S0140-6736(20)31180-6</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehra</surname> <given-names>MR</given-names></name><name><surname>Desai</surname> <given-names>SS</given-names></name><name><surname>Kuy</surname> <given-names>S</given-names></name><name><surname>Henry</surname> <given-names>TD</given-names></name><name><surname>Patel</surname> <given-names>AN</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Retraction: Cardiovascular Disease, Drug Therapy, and Mortality in Covid-19</article-title><source>New England Journal of Medicine</source><volume>382</volume><elocation-id>2582</elocation-id><pub-id pub-id-type="doi">10.1016/S0140-6736(20)31324-6</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="web"><person-group person-group-type="author"><collab>National Institutes of Health (NIH)</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Final NIH policy for data management and sharing</article-title><ext-link ext-link-type="uri" xlink:href="https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html">https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html</ext-link><date-in-citation iso-8601-date="2020-11-05">November 5, 2020</date-in-citation></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicholas</surname> <given-names>D</given-names></name><name><surname>Watkinson</surname> <given-names>A</given-names></name><name><surname>Jamali</surname> <given-names>HR</given-names></name><name><surname>Herman</surname> <given-names>E</given-names></name><name><surname>Tenopir</surname> <given-names>C</given-names></name><name><surname>Volentine</surname> <given-names>R</given-names></name><name><surname>Allard</surname> <given-names>S</given-names></name><name><surname>Levine</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Peer review: still king in the digital age</article-title><source>Learned Publishing</source><volume>28</volume><fpage>15</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1087/20150104</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Alter</surname> <given-names>G</given-names></name><name><surname>Banks</surname> <given-names>GC</given-names></name><name><surname>Borsboom</surname> <given-names>D</given-names></name><name><surname>Bowman</surname> <given-names>SD</given-names></name><name><surname>Breckler</surname> <given-names>SJ</given-names></name><name><surname>Buck</surname> <given-names>S</given-names></name><name><surname>Chambers</surname> <given-names>CD</given-names></name><name><surname>Chin</surname> <given-names>G</given-names></name><name><surname>Christensen</surname> <given-names>G</given-names></name><name><surname>Contestabile</surname> <given-names>M</given-names></name><name><surname>Dafoe</surname> <given-names>A</given-names></name><name><surname>Eich</surname> <given-names>E</given-names></name><name><surname>Freese</surname> <given-names>J</given-names></name><name><surname>Glennerster</surname> <given-names>R</given-names></name><name><surname>Goroff</surname> <given-names>D</given-names></name><name><surname>Green</surname> <given-names>DP</given-names></name><name><surname>Hesse</surname> <given-names>B</given-names></name><name><surname>Humphreys</surname> <given-names>M</given-names></name><name><surname>Ishiyama</surname> <given-names>J</given-names></name><name><surname>Karlan</surname> <given-names>D</given-names></name><name><surname>Kraut</surname> <given-names>A</given-names></name><name><surname>Lupia</surname> <given-names>A</given-names></name><name><surname>Mabry</surname> <given-names>P</given-names></name><name><surname>Madon</surname> <given-names>T</given-names></name><name><surname>Malhotra</surname> <given-names>N</given-names></name><name><surname>Mayo-Wilson</surname> <given-names>E</given-names></name><name><surname>McNutt</surname> <given-names>M</given-names></name><name><surname>Miguel</surname> <given-names>E</given-names></name><name><surname>Paluck</surname> <given-names>EL</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Soderberg</surname> <given-names>C</given-names></name><name><surname>Spellman</surname> <given-names>BA</given-names></name><name><surname>Turitto</surname> <given-names>J</given-names></name><name><surname>VandenBos</surname> <given-names>G</given-names></name><name><surname>Vazire</surname> <given-names>S</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nylenna</surname> <given-names>M</given-names></name><name><surname>Riis</surname> <given-names>P</given-names></name><name><surname>Karlsson</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Multiple blinded reviews of the same two manuscripts: Effects of referee characteristics and publication language</article-title><source>JAMA</source><volume>272</volume><fpage>149</fpage><lpage>151</lpage><pub-id pub-id-type="pmid">8015129</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Grady</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Delete offensive language? Change recommendations? Some editors say it’s OK to alter peer reviews</article-title><source>Science</source><volume>1</volume><elocation-id>abf4690</elocation-id></element-citation></ref><ref id="bib30"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Parks</surname> <given-names>S</given-names></name><name><surname>Gunashekar</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tracking global trends in open peer review</article-title><ext-link ext-link-type="uri" xlink:href="https://publons.com/blog/who-is-using-open-peer-review/">https://publons.com/blog/who-is-using-open-peer-review/</ext-link><date-in-citation iso-8601-date="2020-08-02">August 2, 2020</date-in-citation></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulus</surname> <given-names>FM</given-names></name><name><surname>Cruz</surname> <given-names>N</given-names></name><name><surname>Krach</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The impact factor fallacy</article-title><source>Frontiers in Psychology</source><volume>9</volume><elocation-id>1487</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01487</pub-id><pub-id pub-id-type="pmid">30177900</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname> <given-names>DP</given-names></name><name><surname>Ceci</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Peer-review practices of psychological journals: the fate of published articles, submitted again</article-title><source>Behavioral and Brain Sciences</source><volume>5</volume><fpage>187</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1017/S0140525X00011183</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Pew Research Center</collab></person-group><year iso-8601-date="2019">2019</year><article-title>Trust and mistrust in Americans’ views of scientific experts</article-title><ext-link ext-link-type="uri" xlink:href="https://www.pewresearch.org/science/2019/08/02/trust-and-mistrust-in-americans-views-of-scientific-experts/">https://www.pewresearch.org/science/2019/08/02/trust-and-mistrust-in-americans-views-of-scientific-experts/</ext-link><date-in-citation iso-8601-date="2020-08-02">August 2, 2020</date-in-citation></element-citation></ref><ref id="bib34"><element-citation publication-type="report"><person-group person-group-type="author"><collab>Publons</collab></person-group><year iso-8601-date="2018">2018</year><source>Global State of Peer Review Report</source><publisher-name>Clarivate analysis</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resnik</surname> <given-names>DB</given-names></name><name><surname>Morales</surname> <given-names>M</given-names></name><name><surname>Landrum</surname> <given-names>R</given-names></name><name><surname>Shi</surname> <given-names>M</given-names></name><name><surname>Minnier</surname> <given-names>J</given-names></name><name><surname>Vasilevsky</surname> <given-names>NA</given-names></name><name><surname>Champieux</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effect of impact factor and discipline on journal data sharing policies</article-title><source>Accountability in Research</source><volume>26</volume><fpage>139</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1080/08989621.2019.1591277</pub-id><pub-id pub-id-type="pmid">30841755</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross-Hellauer</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What is open peer review? <italic>A systematic review</italic></article-title><source>F1000Research</source><volume>6</volume><elocation-id>588</elocation-id><pub-id pub-id-type="doi">10.12688/f1000research.11369.1</pub-id><pub-id pub-id-type="pmid">28580134</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowhani-Farid</surname> <given-names>A</given-names></name><name><surname>Barnett</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Has open data arrived at the <italic>British Medical Journal (BMJ)</italic>? An observational study</article-title><source>BMJ Open</source><volume>6</volume><elocation-id>e011784</elocation-id><pub-id pub-id-type="doi">10.1136/bmjopen-2016-011784</pub-id><pub-id pub-id-type="pmid">27737882</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroter</surname> <given-names>S</given-names></name><name><surname>Black</surname> <given-names>N</given-names></name><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Carpenter</surname> <given-names>J</given-names></name><name><surname>Godlee</surname> <given-names>F</given-names></name><name><surname>Smith</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Effects of training on quality of peer review: randomised controlled trial</article-title><source>BMJ</source><volume>328</volume><elocation-id>673</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.38023.700775.AE</pub-id><pub-id pub-id-type="pmid">14996698</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroter</surname> <given-names>S</given-names></name><name><surname>Black</surname> <given-names>N</given-names></name><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Godlee</surname> <given-names>F</given-names></name><name><surname>Osorio</surname> <given-names>L</given-names></name><name><surname>Smith</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>What errors do peer reviewers detect, and does training improve their ability to detect them?</article-title><source>Journal of the Royal Society of Medicine</source><volume>101</volume><fpage>507</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1258/jrsm.2008.080062</pub-id><pub-id pub-id-type="pmid">18840867</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shelomi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Editorial misconduct: Definition, cases, and causes</article-title><source>Publications</source><volume>2</volume><fpage>51</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.3390/publications2020051</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Sievert</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Plotly for R</article-title><ext-link ext-link-type="uri" xlink:href="https://plotly-r.com/">https://plotly-r.com/</ext-link><date-in-citation iso-8601-date="2020-08-02">August 2, 2020</date-in-citation></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silbiger</surname> <given-names>NJ</given-names></name><name><surname>Stubler</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unprofessional peer reviews disproportionately harm underrepresented groups in STEM</article-title><source>PeerJ</source><volume>7</volume><elocation-id>e8247</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.8247</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Classical peer review: an empty gun</article-title><source>Breast Cancer Research</source><volume>12 Suppl 4</volume><elocation-id>S13</elocation-id><pub-id pub-id-type="doi">10.1186/bcr2742</pub-id><pub-id pub-id-type="pmid">21172075</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Taylor and Francis</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Peer review in 2015: A global view</article-title><ext-link ext-link-type="uri" xlink:href="https://authorservices.taylorandfrancis.com/peer-review-global-view">https://authorservices.taylorandfrancis.com/peer-review-global-view</ext-link><date-in-citation iso-8601-date="2020-08-02">August 2, 2020</date-in-citation></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tennant</surname> <given-names>JP</given-names></name><name><surname>Ross-Hellauer</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The limitations to our understanding of peer review</article-title><source>Research Integrity and Peer Review</source><volume>5</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1186/s41073-020-00092-1</pub-id><pub-id pub-id-type="pmid">32368354</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thombs</surname> <given-names>BD</given-names></name><name><surname>Levis</surname> <given-names>AW</given-names></name><name><surname>Razykov</surname> <given-names>I</given-names></name><name><surname>Syamchandra</surname> <given-names>A</given-names></name><name><surname>Leentjens</surname> <given-names>AF</given-names></name><name><surname>Levenson</surname> <given-names>JL</given-names></name><name><surname>Lumley</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Potentially coercive self-citation by peer reviewers: a cross-sectional study</article-title><source>Journal of Psychosomatic Research</source><volume>78</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.jpsychores.2014.09.015</pub-id><pub-id pub-id-type="pmid">25300537</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Ulrichsweb</collab></person-group><year iso-8601-date="2020">2020</year><article-title>A screenshot of the search is viewable at</article-title><ext-link ext-link-type="uri" xlink:href="https://osf.io/vrbn8/">https://osf.io/vrbn8/</ext-link><date-in-citation iso-8601-date="2020-07-11">July 11, 2020</date-in-citation></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Rooyen</surname> <given-names>S</given-names></name><name><surname>Godlee</surname> <given-names>F</given-names></name><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Black</surname> <given-names>N</given-names></name><name><surname>Smith</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effect of open peer review on quality of reviews and on reviewers' recommendations: a randomised trial</article-title><source>BMJ</source><volume>318</volume><fpage>23</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1136/bmj.318.7175.23</pub-id><pub-id pub-id-type="pmid">9872878</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Rooyen</surname> <given-names>S</given-names></name><name><surname>Delamothe</surname> <given-names>T</given-names></name><name><surname>Evans</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Effect on peer review of telling reviewers that their signed reviews might be posted on the web: randomised controlled trial</article-title><source>BMJ</source><volume>341</volume><elocation-id>c5729</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.c5729</pub-id><pub-id pub-id-type="pmid">21081600</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname> <given-names>E</given-names></name><name><surname>Rooney</surname> <given-names>M</given-names></name><name><surname>Appleby</surname> <given-names>L</given-names></name><name><surname>Wilkinson</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Open peer review: a randomised controlled trial</article-title><source>British Journal of Psychiatry</source><volume>176</volume><fpage>47</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1192/bjp.176.1.47</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ware</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Peer review in scholarly journals: Perspective of the scholarly community – results from an international study</article-title><source>Information Services &amp; Use</source><volume>28</volume><fpage>109</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.3233/ISU-2008-0568</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Ware</surname> <given-names>M</given-names></name><collab>Publishing Research Consortium</collab></person-group><year iso-8601-date="2016">2016</year><source>Publishing Research Consortium Peer Review Survey 2015.</source><publisher-loc>Bristol</publisher-loc><publisher-name>Mark Ware Consulting Ltd</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.elsevier.com/__data/assets/pdf_file/0007/655756/PRC-peer-review-survey-report-Final-2016-05-19.pdf">https://www.elsevier.com/__data/assets/pdf_file/0007/655756/PRC-peer-review-survey-report-Final-2016-05-19.pdf</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Ggplot2: Elegant Graphics for Data Analysis</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-98141-3</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wisdom</surname> <given-names>JP</given-names></name><name><surname>Chor</surname> <given-names>KH</given-names></name><name><surname>Hoagwood</surname> <given-names>KE</given-names></name><name><surname>Horwitz</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Innovation adoption: a review of theories and constructs</article-title><source>Administration and Policy in Mental Health and Mental Health Services Research</source><volume>41</volume><fpage>480</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1007/s10488-013-0486-4</pub-id><pub-id pub-id-type="pmid">23549911</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yalow</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Radioimmunoassay: a probe for the fine structure of biologic systems</article-title><source>Science</source><volume>200</volume><fpage>1236</fpage><lpage>1245</lpage><pub-id pub-id-type="doi">10.1126/science.208142</pub-id><pub-id pub-id-type="pmid">208142</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62529.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Allen</surname><given-names>Liz</given-names> </name><role>Reviewer</role><aff><institution>F1000</institution></aff></contrib><contrib contrib-type="reviewer"><name><surname>Ross-Hellauer</surname><given-names>Tony</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib><contrib contrib-type="reviewer"><name><surname>Rodgers</surname><given-names>Peter</given-names> </name><role>Reviewer</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Meta-Research: Journal policies and editors' opinions on peer review&quot; to <italic>eLife</italic> for consideration as a Feature Article. Your article has been reviewed by three peer reviewers, including the <italic>eLife</italic> Features Editor Peter Rodgers. The reviewers have discussed the reviews and have drafted this decision letter to help you prepare a revised submission.</p><p>The following individuals involved in review of your submission have agreed to reveal their identity: Liz Allen (Reviewer #1); Tony Ross-Hellauer (Reviewer #2).</p><p>Summary:</p><p>Reviewer #1: This is an interesting descriptive analysis of journal editors' views – albeit, as the authors acknowledge, a snapshot at a time when there are many changes afoot across scholarly publishing. The discussion of how Editors are thinking about the policies and practice around, for example open data, publishing replication studies, and open peer review, are timely and relevant topics with implications for meta-research more generally. There is also a dearth of evidence around aspects of the publishing process and particularly the “black box” that is peer review. However, there are also a number of points that need to be addressed to make the article suitable for publication.</p><p>Essential revisions:</p><p>1) In terms of the sampling, I wasn't sure why the particular mix of journal subject areas was picked (was this important? why pick these?). It is also not clear why the focus on “high impact journals” is important as part of the sampling frame; the authors do not explain why this was the selection strategy – is it perhaps based upon an assumption that these would be more established journals therefore would have established policies/substantial experience? Or another reason?</p><p>2) Did you distinguish between open-access (OA) journals and non-OA journals? If yes, please discuss what you found. If no, please mention this in the section on the limitations of the survey as the issue of OA vs non-OA journals may influence the views of editors on some of the topics covered in the survey.</p><p>3) Did you distinguish between journals edited by academic editors and those edited by professional editors? If yes, please discuss what you found. If no, please mention this in the section on the limitations of the survey. The issue of academic vs professional editors may influence some of your findings because, for example, one might expect professional editors to be more familiar with journal policies on data etc, and also to have more time to implement/police these policies.</p><p>4) Section on &quot;Reviewers requesting access to raw data&quot;.</p><p>Do you know how many of the journals in your sample mandate data sharing?</p><p>Also, there have been a lot of articles about researchers not being able to get data from published authors, even for papers published in journals that mandate data sharing, and it would be good to cite and discuss some of these articles in the Discussion, and also to mention that more and more funders are insisting on open data. (There are similar problems with journals signing up to guidelines/mandates wrt various aspects of clinical trials, but not enforcing these guidelines/mandates. I know that clinical trials are beyond the scope of this article, but the problems encountered with papers reporting the results of trials highlight the difficulty/complexity of implementing guidelines/mandates).</p><p>5) The practice of editors editing reports when they disagree with the recommendation needs to be discussed further – even if it is only to say something along the lines that it is not clear if this involved deleting comments like &quot;I support publication&quot; or &quot;I oppose publication&quot;, or if it was more extensive, and that unfortunately there were no open responses on this topic.</p><p>Also, please clarify what the figures (8-62%) refer to.</p><p>6) The authors could have perhaps included some more explicit recommendations for further analysis and even policy and practice.</p><p>7) Figure 1: Please replace Figure 1 with a table A6 from Supplementary file 3.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62529.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) In terms of the sampling, I wasn't sure why the particular mix of journal subject areas was picked (was this important? why pick these?).</p></disp-quote><p>Thank you very much for the comments. To address the first point, the journal subject areas were chosen primarily as each one relates to a member of the authorship team’s fields of interest (DGH – Medicine &amp; Physics, HF – Ecology, RH – Psychology, FF – Psychology, Ecology &amp; Economics). Furthermore, the authors felt that these fields would suffice for the purposes of the study as they also have the benefit of representing a broad area of scientific disciplines. For example, the five chosen areas cover each of the five main fields of science as used in the CWTS Leiden Ranking, namely: Social Science &amp; Humanities (Psychology, Economics), Biomedical &amp; Health Sciences (Medicine), Physical Sciences &amp; Engineering (Physics), Life &amp; Earth Science (Ecology) and Mathematics &amp; Computer Sciences (Physics). Some text has been added into the Materials and methods section to clarify this to prospective readers.</p><disp-quote content-type="editor-comment"><p>It is also not clear why the focus on “high impact journals” is important as part of the sampling frame; the authors do not explain why this was the selection strategy – is it perhaps based upon an assumption that these would be more established journals therefore would have established policies/substantial experience? Or another reason?</p></disp-quote><p>The decision to limit the focus to the 300 journals with the highest impact factor in each discipline was largely done for reasons of convenience. From a bibliometric point of view, Clarivate Analytics’ “impact factors” are arguably the most commonly used and well-known impact measure in academia. They are also a simple metric to access and interpret. Furthermore, we are the first to acknowledge that we do not believe impact factors are a surrogate for journal quality (or if they are, they’re a poor one). However, we do acknowledge that there is evidence to suggest that they can provide some useful information on the degree of “<italic>visibility</italic>” of journals within a field (Paulus, Cruz and Krach, 2018). Consequently, given that manually assembling and extracting contact information from all journals within a specific field was not feasible, and our overall aim to survey journals that are well-known to researchers within a particular field, we decided that limiting the scope to popular journals indexed in the Web of Science would be a defensible sampling strategy. Some further text has been added to the Materials and methods section to explain this decision (paragraph two).</p><disp-quote content-type="editor-comment"><p>2) Did you distinguish between open-access (OA) journals and non-OA journals? If yes, please discuss what you found. If no, please mention this in the section on the limitations of the survey as the issue of OA vs non-OA journals may influence the views of editors on some of the topics covered in the survey.</p></disp-quote><p>This is another valid point. Unfortunately, we did not collect this information and so list this as a limitation in the Discussion section (paragraph nine).</p><disp-quote content-type="editor-comment"><p>3) Did you distinguish between journals edited by academic editors and those edited by professional editors? If yes, please discuss what you found. If no, please mention this in the section on the limitations of the survey. The issue of academic vs professional editors may influence some of your findings because, for example, one might expect professional editors to be more familiar with journal policies on data etc, and also to have more time to implement/police these policies.</p></disp-quote><p>As above, this is another piece of information we overlooked. A sentence has been added into the limitations to point this out (paragraph nine).</p><disp-quote content-type="editor-comment"><p>4) Section on &quot;Reviewers requesting access to raw data&quot;.</p><p>Do you know how many of the journals in your sample mandate data sharing?</p><p>Also, there have been a lot of articles about researchers not being able to get data from published authors, even for papers published in journals that mandate data sharing, and it would be good to cite and discuss some of these articles in the Discussion, and also to mention that more and more funders are insisting on open data. (There are similar problems with journals signing up to guidelines/mandates with regards to various aspects of clinical trials, but not enforcing these guidelines/mandates. I know that clinical trials are beyond the scope of this article, but the problems encountered with papers reporting the results of trials highlight the difficulty/complexity of implementing guidelines/mandates).</p></disp-quote><p>Information on how many of the participating journals mandate deposition of data into an external repository is reported in Table 4. The breakdown of these numbers by discipline is also available in Supplementary file 4 (A13D). This is an area of interest to the first author and will be the subject of future research. Some text has been added to contextualise these findings with respect to some of the studies raised in the reviewer’s comment.</p><disp-quote content-type="editor-comment"><p>5) The practice of editors editing reports when they disagree with the recommendation needs to be discussed further – even if it is only to say something along the lines that it is not clear if this involved deleting comments like &quot;I support publication&quot; or &quot;I oppose publication&quot;, or if it was more extensive, and that unfortunately there were no open responses on this topic.</p></disp-quote><p>Another good point. Unfortunately, due to the design of the survey we are unable to clarify exactly what changes editors had in mind when they answered this question. It is plausible that the question may have been interpreted as changing small elements of the report if the editor disagreed with the recommendation, all the way up to switching a recommendation from “accept” to “reject”, or vice-versa. Consequently, making strong conclusions about likely behaviours from these results would be inappropriate. As advised, some text has been added to highlight possible differences in interpretation of the question. Furthermore, some text has been added to share two of the authors’ previous experience of uncovering an altered recommendation, as well as a link to a recent article in <italic>Science</italic> that covered the story in more detail. (Discussion paragraph six).</p><disp-quote content-type="editor-comment"><p>Also, please clarify what the figures (8-62%) refer to.</p></disp-quote><p>These figures are derived from Table 3. They communicate the findings that, depending on the situation, the proportion of editors that stated they would alter a reviewer's report without their permission in this study ranged from 8% (if they disagreed with the recommendation) to 62% (if comments for editor were left in the report). This sentence has been amended slightly to improve the clarity.</p><disp-quote content-type="editor-comment"><p>6) The authors could have perhaps included some more explicit recommendations for further analysis and even policy and practice.</p></disp-quote><p>The authors had initially considered including some recommendations for the broader scientific community, however felt this would be inappropriate given the spirit of the study. We see this study as an attempt to improve understanding of the state and perceptions of peer review in order to stimulate further discussion about peer review, catalyse future research in the area, and importantly, pave the way for policy pieces by researchers who specialise in policymaking and guideline development. Some text has been added into the Conclusion to better highlight this position.</p><disp-quote content-type="editor-comment"><p>7) Figure 1: Please replace Figure 1 with a table A6 from supplementary file 3.</p></disp-quote><p>This has been amended as requested (refer to Table 2 in the revised document).</p></body></sub-article></article>