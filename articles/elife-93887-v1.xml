<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93887</article-id><article-id pub-id-type="doi">10.7554/eLife.93887</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93887.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Individuals with anxiety and depression use atypical decision strategies in an uncertain world</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Zeming</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8091-4413</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Meihua</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2238-0513</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Ting</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1590-9025</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yuhang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0004-5934-2635</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Hanbo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1133-8544</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Quan</surname><given-names>Peng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5416-536X</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Geng</surname><given-names>Haiyang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6115-807X</contrib-id><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0654-715X</contrib-id><email>ruyuanzhang@sjtu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Shanghai Mental Health Center, School of Medicine, Shanghai Jiao Tong University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>School of Psychology, Shanghai Jiao Tong University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>School of Psychology, South China Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04qr3zq92</institution-id><institution>The Center of Psychosomatic Medicine, Sichuan Provincial Center for Mental Health, Sichuan Provincial People's Hospital, University of Electronic Science and Technology of China</institution></institution-wrap><addr-line><named-content content-type="city">Chengdu</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01r4q9n85</institution-id><institution>Centre of Centre for Cognitive and Brain Sciences, Institute of Collaborative Innovation, University of Macau</institution></institution-wrap><addr-line><named-content content-type="city">Macau</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03m2x1q45</institution-id><institution>Department of Psychology, University of Arizona</institution></institution-wrap><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04k5rxe29</institution-id><institution>School of Humanities and Management, Guangdong Medical University</institution></institution-wrap><addr-line><named-content content-type="city">Dongguan</named-content></addr-line><country>China</country></aff><aff id="aff8"><label>8</label><institution>Tianqiao and Chrissy Chen Institute for Translational Research</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02n96ep67</institution-id><institution>Shanghai Key Laboratory of Mental Health and Psychological Crisis Intervention, School of Psychology and Cognitive Science, East China Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Liljeholm</surname><given-names>Mimi</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04gyf1771</institution-id><institution>University of California, Irvine</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>09</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP93887</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-10"><day>10</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-10-24"><day>24</day><month>10</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/wymuc"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-02"><day>02</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93887.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-25"><day>25</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93887.2"/></event></pub-history><permissions><copyright-statement>© 2024, Fang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Fang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93887-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-93887-figures-v1.pdf"/><abstract><p>Previous studies on reinforcement learning have identified three prominent phenomena: (1) individuals with anxiety or depression exhibit a reduced learning rate compared to healthy subjects; (2) learning rates may increase or decrease in environments with rapidly changing (i.e. volatile) or stable feedback conditions, a phenomenon termed <italic>learning rate adaptation</italic>; and (3) reduced learning rate adaptation is associated with several psychiatric disorders. In other words, multiple learning rate parameters are needed to account for behavioral differences across participant populations and volatility contexts in this flexible learning rate (FLR) model. Here, we propose an alternative explanation, suggesting that behavioral variation across participant populations and volatile contexts arises from the use of mixed decision strategies. To test this hypothesis, we constructed a mixture-of-strategies (MOS) model and used it to analyze the behaviors of 54 healthy controls and 32 patients with anxiety and depression in volatile reversal learning tasks. Compared to the FLR model, the MOS model can reproduce the three classic phenomena by using a single set of strategy preference parameters without introducing any learning rate differences. In addition, the MOS model can successfully account for several novel behavioral patterns that cannot be explained by the FLR model. Preferences for different strategies also predict individual variations in symptom severity. These findings underscore the importance of considering mixed strategy use in human learning and decision-making and suggest atypical strategy preference as a potential mechanism for learning deficits in psychiatric disorders.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>probabilistic reversal learning</kwd><kwd>depression</kwd><kwd>anxiety</kwd><kwd>mixed decision strategy</kwd><kwd>heuristic strategy</kwd><kwd>environmental volatility</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012166</institution-id><institution>National Key Research and Development Program of China</institution></institution-wrap></funding-source><award-id>2023YFF1204200</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32100901</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007219</institution-id><institution>Natural Science Foundation of Shanghai</institution></institution-wrap></funding-source><award-id>21ZR1434700</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003399</institution-id><institution>Science and Technology Commission of Shanghai Municipality</institution></institution-wrap></funding-source><award-id>20dz2260300</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A model with a mixture of three different decision strategies is developed to characterize several atypical behavioral patterns of individuals with depression and anxiety in an uncertain reward environment.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Intelligent behavior requires the ability to adapt to an ever-changing environment. For example, foraging animals must be able to track the changing abundance or scarcity of food resources in different locations and at different timescales. Motor control demands the ability to control limbs that constantly vary in their dynamics (due to fatigue, injury, growth, etc.). Human competitors in all kinds of games or sports must be able to learn and adapt to their opponents’ changing strategies. To understand the mechanisms of these abilities, researchers have examined how (and how well) human agents can learn option values and track the dynamic changes in values in a <italic>volatile reversal learning task</italic> (<xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref>). Unlike the traditional probabilistic reversal learning task where the reward probabilities of two options switch only once (<xref ref-type="bibr" rid="bib8">Cools et al., 2002</xref>), this paradigm includes two volatility conditions (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>): the reward probabilities of the two options remain constant in one condition (i.e. the <italic>stable</italic> condition) and switch periodically in the other (i.e. the <italic>volatile</italic> condition).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic diagram of the experimental task in <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>.</title><p>(<bold>A</bold>) In each trial, participants were presented with two stimuli associated with their potential feedback magnitude. They were instructed to choose one of the two stimuli to receive feedback, but only one stimulus would result in feedback. Participants were required to complete tasks across four experimental contexts. (<bold>B</bold>) Each run consisted of 90 trials in the stable context and 90 trials in the volatile context. In the stable context, the true environmental probability remains unchanged, while in the volatile context, the probability flips every 20 trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig1-v1.tif"/></fig><p>Previous studies have often summarized human behaviors in this paradigm using the parameter of <italic>learning rate</italic>, which describes the efficiency with which current information is used to promote learning. These studies typically fit a specific learning rate to each context, resulting in different learning rate values for different contexts. Using this method, previous studies have reached two important conclusions. First, human participants are able to flexibly adapt to changes in environmental volatility, as evidenced by increasing and decreasing the learning rate in response to volatile and stable conditions. This observation is often referred to as the <italic>learning rate adaptation</italic> effect. Second, individuals with several psychiatric disorders, including anxiety and depression, have been found to have a reduced ability to adapt their learning rate in response to environmental volatility (<xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>). This hallmark can also be indicative of atypical behaviors (<xref ref-type="bibr" rid="bib5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>), psychosis (<xref ref-type="bibr" rid="bib23">Powers et al., 2017</xref>), and autism spectrum disorder (<xref ref-type="bibr" rid="bib17">Lawson et al., 2017</xref>).</p><p>However, the current approach to understanding human behaviors using learning rates has two main limitations. First, the traditional approach increases the number of learning rates as the number of contexts increases, thereby increasing the risk of overfitting. Second, this approach implicitly assumes that learning rate differences can account for all behavioral differences between stable/volatile rewarding contexts and group differences between healthy controls and patients with psychiatric disorders. However, the learning rate is not directly observable and is often estimated by model fitting, which limits its interpretability. The goal of this work is to offer an alternative explanation for human learning behaviors in volatile reversal learning tasks, moving beyond the traditional focus on learning rates. We hypothesize that the differences between stable/volatile contexts and between healthy/patient groups mainly arise from preferences for different decision strategies (<xref ref-type="bibr" rid="bib9">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib11">Fan et al., 2023</xref>). We, therefore, constructed a novel MOS model, which postulates that an observer makes decisions by combining three strategies that balance reward and cognitive resources (<xref ref-type="bibr" rid="bib14">Gershman et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">Griffiths et al., 2015</xref>). First, we consider the most rewarding strategy, <italic>Expected Utility</italic> (EU), which guides decision-making based on the expected utility of each option (calculated as probability multiplied by reward magnitude) (<xref ref-type="bibr" rid="bib31">Von and Morgenstern, 1947</xref>). This EU strategy yields the maximum amount of reward, but the utility calculation itself consumes considerable cognitive resources. Alternatively, humans may choose simpler strategies, e.g., the <italic>magnitude-oriented</italic> (MO) strategy, in which only the reward magnitude was considered during the decision process, and the <italic>habitual</italic> (HA) strategy, in which people simply repeat decisions frequently made in the past regardless of reward magnitude (<xref ref-type="bibr" rid="bib34">Wood and Rünger, 2016</xref>). We use the preference for these decision strategies to roughly estimate participants’ decision styles in the volatile reversal task.</p><p>In this study, we apply and examine the MOS model on a dataset previously reported by <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref> and demonstrate its ability to explain the impaired learning behaviors of individuals diagnosed with depression and anxiety. First, we show that depression and anxiety patients exhibit three signature behavioral patterns indicative of inferior task performance. The MOS model not only qualitatively captures all three behavioral patterns but also quantitatively provides a better fit to the behavioral data than previous models. We then revisit the classical learning rate adaptation theory and show that strategy preference readily accounts for two key learning rate adaptation effects observed in prior research. Our work presents an alternative explanation for the effects of environmental volatility on human learning and highlights the importance of understanding atypical patient behaviors through the lens of decision-making strategies rather than solely focusing on learning rate.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We examined human volatile reversal learning behaviors in a public data set reported by <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>. In a volatile reversal learning task, participants chose between two shaped stimuli to receive feedback. Participants received the presented feedback (e.g. ‘27’ on the ‘square’) when choosing the <italic>feedback stimulus</italic>; otherwise, they received ‘0’ (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The task was divided into four contexts: reward or aversive feedback types crossed with stable or volatile conditions. Participants earned points, which were convertible to monetary rewards, in the reward context or received electric shocks in the aversive context. In stable, one stimulus had a higher fixed feedback probability (always 75%), while in volatile, the dominant stimulus switched every 20 trials (either 20% or 80%), requiring active learning of stimulus-feedback contingencies (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Each participant was instructed to complete two runs of the volatile reversal learning task, one in the reward context and the other in the aversive context. Each run consisted of 180 trials, with 90 trials in the stable context and 90 in the volatile context (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). No additional hints were provided about the transition from one context to another; therefore, participants needed to infer the current context on their own.</p><p>Eighty-six participants took part in this experiment, comprising 20patients with major depressive disorder (MDD), 12patients with generalized anxiety disorder (GAD), and 54 healthy control participants. In this article, we grouped the MDD and GAD individuals into a patient group and the remaining 54 participants into a healthy control group. Please refer to the Materials and methods section for a more detailed introduction to the methods and participant groups.</p><sec id="s2-1"><title>Atypical behavioral patterns in MDD and GAD patients</title><p>Patients with MDD and GAD exhibit three key behavioral patterns as compared to healthy controls. First, patients achieved a significantly lower hit rate (averaged across stable and volatile contexts) as compared to the healthy controls (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; <italic>t</italic>(70.541) = 3.326, p = 0.001, Cohen’s d = 0.723). The hit rate refers to the accuracy of a participant in choosing the correct stimulus throughout the task. Specifically, the correct stimulus is the one that yields reward points in the reward context or avoids electric shocks in the aversive context.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Task performance comparison between healthy control participants and patients diagnosed with major depressive disorder (MDD) and generalized anxiety disorder (GAD).</title><p>Significance symbols: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001; <italic>n.s</italic>.: non-significant. Abbreviations: HC, healthy controls; PAT, patients. (<bold>A</bold>) Comparison of hit rates for healthy controls and patients in stable and volatile contexts. Error bars represent the standard deviation of the estimated mean across 86 participants. (<bold>B</bold>) Learning curves for healthy controls and patients throughout the learning process. The dashed line represents the exemplar feedback probability sequence. For runs that do not follow this exemplar sequence (e.g. starting with volatile and then moving to stable conditions), responses were converted to match the exemplar sequence. The learning curves for both groups were then generated by averaging these converted responses across participants within each group. For better visualization, these curves were then smoothed using a Gaussian kernel with a standard deviation of two trials. The blue arrows indicate the apparent deviation between the true feedback probability and the patients’ asymptotic performance. (<bold>C</bold>) Hit rate differences for healthy controls and patients and their relationship with participants’ symptom severities. Error bars represent the standard deviation of the estimated mean across 54 healthy controls and 32 patients, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig2-v1.tif"/></fig><p>Second, we observed two atypical features of learning curves in the patient group (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The patients’ learning curves took more trials to converge to an asymptote (i.e. seemingly slower learning). Additionally, there was a larger apparent deviation (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, blue arrows) from the true feedback probability. The apparent deviation indicates that the learning curve of the patient group could never converge to the true feedback probability, even given a sufficient number of trials in a stable context.</p><p>Third, aside from the lower learning rate and atypical learning curves that indicate inferior performance in the patient group, we further discovered a reduced hit rate difference within the patient group (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <italic>t</italic>(55.648) = 2.038, p = 0.046, Cohen’s d = 0.478). Interestingly, this hit rate difference is marginally associated with the severity of participants’ symptoms (<italic>r</italic>(86) = –0.194, p = 0.074), as measured using the bifactor analysis reported in <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>. This analysis decomposes symptoms into specific factors for anxiety and depression, with the <italic>g-score</italic> representing the common symptoms between them. The hit rate difference across volatile/stable contexts may be due to the setting of true probability (0.8 in the volatile context and 0.75 in the stable context).</p></sec><sec id="s2-2"><title>The mixture-of-strategies model captures group differences in learning behaviors</title><p>In a volatile reversal learning task, each participant in the experiment faces two fundamental challenges. First, they must engage in decision-making, constructing a policy <inline-formula><mml:math id="inf1"><mml:mi>π</mml:mi></mml:math></inline-formula> to determine an action that maximizes benefit. Second, they must learn to figure out the feedback probability <inline-formula><mml:math id="inf2"><mml:mi>ψ</mml:mi></mml:math></inline-formula> for each stimulus, which is not explicitly stated, through their interactions with the environment. To gain insights into how cognitive impairments lead to the above-mentioned atypical behaviors in the patient group, we developed four families of computational models. All models utilize the same reinforcement learning method for learning feedback probability <inline-formula><mml:math id="inf3"><mml:mi>ψ</mml:mi></mml:math></inline-formula> but differ in how they construct their policies <inline-formula><mml:math id="inf4"><mml:mi>π</mml:mi></mml:math></inline-formula> for decision-making.</p><p>Our target model family, known as MOS, posits that behavioral differences across the two participant groups and between stable/volatile contexts can be attributed to varying weightings of multiple decision strategies: EU, MO, and HA<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This particular three-strategy configuration was chosen as the representative model because it best accounts for human behavioral data (Figure 3—figure supplement 1). The EU strategy (<inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) postulates that human agents rationally calculate the value of each stimulus <inline-formula><mml:math id="inf6"><mml:mi>s</mml:mi></mml:math></inline-formula> by multiplying its estimated feedback probability <inline-formula><mml:math id="inf7"><mml:mi>ψ</mml:mi></mml:math></inline-formula> with reward magnitude <inline-formula><mml:math id="inf8"><mml:mi>m</mml:mi></mml:math></inline-formula>. The MO strategy (<inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) only focuses on feedback magnitude <inline-formula><mml:math id="inf10"><mml:mi>m</mml:mi></mml:math></inline-formula>, disregarding feedback probability <inline-formula><mml:math id="inf11"><mml:mi>ψ</mml:mi></mml:math></inline-formula>. This is certainly an irrational strategy but more economical in terms of cognitive efforts. The HA strategy (<inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) reflects the tendency to repeat previous frequent choices, depending on neither feedback magnitude <inline-formula><mml:math id="inf13"><mml:mi>m</mml:mi></mml:math></inline-formula> nor feedback probability <inline-formula><mml:math id="inf14"><mml:mi>ψ</mml:mi><mml:mo>.</mml:mo></mml:math></inline-formula> Parameters <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the weighting of each strategy representing a decision-maker’s preference for each strategy. We fit two MOS variants, MOS6 and MOS22. Both models have identical update rules; however, MOS22, the context-dependent variant, fits a separate set of parameters to each experimental context, whereas MOS6, the context-free variant, uses one set of parameters for all contexts (<xref ref-type="table" rid="table1">Table 1</xref>). This approach applies to the other three model families, each offering two distinct variants.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Model’s parameters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">Context-free parameters</th><th align="left" valign="bottom">Context-dependent parameters</th></tr></thead><tbody><tr><td align="left" valign="bottom">MOS6</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">MOS22</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">FLR6</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">FLR22</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">RS3</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">RS13</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">PH4</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">PH17</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>In contrast to the mixture-of-strategies account, the Flexible-Learning-Rate (FLR) models—the context-free FLR6 and the context-dependent FLR22—hypothesize that behavioral differences between groups and contexts primarily arise from different learning rates, known as learning rate adaptation. These models, reported as the best models by <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>, select stimuli with higher values, estimated by a linear combination of differences in feedback probability, (non-linear) feedback magnitude, and the stimuli’s consistency with habitual behaviors. The Risk-Sensitive (RS) models (RS3 and RS13), adopted from <xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref> and <xref ref-type="bibr" rid="bib5">Browning et al., 2015</xref>, share the same hypothesis about human behavioral differences. These models use the EU strategy for decision-making and consider a subjective distortion of the learned feedback probability when calculating the expected value. To further investigate the hypothesis regarding differences in learning rates, we tested a family of models with a built-in adaptive learning rate, known as the Pearce-Hall models (PH4, PH17). See the Materials and methods section for the detailed model implementations.</p><p>The model fitting reveals that the MOS models accurately account for human behaviors. MOS6 and MOS22 were the best-fitting models in terms of the Bayesian Information Criterion (BIC; <xref ref-type="bibr" rid="bib27">Schwarz, 1978</xref>) and Akaike Information Criterion (AIC; <xref ref-type="bibr" rid="bib1">Akaike, 1974</xref>), respectively (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The group-level Bayesian model comparison (<xref ref-type="bibr" rid="bib26">Rigoux et al., 2014</xref>) further supports MOS6 as the best-fitting model (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). These model comparisons highlight that the MOS models outperform the other three families of models supporting the learning adaptation account, suggesting that behavioral variations might not be fully captured by learning rate adaptations alone.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Models’ quantitative and qualitative fit to human behavioral data.</title><p>Significance symbols: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001; <italic>n.s</italic>.: non-significant. Abbreviations: HC, healthy controls; PAT, patients. (<bold>A</bold>) Relative performance of models compared to the MOS6 model, as measured by the Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). Each dot represents a model’s fit for an individual participant, with error bars showing the standard deviation of the estimated mean across 86 participants. (<bold>B</bold>) Group-level Bayesian model selection as indicated by Protected Exceedance Probability (PXP). (<bold>C–E</bold>) Models' predicted hit rate (<bold>C</bold>) hit rate differences (<bold>D</bold>) and learning curves (<bold>E</bold>) for healthy controls and patients, respectively. Error bars denote the standard deviation of the estimated mean across 54 healthy controls and 32 patients, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>The fit to the human data of different mixture-of-strategies (MOS) variants.</title><p>We compared the target MOS6 model (red) with its 13 alternatives in three categories: lesion models generated by removing strategies from the MOS6 (orange), models with components replaced (green), and extensions generated by adding a new strategy (blue). Note that the PF here is the abbreviation of <italic>Probability of Feedback</italic>, which constructs a policy only depends on the estimated feedback probability, the RD here is the abbreviation of <italic>Random</italic> strategy, and the probability of choosing each stimulus is 1/2. The target EU, MO, and HA strategies construct a simple decision pool but not too simpler compared to the other alternatives. First, the three ‘lesion’ variants, EU + MO, EU + HA, and MO + HA, all exhibit a lower model fittings performance compared to the MOS6 model, with the EU + HA variant being marginally closer. However, the context-independent EU + HA and its context-dependent version (EU + HA18) provide conflicting interpretations of the behavioral differences between healthy controls and patients, suggesting a potential oversimplification of human behavior. Second, replacing the EU strategy with PF or the MO strategy with RD adversely affects fitting performance. The PF strategy’s failure indicates that a mere linear combination of feedback probability and potential magnitude does not account for human decision-making behavior. The RD strategy’s failure confirms that participants were actively using the MO strategy, rather than making random choices. Lastly, the two extension models do not significantly improve the fitting performance, suggesting that the current MOS6 model is good enough describing human behaviors. There is no need to involve additional components.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Hit rates (<bold>A</bold>) and hit rate differences (<bold>B</bold>) for all models.</title><p>Significance symbols: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001; <italic>n.s</italic>.: non-significant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Simulated learning curves for the healthy control (HC) and patient (PAT) groups, each averaged from 100 simulations within the group and were smoothed with a Gaussian kernel (standard deviation of two trials).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig3-figsupp3-v1.tif"/></fig></fig-group><p>The MOS models can not only better capture the data quantitatively, but they can also effectively reproduce the three key behavioral differences between the groups. The MOS models reproduce the lower hit rate (<xref ref-type="fig" rid="fig3">Figure 3C</xref>)<bold>,</bold> reduced hit rate difference (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), and slower learning curves with apparent deviations (<xref ref-type="fig" rid="fig3">Figure 3E</xref>) observed in the patient group, whereas the FLR models struggle to produce all these effects. See <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplements 2</xref>–<xref ref-type="fig" rid="fig3s3">3</xref> for the behavioral patterns for all models.</p><p>In short, we conclude that the MOS models best account for human behavioral data both qualitatively and quantitatively. In the following sections, we will analyze the fitted parameters of the MOS models to interpret the atypical behavioral patterns of the patient group.</p></sec><sec id="s2-3"><title>MDD and GAD patients favor simpler decision strategies</title><p>We first focused on the fitted parameters of the MOS6 model. We compared the weight parameters (<inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) across groups and conducted statistical tests on their logits (<inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). The patient group showed a ~37% preference towards the EU strategy, which is significantly weaker than the ~50% preference in healthy controls (healthy controls’ <inline-formula><mml:math id="inf36"><mml:mi>λ</mml:mi></mml:math></inline-formula>: M = 0.991, SD = 1.416; patients’ <inline-formula><mml:math id="inf37"><mml:mi>λ</mml:mi></mml:math></inline-formula>: M = 0.196, SD = 1.736; <italic>t</italic>(54.948) = 2.162, p = 0.035, Cohen’s d = 0.509; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Meanwhile, the patients exhibited a weaker preference (~27%) for the HA strategy compared to healthy controls (~36%) (healthy controls’ <inline-formula><mml:math id="inf38"><mml:mi>λ</mml:mi></mml:math></inline-formula>: M = 0.657, SD = 1.313; patients’ <inline-formula><mml:math id="inf39"><mml:mi>λ</mml:mi></mml:math></inline-formula>: M = –0.162, SD = 1.561; <italic>t</italic>(56.311) = 2.455, p = 0.017, Cohen’s d = 0.574), but a stronger preference for the MO strategy (14% vs 36%; healthy controls’ <inline-formula><mml:math id="inf40"><mml:mi>λ</mml:mi></mml:math></inline-formula>: M = –1.647, SD = 1.930; patients’ <inline-formula><mml:math id="inf41"><mml:mi>λ</mml:mi></mml:math></inline-formula>: M = –0.034, SD = 2.091; <italic>t</italic>(63.746) = –3.510, p = 0.001, Cohen’s d = 0.801). Most importantly, we also examined the learning rate parameter in the MOS6 but found no group differences (<italic>t</italic>(68.692) = 0.690, p = 0.493, Cohen’s d = 0.151). These results strongly suggest that the differences in decision strategy preferences can account for the learning behaviors in the two groups without necessitating any differences in learning rate per se.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Parameter analyses of the MOS6 model and simulated behaviors for all three strategies.</title><p>Significance symbol conventions are: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001; <italic>n.s</italic>.: non-significant. Abbreviations: HC, healthy controls; PAT, patients. (<bold>A</bold>) The fitted weighting parameters and learning rate of the MOS6 model. The y-axis means averaged preference over different volatile contexts (volatile/stable) and feedback contexts (reward/aversive). <inline-formula><mml:math id="inf42"><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> indicates the averaged weighting parameters for each participant group. Error bars denote the standard deviation of the estimated mean across 54 healthy controls and 32 patients, respectively. (<bold>B</bold>) Simulated hit rates for the three decision strategies. Error bars represent the standard deviation across 200 simulations. The 200 simulations were evenly divided between groups using parameters similar to the healthy control group and the patient group. The groups differed only in their strategy preference (differences in <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) while all other parameters remained constant. For more simulation details, refer to Materials and methods<italic>,</italic> Simulation details<italic>.</italic> (<bold>C</bold>) The average simulated learning curve for each strategy across 200 simulations, was smoothed with a Gaussian kernel (standard deviation of two trials). (<bold>D</bold>) Simulated hit rate differences between volatile and stable for the three decision strategies. Error bars represent the standard deviation across 200 simulations. (<bold>E</bold>) Simulated learning curves for the healthy controls and patients, each averaged from 100 simulations within the group and smoothed with a Gaussian kernel (standard deviation of two trials).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Parameter analyses of the MOS22 model.</title><p>Significance symbol conventions are: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001; <italic>n.s</italic>.: non-significant. Abbreviations: HC, healthy controls; PAT, patients. Error bars denote the standard deviation of the estimated mean. (<bold>A</bold>) The fitted weighting parameters of the MOS22 across participant groups. (<bold>B–D</bold>) The fitted log learning rate of the MOS22 model across participant groups (<bold>B</bold>) volatile contexts (<bold>C</bold>) and outcome valence (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig4-figsupp1-v1.tif"/></fig></fig-group><p>The MOS6 assumes no parameter differences across the four contexts, which may dilute the group differences in learning rate. We further analyzed the MOS22, which explicitly estimates different sets of weighting and learning rate parameters in different contexts (i.e. context-dependent), and found a consistent conclusion about participants’ strategy preferences. We first conducted three separate 2 × 2 × 2 ANOVAs, each setting the logit of a weighting parameter (<inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) as the dependent variable, and participant groups (healthy control/patient) as the between-subject variable, and volatile contexts (stable/volatile), and feedback contexts (reward/aversive) as within-subject variables. We again found a weaker preference for EU (<italic>F</italic>(1, 80) = 13.537, p&lt;0.001, <italic>ƞ</italic><sup>2</sup> = 0.084) and a stronger preference for MO (<italic>F</italic>(1, 80) = 7.791, p = 0.009, <italic>ƞ</italic><sup>2</sup> = 0.046) in the patient group (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). However, unlike the MOS6, the MOS22 revealed no significant group difference was observed in the HA strategy (<italic>F</italic>(1, 80) = 0.020, p = 0.887, <italic>ƞ</italic><sup>2</sup>&lt;0.001), and a significantly a stronger preference for EU under the reward context Bonferroni-<italic>t</italic> = 2.243, p = 0.028, Cohen’s d = 0.209. This suggests a possible confounding between the EU and HA strategies. Next, we examined the learning rates of the MOS22. A 2 × 2 × 2 × 2 ANOVA was performed with the (log) learning rate parameter as the dependent variable, outcome valence (better/worse than expectation) as a within-subject variable in addition to the three independent variables (group/volatile context/feedback context) as introduced above. We again found no significant difference between patients and healthy controls (<italic>F</italic>(1, 77) = 0.393, p = 0.533, <italic>ƞ</italic><sup>2</sup> = 0.003; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Most importantly, the MOS22 model revealed no learning rate adaptation effect, as indicated by the learning rate parameters in the volatile context not being significantly larger than that in the stable context (<italic>F</italic>(1, 77) = 0.126, p = 0.724, <italic>ƞ</italic><sup>2</sup>&lt;0.001; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>). Based on these findings, we drew two conclusions. First, MOS6 and MOS22 made consistent descriptions of participants’ strategy preferences during decisions: the behavioral differences between the two participant groups were mainly attributed to differences in their strategy preferences, rather than their learning rates. Second, the learning rate adaptation effect may be simply explained by context-free strategy preferences. We will further explain this second point in later sections.</p></sec><sec id="s2-4"><title>Understanding patients’ inferior task performances through strategy preferences</title><p>In this section, we illustrate how strategy preferences account for the three learning behavioral differences observed between the two participant groups, as shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. To better understand how each decision strategy influences the three behavioral patterns, we simulated the MOS6 model using the median fitted parameters and outputted the decisions for each strategy (see Simulation details in Materials and methods).</p><p>For hit rate, our simulations showed that the EU strategy achieved the highest hit rate, while the MO strategy basically performed at the chancel-level (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). These results are intuitively understandable. Since the hit rate is defined based on feedback probability, the EU strategy, which actively tracks this probability, should be able to achieve a high hit rate. In contrast, the MO strategy, which completely ignores feedback probability, should achieve a chance-level hit rate. Interestingly, our simulations also showed that the HA strategy achieved an above-chance hit rate. This is because, although the HA strategy appears not to consider feedback probability directly, it still somewhat tracks feedback probability by simply repeating the past choices made by the EU. Accordingly, assigning lower weights to the two higher-hit-rate strategies, EU and HA (i.e. higher weighting on MO), naturally leads to inferior performance in the patients (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>We also visualized the simulated learning curves for each strategy (averaged across the two groups) throughout the task (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). In both stable and volatile contexts, the EU strategy quickly approximates and converges to the true feedback probability. The HA strategy takes more trials to approach the true feedback probability, exhibiting slower learning. The MO strategy does not respond to environmental feedback, resulting in an almost flat learning curve. When the learning curves are combined separately for the two groups, we recover the seemingly slower learning curve in the patient group due to their stronger preference for the MO strategy (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). We also noted the larger apparent deviation from the true feedback probability in the patient group. These two features in <xref ref-type="fig" rid="fig2">Figure 2B</xref> can thus be readily explained by the patients’ stronger preference for the MO strategy, as the MO strategy does not learn feedback probability at all and exhibits a flat learning curve.</p><p>For the hit rate differences between the stable and volatile contexts, our simulations showed that the EU strategy achieves a higher hit rate in the volatile context than in the stable context (i.e. positive hit rate difference) (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). This is attributed to the EU strategy’s active tracking of feedback probability (i.e. the maximum possible hit rate), which increases from 75% in the stable context to 80% in the volatile context. Conversely, there were no changes in the MO strategy’s hit rate from the stable to volatile contexts (i.e. 0 hit rate difference) because MO does not track feedback probability. Additionally, we found that the hit rate of the HA strategy was higher in the stable context than in the volatile (i.e. negative hit rate difference). This is possibly because the HA strategy requires more time to relearn true probability (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), particularly in the volatile context where the true probability frequently flips. Based on this, we can roughly estimate the hit rate difference for the healthy control group as~0.042 (<inline-formula><mml:math id="inf45"><mml:msubsup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:mn>0.3</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:mo>−</mml:mo><mml:mn>0.3</mml:mn><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mn>0.3</mml:mn><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mn>0.36</mml:mn><mml:mo>×</mml:mo><mml:mo>−</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>) and for the patient group as~0.030 (<inline-formula><mml:math id="inf46"><mml:msubsup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:mn>0.3</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:mo>−</mml:mo><mml:mn>0.3</mml:mn><mml:mo>=</mml:mo><mml:mn>0.37</mml:mn><mml:mo>×</mml:mo><mml:mn>0.3</mml:mn><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mn>27</mml:mn><mml:mo>×</mml:mo><mml:mo>−</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>). This explains why healthy controls exhibited slightly a larger hit rate difference than the patient participants (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p></sec><sec id="s2-5"><title>Atypical strategy preferences are connected to the general severity of anxiety and depression</title><p>We investigated the relationship between strategy preferences in the MOS6 model and symptom severity in the patient group (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Our findings indicate that patients with severe symptoms exhibit a weaker preference for the cognitively demanding EU strategy (Pearson’s <italic>r</italic> = –0.221, p = 0.040) and a stronger preference for the simpler MO strategy (Pearson’s <italic>r</italic> = 0.360, p = 0.001). Additionally, there was a significant correlation between symptom severity and the preference for the HA strategy (Pearson’s <italic>r</italic> = –0.285, p = 0.007). These results highlight the strong clinical relevance of strategy preferences.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Predict participants’ symptom severity (<italic>g</italic> score) using strategy preferences of the MOS6 model.</title><p>Each dot represents one participant. The shaded areas reflect 95% confidence intervals of the regression prediction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Strategy preferences predict participants' general factor score (<italic>g</italic> score) in the bifactor analysis reported by <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>.</title><p>The y-axis indicates the averaged preference over different volatility levels (volatile and stable) and feedback types (reward and aversive). This average operation is permitted here because the logit of the weight is normally distributed. The shaded areas reflect 95% confidence intervals of the regression prediction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig5-figsupp1-v1.tif"/></fig></fig-group><p>For completeness, we examined the correlation between learning rate adaptation (log volatile learning rate – log stable learning rate) and symptom severity within the MOS22 model (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Not surprisingly, we found no significant correlation (<italic>r</italic>(86) = 0.130, p = 0.233), which is consistent with our finding of no difference in learning rates across the two volatile contexts.</p></sec><sec id="s2-6"><title>Strategy preferences may explain the learning rate differences across groups and contexts</title><p>Previous studies using probabilistic reversal learning tasks have made three major conclusions about learning rate. First, it has been documented that individuals with anxiety and depression have a smaller learning rate parameter (<xref ref-type="bibr" rid="bib6">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Pike and Robinson, 2022</xref>), thereby exhibiting a slower learning curve (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) and, possibly, a lower hit rate (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Second, human participants have been found to be able to flexibly increase their learning rate in response to high environmental volatility (<xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref>). Third, patient participants may exhibit a deficit in such learning rate adaptation (<xref ref-type="bibr" rid="bib5">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>), exhibiting a lesser extent of increase (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>However, we recognize two limitations in this learning rate interpretation. First, a higher learning rate does not necessarily improve the hit rate; it may lead to overreacting to feedback from stimuli with low probabilities. Second, and more importantly, a reduced learning rate merely prolongs the time needed to approach the true probability (<xref ref-type="bibr" rid="bib4">Boyd and Vandenberghe, 2004</xref>) but cannot explain the apparent deviation from the true probability observed in patient participants (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In contrast, a mixture of strategies can naturally account for these two phenomena. As mentioned in the previous section, the mixture of EU and MO results in both a seemingly lower learning curve and a larger apparent deviation.</p><p>Here, we further demonstrate that the behavioral differences caused by a mixture of strategies could reflect the learning rate adaptation across the stable and volatile contexts. We used the MOS6 to synthesize behavioral data for agents resembling healthy controls and patients by controlling all parameters except for the decision weights. Specifically, we set the weights of <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to 60%, <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to 15% and <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to 25% for the healthy control group, and the weights of <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to 15%, <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to 60% and <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to 25% for the patient group, with all other parameters fixed to the median values across all participants (see more details in Materials and methods). We simulated each group 20 times, and the simulated data reproduced the slower learning curve and the apparent difference in the patient group (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We fit the simulated data generated by MOS6 with the FLR22 model and found significant differences in the (log) learning rate between stable and volatile contexts (paired <italic>t</italic>-test(39 ) = –3.217, p = 0.003, Cohen’s d = 0.721; <xref ref-type="fig" rid="fig6">Figure 6B</xref>). Furthermore, the agent resembling the patient group demonstrated a trend toward reduced learning rate adaptation compared to the agent resembling the healthy control group (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), consistent with the learning rate adaptation theory. These findings suggest that what might be perceived as learning rate adaptation could result from a mixture of strategy preferences. This observation also implies that strategy preferences may, at least partially, explain the maladaptive adaptations in learning rate observed in patients in response to environmental volatility.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Reproduction of the two learning rate adaptation effects using the MOS6 model.</title><p>Significance symbol conventions are: *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001; <italic>n.s</italic>.: non-significant. HC represents the healthy-control-like agent; PAT represents the patient-like agent. (<bold>A</bold>) Simulated learning curves for the healthy controls and patients generated by the MOS6 model. Both curves are averaged over 80 runs of tasks (4 task sequences × 20 experiments) and are smoothed with a Gaussian kernel (standard deviation of two trials). (<bold>B</bold>) The fitted FLR22 learning rate parameters are for the stable context and the volatile context. Error bars denote the standard deviation across 40 synthesized datasets. (<bold>C</bold>) Learning rate adaptations, calculated by log volatile learning rate – log stable learning rate, for the healthy control-like agent and for the patient-like agent. Error bars stand for the standard deviation across 20 synthesized datasets.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig6-v1.tif"/></fig></sec><sec id="s2-7"><title>Model and parameter recovery analyses support model and parameter identifiability in MOS</title><p>Although we have previously demonstrated that the MOS models are quantitatively best-fitting, there are two potential confounding factors. First, it is possible that differences in learning rate, rather than differences in strategy preference, could produce the same behavioral outcomes that are indistinguishable by the model fitting. If this holds, the MOS model might be problematic, as all learning rate differences may be automatically attributed to strategy preferences because of some unknown idiosyncratic model fitting mechanisms. Second, the fact that the MOS models outperform the others may be partly due to an unknown bias in the model design. It is possible that the MOS models always win, irrespective of how the data is generated.</p><p>To circumvent these issues, we conducted parameter recovery analyses on MOS6 and model recovery analyses on all models to investigate the identifiability of true parameters and models. For parameter recovery, we generated 80 synthetic datasets using 80 different parameter sets, each varying the four parameters of interest <inline-formula><mml:math id="inf53"><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>, with the remaining parameters fixed to the median of the fitted parameters (<inline-formula><mml:math id="inf54"><mml:mi>β</mml:mi></mml:math></inline-formula> = 10.803, <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.423). Each synthetic dataset consisted of 10 runs, resulting in a total of 800 synthetic runs (80 parameter sets × 10 runs). For each dataset, we fitted our MOS6 model and compared the fitted parameters to the ground-truth parameters. The parameter recovery results (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) demonstrate that the true parameters can be accurately estimated and identified (all Pearson’s <italic>r</italic>s &gt;0.720), indicating that the effects of learning rate and weighting parameters are not interchangeable in the MOS6 model.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Parameter and model recovery analyses.</title><p>(<bold>A</bold>) Parameterrecovery for the MOS6 model. (<bold>B</bold>) Model recovery analysis, showing the performance of models as evaluated by averaged relative Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), as well as Protected Exceedance Probability (PXP) scores for synthesized data generated from each of the eight models. Darker tiles indicate better fits to the synthesized data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-fig7-v1.tif"/></fig><p>For model recovery, we sampled 40 participants (20 in each group) and used their fitted model parameters to generate synthetic datasets from each of the eight models. For each participant, we simulated 10 runs of behavioral data, resulting in a total of 3200 synthetic runs (8 generating models × 40 participants × 10 runs). We then fit all eight models to every dataset using the MAP method as the same before. The best-fitting model was always the one that generated the data, as indicated by all three quantitative metrics: AIC, BIC, and PXP (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). We note that the RS3 and PH4 models tend to account well for each other. The MOS6 model achieves good fitting performances on synthetic datasets generated by the RS3 and PH4 models, but not vice versa. The slight confusion between RS3, PH4, and MOS6 is because they all include the EU strategy. Most importantly, the MOS and FLR models cannot adequately account for each other’s synthetic datasets, strongly supporting the independent computational effects of strategy preference and learning rate.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this article, we propose to understand humans’ learning behaviors, especially the differences between healthy controls and patients, in the volatile reversal learning task through the lens of a mixture-of-strategies. We develop the MOS model, which assumes that human participants make decisions by combining three distinct components: the EU, MO, and HA strategies. The EU strategy is rewarding but cognitively demanding, in contrast to the other two strategies, which are simpler and heuristic but less rewarding. We show that the MOS model can qualitatively capture several behavioral patterns that cannot be explained by previous models and quantitatively better capture human behaviors and healthy-patient differences when applied to a public dataset. The model reveals that individuals with MDD and GAD exhibit an atypical preference for simpler and less rewarding strategies (i.e. a stronger preference for the MO strategy), and this preference alone could explain their inferior task performance relative to healthy controls, as indicated by lower hit rates, reduced adaptation volatility, and slower learning curves. Furthermore, we demonstrate that the MOS model can reproduce the human behavioral learning rate adaptation effect without changing the learning rate itself. These findings suggest that a mixture of strategies provides an effective and parsimonious explanation for human learning behaviors in volatile reversal tasks.</p><sec id="s3-1"><title>The role of the HA strategy in volatile reversal learning</title><p>Although many observed behavioral differences can be explained by a shift in preference from the EU to the MO strategy among patients, we also explore the potential effects of the HA strategy. Compared to the MO, the HA strategy also saves cognitive resources but yields a significantly higher hit rate (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Therefore, a preference for the HA over the MO strategy may reflect a more sophisticated balance between reward and complexity within an agent (<xref ref-type="bibr" rid="bib15">Gershman, 2020</xref>): when healthier participants exhaust their cognitive resources for the EU strategy, they may cleverly resort to the HA strategy, adopting a simpler strategy but still achieving a certain level of hit rate. This explains the stronger preference for the HA strategy in the HC group (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) and the negative correlation between HA preferences and symptom severity (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Apart from shedding light on the cognitive impairments of patients, the inclusion of the HA strategy significantly enhances the model’s fit to human behaviors (see examples in <xref ref-type="bibr" rid="bib9">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib15">Gershman, 2020</xref>; and also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec id="s3-2"><title>Disassociate the learning rate adaptation and mixture of strategies</title><p>It is well-established that humans apply flexible learning rates in response to environmental volatility, exemplifying the successful application of ideal observer analysis. <xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref> constructed a hierarchical ideal Bayesian observer for the volatile reversal learning task that dynamically models how higher-order environmental volatility influences the updating speed of lower-order feedback probabilities. This model suggests that the human brain estimates environmental volatility, and humans are expected to exhibit a faster-updating speed for feedback probabilities in volatile contexts. Consistent with their results, the context-dependent RS13 model revealed higher learning rate parameters in volatile contexts. <xref ref-type="bibr" rid="bib5">Browning et al., 2015</xref> identified the increase in learning rate from stable to volatile contexts as a hallmark of human sensitivity to environmental volatility. They found that individuals with high trait anxiety showed reduced adaptations, thus indicating lower sensitivity to volatility. <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref> extended this research to MDD and GAD patients receiving either reward or aversive feedback. Furthermore, the phenomenon of an increased learning rate from stable to volatile conditions has also been observed in other paradigms, such as the <italic>Predictive Inference task</italic> (<xref ref-type="bibr" rid="bib20">Nassar et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Nassar et al., 2010</xref>), where participants explicitly report their estimation of environmental statistics, allowing for a direct estimation of the learning rate.</p><p>Based on our findings, we applied the MOS model—an alternative but sufficiently accurate model—to the data collected from the volatile reversal task and found that the expected learning rate adaptation was not observed. Instead, the MOS model points to an alternative explanation that accounts for multiple human behavioral patterns and their symptom severity in a more parsimonious manner, involving fewer parameters. More importantly, it is possible for the MOS model to capture the pattern of learning rate adaptation without necessitating actual changes in learning rates across different contexts. These findings indicate that future studies may systematically compare the accounts of learning rate adaptation and a mixture-of-strategies.</p><p>It is important to note that learning rate adaptations and strategy preferences could simultaneously influence behaviors. However, accurately describing both mechanisms, particularly in terms of differences between populations, requires more refined behavioral paradigms. For example, it would be helpful to use paradigms like the predictive inference task (<xref ref-type="bibr" rid="bib20">Nassar et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Nassar et al., 2010</xref>), which allows participants to directly report their learning rates, to minimize the confounding factors in the decision process.</p></sec><sec id="s3-3"><title>Atypical learning speed in psychiatric diseases</title><p>In the present work, we found that individuals with depression and anxiety display apparent flatter learning curves in the probabilistic learning tasks (shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). We attributed this observation to participants’ strategy preferences. However, in conventional Rescorla-Wagner modeling, learning speed is primarily indicated by the learning rate parameter. For example, <xref ref-type="bibr" rid="bib6">Chen et al., 2015</xref> conducted a systematic review of reinforcement learning in patients with depression and identified 10 out of 11 behavioral datasets showing either comparable or slower learning rates in depressive patients. Nonetheless, depressive patients may not always exhibit slower learning rates. In a recent meta-analysis summarizing 27 articles with 3085 participants, including 1242 with depression and/or anxiety, <xref ref-type="bibr" rid="bib22">Pike and Robinson, 2022</xref> found a reduced reward but enhanced aversive learning rate. This finding yields two practical implications. First, the heterogeneous findings in the literature may arise from heterogeneous pathologies in depression and anxiety. Second, some behavioral variations introduced by strategy preferences might have been misidentified as learning rate effects. The MOS model may provide useful complementary explanations for the consequences of a spectrum of symptoms.</p></sec><sec id="s3-4"><title>Limitations and future directions</title><p>The MOS model was developed to provide context-free interpretations of the learning rate differences observed between stable and volatile contexts, as well as between healthy individuals and patients. However, we also recognize that the MOS account may not justify other learning rate effects based solely on strategy preferences. One such example is valence-specific learning rate differences, where learning rates for better-than-expected outcomes are higher than those for worse-than-expected outcomes (<xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>). When fitted to the behavioral data, the context-dependent MOS22 model does not reveal valence-specific learning rates (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref>). Moreover, the valence-specific effect was not replicated in the FLR22 model when fitted to the synthesized data of MOS6.</p><p>The context-dependent MOS22 model revealed several weak interaction effects, suggesting an interaction between learning adaptation and strategy preferences. For example, patients with MDD and GAD may find it too taxing to increase their learning rates in the volatile context and instead resort to simpler strategies, such as MO, as a compromise. Investigating this hypothesis may require a paradigm incorporating a self-reporting learning rate module, like the predictive inference task (<xref ref-type="bibr" rid="bib19">Nassar et al., 2010</xref>), in volatile reversal learning tasks.</p><p>Theories suggest that humans increase learning rates in the volatile context due to increased perceived uncertainty about environmental statistics (<xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Nassar et al., 2010</xref>), while others propose that strategies enabling more exploration are preferred when managing uncertainty (<xref ref-type="bibr" rid="bib11">Fan et al., 2023</xref>; <xref ref-type="bibr" rid="bib33">Wilson et al., 2014</xref>). To explore these ideas, we may need to adjust the paradigm to offer a wider choice of stimuli, from two to three or four (i.e. set size effects). Another question is why individuals with depression and anxiety tend toward simpler decision-making strategies. <italic>Rumination</italic>, a maladaptive emotion regulation behavior characterized by persistent negative thoughts observed in individuals with depression (<xref ref-type="bibr" rid="bib28">Song et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Yan et al., 2022</xref>), may consume cognitive resources, hindering the use of the more complex but rewarding EU strategy.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>In this section, we provide the mathematical and implementational details of our model. Code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/fangzefunny/policy-analysis">https://github.com/fangzefunny/policy-analysis</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib12">Fang, 2024</xref>).</p><sec id="s4-1"><title>Datasets</title><p>We focused on the data from Experiment 1 reported by <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>. The data is publicly available via <ext-link ext-link-type="uri" xlink:href="https://osf.io/8mzuj/">https://osf.io/8mzuj/</ext-link>. The original study included data from two experiments. The data from Experiment 2 was not used here because it was implemented on Amazon’s Mechanical Turk with no information about the participants' clinical diagnoses. Here, we provide critical information about Experiment 1 (also see <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref> for more technical details).</p><sec id="s4-1-1"><title>Participants</title><p>Eighty-six participants took part in this experiment. The pool includes 20 patients with a major depressive disorder (MDD), 12 patients with a generalized anxiety disorder (GAD), and 54 healthy control participants. The diagnosis was made through a phone screen, an in-person screening session, and the structured clinical interview following DSM-IV-TR (SCID) in 20 MDD patients, 12 GAD patients, and 20 healthy control participants. The remaining 30 healthy control participants were recruited without SCID. In this article, we grouped the MDD and GAD individuals into a patient (PAT) group and the remaining 54 participants into a healthy control (HC) group. The detailed difference between MDD and GAD is not the focus of this paper. We will show later that the general factor behind MDD and GAD is the only factor that predicts learning behavior (see next section for details), a similar result reported in the original study (<xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>).</p></sec><sec id="s4-1-2"><title>Clinical measures</title><p>The severity of anxiety and depression in all participants was measured by several standard clinical questionnaires, including the Spielberger State-Trait Anxiety Inventory (STAI form Y; <xref ref-type="bibr" rid="bib29">Spielberger et al., 1983</xref>), the Beck Depression Inventory (BDI; <xref ref-type="bibr" rid="bib2">Beck et al., 1961</xref>), the Mood and Anxiety Symptoms Questionnaire (MASQ; <xref ref-type="bibr" rid="bib7">Clark and Watson, 1991</xref>; <xref ref-type="bibr" rid="bib32">Watson and Clark, 1991</xref>), the Penn State Worry Questionnaire (<xref ref-type="bibr" rid="bib18">Meyer et al., 1990</xref>), the Center for Epidemiologic Studies Depression Scale (CESD; <xref ref-type="bibr" rid="bib24">Radloff, 1977</xref>), and the Eysenck Personality Questionnaire (EPQ; <xref ref-type="bibr" rid="bib10">Eysenck and Eysenck, 1975</xref>). An exploratory bifactor analysis was then applied to item-level responses in all questionnaires to disentangle the variance that is common to GAD and MDD or unique to each. The results of this analysis summarized participants' symptoms into three orthogonal factors: a general factor (<italic>g</italic>) explaining the common symptoms, a depression-specific factor (<italic>f</italic>1), and an anxiety-specific factor (<italic>f</italic>2), which are all included in the public dataset. Similar to the original study, here we focused on the general factor (<italic>g</italic> score) to indicate the participants' severity of their psychiatric symptoms.</p></sec><sec id="s4-1-3"><title>Stimuli and behavioral task</title><p>In a volatile reversal learning task, participants were instructed on each trial to choose between two stimuli, represented by different shapes, in order to receive feedback. The locations of the two shapes were counterbalanced across trials. The potential amount of feedback (referred to as feedback magnitude) was presented together with the stimuli. Only one of the two stimuli was associated with actual feedback (0 for the other one). The feedback magnitude, ranged between 1–99, was sampled uniformly and independently for each shape from trial to trial. Actual feedback was delivered only if the stimulus associated with feedback was chosen; otherwise, a number ‘0’ was displayed on the screen, signifying that the chosen stimulus returned no reward.</p><p>Participants was supposed to complete this learning and decision-making task in four experimental contexts, two feedback contexts (reward or aversive) × two volatility contexts (stable or volatile). Participants received points in the reward context and an electric shock in the aversive context. The reward points in the reward context were converted into a monetary bonus by the end of the task, ranging from £0 to £10. In the stable context, the dominant stimulus (i.e. a certain stimulus induces the feedback with a higher probability) provided a feedback with a fixed probability of 0.75, while the other one yielded a feedback with a probability of 0.25. In the volatile context, the dominant stimulus’s feedback probability was 0.8, but the dominant stimulus switched between the two every 20 trials. Hence, this design required participants to actively learn and infer the changing stimulus-feedback contingency in the volatile context.</p><p>Each participant was instructed to complete two runs of the volatile reversal learning task, one in the reward context and the other in the aversive context. Each run consisted of 180 trials, with 90 trials in the stable context and 90 in the volatile context. No additional hints were provided about the transition from one context to another; therefore, participants need to infer the current context on their own. A total of 79 participants completed tasks in both feedback contexts. Four participants only completed the task in the reward context, while three participants only completed the aversive task.</p></sec></sec><sec id="s4-2"><title>Computational modeling</title><p>We first introduce our notation system. We denote each stimulus <inline-formula><mml:math id="inf56"><mml:mi>s</mml:mi></mml:math></inline-formula> as one of two possible states <inline-formula><mml:math id="inf57"><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>. The labeled feedback magnitude (i.e. reward points or shock intensity) of the stimulus is <inline-formula><mml:math id="inf58"><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and the feedback probability is <inline-formula><mml:math id="inf59"><mml:mi>ψ</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Following the convention in reinforcement learning (<xref ref-type="bibr" rid="bib30">Sutton and Barto, 2018</xref>), we presume that the decision is made from a policy <inline-formula><mml:math id="inf60"><mml:mi>π</mml:mi></mml:math></inline-formula> that maps the observed magnitudes <inline-formula><mml:math id="inf61"><mml:mi>m</mml:mi></mml:math></inline-formula> and currently maintained feedback probabilities <inline-formula><mml:math id="inf62"><mml:mi>ψ</mml:mi></mml:math></inline-formula> to a distribution over stimuli, <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In a volatile reversal learning task, each participant in the experiment must resolve two fundamental challenges: (1) decision-making, determining an action to maximize benefit; and (2) learning, figuring out the untold feedback probability via their interaction with the environment. Here, we introduce four families of models that all utilize the same reinforcement learning method for learning feedback probability but differ in how they construct their policies for decision-making. First, the MOS model, the target model proposed in this paper, utilizes a decision-making policy consisting of a mixture of three strategies: EU, MO, and HA. Second, the FLR model, reported as the best model by <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>, selects stimuli with higher values. The stimulus value was estimated by a linear combination of differences in feedback probability, (non-linear) feedback magnitude, and the stimuli’s consistency with habitual behaviors. Third, the Risk-Sensitive (RS) model, adopted from <xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref> and <xref ref-type="bibr" rid="bib5">Browning et al., 2015</xref>, utilizes the EU strategy in decision-making and considers a subjective distortion of the learned feedback probability when calculating the expected value. Finally, the Pearce-Hall (PH) model, equipped with a built-in learning rate adaptation mechanism, utilizes the EU strategy for decision-making.</p><p>Notably, the MOS model, which is the core contribution of this study, posits that behavioral differences across the two participant groups and stable/volatile contexts are due to different weightings of multiple decision strategies. In contrast, the other three models posit that behavioral differences mainly arise via different learning rates between groups and contexts.</p><sec id="s4-2-1"><title>The MOS model</title><p>The key signature of the MOS model is that its policy consists of a mixture of three strategies: EU, MO, and HA. Among many possible variants of the MOS models, this particular three-strategy configuration was chosen as the representative model because it best accounts for human behavioral data (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>The EU strategy postulates that human agents rationally calculate the value of each stimulus and use the softmax rule to select an action. In this case, the value of a stimulus should be its expected utility: <inline-formula><mml:math id="inf64"><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mi>ψ</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The probability of choosing a stimulus <inline-formula><mml:math id="inf65"><mml:mi>s</mml:mi></mml:math></inline-formula> thus follows a softmax function.<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf66"><mml:mi>β</mml:mi></mml:math></inline-formula> is the inverse temperature. For simplicity, we rewrite <xref ref-type="disp-formula" rid="equ2">Equation 1</xref> in the following form:<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Different from the EU strategy, the MO strategy postulates that observers only focus on feedback magnitude <inline-formula><mml:math id="inf67"><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, disregarding feedback probability <inline-formula><mml:math id="inf68"><mml:mi>ψ</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. This is certainly an irrational strategy but more economical in terms of cognitive efforts. Feedback magnitudes are explicitly shown with the stimuli in each trial and readily available for related cognitive computation. But feedback probability, as a latent variable, requires trial-by-trial learning and inference, which is more cognitively demanding. The MO strategy is defined as,<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Unlike EU and MO, the HA strategy depends on neither feedback magnitude <inline-formula><mml:math id="inf69"><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> nor feedback probability <inline-formula><mml:math id="inf70"><mml:mi>ψ</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The HA strategy reflects the tendency to repeat previous frequent choices. This tendency reflects the habit of choosing a stimulus, a phenomenon called perseveration in literature (<xref ref-type="bibr" rid="bib15">Gershman, 2020</xref>; <xref ref-type="bibr" rid="bib34">Wood and Rünger, 2016</xref>). For example, if an agent chooses stimulus 1 more often in past trials, she will form a preference for stimulus 1 in future trials. We constructed it as a Bernoulli distribution over the two stimuli <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The trial-by-trial update rule of <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> will be detailed in <xref ref-type="disp-formula" rid="equ6 equ7">Equation 5-6</xref> below.</p><p>We implemented the hybrid policy of a linear mixture of the three strategies following the methods used in <xref ref-type="bibr" rid="bib9">Daw et al., 2011</xref>,<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf73"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the weighting parameters of each strategy. The three weighting parameters should be summed to 1, i.e., <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. We can thus describe the policy an observer adopted just by examining the weighting parameters. Formulating the hybrid model in this way improves the interpretability of the weighting parameters because all three decision strategies are constructed in a Bernoulli format.</p><p>Next, we solve the challenge of probabilistic learning. Two distributions — the feedback probability and the habit—are learned and updated in a trial-by-trial fashion. We updated the feedback probability in a Rescorla-Wagner format (<xref ref-type="bibr" rid="bib25">Rescorla, 1972</xref>):<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the learning rate for feedback probability. <inline-formula><mml:math id="inf78"><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is an indicator function that returns 1 at the true feedback stimulus and 0 otherwise. To keep consistent with <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>, we also explored the valence-specific learning rate,<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> for </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> for </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is the learning rate for better-than-expected outcomes, and <inline-formula><mml:math id="inf80"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> for worse-than-expected outcomes. It is important to note that <xref ref-type="disp-formula" rid="equ7">Equation 6</xref> was only applied to the reward context, and the definitions of ‘better-than-expected’ and ‘worse-than-expected’ should change accordingly in the aversive context, where we defined <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In a similar manner, the habit component is updated.<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf85"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the learning rate for the habitual strategy. <inline-formula><mml:math id="inf86"><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is also an indicator function that returns 1 for the stimulus chosen at the current trial. Intuitively, the stimulus chosen more often will result in a higher <inline-formula><mml:math id="inf87"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for subsequent trials.</p><p>We developed two variants of the MOS model: a context-free and a context-dependent variant. The context-free MOS6 has a total of six free parameters <inline-formula><mml:math id="inf88"><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>. This variant does not include the design of a value-specific learning rate. The context-dependent variant MOS22 has a total of 22 free parameters. Among them <inline-formula><mml:math id="inf89"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are context-free parameters that were held the same across all contexts. Parameters <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are context-dependent parameters that should be fitted independently to each context.</p><p>We fit the context-dependent parameters to each context following a 2 (reward/aversive) × 2 (stable/volatile) factorial structure (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Specifically, the five context-dependent parameters, the positive learning rate parameter <inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, the negative learning rate parameter <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, and three strategies weights <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were fit separately to each context. The remaining two parameters <inline-formula><mml:math id="inf95"><mml:mo>{</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula> were held constant across all four experimental contexts for each participant. Thus, there were 22 free parameters (5 context-dependent parameters × 4 conditions + 2 context-free parameters) of the MOS model in each participant.</p></sec><sec id="s4-2-2"><title>The FLR model</title><p>The FLR model refers to Model 11 (i.e. the best-fitting model) in <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref>. Here, we describe the FLR model using the same notation system as the published paper, which is slightly different from the notations in the MOS model. The FLR model models the probability of selecting stimulus 1 as follows:<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf96"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the inverse temperature parameters of the value of the stimulus 1 and the HA strategy, respectively. The value of stimulus 1 represents the advantage of <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id="inf99"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>,<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf100"><mml:mi>λ</mml:mi></mml:math></inline-formula> is the weighting parameter balancing the two terms. The first term <inline-formula><mml:math id="inf101"><mml:mi>ψ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> indicates the feedback probability difference between the two options. The second term, <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, indicates the feedback magnitude differences scaled by a non-linear factor <inline-formula><mml:math id="inf103"><mml:mi>r</mml:mi></mml:math></inline-formula>. Intuitively, the value <inline-formula><mml:math id="inf104"><mml:mi>v</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> can be understood as the weighted sum of the feedback probability differences and the feedback magnitude difference.</p><p>During the learning stage, the FLR model learns the feedback probability using the same equations in the MOS model (<xref ref-type="disp-formula" rid="equ6 equ7">Equations 5; 6</xref>). The context-free variant FLR6 has six free parameters <inline-formula><mml:math id="inf106"><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. The context-dependent variant FLR22 considers <inline-formula><mml:math id="inf107"><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> as context-free parameters and <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as context-dependent parameters, resulting in a total of 22 free parameters.</p></sec><sec id="s4-2-3"><title>The RS model</title><p>We adopted the RS model from <xref ref-type="bibr" rid="bib3">Behrens et al., 2007</xref>. The RS model assumes that participants apply the EU strategy but with a subjectively distorted feedback probability <inline-formula><mml:math id="inf109"><mml:mover accent="true"><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>,<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mover><mml:mi>ψ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mover><mml:mi>ψ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover><mml:mi>ψ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf110"><mml:mi>β</mml:mi></mml:math></inline-formula> is the inverse temperature. The distorted probability is calculated by,<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mover><mml:mi>ψ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mover><mml:mi>ψ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mover><mml:mi>ψ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf111"><mml:mi>γ</mml:mi></mml:math></inline-formula> indicates participants’ risk sensitivity. When <inline-formula><mml:math id="inf112"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, a participant has an unbiased risk balance. <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> indicate risk-seeking and risk-aversive tendencies, respectively.</p><p>The RS model learns the feedback probability in the same way as the MOS and FLR models (i.e. <xref ref-type="disp-formula" rid="equ6">Equation 5</xref>). The model did not include the HA strategy. The context-free variant RS3 has a total of three free parameters <inline-formula><mml:math id="inf115"><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. The context-dependent variant RS13 considers <inline-formula><mml:math id="inf116"><mml:mo>{</mml:mo><mml:mi>β</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> as a context-free parameter and <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as context-dependent parameters, resulting in a total of 13 free parameters.</p></sec><sec id="s4-2-4"><title>The PH model</title><p>To explicitly incorporate a learning rate adaptation mechanism, we adopt the PH model from <xref ref-type="bibr" rid="bib21">Pearce and Hall, 1980</xref>. This model proposes an adaptive learning rate, as outlined in <xref ref-type="disp-formula" rid="equ6">Equation 5</xref>.<disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf118"><mml:mi>k</mml:mi></mml:math></inline-formula> is a scale factor of the learning rate. Each trial the learning rate is updated in accordance with the absolute prediction error,<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf119"><mml:mi>η</mml:mi></mml:math></inline-formula> is the step size for the learning rate. We have no knowledge of participants’ learning rate values before the experiment, so we need to also fit the initial learning rate value, <inline-formula><mml:math id="inf120"><mml:msubsup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>. The PH model generates a choice through the EU strategy:<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The context-free variant PH4 has a total of four free parameters <inline-formula><mml:math id="inf121"><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. The context-dependent variant PH17 considers <inline-formula><mml:math id="inf122"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:math></inline-formula> as a context-free parameter and as context-dependent parameters, resulting in a total of 17 free parameters.</p></sec></sec><sec id="s4-3"><title>Model fitting</title><p>Parameters were estimated for each participant via the maximum a posteriori (MAP) method. The objective function to maximize is:<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mi>ξ</mml:mi></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ξ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf123"><mml:mi>ξ</mml:mi></mml:math></inline-formula> means the model-free parameters. <inline-formula><mml:math id="inf124"><mml:mi>M</mml:mi></mml:math></inline-formula> is the model and <inline-formula><mml:math id="inf125"><mml:mi>N</mml:mi></mml:math></inline-formula> refers to the number of trials of the participant’s behavioral data. <inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the presented magnitude, true feedback probability, and participants’ responses recorded in each trial.</p><p>Parameter estimation was performed using the <italic>Broyden-Fletcher-Goldfarb-Shanno (BFGS</italic>) algorithm in the <italic>scipy.optimize</italic> module in Python. This algorithm provides an approximation of the inverse Hessian matrix for the parameter, a critical component that can be employed in Bayesian model selection (<xref ref-type="bibr" rid="bib26">Rigoux et al., 2014</xref>). In order to use the BFGS algorithm, we reparametrized the model, thereby transforming the original fitting problem into an unconstrained optimization problem. We carefully tuned the parameter priors to ensure that they had little impact on the fitting results. For each participant, we ran the optimization with 40 randomly chosen initial parameters to avoid local minima.</p><p>Importantly, to fit the weighting parameters (<inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and ensure they summed to 1, we parameterized the weighting parameters as outputs of a softmax function,<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>and fit the logits <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the weights. All logits were assumed to be normally distributed with a prior <inline-formula><mml:math id="inf131"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0,10</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. In the result section, we used both (<inline-formula><mml:math id="inf132"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and (<inline-formula><mml:math id="inf133"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) to represent participants’ strategy preferences. Some of the statistical analyses were performed only on (<inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) because they are normally distributed.</p></sec><sec id="s4-4"><title>Simulation details</title><sec id="s4-4-1"><title>Simulate to understand the three strategies</title><p>We run simulations to understand the effects of the three strategies on hit rate, hit rate difference, and learning curve. We first used the MOS6 to simulate the learning behaviors of the healthy control group in 100 independent experiments. The parameters were set as  <inline-formula><mml:math id="inf135"><mml:mi>β</mml:mi></mml:math></inline-formula> = 10.803, <inline-formula><mml:math id="inf136"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.423, <inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.473, <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 1.138, <inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = –1.547, <inline-formula><mml:math id="inf140"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.686, where the first three parameters represent the median across both groups and the latter three weighting parameters are the median across healthy controls. Each simulated experiment consists of two runs, one showing a stable context first and then a volatile context, and vice versa in the other run. This approach results in a total of 200 runs for the healthy control group. The task sequences were randomly generated using the same design <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref> used for data collection. Similarly, we repeated all the simulation procedures for the patient group, except that the parameters were set to  <inline-formula><mml:math id="inf141"><mml:mi>β</mml:mi></mml:math></inline-formula> = 10.803, <inline-formula><mml:math id="inf142"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.423, <inline-formula><mml:math id="inf143"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.473, <inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.515, <inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = –0.220, <inline-formula><mml:math id="inf146"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.094. Note that we used identical {<inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>} in both groups and only varied {<inline-formula><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>} as the median across the patient participants. We used <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∣</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf151"><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> to evaluate the task performance associated with each strategy (e.g. <xref ref-type="fig" rid="fig4">Figure 4B–D</xref>). We did not run each strategy completely independently because the HA strategy alone cannot complete the task without learning from decisions previously made by the EU strategy.</p></sec><sec id="s4-4-2"><title>Simulate to explain learning rate adaptation using MOS6</title><p>In one simulated experiment, we sampled the four task sequences from the real data. We simulated 20 experiments with the parameters of  <inline-formula><mml:math id="inf152"><mml:mi>β</mml:mi></mml:math></inline-formula> = 10.803, <inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.423, <inline-formula><mml:math id="inf154"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.473, <inline-formula><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.60, <inline-formula><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.15, <inline-formula><mml:math id="inf157"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.25 to mimic the behavior of the healthy control participants. The first three are the median of the fitted parameters across all participants; the latter three were chosen to approximate the strategy preferences of real healthy control participants (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Similarly, we also simulated 20 experiments for the patient group with the identical values of <inline-formula><mml:math id="inf158"><mml:mi>β</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf159"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf160"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, but different strategy preferences  <inline-formula><mml:math id="inf161"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.15, <inline-formula><mml:math id="inf162"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.60, <inline-formula><mml:math id="inf163"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.25. In other words, the only difference in the parameters of the two groups is the switched <inline-formula><mml:math id="inf164"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We then fitted the FLR22 to the behavioral data generated by the MOS6 and examined the learning rate differences across groups and volatile contexts (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Formal analysis, Supervision, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent was obtained for all participants. Procedures for experiment 1 were approved by and complied with the guidelines of the Oxford Central University Research Ethics Committee (protocol numbers: MSD-IDREC-C2-2012-36 and MSD-IDREC-C2-2012-20). Procedures for experiment 2 were approved by and complied with the guidelines of the University of California- Berkeley Committee for the Protection of Human Subjects (protocol ID 2010-12-2638).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93887-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All behavioral data are public via <ext-link ext-link-type="uri" xlink:href="https://osf.io/8mzuj/">Open Science Framework</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Gagne</surname><given-names>C</given-names></name><name><surname>Zika</surname><given-names>O</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Impaired adaptation of learning to contingency volatility in internalizing psychopathology</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/8mzuj/">8mzuj</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the authors of <xref ref-type="bibr" rid="bib13">Gagne et al., 2020</xref> for sharing their data. This work was supported by the National Key R&amp;D Program of China (2023YFF1204200), the National Natural Science Foundation of China (32100901), the Natural Science Foundation of Shanghai (21ZR1434700), the Research Project of Shanghai Science and Technology Commission (20dz2260300), and the Fundamental Research Funds for the Central Universities (to R.-Y.Z.)</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>AT</given-names></name><name><surname>Ward</surname><given-names>C</given-names></name><name><surname>Mendelson</surname><given-names>M</given-names></name><name><surname>Mock</surname><given-names>J</given-names></name><name><surname>Erbaugh</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Beck depression inventory (BDI)</article-title><source>Archives of General Psychiatry</source><volume>4</volume><fpage>561</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1001/archpsyc.1961.01710120031004</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>SP</given-names></name><name><surname>Vandenberghe</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Convex Optimization</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Jocham</surname><given-names>G</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/nn.3961</pub-id><pub-id pub-id-type="pmid">25730669</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Takahashi</surname><given-names>T</given-names></name><name><surname>Nakagawa</surname><given-names>S</given-names></name><name><surname>Inoue</surname><given-names>T</given-names></name><name><surname>Kusumi</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reinforcement learning in depression: A review of computational research</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>55</volume><fpage>247</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.05.005</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>LA</given-names></name><name><surname>Watson</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Tripartite model of anxiety and depression: psychometric evidence and taxonomic implications</article-title><source>Journal of Abnormal Psychology</source><volume>100</volume><fpage>316</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1037//0021-843x.100.3.316</pub-id><pub-id pub-id-type="pmid">1918611</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cools</surname><given-names>R</given-names></name><name><surname>Clark</surname><given-names>L</given-names></name><name><surname>Owen</surname><given-names>AM</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Defining the neural mechanisms of probabilistic reversal learning using event-related functional magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>4563</fpage><lpage>4567</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-11-04563.2002</pub-id><pub-id pub-id-type="pmid">12040063</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id><pub-id pub-id-type="pmid">21435563</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eysenck</surname><given-names>HJ</given-names></name><name><surname>Eysenck</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="1975">1975</year><source>Eysenck Personality Questionnaire (Junior &amp; Adult)</source><publisher-name>EdITS/Educational and Industrial Testing Service</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Trait somatic anxiety is associated with reduced directed exploration and underestimation of uncertainty</article-title><source>Nature Human Behaviour</source><volume>7</volume><fpage>102</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01455-y</pub-id><pub-id pub-id-type="pmid">36192493</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Policy-analysis</data-title><version designator="swh:1:rev:3f17258959411ac4ca97ef5e7ddd121f8dc4ccf5">swh:1:rev:3f17258959411ac4ca97ef5e7ddd121f8dc4ccf5</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:90d9362ddcc35e9bf21556504d931cf2afc4f939;origin=https://github.com/fangzefunny/policy-analysis;visit=swh:1:snp:4a1dc19442ca428c1e48839112a8ed3e02268509;anchor=swh:1:rev:3f17258959411ac4ca97ef5e7ddd121f8dc4ccf5">https://archive.softwareheritage.org/swh:1:dir:90d9362ddcc35e9bf21556504d931cf2afc4f939;origin=https://github.com/fangzefunny/policy-analysis;visit=swh:1:snp:4a1dc19442ca428c1e48839112a8ed3e02268509;anchor=swh:1:rev:3f17258959411ac4ca97ef5e7ddd121f8dc4ccf5</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagne</surname><given-names>C</given-names></name><name><surname>Zika</surname><given-names>O</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Impaired adaptation of learning to contingency volatility in internalizing psychopathology</article-title><source>eLife</source><volume>9</volume><elocation-id>e61387</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61387</pub-id><pub-id pub-id-type="pmid">33350387</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Horvitz</surname><given-names>EJ</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</article-title><source>Science</source><volume>349</volume><fpage>273</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1126/science.aac6076</pub-id><pub-id pub-id-type="pmid">26185246</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Origin of perseveration in the trade-off between reward and complexity</article-title><source>Cognition</source><volume>204</volume><elocation-id>104394</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104394</pub-id><pub-id pub-id-type="pmid">32679270</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rational use of cognitive resources: levels of analysis between the computational and the algorithmic</article-title><source>Topics in Cognitive Science</source><volume>7</volume><fpage>217</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1111/tops.12142</pub-id><pub-id pub-id-type="pmid">25898807</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname><given-names>RP</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adults with autism overestimate the volatility of the sensory environment</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1293</fpage><lpage>1299</lpage><pub-id pub-id-type="doi">10.1038/nn.4615</pub-id><pub-id pub-id-type="pmid">28758996</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>ML</given-names></name><name><surname>Metzger</surname><given-names>RL</given-names></name><name><surname>Borkovec</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Development and validation of the penn state worry questionnaire</article-title><source>Behaviour Research and Therapy</source><volume>28</volume><fpage>487</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1016/0005-7967(90)90135-6</pub-id><pub-id pub-id-type="pmid">2076086</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Bruckner</surname><given-names>R</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Li</surname><given-names>SC</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Eppinger</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Age differences in learning emerge from an insufficient representation of uncertainty in older adults</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11609</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11609</pub-id><pub-id pub-id-type="pmid">27282467</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>JM</given-names></name><name><surname>Hall</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>A model for pavlovian learning: variations in the effectiveness of conditioned but not of unconditioned stimuli</article-title><source>Psychological Review</source><volume>87</volume><fpage>532</fpage><lpage>552</lpage><pub-id pub-id-type="pmid">7443916</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pike</surname><given-names>AC</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reinforcement learning in patients with mood and anxiety disorders vs control individuals: A systematic review and meta-analysis</article-title><source>JAMA Psychiatry</source><volume>79</volume><fpage>313</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1001/jamapsychiatry.2022.0051</pub-id><pub-id pub-id-type="pmid">35234834</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Powers</surname><given-names>AR</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Corlett</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pavlovian conditioning-induced hallucinations result from overweighting of perceptual priors</article-title><source>Science</source><volume>357</volume><fpage>596</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1126/science.aan3458</pub-id><pub-id pub-id-type="pmid">28798131</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radloff</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>The CES-D scale</article-title><source>Applied Psychological Measurement</source><volume>1</volume><fpage>385</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1177/014662167700100306</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Rescorla</surname><given-names>RA</given-names></name></person-group><source>Current Research and Theory</source><publisher-name>Appleton-Century-Crofts</publisher-name><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoux</surname><given-names>L</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian model selection for group studies - revisited</article-title><source>NeuroImage</source><volume>84</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id><pub-id pub-id-type="pmid">24018303</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating the dimension of a model</article-title><source>The Annals of Statistics</source><volume>6</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>X</given-names></name><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>TMC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The inter-relationships of the neural basis of rumination and inhibitory control: neuroimaging-based meta-analyses</article-title><source>Psychoradiology</source><volume>2</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1093/psyrad/kkac002</pub-id><pub-id pub-id-type="pmid">38665140</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spielberger</surname><given-names>CD</given-names></name><name><surname>Vagg</surname><given-names>PR</given-names></name><name><surname>Jacobs</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1983">1983</year><source>Manual for the State-Trait Anxiety Inventory</source><publisher-name>Consulting Psychologists Press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Von</surname><given-names>J</given-names></name><name><surname>Morgenstern</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1947">1947</year><source>Theory of Games and Economic Behavior</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>D</given-names></name><name><surname>Clark</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1991">1991</year><source><italic>The Mood and Anxiety Symptom Questionnaire (MASQ)</italic></source><publisher-name>University of Iowa</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Geana</surname><given-names>A</given-names></name><name><surname>White</surname><given-names>JM</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Humans use directed and random exploration to solve the explore-exploit dilemma</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>2074</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1037/a0038199</pub-id><pub-id pub-id-type="pmid">25347535</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Rünger</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Psychology of Habit</article-title><source>Annual Review of Psychology</source><volume>67</volume><fpage>289</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033417</pub-id><pub-id pub-id-type="pmid">26361052</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>X</given-names></name><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Yuan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Emotion regulation choice in internet addiction: Less reappraisal, lower frontal alpha asymmetry</article-title><source>Clinical EEG and Neuroscience</source><volume>53</volume><fpage>278</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1177/15500594211056433</pub-id><pub-id pub-id-type="pmid">34894803</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93887.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liljeholm</surname><given-names>Mimi</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of California, Irvine</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This study provides a novel and <bold>valuable</bold> alternative explanation for volatility-induced changes in choice behavior, commonly attributed to learning-rate adaptations. Through rigorous and comprehensive computational modeling of previously published data, the authors provide <bold>convincing</bold> support for the claim that apparent learning-rate adaptations may instead reflect a mixture of decision strategies. Furthermore, they demonstrate that differential weighting of the optimal decision strategy is predicted by psychopathology common to depression and anxiety. This work should be of interest to a wide range of scientists, including psychologists, neuroscientists, computer scientists, and clinicians.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93887.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Previous research shows that humans tend to adjust learning in environments where stimulus-outcome contingencies become more volatile. This learning rate adaptation is impaired in some psychiatric disorders, such as depression and anxiety. In this tudy the authors reanalyze previously published data on a reversal learning task with two volatility levels. Through a new model they provide some evidence for an alternative explanation whereby the learning rate adaptation is driven by different decision-making strategies and not learning deficits. In particular, they propose that adjusting of learning can be explained by deviations from the optimal decision-making strategy (based on maximizing expected utility) due to response stickiness or focus on reward magnitude. Furthermore, a factor related to general psychopathology of individuals with anxiety and depression negatively correlated with the weight on the optimal strategy and response stickiness, while it correlated positively with the magnitude strategy (a strategy that ignores the probability of outcome).</p><p>The main strength of the study is a novel and interesting explanation of an otherwise well-established finding in human reinforcement learning. This proposal is supported by rigorously conducted parameter retrieval and the comparison of the novel model to a wide range of previously published models. The authors explore from many angles, if and why the predictions from the new proposed model are superior to previously applied models.</p><p>My previous concerns were addressed in the revised version of the manuscript. I believe that the article now provides a new perspective on a well-established learning effect and offer a novel set of interesting response models that can be applied to a wide array of decision-making problems.</p><p>I see two limitations of the study not mentioned in the discussion of the manuscript. First, the task features binary inputs and responses, therefore unexpected uncertainty (volatility) is impossible to differentiate from the uncertainty about outcomes, and exploration is inseparable from random choices. Future work could validate these findings in task designs that allow to distinguish these processes. Second, clinical results are based on a small sample of patients and should be interpreted with this in mind.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93887.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper presents a new formulation of a computational model of adaptive learning amid environmental volatility. Using a behavioral paradigm and data set made available by the authors of an earlier publication (Gagne et al., 2020), the new model is found to fit the data well. The model's structure consists of three weighted controllers that influence decisions on the basis of (1) expected utility, (2) potential outcome magnitude, and (3) habit. The model offers an interpretation of psychopathology-related individual differences in decision-making behavior in terms of differences in the relative weighting of the three controllers.</p><p>Strengths:</p><p>The newly proposed &quot;mixture of strategies&quot; (MOS) model is evaluated relative to the model presented in the original paper by Gagne et al., 2020 (here called the &quot;flexible learning rate&quot; or FLR model) and two other models. Appropriate and sophisticated methods are used for developing, parameterizing, fitting, and assessing the MOS model, and the MOS model performs well on multiple goodness-of-fit indices. Parameters of the model show decent recoverability and offer a novel interpretation for psychopathology-related individual differences. Most remarkably, the model seems to be able to account for apparent differences in behavioral learning rates between high-volatility and low-volatility conditions even with no true condition-dependent change in the parameters of its learning/decision processes. This finding calls into question a class of existing models that attribute behavioral adaptation to adaptive learning rates.</p><p>Weaknesses:</p><p>The authors have responded to the weaknesses noted previously.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93887.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Zeming</given-names></name><role specific-use="author">Author</role><aff><institution>Shanghai Jiao Tong University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Meihua</given-names></name><role specific-use="author">Author</role><aff><institution>South China Normal University</institution><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Ting</given-names></name><role specific-use="author">Author</role><aff><institution>University of Electronic Science and Technology of China</institution><addr-line><named-content content-type="city">Chengdu</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yuhang</given-names></name><role specific-use="author">Author</role><aff><institution>University of Macau</institution><addr-line><named-content content-type="city">Macau</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Hanbo</given-names></name><role specific-use="author">Author</role><aff><institution>University of Arizona</institution><addr-line><named-content content-type="city">Arizona</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Quan</surname><given-names>Peng</given-names></name><role specific-use="author">Author</role><aff><institution>Guangdong Medical College</institution><addr-line><named-content content-type="city">Dongguan</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Geng</surname><given-names>Haiyang</given-names></name><role specific-use="author">Author</role><aff><institution>Tianqiao and Chrissy Chen Institute for Translational Research</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name><role specific-use="author">Author</role><aff><institution>Shanghai Jiao Tong University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1:</bold></p><p>Point 1.1</p><p>Summary: This paper describes a reanalysis of data collected by Gagne et al. (2020), who investigated how human choice behaviour differs in response to changes in environmental volatility. Several studies to date have demonstrated that individuals appear to increase their learning rate in response to greater volatility and that this adjustment is reduced amongst individuals with anxiety and depression. The present authors challenge this view and instead describe a novel Mixture of Strategies (MOS) model, that attributes individual differences in choice behaviour to different weightings of three distinct decision-making strategies. They demonstrate that the MOS model provides a superior fit to the data and that the previously observed differences between patients and healthy controls may be explained by patients opting for a less cognitively demanding, but suboptimal, strategy.</p><p>Strengths:</p><p>The authors compare several models (including the original winning model in Gagne et al., 2020) that could feasibly fit the data. These are clearly described and are evaluated using a range of model diagnostics. The proposed MOS model appears to provide a superior fit across several tests.</p><p>The MOS model output is easy to interpret and has good face validity. This allows for the generation of clear, testable, hypotheses, and the authors have suggested several lines of potential research based on this.</p></disp-quote><p>We appreciate the efforts in understanding our manuscript. This is a good summary.</p><disp-quote content-type="editor-comment"><p>Point 1.2</p><p>The authors justify this reanalysis by arguing that learning rate adjustment (which has previously been used to explain choice behaviour on volatility tasks) is likely to be too computationally expensive and therefore unfeasible. It is unclear how to determine how &quot;expensive&quot; learning rate adjustment is, and how this compares to the proposed MOS model (which also includes learning rate parameters), which combines estimates across three distinct decision-making strategies.</p></disp-quote><p>We are sorry for this confusion. Actually, our motivation is that previous models only consider the possibility of learning rate adaptation to different levels of environmental volatility. The drawback of previous computational modeling is that they require a large number of parameters in multi-context experiments. We feel that learning rate adaptation may not be the only mechanisms or at least there may exist alternative explanations. Understanding the true mechanisms is particularly important for rehabilitation purposes especially in our case of anxiety and depression. To clarify, we have removed all claims about the learning rate adaptation is “too complex to understand”.</p><disp-quote content-type="editor-comment"><p>Point 1.3</p><p>As highlighted by the authors, the model is limited in its explanation of previously observed learning differences based on outcome value. It's currently unclear why there would be a change in learning across positive/negative outcome contexts, based on strategy choice alone.</p></disp-quote><p>Thanks for mentioning this limitation. We want to highlight two aspect of work.</p><p>First, we developed the MOS6 model primarily to account for the learning rate differences between stable and volatile contexts, and between healthy controls and patients, not for between positive and negative outcomes. In the other words, our model does not eliminate the possibility of different learning rate in positive and negative outcomes.</p><p>Second, Figure 3A shows that FLR (containing different learning parameters for positive/negative outcomes) even performed worse than MOS6 (setting identical learning rate for positive/negative outcomes). This result question whether learning rate differences between positive/negative outcomes exist in our dataset.</p><p>Action: We now include this limitation in lines 784-793 in discussion:</p><p>“The MOS model is developed to offer context-free interpretations for the learning rate differences observed both between stable and volatile contexts and between healthy individuals and patients. However, we also recognize that the MOS account may not justify other learning rate effects based solely on strategy preferences. One such example is the valence-specific learning rate differences, where learning rates for better-than-expected outcomes are higher than those for worse-than-expected outcomes (Gagne et al., 2020). When fitted to the behavioral data, the context-dependent MOS22 model does not reveal valence-specific learning rates (Supplemental Note 4). Moreover, the valence-specific effect was not replicated in the FLR22 model when fitted to the synthesized data of MOS6.”</p><disp-quote content-type="editor-comment"><p>Point 1.4</p><p>Overall the methods are clearly presented and easy to follow, but lack clarity regarding some key features of the reversal learning task.</p><p>Throughout the method the stimuli are referred to as &quot;right&quot; and &quot;left&quot;. It's not uncommon in reversal learning tasks for the stimuli to change sides on a trial-by-trial basis or counterbalanced across stable/volatile blocks and participants. It is not stated in the methods whether the shapes were indeed kept on the same side throughout. If this is the case, please state it. If it was not (and the shapes did change sides throughout the task) this may have important implications for the interpretation of the results. In particular, the weighting of the habitual strategy (within the Mixture of Strategies model) could be very noisy, as participants could potentially have been habitual in choosing the same side (i.e., performing the same motor movement), or in choosing the same shape. Does the MOS model account for this?</p></disp-quote><p>We are sorry for the confusion. Yes, two shapes indeed changed sides throughout the task. We replaced the “left” and “right” with “stimulus 1” and “stimulus 2”. We also acknowledge the possibility that participants may develop a habitual preference for a particular side, rather than a shape. Due to the counterbalance design, habitual on side will introduce a random selection noise in choices, which should be captured by the MOS model through the inverse temperature parameter.</p><disp-quote content-type="editor-comment"><p>Point 1.5</p><p>Line 164: &quot;Participants received points or money in the reward condition and an electric shock in the punishment condition.&quot; What determined whether participants received points or money, and did this differ across participants?</p></disp-quote><p>Thanks! We have the design clarified in lines 187-188:</p><p>“Each participant was instructed to complete two blocks of the volatile reversal learning task, one in the reward context and the other in the aversive context”,</p><p>and in lines:</p><p>“A total of 79 participants completed tasks in both feedback contexts. Four participants only completed the task in the reward context, while three participants only completed the aversive task.”</p><disp-quote content-type="editor-comment"><p>Point 1.6</p><p>Line 167: &quot;The participant received feedback only after choosing the correct stimulus and received nothing else&quot; Is this correct? In Figure 1a it appears the participant receives feedback irrespective of the stimulus they chose, by either being shown the amount 1-99 they are being rewarded/shocked, or 0. Additionally, what does the &quot;correct stimulus&quot; refer to across the two feedback conditions? It seems intuitive that in the reward version, the correct answer would be the rewarding stimulus - in the loss version is the &quot;correct&quot; answer the one where they are not receiving a shock?</p></disp-quote><p>Thanks for raising this issue. We removed the term “correct stimulus” and revised the lines 162-166 accordingly:</p><p>“Only one of the two stimuli was associated with actual feedback (0 for the other one). The feedback magnitude, ranged between 1-99, is sampled uniformly and independently for each shape from trial to trial. Actual feedback was delivered only if the stimulus associated with feedback was chosen; otherwise, a number “0” was displayed on the screen, signifying that the chosen stimulus returns nothing.”</p><disp-quote content-type="editor-comment"><p>Point 1.7</p><p>Line 176: &quot;The whole experiment included two runs each for the two feedback conditions.&quot; Does this mean participants completed the stable and volatile blocks twice, for each feedback condition? (i.e., 8 blocks total, 4 per feedback condition).</p></disp-quote><p>Thanks! We have removed the term “block”, and now we refer to it as “context”. In particular, we removed phrases like “stable block” and “volatile block” and used “context” instead.</p><p>Action: See lines 187-189 for the revised version.</p><p>“Each participant was instructed to complete two runs of the volatile reversal learning task, one in the reward context and the other in the aversive context. Each run consisted of 180 trials, with 90 trials in the stable context and 90 in the volatile context (Fig. 1B).”</p><disp-quote content-type="editor-comment"><p>Point 1.8</p><p>In the expected utility (EU) strategy of the Mixture or Strategies model, the expected value of the stimulus on each trial is produced by multiplying the magnitude and probability of reward/shock. In Gagne et al.'s original paper, they found that an additive mixture of these components better-captured participant choice behaviour - why did the authors not opt for the same strategy here?</p></disp-quote><p>Thanks for asking this. Their strategy basic means the mixture of PF+MO+HA, where PF stands for the feedback probability (e.g., 0.3 or 0.7) without multiplying feedback magnitude. However, ours are EU+MO+HA, where EU stands for feedback probability x feedback magnitude. We did compare these two strategies and the model using their strategy performed much worse than ours (see the red box below).</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Thorough model comparison.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-sa3-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Point 1.9</p><p>How did the authors account for individuals with poor/inattentive responding, my concern is that the habitual strategy may be capturing participants who did not adhere to the task (or is this impossible to differentiate?).</p></disp-quote><p>The current MOS6 model distinguishes between the HA strategy and the inattentive response. Due to the counterbalance design, the HA strategy requires participants to actively track the stimuli on the screen. In contrast, the inattentive responding, like the same motor movement mentioned in Point 1.4, should exhibit random selection in their behavioral data, which should be account by the inverse temperature parameter.</p><disp-quote content-type="editor-comment"><p>Point 1.10</p><p>The authors provide a clear rationale for, and description of, each of the computational models used to capture participant choice behaviour.</p><p>• Did the authors compare different combinations of strategies within the MOS model (e.g., only including one or two strategies at a time, and comparing fit?) I think more explanation is needed as to why the authors opted for those three specific strategies.</p></disp-quote><p>We appreciate this great advice. Following your advice, we conducted a thorough model comparisons. Please refer to Figure R1 above. The detailed text descriptions of all the models in Figure R1 are included in Supplemental Note 1.</p><disp-quote content-type="editor-comment"><p>Point 1.11</p><p>Please report the mean and variability of each of the strategy weights, per group.</p></disp-quote><p>Thanks. We updated the mean of variability of the strategies in lines 490-503:</p><p>“We first focused on the fitted parameters of the MOS6 model. We compared the weight parameters (, ,) across groups and conducted statistical tests on their logits (, ,). The patient group showed a ~37% preference towards the EU strategy, which is significantly weaker than the ~50% preference in healthy controls (healthy controls’ : M = 0.991, SD = 1.416; patients’ : M = 0.196, SD = 1.736; t(54.948) = 2.162, p = 0.035, Cohen’s d = 0.509; Fig. 4A). Meanwhile, the patients exhibited a weaker preference (~27%) for the HA strategy compared to healthy controls (~36%) (healthy controls’ : M = 0.657, SD = 1.313; patients’ : M = -0.162, SD = 1.561; t(56.311) = 2.455, p = 0.017, Cohen’s d = 0.574), but a stronger preference for the MO strategy (36% vs. 14%; healthy controls’ : M = -1.647, SD = 1.930; patients’ : M = -0.034, SD = 2.091; t(63.746) = -3.510, p = 0.001, Cohen’s d = 0.801). Most importantly, we also examined the learning rate parameter in the MOS6 but found no group differences (t(68.692) = 0.690, p = 0.493, Cohen’s d = 0.151). These results strongly suggest that the differences in decision strategy preferences can account for the learning behaviors in the two groups without necessitating any differences in learning rate per se.”</p><disp-quote content-type="editor-comment"><p>Point 1.12</p><p>The authors compare the strategy weights of patients and controls and conclude that patients favour more simpler strategies (see Line 417), based on the fact that they had higher weights for the MO, and lower on the EU.</p><p>(1) However, the finding that control participants were more likely to use the habitual strategy was largely ignored. Within the control group, were the participants significantly more likely to opt for the EU strategy, over the HA? (2) Further, on line 467 the authors state &quot;Additionally, there was a significant correlation between symptom severity and the preference for the HA strategy (Pearson's r = -0.285, p = 0.007).&quot; Apologies if I'm mistaken, but does this negative correlation not mean that the greater the symptoms, the less likely they were to use the habitual strategy?</p><p>I think more nuance is needed in the interpretation of these results, particularly in the discussion.</p></disp-quote><p>Thanks. The healthy participants seemed more likely to opt for the EU strategy, although this difference did not reach significance (paired-t(53) = 1.258, p = 0.214, Cohen’s d = 0.242). We systematically explore the role of HA. Compared to the MO, the HA saves cognitive resources but yields a significantly higher hit rate (Fig. 4A). Therefore, a preference for the HA over the MO strategy may reflect a more sophisticated balance between reward and complexity within an agent: when healthier subjects run out of cognitive resources for the EU strategy, they will cleverly resort to the HA strategy, adopting a simpler strategy but still achieving a certain level of hit rate. This explains the negative symptom-HA correlation. As clever as the HA strategy is, it is not surprising that the health control participants opt more for the HA during decision-making.</p><p>However, we are cautious to draw strong conclusion on (1) non-significant difference between EU and HA within health controls and (2) the negative symptom-HA correlation. The reason is that the MOS22, the context-dependent variant, (1) exhibited a significant higher preference for EU over HA (paired-t(53) = 4.070, p &lt; 0.001, Cohen’s d = 0.825) and (2) did not replicate this negative correlation (Supplemental Information Figure S3).</p><p>Action: Simulation analysis on the effects of HA was introduced in lines 556-595 and Figure 4. We discussed the effects of HA in lines 721-733:</p><p>“Although many observed behavioral differences can be explained by a shift in preference from the EU to the MO strategy among patients, we also explore the potential effects of the HA strategy. Compared to the MO, the HA strategy also saves cognitive resources but yields a significantly higher hit rate (Fig. 4A). Therefore, a preference for the HA over the MO strategy may reflect a more sophisticated balance between reward and complexity within an agent (Gershman, 2020): when healthier participants exhaust their cognitive resources for the EU strategy, they may cleverly resort to the HA strategy, adopting a simpler strategy but still achieving a certain level of hit rate. This explains the stronger preference for the HA strategy in the HC group (Fig. 3A) and the negative correlation between HA preferences and symptom severity (Fig. 5). Apart from shedding light on the cognitive impairments of patients, the inclusion of the HA strategy significantly enhances the model’s fit to human behavior (see examples in Daw et al. (2011); Gershman (2020); and also Supplemental Note 1 and Supplemental Figure S3).”</p><disp-quote content-type="editor-comment"><p>Point 1.13</p><p>Line 513: &quot;their preference for the slowest decision strategy&quot; - why is the MO considered the slowest strategy? Is it not the least cognitively demanding, and therefore, the quickest?</p></disp-quote><p>Sorry for the confusion. In Fig. 5C, we conducted simulations to estimate the learning speed for each strategy. As shown below, the MO strategy exhibits a flat learning curve. Our claim on the learning speed was based solely on simulation outcomes without referring to cognitive demands. Note that our analysis did not aim to compare the cognitive demands of the MO and HA strategies directly.</p><p>Action: We explain the learning speed of the three strategies in lines 571-581.</p><disp-quote content-type="editor-comment"><p>Point 1.14</p><p>The authors argue that participants chose suboptimal strategies, but do not actually report task performance. How does strategy choice relate to the performance on the task (in terms of number of rewards/shocks)? Did healthy controls actually perform any better than the patient group?</p></disp-quote><p>Thanks for the suggestion. The answers are: (1) EU is the most rewarding &gt; the HA &gt; the MO (Fig. 5A), and (2) yes healthy controls did actually perform better than patients in terms of hit rate (Fig. 2).</p><p>Action: We included additional sections on above analyses in lines 561-570 and lines 397-401.</p><disp-quote content-type="editor-comment"><p>Point 1.15</p><p>The authors speculate that Gagne et al. (2020) did not study the relationship between the decision process and anxiety and depression, because it was too complex to analyse. It's unclear why the FLR model would be too complex to analyse. My understanding is that the focus of Gagne's paper was on learning rate (rather than noise or risk preference) due to this being the main previous finding.</p></disp-quote><p>Thanks! Yes, our previous arguments are vague and confusing. We have removed all this kind of arguments.</p><disp-quote content-type="editor-comment"><p>Point 1.16</p><p>Minor Comments:</p><p>• Line 392: Modeling fitting &gt; Model fitting</p><p>• Line 580 reads &quot;The MO and HA are simpler heuristic strategies that are cognitively demanding.&quot;</p><p>- should this read as less cognitively demanding?</p><p>• Line 517: health &gt; healthy</p><p>• Line 816: Desnity &gt; density</p></disp-quote><p>Sorry for the typo! They have all been fixed.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>Point 2.1</p><p>Summary: Previous research shows that humans tend to adjust learning in environments where stimulus-outcome contingencies become more volatile. This learning rate adaptation is impaired in some psychiatric disorders, such as depression and anxiety. In this study, the authors reanalyze previously published data on a reversal-learning task with two volatility levels. Through a new model, they provide some evidence for an alternative explanation whereby the learning rate adaptation is driven by different decision-making strategies and not learning deficits. In particular, they propose that adjusting learning can be explained by deviations from the optimal decision-making strategy (based on maximizing expected utility) due to response stickiness or focus on reward magnitude. Furthermore, a factor related to the general psychopathology of individuals with anxiety and depression negatively correlated with the weight on the optimal strategy and response stickiness, while it correlated positively with the magnitude strategy (a strategy that ignores the probability of outcome).</p></disp-quote><p>Thanks for evaluating our paper. This is a good summary.</p><disp-quote content-type="editor-comment"><p>Point 2.2</p><p>My main concern is that the winning model (MOS6) does not have an error term (inverse temperature parameter beta is fixed to 8.804).</p><p>(1) It is not clear why the beta is not estimated and how were the values presented here chosen. It is reported as being an average value but it is not clear from which parameter estimation. Furthermore, with an average value for participants that would have lower values of inverse temperature (more stochastic behaviour) the model is likely overfitting.</p><p>(2) In the absence of a noise parameter, the model will have to classify behaviour that is not explained by the optimal strategy (where participants simply did not pay attention or were not motivated) as being due to one of the other two strategies.</p></disp-quote><p>We apologize for any confusion caused by our writing. We did set the inverse temperature as a free parameter and quantitatively estimate it during the model fitting and comparison. We also created a table to show the free parameters for each models. In the previous manuscript, we did mention “temperature parameter beta is fixed to 8.804”, but only for the model simulation part, which is conducted to interpret some model behaviors.</p><p>We agree with the concern that using the averaged value over the inverse temperature could lead to overfitting to more stochastic behaviors. To mitigate this issue, we now used the median as a more representative value for the population during simulation. Nonetheless, this change does not affect our conclusion (see simulation results in Figures 4&amp;6).</p><p>Action: We now use the term “free parameter” to emphasize that the inverse temperature was fitted rather than fixed. We also create a new table “Table 1” in line 458 to show all the free parameters within a model. We also update the simulation details in lines 363-391 for more clarifications.</p><disp-quote content-type="editor-comment"><p>Point 2.3</p><p>(3) A model comparison among models with inverse temperature and variable subsets of the three strategies (EU + MO, EU + HA) would be interesting to see. Similarly, comparison of the MOS6 model to other models where the inverse temperature parameter is fixed to 8.804.</p><p>This is an important limitation because the same simulation as with the MOS model in Figure 3b can be achieved by a more parsimonious (but less interesting) manipulation of the inverse temperature parameter.</p></disp-quote><p>Thanks, we added a comparison between the MOS6 and the two lesion models (EU + MO, EU + HA). Please refer to the figure below and Point 1.8.</p><p>We also realize that the MO strategy could exhibit averaged learning curves similar to random selection. To confirm that patients' slower learning rates are due to a preference for the MO strategy, we compared the MOS6 model with a variant (see the red box below) in which the MO strategy is replaced by Random (RD) selection that assigns a 0.5 probability to both choices. This comparison showed that the original MOS6 model with the MO strategy better fits human data.</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-sa3-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Point 2.4</p><p>Furthermore, the claim that the EU represents an optimal strategy is a bit overstated. The EU strategy is the only one of the three that assumes participants learn about the stimulus-outcomes contingencies. Higher EU strategy utilisation will include participants that are more optimal (in maximum utility maximisation terms), but also those that just learned better and completely ignored the reward magnitude.</p></disp-quote><p>Thank you for your feedback. We have now revised the paper to remove all statement about “EU strategy is the optimal” and replaced by “EU strategy is rewarding but complex”. We agree that both the EU strategy and the strategy only focusing on feedback probability (i.e., ignoring the reward magnitude, refer to as the PF strategy) are rewarding but complex beyond two simple heuristics. We also included the later strategy in our model comparisons (see the next section Point 2.5).</p><disp-quote content-type="editor-comment"><p>Point 2.5</p><p>The mixture strategies model is an interesting proposal, but seems to be a very convoluted way to ask: to what degree are decisions of subjects affected by reward, what they've learned, and response stickiness? It seems to me that the same set of questions could be addressed with a simpler model that would define choice decisions through a softmax with a linear combination of the difference in rewards, the difference in probabilities, and a stickiness parameter.</p></disp-quote><p>Thanks for suggesting this model. We did include the proposed linear combination models (see “linear comb.” in the red box below) and found that it performed significantly worse than the MOS6.</p><p>Action: We justified our model selection criterion in the Supplemental Note 1.</p><fig id="sa3fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93887-sa3-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Point 2.6</p><p>Learning rate adaptation was also shown with tasks where decision-making strategies play a less important role, such as the Predictive Inference task (see for instance Nassar et al, 2010). When discussing the merit of the findings of this study on learning rate adaptation across volatility blocks, this work would be essential to mention.</p></disp-quote><p>Thanks for mentioning this great experimental paradigm, which provides an ideal solution for disassociating the probability learning and decision process. We have discussed about this paradigm as well as the associated papers in discussion lines 749-751, 763-765, and 796-801.</p><disp-quote content-type="editor-comment"><p>Point 2.7</p><p>Minor mistakes that I've noticed:</p><p>Equation 6: The learning rate for response stickiness is sometimes defined as alpha_AH or alpha_pi.</p><p>Supplementary material (SM) Contents are lacking in Note1. SM talks about model MOS18, but it is not defined in the text (I am assuming it is MOS22 that should be talked about here).</p></disp-quote><p>Thanks! Fixed.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Point 3.1</p><p>Summary: This paper presents a new formulation of a computational model of adaptive learning amid environmental volatility. Using a behavioral paradigm and data set made available by the authors of an earlier publication (Gagne et al., 2020), the new model is found to fit the data well. The model's structure consists of three weighted controllers that influence decisions on the basis of (1) expected utility, (2) potential outcome magnitude, and (3) habit. The model offers an interpretation of psychopathology-related individual differences in decision-making behavior in terms of differences in the relative weighting of the three controllers.</p><p>Strengths: The newly proposed &quot;mixture of strategies&quot; (MOS) model is evaluated relative to the model presented in the original paper by Gagne et al., 2020 (here called the &quot;flexible learning rate&quot; or FLR model) and two other models. Appropriate and sophisticated methods are used for developing, parameterizing, fitting, and assessing the MOS model, and the MOS model performs well on multiple goodness-of-fit indices. The parameters of the model show decent recoverability and offer a novel interpretation for psychopathology-related individual differences. Most remarkably, the model seems to be able to account for apparent differences in behavioral learning rates between high-volatility and low-volatility conditions even with no true condition-dependent change in the parameters of its learning/decision processes. This finding calls into question a class of existing models that attribute behavioral adaptation to adaptive learning rates.</p></disp-quote><p>Thanks for evaluating our paper. This is a good summary.</p><disp-quote content-type="editor-comment"><p>Point 3.2</p><p>(1) Some aspects of the paper, especially in the methods section, lacked clarity or seemed to assume context that had not been presented. I found it necessary to set the paper down and read Gagne et al., 2020 in order to understand it properly.</p><p>(3) Clarification-related suggestions for the methods section:</p><p>- Explain earlier that there are 4 contexts (reward/shock crossed with high/low volatility). Lines 252-307 contain a number of references to parameters being fit separately per context, but &quot;context&quot; was previously used only to refer to the two volatility levels.</p></disp-quote><p>Action: We have placed the explanation as well as the table about the 4 contexts (stable-reward/stable-aversive/volatile-reward/volatile-aversive) earlier in the section that introduces the experiment paradigm (lines 177-186):</p><p>“Participants was supposed to complete this learning and decision-making task in four experimental contexts (Fig. 1A), two feedback contexts (reward or aversive) two volatility contexts (stable or volatile). Participants received points in the reward context and an electric shock in the aversive context. The reward points in the reward context were converted into a monetary bonus by the end of the task, ranging from £0 to £10. In the stable context, the dominant stimulus (i.e., a certain stimulus induces the feedback with a higher probability) provided a feedback with a fixed probability of 0.75, while the other one yielded a feedback with a probability of 0.25. In the volatile context, the dominant stimulus’s feedback probability was 0.8, but the dominant stimulus switched between the two every 20 trials. Hence, this design required participants to actively learn and infer the changing stimulus-feedback contingency in the volatile context.”</p><disp-quote content-type="editor-comment"><p>- It would be helpful to provide an initial outline of the four models that will be described since the FLR, RS, and PH models were not foreshadowed in the introduction. For the FLR model in particular, it would be helpful to give a narrative overview of the components of the model before presenting the notation.</p></disp-quote><p>Action: We now include an overview paragraph in the section of computation model to outline the four models as well as the hypotheses constituted in the model (lines 202-220).</p><disp-quote content-type="editor-comment"><p>- The subsection on line 343, describing the simulations, lacks context. There are references to three effects being simulated (and to &quot;the remaining two effects&quot;) but these are unclear because there's no statement in this section of what the three effects are.</p><p>- Lines 352-353 give group-specific weighting parameters used for the stimulations of the HC and PAT groups in Figure 4B. A third, non-group-specific set of weighting parameters is given above on lines 348-349. What were those used for?</p><p>- Line 352 seems to say Figure 4A is plotting a simulation, but the figure caption seems to say it is plotting empirical data.</p></disp-quote><p>These paragraphs has been rewritten and the abovementioned issues have been clarified. See lines 363-392.</p><disp-quote content-type="editor-comment"><p>Point 3.2</p><p>(2) There is little examination of why the MOS model does so well in terms of model fit indices. What features of the data is it doing a better job of capturing? One thing that makes this puzzling is that the MOS and FLR models seem to have most of the same qualitative components: the FLR model has parameters for additive weighting of magnitude relative to probability (akin to the MOS model's magnitude-only strategy weight) and for an autocorrelative choice kernel (akin to the MOS model's habit strategy weight). So it's not self-evident where the MOS model's advantage is coming from.</p></disp-quote><p>An intuitive understanding of the FLR model is that it estimates the stimuli value through a linear combination of probability feedback (PF, <inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> ) and (non-linear) magnitude <inline-formula><mml:math id="sa3m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> . See equation:<disp-formula id="sa3equ1"><mml:math id="sa3m3"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sign</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Also, the FLR model include the mechanisms of HA as:<disp-formula id="sa3equ2"><mml:math id="sa3m4"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>In other words, FLR model considers the mechanisms about the probability of feedback (PF)+MO+HA (see Eq. XX in the original study), but our MOS considers the mechanisms of EU+MO+HA. The key qualitative difference lies between FLR and MOS is the usage of the expected utility formula (EU) instead the probability of feedback (PF). The advantage of our MOS model has been fully evidenced by our model comparisons, indicating that human participants multiply probability and magnitude rather than only considering probability. The EU strategy has also been suggested by a large pile of literature (Gershman et al., 2015; Von Neumann &amp; Morgenstern, 1947).</p><p>Making decisions based on the multiplication of feedback probability and magnitude can often yield very different results compared to decisions based on a linear combination of the two, especially when the two magnitudes have a small absolute difference but a large ratio. Let’s consider two cases:</p><p>(1) Stimulus 1: <inline-formula><mml:math id="sa3m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vs. Stimulus 2: <inline-formula><mml:math id="sa3m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>(2) Stimulus 1: <inline-formula><mml:math id="sa3m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vs. Stimulus 2:<inline-formula><mml:math id="sa3m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>The EU strategy may opt for stimulus 2 in both cases, since stimulus 2 always has a larger expected value. However, it is very likely for the PF+MO to choose stimulus 1 in the first case. For example, when <inline-formula><mml:math id="sa3m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.425</mml:mn><mml:mo>&gt;</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.325</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> . If we want the PF+MO to also choose stimulus to align with the EU strategy, we need to increase the weight on magnitude <inline-formula><mml:math id="sa3m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> . Note that in this example we divided the magnitude value by 100 to ensure that probability and magnitude are on the same scale to help illustration.</p><p>In the dataset reported by Gagne, 2020, the described scenario seems to occur more often in the aversive context than in the reward context. To accurately capture human behaviors, FLR22 model requires a significantly larger weight for magnitude in the aversive context <inline-formula><mml:math id="sa3m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> than in the reward context <inline-formula><mml:math id="sa3m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> . Interestingly, when the weights for magnitude in different contexts are forced to be equal, the model (FLR6) fails, exhibiting an almost chance-level performance throughout learning (Fig. 3E, G). In contrast, the MOS6 model, and even the RS3 model, exhibit good performance using one identical set of parameters across contexts. Both MOS6 and RS3 include the EU strategy during decision-making. These findings suggest humans make decisions using the EU strategy rather than PF+MO.</p><p>The focus of our paper is to present that a good-enough model can interpret the same dataset in a completely different perspective, not necessarily to explore improvements for the FLR model.</p><disp-quote content-type="editor-comment"><p>Point 3.3</p><p>One of the paper's potentially most noteworthy findings (Figure 5) is that when the FLR model is fit to synthetic data generated by the expected utility (EU) controller with a fixed learning rate, it recovers a spurious difference in learning rate between the volatile and stable environments. Although this is potentially a significant finding, its interpretation seems uncertain for several reasons:</p><p>- According to the relevant methods text, the result is based on a simulation of only 5 task blocks for each strategy. It would be better to repeat the simulation and recovery multiple times so that a confidence interval or error bar can be estimated and added to the figure.</p><p>- It makes sense that learning rates recovered for the magnitude-oriented (MO) strategy are near zero, since behavior simulated by that strategy would have no reason to show any evidence of learning. But this makes it perplexing why the MO learning rate in the volatile condition is slightly positive and slightly greater than in the stable condition.</p><p>- The pure-EU and pure-MO strategies are interpreted as being analogous to the healthy control group and the patient group, respectively. However, the actual difference in estimated EU/MO weighting between the two participant groups was much more moderate. It's unclear whether the same result would be obtained for a more empirically plausible difference in EU/MO weighting.</p><p>- The fits of the FLR model to the simulated data &quot;controlled all parameters except for the learning rate parameters across the two strategies&quot; (line 522). If this means that no parameters except learning rate were allowed to differ between the fits to the pure-EU and pure-MO synthetic data sets, the models would have been prevented from fitting the difference in terms of the relative weighting of probability and magnitude, which better corresponds to the true difference between the two strategies. This could have interfered with the estimation of other parameters, such as learning rate.</p><p>- If, after addressing all of the above, the FLR model really does recover a spurious difference in learning rate between stable and volatile blocks, it would be worth more examination of why this is happening. For example, is it because there are more opportunities to observe learning in those blocks?</p><p>I would recommend performing a version of the Figure 5 simulations using two sets of MOS-model parameters that are identical except that they use healthy-control-like and patient-like values of the EU and MO weights (similar to the parameters described on lines 346-353, though perhaps with the habit controller weight equated). Then fit the simulated data with the FLR model, with learning rate and other parameters free to differ between groups. The result would be informative as to (1) whether the FLR model still misidentifies between-group strategy differences as learning rate differences, and (2) whether the FLR model still identifies spurious learning rate differences between stable and volatile conditions in the control-like group, which become attenuated in the patient-like group.</p></disp-quote><p>Many thanks for this great advice. Following your suggestions, we now conduct simulations using the median of the fitted parameters. The representations for healthy controls and patients have identical parameters, except for the three preference parameters; moreover, the habit weights are not controlled to be equal. 20 simulations for each representative, each comprising 4 task sequences sampled from the behavioral data. In this case, we could create error bars and perform statistical tests. We found that the differences in learning rates between stable and volatile conditions, as well as the learning rate adaptation differences between healthy controls and patients, still persisted.</p><p>Combined with the discussion in Point 3.2, we justify why a mixture-of-strategy can account for learning rate adaptation as follow. Due to (unknown) differences in task sequences, the MOS6 model exhibits more MO-like behaviors due to the usage of the EU strategy. To capture this behavior pattern, the FLR22 model has to increase its weighting parameter 1-λ for magnitude, which could ultimately drive the FLR22 to adjust the fitted learning rate parameters, exhibiting a learning rate adaptation effect. Our simulations suggest that estimating learning rate just by model fitting may not be the only way to interpret the data.</p><p>Action: We included the simulation details in the method section (lines 381-lines 391)</p><p>“In one simulated experiment, we sampled the four task sequences from the real data. We simulated 20 experiments with the parameters of <inline-formula><mml:math id="sa3m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>10.803</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.423</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.473</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">U</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.60</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">O</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to mimic the behavior of the healthy control participants. The first three are the median of the fitted parameters across all participants; the latter three were chosen to approximate the strategy preferences of real health control participants (Figure 4A). Similarly, we also simulated 20 experiments for the patient group with the identical values of <inline-formula><mml:math id="sa3m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="sa3m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , but different strategy preferences <inline-formula><mml:math id="sa3m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">U</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">O</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.60</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> . In other words, the only difference in the parameters of the two groups is the switched <inline-formula><mml:math id="sa3m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa3m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">O</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> . We then fitted the FLR22 to the behavioral data generated by the MOS6 and examined the learning rate differences across groups and volatile contexts (Fig. 6). ”</p><disp-quote content-type="editor-comment"><p>Point 3.4</p><p>Figure 4C shows that the habit-only strategy is able to learn and adapt to changing contingencies, and some of the interpretive discussion emphasizes this. (For instance, line 651 says the habit strategy brings more rewards than the MO strategy.) However, the habit strategy doesn't seem to have any mechanism for learning from outcome feedback. It seems unlikely it would perform better than chance if it were the sole driver of behavior. Is it succeeding in this example because it is learning from previous decisions made by the EU strategy, or perhaps from decisions in the empirical data?</p></disp-quote><p>Yes, the intuition is that the HA strategy seems to show no learning mechanism. But in reality, it yields a higher hit rate than MO by simply learning from previous decisions made by the EU strategy. We run simulations to confirm this (Figure 4B).</p><disp-quote content-type="editor-comment"><p>Point 3.5</p><p>For the model recovery analysis (line 567), the stated purpose is to rule out the possibility that the MOS model always wins (line 552), but the only result presented is one in which the MOS model wins. To assess whether the MOS and FLR models can be differentiated, it seems necessary also to show model recovery results for synthetic data generated by the FLR model.</p></disp-quote><p>Sure, we conducted a model recovery analysis that include all models, and it demonstrates that MOS and FLR can be fully differentiated. The results of the new model recovery analysis were shown in Fig. 7.</p><disp-quote content-type="editor-comment"><p>Point 3.6</p><p>To the best of my understanding, the MOS model seems to implement valence-specific learning rates in a qualitatively different way from how they were implemented in Gagne et al., 2020, and other previous literature. Line 246 says there were separate learning rates for upward and downward updates to the outcome probability. That's different from using two learning rates for &quot;better&quot;- and &quot;worse&quot;-than-expected outcomes, which will depend on both the direction of the update and the valence of the outcome (reward or shock). Might this relate to why no evidence for valence-specific learning rates was found even though the original authors found such evidence in the same data set?</p></disp-quote><p>Thanks. Following the suggestion, we have corrected our implementation of valence-specific learning rate in all models (see lines 261-268).</p><p>“To keep consistent with Gagne et al., (2020), we also explored the valence-specific learning rate,<disp-formula id="sa3equ3"><mml:math id="sa3m19"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> for </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> for </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="sa3m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the learning rate for better-than-expected outcome, and <inline-formula><mml:math id="sa3m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for worse-than-expected outcome. It is important to note that Eq. 6 was only applied to the reward context, and the definitions of “better-than-expected” and “worse-than-expected” should change accordingly in the aversive context, where we defined <inline-formula><mml:math id="sa3m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="sa3m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa3m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="sa3m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> .</p><p>No main effect of valence on learning rate was found (see Supplemental Information Note 3)</p><disp-quote content-type="editor-comment"><p>Point 3.7</p><p>The discussion (line 649) foregrounds the finding of greater &quot;magnitude-only&quot; weights with greater &quot;general factor&quot; psychopathology scores, concluding it reflects a shift toward simplifying heuristics. However, the picture might not be so straightforward because &quot;habit&quot; weights, which also reflect a simplifying heuristic, correlated negatively with the psychopathology scores.</p></disp-quote><p>Thanks. In contrast the detrimental effects of “MO”, “habit” is actually beneficial for the task. Please refer to Point 1.12.</p><disp-quote content-type="editor-comment"><p>Point 3.8</p><p>The discussion section contains some pejorative-sounding comments about Gagne et al. 2020 that lack clear justification. Line 611 says that the study &quot;did not attempt to connect the decision process to anxiety and depression traits.&quot; Given that linking model-derived learning rate estimates to psychopathology scores was a major topic of the study, this broad statement seems incorrect. If the intent is to describe a more specific step that was not undertaken in that paper, please clarify. Likewise, I don't understand the justification for the statement on line 615 that the model from that paper &quot;is not understandable&quot; - please use more precise and neutral language to describe the model's perceived shortcomings.</p></disp-quote><p>Sorry for the confusion. We have removed all abovementioned pejorative-sounding comments.</p><disp-quote content-type="editor-comment"><p>Point 3.9</p><p>4. Minor suggestions:</p><p>- Line 114 says people with psychiatric illness &quot;are known to have shrunk cognitive resources&quot; - this phrasing comes across as somewhat loaded.</p></disp-quote><p>Thanks. We have removed this argument.</p><disp-quote content-type="editor-comment"><p>- Line 225, I don't think the reference to &quot;hot hand bias&quot; is correct. I understand hot hand bias to mean overestimating the probability of success after past successes. That's not the same thing as habitual repetition of previous responses, which is what's being discussed here.</p></disp-quote><p>Response: Thanks for mentioning this. We have removed all discussions about “hot hand bias”.</p><disp-quote content-type="editor-comment"><p>- There may be some notational inconsistency if alpha_pi on line 248 and alpha_HA on line 253 are referring to the same thing.</p></disp-quote><p>Thanks! Fixed!</p><disp-quote content-type="editor-comment"><p>- Check the notation on line 285 - there may be some interchanging of decimals and commas.</p></disp-quote><p>Thanks! Fixed!</p><disp-quote content-type="editor-comment"><p>Also, would the interpretation in terms of risk seeking and risk aversion be different for rewarding versus aversive outcomes?</p></disp-quote><p>Thanks for asking. If we understand it correctly, risk seeking and risk aversion mechanisms are only present in the RS models, which show clearly worse fitting performance. We thus decide not to overly interpret the fitted parameters in the RS models.</p><disp-quote content-type="editor-comment"><p>- Line 501, &quot;HA and PAT groups&quot; looks like a typo.</p><p>- In Figure 5, better graphical labeling of the panels and axes would be helpful.</p></disp-quote><p>Response: Thanks! Fixed!</p><p>REFERENCES</p><p>Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P., &amp; Dolan, R. J. (2011). Model-based influences on humans' choices and striatal prediction errors. Neuron, 69(6), 1204-1215.</p><p>Gagne, C., Zika, O., Dayan, P., &amp; Bishop, S. J. (2020). Impaired adaptation of learning to contingency volatility in internalizing psychopathology. Elife, 9.</p><p>Gershman, S. J. (2020). Origin of perseveration in the trade-off between reward and complexity. Cognition, 204, 104394.</p><p>Gershman, S. J., Horvitz, E. J., &amp; Tenenbaum, J. B. (2015). Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. Science, 349(6245), 273-278.</p><p>Von Neumann, J., &amp; Morgenstern, O. (1947). Theory of games and economic behavior, 2nd rev.</p></body></sub-article></article>