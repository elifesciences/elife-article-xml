<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64431</article-id><article-id pub-id-type="doi">10.7554/eLife.64431</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-137866"><name><surname>Nuiten</surname><given-names>Stijn Adriaan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9248-166X</contrib-id><email>stijnnuiten@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163674"><name><surname>Canales-Johnson</surname><given-names>Andrés</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2747-8894</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-213142"><name><surname>Beerendonk</surname><given-names>Lola</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4095-5122</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-213143"><name><surname>Nanuashvili</surname><given-names>Nutsa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2408-1821</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98851"><name><surname>Fahrenfort</surname><given-names>Johannes Jacobus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9025-3436</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-213144"><name><surname>Bekinschtein</surname><given-names>Tristan</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-88377"><name><surname>van Gaal</surname><given-names>Simon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6628-4534</contrib-id><email>simonvangaal@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, University of Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>Amsterdam Brain &amp; Cognition, University of Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution>Department of Psychology, University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution>Vicerrectoría de Investigación y Posgrado, Universidad Católica del Maule</institution><addr-line><named-content content-type="city">Talca</named-content></addr-line><country>Chile</country></aff><aff id="aff5"><label>5</label><institution>Behavioural and Clinical Neuroscience Institute</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Swann</surname><given-names>Nicole C</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oregon</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>06</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e64431</elocation-id><history><date date-type="received" iso-8601-date="2020-10-28"><day>28</day><month>10</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-06-02"><day>02</day><month>06</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Nuiten et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Nuiten et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64431-v2.pdf"/><abstract><p>Conflict detection in sensory input is central to adaptive human behavior. Perhaps unsurprisingly, past research has shown that conflict may even be detected in the absence of conflict awareness, suggesting that conflict detection is an automatic process that does not require attention. To test the possibility of conflict processing in the absence of attention, we manipulated task relevance and response overlap of potentially conflicting stimulus features across six behavioral tasks. Multivariate analyses on human electroencephalographic data revealed neural signatures of conflict only when at least one feature of a conflicting stimulus was attended, regardless of whether that feature was part of the conflict, or overlaps with the response. In contrast, neural signatures of basic sensory processes were present even when a stimulus was completely unattended. These data reveal an attentional bottleneck at the level of objects, suggesting that object-based attention is a prerequisite for cognitive control operations involved in conflict detection.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest :</title><p>Focusing your attention on one thing can leave you surprisingly unaware of what goes on around you. A classic experiment known as ‘the invisible gorilla’ highlights this phenomenon. Volunteers were asked to watch a clip featuring basketball players, and count how often those wearing white shirts passed the ball: around half of participants failed to spot that someone wearing a gorilla costume wandered into the game and spent nine seconds on screen.</p><p>Yet, things that you are not focusing on can sometimes grab your attention anyway. Take for example, the ‘cocktail party effect’, the ability to hear your name among the murmur of a crowded room. So why can we react to our own names, but fail to spot the gorilla? To help answer this question, Nuiten et al. examined how paying attention affects the way the brain processes input.</p><p>Healthy volunteers were asked to perform various tasks while the words ‘left’ or ‘right’ played through speakers. The content of the word was sometimes consistent with its location (‘left’ being played on the left speaker), and sometimes opposite (‘left’ being played on the right speaker). Processing either the content or the location of the word is relatively simple for the brain; however detecting a discrepancy between these two properties is challenging, requiring the information to be processed in a brain region that monitors conflict in sensory input.</p><p>To manipulate whether the volunteers needed to pay attention to the words, Nuiten et al. made their content or location either relevant or irrelevant for a task. By analyzing brain activity and task performance, they were able to study the effects of attention on how the word properties were processed.</p><p>The results showed that the volunteers’ brains were capable of dealing with basic information, such as location or content, even when their attention was directed elsewhere. But discrepancies between content and location could only be detected when the volunteers were focusing on the words, or when their content or location was directly relevant to the task.</p><p>The findings by Nuiten et al. suggest that while performing a difficult task, our brains continue to react to basic input but often fail to process more complex information. This, in turn, has implications for a range of human activities such as driving. New technology could potentially help to counteract this phenomenon, aiming to direct attention towards complex information that might otherwise be missed.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cognitive control</kwd><kwd>task-relevance</kwd><kwd>object-based attention</kwd><kwd>EEG</kwd><kwd>decoding</kwd><kwd>cognitive conflict</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2016-STG_715605</award-id><principal-award-recipient><name><surname>van Gaal</surname><given-names>Simon</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When all features of conflicting sensory input are task-irrelevant, the brain can still process its sensory information, whereas conflict detection requires that minimally one stimulus feature is task-relevant or associated with a response.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Every day we are bombarded with sensory information from the environment, and we often face the challenge of selecting the relevant information and ignoring irrelevant – potentially conflicting – information to maximize performance. These selection processes require much effort and our full attention, sometimes rendering us deceptively oblivious to irrelevant sensory input (e.g., chest-banging apes), as illustrated by the famous inattentional blindness phenomenon (<xref ref-type="bibr" rid="bib90">Simons and Chabris, 1999</xref>). However, unattended events that are not relevant for the current task might still capture our attention or interfere with ongoing task performance, for example, when they are inherently relevant to us (e.g., our own name). This is illustrated by another famous psychological phenomenon: the cocktail party effect (<xref ref-type="bibr" rid="bib14">Cherry, 1953</xref>; <xref ref-type="bibr" rid="bib70">Moray, 1959</xref>). Thus, under specific circumstances, task-irrelevant information may capture attentional resources and be subsequently processed with different degrees of depth.</p><p>It is currently a matter of debate which processes require top-down attention (<xref ref-type="bibr" rid="bib21">Dehaene et al., 2006</xref>; <xref ref-type="bibr" rid="bib53">Koch and Tsuchiya, 2007</xref>; <xref ref-type="bibr" rid="bib54">Koelewijn et al., 2010</xref>; <xref ref-type="bibr" rid="bib59">Lamme, 2003</xref>; <xref ref-type="bibr" rid="bib60">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib83">Rousselet et al., 2004</xref>; <xref ref-type="bibr" rid="bib106">VanRullen, 2007</xref>). It was long thought that only basic physical stimulus features or very salient stimuli are processed in the absence of attention (<xref ref-type="bibr" rid="bib96">Treisman and Gelade, 1980</xref>) due to an ‘attentional bottleneck’ at higher levels of analysis (<xref ref-type="bibr" rid="bib7">Broadbent, 1958</xref>; <xref ref-type="bibr" rid="bib24">Deutsch and Deutsch, 1963</xref>; <xref ref-type="bibr" rid="bib58">Lachter et al., 2004</xref>; <xref ref-type="bibr" rid="bib112">Wolfe and Horowitz, 2004</xref>). However, there is now solid evidence that several tasks may in fact still unfold in the (near) absence of attention, including perceptual integration (<xref ref-type="bibr" rid="bib26">Fahrenfort et al., 2017</xref>), the processing of emotional valence (<xref ref-type="bibr" rid="bib86">Sand and Wiens, 2011</xref>; <xref ref-type="bibr" rid="bib92">Stefanics et al., 2012</xref>), semantical processing of written words (<xref ref-type="bibr" rid="bib87">Schnuerch et al., 2016</xref>), and visual scene categorization (<xref ref-type="bibr" rid="bib65">Li et al., 2002</xref>; <xref ref-type="bibr" rid="bib75">Peelen et al., 2009</xref>). Although one should be cautious in claiming complete absence of attention (<xref ref-type="bibr" rid="bib58">Lachter et al., 2004</xref>), these and other studies have pushed the boundaries of input processing that is task-irrelevant (without attention) and may even question the existence of an attentional bottleneck at all, at least for relatively low-level information. Conceivably, the attentional bottleneck is only present at higher, more complex, levels of cognitive processing, like cognitive control functions.</p><p>Over the years, various theories have been proposed with regard to this attentional bottleneck among which are the load theory of selective attention and cognitive control (<xref ref-type="bibr" rid="bib62">Lavie et al., 2004</xref>), the multiple resources theory (<xref ref-type="bibr" rid="bib110">Wickens, 2002</xref>), and the hierarchical central executive bottleneck theory and formalizations thereof in a cortical network model for serial and parallel processing (<xref ref-type="bibr" rid="bib89">Sigman and Dehaene, 2006</xref>; <xref ref-type="bibr" rid="bib118">Zylberberg et al., 2010</xref>; <xref ref-type="bibr" rid="bib119">Zylberberg et al., 2011</xref>). These theories all hinge on the idea that resources for the processing of information are limited and that the brain therefore has to allocate resources to processes that are currently most relevant via selective attention (<xref ref-type="bibr" rid="bib7">Broadbent, 1958</xref>; <xref ref-type="bibr" rid="bib95">Treisman, 1969</xref>). Resource (re-)allocation, and thus flexible behavior, is thought to be governed by an executive network, most prominently involving the prefrontal cortex (<xref ref-type="bibr" rid="bib32">Goldman-Rakic, 1995</xref>; <xref ref-type="bibr" rid="bib33">Goldman-Rakic, 1996</xref>). Information that is deemed task-irrelevant has fewer resources at its disposal and is therefore processed to a lesser extent. When more resources are necessary for processing the task-relevant information, for example, under high perceptual load, processing of task-irrelevant information diminishes (<xref ref-type="bibr" rid="bib61">Lavie et al., 2003</xref>; <xref ref-type="bibr" rid="bib62">Lavie et al., 2004</xref>). Yet even under high perceptual load, task-irrelevant features can be processed when they are part of an attended object (when object-based attention is present) (<xref ref-type="bibr" rid="bib11">Chen, 2012</xref>; <xref ref-type="bibr" rid="bib13">Chen and Cave, 2006</xref>; <xref ref-type="bibr" rid="bib18">Cosman and Vecera, 2012</xref>; <xref ref-type="bibr" rid="bib51">Kahneman et al., 1992</xref>; <xref ref-type="bibr" rid="bib72">O'Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib88">Schoenfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib108">Wegener et al., 2014</xref>). There is currently no consensus which type of information can be processed in parallel by the brain and which attentional mechanisms determine what information passes the attentional bottleneck. One unresolved issue is that most empirical work has investigated the bottleneck with regard to sensory features; however, it is unknown if the bottleneck and the distribution of processing resources also take place for more complex, cognitive processes. Here, we test whether such a high-level attentional bottleneck indeed exists in the human brain.</p><p>Specifically, we aim to test whether cognitive control operations, necessary to identify and resolve conflicting sensory input, are operational when that input is irrelevant for the task at hand (and hence unattended) and what role object-based attention may have in conflict detection. Previous work has shown that the brain has dedicated networks for the detection and resolution of conflict, in which the medial frontal cortex (MFC) plays a pivotal role (<xref ref-type="bibr" rid="bib79">Ridderinkhof et al., 2004</xref>). Conflict detection and subsequent behavioral adaptation is central to human cognitive control, and, hence, it may not be surprising that past research has shown that conflict detection can even occur unconsciously (<xref ref-type="bibr" rid="bib5">Atas et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">D'Ostilio and Garraux, 2012a</xref>; <xref ref-type="bibr" rid="bib42">Huber-Huber and Ansorge, 2018</xref>; <xref ref-type="bibr" rid="bib99">van Gaal et al., 2008</xref>), suggesting that the brain may detect conflict fully automatically and that it may even occur without paying attention (e.g., <xref ref-type="bibr" rid="bib78">Rahnev et al., 2012</xref>). Moreover, it has been shown that this automaticity can be enhanced by training, resulting in more efficient processing of conflict (<xref ref-type="bibr" rid="bib12">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">MacLeod and Dunbar, 1988</xref>; <xref ref-type="bibr" rid="bib99">van Gaal et al., 2008</xref>).</p><p>Conclusive evidence regarding the claim that conflict detection is fully automatic has, to our knowledge, not been provided, and therefore, the necessity of attention for cognitive control operations remains open for debate. Previous studies have shown that cognitive control processes are operational when to-be-ignored features from either a task-relevant or a task-irrelevant stimulus overlap with the behavioral response to be made to the primary task, causing interference in performance (<xref ref-type="bibr" rid="bib67">Mao and Wang, 2008</xref>; <xref ref-type="bibr" rid="bib74">Padrão et al., 2015</xref>; <xref ref-type="bibr" rid="bib117">Zimmer et al., 2010</xref>). In these circumstances, the interfering stimulus feature carries information related to the primary task and is therefore <italic>de facto</italic> not task-irrelevant. Consequently, it is currently unknown whether cognitive control operations are active for conflicting sensory input that is not related to the task at hand. Given the immense stream of sensory input we encounter in our daily lives, conflict between two (unattended) sources of perceptual information is inevitable.</p><p>Here, we investigated whether conflict between two features of an auditory stimulus (its content and its spatial location) would be detected by the brain under varying levels of task relevance of these features. The main aspect of the task was as follows. We presented auditory spoken words (‘left’ and ‘right’ in Dutch) through speakers located on the left and right side of the body. By presenting these stimuli through either the left or the right speaker, content-location conflict arises on specific trials (e.g., the word ‘left’ from the right speaker) but not on others (e.g., the word ‘right’ from the right speaker) (<xref ref-type="bibr" rid="bib8">Buzzell et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Canales-Johnson et al., 2020</xref>). A wealth of previous studies has revealed that conflict arises between task-relevant and task-irrelevant features of the stimulus in these type of tasks (similar to the Simon task and Stroop task; <xref ref-type="bibr" rid="bib25">Egner and Hirsch, 2005</xref>; <xref ref-type="bibr" rid="bib41">Hommel, 2011</xref>). Here, these potentially conflicting auditory stimuli were presented during six different behavioral tasks, divided over two separate experiments, multiple experimental sessions, and different participant groups (both experiments N = 24). In all tasks, we focus on the processing of content-location conflict of the auditory stimulus. There were several critical differences between the behavioral tasks: (1) task relevance of a conflicting feature of the stimulus, (2) task relevance of a non-conflicting feature that was part of a conflicting stimulus, and (3) whether the response to be given mapped onto a conflicting feature of the stimulus. Note that in all tasks only one feature could be task-relevant and that all the other feature(s) had to be ignored. The systematic manipulation of task relevance and the response-mapping allowed us to explore the full landscape of possibilities of how varying levels of attention affect sensory and conflict processing. Electroencephalography (EEG) was recorded and multivariate analyses on the EEG data were used to extract any neural signatures of conflict detection (i.e., theta-band neural oscillations; <xref ref-type="bibr" rid="bib10">Cavanagh and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib16">Cohen and Cavanagh, 2011</xref>) and sensory processing for any of the features of the auditory stimulus. Furthermore, in both experiments we measured behavioral and neural effects of task-irrelevant conflict before and after training on conflict-inducing tasks, aiming to investigate the role of automaticity in the detection of (task-irrelevant) conflict.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment 1: can the brain detect fully task-irrelevant conflict?</title><p>In the first experiment, 24 human participants performed two behavioral tasks (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the auditory conflict task (from hereon: content discrimination task I), the feature ‘sound content’ was task-relevant. Participants were instructed to respond according to the content of the auditory stimulus (‘left’ vs. ‘right’), ignoring its spatial location that could conflict with the content response (presented from the left or right side of the participant). For the other behavioral task, participants performed a demanding visual random dot-motion (RDM) task in which they had to discriminate the direction of vertical motion (from hereon: vertical RDM task), while being presented with the same auditory stimuli – all features of which were thus fully irrelevant for task performance. Behavioral responses on this visual task were orthogonal to the response tendencies potentially triggered by the auditory features, excluding any task- or response-related interference (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Under this manipulation, all auditory features are task-irrelevant and are orthogonal to the response-mapping. To maximize the possibility of observing conflict detection when conflicting features are task-irrelevant and explore the effect of task automatization on conflict processing, participants performed the tasks both before and after extensive training, which may increase the efficiency of cognitive control (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib99">van Gaal et al., 2008</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design of experiment 1.</title><p>(<bold>A, B</bold>) Schematic representation of the experimental design for auditory content discrimination task I (<bold>A</bold>) and vertical random dot-motion (RDM) task (<bold>B</bold>). In both tasks, the spoken words ‘left’ and “right were presented through either a speaker located on the left or right side of the participant. Note that auditory stimuli are only task-relevant in auditory content discrimination task I and not in the vertical RDM task. In this figure, sounds are only depicted as originating from the right, whereas in the experiment the sounds could also originate from the left speaker. (<bold>A</bold>) In content discrimination task I, participants were instructed to report the content (‘left’ or ‘right’) of an auditory stimulus via a button press with their left or right hand, respectively, and to ignore the spatial location at which the auditory stimulus was presented. (<bold>B</bold>) During the vertical RDM task, participants were instructed to report the overall movement direction of dots (up or down) via a button press with their right hand, whilst still being presented with the auditory stimuli, which were therefore task-irrelevant. In both tasks, content of the auditory stimuli could be congruent or incongruent with its location of presentation (50% congruent/incongruent trials). (<bold>C</bold>) Overview of the sequence of the four experimental sessions of this study. Participants performed two electroencephalography sessions during which they first performed the vertical RDM task followed by auditory content discrimination task I. Each session consisted of 1200 trials, divided over 12 blocks, allowing participants to rest in between blocks. In between experimental sessions, participants were trained on auditory content discrimination task I on two training sessions of 1 hr each.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Experiment 1: conflicting information induces slower responses and decreased accuracy only for task-relevant sensory input</title><p>For content discrimination task I, mean error rates (ERs) were 2.6% (<italic>SD</italic> = 2.7%) and mean reaction times (RTs) 477.2 ms (<italic>SD</italic> = 76.1 ms), averaged over all four sessions. For the vertical RDM, mean ERs were 19.2% (<italic>SD</italic> = 6.6%) and mean RTs were 711.4 ms (<italic>SD</italic> = 151.3 ms). The mean ER of vertical RDM indicates that our staircasing procedure was effective (see Materials and methods for details on staircasing performance on the RDM). To investigate whether our experimental design was apt to induce conflict effects for task-relevant sensory input and to test whether conflict effects were still present when sensory input was task-irrelevant, we performed repeated measures (rm-)ANOVAs (2 × 2 × 2 factorial) on mean RTs and ERs gathered during the EEG recording sessions (session 1, ‘before training’; session 4, ‘after training’). This allowed us to include (1) task relevance (yes/no), (2) training (before/after), and (3) congruency of auditory content with location of auditory source (congruent/incongruent). Note that congruency is always defined based on the relationship between two features of the auditorily presented stimuli, also when participants performed the visual task (and therefore the auditory features were task-irrelevant).</p><p>Detection of conflict is typically associated with behavioral slowing and increased ERs. Indeed, we observed that, across both tasks, participants were slower and made more errors on incongruent trials as compared to congruent trials (the conflict effect, RT: <italic>F</italic>(1,23) = 52.83, p&lt;0.001, <inline-formula><mml:math id="inf1"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.70; ER: <italic>F</italic>(1,23) = 9.13, p=0.01, <inline-formula><mml:math id="inf2"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.28). This conflict effect was modulated by task relevance of the auditory features (RT: <italic>F</italic>(1,23) = 152.76, p&lt;0.001, <inline-formula><mml:math id="inf3"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.87; ER: <italic>F</italic>(1,23) = 11.15, p=0.01, <inline-formula><mml:math id="inf4"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.33) and post-hoc ANOVAs (see Materials and methods) showed that the conflict effect was present when the auditory feature content was task-relevant (RT<sub>cont(I)</sub>: <italic>F</italic>(1,23) = 285.00, p&lt;0.001, <inline-formula><mml:math id="inf5"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.93; ER<sub>cont(I)</sub>: <italic>F</italic>(1,23) = 23.85, p&lt;0.001, <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.51; <xref ref-type="fig" rid="fig2">Figure 2A</xref>, left panel), but not when all auditory features were task-irrelevant (RT<sub>VRDM</sub>: <italic>F</italic>(1,23) = 1.96, p=0.18, <inline-formula><mml:math id="inf7"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.08, <italic>BF</italic><sub>01</sub> = 5.41; ER<sub>VRDM</sub>: <italic>F</italic>(1,23) = 0.26, p=0.62, <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.01, <italic>BF</italic><sub>01</sub> = 4.55; <xref ref-type="fig" rid="fig2">Figure 2A</xref>, right panel). Because responses in the vertical RDM were made with the right hand only, we subsequently tested whether the auditory features <italic>in isolation</italic> affected the speed and accuracy of right-hand responses. For example, the spoken word ‘left’ may slow down responses made with the right hand more so than the spoken word ‘right’ (the same logic holds for stimulus location). However, this was not the case. A 2 × 2 × 2 factorial rm-ANOVA on mean RTs with session (before/after training), stimulus content ('left'/'right'), and stimulus location (left/right) showed that RTs were unaffected by sound content (<italic>F</italic>(1,23) = 0.01, p=0.92, <inline-formula><mml:math id="inf9"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.00, <italic>BF</italic><sub>01</sub> = 6.16) and sound location (<italic>F</italic>(1,23) = 0.49, p=0.49, <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.02, <italic>BF</italic><sub>01</sub> = 6.36).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral and multivariate decoding results of experiment 1.</title><p>(<bold>A, B</bold>) All results depicted here are from the merged data of both experimental sessions. The left column of plots shows the results for content discrimination task I, where auditory stimuli and conflicting features were task-relevant. The right column of plots shows the results for the vertical random dot-motion (RDM), where neither the auditory stimulus nor its conflicting features were task-relevant. (<bold>A</bold>) The behavioral results are plotted as conflict effects (incongruent – congruent). Effects of conflict were present in content discrimination task I, with longer reaction times (RTs) (left bar) and increased error rates (ERs) (right bar) for incongruent compared to congruent trials. For the vertical RDM task, no significant effects of conflict were found in behavior. Dots represent individual participants. The behavioral data that is shown here can be found in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>. (<bold>B</bold>) Multivariate classifier accuracies for different stimulus features. We trained classifiers on three stimulus features: auditory congruency, auditory content, and auditory location. Classifier accuracies (area under the curve [AUC]) are plotted across a time-frequency window of −100 ms to 1000 ms and 2–30 Hz. Classifier accuracies are thresholded (cluster-based corrected, one-sided: <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, p&lt;0.05), and significant clusters are outlined with a solid black line. The dotted box shows the predefined ROI on which we performed a hypothesis-driven analysis. The classifier accuracies within this ROI were not significantly greater than chance for the vertical RDM task. Note that conflicting features of the auditory stimulus, content and location, could be decoded from neural data regardless of attention to the auditory stimulus. Information related to auditory congruency was present in a theta-band cluster, but only when the auditory stimulus was attended. *** p&lt;0.001, n.s.: p&gt;0.05.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Behavioral results of experiment 1.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64431-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Effects of behavioral training on behavioral effects of conflict and decoding performance in experiment 1.</title><p>(<bold>A, B</bold>) We performed 2 × 2 repeated measures (rm)-ANOVAs on (<bold>A</bold>) reaction times (RTs) and (<bold>B</bold>) error rates (ERs) in content discrimination task I, with the factors being session and congruency of the auditory stimulus. In (<bold>A, B</bold>), data are plotted as conflict effects (incongruent – congruent) and for separate sessions. The top horizontal line shows significance of the interaction between session and congruency, and markers above the bars indicate significance of paired sample t-tests comparing incongruent and congruent for each run (shown data and results of t-tests can be found in <xref ref-type="supplementary-material" rid="fig2s1sdata1">Figure 2—figure supplement 1—source data 1</xref>). Effects of conflict on RTs (<bold>A</bold>) and ERs (<bold>B</bold>) significantly decreased after behavioral training on this task, suggesting more efficient processing of conflict. Effects of conflict on RTs and ERs were nonetheless present during both sessions. (<bold>C</bold>) There were no clusters for which the difference in congruency decoding between the two sessions in content discrimination task I was significant (left panel), although decoding accuracies within the preselected ROI did decrease with training for content discrimination task I, suggesting more efficient conflict resolution, in line with the behavioral results plotted in (<bold>A, B</bold>). Classifier accuracies for sound content (middle panel) were higher in a delta-theta band cluster after behavioral training, showing that the task-relevant feature was processed better. Location decoding accuracy was not affected by behavioral training as we observed no clusters where the differences between sessions and classifier accuracies within the ROI were also not different between sessions. (<bold>D</bold>) Behavioral training on the content discrimination task did not affect neural processing of auditory features in the vertical random dot-motion (no significant clusters and none of the results were significant when tested for the predefined ROI). Thresholded (cluster-based corrected, p&lt;0.05) accuracies are depicted across the frequency range (2–30 Hz), and significant clusters are outlined with a solid black line. <bold>***</bold>p&lt;0.001, <bold>**</bold>p&lt;0.01, n.s.: p&gt;0.05.</p><p><supplementary-material id="fig2s1sdata1"><label>Figure 2—figure supplement 1—source data 1.</label><caption><title>Behavioral results of experiment 1 - before and after training.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64431-fig2-figsupp1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig2-figsupp1-v2.tif"/></fig></fig-group><p>Participants performed both behavioral tasks before and after extensive training of the content discrimination task to be able to investigate the role of training on conflict processing (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). RTs and ERs in the vertical RDM task were not modulated by behavioral training (RT<sub>VRDM</sub>: <italic>F</italic>(1,23) = 2.07, p=0.16, <inline-formula><mml:math id="inf12"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.08, <italic>BF</italic><sub>01</sub> = 0.32; ER<sub>VRDM</sub>: <italic>F</italic>(1,23) = 0.24, p=0.63, <inline-formula><mml:math id="inf13"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.01, <italic>BF</italic><sub>01</sub> = 3.79). Training did result in a decrease of overall RT on content discrimination task I, although ERs were not affected (RT<sub>cont(I)</sub>: <italic>F</italic>(1,23) = 45.05, p&lt;0.001, <inline-formula><mml:math id="inf14"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.66; ER<sub>cont(I)</sub>: <italic>F</italic>(1,23) = 1.77, p=0.20, <inline-formula><mml:math id="inf15"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.07, <italic>BF</italic><sub>01</sub> = 0.89). Moreover, the effect of conflict on RTs and ERs in this task decreased after behavioral training (RT<sub>cont(I)</sub>: <italic>F</italic>(1,23) = 29.86, p&lt;0.001, <inline-formula><mml:math id="inf16"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.57; ER<sub>cont(I)</sub>: <italic>F</italic>(1,23) = 9.76, p=0.005, <inline-formula><mml:math id="inf17"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.30; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A, B</xref>), suggesting increased efficiency of within-trial conflict resolution mechanisms. All other effects were not reliable (p&gt;0.05).</p></sec><sec id="s2-3"><title>Experiment 1: neural signatures of conflict detection only for task-relevant stimuli</title><p>The observation that conflicting task-irrelevant stimuli had no effect on RTs and ERs, even after substantial training, whereas task-relevant conflicting stimuli did, may not come as a surprise because manual responses on the visual task (motion up/down with index and middle finger of right hand) were fully orthogonal to the potential conflicting nature of the auditory features (i.e., left/right). Further, content discrimination task I and the vertical RDM were independent tasks, requiring different cognitive processes. For example, mean RTs on the vertical RDM were on average 267 ms longer than mean RTs for content discrimination task I. However, caution is required in concluding that conflict detection is absent for task-irrelevant stimuli based on these behavioral results alone as neural and/or behavioral effects can sometimes be observed in isolation (one is observed but not the other, e.g., <xref ref-type="bibr" rid="bib9">Canales-Johnson et al., 2020</xref>; <xref ref-type="bibr" rid="bib103">van Gaal et al., 2014</xref>). Therefore, in order to test whether unattended conflict is detected by the brain we turn to the multivariate pattern analysis (MVPA) of our neural data.</p><p>Plausibly, the neural dynamics of conflict processing for task-irrelevant sensory input are different – in physical (across electrodes) and frequency space – from those related to the processing of conflict when sensory input is task-relevant. Therefore, we applied multivariate decoding techniques in the frequency domain to inspect whether and – if so – to what extent certain stimulus features were processed. These multivariate approaches have some advantages over traditional univariate approaches, for example, they are less sensitive to individual differences in spatial topography, because decoding accuracies are derived at a single participant level (<xref ref-type="bibr" rid="bib27">Fahrenfort et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Grootswagers et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Haxby et al., 2001</xref>). Therefore, group statistics do not critically depend on the presence of effects in specific electrodes or clusters of electrodes. Further, although a wealth of studies have shown that conflict processing is related to an increase in power of theta-band neural oscillations (~4–8 Hz) after stimulus presentation (<xref ref-type="bibr" rid="bib16">Cohen and Cavanagh, 2011</xref>; <xref ref-type="bibr" rid="bib48">Jiang et al., 2015a</xref>; <xref ref-type="bibr" rid="bib71">Nigbur et al., 2012</xref>), it is unknown whether this is also the case for task-irrelevant conflict. By performing our MVPA in frequency space, we could potentially find neural signatures in non-theta frequency bands related to the processing of task-irrelevant conflict. However, due to the temporal and frequency space that has to be covered, strict multiple comparison corrections have to be performed (across time and frequency, see Materials and methods). Therefore, we adopted an additional hypothesis-driven analysis, which also allowed us to obtain evidence for the absence of effects. Throughout this paper, we will discuss our neural data in the following order. First, the MVPAs in the frequency domain are presented for all critical features of the task (congruency, content, location, corrected for multiple comparisons). Then, we report results from the additional hypothesis-driven analysis, where we extracted classifier accuracies from a predefined time-frequency region of interest (ROI) (100–700 ms, 2–8 Hz) on which we performed (Bayesian) tests (see Materials and methods). This ROI was selected based on previous observations of conflict-related theta-band activity (<xref ref-type="bibr" rid="bib16">Cohen and Cavanagh, 2011</xref>; <xref ref-type="bibr" rid="bib17">Cohen and van Gaal, 2014</xref>; <xref ref-type="bibr" rid="bib49">Jiang et al., 2015b</xref>; <xref ref-type="bibr" rid="bib71">Nigbur et al., 2012</xref>). Specifically, for every task and every stimulus feature (i.e., congruency, content, location), we extracted average decoding accuracies from the ROI per participant and performed analyses on these values.</p><p>First, we trained a classifier on data from all EEG electrodes to distinguish between congruent versus incongruent trials, for both content discrimination task I and the vertical RDM task. Above-chance classification accuracies imply that relevant information about the decoded stimulus feature is present in the neural data, meaning that some processing of that feature occurred (<xref ref-type="bibr" rid="bib40">Hebart and Baker, 2018</xref>). We performed our main analysis on the combined data from both EEG sessions, thereby maximizing power to establish effects in our crucial comparisons. We also performed similar analyses on the session-specific data to investigate the role of behavioral training on processing of conflict. These results are discussed more in depth below and are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C, D</xref>.</p><p>Congruency decoding reveals that stimulus congruency was represented in neural data only when conflict was task-relevant (p<italic>&lt;</italic>0.001, one-sided: <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–12 Hz, peak frequency: 4 Hz, time range: 234–609 ms, peak time: 438 ms; <xref ref-type="fig" rid="fig2">Figure 2B</xref>, left panel). The conflict effect roughly falls in the theta-band (4–8 Hz), which confirms a firm body of literature linking conflict detection to post-conflict modulations in theta-band oscillatory dynamics (<xref ref-type="bibr" rid="bib10">Cavanagh and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib16">Cohen and Cavanagh, 2011</xref>; <xref ref-type="bibr" rid="bib17">Cohen and van Gaal, 2014</xref>; <xref ref-type="bibr" rid="bib71">Nigbur et al., 2012</xref>). Activation patterns that were calculated from classifier weights within the predefined time-frequency theta-band ROI (2–8 Hz, 100–700 ms) revealed a clear midfrontal distribution of conflict-related activity (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>). No significant time-frequency cluster was found for the vertical RDM task (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right panel). To quantify the absence of this effect, we followed up this hypothesis-free (with respect to frequency and time) MVPA with a hypothesis-driven analysis focused on the post-stimulus theta-band. This more restricted analysis showed no significant effect (<italic>t</italic>(23) = −0.50, p<italic>=</italic>0.69, <italic>d</italic> = −0.10) and an additional Bayesian analysis revealed moderate evidence in favor of the null hypothesis (i.e., no effect of conflict on theta-band power) than the alternative hypothesis (<italic>BF</italic><sub>01</sub> = 6.53).</p><p>Similar to our observation of decreased behavioral effects of conflict after behavioral training (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A, B</xref>), decoding accuracies in the content discrimination task I were also lower after training (<italic>t</italic>(23) = −3.01, p=0.01<italic>, d</italic> = −0.63; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>), suggesting more efficient conflict resolution, as reflected in neural theta oscillations as well. In the vertical RDM, behavioral training did not affect decoding accuracies of sound congruency (<italic>t</italic>(23) = −1.24, p=0.23, <italic>d</italic> = −0.25, <italic>BF</italic><sub>01</sub> = 2.36; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>).</p></sec><sec id="s2-4"><title>Experiment 1: stimulus features are processed in parallel, independent of task relevance</title><p>Thus, cognitive control networks − or possible substitute networks − are seemingly not capable of detecting conflict when sensory features are task-irrelevant. However, the question remains whether this observation is specific to the conflicting nature of the auditory stimuli or whether the auditory stimuli are not processed whatsoever when attention is reallocated to the visually demanding task. To address this question, we trained classifiers on two other features of the auditory stimuli, that is, location and content, to test whether these features were processed by the brain regardless of task relevance. Indeed, the content of auditory stimuli was processed both when the stimuli were task-relevant (p<italic>&lt;</italic>0.001, one-sided: <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–30 Hz, peak frequency: 4 Hz, time range: 47–1000 ms, peak time: 422 ms; <xref ref-type="fig" rid="fig2">Figure 2B</xref>, left panel) and task-irrelevant (p<italic>&lt;</italic>0.001, one-sided: <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–20 Hz, peak frequency: 6 Hz, time range: 78–547 ms, peak time: 297 ms; <xref ref-type="fig" rid="fig2">Figure 2B</xref>, right panel).</p><p>Similarly, the location of auditory stimuli could also be decoded from neural data for both content discrimination task I (p&lt;0.001, one-sided: <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–18 Hz, peak frequency: 6 Hz, time range: 63–-672 ms, peak time: 203 ms; <xref ref-type="fig" rid="fig2">Figure 2B</xref>, left panel) and the vertical RDM task (p<italic>&lt;</italic>0.001, one-sided: <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–12 Hz, peak frequency: 6 Hz, time range: 156–484 ms, peak time: 281 ms; <xref ref-type="fig" rid="fig2">Figure 2B</xref>, right panel). The above chance performance of the classifiers for the auditory stimulus features demonstrates that location and content information were processed, even when these features were task-irrelevant. Processing of task-irrelevant stimulus features was, however, more transient in time and more narrowband in frequency as compared to processing of the same features in a task-relevant setting. Further, content decoding revealed a much broader frequency spectrum than any of the other comparisons in content discrimination task I. In the next experiment, we show that this is related to the fact that this feature was response-relevant and that this effect therefore partially reflects response preparation and response execution processes. Summarizing, we show that when (conflicting) features of an auditory stimulus are truly and consistently task-irrelevant, the conflict between them is no longer detected by – nor relevant to – the conflict monitoring system, but the features (content and location) are still processed in isolation.</p><p>To investigate if, and how, behavioral training affects processing of sound content and location, we tested whether decoding accuracies for these features were different between the two experimental sessions. Decoding accuracies for the task-relevant feature of content discrimination task I (i.e., sound content) were significantly increased after behavioral training in a delta- to theta-band cluster (p&lt;0.001, one-sided: <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–10 Hz, peak frequency: 2 Hz, time range: 234–484 ms, peak time: 344 ms; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). This suggests that processing of task-relevant information (i.e., sound content) is improved as a result of training. Decoding accuracies for sound location in the content discrimination task were not different before and after behavioral training (no significant clusters; predefined ROI: <italic>t</italic>(23) = 0.12, p=0.91, <italic>d</italic> = 0.02, <italic>BF</italic><sub>01</sub> = 4.63). In the vertical RDM task, behavioral training did not affect the decoding accuracies within the predefined ROI for sound content (<italic>t</italic>(23) = 0.75, p=0.46, <italic>d</italic> = 0.15, <italic>BF</italic><sub>01</sub> = 3.61) and location (<italic>t</italic>(23) = 0.04, p=0.97, <italic>d</italic> = 0.01, <italic>BF</italic><sub>01</sub> = 4.66, also no other significant clusters; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>). This suggests that processing of sound content and location (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), both task-irrelevant auditory features in the vertical RDM task, is automatic and not dependent on training.</p><p>In conclusion, we observed neural signatures of the processing of sensory stimulus features (i.e., location and content of an auditory stimulus) regardless of task relevance of these features, but a lack of integration of these features to form conflict when the auditory stimulus was fully task-irrelevant. Considerable training in content discrimination task I resulted in more efficient conflict processing (i.e., decreased behavioral conflict effects and theta-band activity after training; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) when the auditory stimulus was task-relevant, but this increased automaticity did not lead to detection of conflict when the auditory stimulus was fully task irrelevant.</p></sec><sec id="s2-5"><title>Experiment 2: does detection of conflict depend on task relevance of the stimulus or its individual features?</title><p>The experimental design of the first experiment rendered the auditory features to be located at the extreme ends of the scale of task relevance, that is, either the conflicting features were task-relevant and the conflicting features were consistently mapped to specific responses, or the conflicting features were task-irrelevant and the conflicting features were not mapped to responses. However, to further understand the relationship between the relevance of the conflicting features and the overlap with responses, we performed a second experiment containing four behavioral tasks. For this second experiment, we recruited 24 new participants. We included two auditory conflicting tasks, similar to content discrimination task I. In one of the auditory tasks (from hereon: content discrimination task II, <xref ref-type="fig" rid="fig3">Figure 3A</xref>), participants again had to respond according to the content of the auditory stimulus, whereas in the other auditory task (from hereon: location discrimination task, <xref ref-type="fig" rid="fig3">Figure 3B</xref>) they were instructed to report from which side the auditory stimulus was presented (i.e., left or right speaker).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experimental design of experiment 2.</title><p>(<bold>A–D</bold>) Schematic representation of the experimental design for auditory content discrimination task II (<bold>A</bold>), location discrimination task (<bold>B</bold>), volume oddball task (<bold>C</bold>), and horizontal random dot-motion (RDM) task (<bold>D</bold>). In all tasks, the spoken words ‘left’ and ‘right’ were presented through either a speaker located on the left or right side of the participant. (<bold>A</bold>) In auditory content discrimination task II, participants were instructed to report the content (‘left’ or ‘right’) of an auditory stimulus via a button press with their left or right hand, respectively, and to ignore the location of the auditory stimulus that was presented. (<bold>B</bold>) In the auditory location discrimination task, participants were instructed to report the location (left or right speaker) of an auditory stimulus via a button press with their left or right hand, respectively, and to ignore the content of the auditory stimulus that was presented. (<bold>C</bold>) During the volume oddball task, participants were instructed to detect auditory stimuli that were presented at a lower volume than the majority of the stimuli (i.e., oddballs) by pressing the spacebar with their right hand. (<bold>D</bold>) In the horizontal RDM, participants were instructed to report the overall movement of dots (left or right) via a button press with their left and right hands, respectively, whilst still being presented with the auditory stimuli. In all four tasks, content of the auditory stimuli could be congruent or incongruent with its location of presentation (50% congruent/incongruent trials). (<bold>E</bold>) Order of behavioral tasks in experiment 2. Participants always started with the volume oddball task, followed by the location discrimination task, content discrimination task, and horizontal RDM, in randomized order. Participants ended with another run of the volume oddball task.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig3-v2.tif"/></fig><p>Furthermore, we included two new tasks in which the conflicting features (location and content) were not task-relevant and participants responded to a non-conflicting feature that was part of the conflicting stimulus (from hereon: volume oddball detection task, <xref ref-type="fig" rid="fig3">Figure 3C</xref>) or the auditory stimulus was task-irrelevant but its features - location and content- overlapped with the responses to be given (from hereon: horizontal RDM task, <xref ref-type="fig" rid="fig3">Figure 3D</xref>). The horizontal RDM task was similar to vertical RDM task of experiment 1; however, the dots were now moving on a horizontal plane. In other words, participants were instructed to classify the overall movement of moving dots to either the left or the right. As this is a visual paradigm, the simultaneously presented auditory stimuli are fully task-irrelevant. However, both features of conflict, the content (i.e., ‘left’ and ‘right’) and the location (i.e., left and right speaker), of the auditory stimuli could potentially interfere with participants’ responses on the visual task, thereby inducing a crossmodal Stroop-like type of conflict (<xref ref-type="bibr" rid="bib93">Stroop, 1935</xref>).</p><p>In the volume oddball detection task, participants were presented with the same auditory stimuli as before; however, one out of eight stimuli (12.5%) was presented at a lower volume. Participants were instructed to detect these volume oddballs by pressing the spacebar with their right hand as fast as possible. If they did not hear an oddball, they were instructed to withhold from responding. In this task, theoretically, the selection of an object’s feature (e.g., volume) could lead to the selection of all of its features (e.g., sound content, location), as suggested by theories of object-based attention (<xref ref-type="bibr" rid="bib11">Chen, 2012</xref>). This in turn may lead to conflict detection, even if the conflicting features are task-irrelevant. Similar to experiment 1, we included behavioral training in conflict-inducing tasks to inspect if enhanced automaticity of conflict processing would affect conflict detection under task-irrelevant sensory input. Participants performed 500 trials of the volume oddball detection task twice at the very beginning of a session and at the end of a session (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). During the first run of the task, neither sound content nor sound location was related to any behavioral responses, whereas during the second run these features might have acquired some intrinsic relevance through training on the other tasks. Furthermore, repeated exposure to conflict may prime the conflict monitoring system to exert more control over sensory inputs necessary for more efficient conflict detection, even when these sensory inputs are not task-relevant within the context of the task the participant is performing at that time.</p><p>In order to keep sensory input similar, moving dots (coherence: 0) were presented on the monitor during content discrimination task II, the location discrimination task, and the volume oddball detection task, but these could be ignored. Again, EEG was recorded while participants performed these tasks in order to see if auditory conflict was detected when the auditory stimulus or its conflicting features (i.e., location and content) were task-irrelevant. We performed the same artifact rejection procedure as in experiment 1. For one participant, on average 64.5% (<italic>SD</italic> = 9.9%) of all epochs within each task were removed in this procedure, which is 3.9 standard deviations from the average ratio of removed epochs in this experiment (<italic>M</italic> = 10.3%, <italic>SD</italic> = 13.9%). Therefore, this participant was excluded from the EEG analysis of experiment 2, resulting in N = 23 for the analysis of EEG data.</p></sec><sec id="s2-6"><title>Experiment 2: behavioral effects of conflict only for task-relevant auditory sensory input</title><p>Mean RT in the location discrimination task was 338.2 ms (<italic>SD</italic> = 112.7 ms) and mean ER was 4.5% (<italic>SD</italic> = 3.2%). For content discrimination task II, RTs were on average 364.0 ms (<italic>SD</italic> = 127.4 ms) and ERs were 5.5% (<italic>SD</italic> = 5.8%). For the horizontal RDM, RTs were on average 362.5 ms (<italic>SD</italic> = 116.5 ms) and ERs were 27.7% (<italic>SD</italic> = 5.2%). Mean RTs of the volume oddball task were calculated for correct trials in which a response was made (i.e., hit trials) and was 504.5 ms (<italic>SD</italic> = 178.7 ms, on hit trials). On average, participants had 40.5 hits (<italic>SD</italic> = 12.8) out of 61.8 oddball trials (<italic>SD</italic> = 8.6), per run of 500 trials. We will first discuss the behavioral results of the content discrimination, location discrimination, and horizontal RDM tasks as behavioral performance for the volume oddball task is represented in perceptual sensitivity (d’), rather than ER.</p><p>rm-ANOVAs (3 × 2 factorial) were performed on mean RTs and ERs from these three tasks, with the factors (1) task and (2) congruency of the auditory features. Again, congruency always relates to the combination of the auditory stimulus features sound content ('left' vs. 'right') and sound location (left speaker vs. right speaker). We observed that participants were slower and made more errors on incongruent trials as compared to congruent trials (RT: <italic>F</italic>(1,23) = 75.41, p<italic>&lt;</italic>0.001, <inline-formula><mml:math id="inf24"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.77; ER: <italic>F</italic>(1,23) = 68.00, p &lt;0.001, <inline-formula><mml:math id="inf25"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.75; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). This conflict effect was modulated by task (RT: <italic>F</italic>(1.55,35.71) = 22.80, p&lt;0.001, <inline-formula><mml:math id="inf26"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.50; ER: <italic>F</italic>(1.58, 36.36) = 10.18, p&lt;0.001, <inline-formula><mml:math id="inf27"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>=0.31) and post-hoc paired samples t-tests (incongruent – congruent) showed that conflict effects were only present in tasks where one of the conflicting features was task-relevant (location discrimination task: RT<sub>loc</sub>: <italic>t</italic>(23) = 5.03, p&lt;0.001, <italic>d</italic> = 1.03; ER<sub>loc</sub>: <italic>t</italic>(23) = 6.25, p&lt;0.001, <italic>d</italic> = 1.28; content discrimination task II: RT<sub>cont(II)</sub>: <italic>t</italic>(23) = 8.95, p&lt;0.001, <italic>d</italic> = 1.83; ER<sub>cont(II)</sub>: <italic>t</italic>(23) = 5.93, p&lt;0.001, <italic>d</italic> = 1.21; horizontal RDM task: RT<sub>HRDM</sub>: <italic>t</italic>(23) = 1.44, p=0.16, <italic>d</italic> = 0.29, <italic>BF</italic><sub>01</sub> = 1.88; ER<sub>HRDM</sub>: <italic>t</italic>(23) = 1.65, p=0.11, <italic>d</italic> = 0.34, <italic>BF</italic><sub>01</sub> = 1.44). Although in the horizontal RDM task conflict between sound content and location did not affect the speed of responses, stimulus content and location <italic>in isolation</italic> could have potentially interfered with behavioral performance given the overlap of these features with both the plane of dot direction (left/right) and the response scheme (left/right hand). Indeed, trials containing conflict between sound location and dot direction resulted in slower RTs and increased ERs (RT: <italic>t</italic>(23) = 2.12, p=0.045, <italic>d</italic> = 0.44; ER: <italic>t</italic>(23) = 5.94, p&lt;0.001, <italic>d</italic> = 1.21). Similar effects were observed for trials where sound content conflicted with the dot direction, but onlyin ERs (RT: <italic>t</italic>(23) = 1.72, p=0.10, <italic>d</italic> = 0.35, <italic>BF</italic><sub>01</sub> = 2.85; ER: <italic>t</italic>(23) = 5.55, p&lt;0.001, <italic>d</italic> = 1.13). This shows that sound content and location <italic>in isolation</italic>, even though both features were task-irrelevant, interfered with task performance.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Behavioral and multivariate decoding results for experiment 2.</title><p>(<bold>A, B</bold>) The four columns show data belonging to, from left to right, content discrimination task II, the location discrimination task, the volume oddball detection task, and the horizontal random dot-motion (RDM) task. (<bold>A</bold>) Behavioral results are plotted as conflict effects (incongruent – congruent). Effects of conflict were present in all tasks where the auditory stimulus was task-relevant (content discrimination task II, location discrimination task, and volume oddball). In both auditory discrimination tasks, we observed longer reaction times (RTs) (left bar) and increased error rates (right bar) for incongruent compared to congruent trials. For the volume oddball, we did not observe an effect in RT, but increased sensitivity (d’) on incongruent compared to congruent trials. Dots represent individual participants. The data that is shown here can be found in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>. (<bold>B</bold>) Multivariate classifier accuracies for different stimulus features (auditory congruency, auditory content, and auditory location). Classifier accuracies (area under the curve [AUC]) are plotted across a time-frequency window of −100 ms to 1000 ms and 2–30 Hz. Classifier accuracies are thresholded (cluster-based corrected, one-sided: <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, p&lt;0.05), and significant clusters are outlined with a solid black line. The dotted box shows the predefined ROI on which we performed a hypothesis-driven analysis. Note that the data shown for the volume oddball task was merged over both runs. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; n.s.: p&gt;0.05.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Behavioral results of experiment 2.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64431-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Effects of exposure to conflict inducing task on behavioral effects of conflict and decoding performance in the volume oddball task of experiment 2.</title><p>(<bold>A–D</bold>) We performed 2 × 2 repeated measures ANOVAs on (<bold>A</bold>) reaction times (RTs), (<bold>B</bold>) perceptual sensitivity (d’), (<bold>C</bold>) hit rates, and (<bold>D</bold>) false alarm rates with the factors run number and congruency of the auditory stimulus. In (<bold>A–D</bold>), data are plotted as conflict effects (incongruent – congruent) and for separate runs. The top horizontal line shows significance of the interaction between session and congruency, and markers above the bars indicate significance of paired sample t-tests comparing incongruent and congruent for each run (shown data and results of t-tests can be found in <xref ref-type="supplementary-material" rid="fig4s1sdata1">Figure 4—figure supplement 1—source data 1</xref>). (<bold>A</bold>) RTs were unaffected by auditory congruency and run number (statistics in Results). (<bold>B</bold>) There was no interaction effect between congruency and run number on perceptual sensitivity (d’; statistics in Results), and post-hoc paired sample t-tests (incongruent – congruent) revealed that the effect of congruency on d’ was present during both runs. (<bold>C</bold>) The interaction between congruency and run number was not significant (<italic>F</italic>(1,23) = 2.99, p=0.10, <inline-formula><mml:math id="inf29"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.12, <italic>BF</italic><sub>01</sub> = 1.64), showing that the effect of conflict on hit rate was not different for both runs, although the effect of conflict was present during the first, but not second run. (<bold>D</bold>) False alarm rates were not modulated by the interaction between congruency and run number (<italic>F</italic>(1,23) = 0.00, p=0.99, <inline-formula><mml:math id="inf30"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.00, <italic>BF</italic><sub>01</sub> = 3.45), showing that the effects of conflict were not different between runs. This conflict effect was present during both runs. (<bold>E</bold>) There were no clusters for which the difference in decoding of all features between the two runs of the volume oddball task was significant, and there were also no differences within the preselected ROI (congruency: <italic>t</italic>(22) = 0.07, p=0.95, <italic>d</italic> = 0.01, <italic>BF</italic><sub>01</sub> = 4.56; content: <italic>t</italic>(22) = 0.64, p=0.53, <italic>d</italic> = 0.13, <italic>BF</italic><sub>01</sub> = 3.81; location: <italic>t</italic>(22) = –1.25, <italic>p</italic>=0.22, <italic>d</italic> = –0.26, <italic>BF</italic><sub>01</sub> = 2.29), suggesting that processing of these features was not affected by training. Thresholded (cluster-based corrected, p&lt;0.05) accuracies are depicted across the frequency range (2–30 Hz). Plots show the difference in classifier accuracy between the two runs (run 2 – run 1) of stimulus congruency (left panel), stimulus content (middle panel), and stimulus location (right panel) in the volume oddball task. n.s.: p&gt;0.05.</p><p><supplementary-material id="fig4s1sdata1"><label>Figure 4—figure supplement 1—source data 1.</label><caption><title>Behavioral results of the volume oddball task - first and second run.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64431-fig4-figsupp1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Sensory feature decoding in the time-domain.</title><p>We trained classifiers on either sound content (<bold>A</bold>) or sound location (<bold>B</bold>) in order to see how neural representations of sensory processing were affected by our manipulation of task relevance of these features. (<bold>A</bold>) Sound content could be decoded from most tasks (except the horizontal random dot-motion [RDM]), and decoding accuracies for sound content were highest for the task in which sound content was the task-relevant feature, that is, content discrimination task II. Decoding accuracies were higher for content discrimination task II as compared to the other three tasks (difference start location discrimination: 328 ms; horizontal RDM: 313 ms; volume oddball: 344 ms). (<bold>B</bold>) Sound location could be decoded from all tasks. Again, the task in which the decoded feature was task-relevant, that is, the location discrimination task, showed the highest decoding accuracies. Location decoding performance was improved for the task in which this feature was task-relevant (i.e., location discrimination task) as compared to the other tasks. These differences started from 250 ms (vs. content discrimination task II), 234 ms (vs. horizontal RDM task), and 266 ms (vs. volume oddball task). Shaded areas represent the SEM. Bold traces indicate that feature decoding was significantly (cluster-corrected, one-sided t-test, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, p&lt;0.05) above chance. Horizontal black lines at the bottom depict where feature decoding is significantly different (cluster-corrected, two-sided t-test, p&lt;0.05) between the task in which the feature was task-relevant versus where it was task-irrelevant. CD II: content discrimination task II; HRDM: horizontal RDM task; LD: location discrimination task; VO: volume oddball detection task.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig4-figsupp2-v2.tif"/></fig></fig-group><p>For the volume oddball task, we tested the effect of auditory congruency on RTs of trials which, by virtue of task instruction, only covers oddball trials in which a correct response was made (i.e., hits). rm-ANOVAs (2 × 2 factorial) with the factors run number and feature congruency revealed no effects of auditory conflict on RT (<italic>F</italic>(1,23) = 2.78, p=0.11, <inline-formula><mml:math id="inf32"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.10, <italic>BF</italic><sub>01</sub> = 3.34; <xref ref-type="fig" rid="fig4">Figure 4A</xref>, no effects of training, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). To test whether <italic>individual</italic> features of the auditory stimulus interfered with right-hand responses, we performed an additional 2 × 2 factorial rm-ANOVA with sound content and location as factors. Auditory content significantly affected RTs, whereas sound location did not (content: <italic>F</italic>(1,23) = 25.41, p&lt;0.001, <inline-formula><mml:math id="inf33"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.53; location: <italic>F</italic>(1,23) = 0.99, p=0.33, <inline-formula><mml:math id="inf34"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.04, <italic>BF</italic><sub>01</sub> = 3.90). Specifically, RTs were slower for the spoken word 'left' (incongruent with the responding hand, <italic>M</italic> = 528.6 ms, <italic>SD</italic> = 193.1 ms) as compared to the spoken word 'right' (congruent with the responding hand, <italic>M</italic> = 493.3 ms, <italic>SD</italic> = 174.9 ms), revealing interference of sound content <italic>in isolation</italic> on right-hand responses, similar to the horizontal RDM task. Although conflict between the auditory features was not present in RTs, we did observe that sensitivity (d’) increased for incongruent (<italic>M</italic> = 2.66, <italic>SD</italic> = 0.77) compared to congruent trials (<italic>M</italic> = 2.11, <italic>SD</italic> = 0.81, <italic>F</italic>(1,23) = 45.62, p&lt;0.001, <inline-formula><mml:math id="inf35"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.67; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). These results show that volume oddball detection performance increases on trials that contain conflict between sound content and location. This effect of conflict on behavioral performance can already be found in the first run, when sound content and location had not yet been related to any responses/task and were thus fully task-irrelevant (<italic>t</italic>(23) = 5.71, p&lt;0.001, <italic>d</italic> = 1.17). There was no significant interaction between run number and auditory stimulus congruency (<italic>F</italic>(1,23) = 1.40, p=0.25, <inline-formula><mml:math id="inf36"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.06, <italic>BF</italic><sub>01</sub> = 2.13; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>; hit rates and false alarms are plotted in this figure supplement as well).</p></sec><sec id="s2-7"><title>Experiment 2: detection of conflict occurs when any feature of a stimulus is related to the response on the primary task or is task-relevant (even a non-conflicting feature)</title><p>We again trained multivariate classifiers on single-trial time-frequency data to test whether the auditory stimulus features (i.e., content, location, and congruency) were processed when (1) the auditory conflicting features were task-relevant and overlapped with the response-mapping (content and location discrimination tasks), (2) the auditory conflicting features were task-irrelevant and another feature of the conflicting stimulus was task-relevant (volume oddball task), or when (3) the auditory conflicting features were task-irrelevant, but its conflicting features overlapped with the response-mapping in the task (horizontal RDM task).</p><p>Cluster-based analyses across the entire T-F range revealed neural signatures of conflict processing in the theta-band when the content of the auditory stimulus was task-relevant (content discrimination task II: p&lt;0.001, one-sided: <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 4–10 Hz, peak frequency: 4 Hz, time range: 328–953 ms, peak time: 438 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>) and when the volume of the auditory stimulus was task-relevant (volume oddball task: p=0.03, one-sided: <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–6 Hz, peak frequency: 2 Hz, time range: 234–516 ms, peak time: 438 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>: no training effects for the volume oddball task). Both observations of congruency decoding are in line with the presence of conflict-related behavioral effects in these tasks (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). No significant clusters of above-chance classifier accuracy were found after correcting for multiple comparisons in the location discrimination task and the horizontal RDM task (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). However, a hypothesis-driven analysis focused on the post-stimulus theta-band (2–8 Hz, 100–700 ms) revealed that congruency decoding accuracies within this ROI were significantly above chance for both tasks as well (location discrimination: <italic>t</italic>(22) = 2.00, p=0.03, <italic>d</italic> = 0.42; horizontal RDM: <italic>t</italic>(22) = 2.89, p<italic>=</italic>0.004, d = 0.60). Thus, we observed that conflict of the auditory stimulus is detected when one of the auditory conflicting features is task-relevant (content and location discrimination tasks), when one of its non-conflicting features is task-relevant (volume oddball task), and when none of the auditory features is task-relevant but these features overlap with the response-mapping of the task (horizontal RDM task). To qualify the differences between tasks, we combine the data from all experiments and compare effect sizes across tasks at the end of this section (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Processing of sensory and conflict features for different levels of task relevance.</title><p>(<bold>A</bold>) Summary of the decoding results of all behavioral tasks, sorted by congruency decoding effect size (Cohen’s <italic>d</italic><sub>z</sub>) in a preselected time-frequency ROI. The data in these plots are identical to the ones shown in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig4">4</xref>. (<bold>B</bold>) Effect sizes are shown for all task/feature combinations derived from a predefined ROI (2–8 Hz and 100–700ms) and sorted according to effect size of congruency decoding. Effect sizes for congruency decoding were dependent on behavioral task (downward slope of the green line), whereas this was not the case, or less so, for the decoding of content and location. The data can be found in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>. CD II: content discrimination task II; CD I: content discrimination task I; HRDM: horizontal RDM task; VO: volume oddball detection task; LD: location discrimination task; VRDM: vertical RDM task; n.s.: p&gt;0.05.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Decoding results within ROI for all tasks.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64431-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Topographic maps of reconstructed activation patterns and effects sizes for alternative ROIs.</title><p>(<bold>A</bold>) Decoding weights were extracted from the ROI that was used in the final analysis (100–700 ms, 2–8 Hz). Then these weights were transformed into activation patterns by multiplying them with the covariance in the electroencephalography data. The topographical maps of the two behavioral tasks in which congruency decoding was most accurate (content discrimination tasks I and II) reveal a clear midfrontal distribution, which is commonly found in the literature (<xref ref-type="bibr" rid="bib16">Cohen and Cavanagh, 2011</xref>; <xref ref-type="bibr" rid="bib17">Cohen and van Gaal, 2014</xref>; <xref ref-type="bibr" rid="bib48">Jiang et al., 2015a</xref>; <xref ref-type="bibr" rid="bib71">Nigbur et al., 2012</xref>). (<bold>B–D</bold>) The reported results from our ROI analyses (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) could have been accidental due to the fact that the ROI was chosen on the basis of previous results. In order to exclude this possibility, we performed the exact same analyses (repeated measures ANCOVA with factors being task and feature) on data that were extracted from three different ROIs. Crucially, we found interaction effects between behavioral task and stimulus feature for all ROIs (ROI 2: <italic>F</italic>(10,402) = 20.31, p&lt;0.001, <inline-formula><mml:math id="inf39"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.34; ROI 3: <italic>F</italic>(10,402) = 12.25, p&lt;0.001, <inline-formula><mml:math id="inf40"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.23; ROI 4: <italic>F</italic>(10,402) = 14.47, p&lt;0.001, <inline-formula><mml:math id="inf41"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.27). Behavioral tasks on the x-axis are sorted by magnitude of the effect size of congruency decoding, similar to <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Note that in all figures the VRDM task is the one with the lowest effect size in congruency. Interestingly, for all ROIs the same pattern as in <xref ref-type="fig" rid="fig5">Figure 5B</xref> is visible, namely that congruency decoding deteriorates to a point where accuracies are no longer significant, whereas for decoding of sensory stimulus features this is not the case. CD II: content discrimination task II; CD I: content discrimination task I; HRDM: horizontal RDM task; VO: volume oddball detection task; LD: location discrimination task; VRDM: vertical RDM task. ± p&lt;0.06, n.s.: p&gt;0.06.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64431-fig5-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-8"><title>Experiment 2: sensory features are processed in all tasks</title><p>Next, we trained classifiers to distinguish trials based on sound location and content in order to inspect sensory processing. We found neural signatures of the processing of sound content for all four tasks: content discrimination II (p&lt;0.001, one-sided: <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–30 Hz, peak frequency: 4 Hz, time range: 203–1000 ms, peak time: 469; <xref ref-type="fig" rid="fig4">Figure 4B</xref>), location discrimination (p<italic>=</italic>0.01, one-sided: <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–6 Hz, peak frequency: 2 Hz, time range: 313–641 ms, peak time: 563 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>), horizontal RDM task (p&lt;0.001, one-sided: <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 4–16 Hz, peak frequency: 4 Hz, time range: 78–328 ms, peak time: 281 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). For the volume oddball task, we observed a delta/theta-band cluster and a late beta-band cluster (delta/theta: p&lt;0.001, one-sided: <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–8 Hz, peak frequency: 4 Hz, time range: 94–797 ms, peak time: 281 ms; late beta: p=0.01, one-sided: <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 12–20 Hz, peak frequency: 20 Hz, time range: 672–953 ms, peak time: 828 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><p>Furthermore, sound location could be decoded from the content discrimination task II (delta/theta: p<italic>=</italic>0.02, one-sided: <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–6 Hz, peak frequency: 2 Hz, time range: 453–688 ms, peak time: 609 ms; alpha: p=0.03, one-sided: <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 10–12 Hz, peak frequency: 12 Hz, time range: 531–750 ms, peak time: 578 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>), the location discrimination task (p&lt;0.001, one-sided: <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–30 Hz, peak frequency: 2 Hz, time range: 109–1000 ms, peak time: 453 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>), and the volume oddball task (p&lt;0.001, one-sided: <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, cluster-corrected; frequency range: 2–22 Hz, peak frequency: 10 Hz, time range: −47 ms to 891 ms, peak time: 469 ms; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Initially, we did not observe a significant cluster of location decoding in the horizontal RDM, although the hypothesis-driven analysis revealed that accuracies within the predefined theta-band ROI were significantly above chance level as well (<italic>t</italic>(23) = 2.47, p=0.01, <italic>d</italic> = 0.51).</p><p>One aspect of these results is worth highlighting. When participants responded to the location of the auditory stimulus, location decoding revealed a broadband power spectrum, similar to sound content decoding when sound content was task-relevant (content discrimination tasks). This broad frequency decoding may be due to the fact that these features were task-relevant, but these results may also partially reflect response preparation and response execution processes as these auditory features were directly associated with a specific response. In order to test whether the earliest sensory responses were already modulated by task relevance and to link this to previous event-related potential (ERP) studies (<xref ref-type="bibr" rid="bib1">Alilović et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Molloy et al., 2015</xref>; <xref ref-type="bibr" rid="bib111">Woldorff et al., 1993</xref>), we performed an additional time-domain multivariate analysis on these sensory features (T-F analyses are not well suited to address questions about the timing of processes). Because we were interested in the earliest sensory responses, we performed this analysis on data from experiment 2 only as all task parameters were best matched (e.g., in all tasks, a visual stimulus was presented, no training, etc.). We observed increased decoding for task-relevant sensory features compared to task-irrelevant features, starting ~250 ms (sound location RT: <italic>M</italic> = 338 ms) and ~330 ms (sound content RT: <italic>M</italic> = 364 ms) after stimulus presentation (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The onset of these differences starts before a response is made, which may suggest that sensory processing of these features is indeed affected by task relevance; however, processes building up towards motor execution, such as decision-making and response preparation processes, cannot be excluded as potential factors driving the higher decoding accuracies in tasks where specific features are task-relevant and hence correlated with decision and motor processes. These results are elaborated upon in the Discussion.</p><p>In conclusion, in line with the behavioral results, we observed that the processing of conflict between two stimulus features (i.e., location and content of an auditory stimulus) was present in all tasks of experiment 2. This indicates that conflict can be detected when one of the auditory conflicting features is task-relevant (content and location discrimination tasks), when one of its non-conflicting features is task-relevant (volume oddball task), and when there is overlap in the response-mapping with any of its task-irrelevant conflicting features (horizontal RDM task). Overall, this reveals that when the conflicting stimulus itself is attended or when its conflicting features overlap with the response scheme, all of its features seem to be processed and integrated to elicit conflict.</p></sec><sec id="s2-9"><title>All experiments: decreasing task relevance hampers cognitive control operations, but not sensory processing</title><p>The neural data from all six different tasks over two experiments suggests that if sensory input is task-irrelevant, processing of that information is preserved, while cognitive control operations are strongly hampered (<xref ref-type="fig" rid="fig2">Figures 2B,</xref> <xref ref-type="fig" rid="fig4">4B,</xref> and <xref ref-type="fig" rid="fig5">5B</xref>). To quantify this observation, we calculated Cohen's <italic>d</italic><sub>z</sub> for all tasks and features (based on the preselected ROI), sorted tasks according to the effect sizes of congruency decoding, and plotted Cohen's <italic>d</italic><sub>z</sub> across tasks for all features (conflict, content, and location; <xref ref-type="fig" rid="fig5">Figure 5</xref>). For each feature and task, we extracted individual classifier area under the curve (AUC) values and performed an analysis of covariance (ANCOVA) on these accuracies, with fixed effects being task and stimulus feature. We found main effects for behavioral task and stimulus feature (task: <italic>F</italic>(5,402) = 17.25, p&lt;0.001, <inline-formula><mml:math id="inf51"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.18; stimulus feature: <italic>F</italic>(2,402) = 44.61, p&lt;0.001, <inline-formula><mml:math id="inf52"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.18). Crucially, the interaction between task and stimulus feature was also significant (<italic>F</italic>(10,402) = 18.80, p&lt;0.001, <inline-formula><mml:math id="inf53"><mml:msubsup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = 0.32), showing that the accuracy of conflict decoding decreased more across tasks as compared to content and location decoding. We next performed t-tests (one-sided, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> &gt; 0.5) on ROI accuracies from every task/feature combination to assess decoding performance for all stimulus features for the different task-relevance manipulations (see <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>). We observed that congruency decoding accuracies were strongly influenced by task, whereas this was not the case for decoding accuracies of stimulus content and location (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Note that these results were robust and not dependent on the specific ROI that was selected because using other ROI windows led to similar patterns of results, that is, decreased congruency decoding under task-irrelevant input, but relatively stable sensory feature decoding (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C, D</xref>). Classifier weights were extracted from the ROI for all tasks and features, transformed to activation patterns and plotted in topomaps, to show the patterns of activity underlying the decoding results (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Although it has been hypothesized for a long time that only basic physical properties of task-irrelevant sensory input are processed (<xref ref-type="bibr" rid="bib96">Treisman and Gelade, 1980</xref>), over the past few years an overabundance of processes has been found to be preserved in the absence of attention (<xref ref-type="bibr" rid="bib26">Fahrenfort et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Li et al., 2002</xref>; <xref ref-type="bibr" rid="bib75">Peelen et al., 2009</xref>). Here, we aimed to push the limits of the brain's capacity to process unattended information and addressed whether cognitive control networks can be recruited when conflicting features of sensory input are task-irrelevant. Interestingly, similar cognitive control functions have been shown to occur when stimuli are masked and hence conscious awareness is strongly reduced (<xref ref-type="bibr" rid="bib5">Atas et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">D'Ostilio and Garraux, 2012b</xref>; <xref ref-type="bibr" rid="bib42">Huber-Huber and Ansorge, 2018</xref>; <xref ref-type="bibr" rid="bib49">Jiang et al., 2015b</xref>; <xref ref-type="bibr" rid="bib50">Jiang et al., 2018</xref>; <xref ref-type="bibr" rid="bib99">van Gaal et al., 2008</xref>; <xref ref-type="bibr" rid="bib101">van Gaal et al., 2011</xref>).</p><p>In two omnibus experiments with six different tasks, we presented stimuli with potentially auditory-spatial conflicting stimulus features (e.g., the word ‘left’ presented on the right side) to participants, whilst they were performing several behavioral tasks. These tasks manipulated whether the features sound content and location of the auditory stimulus were task-relevant and whether these features were mapped to specific overlapping responses of the primary task. We observed clear signals of conflict processing in behavior (i.e., longer RTs, increased ERs, increased sensitivity) and brain activity (i.e., above-chance decoding accuracy in the theta-band) when the conflicting features of the auditory stimulus were task-relevant, that is, in the content and location discrimination tasks, when another non-conflicting feature of the auditory stimulus was task-relevant, but the conflicting features content and location were not (volume oddball task) and when the conflicting features were not task-relevant but when they overlapped with the response scheme of the task (horizontal RDM task). When the features of the auditory stimulus were task-irrelevant and orthogonal to the response scheme of the primary task, that is, in the vertical RDM task, we did not observe any effects of conflict in behavior or neural measures. The absence of conflict effects was supported by Bayesian analyses, showing reliable evidence in favor of the null hypothesis. Strikingly, the individual stimulus features, that is, stimulus location/content, were always processed, regardless of their task relevance and response relevance. Note that this dissociation – hampered conflict processing yet preserved perceptual processing – cannot be explained by a lack of statistical power because decoding accuracy of stimulus location/content was comparable between behavioral tasks. These results highlight that relatively basic stimulus properties escape the attentional bottleneck, lending support to previous studies (e.g., <xref ref-type="bibr" rid="bib26">Fahrenfort et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Li et al., 2002</xref>; <xref ref-type="bibr" rid="bib75">Peelen et al., 2009</xref>; <xref ref-type="bibr" rid="bib86">Sand and Wiens, 2011</xref>; <xref ref-type="bibr" rid="bib96">Treisman and Gelade, 1980</xref>), but furthermore showcase that an attentional bottleneck for detecting conflict (integration of stimulus features) exists upstream in the hierarchy of cognitive processing. Below we link the observed results to the existing literature.</p><sec id="s3-1"><title>Object-based attention as a prerequisite for feature integration (leading to conflict)</title><p>So why are auditory content and location not integrated to form conflict when all of the auditory features are task-irrelevant? It has been suggested that the MFC is crucial for monitoring the presence of conflict, through the detection of coinciding inputs (<xref ref-type="bibr" rid="bib15">Cohen, 2014</xref>). In our paradigm, it thus seems crucial that information related to the auditory features content and location reaches the MFC to be able to detect conflict, although control networks can undergo reconfiguration under certain circumstances (<xref ref-type="bibr" rid="bib9">Canales-Johnson et al., 2020</xref>). Previous studies have shown that task-irrelevant stimuli can still undergo elaborate processing (<xref ref-type="bibr" rid="bib65">Li et al., 2002</xref>; <xref ref-type="bibr" rid="bib75">Peelen et al., 2009</xref>). Our decoding results show that task-irrelevant features are indeed processed by the brain (<xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig4">4B,</xref> and <xref ref-type="fig" rid="fig5">5</xref>). Interestingly, conflict between two task-irrelevant features was detected when another feature of the conflicting stimulus was task-relevant (volume oddball task) or when the conflicting features had overlap with the overall response scheme (horizontal RDM task), but remained undetected when none of the auditory features was task-relevant and there was no overlap with the response scheme (vertical RDM task). We argue that this difference is due to the fact that in the volume oddball and horizontal RDM tasks, the task-irrelevant conflicting features were selected through object-based attention. Theories of object-based attention have suggested that when one stimulus feature of an object is task-relevant and selected, attention ‘spreads’ to all other features of the attended stimulus, even when these features are task-irrelevant or part of a different stimulus or modality (<xref ref-type="bibr" rid="bib11">Chen, 2012</xref>; <xref ref-type="bibr" rid="bib13">Chen and Cave, 2006</xref>; <xref ref-type="bibr" rid="bib72">O'Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib97">Turatto et al., 2005</xref>; <xref ref-type="bibr" rid="bib108">Wegener et al., 2014</xref>; <xref ref-type="bibr" rid="bib113">Xu, 2010</xref>). In the volume oddball task, a non-conflicting feature of the auditory stimulus (volume) was task-relevant, but this allowed for the selection of the other task-irrelevant features through object-based attention. In the horizontal RDM task, on the other hand, the conflicting features of the task-irrelevant auditory stimulus overlapped with the overall response scheme or task-set of the participant, namely discriminating rightward- versus leftward-moving dots. This may have led to the automatic classification of all sensory input according to this task-set (as either coding for ‘left’ or ‘right’), even when that input was not relevant for the task at hand. Possibly, through this classification, attentional resources could be exploited for the processing of these task-irrelevant features. This is especially interesting because in all conflict analyses incongruency was defined as the mismatch between the two features of the auditory stimulus (location and sound) and not between a visual feature (leftward-moving dots) and one feature of the auditory stimulus (e.g., the word ‘right’). Note that we report additional behavioral results that show clear indications of conflict when the task-relevant feature of the visual stimulus interferes directly with a <italic>single</italic> task-irrelevant feature of the auditory task (e.g., auditory content-dot-motion conflict).</p><p>When inspecting the T-F maps for the vertical RDM task, the relatively fleeting temporal characteristics of the processing of the task-irrelevant stimulus features (sound content and location) might suggest that the integration of these features may not be possible due to a lack of time as proposed in the incremental grouping model of attention (<xref ref-type="bibr" rid="bib80">Roelfsema, 2006</xref>; <xref ref-type="bibr" rid="bib81">Roelfsema and Houtkamp, 2011</xref>). However, the time window in which conflict was decodable when the auditory conflicting features were task-relevant coincides with the time range in which these features could be decoded when the auditory conflicting features were task-irrelevant (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Therefore, it seems unlikely that the more temporally constrained processing of task-irrelevant stimulus features is the cause of hampered conflict detection. Besides time being a factor, the processing of task-irrelevant features in the vertical RDM task may have also been too constrained to (early) sensory cortices and therefore could not progress to integration networks, including the MFC, necessary for the detection of conflict. Speculatively, the processing of task-irrelevant auditory features was relatively superficial due to the relatively few remaining resources (<xref ref-type="bibr" rid="bib62">Lavie et al., 2004</xref>; <xref ref-type="bibr" rid="bib89">Sigman and Dehaene, 2006</xref>; <xref ref-type="bibr" rid="bib118">Zylberberg et al., 2010</xref>; <xref ref-type="bibr" rid="bib119">Zylberberg et al., 2011</xref>), and combined with a lack of object-based attention, this may have prevented the propagation of the information to the MFC. It has been hypothesized that unattended (sometimes referred to as ‘preconscious’; <xref ref-type="bibr" rid="bib21">Dehaene et al., 2006</xref>; <xref ref-type="bibr" rid="bib22">Dehaene and Changeux, 2011</xref>) stimuli are not propagated deeply in the brain, but still allow for shallow recurrent interactions in sensory cortices. The poor spatial resolution of EEG measurements and the specifics of our experimental setup, however, do not allow to test these ideas regarding the involvement of spatially distinct cortices. Yet, previous work of our group suggests that task-irrelevant nonconscious information does not propagate to the frontal cortices, whereas task-relevant nonconscious information does. We demonstrated that masked task-irrelevant conflicting cues induced similar early processing in sensory cortices as compared to masked task-relevant cues, but prohibit activation of frontal cortices (<xref ref-type="bibr" rid="bib99">van Gaal et al., 2008</xref>). These findings are not conclusive, and so we believe that uncovering the role of task relevance in processing of (nonconscious) information deserves more attention in future work (see also <xref ref-type="bibr" rid="bib102">van Gaal et al., 2012</xref> for a discussion on this issue).</p></sec><sec id="s3-2"><title>Sensory processing is weakened, but conflict processing hampered in the absence of task relevance</title><p>We show that conflict processing is absent when conflicting features are fully task-irrelevant, while evidence of sensory processing is still present in neural data (<xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig4">4B,</xref> and <xref ref-type="fig" rid="fig5">5</xref>). Even though sensory processing of auditory features seems relatively preserved under various levels of task relevance of these features, it appears that sensory processing may in fact also be affected when the feature is task-irrelevant (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), in line with previous studies (e.g., <xref ref-type="bibr" rid="bib1">Alilović et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Jehee et al., 2011</xref>; <xref ref-type="bibr" rid="bib55">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib57">Kouider et al., 2016</xref>), although to a lesser extent than conflict processing (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). For example, when sound location is the task-relevant feature (i.e., in the location discrimination task), decoding accuracies for that feature are more broadband in the frequency domain (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) and higher in the time domain (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), compared to location decoding performance in other tasks. This increased decoding accuracy is present even before a response has been made, suggesting decreased early stage sensory processing in tasks where the decoded feature is not task-relevant. However, despite sensory processing being weakened under decreasing levels of task relevance, it is not diminished, in line with previous findings of ongoing processing in the (near) absence of attention (<xref ref-type="bibr" rid="bib26">Fahrenfort et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Li et al., 2002</xref>; <xref ref-type="bibr" rid="bib75">Peelen et al., 2009</xref>). Processing of conflict between the two interfering auditory features, on the contrary, is hampered when the features are fully task-irrelevant. This is further supported by the significant interaction between task and feature in terms of decoding performance within the predefined ROI (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Summarizing, although processing of sensory features is degraded under decreasing levels of task relevance it is present regardless of attention, whereas detection of conflict between these features is no longer possible when the features are fully task-irrelevant.</p><p>Besides object-based attention, the process through which attentional resources are allocated to the processing of task-irrelevant features of a task-relevant object, other mechanisms might also play a role in the extent to which sensory information is processed, such as the active suppression of task-irrelevant information. It has been shown that task-irrelevant information that is response-relevant, and can thus potentially interfere with performance on the primary task, can be suppressed to minimize interference (<xref ref-type="bibr" rid="bib4">Appelbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Janssens et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Polk et al., 2008</xref>; but see <xref ref-type="bibr" rid="bib25">Egner and Hirsch, 2005</xref>). This would result in more reduced sensory processing, indexed by lower decoding performance, for task-irrelevant features that are response-relevant than for task-irrelevant features that are not. Disentangling the effects of such mechanisms, object-based attention and their possible interactions on the processing of sensory and cognitive information, however, falls outside the scope of this work.</p></sec><sec id="s3-3"><title>Disentangling effects of conflict and task difficulty</title><p>For our main analysis, we trained a multivariate classifier on congruent versus incongruent trials and observed effects of task relevance of the performance of the classifier, that is, decoding performance was hampered when conflicting features were fully task-irrelevant (<xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig4">4B,</xref> and <xref ref-type="fig" rid="fig5">5B</xref>). Moreover, we report behavioral effects of conflict in all <italic>auditory</italic> tasks as well (<xref ref-type="fig" rid="fig2">Figures 2A</xref> and <xref ref-type="fig" rid="fig4">4A</xref>). Given that behavioral performance on the auditory tasks is worse for incongruent trials as compared to congruent trials, one may wonder whether our multivariate decoder is in fact picking up information related to conflict detection or processes related to task difficulty. Whether medial frontal theta-band oscillations are a reflection of conflict detection or task difficulty, and whether these factors can be dissociated in principle, has been the topic of debate in the literature (<xref ref-type="bibr" rid="bib35">Grinband et al., 2011a</xref>; <xref ref-type="bibr" rid="bib36">Grinband et al., 2011b</xref>; <xref ref-type="bibr" rid="bib68">McKay et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">Ruggeri et al., 2019</xref>; <xref ref-type="bibr" rid="bib114">Yeung et al., 2011</xref>). On the one hand, it has been shown that activity in the dorsal medial prefrontal cortex is related to RT, suggesting that neural markers of conflict may in fact reflect time on task (<xref ref-type="bibr" rid="bib36">Grinband et al., 2011b</xref>; <xref ref-type="bibr" rid="bib36">Grinband et al., 2011b</xref>; <xref ref-type="bibr" rid="bib84">Ruggeri et al., 2019</xref>). On the contrary, other research has shown that enhanced prefrontal theta-band oscillations are found in conflicting trials even when controlling for RT (<xref ref-type="bibr" rid="bib17">Cohen and van Gaal, 2014</xref>) or task difficulty (<xref ref-type="bibr" rid="bib68">McKay et al., 2017</xref>). The decoding results presented in this work likely reflect conflict processing, and not just task difficulty, for two reasons. First, the spatial distribution and time-frequency dynamics of the congruency decoding results are comparable to those more commonly found in the literature on conflict processing, even in a study where conflicting signals were matched for RT (<xref ref-type="bibr" rid="bib17">Cohen and van Gaal, 2014</xref>). Specifically, using the content discrimination task of experiment 1 as example, we observe effects of conflict centered on the theta-band and ~230–610 ms post-conflict presentation, with a clear medial frontal spatial profile (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>). Second, auditory stimulus conflict was decodable from neural data for two tasks in which there were either no effects of conflict – or task difficulty – on behavioral performance (i.e., horizontal RDM task), or even increased behavioral performance on conflicting trials (i.e., volume oddball task). Therefore, we believe that the observed congruency decoding results presented here are mainly driven by the detection of conflicting sensory inputs and are not, or much less so, driven by task difficulty.</p></sec><sec id="s3-4"><title>Conflict between features of a task-irrelevant stimulus versus conflict between stimuli</title><p>Contrary to the current study, previous studies using a variety of conflict-inducing paradigms and attentional manipulations reported conflict effects in behavior and electrophysiological recordings induced by unattended stimuli or stimulus features (<xref ref-type="bibr" rid="bib67">Mao and Wang, 2008</xref>; <xref ref-type="bibr" rid="bib74">Padrão et al., 2015</xref>; <xref ref-type="bibr" rid="bib117">Zimmer et al., 2010</xref>). However, our study deviates from those studies in several crucial aspects. First, we explicitly separate task-relevant stimulus features that cause conflict and task-relevant features that do not, parsing the cognitive components that induce cognitive control in this context. Furthermore, in the RDM and volume oddball tasks we tested whether conflict between two task-irrelevant features could be detected by the brain. Specifically, we investigated if conflict between two task-irrelevant features would be detected in the presence or absence of object-based attention (e.g., volume oddball task vs. vertical RDM task), also manipulating whether task-irrelevant conflicting features mapped onto the response or not (horizontal RDM task vs. vertical RDM task). This approach is crucially different from previous studies that exclusively tested whether a task-irrelevant or unattended stimulus (feature) could interfere with processing of a task-relevant feature (<xref ref-type="bibr" rid="bib67">Mao and Wang, 2008</xref>; <xref ref-type="bibr" rid="bib74">Padrão et al., 2015</xref>; <xref ref-type="bibr" rid="bib117">Zimmer et al., 2010</xref>). Under such conditions, at least one source contributing to the generation of conflict (i.e., the task-relevant stimulus) is fully attended, and therefore, one cannot claim that under those circumstances conflict detection occurs outside the scope of attention.</p><p>It can be argued that in our horizontal RDM task the task-irrelevant auditory features (location and content) that mapped onto the response of the primary task could interfere with the processing of horizontal dot-motion, that is, the task-relevant feature. This is in fact true, as we found effects of auditory content-dot-motion and auditory location-dot-motion conflict in behavior (both on RTs and ERs). This highlights that a single feature of a task-irrelevant stimulus can interfere with the response to a task-relevant stimulus when there are overlapping feature-response-mappings. This is different from two features of a task-irrelevant stimulus to produce inherent conflict (e.g., between auditory content and location), which is what we specifically investigated by always testing the presence of auditory content-location conflict only. A similar argument might be made for our vertical RDM and volume oddball tasks because in those cases the auditorily presented stimuli could potentially conflict with responses that were exclusively made with the right hand, for example, the spoken word ‘left’ or the sound from left location may conflict generally more with a right-hand response (independent of the up/down classification or oddball detection) than the spoken word ‘right’ or the sound from right location. In the vertical RDM task, the auditorily presented stimuli were truly task-irrelevant as both stimulus content and location in isolation did not affect behavior. In the volume oddball task, sound content and location were task-irrelevant features, but these features were part of the attended stimulus and hence selected through object-based attention. In this task, the content of the auditory stimuli (e.g., ‘left’) did interfere with right-hand responses to the volume oddball task, resulting in longer RTs (compared to ‘right’). Moreover, in this task we did find behavioral and neural effects of conflict between two auditory features (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The absence of conflict effects in the vertical RDM and presence of such effects in the volume oddball task and horizontal RDM indicates that at least one feature of the stimulus containing the conflicting features should be task-relevant or associated with a response in order for conflict to be detected. Summarizing, we show that the brain is not able to detect conflict that emerges between two features of a task-irrelevant stimulus in the absence of object-based attention.</p><p>Lastly, in other studies, conflicting stimuli were often task-irrelevant on one trial (e.g., because they were presented at an unattended location) but task-relevant on the next (e.g., because they were presented at the attended location) (e.g., <xref ref-type="bibr" rid="bib74">Padrão et al., 2015</xref>; <xref ref-type="bibr" rid="bib117">Zimmer et al., 2010</xref>). Such trial-by-trial fluctuations of task relevance allow for across-trial modulations to confound any current trial effects (e.g., conflict-adaptation effect) and also induce a ‘stand-by attentional mode’ where participants never truly disengage to be able to determine if a stimulus is task-relevant. We prevented such confounding effects in the present study, where the (potentially) conflicting features or the auditory stimulus were task-irrelevant on every single trial in the vertical RDM, horizontal RDM, and volume oddball task.</p></sec><sec id="s3-5"><title>Differences between response conflict and perceptual conflict cannot account for absence of conflict detection in task-irrelevant sensory input</title><p>One difference between the content and location discrimination tasks, on the one hand, and the volume oddball and RDM tasks, on the other, was the task relevance of the (conflicting) auditory features. Another major difference between these groups of tasks was, consequently, the origin of the conflict. When the auditory stimuli were task-relevant, the origin of conflict was found in the interference of a task-irrelevant feature on behavioral performance, whereas for the other tasks this was not the case. We argued that in the volume oddball and RDM tasks salient auditory stimuli could be <italic>intrinsically</italic> conflicting. Intrinsic conflict is often referred to as perceptual conflict, as opposed to the aforementioned behavioral conflict (<xref ref-type="bibr" rid="bib56">Kornblum, 1994</xref>). Although perceptual conflict effects are usually weaker than response conflict effects, both in behavior and electrophysiology (<xref ref-type="bibr" rid="bib29">Frühholz et al., 2011</xref>; <xref ref-type="bibr" rid="bib105">van Veen et al., 2001</xref>; <xref ref-type="bibr" rid="bib107">Wang et al., 2014</xref>), this difference in the origin of the conflict is unlikely to explain why we did not observe effects of conflict under task-irrelevant sensory input, as opposed to earlier studies.</p><p>First, several neurophysiological studies have previously reported electrophysiological modulations by perceptual conflict centered on the MFC (<xref ref-type="bibr" rid="bib48">Jiang et al., 2015a</xref>; <xref ref-type="bibr" rid="bib71">Nigbur et al., 2012</xref>; <xref ref-type="bibr" rid="bib105">van Veen et al., 2001</xref>; <xref ref-type="bibr" rid="bib107">Wang et al., 2014</xref>; <xref ref-type="bibr" rid="bib116">Zhao et al., 2015</xref>). Second, an earlier study using a task similar to ours (but including only task-relevant stimuli) showed effects of perceptual conflict, that is, unrelated to response-mapping, in both behavior and neural measures (<xref ref-type="bibr" rid="bib8">Buzzell et al., 2013</xref>). Third, the prefrontal monitoring system has previously been observed to respond when participants view other people making errors (<xref ref-type="bibr" rid="bib44">Jääskeläinen et al., 2016</xref>; <xref ref-type="bibr" rid="bib104">van Schie et al., 2004</xref>), suggesting that cognitive control can be triggered without the need to make a response. Fourth, in our volume oddball task, where conflict was perceptual in nature as well, we did observe conflict effects, both in behavior and neural data.</p></sec><sec id="s3-6"><title>Inattentional deafness or genuine processing of stimulus features?</title><p>The lack of conflict effects in the vertical RDM task might suggest a case of inattentional deafness, a phenomenon known to be induced by demanding visual tasks, which manifests itself in weakened early (~100 ms) auditory evoked responses (<xref ref-type="bibr" rid="bib69">Molloy et al., 2015</xref>). Interestingly, human speech seems to escape such load modulations and is still processed when unattended and task-irrelevant (<xref ref-type="bibr" rid="bib73">Olguin et al., 2018</xref>; <xref ref-type="bibr" rid="bib82">Röer et al., 2017</xref>; <xref ref-type="bibr" rid="bib115">Zäske et al., 2016</xref>), potentially because of its inherent relevance, similar to (other) evolutionary relevant stimuli such as faces (<xref ref-type="bibr" rid="bib28">Finkbeiner and Palermo, 2017</xref>; <xref ref-type="bibr" rid="bib61">Lavie et al., 2003</xref>). Indeed, the results of our multivariate analyses demonstrate that spoken words are processed (at least to some extent) when they are task-irrelevant as stimulus content (the words ‘left’ and ‘right’, middle row, <xref ref-type="fig" rid="fig2">Figures 2B</xref> and <xref ref-type="fig" rid="fig4">4B</xref>, and <xref ref-type="fig" rid="fig5">Figure 5B</xref>) and stimulus location (whether the word was presented on the left or the right side’, bottom row, <xref ref-type="fig" rid="fig2">Figures 2B</xref> and <xref ref-type="fig" rid="fig4">4B</xref>, and <xref ref-type="fig" rid="fig5">Figure 5B</xref>) could be decoded from time-frequency data for all behavioral tasks. For all tasks, classification of stimulus content was present in the theta-band (4–8 Hz), which is in line with a previously proposed theoretical role for theta oscillations in speech processing, namely that they track the acoustic envelope of speech (<xref ref-type="bibr" rid="bib31">Giraud and Poeppel, 2012</xref>). After this initial processing stage, further processing of stimulus content is reflected in more durable broadband activity for the content discrimination tasks, possibly related to higher-order processes (e.g., semantic) and response preparation (middle row and left column, <xref ref-type="fig" rid="fig2">Figures 2B</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Similarly to processing of stimulus content, processing of stimulus location was most strongly reflected in the delta- to theta-range for all tasks (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), which may relate to the auditory N1 ERP component, an ERP signal that is modulated by stimulus location (<xref ref-type="bibr" rid="bib30">Fuentemilla et al., 2006</xref>; <xref ref-type="bibr" rid="bib64">Lewald and Getzmann, 2011</xref>; <xref ref-type="bibr" rid="bib85">Salminen et al., 2015</xref>). We also observed above-chance location decoding in the alpha-band for task-relevant auditory stimuli, convergent with the previously reported role of alpha-band oscillations in orienting and allocating audio-spatial attention (<xref ref-type="bibr" rid="bib109">Weisz et al., 2014</xref>).</p><p>Thus, the characteristics of the early sensory processing of (task-irrelevant) auditory stimulus features in our study are in line with recent findings of auditory and speech processing. Moreover, our observations are in line with recent empirical findings that suggest a dominant role for late control operations, as opposed to early selection processes, in resolving conflict (<xref ref-type="bibr" rid="bib43">Itthipuripat et al., 2019</xref>). Specifically, this work showed that in a Stroop-like paradigm both target and distractor information is analyzed fully, after which the conflicting input is resolved. Extending on this, we show preserved initial processing of task-irrelevant sensory input, but hampered late control operations necessary to detect conflict, at least in the current setup.</p></sec><sec id="s3-7"><title>Automatization of conflict processing does not promote detection of task-irrelevant conflict</title><p>Previous studies investigating conflict through masking procedures concluded that conflict detection by the MFC may happen automatically and is still operational under strongly reduced levels of stimulus visibility (<xref ref-type="bibr" rid="bib20">D'Ostilio and Garraux, 2012b</xref>; <xref ref-type="bibr" rid="bib49">Jiang et al., 2015b</xref>; <xref ref-type="bibr" rid="bib99">van Gaal et al., 2008</xref>). Such automaticity can often be enhanced through training of the task. For example, training in a stop-signal paradigm in which stop-signals were rendered unconscious through masking led to an increase in the strength of behavioral modulations of these stimuli (<xref ref-type="bibr" rid="bib100">van Gaal et al., 2009</xref>). In order to see whether enhancing such automaticity could hypothetically increase the likelihood of conflict detection, we included extensive training sessions in the first experiment and had measurements of the volume oddball task before and after exposure to conflicting tasks in the second experiment. In experiment 1, we found no neural effects of conflict detection in the vertical RDM task, even when participants had been trained on the auditory task for 3600 trials (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>). Training did result in a decrease of behavioral and neural conflict effects in content discrimination task I of experiment 1, indicating that our training procedure was successful (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A–C</xref>) and suggesting more efficient functioning of conflict resolution mechanisms. In experiment 2, participants performed the volume oddball task twice, once before and once after sound location and content had been mapped to responses. Again, we aimed to see if training on conflict tasks would enhance automaticity of conflict processing in a paradigm where the auditory conflicting features were task-irrelevant. We did not find any statistically reliable differences in behavioral conflict effects or accuracy of congruency decoding between the two runs (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Therefore, it seems that the automaticity of conflict detection by the MFC and associated networks does not hold when the auditory stimulus is task-irrelevant (at least after the extent of training and exposure as studied here).</p></sec><sec id="s3-8"><title>Increased detection performance on conflicting trials</title><p>Remarkably, we report increased behavioral performance on the volume oddball task for incongruent trials as compared to congruent trials (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B–D</xref>). Speculatively, this increased behavioral performance (d’) on incongruent trials may be due to attentional capture of conflicting stimuli. Attentional capture is the involuntary shift of attention towards salient stimuli (<xref ref-type="bibr" rid="bib6">Awh et al., 2012</xref>; <xref ref-type="bibr" rid="bib94">Theeuwes, 2010</xref>). Possibly, the detection of conflict between sound content and location is a salient event causing (re-)capture of attention towards the auditory stimulus, resulting in better processing of the stimulus information and ultimately better oddball detection performance. Thus, following the detection of conflict, frontal networks would have to exert control over attentional resources and direct them towards the source of the conflict. Interestingly, cases of frontal control over attentional processes have been demonstrated in the past, for example, showing that task-irrelevant distractors that have been related to reward induce stronger attentional capture (<xref ref-type="bibr" rid="bib3">Anderson et al., 2011</xref>) and that high working memory load increases the strength of attentional capture by distractors (<xref ref-type="bibr" rid="bib63">Lavie and Fockert, 2006</xref>). The present study was however not optimized to test directly which underlying mechanisms are associated with increased sensitivity of conflicting sensory input, and this issue merits further experimentation.</p></sec><sec id="s3-9"><title>Conclusion</title><p>Summarizing, high-level cognitive processes that require the integration of conflict inducing stimulus features are strongly hampered when none of the stimulus features of the conflict inducing stimulus are task-relevant and hence object-based attention is absent. This work nicely extends previous findings of perceptual processing outside the scope of attention (<xref ref-type="bibr" rid="bib75">Peelen et al., 2009</xref>; <xref ref-type="bibr" rid="bib86">Sand and Wiens, 2011</xref>; <xref ref-type="bibr" rid="bib87">Schnuerch et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Tusche et al., 2013</xref>), but suggests crucial limitations of the brain’s capacity to process task-irrelevant ‘complex’ cognitive control-initiating stimuli, indicative of an attentional bottleneck to detect conflict at high levels of information analysis. In contrast, the processing of more basic physical features of sensory input appears to be less deteriorated when input is task-irrelevant (<xref ref-type="bibr" rid="bib58">Lachter et al., 2004</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We performed two separate experiments, each containing multiple behavioral tasks. For each of these experiments, we recruited 24 healthy human participants from the University of Amsterdam. None of the participants took part in both experiments. So, in total 48 participants (37 females) aged 18–30 participated in this experiment for monetary compensation or participant credits. All participants had normal or corrected-to-normal vision and had no history of head injury or physical and mental illness. This study was approved by the local ethics committee of the University of Amsterdam, and written informed consent was obtained from all participants after explanation of the experimental protocol. We will describe the experimental design and procedures for the two experiments separately. Data analyses and statistics were similar across experiments and will be discussed in the same section.</p></sec><sec id="s4-2"><title>Experiment 1: design and procedures</title><p>Participants performed two tasks in which conflicting auditory stimuli were either task-relevant or task-irrelevant. In both tasks, conflict was elicited through a paradigm adapted from <xref ref-type="bibr" rid="bib8">Buzzell et al., 2013</xref>, in which spatial information and content of auditory stimuli could interfere. In content discrimination task I, participants had to respond to the auditory stimuli, whereas in vertical RDM task participants had to perform a demanding RDM task, while the auditory conflicting stimuli were still presented (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Participants performed both tasks on two experimental sessions of approximately 2.5 hr. In between these two experimental sessions, participants had two training sessions of 1 hr during which they only performed the task-relevant task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). On each experimental session, participants first performed a shortened version of the RDM task to determine the appropriate coherency of the moving dots (73–77% correct), followed by the actual task-irrelevant auditory task, and finally the task-relevant auditory task. Participants were seated in a darkened, sound-isolated room, 50 cm from a 69 × 39 cm screen (frequency: 120 Hz, resolution: 1920 × 1080, RGB: 128, 128, 128). Both tasks were programmed in MATLAB (R2012b, The MathWorks, Inc), using functionalities from Psychtoolbox (<xref ref-type="bibr" rid="bib52">Kleiner et al., 2007</xref>).</p></sec><sec id="s4-3"><title>Experiment 1: behavioral tasks</title><sec id="s4-3-1"><title>Auditory content discrimination task I</title><p>In the task-relevant auditory conflict task, the spoken words ‘links’ (i.e., ‘left’ in Dutch) and ‘rechts’ (i.e., ‘right’ in Dutch) were presented through speakers located on both sides of the participant (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Auditory stimuli were matched in duration and sample rate (44 kHz) and were recorded by the same male voice. By presenting these stimuli through either the left or the right speaker, content-location conflict arose on specific trials (e.g., the word ‘left’ through the right speaker). Trials were classified accordingly as either congruent (i.e., location and content are the same) or incongruent (i.e., location and content are different). Participants were instructed to respond as fast and accurate as possible by pressing left (‘a’) or right (‘l’) on a keyboard located in front of the participants, according to the stimulus content, ignoring stimulus location. Responses had to be made with the left or right index finger, respectively. The task was divided into 12 blocks of 100 trials each, allowing participants to rest in between blocks. After stimulus presentation, participants had a 2 s period in which they could respond. A variable inter-trial interval between 850 and 1250 ms was initiated directly after the response. If no response was made, the subsequent trial would start after the 2 s response period. Congruent and incongruent trials occurred equally often (i.e., 50% of all trials) as expectancy of conflict has been shown to affect conflict processing (<xref ref-type="bibr" rid="bib91">Soutschek et al., 2015</xref>). Due to an error in the script, there was a disbalance in the amount of trials coming from the left (70%) versus right (30%) speaker location for the first 14 participants. However, the amount of congruent versus incongruent and ‘left’ versus ‘right’ trials was equally distributed. For the upcoming analyses, all trial classes were balanced in trial count.</p></sec></sec><sec id="s4-4"><title>Vertical random-dot-motion task</title><p>In the task-irrelevant auditory task, participants performed an RDM task in which they had to discriminate the motion (up vs. down) of white dots (n = 603) presented on a black circle (RGB: 0, 0, 0; ~14° visual angle; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Onset of the visual stimulus was paired with the presentation of the auditory conflicting stimulus. Participants were instructed to respond according to the direction of the dots by pressing the ‘up’ or ‘down’ key on a keyboard with their right hand as fast and accurate as possible. Again, participants could respond in a 2 s time interval, which was terminated after responses, and followed by an inter-trial interval of 850–1250 ms. Task difficulty, in terms of dot-motion coherency (i.e., proportion of dots moving in the same direction), was titrated between blocks to 73–77% correct of all trials within that block. Similar to content discrimination task I, the vertical RDM was divided into 12 blocks containing 100 trials each, separated by short breaks. Again, congruent and incongruent trials, with respect to the auditory stimuli, occurred equally often.</p></sec><sec id="s4-5"><title>Experiment 2: design and procedures</title><p>In the second experiment, we wanted to investigate whether it is task irrelevance of the auditory stimulus itself or task irrelevance of the auditory features (i.e., content and location) that determine whether prefrontal control processes are hampered. Participants performed two tasks in which auditory stimuli were fully task-relevant (location discrimination and content discrimination), one task in which the auditory stimulus was relevant but the features auditory location and content were not (volume oddball) and one task in which the auditory stimulus itself was task-irrelevant but its features location and content could potentially interfere with behavior (horizontal RDM). Participants came to the lab for one session, lasting 3 hr. Each session started with the volume oddball task, followed by (in a counterbalanced order) the location discrimination, content discrimination, and horizontal RDM tasks, and ended with a block of the volume oddball task again.</p><p>We included the location discrimination and content discrimination tasks both to replicate the results of experiment 1 and also to see if there would be differences in these results between the two tasks. Specifically, we investigated whether processing of auditory stimulus features – as indicated by multivariate classification accuracies – would differ between the two tasks. Participants were seated in a darkened, sound-isolated room, 50 cm from a 69 × 39 cm screen (frequency: 120 Hz, resolution: 1920 × 1080, RGB: 128, 128, 128). All four experiments were programmed in Python 2.7 using functionalities from PsychoPy (<xref ref-type="bibr" rid="bib76">Peirce et al., 2019</xref>).</p></sec><sec id="s4-6"><title>Experiment 2: behavioral tasks</title><sec id="s4-6-1"><title>Auditory content discrimination task (II)</title><p>The auditory content discrimination task from experiment 2 is a (near identical) replication of the auditory content discrimination task from experiment 1. Participants were fixating on a fixation mark in the center of the screen. Again, the spoken words ‘links’ (i.e., ‘left’ in Dutch) and ‘rechts’ (i.e., ‘right’ in Dutch) were presented through speakers located on both sides of the participant. Participants were instructed to respond according to the stimulus content by pressing left (‘A’) or right (‘L’) on a keyboard located in front of them, with their left and right index fingers, respectively. Concurrently, on every trial, a black disk with randomly moving dots (coherence: 0) was presented to keep sensory input similar between tasks. After stimulus presentation, participants had an 800 ms period in which they could respond. After a response, the response window would be terminated directly. A variable inter-trial interval (ITI) between 250 and 450 ms was initiated directly after the response. If no response was made, the subsequent trial would start after the ITI. All stimulus features (i.e., sound content, location, and congruency) were presented in a balanced manner (e.g., 50% congruent, 50% incongruent). The task was divided into six blocks of 100 trials each, allowing participants to rest in between blocks.</p></sec></sec><sec id="s4-7"><title>Auditory location discrimination task</title><p>The auditory location discrimination task was identical to the auditory content discrimination task II, with the exception that participants were now instructed to respond according to the location of the auditory stimulus. Thus, participants had to press a left button (‘A’) for sounds coming from a left speaker and right button (‘L’) for sounds coming from a right speaker. Again, participants performed six blocks of 100 trials.</p></sec><sec id="s4-8"><title>Volume oddball task</title><p>In the volume oddball task, the same auditory stimuli were presented. Again, on every trial, a black disk with randomly moving dots (coherence: 0) was presented to keep sensory input similar between tasks. Occasionally an auditory stimulus would be presented at a lower volume. The initial volume of the oddballs was set to 70%, but was staircased in between blocks to yield 83–87% correct answers. If participants' performance on the previous block was below or above this range, the volume increased or decreased with 5%, respectively. The odds of a trial being an oddball trial were 1/8 (drawn from a uniform distribution). Participants were instructed to detect these oddballs by pressing the spacebar as fast as possible whenever they thought they heard a volume oddball. If they thought that the stimulus was presented at a normal volume, they were instructed to refrain from responding. The response interval was 800 ms, which was terminated at response. A variable inter-trial interval of 150–350 ms was initiated after this response interval. Participants performed two runs of this task, at the beginning of each session and at the end of each session. Each run contained five blocks of 100 trials each.</p></sec><sec id="s4-9"><title>Horizontal RDM task</title><p>In the horizontal RDM task, participants had to discriminate the motion (left vs. right) of white dots (n = 603) presented on a black circle (RGB: 0, 0, 0; ~14° visual angle). Onset of the visual stimulus was paired with the presentation of the auditory stimulus. Participants were instructed to respond according to the direction of the dots by pressing a left key (‘A’) or right key (‘L’) on a keyboard with left and right index fingers, respectively, as fast and accurate as possible. Participants could respond in an 800 ms time interval, which was terminated after responses, and followed by an inter-trial interval of 250–450 ms. Task difficulty, in terms of dot-motion coherency (i.e., proportion of dots moving in the same direction), was set to 0.3 in the first block. This value indicated an intermediate coherence as every trial in that block could be the intermediate coherence, but also half (0.15) or twice this intermediate coherence (0.6). The intermediate coherence was titrated between blocks to 73–77% correct. If behavioral performance fell outside that range, 0.01 was added to or subtracted from the intermediate coherence. The horizontal RDM consisted of 10 blocks of 60 trials. All stimulus features (i.e., sound content, location, and congruency) were presented in a balanced manner (e.g., 50% congruent, 50% incongruent).</p></sec><sec id="s4-10"><title>Data analysis</title><p>We were primarily interested in the effects of congruency of the auditory stimuli on both behavioral and neural data. Therefore, we defined trial congruency on the basis of these auditory stimuli in all behavioral tasks of the two experiments. All behavioral analyses were programmed in MATLAB (R2017b, The MathWorks, Inc).</p></sec><sec id="s4-11"><title>Statistical analysis of behavioral data</title><p>In all tasks, trials with an RT &lt;100 ms or &gt;1500 ms were excluded from behavioral analyses. Missed trials were excluded in all tasks (except the volume oddball task) as well. In order to investigate whether current trial conflict effects were present under varying levels of task relevance and to inspect if training on/exposure to conflict-inducing tasks modulated such conflict effects, we performed rm-ANOVAs on different behavioral measures. For all tasks, excluding the volume oddball task, the rm-ANOVAs were performed on the ER over all trials and RTs on correct trials. For the volume oddball task, perceptual sensitivity (d’; <xref ref-type="bibr" rid="bib34">Green and Swets, 1966</xref>) and RTs of correct trials (i.e., ‘hit’ trials) were analyzed with rm-ANOVAs. If the assumption of sphericity was violated, we applied a Greenhouse–Geisser correction. For the tasks from experiment 1, we performed these ANOVAs with task relevance, training (before vs. after) and current trial congruency as factors (2 × 2 × 2 factorial design). Additional post-hoc rm-ANOVAs, for content discrimination task I and vertical RDM task separately (2 × 2 factorial design), were used to inspect the origin of significant factors that were modulated by task relevance.</p><p>For content discrimination task I, the location discrimination task, and horizontal RDM task, we performed a rm-ANOVA with task and congruency as factors (3 × 2 factorial design). For the volume oddball task, we performed a rm-ANOVA with congruency and run number as factors (2 × 2 factorial design) on RTs, d’ scores, hit rates, and false alarm rates. We also applied paired sample t-tests comparing the difference in these variables between incongruent and congruent for all tasks, within each experimental session (vertical RDM, content discrimination task I) and run (volume oddball task).</p><p>To test interference of the <italic>individual</italic> auditory features, sound content and sound location, on performance on the vertical RDM task and volume oddball task, we performed rm-ANOVAs on RTs with sound location and sound content as factors (2 × 2 factorial design). Additionally, for the horizontal RDM, we tested for auditory-visual conflict effects (i.e., conflict between sound content/location and dot direction) in RT and ER with paired sample t-tests comparing incongruent and congruent trials.</p><p>In case of null findings, we performed a Bayesian analysis (rm-ANOVA or paired sample t-test) with identical parameters and settings on the same data to test if there was actual support of the null hypothesis (<xref ref-type="bibr" rid="bib46">JASP Team, 2018</xref>).</p></sec><sec id="s4-12"><title>Analysis of EEG data</title><p>EEG data were analyzed using custom-made software written in MATLAB, with support from the toolboxes EEGLAB (<xref ref-type="bibr" rid="bib23">Delorme and Makeig, 2004</xref>) and ADAM (<xref ref-type="bibr" rid="bib27">Fahrenfort et al., 2018</xref>).</p></sec><sec id="s4-13"><title>Acquisition and preprocessing</title><p>EEG data were recorded with a 64-channel BioSemi apparatus (BioSemi B.V., Amsterdam, The Netherlands) at 512 Hz. Vertical eye movements were recorded with electrodes located above and below the left eye, and horizontal eye movements were recorded with electrodes located at the outer canthi of the left and the right eye. All EEG traces were re-referenced to the average of two electrodes located on the left and right earlobes (mastoidal reference for one participant in experiment 1). The data were band-pass filtered offline, with cutoff frequencies of 0.01–50 Hz. Next, epochs were created by taking data from −1 s to 2 s around onset of stimulus presentation. We then rejected epochs containing blink artifacts and high-voltage artifacts. Blinks were defined as VEOG data exceeding a threshold of ±100 mV in a time window of 0–800 ms post-stimulus. This procedure resulted in the removal of 5.03% (SD = 9.71%) of epochs in experiment 1% and 6.10% (<italic>SD</italic> = 7.54%) of epochs in experiment 2. Subsequently, high-voltage artifacts were defined as events where voltage exceeded a threshold of ±300 mV in a time window of 0–800 ms post-stimulus on any EEG channel. With this second round of artifact rejection, 3.65% (<italic>SD</italic> = 7.96%) of all epochs were removed in experiment 1% and 4.16% (<italic>SD</italic> = 8.86%) in experiment 2. In total, 8.68% (<italic>SD</italic> = 12.09%) of all trials were removed in experiment 1% and 10.26% (<italic>SD</italic> = 13.92%) were removed in experiment 2. Note that this procedure ensures the absence of blinks and high-amplitude artifacts within the predefined ROI time window of 100–700 ms. The data of one participant in experiment 2 contained many artifacts. Specifically, across all five tasks performed in experiment 2, 64.46% (<italic>SD</italic> = 9.88%) of all epochs were rejected for this participant. This was more than three standard deviations from the average number of rejected trials across participants and files and left too few trials for the decoding analysis. Therefore, this participant was removed from all EEG analyses.</p></sec><sec id="s4-14"><title>Time-frequency-domain multivariate pattern analysis (decoding)</title><p>We applied a multivariate backwards decoding model to EEG data that was transformed to the time-frequency domain. We used multivariate analyses both because its higher sensitivity in comparison with univariate analyses and to inspect if and to what extent different stimulus features (i.e., location and content) were processed in both tasks, without having to preselect spatial or time-frequency ROIs. The ADAM toolbox was used for time-frequency decomposition and decoding (<xref ref-type="bibr" rid="bib27">Fahrenfort et al., 2018</xref>). Single-trial power spectra were computed by convolving the EEG data with a complex wavelet (wavelet size of 0.5 s) after the application of a Hann taper (epochs: −100 ms to 1000 ms, 2–30 Hz in linear steps of 2 Hz). Raw time-frequency data contained both induced and evoked power. Trials were classified according to current trial stimulus features (i.e., location and content), resulting in four trial types. As decoding algorithms are known to be time-consuming, epochs were resampled to 64 Hz. Then, we applied a decoding algorithm to the data according to a 20-fold cross-validation scheme using either stimulus location, stimulus content, or congruency as stimulus class. Specifically, a linear discriminant analysis (LDA) was trained to discriminate between stimulus classes (e.g., left vs. right speaker location, etc.). Classification accuracy was computed as the AUC, a measure derived from Signal Detection Theory (<xref ref-type="bibr" rid="bib34">Green and Swets, 1966</xref>).</p><p>The multivariate classifiers were on different subsets of trials, depending on the behavioral task. For the auditory tasks (content discrimination I and II, location discrimination, and volume oddball detection), only correct trials were included in the analysis as errors tend to elicit a similar, albeit not identical, neural response as cognitive conflict and errors are more likely on incongruent trials (<xref ref-type="bibr" rid="bib17">Cohen and van Gaal, 2014</xref>). For the volume oddball detection task, we additionally excluded all oddball trials, thus only testing correct rejections, in order to prevent conflict arising between responses made exclusively with the right hand and sound content and location. For the two visual tasks (horizontal and vertical RDM), we trained the classifier on all trials.</p></sec><sec id="s4-15"><title>Topographical maps of ROI decoding</title><p>Topographical maps were created in order to investigate the spatial sources of activity related to the processing of the auditory features (content, location, and congruency). We first extracted classifier weights for each task and feature from the predefined ROI (100–700 ms, 2–8 Hz), allowing us to directly compare the spatial distributions between features and tasks. However, raw classifier weights are not interpretable as neural sources of activity and therefore have to be reconstructed (<xref ref-type="bibr" rid="bib38">Haufe et al., 2014</xref>). Thus, classifier weights were transformed to activation patterns by multiplying them with the covariance in the EEG data. The topographical activity maps of tasks and features with low decoding performance should be interpreted with caution as activation patterns reconstructed from classifier weights may be unreliable when decoding performance is low (<xref ref-type="bibr" rid="bib38">Haufe et al., 2014</xref>).</p></sec><sec id="s4-16"><title>Time-domain multivariate pattern analysis (decoding)</title><p>We applied a time-domain decoding analysis on EEG data to inspect the possible effect of task relevance of a stimulus feature on sensory processing. For this analysis, only EEG data from the tasks of experiment 2 were used as its parameters were comparable between tasks in this experiment (e.g., always visual stimulus present, no extensive training). For the analysis, we trained linear classifiers (LDA) to discriminate sound content (‘left’ vs. ‘right’) or sound location (left speaker vs. right speaker). First, epochs (from −100 ms to 1000 ms around stimulus presentation) were resampled to 64 Hz similar to the time-frequency decoding analyses. Then, the models were trained and tested according to a 20-fold cross-validation scheme. The AUC scores we obtained via multivariate analyses of our EEG data were tested per timepoint with one-sided t-tests (<inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5) across participants against chance level (50%). These t-tests were corrected for multiple comparisons over time using cluster-based permutation tests (p&lt;0.05, 1000 iterations). For each decoded stimulus feature, we then compared the decoding accuracies of the behavioral task in which the feature was task-relevant to all other tasks in a pairwise fashion (e.g., location decoding under location discrimination task vs. horizontal RDM task), with cluster-corrected two-sided t-tests against 0.</p></sec><sec id="s4-17"><title>Statistical analysis of EEG data</title><p>The AUC scores we obtained via multivariate analyses of our EEG data were tested per timepoint and frequency with one-sided t-tests (<inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5) across participants against chance level (50%). These t-tests were corrected for multiple comparisons over time and frequency using cluster-based permutation tests (p&lt;0.05, 1000 iterations). This procedure yields time-frequency clusters of significant above-chance classifier accuracy, indicative of information processing. Note that this procedure yields results that should be interpreted as fixed effects (<xref ref-type="bibr" rid="bib2">Allefeld et al., 2016</xref>), but is nonetheless standard in the scientific community.</p><p>In addition to the cluster analysis, we performed hypothesis-driven analyses on classifier accuracies that were extracted from a predefined time-frequency ROI. All these analyses were performed in JASP (<xref ref-type="bibr" rid="bib46">JASP Team, 2018</xref>). We then applied an ANCOVA on these accuracies with fixed effects being task and stimulus feature. Next, one-sample t-tests (one-sided, <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5) were performed on every task/feature combination to determine whether decoding accuracy of a specific feature within our preselected ROI was above chance during the various behavioral tasks. Additional Bayesian one-sample t-tests (one-sided, <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt;0.5, Cauchy scale = 0.71) were performed to inspect evidence in favor of the null hypothesis that decoding accuracy was not above chance.</p><p>We performed the same analysis on different ROIs. The results of those analyses can be found in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B–D</xref>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Written informed consent was obtained from all participants after explanation of the experimental protocol. This study was approved by the local ethics committee of the University of Amsterdam (projects: 2015-BC-4687, 2017-BC-8257, 2019-BC-10711).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-64431-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data and analysis scripts used in this article is available on Figshare: <ext-link ext-link-type="uri" xlink:href="https://uvaauas.figshare.com/projects/Preserved_sensory_processing_but_hampered_conflict_detection_when_stimulus_input_is_task-irrelevant/115020">https://uvaauas.figshare.com/projects/Preserved_sensory_processing_but_hampered_conflict_detection_when_stimulus_input_is_task-irrelevant/115020</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Nuiten</surname><given-names>SA</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name><name><surname>Beerendonk</surname><given-names>L</given-names></name><name><surname>Nanuashvili</surname><given-names>N</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Analyses scripts for manuscript: Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.21942/uva.14730297</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Nuiten</surname><given-names>SA</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name><name><surname>Beerendonk</surname><given-names>L</given-names></name><name><surname>Nanuashvili</surname><given-names>N</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Raw behavioral dataset for manuscript: Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant</data-title><source>DANS</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://www.narcis.nl/dataset/RecordID/doi%3A10.21942%2Fuva.14709420.v1">14709420.v1</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Nuiten</surname><given-names>SA</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name><name><surname>Beerendonk</surname><given-names>L</given-names></name><name><surname>Nanuashvili</surname><given-names>N</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Decoded EEG (time-frequency) dataset for manuscript: Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.21942/uva.14730402.v1</pub-id></element-citation></p><p><element-citation id="dataset4" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Nuiten</surname><given-names>SA</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name><name><surname>Beerendonk</surname><given-names>L</given-names></name><name><surname>Nanuashvili</surname><given-names>N</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Decoded EEG (time-domain) dataset for manuscript: Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.21942/uva.14754870.v1</pub-id></element-citation></p><p><element-citation id="dataset5" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Nuiten</surname><given-names>SA</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name><name><surname>Beerendonk</surname><given-names>L</given-names></name><name><surname>Nanuashvili</surname><given-names>N</given-names></name><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Raw EEG dataset for manuscript: Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant</data-title><source>figshare</source><pub-id assigning-authority="other" pub-id-type="doi">10.21942/uva.14709420.v1</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alilović</surname> <given-names>J</given-names></name><name><surname>Timmermans</surname> <given-names>B</given-names></name><name><surname>Reteig</surname> <given-names>LC</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Slagter</surname> <given-names>HA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>No evidence that predictions and attention modulate the first feedforward sweep of cortical information processing</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>2261</fpage><lpage>2278</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz038</pub-id><pub-id pub-id-type="pmid">30877784</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allefeld</surname> <given-names>C</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Valid population inference for information-based imaging: from the second-level t-test to prevalence inference</article-title><source>NeuroImage</source><volume>141</volume><fpage>378</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.040</pub-id><pub-id pub-id-type="pmid">27450073</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>BA</given-names></name><name><surname>Laurent</surname> <given-names>PA</given-names></name><name><surname>Yantis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Value-driven attentional capture</article-title><source>PNAS</source><volume>108</volume><fpage>10367</fpage><lpage>10371</lpage><pub-id pub-id-type="doi">10.1073/pnas.1104047108</pub-id><pub-id pub-id-type="pmid">21646524</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appelbaum</surname> <given-names>LG</given-names></name><name><surname>Smith</surname> <given-names>DV</given-names></name><name><surname>Boehler</surname> <given-names>CN</given-names></name><name><surname>Chen</surname> <given-names>WD</given-names></name><name><surname>Woldorff</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Rapid modulation of sensory processing induced by stimulus conflict</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>2620</fpage><lpage>2628</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21575</pub-id><pub-id pub-id-type="pmid">20849233</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atas</surname> <given-names>A</given-names></name><name><surname>Desender</surname> <given-names>K</given-names></name><name><surname>Gevers</surname> <given-names>W</given-names></name><name><surname>Cleeremans</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociating perception from action during conscious and unconscious conflict adaptation</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>42</volume><fpage>866</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1037/xlm0000206</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Awh</surname> <given-names>E</given-names></name><name><surname>Belopolsky</surname> <given-names>AV</given-names></name><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Top-down versus bottom-up attentional control: a failed theoretical dichotomy</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>437</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.06.010</pub-id><pub-id pub-id-type="pmid">22795563</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Broadbent</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="1958">1958</year><source>Perception and Communication</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzzell</surname> <given-names>GA</given-names></name><name><surname>Roberts</surname> <given-names>DM</given-names></name><name><surname>Baldwin</surname> <given-names>CL</given-names></name><name><surname>McDonald</surname> <given-names>CG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An electrophysiological correlate of conflict processing in an auditory spatial stroop task: the effect of individual differences in navigational style</article-title><source>International Journal of Psychophysiology</source><volume>90</volume><fpage>265</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2013.08.008</pub-id><pub-id pub-id-type="pmid">23994425</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canales-Johnson</surname> <given-names>A</given-names></name><name><surname>Beerendonk</surname> <given-names>L</given-names></name><name><surname>Blain</surname> <given-names>S</given-names></name><name><surname>Kitaoka</surname> <given-names>S</given-names></name><name><surname>Ezquerro-Nassar</surname> <given-names>A</given-names></name><name><surname>Nuiten</surname> <given-names>S</given-names></name><name><surname>Fahrenfort</surname> <given-names>J</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Bekinschtein</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Decreased alertness reconfigures cognitive control networks</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>7142</fpage><lpage>7154</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0343-20.2020</pub-id><pub-id pub-id-type="pmid">32801150</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>JF</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Frontal theta as a mechanism for cognitive control</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>414</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.04.012</pub-id><pub-id pub-id-type="pmid">24835663</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Object-based attention: a tutorial review</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>74</volume><fpage>784</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.3758/s13414-012-0322-z</pub-id><pub-id pub-id-type="pmid">22673856</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>A</given-names></name><name><surname>Tang</surname> <given-names>D</given-names></name><name><surname>Chen</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Training reveals the sources of stroop and flanker interference effects</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e76580</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0076580</pub-id><pub-id pub-id-type="pmid">24146892</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Cave</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>When does visual attention select all features of a distractor?</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>32</volume><fpage>1452</fpage><lpage>1464</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.32.6.1452</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherry</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Some experiments on the recognition of speech, with one and with two ears</article-title><source>The Journal of the Acoustical Society of America</source><volume>25</volume><fpage>975</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1121/1.1907229</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MX</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A neural microcircuit for cognitive conflict detection and signaling</article-title><source>Trends in Neurosciences</source><volume>37</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2014.06.004</pub-id><pub-id pub-id-type="pmid">25034536</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MX</given-names></name><name><surname>Cavanagh</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Single-trial regression elucidates the role of prefrontal theta oscillations in response conflict</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>30</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00030</pub-id><pub-id pub-id-type="pmid">21713190</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MX</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Subthreshold muscle twitches dissociate oscillatory neural signatures of conflicts from errors</article-title><source>NeuroImage</source><volume>86</volume><fpage>503</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.033</pub-id><pub-id pub-id-type="pmid">24185026</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cosman</surname> <given-names>JD</given-names></name><name><surname>Vecera</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Object-based attention overrides perceptual load to modulate visual distraction</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>38</volume><fpage>576</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1037/a0027406</pub-id><pub-id pub-id-type="pmid">22390296</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D'Ostilio</surname> <given-names>K</given-names></name><name><surname>Garraux</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Brain mechanisms underlying automatic and unconscious control of motor action</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>265</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00265</pub-id><pub-id pub-id-type="pmid">23055963</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D'Ostilio</surname> <given-names>K</given-names></name><name><surname>Garraux</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Dissociation between unconscious motor response facilitation and conflict in medial frontal Areas</article-title><source>European Journal of Neuroscience</source><volume>35</volume><fpage>332</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07941.x</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Changeux</surname> <given-names>JP</given-names></name><name><surname>Naccache</surname> <given-names>L</given-names></name><name><surname>Sackur</surname> <given-names>J</given-names></name><name><surname>Sergent</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Conscious, Preconscious, and subliminal processing: a testable taxonomy</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>204</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.03.007</pub-id><pub-id pub-id-type="pmid">16603406</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Changeux</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Experimental and theoretical approaches to conscious processing</article-title><source>Neuron</source><volume>70</volume><fpage>200</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.018</pub-id><pub-id pub-id-type="pmid">21521609</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname> <given-names>A</given-names></name><name><surname>Makeig</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deutsch</surname> <given-names>JA</given-names></name><name><surname>Deutsch</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Attention: some theoretical considerations</article-title><source>Psychological Review</source><volume>70</volume><fpage>80</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1037/h0039515</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname> <given-names>T</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cognitive control mechanisms resolve conflict through cortical amplification of task-relevant information</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1784</fpage><lpage>1790</lpage><pub-id pub-id-type="doi">10.1038/nn1594</pub-id><pub-id pub-id-type="pmid">16286928</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>van Leeuwen</surname> <given-names>J</given-names></name><name><surname>Olivers</surname> <given-names>CN</given-names></name><name><surname>Hogendoorn</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceptual integration without conscious access</article-title><source>PNAS</source><volume>114</volume><fpage>3744</fpage><lpage>3749</lpage><pub-id pub-id-type="doi">10.1073/pnas.1617268114</pub-id><pub-id pub-id-type="pmid">28325878</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>van Driel</surname> <given-names>J</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Olivers</surname> <given-names>CNL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>From ERPs to MVPA using the Amsterdam decoding and modeling toolbox (ADAM)</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>368</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00368</pub-id><pub-id pub-id-type="pmid">30018529</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkbeiner</surname> <given-names>M</given-names></name><name><surname>Palermo</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The role of spatial attention in nonconscious processing A comparison of face and nonface stimuli</article-title><source>Psychological Science</source><volume>20</volume><fpage>42</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02256.x</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname> <given-names>S</given-names></name><name><surname>Godde</surname> <given-names>B</given-names></name><name><surname>Finke</surname> <given-names>M</given-names></name><name><surname>Herrmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spatio-temporal brain dynamics in a combined stimulus-stimulus and stimulus-response conflict task</article-title><source>NeuroImage</source><volume>54</volume><fpage>622</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.071</pub-id><pub-id pub-id-type="pmid">20691791</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuentemilla</surname> <given-names>L</given-names></name><name><surname>Marco-Pallarés</surname> <given-names>J</given-names></name><name><surname>Grau</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Modulation of spectral power and of phase resetting of EEG contributes differentially to the generation of auditory event-related potentials</article-title><source>NeuroImage</source><volume>30</volume><fpage>909</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.10.036</pub-id><pub-id pub-id-type="pmid">16376575</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Architecture of the prefrontal cortex and the central executive</article-title><source>Annals of the New York Academy of Sciences</source><volume>769</volume><fpage>71</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.1995.tb38132.x</pub-id><pub-id pub-id-type="pmid">8595045</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The prefrontal landscape: implications of functional architecture for understanding human mentation and the central executive</article-title><source>Philosophical Transactions of the Royal Society of London Series B: Biological Sciences</source><volume>351</volume><fpage>1445</fpage><lpage>1453</lpage><pub-id pub-id-type="doi">10.1098/rstb.1996.0129</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname> <given-names>DM</given-names></name><name><surname>Swets</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-name>John Wiley</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grinband</surname> <given-names>J</given-names></name><name><surname>Savitskaya</surname> <given-names>J</given-names></name><name><surname>Wager</surname> <given-names>TD</given-names></name><name><surname>Teichert</surname> <given-names>T</given-names></name><name><surname>Ferrera</surname> <given-names>VP</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Conflict, error likelihood, and RT: response to Brown &amp; yeung et al</article-title><source>NeuroImage</source><volume>57</volume><fpage>320</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.04.027</pub-id><pub-id pub-id-type="pmid">21554960</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grinband</surname> <given-names>J</given-names></name><name><surname>Savitskaya</surname> <given-names>J</given-names></name><name><surname>Wager</surname> <given-names>TD</given-names></name><name><surname>Teichert</surname> <given-names>T</given-names></name><name><surname>Ferrera</surname> <given-names>VP</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>The dorsal medial frontal cortex is sensitive to time on task, not response conflict or error likelihood</article-title><source>NeuroImage</source><volume>57</volume><fpage>303</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.12.027</pub-id><pub-id pub-id-type="pmid">21168515</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoding dynamic brain patterns from evoked responses: a tutorial on multivariate pattern analysis applied to time series neuroimaging data</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>677</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id><pub-id pub-id-type="pmid">27779910</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname> <given-names>S</given-names></name><name><surname>Meinecke</surname> <given-names>F</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Dähne</surname> <given-names>S</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Blankertz</surname> <given-names>B</given-names></name><name><surname>Bießmann</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deconstructing multivariate decoding for the study of brain function</article-title><source>NeuroImage</source><volume>180</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.005</pub-id><pub-id pub-id-type="pmid">28782682</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hommel</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The simon effect as tool and heuristic</article-title><source>Acta Psychologica</source><volume>136</volume><fpage>189</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2010.04.011</pub-id><pub-id pub-id-type="pmid">20507830</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber-Huber</surname> <given-names>C</given-names></name><name><surname>Ansorge</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unconscious conflict adaptation without feature-repetitions and response time carry-over</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>44</volume><fpage>169</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1037/xhp0000450</pub-id><pub-id pub-id-type="pmid">29431465</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itthipuripat</surname> <given-names>S</given-names></name><name><surname>Deering</surname> <given-names>S</given-names></name><name><surname>Serences</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>When conflict Cannot be avoided: relative contributions of early selection and frontal executive control in mitigating stroop conflict</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>5037</fpage><lpage>5048</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz042</pub-id><pub-id pub-id-type="pmid">30877786</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jääskeläinen</surname> <given-names>IP</given-names></name><name><surname>Halme</surname> <given-names>H-L</given-names></name><name><surname>Agam</surname> <given-names>Y</given-names></name><name><surname>Glerean</surname> <given-names>E</given-names></name><name><surname>Lahnakoski</surname> <given-names>JM</given-names></name><name><surname>Sams</surname> <given-names>M</given-names></name><name><surname>Tapani</surname> <given-names>K</given-names></name><name><surname>Ahveninen</surname> <given-names>J</given-names></name><name><surname>Manoach</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural mechanisms supporting evaluation of others’ errors in real-life like conditions</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/srep18714</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janssens</surname> <given-names>C</given-names></name><name><surname>De Loof</surname> <given-names>E</given-names></name><name><surname>Boehler</surname> <given-names>CN</given-names></name><name><surname>Pourtois</surname> <given-names>G</given-names></name><name><surname>Verguts</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Occipital alpha power reveals fast attentional inhibition of incongruent distractors</article-title><source>Psychophysiology</source><volume>55</volume><elocation-id>e13011</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13011</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><collab>JASP Team</collab></person-group><year iso-8601-date="2018">2018</year><data-title>JASP</data-title><source>Jasp-Stats</source><version designator="0.8.6.0">0.8.6.0</version><ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/">https://jasp-stats.org/</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>Brady</surname> <given-names>DK</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attention improves encoding of task-relevant features in the human visual cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>8210</fpage><lpage>8219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6153-09.2011</pub-id><pub-id pub-id-type="pmid">21632942</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Q</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Conflict awareness dissociates theta-band neural dynamics of the medial frontal and lateral frontal cortex during trial-by-trial cognitive control</article-title><source>NeuroImage</source><volume>116</volume><fpage>102</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.04.062</pub-id><pub-id pub-id-type="pmid">25957992</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Q</given-names></name><name><surname>Van Gaal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>EEG neural oscillatory dynamics reveal semantic and response conflict at difference levels of conflict awareness</article-title><source>Scientific Reports</source><volume>5</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/srep12008</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>J</given-names></name><name><surname>Correa</surname> <given-names>CM</given-names></name><name><surname>Geerts</surname> <given-names>J</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The relationship between conflict awareness and behavioral and oscillatory signatures of immediate and delayed cognitive control</article-title><source>NeuroImage</source><volume>177</volume><fpage>11</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.007</pub-id><pub-id pub-id-type="pmid">29751059</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname> <given-names>D</given-names></name><name><surname>Treisman</surname> <given-names>A</given-names></name><name><surname>Gibbs</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The reviewing of object files: object-specific integration of information</article-title><source>Cognitive Psychology</source><volume>24</volume><fpage>175</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(92)90007-O</pub-id><pub-id pub-id-type="pmid">1582172</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname> <given-names>M</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoobox-3?</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1068/v070821</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Tsuchiya</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Attention and consciousness: two distinct brain processes</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>16</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.10.012</pub-id><pub-id pub-id-type="pmid">17129748</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelewijn</surname> <given-names>T</given-names></name><name><surname>Bronkhorst</surname> <given-names>A</given-names></name><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention and the multiple stages of multisensory integration: a review of audiovisual studies</article-title><source>Acta Psychologica</source><volume>134</volume><fpage>372</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2010.03.010</pub-id><pub-id pub-id-type="pmid">20427031</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Rahnev</surname> <given-names>D</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>Lau</surname> <given-names>HC</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Attention reverses the effect of prediction in silencing sensory signals</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>2197</fpage><lpage>2206</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr310</pub-id><pub-id pub-id-type="pmid">22047964</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornblum</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The way irrelevant dimensions are processed depends on what they overlap with: the case of stroop- and Simon-like stimuli</article-title><source>Psychological Research</source><volume>56</volume><fpage>130</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1007/BF00419699</pub-id><pub-id pub-id-type="pmid">8008775</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kouider</surname> <given-names>S</given-names></name><name><surname>Barbot</surname> <given-names>A</given-names></name><name><surname>Madsen</surname> <given-names>KH</given-names></name><name><surname>Lehericy</surname> <given-names>S</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Task relevance differentially shapes ventral visual stream sensitivity to visible and invisible faces</article-title><source>Neuroscience of Consciousness</source><volume>2016</volume><elocation-id>niw021</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niw021</pub-id><pub-id pub-id-type="pmid">30109131</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lachter</surname> <given-names>J</given-names></name><name><surname>Forster</surname> <given-names>KI</given-names></name><name><surname>Ruthruff</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Forty-five years after Broadbent (1958): still no identification without attention</article-title><source>Psychological Review</source><volume>111</volume><fpage>880</fpage><lpage>913</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.4.880</pub-id><pub-id pub-id-type="pmid">15482066</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Why visual attention and awareness are different</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>12</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)00013-X</pub-id><pub-id pub-id-type="pmid">12517353</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VAF</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01657-X</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname> <given-names>N</given-names></name><name><surname>Ro</surname> <given-names>T</given-names></name><name><surname>Russell</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The role of perceptual load in processing distractor faces</article-title><source>Psychological Science</source><volume>14</volume><fpage>510</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.03453</pub-id><pub-id pub-id-type="pmid">12930485</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname> <given-names>N</given-names></name><name><surname>Hirst</surname> <given-names>A</given-names></name><name><surname>de Fockert</surname> <given-names>JW</given-names></name><name><surname>Viding</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Load theory of selective attention and cognitive control</article-title><source>Journal of Experimental Psychology: General</source><volume>133</volume><fpage>339</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.133.3.339</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname> <given-names>N</given-names></name><name><surname>Fockert</surname> <given-names>Jde</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Frontal control of attentional capture in visual search</article-title><source>Visual Cognition</source><volume>14</volume><fpage>863</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1080/13506280500195953</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname> <given-names>J</given-names></name><name><surname>Getzmann</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>When and where of auditory spatial processing in cortex: a novel approach using electrotomography</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e25146</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0025146</pub-id><pub-id pub-id-type="pmid">21949873</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>FF</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Rapid natural scene categorization in the near absence of attention</article-title><source>PNAS</source><volume>99</volume><fpage>9596</fpage><lpage>9601</lpage><pub-id pub-id-type="doi">10.1073/pnas.092277599</pub-id><pub-id pub-id-type="pmid">12077298</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname> <given-names>CM</given-names></name><name><surname>Dunbar</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Training and Stroop-like interference: evidence for a continuum of automaticity</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>14</volume><fpage>126</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.14.1.126</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname> <given-names>W</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The active inhibition for the processing of visual irrelevant conflict information</article-title><source>International Journal of Psychophysiology</source><volume>67</volume><fpage>47</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2007.10.003</pub-id><pub-id pub-id-type="pmid">17999937</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKay</surname> <given-names>CC</given-names></name><name><surname>van den Berg</surname> <given-names>B</given-names></name><name><surname>Woldorff</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural cascade of conflict processing: not just time-on-task</article-title><source>Neuropsychologia</source><volume>96</volume><fpage>184</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.12.022</pub-id><pub-id pub-id-type="pmid">28017818</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molloy</surname> <given-names>K</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Lavie</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inattentional deafness: visual load leads to Time-Specific suppression of auditory evoked responses</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>16046</fpage><lpage>16054</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2931-15.2015</pub-id><pub-id pub-id-type="pmid">26658858</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moray</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Attention in dichotic listening: affective cues and the influence of instructions</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>11</volume><fpage>56</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1080/17470215908416289</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nigbur</surname> <given-names>R</given-names></name><name><surname>Cohen</surname> <given-names>MX</given-names></name><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name><name><surname>Stürmer</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Theta dynamics reveal domain-specific control over stimulus and response conflict</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>1264</fpage><lpage>1274</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00128</pub-id><pub-id pub-id-type="pmid">21861681</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Craven</surname> <given-names>KM</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>fMRI evidence for objects as the units of attentional selection</article-title><source>Nature</source><volume>401</volume><fpage>584</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1038/44134</pub-id><pub-id pub-id-type="pmid">10524624</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olguin</surname> <given-names>A</given-names></name><name><surname>Bekinschtein</surname> <given-names>TA</given-names></name><name><surname>Bozic</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural encoding of attended continuous speech under different types of interference</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1606</fpage><lpage>1619</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01303</pub-id><pub-id pub-id-type="pmid">30004849</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padrão</surname> <given-names>G</given-names></name><name><surname>Rodriguez-Herreros</surname> <given-names>B</given-names></name><name><surname>Pérez Zapata</surname> <given-names>L</given-names></name><name><surname>Rodriguez-Fornells</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Exogenous capture of medial-frontal oscillatory mechanisms by unattended conflicting information</article-title><source>Neuropsychologia</source><volume>75</volume><fpage>458</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.07.004</pub-id><pub-id pub-id-type="pmid">26151855</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural mechanisms of rapid natural scene categorization in human visual cortex</article-title><source>Nature</source><volume>460</volume><fpage>94</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/nature08103</pub-id><pub-id pub-id-type="pmid">19506558</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>J</given-names></name><name><surname>Gray</surname> <given-names>JR</given-names></name><name><surname>Simpson</surname> <given-names>S</given-names></name><name><surname>MacAskill</surname> <given-names>M</given-names></name><name><surname>Höchenberger</surname> <given-names>R</given-names></name><name><surname>Sogo</surname> <given-names>H</given-names></name><name><surname>Kastman</surname> <given-names>E</given-names></name><name><surname>Lindeløv</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PsychoPy2: experiments in behavior made easy</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id><pub-id pub-id-type="pmid">30734206</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polk</surname> <given-names>TA</given-names></name><name><surname>Drake</surname> <given-names>RM</given-names></name><name><surname>Jonides</surname> <given-names>JJ</given-names></name><name><surname>Smith</surname> <given-names>MR</given-names></name><name><surname>Smith</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attention enhances the neural processing of relevant features and suppresses the processing of irrelevant features in humans: a functional magnetic resonance imaging study of the stroop task</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>13786</fpage><lpage>13792</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1026-08.2008</pub-id><pub-id pub-id-type="pmid">19091969</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnev</surname> <given-names>DA</given-names></name><name><surname>Huang</surname> <given-names>E</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Subliminal stimuli in the near absence of attention influence top-down cognitive control</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>74</volume><fpage>521</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.3758/s13414-011-0246-z</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name><name><surname>Ullsperger</surname> <given-names>M</given-names></name><name><surname>Crone</surname> <given-names>EA</given-names></name><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The role of the medial frontal cortex in cognitive control</article-title><source>Science</source><volume>306</volume><fpage>443</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1126/science.1100301</pub-id><pub-id pub-id-type="pmid">15486290</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical algorithms for perceptual grouping</article-title><source>Annual Review of Neuroscience</source><volume>29</volume><fpage>203</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112939</pub-id><pub-id pub-id-type="pmid">16776584</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname> <given-names>PR</given-names></name><name><surname>Houtkamp</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Incremental grouping of image elements in vision</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>73</volume><fpage>2542</fpage><lpage>2572</lpage><pub-id pub-id-type="doi">10.3758/s13414-011-0200-0</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Röer</surname> <given-names>JP</given-names></name><name><surname>Körner</surname> <given-names>U</given-names></name><name><surname>Buchner</surname> <given-names>A</given-names></name><name><surname>Bell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Semantic priming by irrelevant speech</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>1205</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1186-3</pub-id><pub-id pub-id-type="pmid">27798754</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousselet</surname> <given-names>GA</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name><name><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>How parallel is visual processing in the ventral pathway?</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>363</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.06.003</pub-id><pub-id pub-id-type="pmid">15335463</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruggeri</surname> <given-names>P</given-names></name><name><surname>Meziane</surname> <given-names>HB</given-names></name><name><surname>Koenig</surname> <given-names>T</given-names></name><name><surname>Brandner</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A fine-grained time course investigation of brain dynamics during conflict monitoring</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>3667</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-40277-3</pub-id><pub-id pub-id-type="pmid">30842528</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salminen</surname> <given-names>NH</given-names></name><name><surname>Takanen</surname> <given-names>M</given-names></name><name><surname>Santala</surname> <given-names>O</given-names></name><name><surname>Lamminsalo</surname> <given-names>J</given-names></name><name><surname>Altoè</surname> <given-names>A</given-names></name><name><surname>Pulkki</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Integrated processing of spatial cues in human auditory cortex</article-title><source>Hearing Research</source><volume>327</volume><fpage>143</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2015.06.006</pub-id><pub-id pub-id-type="pmid">26074304</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sand</surname> <given-names>A</given-names></name><name><surname>Wiens</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Processing of unattended, simple negative pictures resists perceptual load</article-title><source>NeuroReport</source><volume>22</volume><fpage>348</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e3283463cb1</pub-id><pub-id pub-id-type="pmid">21464776</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnuerch</surname> <given-names>R</given-names></name><name><surname>Kreitz</surname> <given-names>C</given-names></name><name><surname>Gibbons</surname> <given-names>H</given-names></name><name><surname>Memmert</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Not quite so blind: semantic processing despite inattentional blindness</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>42</volume><fpage>459</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1037/xhp0000205</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenfeld</surname> <given-names>MA</given-names></name><name><surname>Hopf</surname> <given-names>JM</given-names></name><name><surname>Merkel</surname> <given-names>C</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Object-based attention involves the sequential activation of feature-specific cortical modules</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>619</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1038/nn.3656</pub-id><pub-id pub-id-type="pmid">24561999</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamics of the central bottleneck: dual-task and task uncertainty</article-title><source>PLOS Biology</source><volume>4</volume><elocation-id>e220</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0040220</pub-id><pub-id pub-id-type="pmid">16787105</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simons</surname> <given-names>DJ</given-names></name><name><surname>Chabris</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Gorillas in our midst: sustained inattentional blindness for dynamic events</article-title><source>Perception</source><volume>28</volume><fpage>1059</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1068/p281059</pub-id><pub-id pub-id-type="pmid">10694957</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soutschek</surname> <given-names>A</given-names></name><name><surname>Stelzel</surname> <given-names>C</given-names></name><name><surname>Paschke</surname> <given-names>L</given-names></name><name><surname>Walter</surname> <given-names>H</given-names></name><name><surname>Schubert</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissociable effects of motivation and expectancy on conflict processing: an fMRI study</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>409</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00712</pub-id><pub-id pub-id-type="pmid">25203271</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanics</surname> <given-names>G</given-names></name><name><surname>Csukly</surname> <given-names>G</given-names></name><name><surname>Komlósi</surname> <given-names>S</given-names></name><name><surname>Czobor</surname> <given-names>P</given-names></name><name><surname>Czigler</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Processing of unattended facial emotions: a visual mismatch negativity study</article-title><source>NeuroImage</source><volume>59</volume><fpage>3042</fpage><lpage>3049</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.041</pub-id><pub-id pub-id-type="pmid">22037000</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroop</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1935">1935</year><article-title>Studies of interference in serial verbal reactions</article-title><source>Journal of Experimental Psychology</source><volume>18</volume><fpage>643</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1037/h0054651</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Top-down and bottom-up control of visual selection</article-title><source>Acta Psychologica</source><volume>135</volume><fpage>77</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2010.02.006</pub-id><pub-id pub-id-type="pmid">20507828</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Strategies and models of selective attention</article-title><source>Psychological Review</source><volume>76</volume><fpage>282</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1037/h0027242</pub-id><pub-id pub-id-type="pmid">4893203</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname> <given-names>AM</given-names></name><name><surname>Gelade</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>A feature-integration theory of attention</article-title><source>Cognitive Psychology</source><volume>12</volume><fpage>97</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(80)90005-5</pub-id><pub-id pub-id-type="pmid">7351125</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turatto</surname> <given-names>M</given-names></name><name><surname>Mazza</surname> <given-names>V</given-names></name><name><surname>Umiltà</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Crossmodal object-based attention: auditory objects affect visual processing</article-title><source>Cognition</source><volume>96</volume><fpage>B55</fpage><lpage>B64</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2004.12.001</pub-id><pub-id pub-id-type="pmid">15925569</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tusche</surname> <given-names>A</given-names></name><name><surname>Kahnt</surname> <given-names>T</given-names></name><name><surname>Wisniewski</surname> <given-names>D</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Automatic processing of political preferences in the human brain</article-title><source>NeuroImage</source><volume>72</volume><fpage>174</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.020</pub-id><pub-id pub-id-type="pmid">23353599</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Frontal cortex mediates unconsciously triggered inhibitory control</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>8053</fpage><lpage>8062</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1278-08.2008</pub-id><pub-id pub-id-type="pmid">18685030</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name><name><surname>van den Wildenberg</surname> <given-names>WP</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dissociating consciousness from inhibitory control: evidence for unconsciously triggered response inhibition in the stop-signal task</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>35</volume><fpage>1129</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1037/a0013551</pub-id><pub-id pub-id-type="pmid">19653754</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Lamme</surname> <given-names>VAF</given-names></name><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dissociable brain mechanisms underlying the conscious and unconscious control of behavior</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>91</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21431</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Cohen</surname> <given-names>MX</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of consciousness in cognitive control and decision making</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>121</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00121</pub-id><pub-id pub-id-type="pmid">22586386</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Naccache</surname> <given-names>L</given-names></name><name><surname>Meuwese</surname> <given-names>JDI</given-names></name><name><surname>van Loon</surname> <given-names>AM</given-names></name><name><surname>Leighton</surname> <given-names>AH</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Can the meaning of multiple words be integrated unconsciously?</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130212</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0212</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Schie</surname> <given-names>HT</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Coles</surname> <given-names>MG</given-names></name><name><surname>Bekkering</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Modulation of activity in medial frontal and motor cortices during error observation</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>549</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1038/nn1239</pub-id><pub-id pub-id-type="pmid">15107858</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Veen</surname> <given-names>V</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Stenger</surname> <given-names>VA</given-names></name><name><surname>Carter</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Anterior cingulate cortex, conflict monitoring, and levels of processing</article-title><source>NeuroImage</source><volume>14</volume><fpage>1302</fpage><lpage>1308</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0923</pub-id><pub-id pub-id-type="pmid">11707086</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The power of the feed-forward sweep</article-title><source>Advances in Cognitive Psychology</source><volume>3</volume><fpage>167</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.2478/v10053-008-0022-3</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>K</given-names></name><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Zheng</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal and spectral profiles of stimulus-stimulus and stimulus-response conflict processing</article-title><source>NeuroImage</source><volume>89</volume><fpage>280</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.045</pub-id><pub-id pub-id-type="pmid">24315839</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wegener</surname> <given-names>D</given-names></name><name><surname>Galashan</surname> <given-names>FO</given-names></name><name><surname>Aurich</surname> <given-names>MK</given-names></name><name><surname>Kreiter</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attentional spreading to task-irrelevant object features: experimental support and a 3-step model of attention for object-based selection and feature-based processing modulation</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>414</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00414</pub-id><pub-id pub-id-type="pmid">24959132</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisz</surname> <given-names>N</given-names></name><name><surname>Müller</surname> <given-names>N</given-names></name><name><surname>Jatzev</surname> <given-names>S</given-names></name><name><surname>Bertrand</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Oscillatory alpha modulations in right auditory regions reflect the validity of acoustic cues in an auditory spatial attention task</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>2579</fpage><lpage>2590</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht113</pub-id><pub-id pub-id-type="pmid">23645711</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickens</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Multiple resources and performance prediction</article-title><source>Theoretical Issues in Ergonomics Science</source><volume>3</volume><fpage>159</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1080/14639220210123806</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woldorff</surname> <given-names>MG</given-names></name><name><surname>Gallen</surname> <given-names>CC</given-names></name><name><surname>Hampson</surname> <given-names>SA</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Pantev</surname> <given-names>C</given-names></name><name><surname>Sobel</surname> <given-names>D</given-names></name><name><surname>Bloom</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Modulation of early sensory processing in human auditory cortex during auditory selective attention</article-title><source>PNAS</source><volume>90</volume><fpage>8722</fpage><lpage>8726</lpage><pub-id pub-id-type="doi">10.1073/pnas.90.18.8722</pub-id><pub-id pub-id-type="pmid">8378354</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname> <given-names>JM</given-names></name><name><surname>Horowitz</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>495</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1038/nrn1411</pub-id><pub-id pub-id-type="pmid">15152199</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural fate of task-irrelevant features in object-based processing</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>14020</fpage><lpage>14028</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3011-10.2010</pub-id><pub-id pub-id-type="pmid">20962223</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname> <given-names>N</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Errors of interpretation and modeling: a reply to grinband et al</article-title><source>NeuroImage</source><volume>57</volume><fpage>316</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.04.029</pub-id><pub-id pub-id-type="pmid">21530662</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zäske</surname> <given-names>R</given-names></name><name><surname>Perlich</surname> <given-names>MC</given-names></name><name><surname>Schweinberger</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>To hear or not to hear: voice processing under visual load</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>78</volume><fpage>1488</fpage><lpage>1495</lpage><pub-id pub-id-type="doi">10.3758/s13414-016-1119-2</pub-id><pub-id pub-id-type="pmid">27150617</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>J</given-names></name><name><surname>Liang</surname> <given-names>WK</given-names></name><name><surname>Juan</surname> <given-names>CH</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Wang</surname> <given-names>S</given-names></name><name><surname>Zhu</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissociated stimulus and response conflict effect in the stroop task: evidence from evoked brain potentials and brain oscillations</article-title><source>Biological Psychology</source><volume>104</volume><fpage>130</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2014.12.001</pub-id><pub-id pub-id-type="pmid">25511611</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmer</surname> <given-names>U</given-names></name><name><surname>Itthipanyanan</surname> <given-names>S</given-names></name><name><surname>Grent-’t-Jong</surname> <given-names>T</given-names></name><name><surname>Woldorff</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The electrophysiological time course of the interaction of stimulus conflict and the multisensory spread of attention</article-title><source>European Journal of Neuroscience</source><volume>31</volume><fpage>1744</fpage><lpage>1754</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2010.07229.x</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Fernández Slezak</surname> <given-names>D</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The brain's router: a cortical network model of serial processing in the primate brain</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000765</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000765</pub-id><pub-id pub-id-type="pmid">20442869</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The human turing machine: a neural framework for mental programs</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>293</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.05.007</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64431.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Swann</surname><given-names>Nicole C</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oregon</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Krämer</surname><given-names>Ulrike M</given-names></name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Samaha</surname><given-names>Jason</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Nuiten and colleagues have conducted a well-designed series of electroencephalographic experiments to investigate if conflict detection depends on conscious awareness. They used a combination of behavioral findings and sophisticated modeling of EEG data to conclude that conflict was only present when there was at least some degree of task relevance and that there was a dependence on object-based attention. In contrast, sensory processing was preserved regardless of attentional status.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Intact sensory processing but hampered conflict detection when stimulus input is task-irrelevant&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Ulrike M. Krämer, PhD (Reviewer #1); Jason Samaha (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>Nuiten and colleagues have conducted a well-designed series of EEG experiments to investigate if conflict detection depends on conscious awareness. They used a combination of behavioral findings and MVPA analysis of EEG data to conclude that conflict was only present when there was at least some degree of task relevance and that there was a dependence on object-based attention. In contrast, sensory processing was intact regardless of attentional status.</p><p>Overall, the reviewers were enthusiastic about the manuscript. However, there were some concerns and questions which we discuss below.</p><p>Essential revisions:</p><p>1) The reviewers felt that describing the sensory processing as &quot;intact&quot; regardless of attention was too vague. While there does seem to be some degree of classification possible for sensory processing regardless of task relevance – it appears to be graded. For instance, there are clear differences between the decodability of auditory stimulus properties when the auditory stimulus is task-relevant versus not. Some of this can be attributed to decision/motor processes, as the authors point out, but some changes might reflect changes in sensory processing – for example, the earlier aspects of the decoding differences. In this case, the conclusion would be that sensory processes are 'relatively' intact, but potentially modulated by task-relevance. We recommend the authors show sensory ERPs when the auditory stimulus is task-relevant or not to help assess whether the task manipulation keeps sensory responses completely intact or the authors otherwise justify how their results support the idea that sensory processing is &quot;intact&quot;.</p><p>2) The reviewers had some questions about what might be driving the congruency decoding results. Given that the congruent and non-congruent conditions produce large differences in behavior in almost all tasks (except the RDM tasks, where no conflict decoding was found), is it possible that the decoder is just picking up on the task difficulty, rather than conflict detection per se? For example, perhaps the decoder results reflect differences in decision making as a result of conflict, rather than the neural signature of the conflict detection process. Is there anything about the temporal/spatial dynamics of theta amplitude that would indicate a specific conflict detection process is underlying decodability?</p><p>3) The reviewers were unclear about the interpretation of the decoding results in terms of object-based attention. Specifically, on page 13 the authors write: &quot;Second, the time-frequency (T-F) windows of significant sound content and location decoding in the volume oddball task was considerably more extended in both time and frequency space than observed in the two RDM tasks. This highlights that the presence of object-based attention in the volume oddball task, because one feature of the auditory stimulus was task-relevant, led to the rapid attentional selection and hence neural enhancement of the task-irrelevant features sound content and location this task only&quot;. If this explanation were true, and object-based attention was the mechanism, wouldn't a similarly broad time/frequency decoding effect be observed in the content decoding during location discrimination task, since the sound is the object of attention in that task as well? However, the data (4b) seem to only show a small T/F window of content decoding on the location task, suggesting that object-based decoding is either not operating in that task or that the broad T/F decoding does not actually reflect object-based selection. Can the authors comment on this apparent discrepancy?</p><p>4) The reviewers found it unfortunate that information about which electrodes contributed to decoding was lost in this analysis and felt that the manuscript would be improved if at least some topographical information were included.</p><p>5) The authors state that results in Figure 5 are based on a specific time-frequency ROI which was hypothesis-driven and pre-defined (p.24, lines 911). However, they go on in the next sentence to explain that the ROI was selected visually based on the most-significant clusters. Please clarify how the ROI was defined – was it hypothesis-driven or data-driven? If it was data-driven how were multiple comparison corrections handled?</p><p>6) The reviewers were surprised by the choice to not perform any artifact rejection on the data. We strongly suggest that appropriate artifact reject be applied, or, at the very least, the choice to not perform artifact rejection be better justified.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64431.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The reviewers felt that describing the sensory processing as “intact” regardless of attention was too vague. While there does seem to be some degree of classification possible for sensory processing regardless of task relevance – it appears to be graded. For instance, there are clear differences between the decodability of auditory stimulus properties when the auditory stimulus is task-relevant versus not. Some of this can be attributed to decision/motor processes, as the authors point out, but some changes might reflect changes in sensory processing – for example, the earlier aspects of the decoding differences. In this case, the conclusion would be that sensory processes are ‘relatively’ intact, but potentially modulated by task-relevance. We recommend the authors show sensory ERPs when the auditory stimulus is task-relevant or not to help assess whether the task manipulation keeps sensory responses completely intact or the authors otherwise justify how their results support the idea that sensory processing is “intact”.</p></disp-quote><p>We agree with the reviewers that, on the basis of the results presented in the manuscript, we cannot claim “intact” sensory processing regardless of task-relevance. The use of the word “<italic>intact”</italic> may indeed suggest that the processing of sensory features is unchanged under various levels of task-relevance, which we did not mean to bring across. Indeed, a substantial body of evidence has shown reduced evoked responses to stimuli that were not attended or were task-irrelevant (Alilović et al., 2019; Jehee et al., 2011; Kok et al., 2012; Molloy et al., 2015b; Woldorff et al., 1993). Our time-frequency decoding results point to the same idea, e.g. location decoding is more broadband and better when the auditory stimulus is task-relevant versus when it is not (e.g. manuscript Figure 2B). We understand the reviewers’ suggestion to inspect sensory ERPs, to test this issue further. As such, we have now performed an additional analysis to inspect the early sensory evoked responses as a factor of task-relevance. We decided to use a time-domain decoding approach however, instead of a more traditional ERP analysis, for two reasons. We wanted to prevent obscuring possible sensory effects due to electrode selection, necessary for computing ERPs but not for decoding, because we did not have strong a priori expectations about the exact scalp topography of the processing of sound content and location as a factor of task-relevance. Second, because we have used multivariate approaches throughout the manuscript so far, this better fitted the general approach taken in this project. Below, we have added a paragraph of the Methods section where we describe the details of the time-domain decoding analysis. Also, we added excerpts of the manuscript (Results) where we present the results of this new analysis (Figure 4 – Supplement 2).</p><p>As you will see, these additional analyses suggest that sensory processing is indeed graded, although it remains difficult to know with certainty, because of the direct association between specific responses/decision processes and specific features in some tasks but not others. We thus agree with the reviewers that the effects we report in the manuscript (manuscript Figures 2, 4 and 5; Figure 4—figure supplement 2) need more subtle phrasing. We have decided to refer to sensory processing as <italic>preserved</italic> instead of <italic>intact</italic> throughout the manuscript. As such, we have changed the title of the revised manuscript into ‘Preserved sensory processing but hampered conflict detection when stimulus input is task-irrelevant’. We hope this better qualifies the graded relationship between sensory processing and task-relevance. Also, we have added a paragraph to the Discussion where we discuss the graded sensory processing as a factor of task-relevance and what that means for our reported findings. We have also added that paragraph below.</p><p>In Methods section, page 30</p><p>“We applied a time-domain decoding analysis on EEG data, to inspect the possible effect of task relevance of a stimulus feature on sensory processing. […] For each decoded stimulus feature, we then compared the decoding accuracies of the behavioral task in which the feature was task-relevant, to all other tasks in a pairwise fashion (e.g. location decoding under location discrimination task versus horizontal RDM task), with cluster-corrected two-sided t-tests against 0.”</p><p>In Results section, page 17</p><p>“In order to test whether the earliest sensory responses were already modulated by task relevance, and to link this to previous ERP studies (Alilović et al., 2019; Molloy et al., 2015; Woldorff et al., 1993), we performed an additional time-domain multivariate analysis on these sensory features (T-F analysis are not well suited to address questions about the timing of processes). […] These results are elaborated upon in the Discussion.”</p><p>In Discussion, pages 21-22</p><p>“We show that conflict processing is absent when conflicting features are fully task-irrelevant, while evidence of sensory processing is still present in neural data (Figures 2B, 4B and 5). […] Summarizing, although processing of sensory features is degraded under decreasing levels of task relevance it is present regardless of attention, whereas detection of conflict between these features is no longer possible when the features are fully task-irrelevant.”</p><disp-quote content-type="editor-comment"><p>2) The reviewers had some questions about what might be driving the congruency decoding results. Given that the congruent and non-congruent conditions produce large differences in behavior in almost all tasks (except the RDM tasks, where no conflict decoding was found), is it possible that the decoder is just picking up on the task difficulty, rather than conflict detection per se? For example, perhaps the decoder results reflect differences in decision making as a result of conflict, rather than the neural signature of the conflict detection process. Is there anything about the temporal/spatial dynamics of theta amplitude that would indicate a specific conflict detection process is underlying decodability?</p></disp-quote><p>We thank the reviewers for this interesting question. The reviewers point to an issue concerning the source of our above-chance decoding of congruency. Whether typical neural effects in research on conflict processing, i.e. enhanced medial frontal theta-oscillations, are related to the detection of conflict or are a marker of task-difficulty has been debated in the field (Grinband et al., 2011a, 2011b; Yeung et al., 2011). We agree that this distinction is important and deserves more elaboration in the manuscript. We have therefore added a paragraph to the Discussion in which we address this issue. We have added that paragraph below.</p><p>In Discussion, page 22</p><p>“For our main analysis we trained a multivariate classifier on congruent versus incongruent trials and observed effects of task relevance of the performance of the classifier, i.e. decoding performance was hampered when conflicting features were fully task-irrelevant (Figures 2B, 4B and 5B). […] Therefore, we believe that the observed congruency decoding results presented here are mainly driven by the detection of conflicting sensory inputs and are not, or much less so, driven by task difficulty.”</p><disp-quote content-type="editor-comment"><p>3) The reviewers were unclear about the interpretation of the decoding results in terms of object-based attention. Specifically, on page 13 the authors write: &quot;Second, the time-frequency (T-F) windows of significant sound content and location decoding in the volume oddball task was considerably more extended in both time and frequency space than observed in the two RDM tasks. This highlights that the presence of object-based attention in the volume oddball task, because one feature of the auditory stimulus was task-relevant, led to the rapid attentional selection and hence neural enhancement of the task-irrelevant features sound content and location this task only&quot;. If this explanation were true, and object-based attention was the mechanism, wouldn't a similarly broad time/frequency decoding effect be observed in the content decoding during location discrimination task, since the sound is the object of attention in that task as well? However, the data (4b) seem to only show a small T/F window of content decoding on the location task, suggesting that object-based decoding is either not operating in that task or that the broad T/F decoding does not actually reflect object-based selection. Can the authors comment on this apparent discrepancy?</p></disp-quote><p>This is a good point, and we thank the reviewers for this comment. After reconsideration of our results, we agree with the reviewers that object-based attention alone cannot explain why sensory feature decoding is more durable and broadband when the auditory stimulus is task-relevant versus task-irrelevant, as it falls short in explaining the more fleeting and narrowband decoding results for task-irrelevant auditory features (e.g. content) during the auditory tasks (e.g. location discrimination task). We have therefore removed this explanation of the decoding results (previous lines 448-453) and supplementary figure S5 from the manuscript. Moreover, we have added a paragraph to the Discussion in which we discuss a possible second mechanism, besides object-based attention, that may be at play and may affect the decoding results, depending on the specific task manipulation. We have added that paragraph below.</p><p>In Discussion, page 22</p><p>“Besides object-based attention, the process through which attentional resources are allocated to the processing of task-irrelevant features of a task-relevant object, other mechanisms might also play a role in the extent to which sensory information is processed, such as the active suppression of task-irrelevant information. […] Disentangling the effects of such mechanisms, object-based attention and their possible interactions on the processing of sensory and cognitive information, however, falls outside the scope of this work.”</p><disp-quote content-type="editor-comment"><p>4) The reviewers found it unfortunate that information about which electrodes contributed to decoding was lost in this analysis and felt that the manuscript would be improved if at least some topographical information were included.</p></disp-quote><p>We agree with the reviewers that information pertaining to the spatial sources of our effects could provide additional insights. The decoding weights of EEG-channels are, however, not interpretable as neural sources and therefore they have to be transformed back to activity patterns (Haufe et al., 2014). In order to compare topographic maps for all tasks and features, we have extracted the decoder weights from the preselected time-frequency ROI used for manuscript figure 5 for all tasks and contrasts (sound content, sound location and conflict). Then, these weights were transformed to activity patterns by multiplying them with the covariance in the EEG data (Haufe et al., 2014). We have added the topographic maps and the parts from the Results and Methods of the manuscript where we discuss these maps (Figure 5 – Supplement 1A).</p><p>In Methods, page 30</p><p>“Topographical maps were created in order to investigate the spatial sources of activity related to the processing of the auditory features (content, location and congruency). […] The topographical activity maps of tasks and features with low decoding performance should be interpreted with caution, as activation patterns reconstructed from classifier weights may be unreliable when decoding performance is low (Haufe et al., 2014).”</p><p>In Results, page 9</p><p>“Activation patterns that were calculated from classifier weights within the predefined time-frequency theta-band ROI (2Hz-8Hz, 100ms-700ms) revealed a clear midfrontal distribution of conflict related activity (Figure 5 – Supplement 1A).”</p><p>In Results, page 18</p><p>“Classifier weights were extracted from the ROI for all tasks and features, transformed to activation patterns and plotted in topomaps, to show the patterns of activity underlying the decoding results (Figure 5 – Supplement 1A).”</p><disp-quote content-type="editor-comment"><p>5) The authors state that results in Figure 5 are based on a specific time-frequency ROI which was hypothesis-driven and pre-defined (p.24, lines 911). However, they go on in the next sentence to explain that the ROI was selected visually based on the most-significant clusters. Please clarify how the ROI was defined – was it hypothesis-driven or data-driven? If it was data-driven how were multiple comparison corrections handled?</p></disp-quote><p>We apologize, the wording used in the outlined section was ambiguous and we have therefore revised it. The time-frequency ROI was selected on the basis of previous work, including our own, investigating the role of medial frontal theta-oscillations in conflict processing (Cohen and Cavanagh, 2011; Cohen and van Gaal, 2014; Jiang et al., 2015; Nigbur et al., 2012). Furthermore, we would like to emphasize that we verified whether ROI selection influenced our main findings, by applying the same analyses to other time-frequency ROIs. These findings are depicted in the supplements and show a similar pattern as the ROI presented in the main text, i.e. a stronger decline in congruency decoding accuracy under manipulations of task-relevance as opposed to decoding accuracy of sensory features. We have added the revised section below.</p><p>In Results, page 7</p><p>“Then, we report results from the additional hypothesis-driven analysis, where we extracted classifier accuracies from a predefined time-frequency ROI (100ms-700ms, 2Hz-8Hz) on which we performed (Bayesian) tests (see Methods). […] Specifically, for every task and every stimulus feature (i.e. congruency, content, location), we extracted average decoding accuracies from the ROI per participant and performed analyses on these values.”</p><disp-quote content-type="editor-comment"><p>6) The reviewers were surprised by the choice to not perform any artifact rejection on the data. We strongly suggest that appropriate artifact reject be applied, or, at the very least, the choice to not perform artifact rejection be better justified.</p></disp-quote><p>We have followed the reviewers’ suggestion to perform further preprocessing on our data. Thus, the data discussed in our revised manuscript, and this letter, are fully updated.</p><p>We initially did not perform any artefact rejection, as in our experience multivariate analyses on large EEG datasets are robust to artefacts. For smaller datasets, such as the data belonging to Experiment 2, we believed that preserving as much data as possible would improve classification performance by increasing the training set size of the model. However, we understand the reviewers’ suggestion to perform artefact rejection. We reasoned this would indeed strengthen the results presented in the paper, because readers may wonder whether our (null-)findings were perhaps a result of noisy data due to our lack of artefact rejection. We have now performed the same multivariate analyses as in the original version of the manuscript, but this time after additional preprocessing steps</p><p>We were pleased that the reviewers suggested to do additional preprocessing for several reasons. First, the majority of the decoding results remained qualitatively unchanged, strengthening our confidence in the robustness of these results. Interestingly, however, we do find significant congruency decoding in the predefined ROI for the horizontal RDM task after applying these preprocessing steps (manuscript Figure 4B), whereas previously no effects of congruency were observed in this task. Above-chance congruency decoding in the HRDM was robust and independent of the specific definition of the ROI used (manuscript Figure 5 – Supplement 1B-D). This observation of congruency decoding within the theta-band shows that when a task-irrelevant stimulus has features that are response-relevant (i.e. overlap with the response scheme), these two features are still integrated in the brain to form conflict. In contrast, when features of a task-irrelevant stimulus are not response-relevant, i.e. in the vertical RDM task where the response is orthogonal to sound content and location, conflict between these auditory features is not detected. We have incorporated this interesting new finding in the revised manuscript, and have updated the abstract, results and Discussion section accordingly. Below, we have added the section from the Methods where we describe the preprocessing pipeline and an excerpt from the Discussion where we discuss the new results.</p><p>In Methods, page 29</p><p>“EEG-data were recorded with a 64-channel BioSemi apparatus (BioSemi B.V., Amsterdam, The Netherlands), at 512Hz. […] This was more than 3 standard deviations from the average number of rejected trials across participants and files and left too few trials for the decoding analysis. Therefore this participant was removed from all EEG analyses.”</p><p>In Discussion, pages 20-21</p><p>“In the horizontal RDM task on the other hand, the conflicting features of the task-irrelevant auditory stimulus overlapped with the overall response scheme or task-set of the participant, namely discriminating rightwards versus leftwards moving dots. […] Note that we report additional behavioral results that show clear indications of conflict when the task-relevant feature of the visual stimulus interferes directly with a <italic>single</italic> task-irrelevant feature of the auditory task (e.g., auditory content-dot motion conflict).”</p><p>References:</p><p>Alilović, J., Timmermans, B., Reteig, L. C., van Gaal, S., and Slagter, H. A. (2019). No Evidence that Predictions and Attention Modulate the First Feedforward Sweep of Cortical Information Processing. Cerebral Cortex, 29(5), 2261–2278. https://doi.org/10.1093/cercor/bhz038Anderson, B. A., Laurent, P. A., and Yantis, S. (2011). Value-driven attentional capture. Proceedings of the National Academy of Sciences, 108(25), 10367–10371. https://doi.org/10.1073/pnas.1104047108Appelbaum, L. G., Smith, D. V., Boehler, C. N., Chen, W. D., and Woldorff, M. G. (2011). Rapid Modulation of Sensory Processing Induced by Stimulus Conflict. Journal of Cognitive Neuroscience, 23(9), 2620–2628. https://doi.org/10.1162/jocn.2010.21575Awh, E., Belopolsky, A. V., and Theeuwes, J. (2012). Top-down versus bottom-up attentional control: A failed theoretical dichotomy. Trends in Cognitive Sciences, 16(8), 437–443. https://doi.org/10.1016/j.tics.2012.06.010Cohen, M. X., and Cavanagh, J. F. (2011). Single-Trial Regression Elucidates the Role of Prefrontal Theta Oscillations in Response Conflict. Frontiers in Psychology, 2. https://doi.org/10.3389/fpsyg.2011.00030Cohen, M. X., and van Gaal, S. (2014). Subthreshold muscle twitches dissociate oscillatory neural signatures of conflicts from errors. NeuroImage, 86, 503–513. https://doi.org/10.1016/j.neuroimage.2013.10.033Egner, T., and Hirsch, J. (2005). Cognitive control mechanisms resolve conflict through cortical amplification of task-relevant information. Nature Neuroscience, 8(12), 1784–1790. https://doi.org/10.1038/nn1594Fahrenfort, J. J., Van Driel, J., van Gaal, S., and Olivers, C. N. L. (2018). From ERPs to MVPA using the Amsterdam Decoding and Modeling toolbox (ADAM). Frontiers in Neuroscience – Brain Imaging Methods, 12(July). https://doi.org/10.3389/fnins.2018.00368Grinband, J., Savitskaya, J., Wager, T. D., Teichert, T., Ferrera, V. P., and Hirsch, J. (2011a). Conflict, error likelihood, and RT: Response to Brown and Yeung et al. NeuroImage, 57(2), 320–322. https://doi.org/10.1016/j.neuroimage.2011.04.027Grinband, J., Savitskaya, J., Wager, T. D., Teichert, T., Ferrera, V. P., and Hirsch, J. (2011b). The Dorsal Medial Frontal Cortex is Sensitive to Time on Task, Not Response Conflict or Error Likelihood. NeuroImage, 57(2), 303–311. https://doi.org/10.1016/j.neuroimage.2010.12.027Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J.-D., Blankertz, B., and Bießmann, F. (2014). On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87, 96–110. https://doi.org/10.1016/j.neuroimage.2013.10.067Janssens, C., De Loof, E., Boehler, C. N., Pourtois, G., and Verguts, T. (2018). Occipital α power reveals fast attentional inhibition of incongruent distractors. Psychophysiology, 55(3). https://doi.org/10.1111/psyp.13011Jehee, J. F. M., Brady, D. K., and Tong, F. (2011). Attention Improves Encoding of Task-Relevant Features in the Human Visual Cortex. Journal of Neuroscience, 31(22), 8210–8219. https://doi.org/10.1523/JNEUROSCI.6153-09.2011Jiang, J., Zhang, Q., and van Gaal, S. (2015). Conflict awareness dissociates theta-band neural dynamics of the medial frontal and lateral frontal cortex during trial-by-trial cognitive control. NeuroImage, 116, 102–111. https://doi.org/10.1016/j.neuroimage.2015.04.062Jiang, J., Zhang, Q., and Van Gaal, S. (2015). EEG neural oscillatory dynamics reveal semantic and response conflict at difference levels of conflict awareness. Scientific Reports, 5(July), 1–11. https://doi.org/10.1038/srep12008Jiang, Jun, Zhang, Q., and Van Gaal, S. (2015). EEG neural oscillatory dynamics reveal semantic and response conflict at difference levels of conflict awareness. Scientific Reports, 5(1), 12008. https://doi.org/10.1038/srep12008Kok, P., Rahnev, D., Jehee, J. F. M., Lau, H. C., and de Lange, F. P. (2012). Attention Reverses the Effect of Prediction in Silencing Sensory Signals. Cerebral Cortex, 22(9), 2197–2206. https://doi.org/10.1093/cercor/bhr310Lavie, N., and de Fockert, J. (2006). Frontal control of attentional capture in visual search. Visual Cognition, 14(4–8), 863–876. https://doi.org/10.1080/13506280500195953McKay, C. C., van den Berg, B., and Woldorff, M. G. (2017). Neural cascade of conflict processing: Not just time-on-task. Neuropsychologia, 96, 184–191. https://doi.org/10.1016/j.neuropsychologia.2016.12.022Molloy, K., Griffiths, T. D., Chait, M., and Lavie, N. (2015a). Inattentional Deafness: Visual Load Llleads to Time-Specific Suppression of Auditory Evoked Responses. Journal of Neuroscience, 35(49), 16046–16054. https://doi.org/10.1523/JNEUROSCI.2931-15.2015Molloy, K., Griffiths, T. D., Chait, M., and Lavie, N. (2015b). Inattentional Deafness: Visual Load Leads to Time-Specific Suppression of Auditory Evoked Responses. Journal of Neuroscience, 35(49), 16046–16054. https://doi.org/10.1523/JNEUROSCI.2931-15.2015Nigbur, R., Cohen, M. X., Ridderinkhof, K. R., and Stürmer, B. (2012a). Theta Dynamics Reveal Domain-specific Control over Stimulus and Response Conflict. Journal of Cognitive Neuroscience, 24(5), 1264–1274. https://doi.org/10.1162/jocn_a_00128Nigbur, R., Cohen, M. X., Ridderinkhof, K. R., and Stürmer, B. (2012b). Theta Dynamics Reveal Domain-specific Control over Stimulus and Response Conflict. Journal of Cognitive Neuroscience, 24(5), 1264–1274. https://doi.org/10.1162/jocn_a_00128Polk, T. A., Drake, R. M., Jonides, J. J., Smith, M. R., and Smith, E. E. (2008). Attention Enhances the Neural Processing of Relevant Features and Suppresses the Processing of Irrelevant Features in Humans: A Functional Magnetic Resonance Imaging Study of the Stroop Task. The Journal of Neuroscience, 28(51), 13786–13792. https://doi.org/10.1523/JNEUROSCI.1026-08.2008Ruggeri, P., Meziane, H. B., Koenig, T., and Brandner, C. (2019). A fine-grained time course investigation of brain dynamics during conflict monitoring. Scientific Reports, 9(1), 3667. https://doi.org/10.1038/s41598-019-40277-3Theeuwes, J. (2010). Top–down and bottom–up control of visual selection. Acta Psychologica, 135(2), 77–99. https://doi.org/10.1016/j.actpsy.2010.02.006Woldorff, M. G., Gallen, C. C., Hampson, S. A., Hillyard, S. A., Pantev, C., Sobel, D., and Bloom, F. E. (1993). Modulation of early sensory processing in human auditory cortex during auditory selective attention. Proceedings of the National Academy of Sciences, 90(18), 8722–8726. https://doi.org/10.1073/pnas.90.18.8722Yeung, N., Cohen, J. D., and Botvinick, M. M. (2011). Errors of interpretation and modeling: A reply to Grinband et al. NeuroImage, 57(2), 316–319. https://doi.org/10.1016/j.neuroimage.2011.04.029</p></body></sub-article></article>