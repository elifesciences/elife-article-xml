<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">40231</article-id><article-id pub-id-type="doi">10.7554/eLife.40231</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Communication</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Real-time experimental control using network-based parallel processing</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-118305"><name><surname>Kim</surname><given-names>Byounghoon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7159-5134</contrib-id><email>bkim10@wisc.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118342"><name><surname>Kenchappa</surname><given-names>Shobha Channabasappa</given-names></name><email>ckshobha@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118311"><name><surname>Sunkara</surname><given-names>Adhira</given-names></name><email>adhirasunkara@gmail.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118312"><name><surname>Chang</surname><given-names>Ting-Yu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3964-0905</contrib-id><email>tchang47@wisc.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118313"><name><surname>Thompson</surname><given-names>Lowell</given-names></name><email>lwthompson@wisc.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118314"><name><surname>Doudlah</surname><given-names>Raymond</given-names></name><email>doudlah@wisc.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-117318"><name><surname>Rosenberg</surname><given-names>Ari</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8606-2987</contrib-id><email>ari.rosenberg@wisc.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Neuroscience, School of Medicine and Public Health</institution><institution>University of Wisconsin–Madison</institution><addr-line><named-content content-type="city">Madison</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Surgery</institution><institution>Stanford University School of Medicine</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Nelson</surname><given-names>Sacha B</given-names></name><role>Reviewing Editor</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>02</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e40231</elocation-id><history><date date-type="received" iso-8601-date="2018-07-19"><day>19</day><month>07</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-01-29"><day>29</day><month>01</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Kim et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Kim et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-40231-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.40231.001</object-id><p>Modern neuroscience research often requires the coordination of multiple processes such as stimulus generation, real-time experimental control, as well as behavioral and neural measurements. The technical demands required to simultaneously manage these processes with high temporal fidelity is a barrier that limits the number of labs performing such work. Here we present an open-source, network-based parallel processing framework that lowers this barrier. The Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework offers multiple advantages: (<italic>i</italic>) a modular design that is agnostic to coding language(s) and operating system(s) to maximize experimental flexibility and minimize researcher effort, (<italic>ii</italic>) simple interfacing to connect multiple measurement and recording devices, (<italic>iii</italic>) high temporal fidelity by dividing task demands across CPUs, and (<italic>iv</italic>) real-time control using a fully customizable and intuitive GUI. We present applications for human, non-human primate, and rodent studies which collectively demonstrate that the REC-GUI framework facilitates technically demanding, behavior-contingent neuroscience research.</p><p><bold>Editorial note:</bold> This article has been through an editorial process in which the authors decide how to respond to the issues raised during peer review. The Reviewing Editor's assessment is that all the issues have been addressed (<xref ref-type="decision-letter" rid="SA1">see decision letter</xref>).</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>experimental control</kwd><kwd>control system</kwd><kwd>network communication</kwd><kwd>parallel processing</kwd><kwd>neuroscience software</kwd><kwd>open-source</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd><italic>Rhesus Macaque</italic></kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Alfred P. Sloan Foundation</institution></institution-wrap></funding-source><award-id>FG-2016-6468</award-id><principal-award-recipient><name><surname>Rosenberg</surname><given-names>Ari</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001391</institution-id><institution>Whitehall Foundation</institution></institution-wrap></funding-source><award-id>2016-08-18</award-id><principal-award-recipient><name><surname>Rosenberg</surname><given-names>Ari</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>DC014305</award-id><principal-award-recipient><name><surname>Rosenberg</surname><given-names>Ari</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY029438</award-id><principal-award-recipient><name><surname>Rosenberg</surname><given-names>Ari</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007046</institution-id><institution>Greater Milwaukee Foundation</institution></institution-wrap></funding-source><award-id>Shaw Scientist Award</award-id><principal-award-recipient><name><surname>Rosenberg</surname><given-names>Ari</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework can facilitate cutting-edge neuroscience research by providing precise experimental control using high-level programming environments familiar to many experimentalists.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Many areas of neuroscience research require real-time experimental control contingent on behavioral and neural events, rendering and presentation of complex stimuli, and high-density measurements of neural activity. These processes must operate in parallel, and with high temporal resolution. The number of labs that can perform such research is limited by the technical demands required to set up and maintain an appropriate control system. In particular, a control system must balance the need to precisely coordinate different processes and the flexibility to implement new experimental designs with minimal effort. Systems favoring precision over usability can hinder productivity because there is a large overhead to learning esoteric or low-level coding languages, and extensive coding demands slow the development of new paradigms. In contrast, systems favoring usability over precision can limit the complexity of supportable paradigms and the ability to perform experiments with high computational demands. Here we present the Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework, which overcomes technical challenges limiting previous solutions by using network-based parallel processing to provide both system precision and usability.</p><p>The REC-GUI framework segregates tasks into major groups such as: (<italic>i</italic>) experimental control and signal monitoring, (<italic>ii</italic>) stimulus rendering and presentation, and (<italic>iii</italic>) external data acquisition for high-density neural recordings. Each major group is executed on a different CPU, with communications between CPUs achieved using internet protocols. Additional CPUs can be used to support any number of major groups, as needed. The REC-GUI framework aims to overcome technical challenges that hinder productivity and consume lab resources by providing a solution that reduces the time and effort required to implement experimental paradigms. Towards this goal, we developed the REC-GUI framework using cross-platform, high-level programming environments. For example, the GUI used for experimental control and signal monitoring is coded in Python. The framework is inherently modular, so system components can be modified or substituted to meet research needs (e.g., using MATLAB to present visual stimuli, or Python to track an animal’s position). Because the REC-GUI framework achieves precise experimental control with high-level programming environments, it has a low barrier to entry and can be customized without professional programmers or low-level coding languages.</p><p>We demonstrate the generality of the REC-GUI framework using three distinct applications. The first investigates the neural basis of three-dimensional (3D) vision in non-human primates. This application shows that REC-GUI can support state-of-the-art stimulus display capabilities during computationally demanding behavior-contingent experiments, and seamlessly integrate with external data acquisition systems to precisely align behavioral and neuronal recordings. The second investigates 3D vision in humans. This application shows that REC-GUI can implement real-time experimental control using inputs from multiple external devices, and that the experimental control CPU can also support data acquisition. The third implements behavioral training and assessment of mice in a passive avoidance task. This application shows that REC-GUI can control experiments without a fixed trial structure by automating the transformation of external device inputs (e.g., position information from a video tracker) into control outputs for other devices (e.g., a shock scrambler). Our tests demonstrate that the REC-GUI framework facilitates technically demanding experiments necessary for relating neural activity to behavior and perception in a broad range of experimental modalities.</p><p>We provide sample code and hardware configurations as templates for adaptation and customization. The REC-GUI website (accessible from <ext-link ext-link-type="uri" xlink:href="https://rosenberg.neuro.wisc.edu/">https://rosenberg.neuro.wisc.edu/</ext-link>) includes a discussion forum, GitHub link to the software and user manual, and links to relevant downloads. The REC-GUI framework can reduce time needed for programming experimental protocols, facilitate time to focus on research questions and design, improve reproducibility by simplifying the replication of paradigms, and enable scientific advances by reducing technical barriers associated with complex neuroscience studies.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Approaches to experimental control</title><p>There are multiple ways to implement experimental control. Here we benchmark alternatives and identify an option that satisfies the joint needs of high temporal precision, experimental flexibility, and minimizing coding demands. To illustrate differences between approaches, consider the task of mapping the visual receptive field of a neuron in a behaving monkey. To perform this task, the monkey must hold its gaze on a fixation target presented on a screen. While the monkey maintains fixation, the experimenter must control the movement of a visual stimulus, such as a bar, on the screen. The success of the mapping depends upon the monkey maintaining accurate and precise gaze on the target. If the monkey breaks fixation, the stimulus must disappear until fixation is reacquired. The code required to implement this task includes three major processes that interface hardware and software: (<italic>i</italic>) eye position tracker with monitoring routines, (<italic>ii</italic>) visual display for stimulus presentation with functions to account for fixation status, and (iii) interactive experimenter control for moving the stimulus and changing parameters in real time.</p><p>A <italic>serial processing framework</italic> executes these processes serially within a while-loop (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). As a consequence of serial processing, every iteration of the loop has a pause in eye position monitoring while the stimulus-related and user-related processes finish. This delay can be problematic, especially if the stimulus rendering demands are high. For example, if rendering and presenting the stimulus takes longer than one cycle of the eye position monitoring process, eye position data will be lost, and the accuracy of the gaze-contingent control compromised. Using this problem as an example of the challenges that arise for a control system, we next consider a solution using multithreading.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.002</object-id><label>Figure 1.</label><caption><title>Experimental control frameworks.</title><p>(<bold>A</bold>) Serial processing: All processes are executed serially in a while-loop. (<bold>B</bold>) Multithreading: A single CPU executes multiple processes in parallel on different threads. (<bold>C</bold>) Network-based parallel processing: Multiple processes are executed in parallel on different CPUs coordinated over a network. Individual processes in CPU #2 can be implemented serially or using multithreading. Arrows indicate the direction of information flow.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig1-v1.tif"/></fig><p>A <italic>multithreading framework</italic> provides a solution to this problem by allowing the CPU to execute multiple processes concurrently (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Specifically, by separating the eye position monitoring, stimulus-related, and user-related processes onto independent threads, a CPU can execute the processes in parallel such that eye position monitoring can proceed without having to wait for the stimulus- and user-related processes to finish. However, some major coding environments, such as MATLAB, do not currently support multithreading for customized routines. This limitation in multithreading together with the high system demands of such environments can limit real-time experimental control capabilities. Furthermore, since all tasks are implemented on a single CPU, unresolvable system conflicts may arise if different hardware components are only compatible with certain operating systems.</p><p><italic>A network-based parallel processing framework</italic> provides a versatile solution to this problem by dividing experimental tasks across multiple CPUs (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This allows tasks to be executed as parallel processes even if multithreading is not supported. Thus, a major benefit of the REC-GUI framework is that challenges arising from limited multithreading support can be resolved without sacrificing the development benefits of software packages that minimize the need for low-level coding knowledge, such as Psychtoolbox which is widely used for stimulus generation (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib16">Pelli, 1997</xref>; <xref ref-type="bibr" rid="bib14">Kleiner et al., 2007</xref>). This is particularly valuable if computationally demanding, real-time stimulus rendering is required. A further benefit not possible with multithreading on a single CPU is that different task components can be implemented using different coding languages and on different operating systems. This feature is especially beneficial since research often requires multiple distinct system components. In two applications, we highlight this versatility by using MATLAB to render and present stimuli with Psychtoolbox 3 on one CPU, and Python to run the GUI that implements experimental control on a second CPU. With this configuration, information about changes in fixation status and user inputs to the GUI are relayed via network packets to MATLAB which updates the stimulus accordingly. This ensures that effectively no cycles of eye position data are lost since sending a network packet takes microseconds. More broadly, network-based parallel processing allows the REC-GUI framework to support a wide range of experimental preparations, as long as the system components support network interfacing.</p></sec><sec id="s2-2"><title>Overview of the REC-GUI framework</title><p>Experimental control is implemented in the REC-GUI framework using network-based parallel processing. The framework divides tasks into major groups, such as: (<italic>i</italic>) experimental control and signal monitoring, (<italic>ii</italic>) stimulus rendering and presentation, and (<italic>iii</italic>) external data acquisition. The number of groups and how tasks are divided between them is determined based on experimental needs. Different groups are executed on separate CPUs that communicate through internet protocols: user datagram protocol (UDP) and transmission control protocol (TCP). The choice of where to use UDP or TCP depends on the task demands. UDP is fast because it does not perform error checking (i.e., processing continues without waiting for a return signal confirming if a data packet was received). TCP is slower than UDP, but more reliable because it uses error checking and temporally ordered data transmission. To demonstrate the generality of the REC-GUI framework, we implemented three distinct applications using different system configurations.</p></sec><sec id="s2-3"><title>Three example applications</title><p><italic>Application 1: Neural basis of 3D vision in non-human primates.</italic> A 3D orientation discrimination task was performed by a rhesus monkey (<italic>Macaca mulatta</italic>). Surgeries and procedures were approved by the Institutional Animal Care and Use Committee at the University of Wisconsin–Madison, and in accordance with NIH guidelines. A male rhesus monkey was surgically implanted with a Delrin ring for head restraint. At the time of the procedure, the monkey was ~4 years of age and 7 kg in weight. After recovery, the monkey was trained to fixate a visual target within 2° version and 1° vergence windows using standard operant conditioning techniques.</p><p>The monkey was then trained to perform an eight-alternative 3D orientation discrimination task (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). In the task, the monkey viewed 3D oriented planar surfaces, and reported the direction of planar tilt with a saccadic eye movement to an appropriate choice target. Planar surfaces were presented at tilts ranging from 0° to 315° in 45° steps, and slants ranging from 15° to 60° in 15° steps. A frontoparallel plane (tilt undefined, slant = 0°) was also presented. The surfaces were defined as random dot stereograms with perspective and stereoscopic cues (N = 250 dots; dots subtended 0.3° of visual angle at the screen distance of 57 cm). At the start of each trial, the monkey fixated a target at the center of an otherwise blank screen for 300 ms. A 20° stimulus centered on the target was then presented for 1,000 ms while fixation was maintained. The stimulus and fixation target then disappeared, and eight choice targets appeared at a radial distance of 11.5° with angular locations of 0° to 315° in 45° increments (corresponding to the possible planar tilts). The monkey was rewarded with a drop of water or juice for choosing the target in the direction that the plane was nearest. If fixation was broken before the appearance of the choice targets, the trial was aborted.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.003</object-id><label>Figure 2.</label><caption><title>Behavioral tasks.</title><p>(<bold>A</bold>) Application 1: Neural basis of 3D vision in non-human primates. A monkey fixated a target (red) at the screen center for 300 ms. A planar surface was then presented at one of eight tilts for 1000 ms (one eye’s view is shown) while fixation was maintained. The plane and fixation target then disappeared, and eight choice targets corresponding to the possible tilts appeared. A liquid reward was provided for a saccade (yellow arrow) to the target in the direction that the plane was nearest. (<bold>B</bold>) Application 2: 3D vision in humans. The task was similar to the monkey task, but there were sixteen tilts/targets, and choices were reported using the cursor of a computer mouse that appeared at the end of the stimulus presentation. In <italic>A</italic> and <italic>B</italic>, dot sizes and numbers differ from in the actual experiments. (<bold>C</bold>) Application 3: Passive avoidance task in mice. A mouse was placed in a bright room that was separated from a dark room by a gate. After the gate was lifted, the mouse entered the dark room. This was automatically detected by the GUI which triggered an electric shock through the floor of the dark room.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig2-v1.tif"/></fig><p>To confirm the ability to accurately and precisely align stimulus-related, behavior-related, and neuronal events using the REC-GUI framework, we measured the 3D orientation tuning of neurons in the caudal intraparietal (CIP) area while the monkey performed the task (<xref ref-type="bibr" rid="bib17">Rosenberg et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Rosenberg and Angelaki, 2014a</xref>; <xref ref-type="bibr" rid="bib19">Rosenberg and Angelaki, 2014b</xref>). A tungsten microelectrode (~1 MΩ; FHC, Inc.) was targeted to CIP using magnetic resonance imaging scans. The CARET software was used to segment visual areas, and CIP was identified as the lateral occipitoparietal zone (<xref ref-type="bibr" rid="bib21">Van Essen et al., 2001</xref>; <xref ref-type="bibr" rid="bib17">Rosenberg et al., 2013</xref>). A recording grid for guiding electrode penetrations was aligned with the MRI in stereotaxic coordinates using ear bar and grid markers (<xref ref-type="bibr" rid="bib15">Laurens et al., 2016</xref>). Neuronal responses were sampled and digitized at 30 Khz using a Scout Processor (Ripple, Inc.). Single-neuron action potentials were identified by waveform (voltage-time profile) using the Offline Sorter (Plexon, Inc.).</p><p><italic>Application 2: 3D vision in humans.</italic> A variation of the 3D orientation discrimination task was performed by an adult human male with corrected-to-normal vision (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The experimental procedures were approved by the University of Wisconsin–Madison Institutional Review Board, and carried out in accordance with the Declaration of Helsinki. Planar surfaces were presented at sixteen tilts (0° to 337.5° in 22.5° steps), with a slant of 60°. The surfaces were 10° and defined as random dot stereograms with perspective and stereoscopic cues (N = 75 dots; dot size = 0.3° at the screen distance of 57 cm). The trial structure matched the monkey experiment. A chin and forehead rest was used to facilitate a stable head position. Fixation was maintained within 3° version and 2° vergence windows. After a stimulus was presented, the fixation target disappeared and a computer mouse cursor appeared at that location. Sixteen choice targets (0° to 337.5° in 22.5° steps) appeared at a radial distance of 6.5°. The participant then used the mouse to select the choice target corresponding to the perceived planar tilt. If fixation was broken before the appearance of the choice targets, the trial was aborted.</p><p><italic>Application 3: Passive avoidance task in mice.</italic> We implemented a passive avoidance task designed to test learning and memory of fear events in rodents (<xref ref-type="bibr" rid="bib5">Coronas-Samano et al., 2016</xref>). The task was performed by a male mouse (C57BL/6, ~5 months of age and 35 g in weight). All procedures were approved by the Institutional Animal Care and Use Committee at the University of Wisconsin–Madison, and in accordance with NIH guidelines. The task was performed in a two-way shuttle box with two rooms (one bright, the other dark) divided by a wall with a gate (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). A video tracking module provided a top-down view of the bright room. The mouse was placed in the bright room with the gate closed. After 3 min, the gate was manually opened. Mice naturally prefer the dark room, so they are expected to enter it after a short delay. The mouse was removed from the dark room ~ 3 s after entering. This procedure was repeated twice. The shock scrambler was then activated, and the task was repeated three more times. The scrambler was triggered if the mouse left the bright room, delivering a small electric shock (&lt;0.9 mA), and the gate was manually closed. After ~ 3 s of shock delivery, the mouse was removed from the dark room and placed in its home cage for ~5 min. If the mouse did not enter the dark room after 3 min, it was moved to its home cage.</p></sec><sec id="s2-4"><title>System configurations for the three applications</title><p><italic>Application 1: Neural basis of 3D vision in non-human primates.</italic> This application implements a 3D discrimination task with neuronal recordings using three major task groups: (<italic>i</italic>) experimental control, signal monitoring, and external device control, (<italic>ii</italic>) stimulus rendering and presentation, and (<italic>iii</italic>) external data acquisition (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The application requires the stereoscopic display of stimuli rendered using multi-view geometry (<xref ref-type="bibr" rid="bib7">Hartley and Zisserman, 2003</xref>), eye tracking for gaze-contingent stimulus presentation and detecting perceptual reports, reward delivery, and neuronal recordings. Visual stimuli were rendered as right and left eye ‘half-images’ using Psychtoolbox 3 in MATLAB 2017a, and run on Linux (Ubuntu 16.04, Intel Xeon Processor, 24 GB RAM, NVIDIA GeForce GTX 970). A PROPixx 3D projector (VPixx Technologies, Inc.) with circular polarizer was used to rear-project the stimuli onto a screen at 240 Hz, using the ‘flip’ command (<xref ref-type="bibr" rid="bib14">Kleiner et al., 2007</xref>) to alternately present the appropriate half-image to the right or left eye (120 Hz per eye). To account for the temporal difference between the projector’s refresh rate and the execution rate of the MATLAB script, the flip command was set to wait until the next available cycle of the projector refresh. This minimized the variability in the delay between the flip command and the appearance of the stimulus. A second setup using a 3D monitor (LG Electronics, Inc.) and NVIDIA-2 3D Vision Kit operating at 120 Hz with shutter glasses (run on Windows 10, Intel Xeon processor, 8 GB RAM, NVIDIA Quadro K4000 graphics card) was used to confirm that the framework is robust to system changes (dashed lines in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Results from this setup are not presented since they confirm the more stringent testing results from the first setup.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.004</object-id><label>Figure 3.</label><caption><title>Three system configurations using the Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework.</title><p>Experimental control and monitoring are achieved using a GUI (green box) that coordinates components such as: (i) stimulus rendering and presentation, (ii) external input devices, (iii) external output devices, and (iv) data acquisition. Arrows indicate the direction of information flow. (<bold>A</bold>) Application 1: Neural basis of 3D vision in non-human primates. This application implements a gaze-contingent vision experiment with neuronal recordings. Dashed lines show an alternative communication pathway. (<bold>B</bold>) Application 2: 3D vision in humans. This application implements a gaze-contingent vision experiment without external devices for data acquisition or reward delivery. (<bold>C</bold>) Application 3: Passive avoidance task in mice. This application uses a video tracking module and external shock scrambler to automate behavioral training and assessment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.40231.005</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Experimental routine and communication flowchart between the stimulus CPU and experimental control CPU showing the exchange of parameters for stimulus rendering and behavioral control for Application 1.</title><p>Arrows indicate the direction of information flow.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.40231.006</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Network configuration for Application 1: Neural basis of 3D vision in non-human primates.</title><p>The Scout Processor and EyeLink have non-configurable IP addresses, so two network switches and multiple network interface cards (NICs) are required to route different subgroups of IP addresses to the stimulus and experimental control CPUs. Arrows indicate the direction of information flow.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig3-figsupp2-v1.tif"/></fig></fig-group><p>The REC-GUI framework supports eye tracking using video or scleral search coil (<xref ref-type="bibr" rid="bib10">Judge et al., 1980</xref>) methods (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>). For this application, we used video tracking with an EyeLink 1000 plus (SR-Research, Inc.). Binocular eye positions were sampled (1 kHz) and digitized by the EyeLink, and the measurements were sent to the GUI through a TCP connection for real-time analysis. The same measurements were converted into an analog signal and transferred to the Scout Processor for offline analysis. For the GUI to perform real-time analysis of eye movements measured with search coils, the analog outputs of the coil system would need to be sampled and digitized using an analog to digital converter (e.g., USB-1608G, Measurement Computing, Inc.). The same outputs would be transferred to the Scout Processor for offline analysis.</p><p>A Scout Processor connected to a Windows 10 CPU (Intel Xeon Processor, 8 GB RAM, NVIDIA GeForce GT 720) was used for the acquisition of electrophysiological data and saving analog and digital signals from external devices. The Scout Processor generated a data file with synchronized behavioral and neuronal signals. Other acquisition systems that support network interfacing can be substituted. Since timestamps are generated in the Scout Processor (a standard acquisition system function), no additional synchronization was required to temporally align the input signals. The GUI saved a backup file on the experimental control CPU containing all data that it transmitted and received along with event codes signaling the occurrence of experimental (e.g., fixation point on, stimulus on) and behavioral (e.g., fixation acquired, choice made) events.</p><p>Since the application has three major groups, it requires three CPUs: an experimental control CPU, a stimulus CPU, and a data acquisition CPU. A schematic of the experimental routine and communication between the experimental control and stimulus CPUs is shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. The experimental control CPU uses multithreading to run two threads that: (<italic>i</italic>) monitor and evaluate eye position, and (<italic>ii</italic>) update parameters in real-time using inputs to the GUI. The stimulus CPU uses a while-loop to render stimuli with parameters provided by the GUI, and to perform gaze-contingent stimulus presentation by repeatedly querying the GUI about the fixation status through a UDP connection. Importantly, the UDP connection is established in asynchronous mode so that the while-loop (execution flow) does not pause while waiting for data packets, which could cause dropped frames and time lags. This application thus tests if the framework can accurately and precisely control external hardware under computationally demanding scenarios, and coordinate with external acquisition devices. The major challenge here is the real-time rendering of 3D stimuli while achieving state-of-the-art display capabilities (high frame rate, 240 Hz presentation) without dropped frames, time lags, or compromising the accuracy of the gaze contingencies.</p><p><italic>Application 2: 3D vision in humans.</italic> This application implements a psychophysical task without neural recordings or reward delivery using two major task groups: (<italic>i</italic>) experimental control, signal monitoring, and data acquisition, and (<italic>ii</italic>) stimulus rendering and presentation (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). A VIEWPixx/3D LED display screen (VPixx Technologies, Inc.) operating at 120 Hz with active shutter glasses was used for stereoscopic presentation. As in Application 1, eye tracking was used for gaze-contingent stimulus presentation. However, perceptual reports were made using the cursor of a computer mouse connected to the experimental control CPU. To implement this change, the eye position monitoring routine used in the first application during the perceptual reporting phase of a trial (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, ‘saccade made’ on the bottom left side under Stimulus CPU) was exchanged for a routine that tracks the mouse cursor position and registers ‘clicks’. Experimental control in Application 2 thus required two independent external device inputs, one to track eye position and a second to track mouse cursor position. Unlike in Application 1, the experimental control CPU served as the data acquisition system, saving eye position and computer mouse traces in addition to trial information. This application thus tests if the framework can implement real-time experimental control based on inputs from multiple external devices, and support data acquisition without an external system.</p><p><italic>Application 3: Passive avoidance task in mice.</italic> This application implements a passive avoidance task using two major task groups: (<italic>i</italic>) experimental control, signal monitoring, external device control, and data acquisition, and (<italic>ii</italic>) real-time video tracking of mouse position (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). A shuttle box (ENV-010MC, Med Associates, Inc.) was divided into bright and dark rooms by a gate. The position of the mouse within the bright room was tracked using a custom real-time video tracking module that we will describe in future work. However, any commercial tracker with analog or digital outputs could be substituted. Briefly, the tracking module segregates an object from the background using color, and estimates position as the center of mass. The module is coded in Python with the OpenCV library (<xref ref-type="bibr" rid="bib3">Bradski and Kaehler, 2008</xref>), and runs on Linux (Raspbian, Raspberry Pi Foundation). A Raspberry Pi B + with Pi camera module v2 was used to track the mouse with 300 x 400 pixel resolution at ~ 42 frames per second. The module sends position information to the GUI, which monitors and records the tracking results as well as the timing of events (e.g., opening of the gate dividing the two rooms, and the mouse entering the dark room). If the GUI detects that the mouse is not in the bright room, it can activate a shock scrambler (ENV-414S, Med Associates, Inc.) to deliver a foot shock through the floor of the dark room. The experimental control CPU was also used for data acquisition. Using the example of behavioral training and assessment, this application thus tests if the framework can automate experimental control in a task without a fixed trial structure by transforming external device inputs into control outputs for other devices.</p></sec><sec id="s2-5"><title>System communications</title><p>In the REC-GUI framework, system components communicate through four types of connections: UDP, TCP, analog signals, and transistor-to-transistor logic (TTL) pulses. UDP and TCP connections are achieved with network switches. To provide an example of system communications, we use Application 1 since it has the most complicated structure of the three applications tested. Communications between the experimental control CPU, stimulus CPU, EyeLink (eye tracking system), and Scout Processor (data acquisition system) are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. Two groups of digital event codes are carried over UDP and TCP connections: (<italic>i</italic>) stimulus-related (fixation target on/off, stimulus on/off, choice targets on/off, etc.), and (<italic>ii</italic>) behavior-related (fixation acquired, fixation broken, choice made, etc.). Stimulus-related event codes are triggered in MATLAB and sent to the GUI using UDP and the Scout Processor using TCP. Behavior-related events triggered by the experimental control CPU are sent to MATLAB which relays the codes to the Scout Processor.</p><p>Experimental systems often require multiple pieces of specialized hardware produced by different companies. Network-based communications with such hardware must often occur over non-configurable, predefined subgroups of IP addresses. This occurs in Application 1 since both the EyeLink and Scout Processor have non-configurable, predefined subgroups of IP addresses that cannot be routed over the same network interface card (NIC) without additional configuration of the routing tables. In such cases, a simple way to set up communication with the hardware is to use multiple parallel networks with a dedicated network switch for each piece of hardware. For Application 1, this requires two network switches, and both the stimulus and experimental control CPUs require two NICs assigned to different subgroups of IP addresses (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). With this configuration, the stimulus and experimental control CPUs can both directly communicate with the EyeLink and Scout Processor.</p><p>Digital communications (TTL pulses) are used for control signals. For example, to open a solenoid valve to deliver a liquid reward (Application 1), or trigger a shock scrambler to provide an electric shock (Application 3). TTL pulses can be controlled by MATLAB and generated by a DataPixx (VPixx Technologies, Inc.; solid blue line in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). They can also be controlled by the GUI and generated by a digital input/output (DIO) interface (e.g., USB-1608G, Measurement Computing, Inc.; dashed blue line in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, blue line in <xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p></sec><sec id="s2-6"><title>Configurable experimental control GUI</title><p>A core component of the REC-GUI framework is an intuitive and fully configurable GUI for experimental control and signal monitoring (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The GUI contains six main panels: a monitoring window, task control panel, sending panel, receiving panel, data log, and configurable interfaces for specialized tools. To accommodate different experimental paradigms, the GUI provides a simple method for adding and removing parameters from the control set, as well as manipulating the parameters. The GUI was coded in Python 2.7, and run on Linux for all tested applications (Ubuntu 14.04; Applications 1 and 2: Intel i3 Processor, 8 GB RAM, Intel HD500; Application 3: Core 2 Duo 2.80 GHz, 4 GB RAM, NVIDIA GeForce4 MX).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.007</object-id><label>Figure 4.</label><caption><title>Graphical user interface.</title><p>The GUI is a customizable experimental control panel, configured here for vision experiments. The upper left corner is a monitoring window, showing a scaled depiction of the visual display. Fixation windows and eye position markers are at the center. Eight choice windows for Application 1 are also shown (correct choice in red, distractors in blue). Below the monitoring window are eye configuration and receptive field mapping tools, which can be substituted for other experiment-specific tools. Task control for starting, pausing, or stopping a protocol is at the center top, along with subject- and system-specific configuration parameters. The sending panel in the upper right allows the experimenter to modify task parameters in real time. The receiving panel below that is used to display information about the current stimulus and experiment progress. The lower right panel shows a data log.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.40231.008</object-id><label>Figure 4—figure supplement 1.</label><caption><title>GUI configured with programmable command buttons for Application 3.</title><p>The display configuration tool controls the monitoring window panel. The programmable command buttons execute user-defined callback routines.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Information about the ongoing status of an experiment and graphical feedback of critical data is often required to monitor and adjust parameters in real time. The GUI displays continuously sampled measurements (e.g., eye position, animal location, joint configuration, firing rate, etc.) in the monitoring window. To visually evaluate contingencies, the monitoring window can display boundary conditions (e.g., fixation windows, or demarcated areas of an arena). Such boundaries can be used to automatically trigger event signals (e.g., TTL pulses) that control external devices such as a solenoid for delivering a liquid reward or electrical stimulator. An experimenter can turn boundaries on/off, change their size, number, locations, etc. in real time through inputs to the GUI (see user manual).</p><p>Such changes are implemented in the REC-GUI framework using UDP communications to send/receive strings (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Each string in the UDP packet consists an identifier (e.g., −106; a number which the stimulus code uniquely associates with a specific variable such as ‘StimulusDuration’) and a value (e.g., one to specify a stimulus duration of 1 s), followed by a terminator (qqqq….q/padded to 1024 characters). In this example, the string would be −106 one qqqq….q/ (see user manual for details). The GUI contains separate panels for sending and receiving UDP packets between the experimental control CPU and CPUs implementing major task groups (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For example, in the sending panel, an experimenter can enter values (e.g., 1) for predefined variables (e.g., ‘StimulusDuration’) with associated identifiers (e.g., ‘−106’) and click ‘Submit’ to send UDP packets to a stimulus CPU. In the first two applications, the sending panel sends control information to a stimulus CPU. In the third application, it sends information to a video tracking module. It can send information to any machine capable of receiving UDP packets, as required for an experiment. The receiving panel allows the experimenter to predefine identifiers and variable names for receiving and displaying information from other CPUs (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For example, the current trial number can be displayed using the identifier 205 and variable name ‘TrialNum’. In this way, ongoing experimental information can be monitored in real time. This text-based approach provides a simple way to reconfigure the GUI to meet the demands of different experimental paradigms.</p><p>The GUI also contains placeholders in the lower left corner that allow experimenters to configure interfaces for specialized tools. We currently provide two default configurations. The first has interfaces to control eye calibration and receptive field mapping for vision studies. This version was used for Applications 1 and 2 (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The second has programmable command buttons for executing user-defined callback routines, and was used for Application 3 (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). These interfaces can be substituted with other tools by editing the Python GUI code (see user manual).</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><sec id="s3-1"><title>Network and system performance tests</title><p>The REC-GUI framework uses network-based parallel processing to implement experimental control and synchronize devices. In this section, we use Application 1 (neural basis of 3D vision in non-human primates) to test the latency of communication between the stimulus and experimental control CPUs resulting from hardware and software processing, as well as the performance of the main loop responsible for stimulus rendering and presentation. These tests demonstrate that the REC-GUI framework attains accurate and precise experimental control while implementing computationally demanding routines and enforcing behavioral contingencies.</p><p>Since different experimental processes (‘major groups’) are implemented on separate CPUs that communicate over a network, it is possible that limitations in network capacity can introduce delays that adversely affect performance. To test this possibility, we measured the delay resulting from the network hardware. Network latency was measured using a simple ‘ping’ command that is used to test reachability, signal fidelity, and latency in network communication between two hosts (<xref ref-type="bibr" rid="bib1">Abdou et al., 2017</xref>). This ping sends an internet control message protocol requesting an echo reply from the target host. The latency of the ping is the round-trip duration of the packets between the two hosts. Network latency between the experimental control and stimulus CPUs was negligibly small (average latency = 24 µs; standard deviation: σ = 5.1 µs; N = 1000 pings), indicating that the network hardware did not introduce substantial delays that could adversely affect performance (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.009</object-id><label>Figure 5.</label><caption><title>System performance.</title><p>(<bold>A</bold>) Latency introduced by the network hardware (N = 1000 pings). (<bold>B</bold>) Overall system performance measured using the round-trip latency of UDP packets between the experimental control (GUI) and stimulus (MATLAB) CPUs with complex stimulus rendering routines (‘high demand’ on the system; N = 500 round-trip packet pairs). (<bold>C</bold>) Same as B without the stimulus rendering routines (‘low demand’ on the system; N = 500). (<bold>D</bold>) Duration of the main while-loop in the stimulus CPU with complex stimulus rendering routines (blue bars) or without (orange bars), N = 3000 iterations each. Vertical gray dotted lines mark mean durations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig5-v1.tif"/></fig><p>Next we measured the overall system performance which includes hardware delays tested above as well as software delays from the main loops (i.e., all executed processes) on both CPUs. In this test, the GUI sends a UDP packet to the stimulus CPU and after that packet is detected, MATLAB returns another UDP packet. We measured the latency of multiple round-trip sent/received packets (GUI → MATLAB → GUI), and the distribution of latencies is shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. On average, the total duration of the round-trip packets for the first application was 6.79 ms (σ = 2.9 ms; N = 500). This latency determines the time interval required to synchronize the CPUs when the experiment includes a computationally intensive task (in this case, rendering large 3D stimuli with depth variations and occlusion testing). For many experiments, performance would exceed this level. To determine an upper-bound of performance with this configuration, we removed all processes related to generating the 3D visual stimuli from the MATLAB main loop and measured the round-trip latency of the UDP packets. The average round-trip latency dropped to 4.3 ms (σ = 2.3 ms; N = 500; <xref ref-type="fig" rid="fig5">Figure 5C</xref>). Note that this latency only limits real-time control, not the temporal alignment that can be achieved offline, as shown below using neuronal recording tests. Even with the longer latencies that occurred when rendering 3D stimuli, the achieved millisecond-level of control synchronization is sufficient for most experiments.</p><p>Lastly, we measured the duration of the main loop in the stimulus CPU, excluding the UDP processing time for communication with the experimental control CPU. This test includes all serially executed processes associated with stimulus rendering and presentation, as well as conditional statements for controlling experimental flow. The duration of the main loop was on average 3.39 ms (σ = 1.3 ms; N = 3000 iterations) when rendering the 3D stimuli, thus accounting for about half the time required to synchronize the CPUs (blue bars in <xref ref-type="fig" rid="fig5">Figure 5D</xref>). To determine a lower-bound duration of the MATLAB main loop with this configuration, we removed the stimulus-related routines. After removing this code, the average latency of the main loop dropped to 1.74 ms (σ = 0.11 ms; N = 3000 iterations; orange bars in <xref ref-type="fig" rid="fig5">Figure 5D</xref>). These tests demonstrate that the REC-GUI framework can facilitate complex experimental tasks in real-time, with very low latencies, and using high-level programming environments.</p></sec><sec id="s3-2"><title>External device control tests</title><p>The ability to accurately and precisely control external devices is critical for experimental studies. In this section, we use the first application (neural basis of 3D vision in non-human primates) to test the ability of the REC-GUI framework to implement such control over the real-time rendering and presentation of large, computationally intensive stereoscopic stimuli at 240 Hz while enforcing fixation. As discussed in the Materials and methods, the concern is that this combination can cause dropped frames and/or time lags. Indeed, previous attempts to use high-level programming environments to present stimuli at this high frame rate while enforcing fixation have failed (see Discussion). To assess the fidelity of the presentation, phototransistor circuits were used to track the appearance of stimuli on the screen by detecting a small bright patch in the lower right or left corner of the corresponding eye’s half-image (see user manual).</p><p>Voltage traces produced by the phototransistor circuits were saved by the Scout Processor to provide a precise signal for aligning events to the stimulus as well as to confirm the fidelity of stimulus presentation on a trial-by-trial basis. For example, if dropped frames occur on a certain trial, that can be detected and the trial discarded during data analysis. Representative traces showing the presentation of right (blue) and left (orange) eye images are shown in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. The latency between the initial flip command and the appearance of the stimulus was very short (average delay = 4.71 ms, σ = 0.16 ms, N = 500 trials), which is only ~ 540 μs longer than the video refresh cycle of 4.17 ms (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The fidelity of alternating right and left eye frames was confirmed by assessing the time difference between the two voltage traces. Since the stereoscopic images were presented at 240 Hz (120 Hz per eye), the right and left eye frame signals should be temporally shifted by ~ 4.17 ms. We measured the time difference between each alternation of the right and left eye frames over the 1 s stimulus duration for 500 trials. The average time difference was 4.16 ms (σ = 0.066 ms; min = 3.9; max = 4.4), indicating that the right and left eye frames were well synchronized at the intended 240 Hz stimulus presentation rate (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This also confirms that no frames were dropped since dropped frames would appear as time differences ≥ 12.5 ms. These test results demonstrate that the REC-GUI framework can use high-level programming environments to achieve accurate and precise control of external devices during computationally demanding tasks.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.010</object-id><label>Figure 6.</label><caption><title>Quantifying the fidelity of external device control.</title><p>(<bold>A</bold>) Right and left eye frame signals measured on the screen. The anti-phase rise and fall of the two signals indicates that they are temporally synchronized. (<bold>B</bold>) Latency between the initial flip command in MATLAB and the appearance of the stimulus (N = 500 trials). (<bold>C</bold>) Time differences between the two eyes’ frames peak at 4.16 ms, indicating that the intended 240 Hz stimulus presentation was reliably achieved (N = 500 trials). Voltage traces were sampled at 30 kHz. Mean times are indicated by vertical gray dotted lines.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig6-v1.tif"/></fig></sec><sec id="s3-3"><title>Verification of real-time behavioral monitoring</title><p>Experimental control based on real-time monitoring of ongoing processes is critical for many studies. In this section, we confirm that the REC-GUI framework can be customized for a variety of neuroscience questions by presenting behavioral monitoring results from applications involving humans, non-human primates, and rodents.</p><p><italic>Application 1: Neural basis of 3D vision in non-human primates.</italic> In this application, we presented 3D planar surfaces contingent on the monkey maintaining fixation on a target (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Eye position data were relayed to the GUI which implemented routines to evaluate if the monkey was holding fixation on the target, if fixation was broken, or if a particular choice was made. Depending on the results of these routines, the GUI directed the stimulus CPU to enter a particular experimental state (e.g., fixation only, stimulus on, choice targets on, etc.; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>To demonstrate that the GUI successfully monitored the monkey’s eye positions, we show representative horizontal and vertical eye displacements as a function of time for 34 successfully completed trials in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. Two-dimensional (2D) eye traces showing that saccadic eye movements to each of the eight choice targets were accurately detected are shown in <xref ref-type="fig" rid="fig7">Figure 7B</xref>. As a further test, we implemented a fixation-in-depth task to show that the GUI can successfully enforce both version and vergence eye positions simultaneously. Fixation targets were presented at three depths relative to the screen, and the monkey was required to maintain fixation for 1 s. As shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, the GUI successfully enforced both version and vergence gaze contingencies. These tests demonstrate that the REC-GUI framework can implement experimental control contingent on real-time behavioral measurements.</p><fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.011</object-id><label>Figure 7.</label><caption><title>Application 1: Neural basis of 3D vision in non-human primates.</title><p>Verifying behavior-contingent experimental control. (<bold>A</bold>) Left and right eye traces as a function time during the 3D orientation discrimination task, aligned to the stimulus onset (N = 34 trials). The traces end when the choice was detected by the GUI. Horizontal and vertical components of the eye movements are shown in purple and red, respectively. Version was enforced using a 2° window. Vergence was enforced using a 1° window (data not shown). (<bold>B</bold>) Two-dimensional (2D) eye movement traces shown from stimulus onset until the choice was detected by the GUI (same data as in <bold>A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.40231.012</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Enforcement of version and vergence during fixation at different depths.</title><p>Stereoscopically rendered targets were presented at three depths relative to the screen. Fixation was held on a target for 1 s. Top row shows eye traces while fixation was held at each depth (N = 4 trials per depth). Dotted circles show the 2° version windows. Bottom row shows vergence error. Gray dotted lines show the 1° vergence window.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig7-figsupp1-v1.tif"/></fig></fig-group><p><italic>Application 2: 3D vision in humans.</italic> Similar to the first application, we presented 3D planar surfaces contingent on the participant maintaining fixation on a target (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Representative horizontal and vertical eye displacements are shown as a function of time for 34 successfully completed trials in <xref ref-type="fig" rid="fig8">Figure 8A</xref>. The movements of the computer mouse used to select one of the sixteen choice targets are shown for the same trials in <xref ref-type="fig" rid="fig8">Figure 8B</xref>. The 2D traces of both eyes and the mouse cursor are shown <xref ref-type="fig" rid="fig8">Figure 8C</xref>. This test shows that the REC-GUI framework can implement experimental control contingent on inputs from multiple external devices (i.e., eye tracker and computer mouse). Since all data analyzed in this subsection were saved by the experimental control CPU, this test further shows that inputs from external devices can be precisely aligned to stimulus-related events without a dedicated data acquisition system, such as the one used in Application 1. For example, this makes it possible to analyze the coordination of different motor effectors, such as the eyes and arms, without a dedicated acquisition system. Along this line, note that when selecting the choice target, the movement of the computer mouse initially lagged behind the movement of the eyes (<xref ref-type="fig" rid="fig8">Figure 8D</xref>).</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.013</object-id><label>Figure 8.</label><caption><title>Application 2: 3D vision in humans.</title><p>Verifying behavior-contingent experimental control. (<sc><bold>A</bold></sc>) Left eye traces as a function time during the 3D orientation discrimination task, aligned to the stimulus onset (N = 34 trials). The traces end when the choice (mouse click on a target) was detected by the GUI. Horizontal and vertical components of the eye movements are shown in purple and red, respectively. Version was enforced using a 3° window. Vergence was enforced using a 2° window (data not shown). (<bold>B</bold>) Computer mouse cursor traces for the same trials. The cursor appeared at the location of the fixation target at the end of the stimulus presentation. (<bold>C</bold>) 2D eye and mouse cursor traces ending when the choice was detected by the GUI. (<bold>D</bold>) Movement of both eyes and the mouse cursor for a single trial. Note that the movement of the mouse cursor was delayed relative to the movement of the eyes but caught up shortly after movement initiation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig8-v1.tif"/></fig><p><italic>Application 3: Passive avoidance task in mice.</italic> We implemented a passive avoidance task in which a two-way shuttle box was divided into bright and dark rooms by a gate (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Position within the bright room was automatically monitored using a video tracking module. The mouse initially explored a large portion of the bright room, and promptly entered the dark room when the gate was opened (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). After activating the shock scrambler, when the GUI detected that the mouse entered the dark room, it sent a control signal to the scrambler to deliver a small electric shock (&lt;0.9 mA). The entry door was manually closed. The mouse was removed from the dark room after ~ 3 s of shock delivery, and placed in its home cage for ~ 5 min. The mouse was then returned to the bright room (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). After the foot shock, the mouse explored a smaller area of the bright room (<xref ref-type="fig" rid="fig9">Figure 9C</xref>) and never entered the dark room (<xref ref-type="fig" rid="fig9">Figure 9D</xref>). The entry latencies and movements of the mouse were saved on the experimental control CPU. This application provides an example of automating behavioral training and assessment using the REC-GUI framework. More broadly, this test shows that the REC-GUI framework can automate experimental control by transforming external device inputs (e.g., video tracking data) into control outputs for other devices (e.g., a shock scrambler).</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.014</object-id><label>Figure 9.</label><caption><title>Application 3: Passive avoidance task in mice.</title><p>Verifying automated behavior-contingent experimental control. (<bold>A,B</bold>) The left column shows the movement of a mouse (blue trace) within the bright room before (<bold>A</bold>) and after (<bold>B</bold>) foot shock. The right column shows the movement as a function of time, and marks the gate opening. In <italic>A</italic>, the mouse’s entry into the dark room and the foot shock are also marked. (<bold>A</bold>) Before foot shock. (<bold>B</bold>) After foot shock. (<bold>C</bold>) The percentage of the bright room explored by the mouse before (blue bars) and after (orange bars) foot shock. The mouse explored much less area after the shock. (<bold>D</bold>) Time to enter the dark room after the gate was opened. Before the foot shock (blue bars), the mouse quickly entered the dark room. After the foot shock (orange bars), the mouse never entered the dark room (sessions were ended after 3 minutes).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig9-v1.tif"/></fig></sec><sec id="s3-4"><title>High precision temporal alignment of multiple data streams</title><p>We used the first application (neural basis of 3D vision in non-human primates) to confirm the ability to align neuronal, stimulus, and behavioral events in time. While the monkey performed the discrimination task, we measured the 3D surface orientation tuning of a CIP neuron. First, we confirmed the ability to precisely align stimulus-driven neuronal responses to the stimulus onset detected by the phototransistor. A raster plot showing the timing of action potentials for 545 trials aligned to the stimulus onset (each row is a different trial) along with the spike density function (red curve; convolution with a double exponential) is shown in <xref ref-type="fig" rid="fig10">Figure 10A</xref>. The alignment of spike times relative to the stimulus onset reveals a sharp visual transient, consistent with the properties of CIP. Second, we confirmed the ability to align spike times to the choice saccades. Saccade onsets were detected offline as the first time point at which the eye movement was faster than 150°/s (<xref ref-type="bibr" rid="bib12">Kim and Basso, 2008</xref>; <xref ref-type="bibr" rid="bib13">Kim and Basso, 2010</xref>). Spike times were aligned to the saccade onset and the spike density function calculated (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Note the build-up of activity preceding the saccade that was not evident when the spike times were aligned to the stimulus onset. As expected in CIP (<xref ref-type="bibr" rid="bib17">Rosenberg et al., 2013</xref>), the neuron was tuned for slant and tilt (<xref ref-type="fig" rid="fig10">Figure 10C</xref>). These tests confirm the ability to precisely align events recorded using the REC-GUI framework.</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.40231.015</object-id><label>Figure 10.</label><caption><title>Temporal alignment of action potentials to stimulus-related and behavioral events.</title><p>(<bold>A</bold>) Raster plot showing spike times aligned to the stimulus onset (N = 545 trials). Shaded region marks the stimulus duration. (<bold>B</bold>) Raster plot showing spike times aligned to the saccade onset. Each row is a different trial, and each dot marks a single action potential. Red curves are spike density functions. (<bold>C</bold>) 3D surface orientation tuning. Each curve shows tilt tuning at a fixed slant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40231-fig10-v1.tif"/></fig></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>Test results from applications involving humans, non-human primates, and rodents confirm that the REC-GUI framework provides a solution for implementing demanding neuroscience studies with accurate and precise experimental control. By achieving millisecond-level control with high-level programming environments, the framework can help researchers overcome technical challenges that hinder research, without the need for low-level programing languages or professional programmers. Beyond experiments with a fixed trial structure, the framework can support research involving the use of naturalistic, complex stimuli that are dynamically updated based on real-time behavioral or neural measurements. In Application 3, we showed that the framework can automate real-time behavioral monitoring for tasks without a fixed trial structure, and perform experimental manipulations based on defined criteria. Similarly, the framework can support more complex closed-loop experiments in which multisensory visual–vestibular stimuli are updated based on active steering behaviors rather than passive, predefined motion profiles. Alternatively, stimuli can be updated or neural activity perturbed using short-latency stimulation triggered by real-time behavioral or neural events. Such capabilities will be critical to understanding the relationship between the dynamic activity of neural populations, perception, and action during natural behaviors. The framework can facilitate such research by reducing the overhead associated with parlaying the necessary technology into a cohesive control system. Thus, a major contribution of the REC-GUI framework is in achieving experimental flexibility and high temporal precision while minimizing coding demands. While it is inescapable that programming is required for customization, the level of coding ability possessed by many graduate students (e.g., translating the logical steps to move through a series of experimental states into MATLAB or Python code) is sufficient to implement diverse applications. Indeed, by reducing technical hurdles and providing a flexible experimental framework, complex protocols will be accessible to a greater number of labs, and the opportunity for discovery increased.</p><sec id="s4-1"><title>Advantages and limitations of the REC-GUI framework</title><p>The REC-GUI framework offers several advantages compared to other currently available control systems. First, network-based parallel processing makes the framework inherently modular and highly flexible. Since network-based parallel processing is a standard approach to coordinating multiple CPUs, it is well-documented online, and therefore setting up the necessary networks does not pose a substantial technical hurdle. Moreover, since the only constraint on incorporating specialized software or hardware is that network interfacing is supported, a broad range of devices for stimulus presentation (visual displays, speakers, pellet droppers, etc.), behavioral measurement (button presses, eye movements, position, etc.), and other experimental needs (stimulator, osmotic pump, etc.) can be readily incorporated into experimental setups. Such flexibility allows the framework to be adapted to a broad range of preparations (<italic>in vitro</italic>, anesthetized, or awake-behaving) and research domains (sensory, cognitive, motor, etc.), and makes it agnostic to the type of data recorded (electrophysiological, optical, magnetic resonance imaging, etc.). Second, dividing computing demands across CPUs improves system performance and enables researchers to implement system components using different coding languages and operating systems. This reduces compatibility issues and increases the efficiency with which experimental setups are configured. For example, some applications require multiple external acquisition systems. While we have not tested the framework with such an application, it would be straightforward to temporally align events saved on the different systems by having the GUI send synchronizing signals to each of them. Likewise, while the performance of the REC-GUI framework, as implemented here, is sufficient for most experimental needs, if greater computational speed is required, that can be achieved using a lower-level programming language such as C or C ++ for stimulus rendering and presentation. Even greater processing speed could be achieved using an onboard microprocessor with software loaded onto the ROM. While this is possible with the REC-GUI framework, such implementations would increase the coding demands, defeating the goal to minimize coding efforts. Third, diverse experimental components (behavior, neural recordings, stimulation, etc.) can occur in parallel at different temporal resolutions, and be precisely aligned, allowing for multi-faceted research. Lastly, a user-friendly, fully customizable GUI allows for intuitive and flexible experimental design changes.</p></sec><sec id="s4-2"><title>Comparisons with other control systems</title><p>We briefly benchmark several existing control systems against the REC-GUI framework. One commercially available system that is used in a broad range of neuroscience applications is Spike2 (CED, Inc.). That system includes software and multifunctional hardware for analog and digital interfacing (including multiple analog-to-digital channels with large bandwidths for neural recordings), providing robust experimental control and data acquisition for experiments such as <italic>in vitro</italic> patch clamp recordings (<xref ref-type="bibr" rid="bib8">Hasenstaub et al., 2005</xref>), recordings of stimulus-driven neuronal activity in anesthetized cats (<xref ref-type="bibr" rid="bib20">Rosenberg and Issa, 2011</xref>), and multisensory studies in awake-behaving monkeys (<xref ref-type="bibr" rid="bib18">Rosenberg and Angelaki, 2014a</xref>). One potential downside is that the Spike2 scripting language is designed for their acquisition hardware, and sufficiently complex that the company hosts training events. In contrast, the REC-GUI framework is not coupled to specific hardware, and relies on widely used high-level programming environments.</p><p>The Laboratory of Sensorimotor Research (LSR) real-time software suite is a free package including a GUI, stimulus rendering, data acquisition, and offline analysis tools (<xref ref-type="bibr" rid="bib9">Hays et al., 1982</xref>). Similar to REC-GUI, the LSR suite divides experimental control, stimulus processing, and data acquisition across CPUs. For the LSR suite, this is achieved using TCP or direct digital connections with National Instruments Cards. The LSR suite is highly powerful, and can meet the demands of complex behavioral paradigms. However, the LSR scripting language is complex, making the overhead of learning the system and developing new experiments quite high (<xref ref-type="bibr" rid="bib6">Eastman and Huk, 2012</xref>). The REC-GUI framework reduces this overhead, particularly as it relates to modifying or adding new experimental tasks, by achieving accurate and precise experimental control with high-level programming environments that are widely used by the neuroscience community. Additionally, the LSR suite is largely specialized for visuomotor studies, whereas the REC-GUI framework is agnostic to the area of neuroscience research.</p><p>A freely available system that is fully implemented in MATLAB is MonkeyLogic (<xref ref-type="bibr" rid="bib2">Asaad et al., 2013</xref>). The system achieves millisecond-level temporal resolution, and provides a user interface with real-time behavioral monitoring. In addition, it is convenient to implement control flows for new behavioral tasks. MonkeyLogic is designed for a single CPU, so experimental control is performed serially due to MATLAB’s multithreading limitations. This can be problematic for real-time control when the stimulus demands are high. For example, since stimuli are transferred to the video buffer during the inter-trial interval without compression, it may not be suitable for presenting long-duration stimuli. Along this line, the MonkeyLogic forum indicates that it cannot support 240 Hz visual stimulus presentation, as used in the first application we tested (<ext-link ext-link-type="uri" xlink:href="http://forums.monkeylogic.org/post/high-refresh-rates-vpixx-8408242">http://forums.monkeylogic.org/post/high-refresh-rates-vpixx-8408242</ext-link>). Furthermore, this system is limited to experiments with well-defined trial structures, rather than closed-loop or continuous experiments (e.g., Application 3 described here). Display limitations will exist for other single CPU, MATLAB-based control systems, though some limitations may be partially remediated if the real-time monitoring and control features provided by a GUI are eliminated (<xref ref-type="bibr" rid="bib6">Eastman and Huk, 2012</xref>). Such systems may be ideal for tasks that do not have high real-time behavioral contingency and stimulus demands, since network communications make the REC-GUI framework more complex. There is also some added cost compared to single CPU systems since REC-GUI requires multiple CPUs. In exchange, the REC-GUI framework allows high-level programming environments to be used to robustly control computationally demanding and behaviorally complex experiments.</p></sec><sec id="s4-3"><title>Ongoing open-source development</title><p>We will maintain a webpage for supporting the REC-GUI framework as an open-source project (<ext-link ext-link-type="uri" xlink:href="https://recgui2018.wixsite.com/rec-gui">https://recgui2018.wixsite.com/rec-gui</ext-link>). The webpage currently includes a forum for discussion and assistance with customization, as well as links to relevant downloads. To track versioning associated with future developments and customization, the software is stored on GitHub (<xref ref-type="bibr" rid="bib11">Kim et al., 2019</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/rec-gui">https://github.com/elifesciences-publications/rec-gui</ext-link>). To increase the generality and functionality of the framework, we encourage others to make development contributions. Our hope is that REC-GUI will help researchers perform cutting-edge research by reducing time spent solving technical problems and increasing time focused on experimental questions and design. The REC-GUI framework can also help promote research transparency, standardize data acquisition, and improve reproducibility by facilitating the replication of experimental paradigms.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgments</title><p>We thank Heather Mitchell, Xinyu Zhao, Qiang Chang, and the IDD Models Core supported by Waisman Center IDDRC (U54HD090256) for help with the passive avoidance task. This work was supported by the Alfred P. Sloan Foundation, Whitehall Foundation Research Grant 2016-08-18, Shaw Scientist Award from the Greater Milwaukee Foundation, and National Institutes of Health Grants DC014305 and EY029438. Further support was provided by National Institutes of Health Grant P51OD011106 to the Wisconsin National Primate Research Center, University of Wisconsin – Madison.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Validation, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Software, Validation, Visualization, Writing—original draft</p></fn><fn fn-type="con" id="con6"><p>Resources, Validation, Visualization</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This study was performed in strict accordance with the Declaration of Helsinki. All experimental procedures were approved by the Institutional Review Board at the University of Wisconsin-Madison (Protocol #: 2016-1283). All participants provided informed consent.</p></fn><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations of the National Institutes of Health's Guide for the Care and Use of Laboratory Animals. All experimental procedures and surgeries were approved by the Institutional Animal Care and Use Committee (IACUC) at the University of Wisconsin-Madison (Protocol #s: G005229, G5373).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.40231.016</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40231-transrepform-v1.pdf"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All source code for implementing the Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework is available for download through the REC-GUI GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/rec-gui/rec-gui">https://github.com/rec-gui/rec-gui</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/rec-gui">https://github.com/elifesciences-publications/rec-gui</ext-link>).</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abdou</surname> <given-names>AR</given-names></name><name><surname>Matrawy</surname> <given-names>A</given-names></name><name><surname>van Oorschot</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accurate manipulation of delay-based internet geolocation</article-title><source>ASIA CCS '17</source><conf-name>Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</conf-name><fpage>887</fpage><lpage>898</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname> <given-names>WF</given-names></name><name><surname>Santhanam</surname> <given-names>N</given-names></name><name><surname>McClellan</surname> <given-names>S</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-performance execution of psychophysical tasks with complex visual stimuli in MATLAB</article-title><source>Journal of Neurophysiology</source><volume>109</volume><fpage>249</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1152/jn.00527.2012</pub-id><pub-id pub-id-type="pmid">23034363</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bradski</surname> <given-names>G</given-names></name><name><surname>Kaehler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Learning OpenCV: Computer vision with the OpenCV library</source><publisher-name>O'Reilly Media, Inc</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coronas-Samano</surname> <given-names>G</given-names></name><name><surname>Baker</surname> <given-names>KL</given-names></name><name><surname>Tan</surname> <given-names>WJ</given-names></name><name><surname>Ivanova</surname> <given-names>AV</given-names></name><name><surname>Verhagen</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fus1 KO mouse as a model of oxidative Stress-Mediated sporadic alzheimer's Disease: Circadian Disruption and Long-Term Spatial and Olfactory Memory Impairments</article-title><source>Frontiers in Aging Neuroscience</source><volume>8</volume><elocation-id>268</elocation-id><pub-id pub-id-type="doi">10.3389/fnagi.2016.00268</pub-id><pub-id pub-id-type="pmid">27895577</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eastman</surname> <given-names>KM</given-names></name><name><surname>Huk</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>PLDAPS: A hardware architecture and software toolbox for neurophysiology requiring complex visual stimuli and online behavioral control</article-title><source>Frontiers in Neuroinformatics</source><volume>6</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2012.00001</pub-id><pub-id pub-id-type="pmid">22319490</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartley</surname> <given-names>R</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Multiple View Geometry in Computer Vision</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasenstaub</surname> <given-names>A</given-names></name><name><surname>Shu</surname> <given-names>Y</given-names></name><name><surname>Haider</surname> <given-names>B</given-names></name><name><surname>Kraushaar</surname> <given-names>U</given-names></name><name><surname>Duque</surname> <given-names>A</given-names></name><name><surname>McCormick</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Inhibitory postsynaptic potentials carry synchronized frequency information in active cortical networks</article-title><source>Neuron</source><volume>47</volume><fpage>423</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.06.016</pub-id><pub-id pub-id-type="pmid">16055065</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hays</surname> <given-names>AV</given-names></name><name><surname>Richmond</surname> <given-names>BJ</given-names></name><name><surname>Optican</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>A UNIX-based multiple process system for real-time data acquisition and control</source><publisher-name>Osti</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Judge</surname> <given-names>SJ</given-names></name><name><surname>Richmond</surname> <given-names>BJ</given-names></name><name><surname>Chu</surname> <given-names>FC</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Implantation of magnetic search coils for measurement of eye position: An improved method</article-title><source>Vision Research</source><volume>20</volume><fpage>535</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(80)90128-5</pub-id><pub-id pub-id-type="pmid">6776685</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>B</given-names></name><name><surname>Kenchappa</surname> <given-names>SC</given-names></name><name><surname>Sunkara</surname> <given-names>A</given-names></name><name><surname>Chang</surname> <given-names>TY</given-names></name><name><surname>Thompson</surname> <given-names>L</given-names></name><name><surname>Doudlah</surname> <given-names>R</given-names></name><name><surname>Rosenberg</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>REC-GUI</data-title><source>GitHub</source><version designator="f7faa1f">f7faa1f</version><ext-link ext-link-type="uri" xlink:href="https://github.com/rec-gui/rec-gui">https://github.com/rec-gui/rec-gui</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>B</given-names></name><name><surname>Basso</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Saccade target selection in the superior colliculus: A signal detection theory approach</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>2991</fpage><lpage>3007</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5424-07.2008</pub-id><pub-id pub-id-type="pmid">18354003</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>B</given-names></name><name><surname>Basso</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A probabilistic strategy for understanding action selection</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>2340</fpage><lpage>2355</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1730-09.2010</pub-id><pub-id pub-id-type="pmid">20147560</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname> <given-names>M</given-names></name><name><surname>Brainard</surname> <given-names>D</given-names></name><name><surname>Pelli</surname> <given-names>D</given-names></name><name><surname>Ingling</surname> <given-names>A</given-names></name><name><surname>Murray</surname> <given-names>R</given-names></name><name><surname>Broussard</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><elocation-id>1</elocation-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurens</surname> <given-names>J</given-names></name><name><surname>Kim</surname> <given-names>B</given-names></name><name><surname>Dickman</surname> <given-names>JD</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Gravity orientation tuning in macaque anterior thalamus</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1566</fpage><lpage>1568</lpage><pub-id pub-id-type="doi">10.1038/nn.4423</pub-id><pub-id pub-id-type="pmid">27775722</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname> <given-names>A</given-names></name><name><surname>Cowan</surname> <given-names>NJ</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The visual representation of 3D object orientation in parietal cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>19352</fpage><lpage>19361</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3174-13.2013</pub-id><pub-id pub-id-type="pmid">24305830</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname> <given-names>A</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Gravity influences the visual representation of object tilt in parietal cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>14170</fpage><lpage>14180</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2030-14.2014</pub-id><pub-id pub-id-type="pmid">25339732</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname> <given-names>A</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Reliability-dependent contributions of visual orientation cues in parietal cortex</article-title><source>PNAS</source><volume>111</volume><fpage>18043</fpage><lpage>18048</lpage><pub-id pub-id-type="doi">10.1073/pnas.1421131111</pub-id><pub-id pub-id-type="pmid">25427796</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname> <given-names>A</given-names></name><name><surname>Issa</surname> <given-names>NP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The Y cell visual pathway implements a demodulating nonlinearity</article-title><source>Neuron</source><volume>71</volume><fpage>348</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.044</pub-id><pub-id pub-id-type="pmid">21791292</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Lewis</surname> <given-names>JW</given-names></name><name><surname>Drury</surname> <given-names>HA</given-names></name><name><surname>Hadjikhani</surname> <given-names>N</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Bakircioglu</surname> <given-names>M</given-names></name><name><surname>Miller</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Mapping visual cortex in monkeys and humans using surface-based atlases</article-title><source>Vision Research</source><volume>41</volume><fpage>1359</fpage><lpage>1378</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00045-1</pub-id><pub-id pub-id-type="pmid">11322980</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.40231.018</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Nelson</surname><given-names>Sacha B</given-names></name><role>Reviewing Editor</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Nelson</surname><given-names>Sacha B</given-names></name><role>Reviewer</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Desai</surname><given-names>Niraj</given-names> </name><role>Reviewer</role><aff><institution>Stony Brook University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter, peer reviews, and accompanying author responses.</p></boxed-text><p>[<bold>Editorial note:</bold> This article has been through an editorial process in which the authors decide how to respond to the issues raised during peer review. The Reviewing Editor's assessment is that all the issues have been addressed.]</p><p>Thank you for submitting your article &quot;Real-time experimental control using network-based parallel processing&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Sacha B Nelson as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individual involved in review of your submission has also agreed to reveal their identity: Niraj Desai (Reviewer #2).</p><p>We strongly encourage you to revise your manuscript to deal with the issues raised in the reviews. As you can see, the reviewers feel that the present version does not live up to the claims you make for it, and we hope that you revise it to come far closer to a more useful contribution. As you know, these reviews and your responses will be published with the manuscript, as well as the editors' assessment of how responsive your revision is to the reviews.</p><p>Summary:</p><p>This is a description of a software package intended to generalize the problem of real time acquisition and control of rapidly changing signals, such as might be encountered in neuroscience experiments relating neurobiological signals to behavior. Although this is a potentially highly useful undertaking, the paper describes only a single use case involving primate vision experiments and many of the features of the described software seem highly specialized for this application with little information about the true generality of the described tool. In addition, reviewers were concerned that the overall approach lacked novelty and so a mere &quot;proof of principle&quot; was of limited value.</p><p>Major concerns:</p><p>As you can see in the reviews below, the reviewers largely agree on these central recommendations:</p><p>1) Maximize the generality of the described tool/resource by demonstrating application to multiple diverse experimental settings and by providing information on how to customize the software for any of a range of applications. This could be achieved, for example, by briefly describing the application to the psychophysics experiments alluded to, and by collaborating with another lab to tailor the system to their acquisition/control needs in a very different (ideally non-vision) setting.</p><p>2) Describe the performance in more general terms (e.g. as is done in Figure 5), rather than in terms specific to the major use case for which it was developed. This also applies to the discussion, which is now very focussed on software for monkey vision experiments.</p><p>3) Think through and describe the resources needed to support application of the software to other use cases. This includes a plan for maintenance of the software (e.g. via GitHub) as well as much more detailed supplementary instructions for modification and use.</p><p>4) Clearly state existing limitations to the generality. State what has and has not been tested and what is and is not possible with the existing software.</p><p>5) Although the issue of novelty of the approach was raised by one of the reviewers, and this view is shared, it will likely not be helpful to try to rewrite the manuscript to stress the novelty. Taking steps to ensure the usefulness and then highlighting them will probably be more fruitful in terms of improving the manuscript.</p><p>Separate reviews:</p><p><italic>Reviewer #1:</italic> </p><p>The authors describe a new software suite for data acquisition in neurophysiology experiments. The intriguing features promised in the early parts of the paper are that it is highly modular, agnostic to coding language and operating system, and hence broadly applicable to many types of real time data acquisition and control experiments. The authors achieve high performance in terms of simultaneously computing stimuli, monitoring behavior and acquiring neurophysiological signals by segregating tasks and running them on separate processors linked by internet protocols, augmented by standard digital IO.</p><p>The rest of the paper is somewhat disappointing in that it mainly describes applicability to a single type of experiment (visual stimulation and recording of eye position and extracellular action potentials in behaving primates). Hence it is not likely to be obvious to most readers not performing very similar experiments, how they might configure the system for very different experiments. Ideally, a paper describing a general purpose system would apply it to more than one system. The figures are mostly devoted to the performance relevant to their specific experiment and so will not interest many readers. Figure 5 gives some idea of the performance in a general sense of the interaction between components, but Figures 3,4,6,7,8 are highly specific to the particular project and so are really not especially relevant to most readers interested in a more general tool. These figures would mostly be better as supplements to figures that measure performance or describe capabilities in a more general way. This lack of generality is exacerbated by the fact that the link to the downloads related to the software is nonfunctional.</p><p>Readers would want to know things like: which operating systems were actually tested? Which coding languages were used? Which acquisition hardware was tested? What aspects of the GUI are configurable and which are hard coded? They would want examples of experiments performed that are of multiple types involving different types of signals and different control paradigms. Otherwise, this publication would really be much more appropriate for a more specialized audience (e.g. the monkeyLogic paper was published in J. Neurophysiol. the Psychophysics Toolbox paper was in Spatial Vision and an update was published in Perception).</p><p>Finally, the discussion contains comparisons to other systems, but these comparisons are again formulated in terms of a very specific experiment's requirements (e.g. 240 Hz visual stimulus presentation) rather than in more generic terms of bandwidth for acquisition and control.</p><p>Additional data files and statistical comments:</p><p>The support for the software is a critical part of publication and of evaluation of the manuscript. The web site allowing access to the User Manual and other downloads was not accessible, even though these were included with the submission. As noted above, a User Manual for modifying the software and applying it to other kinds of experiments is also needed.</p><p><italic>Reviewer #2:</italic> </p><p>Kim and colleagues present a parallel processing framework to integrate electrophysiological (or other neural) measurements, stimulus generation, and behavioral control. Their basic idea is that complex experiments can be run in a flexible manner if different tasks are spread out between different CPUs, which communicate with each other via standard internet protocols (TCP and UDP). The idea is intriguing and the implementation the authors themselves use is an excellent proof of principle. The major weakness is that the authors offer very little guidance to researchers who have different equipment or study a different experimental system, or simply have different experimental objectives and constraints. The project as it stands – the manuscript, the supplementary software examples, and the (under construction) website – is too particular to the authors' own work; it would be difficult for others to generalize from this.</p><p>Strengths:</p><p>1) The basic idea is clever: breaking up complex experiments into modules definitely seems like a good way to combine flexibility, computationally-intensive processes, and temporal fidelity. And exploiting existing protocols for network communication should make integration of these processes seamless. Many types of experimental programs could benefit from the general approach.</p><p>2) The kinds of experiments these authors themselves do – awake monkeys, complex visual stimuli, real-time control – are very technically challenging. Their success in using the general approach in this particular case is therefore reassuring, and their software likely will help others who do the same kinds of experiments.</p><p>Weakness:</p><p>3) But what about everybody else? The authors make broad claims for their framework:</p><p>&quot;Since the only constraint on incorporating specialized software or hardware is that network interfacing is supported, a broad range of devices for stimulus presentation (visual displays, speakers, pellet droppers, etc.), behavioral measurement (button presses, eye movements, biometrics, location, etc.), and other experimental needs (stimulator, osmotic pump, etc.) can be used out of the box. Such flexibility allows the framework to be adapted to a broad range of experimental preparations (in vitro, anesthetized, or awake-behaving) and neuroscience research domains (sensory, cognitive, motor, etc.), and makes it agnostic to the type of neural data recorded (electrophysiological, optical, magnetic resonance imaging, etc.).&quot;</p><p>I suspect these claims are true, but I also think that the authors haven't demonstrated that they're true. Their vision experiment is the only implementation described and the only example given. In fact, the authors' separate work on &quot;perceptual learning studies with adolescents with autism&quot; is alluded to, but not described.</p><p>More importantly, the authors never really explain – in detail – how to implement the framework. The descriptions are either too particular to the authors' own equipment or too general and/or vague to be useful. The manuscript assures readers that implementation is straightforward in language that is, perhaps, meant to be reassuring but comes off as glib: &quot;Other acquisition systems that support network interfacing can be substituted with minimal effort.… The default GUI contains interfaces to control eye calibration and receptive field mapping, but these can be easily substituted with other tools.&quot; A reasonable reader response: &quot;Okay.… sure.… if you say so.… but how do I do this?!&quot;</p><p>In making this criticism, I'm motivated by the question of what a methods paper is for. My view is that the best methods papers offer not only a proof of principle but serve as a how-to guide. In the Introduction, the authors indicate that they hope their framework will benefit labs that lack the technical skills to design complex experimental systems themselves. That is a valuable goal – and a feasible one – but, in my opinion, they haven't reached that goal here. In fact, even labs with a lot of technical expertise but different equipment will need to &quot;reinvent the wheel&quot; to get this working.</p><p>What I would suggest is that the authors describe – perhaps in supplementary material so as not to disrupt the flow of the main text too much – how they built their system step by step by step. How both physically and in software do you connect a piece of hardware with a CPU using TCP? How do you set up a network interface card? Maybe show screen captures or pictures of the equipment. Do something to make this more tangible.</p><p>A related point: the software examples are not enough. Reading code written by someone else is one of life's great miseries, no matter the talent of the original programmer, and in this case is not especially helpful in understanding how the REC-GUI framework works.</p><p>Minor Comments:</p><p>1) The Introduction promises something very broad – a general system that can be used easily without great technical investment. Unless and until the rest of the manuscript fulfills this promises, it should be toned down. It should state more clearly and narrowly what is actually in the paper.</p><p>2) The project should live on Github or someplace similar. One reason open-source projects fail to gain traction is when they are seen as too proprietary. REC-GUI will only be important if it is adopted by a wide community, outside of the originating lab. The software shouldn't live on that lab's website. Also, the authors write: &quot;We will maintain the REC-GUI framework as an open-source project.….&quot; What does that mean exactly? Maintain it for a year? For five years? 10? 20? Things change in the life of a lab; people graduate and move on. If this project is to be real, it can't be tied to one lab.</p><p>3) Although the Abstract and Introduction talk about being independent of a particular operating system, but the website indicates that the Ubuntu version of Linux is &quot;highly recommended&quot; for the GUI. That is a real limitation and should be mentioned explicitly in the manuscript.</p><p><italic>Reviewer #3:</italic> </p><p>The authors describe a UDP/TCP approach for coordinating eye tracking, electrophysiology, and behavioral input across multiple computers. The loop interval is about 5ms. The authors can transmit information among the various CPUs, and presumably more CPUs could be integrated into this system. The authors have done a good job of writing down and describing their system. It is faster than most, with a 5ms cycle time.</p><p>My biggest problem with the article is that there really isn't any novelty. Every lab I've been in has solved this problem similarly, with multiple CPUs and some communication across them via the network. Some labs I know use some combination of UDP/TCP, file systems, and digital TTL signals. In my opinion, this article doesn't belong in <italic>eLife</italic>, I don't think it will appeal to its broad readership. It's not novel enough.</p><p>At the same time, not to discourage the authors, I applaud the fact that they have written this down and have made it available to other groups.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.40231.019</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Major concerns:</p><p>As you can see in the reviews below, the reviewers largely agree on these central recommendations:</p><p>1) Maximize the generality of the described tool/resource by demonstrating application to multiple diverse experimental settings and by providing information on how to customize the software for any of a range of applications. This could be achieved, for example, by briefly describing the application to the psychophysics experiments alluded to, and by collaborating with another lab to tailor the system to their acquisition/control needs in a very different (ideally non-vision) setting.</p></disp-quote><p>Based on the reviewers’ comments and a follow-up discussion with the editor, our revisions focus on demonstrating the generality of the REC-GUI framework using three distinct applications with humans, non-human primates, and rodents. The non-human primate application confirms the ability of the framework to accurately and precisely control external hardware under computationally demanding scenarios while achieving millisecond level precision. We added details of a human psychophysics application to show that the framework can coordinate experimental control based on multiple external device inputs. This application further shows that measurements made by multiple external devices can be saved and precisely aligned without external data acquisition systems. We added a passive avoidance task with mice to show how the framework can be used to automate behavioral training and assessment by transforming input device signals into control signals for external devices. Together, these three applications show generality across animal models, as well as experimental modalities (e.g., neural recordings during behavior, human psychophysics, and animal behavioral training and assessment).</p><p>We greatly expanded the user manual and supporting code. These provide detailed examples of coding structures used by the REC-GUI framework, including three ready-to-use scripts for implementing specific protocols: (i) a gaze fixation task (fixation.m), (ii) an eye calibration routine (calibration.m), and (iii) a receptive field mapping routine (receptive_field_mapping.m). The examples implement vision-related tasks, but the structures are applicable to a broad range of experiments, regardless of the sensory or motor modality investigated. For example, the eye calibration routine can be modified into a motor reach calibration routine with minimal effort. A main point of these examples is to illustrate how the GUI communicates with hardware and other software to implement tasks. We also include a basic template for implementing new protocols (start_coding.m), and code implementing a simple closed-loop task (start_coding_closedloop.m). In particular, the closed-loop task is a useful starting point for learning to use and customize the REC-GUI framework since it does not require specialized hardware or software (beyond Python and MATLAB).</p><p>We believe that these additions provide sufficient information to guide a wide range of customizations of the REC-GUI framework. We further emphasized that support will be provided through the REC-GUI forum (https://recgui2018.wixsite.com/rec-gui/forum) to cover any questions that may arise, and that the REC-GUI framework is an open-source project that will continue to evolve. We will routinely update the user manual based on developments and forum discussions. The user manual can be downloaded from the REC-GUI GitHub (https://github.com/rec-gui).</p><disp-quote content-type="editor-comment"><p>2) Describe the performance in more general terms (e.g. as is done in Figure 5), rather than in terms specific to the major use case for which it was developed. This also applies to the discussion, which is now very focused on software for monkey vision experiments.</p></disp-quote><p>We expanded the analyses quantifying system performance (Figure 5). Specifically, we characterize upper and lower bound estimates of performance when relying entirely on high-level programming environments to implement an experiment. We also provide specific test results from three applications using different system configurations to support studies with humans, non-human primates, and rodents.</p><p>To ensure that the testing results are relevant to experimentalists and readily interpretable, they were performed using specific applications. To increase the generality of the tests, we framed the motivations for them and the interpretation of the results in terms of broader experimental needs.</p><p>Within the Discussion, we expanded text regarding how the REC-GUI framework can be applied to a broad range of neuroscience questions ranging from in vitro experiments to closed-loop multisensory studies. In addition to other control systems previously discussed, we now discuss the Spike2 (CED, Inc.) system which is used in a wide range of applications.</p><disp-quote content-type="editor-comment"><p>3) Think through and describe the resources needed to support application of the software to other use cases. This includes a plan for maintenance of the software (e.g. via GitHub) as well as much more detailed supplementary instructions for modification and use.</p></disp-quote><p>The REC-GUI webpage (https://recgui2018.wixsite.com/rec-gui) hosts a discussion forum and contains links to essential downloads. In addition to the user manual, we indicate in the text that the forum is an important resource for customization. The added GitHub stores the user manual, software, and tracks versioning (https://github.com/rec-gui). Throughout the text, we provide examples of what would be needed for the framework to support applications that were not tested. These additions largely occur in the Materials and methods and Discussion sections. As discussed above, we expanded the user manual and included additional example code to further clarify modification and use.</p><disp-quote content-type="editor-comment"><p>4) Clearly state existing limitations to the generality. State what has and has not been tested and what is and is not possible with the existing software.</p></disp-quote><p>We have clarified limitations to the generality of the REC-GUI framework throughout the manuscript as they relate to hardware, software, and the types of experimental paradigms. For example, we emphasize that customization does require programming, but highlight that the necessary coding abilities are of the level expected of graduate students. We also reiterate that network interfacing is a requirement to integrate hardware or software with the framework. In the Discussion, we mention applications and extensions that have not been tested, and what would be required to implement them. We additionally discuss performance limitations, and how future developments can reduce those limitations (though in some cases, doing so would require more advanced programming abilities). We encourage contributions from the scientific community to further develop the generality and functionality of the framework.</p><disp-quote content-type="editor-comment"><p>5) Although the issue of novelty of the approach was raised by one of the reviewers, and this view is shared, it will likely not be helpful to try to rewrite the manuscript to stress the novelty. Taking steps to ensure the usefulness and then highlighting them will probably be more fruitful in terms of improving the manuscript.</p></disp-quote><p>Network-based parallel processing in and of itself is, of course, not novel. But the application of such a framework to provide experimental flexibility and high temporal precision while minimizing coding demands is unique. The unmet need for a system that jointly meets these competing goals is evident since there have been multiple previous attempts to develop such a system. However, existing alternatives have had limited success compared to the REC-GUI framework. We have further clarified this point and highlighted the usefulness of the framework in the Discussion.</p></body></sub-article></article>