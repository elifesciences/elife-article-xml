<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">71132</article-id><article-id pub-id-type="doi">10.7554/eLife.71132</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>An image reconstruction framework for characterizing initial visual encoding</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-241711"><name><surname>Zhang</surname><given-names>Ling-Qi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8468-7927</contrib-id><email>lingqiz@sas.upenn.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-244154"><name><surname>Cottaris</surname><given-names>Nicolas P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1829-6340</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-115968"><name><surname>Brainard</surname><given-names>David H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9827-543X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meister</surname><given-names>Markus</given-names></name><role>Reviewing Editor</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>01</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e71132</elocation-id><history><date date-type="received" iso-8601-date="2021-06-09"><day>09</day><month>06</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-01-14"><day>14</day><month>01</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-06-02"><day>02</day><month>06</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.06.02.446829"/></event></pub-history><permissions><copyright-statement>© 2022, Zhang et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Zhang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-71132-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-71132-figures-v3.pdf"/><abstract><p>We developed an image-computable observer model of the initial visual encoding that operates on natural image input, based on the framework of Bayesian image reconstruction from the excitations of the retinal cone mosaic. Our model extends previous work on ideal observer analysis and evaluation of performance beyond psychophysical discrimination, takes into account the statistical regularities of the visual environment, and provides a unifying framework for answering a wide range of questions regarding the visual front end. Using the error in the reconstructions as a metric, we analyzed variations of the number of different photoreceptor types on human retina as an optimal design problem. In addition, the reconstructions allow both visualization and quantification of information loss due to physiological optics and cone mosaic sampling, and how these vary with eccentricity. Furthermore, in simulations of color deficiencies and interferometric experiments, we found that the reconstructed images provide a reasonable proxy for modeling subjects’ percepts. Lastly, we used the reconstruction-based observer for the analysis of psychophysical threshold, and found notable interactions between spatial frequency and chromatic direction in the resulting spatial contrast sensitivity function. Our method is widely applicable to experiments and applications in which the initial visual encoding plays an important role.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>early vision</kwd><kwd>ideal observer</kwd><kwd>bayesian model</kwd><kwd>image statistics</kwd><kwd>peripheral vision</kwd><kwd>color vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Facebook Reality Labs</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zhang</surname><given-names>Ling-Qi</given-names></name><name><surname>Cottaris</surname><given-names>Nicolas P</given-names></name><name><surname>Brainard</surname><given-names>David</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A computational model of the initial visual encoding together with Bayesian image reconstruction quantifies how that encoding, combined with the statistical regularities of natural images, shapes key aspects of visual perception.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Visual perception begins at the retina, which takes sensory measurements of the light incident at the eyes. This initial representation is then transformed by computations that support perceptual inferences about the external world. Even these earliest sensory measurements, however, do not preserve all of the information available in the light signal. Factors such as optical aberrations, spatial and spectral sampling by the cone mosaic, and noise in the cone excitations all limit the information available downstream.</p><p>One approach to understanding the implications of such information loss is ideal observer analysis, which evaluates the optimal performance on psychophysical discrimination tasks. This allows for quantification of the limits imposed by features of the initial visual encoding, as well as predictions of the effect of variation in these features (<xref ref-type="bibr" rid="bib39">Geisler, 1989</xref>; <xref ref-type="bibr" rid="bib40">Geisler, 2011</xref>). Ideal observer analysis separates effects due to the visual representation from inefficiencies in the processes that mediate the discrimination decisions themselves. Such analyses have often been applied to analyze performance for simple artificial stimuli, assuming that the stimuli to be discriminated are known exactly (<xref ref-type="bibr" rid="bib8">Banks et al., 1987</xref>; <xref ref-type="bibr" rid="bib33">Davila and Geisler, 1991</xref>) or known statistically with some uncertainty (<xref ref-type="bibr" rid="bib73">Pelli, 1985</xref>; <xref ref-type="bibr" rid="bib41">Geisler, 2018</xref>). The ideal observer approach has been extended to consider decision processes that learn aspects of the stimuli being discriminated, rather than being provided with these a priori, and extended to handle discrimination and estimation tasks with naturalistic stimuli (<xref ref-type="bibr" rid="bib22">Burge and Geisler, 2011</xref>; <xref ref-type="bibr" rid="bib23">Burge and Geisler, 2014</xref>; <xref ref-type="bibr" rid="bib91">Singh et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Chin and Burge, 2020</xref>; <xref ref-type="bibr" rid="bib51">Kim and Burge, 2020</xref>). For a recent review, see <xref ref-type="bibr" rid="bib24">Burge, 2020</xref>; also see <xref ref-type="bibr" rid="bib97">Tjan and Legge, 1998</xref> and <xref ref-type="bibr" rid="bib30">Cottaris et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Cottaris et al., 2020</xref>.</p><p>It is generally accepted that the visual system has internalized the statistical regularities of natural scenes, so as to take advantage of these regularities for making perceptual inferences (<xref ref-type="bibr" rid="bib7">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib36">Field, 1987</xref>; <xref ref-type="bibr" rid="bib87">Shepard, 1987</xref>; <xref ref-type="bibr" rid="bib52">Knill et al., 1996</xref>). This motivates interest in extending ideal observer analysis to apply to fully naturalistic input, while incorporating the statistical regularities of natural scenes (<xref ref-type="bibr" rid="bib24">Burge, 2020</xref>). Here, we pursue an approach to this goal that, in addition, extends the evaluation of performance to a diverse set of objectives.</p><p>We developed a method that, under certain assumptions, optimally reconstructs images from noisy cone excitations, with the excitations generated from an accurate image-computable model of the front end of the visual system (<xref ref-type="bibr" rid="bib30">Cottaris et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Cottaris et al., 2020</xref>). (We use the term ‘image-computable’ here in contrast with observer models that operate on abstract and/or hypothetical internal representations.) The image reconstruction approach provides us with a unified framework for characterizing the information loss due to various factors in the initial encoding. In the next sections, we show analyses that: (1) use image reconstruction error as an information metric to understand the retinal mosaic ‘design’ problem, with one example examining the implications of different allocations of retinal cone types; (2) allow both visualization and quantification of information loss due to physiological optics and cone mosaic sampling and how this varies with eccentricity, as well as with different types of color deficiency; (3) combine the image reconstruction approach with analysis of psychophysical discrimination, thus providing a way to incorporate into such analyses the assumption that our visual system takes into account the statistical regularities of natural images.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We developed a Bayesian method to reconstruct images from sensory measurements, which we describe briefly here (see Materials and methods for details). We begin with a forward model that expresses the relation between an image and its visual representation at a well-defined stage in the visual pathway. Here that stage is the excitations of the photoreceptors of the retinal cone mosaic, so that our model accounts for blur in retinal image formation, spatial and spectral sampling by the cone mosaic, and the noise in the cone excitations. The approach is general, however, and may be applied to other sites in the visual pathways (see e.g. <xref ref-type="bibr" rid="bib68">Naselaris et al., 2009</xref>; <xref ref-type="bibr" rid="bib72">Parthasarathy et al., 2017</xref>). Our forward model is implemented within the open-source software package ISETBio (<ext-link ext-link-type="uri" xlink:href="http://isetbio.org/">isetbio.org</ext-link>; <xref ref-type="fig" rid="fig1">Figure 1A–C</xref>) which encapsulates the probabilistic relationship between the stimulus (i.e. pixel values of a displayed RGB image) and the cone excitations (i.e. trial-by-trial photopigment isomerizations). ISETBio simulates the process of displaying an image on a monitor (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), the wavelength-dependent optical blur of the human eye and spectral transmission through the lens and the macular pigment (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), as well as the interleaved spatial and chromatic sampling of the retinal image by the L, M, and S cones (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Noise in the cone signals is characterized by a Poisson process. The forward model allows us to compute the <italic>likelihood</italic> function. The likelihood function represents the probability that an observed pattern of cone excitations was produced by any given image.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Model of the initial visual encoding and Bayesian reconstruction from cone mosaic excitation.</title><p>(<bold>A</bold>) The visual stimulus, in our case a natural image in RGB format, is displayed on a simulated monitor, which generates a hyperspectral scene representation of that image. (<bold>B</bold>) The hyperspectral image is blurred with a set of wavelength-dependent point-spread functions typical of human optics. We also account for spectral transmission through the lens and the macular pigment. This process produces the retinal image at the photoreceptor plane. (<bold>C</bold>) The retinal image is then sampled by a realistic cone mosaic, which generates cone excitations (isomerizations) for each cone. The trial-by-trial variability in the cone excitations is modeled as a Poisson process. (<bold>D</bold>) Our Bayesian reconstruction method takes the pattern of cone excitations as input and estimates the original stimulus (RGB image) based on the likelihood function and a statistical model (prior distribution) of natural images (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig1-v3.tif"/></fig><p>To obtain a <italic>prior</italic> over natural images, we applied independent components analysis (ICA, see Materials and methods) to a large dataset of natural images (<xref ref-type="bibr" rid="bib81">Russakovsky et al., 2015</xref>), and fit an exponential probability density function to the individual component weights (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The prior serves as our description of the statistical structure of natural images.</p><p>Given the likelihood function, prior distribution, and an observed pattern of cone excitations, we can then obtain a reconstruction of the original image stimulus by applying Bayes rule to find the posterior probability of any image given that pattern. We take the reconstructed image as the one that maximizes the posteriori probability (MAP estimate, see Materials and methods) (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><sec id="s2-1"><title>Basic properties of the reconstructions</title><p>To understand the consequences of initial visual encoding, we need to study the interaction between the likelihood function (i.e. our model of the initial encoding) and the statistics of natural images (i.e. the image prior). There are strong constraints on the statistical structure of natural images, such that natural images occupy only a small manifold within the space of all possible images. The properties of the initial encoding produce ambiguities with respect to what image is displayed when only the likelihood function is considered, but if these can be resolved by taking advantage of the statistical regularities of the visual environment, they should in principle, not prohibit effective visual perception. To illustrate this point, consider the simple example of discrete signal sampling: Based on the sampled signal, one cannot distinguish between the original signal from all its possible aliases (<xref ref-type="bibr" rid="bib15">Bracewell, 1986</xref>). However, with the prior knowledge that the original signal contains only frequencies below the Nyquist frequency of the sampling array, this ambiguity is resolved. In the context of our current study, the role of the natural image prior comes in several forms, as we will demonstrate in Results. First, since the reconstruction problem is underdetermined, the prior is a regularizer, providing a unique MAP estimate; Second, the prior acts as a denoiser, counteracting the Poisson noise in the cone excitation; Lastly, the prior guides the spatial and spectral demosaicing of the signals provided via the discrete sampling of the retinal image by the cone mosaic.</p><p>To highlight the importance of prior information while holding the likelihood function fixed, we can vary a parameter <inline-formula><mml:math id="inf1"><mml:mi>γ</mml:mi></mml:math></inline-formula> that adjusts the weight of the log-prior term in the reconstruction objective function (see Materials and methods). Explicitly manipulating <inline-formula><mml:math id="inf2"><mml:mi>γ</mml:mi></mml:math></inline-formula> reveals the effect of the prior on the reconstruction (<xref ref-type="fig" rid="fig2">Figure 2</xref>). When <inline-formula><mml:math id="inf3"><mml:mi>γ</mml:mi></mml:math></inline-formula> is small, the reconstruction is corrupted by the noise and the ambiguity of the initial visual encoding (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). When <inline-formula><mml:math id="inf4"><mml:mi>γ</mml:mi></mml:math></inline-formula> is large, the prior leads to desaturation and over-smoothing (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) in the reconstruction. For the rest of our simulations, the value of <inline-formula><mml:math id="inf5"><mml:mi>γ</mml:mi></mml:math></inline-formula> is determined on the training set by a cross-validation procedure that minimizes the reconstruction error, unless specified otherwise (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Effect of prior weight on reconstructed image.</title><p>Reconstruction error for an example natural image using a 1 deg foveal mosaic and root sum of squared distance (RSS, y-axis) in the pixel space as the error metric, as a function of weight <inline-formula><mml:math id="inf6"><mml:mi>γ</mml:mi></mml:math></inline-formula> on the log-prior term (x-axis, see Materials and methods) in the reconstruction objective function. The reconstructed image obtained with each particular <inline-formula><mml:math id="inf7"><mml:mi>γ</mml:mi></mml:math></inline-formula> value is shown alongside each corresponding point. Image (<bold>C</bold>) corresponds to the value of <inline-formula><mml:math id="inf8"><mml:mi>γ</mml:mi></mml:math></inline-formula> obtained through the cross-validation procedure (see Materials and methods). The images at the bottom are magnified versions of a subset of the images for representative <inline-formula><mml:math id="inf9"><mml:mi>γ</mml:mi></mml:math></inline-formula> values, as indicated by the solid dots in the plot.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig2-v3.tif"/></fig><p>To further elucidate properties of the Bayesian reconstruction, especially the interaction between the likelihood and prior, we plotted a few representative images in a log-prior, log-likelihood coordinate system, given a particular instance of cone excitations (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The optimal reconstruction, taken as the MAP estimate, has both a high prior probability and likelihood value as expected (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). In fact, for our reconstruction algorithm, there should not exist any image above the <inline-formula><mml:math id="inf10"><mml:mi>γ</mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>y</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>c</mml:mi></mml:math></inline-formula> line that goes through <bold>A</bold> (solid line, <xref ref-type="fig" rid="fig3">Figure 3</xref>), otherwise the optimization routine has failed to find the global optimum. The original image stimulus (ground truth) has a slightly lower likelihood value, mainly due to noise present in the cone excitations, and also a slightly lower prior probability, possibly due to the fact that our prior is only an approximation to the true natural image distribution (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The detrimental effect of noise becomes prominent in a maximum likelihood estimate (MLE, <xref ref-type="fig" rid="fig3">Figure 3C</xref>): Noise in the cone excitations is interpreted as true variation in the original image stimulus, thus slightly increasing the likelihood value but also creating artifacts. Such artifacts are penalized by the prior in other reconstructions. Furthermore, even without the presence of noise, other features of the initial visual encoding (e.g. <xref ref-type="fig" rid="fig1">Figure 1B and C</xref>) cause loss of information and ambiguity for the reconstruction. This is illustrated by a set of images that lie on the equal likelihood line with the MAP reconstruction (<xref ref-type="fig" rid="fig3">Figure 3D</xref>): There exist an infinite set of variations in the image (stimulus) that have no effect on the value of the likelihood function (i.e. variations within the null space of the linear likelihood render matrix, see Materials and methods). Thus, the cone excitations provide no information to distinguish between images that differ by such variations. However, as with the case of noise, variations inconsistent with natural images are discouraged by the prior. (Another implication of the existence of the null space is that the MLE solution to the reconstruction problem is actually underdetermined, as an entire subspace of images can have the same likelihood value. In the figure we show one arbitrarily chosen MLE estimate.) Other corruptions of the image, such as addition of white noise in the RGB pixel space, are countered by both the likelihood and prior (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Lastly, for illustrative purposes, we can increase the prior probability of the reconstruction relative to the optimal by making it spatially or chromatically more uniform (<xref ref-type="fig" rid="fig3">Figure 3F</xref>), but doing so decreases the likelihood.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Solution space of image reconstruction.</title><p>Given a particular instance of cone excitations, we can evaluate the (log-)prior probability (x-axis) and (log-)likelihood value (y-axis) for arbitrary images. Here, a few representative images are shown together with their corresponding location in a log-prior, log-likelihood coordinate system. (<bold>A</bold>) The optimal MAP reconstruction obtained via the reconstruction algorithm. The solid line shows <inline-formula><mml:math id="inf11"><mml:mi>γ</mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>y</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>c</mml:mi></mml:math></inline-formula>, with the value of <inline-formula><mml:math id="inf12"><mml:mi>c</mml:mi></mml:math></inline-formula> evaluated at the optimal reconstruction and with the value of <inline-formula><mml:math id="inf13"><mml:mi>γ</mml:mi></mml:math></inline-formula> matched to that obtained through cross-validation. (<bold>B</bold>) Original input image (ground truth). (<bold>C</bold>) A reconstruction generated by maximum likelihood estimation (MLE, set <inline-formula><mml:math id="inf14"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>). Note that the maximum likelihood reconstruction shown is not unique, since adding any pattern from the null space of the likelihood matrix leads to a different reconstruction with the same maximum likelihood. Here one arbitrarily chosen MLE reconstruction is shown. (<bold>D</bold>) Optimal reconstruction, corrupted by patterns randomly sampled from the null space of the likelihood render matrix (see Materials and methods). These have the same likelihood as the optimal reconstruction, but lower prior probability. (<bold>E</bold>) Optimal reconstruction, corrupted by white noise in RGB space. (<bold>F</bold>) Grayscale version of the optimal reconstruction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig3-v3.tif"/></fig></sec><sec id="s2-2"><title>Optimal allocation of retinal photoreceptors</title><p>Within the Bayesian reconstruction framework, the goal of the visual front end can be characterized as minimizing the average error in reconstruction across the set of natural images. In this context, we can ask how to choose various elements of the initial encoding, subject to constraints, to minimize the expected reconstruction error under the natural image prior (<xref ref-type="bibr" rid="bib58">Levin et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Manning and Brainard, 2009</xref>). More formally, we seek the ‘design’ parameters <inline-formula><mml:math id="inf15"><mml:mi mathvariant="normal">θ</mml:mi></mml:math></inline-formula> of a visual system:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mi>x</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf17"><mml:mi>x</mml:mi></mml:math></inline-formula> represents individual samples of natural images, <inline-formula><mml:math id="inf18"><mml:mi>m</mml:mi></mml:math></inline-formula> represents instances of cone excitation (i.e. sensory measurements), and <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is our model of the initial encoding (i.e. likelihood function). The particular features under consideration of the modeled visual system are indicated explicitly by the parameter vector <inline-formula><mml:math id="inf20"><mml:mi mathvariant="normal">θ</mml:mi></mml:math></inline-formula>. The MAP image reconstruction is indicated by <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf22"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>∙</mml:mo><mml:mo>,</mml:mo><mml:mo>∙</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> is a loss function that assesses reconstruction error. In practice, the expectations are approximated by taking the average over large samples of natural images and cone excitations. (For simplicity in the development here, we did not include the parameter <inline-formula><mml:math id="inf23"><mml:mi>γ</mml:mi></mml:math></inline-formula> that we incorporated into our reconstruction algorithm in the equations above. It was included in the actual computations that investigated the reconstruction performance. Also note that the MAP estimate is not in general the one that minimizes the expected loss. We use the MAP estimate as a computationally tractable proxy for the loss-minimizing estimate.)</p><p>One intriguing design problem is the allocation of cone photoreceptor types: The maximum number of photoreceptors (cones) per unit area is bounded due to biological constraints. How should the visual system assign this limited resource across the three different types of cones? It has been observed in human subjects that there is a relatively sparse population of S cones, while large individual variability exists in the L/M cone ratio (<xref ref-type="bibr" rid="bib46">Hofer et al., 2005</xref>). Previous research has used information-theoretical measures combined with approximations to address this question (<xref ref-type="bibr" rid="bib38">Garrigan et al., 2010</xref>). Here, we empirically evaluated a loss function (i.e. we used root sum of squares distance in the RGB pixel space as well as the S-CIELAB space) on the reconstructed images, while systematically changing the allocation of retinal cone types (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Effect of the allocation of retinal cone types on reconstruction.</title><p>Average image reconstruction error from a 1 deg foveal mosaic on a set of natural images from the evaluation set, computed as root sum of squares (RSS) distance in the RGB pixel space (y-axis, left panels) and the S-CIELAB space (y-axis, right panels), as a function of different allocations of retinal photoreceptor (cone) types in the mosaic. (<bold>A</bold>) Average (over evaluation images) reconstruction error as a function of %L cone (top x-axis), or L:M cone ratio (bottom x-axis). Example mosaics with different %L values are shown below the plot. Error bars indicate ±1 SEM. (<bold>B</bold>) Average reconstruction error as a function of %S cone (top x-axis), or S:(L + M) cone ratio (bottom x-axis). Example mosaics with different %S values are shown below the plot. Error bars indicate ±1 SEM across sampled images. See <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for a replication of the same analysis with hyperspectral images.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Factors that contribute to optimal S cone proportion.</title><p>Average reconstruction error as a function of S-cone proportion, computed as RSS of pixel values for the R- (left), G- (middle), and B-planes (right), respectively. Under typical conditions (red), a low S-cone ratio is optimal for all three planes. Removing lens pigment and macular pigment from the simulations (blue) increases the SNR of the S cones by increasing their average quantum catch, but has little effect on the optimal S-cone proportion for any of the image planes. Correcting chromatic aberration (green) while retaining lens pigment and macular pigment greatly improves the information provided by the S cones for the B-plane, but not for the R- and G- planes. Error bars indicate ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig4-figsupp1-v3.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Effect of the allocation of retinal cone types on reconstruction of hyperspectral images.</title><p>Average reconstruction error as a function of L-cone proportion (top) and S-cone proportion (bottom), computed as RSS of pixel values over space and wavelength, for a set of evaluation hyperspectral images of size of <inline-formula><mml:math id="inf24"><mml:mn>18</mml:mn><mml:mi>*</mml:mi><mml:mn>18</mml:mn></mml:math></inline-formula> and 15 uniform wavelength sample between 420 nm and 700 nm (see Materials and methods). Error bars indicate ±1 SEM. The results corroborated our main conclusion obtained with RGB images, shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig4-figsupp2-v3.tif"/></fig></fig-group><p>Interestingly, we found that large variations (nearly a 10-fold range) in the assignment of L and M cones have little impact on the average reconstruction error (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Only when the proportion of L or M cones becomes very low is there a substantial increase in reconstruction error, as the modeled visual system approaches dichromacy. On the other hand, the average reconstruction error as a function of the proportion of S cones shows a clear optimum at a small S-cone proportion (~10%; <xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><p>Our results are in agreement with a previous analysis in showing that the empirically observed allocation of retinal photoreceptor type is consistent with the principle of optimal design (<xref ref-type="bibr" rid="bib38">Garrigan et al., 2010</xref>; also see <xref ref-type="bibr" rid="bib58">Levin et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Manning and Brainard, 2009</xref>; <xref ref-type="bibr" rid="bib84">Sampat et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Jiang et al., 2017</xref>). The indifference to L/M ratio can be explained by the large spatial and chromatic correlations present in natural images, together with the high overlap in L- and M-cone spectral sensitivities. This leads to a high correlation in the excitations of neighboring L and M cones in response to natural images, allowing cones of one type to be substituted for cones of the other type with little effect on reconstruction error (see the next paragraph for additional analysis on this point). Additional analysis (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) revealed that the sensitivity to S cone proportion is due to a combination of two main factors: (1) chromatic aberrations, which blur the retinal image at short wavelengths and reduce the value of dense spatial sampling at these wavelengths; and (2) S cones mainly contribute to the estimation of pixel values in the B-pixel plane, whereas L and M cone contribute to both the R- and G-pixel planes (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This makes L and M cones more informative than S cones, given the particular loss functions we employ to evaluate reconstruction error. To further validate our conclusion, we have also replicated our analysis with a dataset of hyperspectral (as opposed to RGB) images (<xref ref-type="bibr" rid="bib67">Nascimento et al., 2002</xref>; <xref ref-type="bibr" rid="bib26">Chakrabarti and Zickler, 2011</xref>), with a loss function applied directly to the whole spectrum, and have obtained similar results (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, also see Materials and methods).</p><p>To further study the role of statistical regularities in the optimal allocation of photoreceptor type, we repeated the L-cone proportion analysis above, but on different sets of synthetic image datasets for which the spatial and chromatic correlations in the images were manipulated explicitly (see Materials and methods). The dependence of the average reconstruction error on the L-cone proportion decreases as the chromatic correlation in the signal increases (<xref ref-type="fig" rid="fig5">Figure 5</xref>). A decrease of spatial correlation has little impact on the shape of the curves, but increases the overall magnitude of reconstruction error (<xref ref-type="fig" rid="fig5">Figure 5</xref>; to highlight the shape, the scale of the y-axis is different across rows and columns. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for the same plot with matched y-axis scale). When both the chromatic and spatial correlation are high, there is a large margin of L-cone proportion within which the reconstruction error is close to the optimal (minimal) point (<xref ref-type="fig" rid="fig5">Figure 5</xref>, shaded area). This analysis highlights the importance of considering visual system design in the context of the statistical properties (prior distribution) of natural images, as it shows that the conclusions drawn can vary with these properties (<xref ref-type="bibr" rid="bib9">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib34">Derrico and Buchsbaum, 1991</xref>; <xref ref-type="bibr" rid="bib10">Barlow and Földiàgk, 1989</xref>; <xref ref-type="bibr" rid="bib5">Atick et al., 1992</xref>; <xref ref-type="bibr" rid="bib59">Lewis and Zhaoping, 2006</xref>; <xref ref-type="bibr" rid="bib58">Levin et al., 2008</xref>; <xref ref-type="bibr" rid="bib12">Borghuis et al., 2008</xref>; <xref ref-type="bibr" rid="bib38">Garrigan et al., 2010</xref>; <xref ref-type="bibr" rid="bib98">Tkacik et al., 2010</xref>; <xref ref-type="bibr" rid="bib6">Atick, 2011</xref>; <xref ref-type="bibr" rid="bib24">Burge, 2020</xref>). Natural images are thought to have both high spatial and high chromatic correlation (<xref ref-type="bibr" rid="bib107">Webster and Mollon, 1997</xref>; <xref ref-type="bibr" rid="bib67">Nascimento et al., 2002</xref>; <xref ref-type="bibr" rid="bib38">Garrigan et al., 2010</xref>), making the results shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> consistent with those in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Effect of spatial and chromatic correlation on the optimal allocation of photoreceptors.</title><p>Average image reconstruction error from a half-degree square foveal mosaic on different sets of synthetic images, computed as root sum of squares (RSS) distance in the RGB pixel space, as a function of %L cone (L:M cone ratio) of the mosaic (i.e. similar to <xref ref-type="fig" rid="fig4">Figure 4A</xref>, left column). The shaded areas represent %L values that correspond to RSS values within a +0.1 RSS margin of the optimal (minimum RSS) point. Within each panel, synthetic images were sampled from a Gaussian distribution with specified spatial and chromatic correlation, as indicated by example images on the top row and rightmost column, and reconstruction was performed with the corresponding Gaussian prior (see Materials and methods). The overall RSS is reduced compared to <xref ref-type="fig" rid="fig4">Figure 4</xref> due to the smaller image size used and the fact that the images were drawn from a different prior, as well as because the prior used in reconstruction exactly describes the images for this case. In addition, reconstruction error bars are negligible due to the large image sample size used.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Effect of spatial and chromatic correlation on the optimal allocation of photoreceptors (with matched y-axis).</title><p>Same as <xref ref-type="fig" rid="fig5">Figure 5</xref> but with matched y-axis to highlight the overall magnitude of errors across the different conditions. Average image reconstruction error from a half-degree square foveal mosaic on different sets of synthetic images, computed as root sum of squares (RSS) distance in the RGB pixel space, as a function of %L cone (L:M cone ratio) of the mosaic The shaded areas represent %L values that correspond to RSS values within a +0.1 RSS margin from the optimal (minimum RSS) point.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig5-figsupp1-v3.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Visualization of color deficiency with image reconstruction</title><p>In addition to quantification, the reconstruction framework also provides a method for visualizing the effect of information loss in the initial visual encoding. We know that extreme values of L:M cone ratio create essentially dichromatic retinal mosaics, and from the analysis above we observed that these lead to high reconstruction error. To understand the nature of this error, we can directly visualize the reconstructed images.</p><p><xref ref-type="fig" rid="fig6">Figure 6A</xref> shows reconstructions of a set of example images from different dichromatic retinal mosaics. While the spatial structure of the original images is largely retained in the reconstructions, each type of dichromacy creates a distinct pattern of color confusions and shifts in the reconstructed color. Note that in the case where there is no simulated cone noise (as in <xref ref-type="fig" rid="fig6">Figure 6</xref>), the original image has a likelihood at least as high as the reconstruction obtained via our method. Thus, the difference between the original images and each of the corresponding dichromatic reconstructions is driven by the image prior. On the other hand, the difference in the reconstructions across the three types of dichromacy illustrates how the different dichromatic likelihood functions interact with the prior.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Visualization of the effect of dichromacy.</title><p>Reconstructions of a set of example images in the evaluation set from different types of 1 degree foveal dichromatic retinal mosaics (protanopia, deuteranopia, tritanopia) together with other previously proposed methods for predicting color appearance for dichromats. (<bold>A</bold>) Our method; (<bold>B</bold>) <xref ref-type="bibr" rid="bib20">Brettel et al., 1997</xref>; (<bold>C</bold>) <xref ref-type="bibr" rid="bib47">Jiang et al., 2016</xref>. Cone noise was not simulated for the images shown in this figure, since the comparison methods operate directly on the input images. See Materials and methods for a brief description of the implementation of the two other methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig6-v3.tif"/></fig><p>One might speculate as to whether the reconstructions predict color appearance as experienced by dichromats. To approach this, we compare the reconstructions with two other methods that have been proposed to predict the color appearance for dichromats (<xref ref-type="bibr" rid="bib20">Brettel et al., 1997</xref>; <xref ref-type="bibr" rid="bib47">Jiang et al., 2016</xref>). To determine an image based on the excitations of only two classes of cones, any method will need to rely on a set of regularizing assumptions to resolve the ambiguity introduced by the dichromatic retinas. <xref ref-type="bibr" rid="bib20">Brettel et al., 1997</xref> started with the trichromatic cone excitations of each image pixel, and projected these onto a biplanar surface, with each plane defined by the neutral color axis and an anchoring stimulus identified through color appearance judgments made across the two eyes of unilateral dichromats. The resulting trichromatic excitations were then used to determine the rendered RGB values (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). <xref ref-type="bibr" rid="bib47">Jiang et al., 2016</xref> also adopted a reconstruction approach, but one that reconstructed the incident spectrum from the dichromatic cone excitations at each pixel. They then projected the estimated spectra onto trichromatic cone excitations, and used these to render the RGB values (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In their method, a spectral smoothness constraint was introduced to regularize the spectral estimates, which favors desaturated spectra. In this sense, their prior is similar to ours: The sparse prior we used is centered on the average image, which is desaturated, and also encourages achromatic content due to the high correlations across color channels. One noticeable difference between our method and the other two is that ours takes into account the spatial structure of the image.</p><p>Interestingly, although there are differences in detail between the images obtained, in many cases the different methods produce visualizations that are quite similar. We find the general agreement between the reconstruction-based methods and the one based on subject reports an encouraging sign that the reconstruction approach can be used to predict aspects of appearance.</p><p>Anomalous trichromacy is another form of color deficiency that is commonly found in human observers. For example, in deuteranomaly, the spectral sensitivity of the M cones is shifted toward that of the L cones (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Since the three cone spectral sensitivity functions are linearly independent of each other, in the absence of noise we should be able to obtain a trichromatic reconstruction from the excitations of the deuteranomalous mosaic. However, in the presence of noise, we expect that the high degree of overlap between M and L spectral sensitivities will result in a lower signal-to-noise ratio (SNR) in the difference between M- and L-cone excitations, compared to that of a normal trichromatic observer, and thus lead to worse reconstructions. We performed image reconstructions for a normal trichromatic (with a peak spectral sensitivity of M cone at 530 nm) and a deuteranomalous (with a peak spectral sensitivity of M cone at 550 nm) 1 deg foveal mosaic at different overall light intensity levels (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Due to the nature of Poisson noise, the higher the light intensity, the higher the SNR of the cone excitations. At high light intensities, the reconstructions are similar for the normal and deuteranomalous mosaics (first row). At lower intensities, however, the deuteranomalous reconstruction lacks chromatic content still present in the normal reconstruction (second and third row). The increase in noise also reduces the amount of spatial detail in the reconstructed images, due to the denoising effect driven by the image prior. Furthermore, a loss of chromatic content is also seen for the reconstruction from the normal mosaic at the lowest light level (last row). This observation may be connected to the fact that biological visual systems that operate at low light levels are typically monochromatic, potentially to increase the SNR of spatial vision at the cost of completely disregarding color (e.g. the monochromatic human rod system; see <xref ref-type="bibr" rid="bib62">Manning and Brainard, 2009</xref> for a related and more detailed treatment; also see <xref ref-type="bibr" rid="bib104">Walls, 1942</xref>; <xref ref-type="bibr" rid="bib80">Rushton, 1962</xref>; <xref ref-type="bibr" rid="bib101">van Hateren, 1993</xref>; <xref ref-type="bibr" rid="bib54">Land and Osorio, 2003</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Comparison of normal and deuteranomalous observers at varying light intensities.</title><p>Image reconstructions for a set of example images in the evaluation set from 1 degree, foveal (<bold>A</bold>) normal trichromatic and (<bold>B</bold>) deuteranomalous trichromatic mosaics at four different overall light intensity levels that lead to different Poisson signal-to-noise ratios in the cone excitations. The average excitations (photo-isomerizations) per cone per 50ms integration time is chosen to be approximately 10<sup>4</sup> for <italic>Outdoor Daylight</italic>, 10<sup>3</sup> for <italic>LCD Monitor</italic>, 10<sup>2</sup> for <italic>Dim Light</italic>, and 10<sup>1</sup> for <italic>Twilight</italic> (<xref ref-type="bibr" rid="bib59">Lewis and Zhaoping, 2006</xref>; <xref ref-type="bibr" rid="bib96">Stockman and Sharpe, 2006</xref>). The prior weight parameter in these set of simulations was set based on a cross-validation procedure that minimizes RMSE <inline-formula><mml:math id="inf25"><mml:mfenced separators="|"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> . To highlight interaction between noise and the prior, we have also included a set of reconstructions with the prior weight set to a much lower level <inline-formula><mml:math id="inf26"><mml:mfenced separators="|"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> , see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig7-v3.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Reconstruction with a weak prior across SNR levels.</title><p>Image reconstructions for a set of example images in the evaluation set from 1 degree, foveal (<bold>A</bold>) normal trichromatic and (<bold>B</bold>) deuteranomalous trichromatic mosaics at five different overall light intensity levels that lead to different Poisson signal-to-noise ratios in the cone excitations. The average excitations (photo-isomerizations) per cone per 50ms integration time is chosen to be approximately 10<sup>4</sup> for <italic>Outdoor Daylight</italic>, 10<sup>3</sup> for <italic>LCD Monitor</italic>, 10<sup>2</sup> for <italic>Dim Light</italic>, and 10<sup>1</sup> for <italic>Twilight</italic> (<xref ref-type="bibr" rid="bib59">Lewis and Zhaoping, 2006</xref>; <xref ref-type="bibr" rid="bib96">Stockman and Sharpe, 2006</xref>). To highlight the effect of noise and prior, the prior weight was set to a much lower level <inline-formula><mml:math id="inf27"><mml:mfenced separators="|"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> than the optimal value <inline-formula><mml:math id="inf28"><mml:mfenced separators="|"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> used for the results shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig7-figsupp1-v3.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Effect of physiological optics and mosaic spatial sampling</title><p>So far, our visualizations have focused on chromatic information loss due to a reduced number of cone types or a shift in cone spectral sensitivity. However, imperfection in the physiological optics, combined with the spatial sampling of retinal mosaic, also introduces significant loss of information. Furthermore, the interleaved nature of the mosaic means that color and pattern are entangled at the very initial stage of visual processing (<xref ref-type="bibr" rid="bib18">Brainard, 2019</xref>). To highlight these effects, we reconstructed natural images from 1 deg patches of mosaics at different retinal eccentricities across the visual field, with (1) changes in optical aberrations (<xref ref-type="bibr" rid="bib74">Polans et al., 2015</xref>); (2) increases in size and decreases in density of the photoreceptors (<xref ref-type="bibr" rid="bib32">Curcio et al., 1990</xref>); and (3) decreases in the density of the macular pigment (<xref ref-type="bibr" rid="bib70">Nolan et al., 2008</xref>; <xref ref-type="bibr" rid="bib76">Putnam and Bland, 2014</xref>). The degradation in the quality of the reconstructed images can be clearly observed as we move from the fovea to the periphery (<xref ref-type="fig" rid="fig8">Figure 8</xref>; See <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> for an enlarged view of the mosaic and optics). For some retinal locations, the elongated point-spread function (PSF) also introduces a salient directional blur (<xref ref-type="fig" rid="fig8">Figure 8E and F</xref>). For a simple quantification of the average reconstruction error as a function of visual eccentricity, see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Image reconstruction with optics/mosaic at different retinal eccentricities.</title><p>Image reconstructions for a set of example images in the evaluation set from 1 degree patches of mosaic at different retinal eccentricities. The coordinates at the top of each column indicate the horizontal and vertical eccentricity of the patch used for that column. The image at the top left of each column shows a contour plot of the point-spread function relative to an expanded view of the cone mosaic used for that column, while the image at the top right of each column shows the full 1 degree mosaic (see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> for an enlarged view of the mosaic and optics).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig8-v3.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Optics and cone mosaic at different retinal eccentricities.</title><p>Enlarged view of the top panels of <xref ref-type="fig" rid="fig8">Figure 8</xref>. The coordinates at the top of each pair indicate the horizontal and vertical eccentricity of the retinal patch. The left image of each pair shows a contour plot of the point-spread function relative to an expanded view of the cone mosaic, while the right image of each pair shows the full 1 degree mosaic used in the simulation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig8-figsupp1-v3.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Reconstruction error at different visual eccentricities.</title><p>Average image reconstruction error, computed as RSS of pixel values for both the RGB images (left y-axis), and corresponding gray scale images to measure the spatial error, define as the first PC based on a PCA analysis of our image dataset (i.e. <inline-formula><mml:math id="inf29"><mml:mn>0.57</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>0.59</mml:mn><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mn>0.56</mml:mn><mml:mi>B</mml:mi><mml:mo>,</mml:mo></mml:math></inline-formula> right y-axis), as a function of the visual eccentricity location of a 1 deg retinal mosaic. Error bars indicate ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig8-figsupp2-v3.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>Image reconstruction with different point spread functions.</title><p>(<bold>A</bold>) Image reconstructions for a set of example images in the evaluation set from 1 degree patches of mosaic at (10, 10) degree eccentricity, but with PSFs sampled from different visual eccentricities as indicated by the top panel. (<bold>B</bold>) The average differential reconstruction error (i.e. difference in RSS compared to the lowest value obtained among the simulations) as a function of the eccentricity of the PSFs used. Error bars represent ±1 SEM. To separate the spatial and chromatic error, we perform a PCA analysis on the RGB images. The RSS along the first PC (<inline-formula><mml:math id="inf30"><mml:mn>0.57</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>0.59</mml:mn><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mn>0.56</mml:mn><mml:mi>B</mml:mi></mml:math></inline-formula>) corresponds to the spatial error (left axis), while the RSS along the second and third PCs (<inline-formula><mml:math id="inf31"><mml:mn>0.76</mml:mn><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mn>0.13</mml:mn><mml:mi>G</mml:mi><mml:mo>-</mml:mo><mml:mn>0.64</mml:mn><mml:mi>B</mml:mi><mml:mo>;</mml:mo><mml:mi> </mml:mi><mml:mo>-</mml:mo><mml:mn>0.31</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>0.80</mml:mn><mml:mi>G</mml:mi><mml:mo>-</mml:mo><mml:mn>0.52</mml:mn><mml:mi>B</mml:mi></mml:math></inline-formula>) quantify the chromatic error (right axis). With the range of PSFs in our simulation, the minimal spatial error is obtained with the PSF at (10, 10) deg (i.e. the PSF that matched to the mosaic), and the minimal chromatic error is obtained with the largest PSF, corresponding to (18, 18) deg.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig8-figsupp3-v3.tif"/></fig><fig id="fig8s4" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 4.</label><caption><title>Image reconstruction at peripheral eccentricities with maximum likelihood estimation (MLE).</title><p>Image reconstructions obtained using maximum likelihood estimation for a few example images in the evaluation set from 1 degree patches of mosaic at different retinal eccentricities, as indicated at the top of each column. Note that simulation of cone excitation noise is turned off for these reconstructions. Note also that the MLE reconstructions are not unique (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). The MLE reconstructions shown here were chosen arbitrarily as the ones converged upon by our particular numerical search algorithm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig8-figsupp4-v3.tif"/></fig></fig-group><p>The consequences of irregular spatial sampling by the cone mosaic have been previously studied with the framework of signal processing (<xref ref-type="bibr" rid="bib92">Snyder et al., 1977</xref>; <xref ref-type="bibr" rid="bib112">Yellott, 1983</xref>). Our results highlight that optimizing the initial visual encoding depends in rich ways on the interplay between the cone sampling and the optics. While less information (i.e. at more eccentric locations) does lead to overall lower quality reconstructions (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>), exactly which aspects of the reconstructions are incorrect can vary in subtle ways. Concretely, in <xref ref-type="fig" rid="fig8">Figure 8</xref>, we observe a trade-off across visual eccentricity between spatial and chromatic vision. In the image of the dragonfly, for example, the reconstructed colors are desaturated at intermediate eccentricities (e.g. <xref ref-type="fig" rid="fig8">Figure 8C and D</xref>), compared with the fovea (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) and more eccentric locations (<xref ref-type="fig" rid="fig8">Figure 8E and F</xref>). The desaturation is qualitatively consistent with the literature that indicates a decrease in chromatic sensitivity at peripheral visual eccentricities, at least for the red-green axis of color perception and for some stimulus spatial configurations (<xref ref-type="bibr" rid="bib103">Virsu and Rovamo, 1979</xref>; <xref ref-type="bibr" rid="bib66">Mullen and Kingdom, 1996</xref>; but see <xref ref-type="bibr" rid="bib44">Hansen et al., 2009</xref>). To further elucidate this richness, in an additional analysis, we systematically varied the size of the PSF for a fixed peripheral retinal mosaic. This revealed that (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>): (1) A larger PSF does lead to better estimate of chromatic content, albeit eventually at the cost of spatial content. (2) In general, an appropriate amount of optical blur is required to achieve the best overall image reconstruction performance, presumably due to its prevention of aliasing. We will treat the issue of spatial aliasing further in the next section.</p><p>Lastly, to emphasize the importance of the natural image prior, we performed a set of maximum likelihood reconstructions with no explicit prior constraint, which resulted in images with less coherent spatial structure and lower fidelity color appearance (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref>). Thus, the prior here is critical for the proper demosaicing and interpolation of the information provided by the sparse cone sampling at these peripheral locations.</p></sec><sec id="s2-5"><title>Spatial aliasing</title><p>As we have alluded to above, the retinal mosaic and physiological optics can also interact in other important ways: Both in humans and other species, it has been noted that the optical cut-off of the eye is reasonably matched to the spacing of the photoreceptors (i.e. the mosaic Nyquist frequency), enabling good spatial resolution while minimizing spatial aliasing due to discrete sampling (<xref ref-type="bibr" rid="bib109">Williams, 1985</xref>; <xref ref-type="bibr" rid="bib93">Snyder et al., 1986</xref>; <xref ref-type="bibr" rid="bib55">Land and Nilsson, 2012</xref>). In contrast to our work, these analyses did not take into account the fact that the cone mosaic interleaves multiple spectral classes of cones (but see <xref ref-type="bibr" rid="bib110">Williams et al., 1991</xref>; <xref ref-type="bibr" rid="bib17">Brainard, 2015</xref>), and here we revisit classic experiments on spatial aliasing for a trichromatic mosaic using our reconstruction framework.</p><p>Experimentally, it has been demonstrated that with instruments that <italic>bypass</italic> the physiological optics and present high contrast grating stimuli directly on the retina, human subjects can detect spatial frequencies up to 200 cyc/deg (<xref ref-type="bibr" rid="bib109">Williams, 1985</xref>). For foveal viewing, subjects also report having a percept resembling a pattern of ‘two-dimensional noise’ and/or ‘zebra stripes’ when viewing those high spatial frequency stimuli (<xref ref-type="bibr" rid="bib109">Williams, 1985</xref>). For peripheral viewing, high frequency vertical gratings can be perceived as horizontal (and vice-versa; <xref ref-type="bibr" rid="bib29">Coletta and Williams, 1987</xref>). We explored these effects within our framework as follows: We reconstructed a set of vertical chromatic grating stimuli from the cone excitations of a foveal and a peripheral mosaic. To simulate the interferometric experimental conditions of <xref ref-type="bibr" rid="bib109">Williams, 1985</xref>, we used diffraction-limited optics with no longitudinal chromatic aberration (LCA), allowing high-frequency stimuli to reach the cone mosaic directly. For gratings that are above the typical optical cut-off frequency, we obtained reconstructions that (1) are quite distinct from a uniform field, which would allow them to be reliably detected in a discrimination protocol; and (2) lack the coherent vertical structure of the original stimulus (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Concretely, the reconstructions recapitulate the ‘zebra stripe’ percept reported at approximately 120 cyc/deg in the fovea (<xref ref-type="fig" rid="fig9">Figure 9A</xref>); as well as the orientation-reversal effect at an appropriate spatial frequency in the periphery (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). Both results corroborate previous theoretical analysis and psychophysical measurements (<xref ref-type="bibr" rid="bib109">Williams, 1985</xref>; <xref ref-type="bibr" rid="bib29">Coletta and Williams, 1987</xref>), but now taking the trichromatic nature of the mosaic into account. On the other hand, with full optical aberrations, the reconstructed images became mostly uniform at these high spatial frequencies (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). Since our method accounts for trichromacy, we have also made the prediction that for achromatic grating stimuli viewed under similar diffraction-limited conditions, while the spatial aliasing pattern will be comparable, additional chromatic aliasing should be visible (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>; also see <xref ref-type="bibr" rid="bib110">Williams et al., 1991</xref>; <xref ref-type="bibr" rid="bib17">Brainard, 2015</xref>).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Reconstruction of chromatic grating stimuli without optical aberrations.</title><p>Image reconstruction of chromatic grating stimuli with increasing spatial frequency from (<bold>A</bold>) a 0.2 deg foveal mosaic and (<bold>B</bold>) a 1 deg peripheral mosaic at (18, 18) degree retinal eccentricity, using diffraction-limited optics without LCA. The leftmost images show an expanded view of the cone mosaic relative to a contour plot of a typical point-spread function at that eccentricity. Images were modulations of the red channel of the simulated monitor, to mimic the 633 nm laser used in the interferometric experiments. The exact frequency of the stimuli being used for each condition is as denoted in the figure. For a more extended comparison between reconstructions with and without optical aberrations, see <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref> and <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig9-v3.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Reconstruction of chromatic grating stimuli with/without optical aberrations.</title><p>Image reconstruction of chromatic grating stimuli with increasing spatial frequency from (<bold>A</bold>) a 0.2 deg foveal mosaic and (<bold>B</bold>) a 1 deg peripheral mosaic at (18, 18) degrees retinal eccentricity with full optical aberrations (left columns) and with diffraction-limited optics (right columns). The top left images show a contour plot of the point-spread function relative to an expanded view of the cone mosaic, while the top right images show the full mosaic. Images were modulations of the red channel of the simulated monitor, to mimic the 633 nm laser used in the interferometric experiments. The exact frequency of the stimuli being used for each condition is as denoted in the figure. Note that the mottle observed in the reconstructions with full optical aberrations at high spatial frequencies match the reconstruction of a uniform field of saturated red stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig9-figsupp1-v3.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>Reconstruction of achromatic grating stimuli with/without optical aberrations.</title><p>Image reconstructions of achromatic grating stimuli with increasing spatial frequency from (<bold>A</bold>) a 0.2 deg foveal mosaic and (<bold>B</bold>) a 1 deg peripheral mosaic at (18, 18) degree retinal eccentricity with full optical aberration (left columns) and with diffraction-limited optics (right columns). The top left images show a contour plot of the point-spread function relative to an expanded view of the cone mosaic, while the top right images show the full mosaic. The exact frequency of the stimuli being used for each condition is as denoted in the figure. The reconstruction shows similar spatial aliasing as in <xref ref-type="fig" rid="fig9">Figure 9</xref> and <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>, but shows an additional pattern of chromatic aliasing that arises because of the interleaved sampling by a mosaic of different cone types (<xref ref-type="bibr" rid="bib110">Williams et al., 1991</xref>; <xref ref-type="bibr" rid="bib16">Brainard et al., 2008</xref>). Whether such chromatic aliasing would actually be observed if a subject viewed achromatic gratings under diffraction-limited conditions is to our knowledge, an open question.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig9-figsupp2-v3.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Contrast sensitivity function</title><p>Our framework can also be adapted to perform ideal observer analysis for psychophysical discrimination (threshold) tasks, which have been used previously to evaluate the information available in the initial encoding. Here, we use the reconstructed images as the basis for discrimination decisions. This is potentially important since even the early post-receptoral visual representation (e.g. retinal ganglion cells), on which downstream decisions must be based, is likely shaped by the regularities of our visual environment (<xref ref-type="bibr" rid="bib5">Atick et al., 1992</xref>; <xref ref-type="bibr" rid="bib12">Borghuis et al., 2008</xref>; <xref ref-type="bibr" rid="bib50">Karklin and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib6">Atick, 2011</xref>). Our method provides a way to extend ideal observer analysis to incorporate these statistical regularities.</p><p>Concretely, we predicted and compared the diffraction-limited spatial contrast sensitivity function (CSF) for gratings with a half-degree spatial extent (see Materials and methods). First, we applied the classic signal-known-exactly ideal observer to the Poisson distributed excitations of the simulated cone mosaic. We computed CSFs for both achromatic (L + M) and chromatic (L - M) grating modulations, with matched cone contrast measured as the vector length of the cone contrast vector. As expected, the ideal observer at the cone excitations produces nearly identical CSFs for the contrast-matched L + M and L - M modulations; also, as expected, these fall off with spatial frequency, primarily because of optical blur (<xref ref-type="fig" rid="fig10">Figure 10A</xref>).</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Contrast sensitivity functions.</title><p>Contrast sensitivity, defined as the inverse of threshold contrast, for (<bold>A</bold>) a Poisson 2AFC ideal observer, and (<bold>B</bold>) an image reconstruction-based observer (see Materials and methods), as a function of the spatial frequency of stimulus in either the L + M direction (black) and L - M cone contrast direction (red). Contrast was measured as the vector length of the cone contrast vector, which is matched across the two color directions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig10-v3.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>Contrast sensitivity function of a MLE reconstruction observer.</title><p>Contrast sensitivity, defined as the inverse of threshold contrast, for an image reconstruction-based observer <italic>without</italic> the prior term (<inline-formula><mml:math id="inf32"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) as a function of the spatial frequency of stimulus in either L + M direction (black) and L - M direction (red) with equal RMS cone contrast. Note that the MLE reconstructions are not unique (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). In the computations whose results are shown here, the MLE reconstructions were chosen arbitrarily as the ones converged upon by our particular numerical search algorithm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-fig10-figsupp1-v3.tif"/></fig></fig-group><p>Next, we reconstructed images from the cone excitations produced by the grating stimuli. A template-matching observer based on the noise-free reconstructions was then applied to the noisy reconstructions (see Materials and methods). The image-reconstruction observer shows significant interactions between spatial frequency and chromatic direction. Sensitivity in the L + M direction is relatively constant with spatial frequency. Sensitivity in the L – M direction starts out higher than L + M at low spatial frequencies, but drops significantly and is lower than L + M at high spatial frequencies (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). We attribute these effects to the role of the image prior in the reconstructions, which leads to selective enhancement/attenuation of different image components. In support of this idea, we also found that an observer based on maximum likelihood reconstruction without the explicit prior term produced CSFs similar in shape to the Poisson ideal observer (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>).</p><p>It is intriguing that the CSFs from the reconstruction-based observer show substantially higher sensitivity for L - M than for L + M modulations at low spatial frequencies (with equated RMS cone contrast), but with a more rapid falloff such that the sensitivity for L + M modulations is higher at high spatial frequencies. Both of these features are characteristic of the CSFs of human vision (<xref ref-type="bibr" rid="bib65">Mullen, 1985</xref>; <xref ref-type="bibr" rid="bib2">Anderson et al., 1991</xref>; <xref ref-type="bibr" rid="bib27">Chaparro et al., 1993</xref>; <xref ref-type="bibr" rid="bib86">Sekiguchi et al., 1993</xref>). A more comprehensive exploration of this effect and its potential interaction with other decision rule used in the calculation awaits future research.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We developed a Bayesian image reconstruction framework for characterizing the initial visual encoding, by combining an accurate image-computable forward model together with a sparse coding model of natural image statistics. Our method enables both quantification and visualization of information loss due to various factors in the initial encoding, and unifies the treatment of a diverse set of issues that have been studied in separate, albeit related, ways. In several cases, we were able to extend previous studies by eliminating simplifying assumptions (e.g. by the use of realistic, large cone mosaics that operate on high-dimensional, naturalistic image input). To summarize succinctly, we highlight here the following novel results and substantial extensions of previous findings: (1) When considering the allocation of different cone types on the human retina, we demonstrated the importance of the spatial and spectral correlation structure of the image prior; (2) As we examined reconstructions as a way to visualize information loss, we observed rich interactions in how the appearances of the reconstruction vary with mosaic sampling, physiological optics, and the SNR of the cone excitations; (3) We found that the reconstructions are consistent with empirical reports of retinal spatial aliasing obtained with interferometric stimuli, adding an explicit image prior component and extending consideration of the interleaved nature of the trichromatic retinal cone mosaic relative to the previous treatment of these phenomena; (4) We linked image reconstructions to spatio-chromatic contrast sensitivity functions by applying a computational observer for psychophysical discrimination to the reconstructions. Below, we provide an extended discussion of key findings, as well as of some interesting open questions and future directions.</p><p>First, we cast retinal mosaic design as a ‘likelihood design’ problem. We found that the large natural variations of L- and M-cone proportion, and the relatively stable but small S-cone proportion, can both be explained as an optimal design that minimizes the expected image reconstruction loss. This is closely related to an alternative formalism, often termed ‘efficient coding’, which seeks to maximize the amount of information transmission (<xref ref-type="bibr" rid="bib9">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib50">Karklin and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib108">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="bib90">Sims, 2018</xref>). In both cases, the optimization problem is subject to realistic biological constraints and incorporates natural scene statistics. Previous work (<xref ref-type="bibr" rid="bib38">Garrigan et al., 2010</xref>) conducted a similar analysis with consideration of natural scene statistics, physiological optics, and cone spectral sensitivity, using an information maximization criterion. One advance enabled by our work is that we are able to fully simulate a 1 deg mosaic with naturalistic input, as opposed to the information-theoretical measures used by Garrigan et al., which became intractable as the size of the mosaic and the dimensionality of the input increased. In fact, <xref ref-type="bibr" rid="bib38">Garrigan et al., 2010</xref> approximated by estimating the exact mutual information for small mosaic size (<inline-formula><mml:math id="inf33"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mn>6</mml:mn></mml:math></inline-formula> cones) and then extrapolated to larger cone mosaics using a scaling law (<xref ref-type="bibr" rid="bib12">Borghuis et al., 2008</xref>). The fact that the two theories corroborate each other well is reassuring and suggests that the results are robust to the details of the analysis.</p><p>Our approach could be applied to analyzing the retinal mosaic characteristics of different animals. Adult zebrafish, for example, feature a highly regular mosaic with fixed 2:2:1:1 R:G:B:U cone ratios (<xref ref-type="bibr" rid="bib35">Engström, 1960</xref>). Since our analysis has highlighted the importance of prior statistics in determining the optimal design, one might speculate whether this regularity results from the particular visual world of zebrafish (i.e. underwater, low signal-to-noise ratio), which perhaps demands a more balanced ratio of different cone types to achieve the maximum amount of information transmission. Further study that characterizes in detail the natural scene statistics of the zebrafish’s environment might help us to better understand this question (<xref ref-type="bibr" rid="bib116">Zimmermann et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Cai et al., 2020</xref>). It would also be interesting to incorporate into the formulation an explicit specification of how the goal of vision might vary across species. One extension to the current approach to incorporate this would be to specify an explicit loss function for each species and find the reconstruction that minimizes the expected (over the posterior of images) loss (<xref ref-type="bibr" rid="bib11">Berger, 1985</xref>), although implementing this approach would be computationally challenging. Related is the task-specific accuracy maximization analysis formulation (<xref ref-type="bibr" rid="bib22">Burge and Geisler, 2011</xref>; see <xref ref-type="bibr" rid="bib24">Burge, 2020</xref> for a review).</p><p>Second, we applied our framework to cone excitations of retinal mosaics with varying degrees of optical quality, photoreceptor size, density, and cone spectral sensitivity. The reconstructed images reflect accurately the information loss in the initial encoding, including spatial blur due to optical aberration and mosaic sampling, pixel noise due to Poisson variability in the cone excitations, and reduction of chromatic contrast in anomalous trichromacy. Although we have mainly focused on visualization of these effects in our current paper, it would be possible to perform quantitative analyses. In fact, our reconstruction algorithm could provide a natural ‘front-end’ extension to many image-based perceptual quality metrics, such as spatial CIELAB (<xref ref-type="bibr" rid="bib113">Zhang and Wandell, 1997</xref>; <xref ref-type="bibr" rid="bib60">Lian, 2020</xref>), structural similarity (<xref ref-type="bibr" rid="bib105">Wang et al., 2004</xref>), low-level feature similarity (FSIM; <xref ref-type="bibr" rid="bib114">Zhang et al., 2011</xref>), or neural network-based approaches (<xref ref-type="bibr" rid="bib13">Bosse et al., 2018</xref>). Doing so would incorporate factors related to the initial visual encoding explicitly into the resulting image quality metrics.</p><p>In addition, when SNR is high, we found that we are able to fully recover color information even from an anomalous trichromatic mosaic. As SNR drops, this becomes less feasible. Although our analysis does underestimate the amount of total noise in the visual system (i.e. we only consider noise at cone excitations, but see <xref ref-type="bibr" rid="bib1">Ala-Laurila et al., 2011</xref> for a detailed treatment of noise in the retina), this nonetheless suggests that a downstream circuit that properly compensates for the shift in cone spectral sensitivity can, in principle, maintain relatively normal color perception in the low noise regime (<xref ref-type="bibr" rid="bib99">Tregillus et al., 2021</xref>). This may potentially be related to some reports of less than expected difference in color perception between anomalous trichromats and color normal observers (<xref ref-type="bibr" rid="bib14">Bosten, 2019</xref>; <xref ref-type="bibr" rid="bib61">Lindsey et al., 2020</xref>).</p><p>Third, we speculate that image reconstruction could provide a reasonable proxy for modeling percepts in various psychophysical experiments. We found that images reconstructed from dichromatic mosaics resemble results generated by previously proposed methods for visualizing dichromacy, including one that uses explicit knowledge of dichromatic subjects’ color appearance reports (<xref ref-type="bibr" rid="bib20">Brettel et al., 1997</xref>). We have also reproduced the ‘zebra stripes’ and ‘orientation reversal’ aliasing patterns when reconstructing images from cone excitations to spatial frequencies above the mosaic Nyquist limit, similar to what has been documented experimentally in human subjects (<xref ref-type="bibr" rid="bib109">Williams, 1985</xref>; <xref ref-type="bibr" rid="bib29">Coletta and Williams, 1987</xref>). In a similar vein, previous work has used a simpler image reconstruction method to model the color appearance of small spots light stimulus presented to single cones using adaptive optics (<xref ref-type="bibr" rid="bib16">Brainard et al., 2008</xref>). Our method could also be applied to such questions, and also to a wider range of adaptive optics (AO) experiments (e.g. <xref ref-type="bibr" rid="bib85">Schmidt et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Neitz et al., 2020</xref>), to help understand the extent to which image reconstruction can capture perceptual behavior. More speculatively, it may be possible to use calculations performed within the image reconstruction framework to synthesize stimuli that will maximally discriminate between different hypothesis about how the excitations of sets of cones are combined to form percepts, particularly with the emergence of technology that enables precise experimental control over the stimulation of individual cones in human subjects (<xref ref-type="bibr" rid="bib45">Harmening et al., 2014</xref>; <xref ref-type="bibr" rid="bib83">Sabesan et al., 2016</xref>; <xref ref-type="bibr" rid="bib85">Schmidt et al., 2019</xref>).</p><p>Last, we showed that our method can be used in conjunction with analysis of psychophysical discrimination performance, bringing to this analysis the role of statistical regularities of natural images. In our initial exploration, we found that the image-reconstruction based observer exhibits significant interaction between spatial frequency and chromatic direction in its contrast sensitivity function, a behavior distinct from its Poisson ideal observer counterpart, and is more similar to the human observer. Future computations will be needed to understand in more detail whether the reconstruction approach can account for other features of human psychophysical discrimination performance that are not readily explained by ideal-observer calculations applied to the cone excitations.</p><p>Our current model only considers the representation up to and including the excitations of the cone mosaic. Post-excitation factors (e.g. retinal ganglion cells), especially in the peripheral visual field, are likely to lead to additional information loss. In this regard, we are eager to incorporate realistic models of retinal ganglion cells into the ISETBio pipeline. Nevertheless, the value of the analysis we have presented is to elucidate exactly what phenomena can or cannot be attributed to factors up to the cone excitations, thus helping to dissect the role of different stages of processing in determining behavior. For example, we found there is desaturation of chromatic content in reconstructed images in the periphery, with the details depending on interactions between the physiological optics, cone mosaic sampling, macular pigment density, and the model of natural image statistics. This is in contrast to more traditional explanations of the decrease in peripheral chromatic sensitivity, which often consider it in the context of models of how different cone types are wired to retinal ganglion cells (e.g. <xref ref-type="bibr" rid="bib57">Lennie et al., 1991</xref>; <xref ref-type="bibr" rid="bib66">Mullen and Kingdom, 1996</xref>; <xref ref-type="bibr" rid="bib44">Hansen et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Field et al., 2010</xref>; <xref ref-type="bibr" rid="bib111">Wool et al., 2018</xref>). Whether the early vision factors are sufficient to account for the full variation in chromatic sensitivity awaits a more detailed future study, but the fact that early vision factors can play a role through their effect on the available chromatic information is a novel insight that should be incorporated into thinking about the role of post-excitation mechanisms.</p><p>More generally, we can consider the locus of the signals analyzed in the context of the encoding-decoding dichotomy of sensory perception (<xref ref-type="bibr" rid="bib94">Stocker and Simoncelli, 2006</xref>; <xref ref-type="bibr" rid="bib82">Rust and Stocker, 2010</xref>). Here, we reconstruct images from cone excitations, thus post-excitation processing may be viewed as part of the brain’s implementation of the reconstruction algorithm. When we apply such an algorithm to, for example, the output of retinal ganglion cells, we shift the division. Our view is that analyses at multiple stages are of interest, and eventual comparisons between them are likely to shed light on the role of each stage.</p><p>Our current model also does not take into account fixational eye movements, which displace the retinal image at a time scale shorter than the integration period we have used here (<xref ref-type="bibr" rid="bib64">Martinez-Conde et al., 2004</xref>; <xref ref-type="bibr" rid="bib21">Burak et al., 2010</xref>). It has been shown that these small eye movements can increase psychophysically-measured visual acuity relative to that obtained with retinally-stabilized stimuli (<xref ref-type="bibr" rid="bib79">Rucci et al., 2007</xref>; <xref ref-type="bibr" rid="bib77">Ratnam et al., 2017</xref>). An intuition behind this is that fixational eye movements can increase the effective cone sampling density, if the visual system can sensibly combine information obtained across multiple fixation locations. This intuition is supported by computational analyses that integrate information across fixations while simultaneously estimating the eye movement path (<xref ref-type="bibr" rid="bib21">Burak et al., 2010</xref>; <xref ref-type="bibr" rid="bib3">Anderson et al., 2020</xref>). In their analysis, <xref ref-type="bibr" rid="bib21">Burak et al., 2010</xref> showed the effectiveness of their algorithm depended both on the integration time of the sensory units whose excitations were processed, and also on the receptive field properties of those units. In addition, consideration of the effects of fixational eye movement might also benefit from an accurate model of the temporal integration that occurs within each cone, as a consequence of the temporal dynamics of the phototransduction cascade (<xref ref-type="bibr" rid="bib4">Angueyra and Rieke, 2013</xref>). ISETBio in its current form implements a model of the phototransduction cascade as well as of fixational eye movements (see <xref ref-type="bibr" rid="bib31">Cottaris et al., 2020</xref>). Future work should be able to extend our current results through the study of dynamic reconstruction algorithms within ISETBio.</p><p>Since our framework is centered on image reconstruction, one may naturally wonder whether we should have applied the more ‘modern’ technique of convolutional neural networks (CNNs), which have become the standard for image processing-related tasks (<xref ref-type="bibr" rid="bib53">Krizhevsky et al., 2012</xref>). For our scientific purposes, the Bayesian framework offers an important advantage in its <italic>modularity</italic>, namely, the likelihood and prior are two separate components that can be built independently. This allows us to easily isolate and manipulate one of them (e.g. likelihood) while holding the other constant (e.g. prior), something we have done throughout this paper. In addition, building the likelihood function (i.e. render matrix <inline-formula><mml:math id="inf34"><mml:mi>R</mml:mi></mml:math></inline-formula>, see Materials and methods) is a forward process that is computationally very efficient. Performing a similar analysis with the neural network approach (or supervised learning in general) would require re-training of the network with a newly generated dataset (i.e. cone excitations paired with the corresponding images) for <italic>every</italic> condition in our analysis.</p><p>However, the ability of neural networks to represent more complex natural image priors (<xref ref-type="bibr" rid="bib100">Ulyanov et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Kadkhodaie and Simoncelli, 2021</xref>) is of great interest. Currently, we have chosen a rather simple, parametric description of natural image statistics, which leads to a numerical MAP solution. Previous work has proposed methods that alternate, within each iteration, between regularized reconstruction and denoising, which effectively allow for transfer of the prior implicit in an image denoiser (e.g. a deep neural network denoiser) to be applied to any other domain with a known likelihood model (<xref ref-type="bibr" rid="bib102">Venkatakrishnan et al., 2013</xref>; <xref ref-type="bibr" rid="bib78">Romano et al., 2017</xref>). More recently, <xref ref-type="bibr" rid="bib49">Kadkhodaie and Simoncelli, 2021</xref> developed a related but more explicit and direct technique to extract the image prior (a close approximation to the gradient of the log-prior density, to be precise) from a denoising deep neural network, which could be applied to our image reconstruction problem. We think this represents a promising direction, and in the future plan to incorporate more sophisticated priors, to evaluate the robustness of our conclusions to variations and improvements in the image prior.</p><p>To conclude, we believe our method is widely applicable to many experiments (e.g. adaptive optics psychophysics) designed for studying the initial visual encoding, for modeling the effect of changes of various components in the encoding process (e.g. in clinical conditions), and for practical applications (e.g. perceptual quality metric) in which the initial visual encoding plays an important role.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>The problem of reconstructing images from neural signals can be considered in the general framework of estimating a signal <inline-formula><mml:math id="inf35"><mml:mi>x</mml:mi></mml:math></inline-formula>, given an (often lower-dimensional and noisy) measurement <inline-formula><mml:math id="inf36"><mml:mi>m</mml:mi></mml:math></inline-formula>. We take a Bayesian approach. Specifically, we model the generative process of measurement as the conditional probability <inline-formula><mml:math id="inf37"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and the prior distribution of the signal as the probability density <inline-formula><mml:math id="inf38"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. We then take the estimate of the signal, <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, as the maximum a posteriori estimate <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We next explain in detail how each part of the Bayesian estimate is constructed.</p><sec id="s4-1"><title>Likelihood function</title><p>In our particular problem, <inline-formula><mml:math id="inf41"><mml:mi>x</mml:mi></mml:math></inline-formula> is a column vector containing the (vectorized) RGB pixel values of an input image of dimension <inline-formula><mml:math id="inf42"><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mn>3</mml:mn></mml:math></inline-formula>, where <inline-formula><mml:math id="inf43"><mml:mi>N</mml:mi></mml:math></inline-formula> is the linear pixel size of the display. Below we will generalize from RGB images to hyperspectral images. The column vector <inline-formula><mml:math id="inf44"><mml:mi>m</mml:mi></mml:math></inline-formula> contains the excitations of the <inline-formula><mml:math id="inf45"><mml:mi>M</mml:mi></mml:math></inline-formula> cone photoreceptors. The relationship between <inline-formula><mml:math id="inf46"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:mi>m</mml:mi></mml:math></inline-formula> is modeled by the ISETBio software (<xref ref-type="bibr" rid="bib30">Cottaris et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Cottaris et al., 2020</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>). ISETBio simulates in detail the process of displaying an image on the monitor, the wavelength-dependent optical blur of the human eye and spectral transmission through the lens and the macular pigment, as well as the interleaved sampling of the retinal image by the L, M and S cone mosaic. For the majority of simulations presented in our paper, we simulate a 1 deg foveal retina mosaic, which contains approximately 11,000 cone photoreceptors. A stochastic procedure was used to generate approximately hexagonal mosaics with eccentricity-varying cone density matched to that of the human retina (<xref ref-type="bibr" rid="bib32">Curcio et al., 1990</xref>). See <xref ref-type="bibr" rid="bib30">Cottaris et al., 2019</xref> for a detailed description of the algorithm. We use a wavelength-dependent point spread function empirically measured in human subjects (<xref ref-type="bibr" rid="bib63">Marimont and Wandell, 1994</xref>; <xref ref-type="bibr" rid="bib30">Cottaris et al., 2019</xref>), with a pupil size of 3 mm. We took the cone integration time to be 50 ms. The input images of size <inline-formula><mml:math id="inf48"><mml:mn>128</mml:mn><mml:mi>*</mml:mi><mml:mn>128</mml:mn><mml:mi>*</mml:mi><mml:mn>3</mml:mn></mml:math></inline-formula> were displayed on a simulated typical CRT monitor (simulated with a 12 bit-depth in each of the RGB channels to avoid quantization artifacts).</p><p>Once the RGB pixel values in the original image are linearized, all the processes involved in the relation between <inline-formula><mml:math id="inf49"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:mi>m</mml:mi></mml:math></inline-formula>, including image formation by the optics of the eye and the relation between retinal irradiance and cone excitations, are well described as linear operations. Furthermore, the instance-to-instance variability in cone excitations is described by a Poisson process acting independently in each cone. Thus <inline-formula><mml:math id="inf51"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the product of Poisson probability mass functions, one for each cone, with the Poisson mean parameter <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each cone determined by a linear transformation of the input image <inline-formula><mml:math id="inf53"><mml:mi>x</mml:mi></mml:math></inline-formula>. We describe the linear transformation between <inline-formula><mml:math id="inf54"><mml:mi>x</mml:mi></mml:math></inline-formula> and the vector of Poisson mean parameters <inline-formula><mml:math id="inf55"><mml:mi>λ</mml:mi></mml:math></inline-formula> by a matrix <inline-formula><mml:math id="inf56"><mml:mi>R</mml:mi></mml:math></inline-formula>, and thus obtain:<disp-formula id="equ2">.<mml:math id="m2"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>|</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>R</mml:mi><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>We refer to the matrix <inline-formula><mml:math id="inf57"><mml:mi>R</mml:mi></mml:math></inline-formula> as the <italic>render matrix</italic>. This matrix together with the Poisson variability encapsulates the properties of the initial visual encoding through to the level of the cone excitations. In cases where we parameterize properties of the initial visual encoding (parameters denoted by <inline-formula><mml:math id="inf58"><mml:mi mathvariant="normal">θ</mml:mi></mml:math></inline-formula> in the main text above), the render matrix is a function of these parameters.</p><p>Although ISETBio can compute the relation between the linearized RGB image values at each pixel and the mean excitation of each cone, it does so in a general way that does not exploit the linearity of the relation. To speed the computations, we use ISETBio to precompute <inline-formula><mml:math id="inf59"><mml:mi>R</mml:mi></mml:math></inline-formula>. Each column of <inline-formula><mml:math id="inf60"><mml:mi>R</mml:mi></mml:math></inline-formula> is a vector of mean cone excitations <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to a basis image <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with one entry set to one and the remaining entry set to zero. To determine <inline-formula><mml:math id="inf63"><mml:mi>R</mml:mi></mml:math></inline-formula>, we use ISETBio to compute explicitly each of its columns <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . We verified that calculating mean cone excitations from an image via <inline-formula><mml:math id="inf65"><mml:mi>R</mml:mi><mml:mi>x</mml:mi></mml:math></inline-formula> yields the same result as applying the ISETBio pipeline directly to the image.</p><p>See Code and data availability for parameters used in the simulation including display specifications (i.e. RGB channel spectra, display gamma function) and cone mosaic setup (i.e. cone spectral sensitivities, lens pigment and macular pigment density and absorption spectra), as well as some of the pre-computed render matrices.</p></sec><sec id="s4-2"><title>Null space of render matrix</title><p>To understand the information lost between an original RGB image and the mean cone excitations, we can take advantage of the linearity property of the render matrix. Variations in the image space that are within the null space of the (low-rank) render matrix <inline-formula><mml:math id="inf66"><mml:mi>R</mml:mi></mml:math></inline-formula> will have no effect on the likelihood. That is, the cone excitation pattern provides no information to disambiguate between image variants that differ only by vectors within the null space of <inline-formula><mml:math id="inf67"><mml:mi>R</mml:mi></mml:math></inline-formula>. To obtain the null space of <inline-formula><mml:math id="inf68"><mml:mi>R</mml:mi></mml:math></inline-formula>, we used MATLAB function <italic>null</italic>, which computes the singular value decomposition of <inline-formula><mml:math id="inf69"><mml:mi>R</mml:mi></mml:math></inline-formula>. The set of right singular vectors whose associated singular values are 0 form a basis for the null space.</p><p>As an illustration, we generated random samples of images from the null space by taking linear combinations of its orthonormal basis vectors, where the weights are sampled independently from a Gaussian distribution with a mean of 0 and a standard deviation of 0.3. As shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, altering an image by adding to it samples from the null space has no effect on the likelihood.</p></sec><sec id="s4-3"><title>Prior distribution</title><p>We also need to specify a prior distribution <inline-formula><mml:math id="inf70"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The problem of developing statistical models of natural images has been studied extensively using numerous approaches, and remains challenging (<xref ref-type="bibr" rid="bib89">Simoncelli, 2005</xref>). The high-dimensionality and complex structure of natural images makes it difficult to determine a high-dimensional joint distribution that properly captures the various forms of correlation and higher-order dependencies of natural images. Here, we have implemented two relatively simple forms of <inline-formula><mml:math id="inf71"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>We first introduce a simple Gaussian prior <inline-formula><mml:math id="inf72"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> to set up the basic concepts and notations for image prior based on basis vectors. In particular, for the Gaussian prior, we assume <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For convenience, we zero-centered our images when building priors, making <inline-formula><mml:math id="inf74"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. The actual mean value of each pixel is added back to each image when computing the likelihood and at the end of the reconstruction procedure. The covariance matrix <inline-formula><mml:math id="inf75"><mml:mi>Σ</mml:mi></mml:math></inline-formula> can be estimated empirically, from a large dataset of natural images. Note that we can write the covariance matrix as its eigen-decomposition: <inline-formula><mml:math id="inf76"><mml:mi>Σ</mml:mi><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mi>Λ</mml:mi><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> . Defining <inline-formula><mml:math id="inf77"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>x</mml:mi></mml:math></inline-formula>, we have:<disp-formula id="equ3">.<mml:math id="m3"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This derivation provides a convenient way of expressing our image prior: We can project images onto an appropriate set of basis vectors, and impose a prior distribution on the projected coefficients. In the case above, if we choose the basis vectors as the column vectors of <inline-formula><mml:math id="inf78"><mml:msup><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> , we obtain an image prior by assuming that the entries of <inline-formula><mml:math id="inf79"><mml:mi>β</mml:mi></mml:math></inline-formula> are each independently distributed as a univariant standard Gaussian (<xref ref-type="bibr" rid="bib89">Simoncelli, 2005</xref>). Such a Gaussian prior can describe the first and second order statistics of natural images, but fails to capture important higher order structure (<xref ref-type="bibr" rid="bib75">Portilla et al., 2003</xref>).</p><p>Our second model of <inline-formula><mml:math id="inf80"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> emerges from the basis set formulation. Rather than choosing the basis vectors from the eigen-decomposition as above and using a Gaussian distribution over the weights <inline-formula><mml:math id="inf81"><mml:mi>β</mml:mi></mml:math></inline-formula>, we instead choose an over-complete set of basis vectors using independent components analysis, and model the distribution of the entries of weight vector <inline-formula><mml:math id="inf82"><mml:mi>β</mml:mi></mml:math></inline-formula> using the long-tailed distribution Laplace distribution. This leads to a sparse coding model of natural images (<xref ref-type="bibr" rid="bib71">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib88">Simoncelli and Olshausen, 2001</xref>). More specifically, we learned a set of <inline-formula><mml:math id="inf83"><mml:mi>K</mml:mi><mml:mi> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>K</mml:mi><mml:mo>≥</mml:mo><mml:mn>3</mml:mn><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> basis vectors that lead to a sparse representation of our image dataset, through the reconstruction independent component analysis (RICA) algorithm (<xref ref-type="bibr" rid="bib56">Le et al., 2011</xref>) applied to whitened images, and took these as the columns of the basis matrix <inline-formula><mml:math id="inf84"><mml:mi>E</mml:mi></mml:math></inline-formula>. Our image prior in this case can be written as <inline-formula><mml:math id="inf85"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> , with <inline-formula><mml:math id="inf86"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mi>x</mml:mi></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf87"><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> represents the pseudoinverse of matrix <inline-formula><mml:math id="inf88"><mml:mi>E</mml:mi></mml:math></inline-formula>, and<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>b</mml:mi></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="italic">e</mml:mi><mml:mi mathvariant="italic">x</mml:mi><mml:mi mathvariant="italic">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that we further scaled each column of <inline-formula><mml:math id="inf89"><mml:mi>E</mml:mi></mml:math></inline-formula> to equalize the variance across <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ’s.</p><p>Both methods outlined above can be applied directly to small image patches. They are computationally intractable for larger images, however, since the calculation of basis vectors will involve either an eigen-decomposition of a large covariance matrix or independent component analysis of a set of high-dimensional image vectors. To address this limitation, we iteratively apply the prior distributions we have constructed above to overlapping small patches of the same size within a large image (<xref ref-type="bibr" rid="bib43">Guleryuz, 2006</xref>).</p><p>To illustrate the idea, consider the following example: Assume we have constructed a prior distribution <inline-formula><mml:math id="inf91"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></inline-formula> for small image patches <inline-formula><mml:math id="inf92"><mml:mi>y</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></inline-formula> To model a larger image <inline-formula><mml:math id="inf94"><mml:mi>x</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf95"><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></inline-formula> we could consider viewing <inline-formula><mml:math id="inf96"><mml:mi>x</mml:mi></mml:math></inline-formula> as composed of <inline-formula><mml:math id="inf97"><mml:mi>p</mml:mi><mml:mi>*</mml:mi><mml:mi>p</mml:mi></mml:math></inline-formula> independent patches of non-overlapping <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> ’s. Under this assumption, the prior on <inline-formula><mml:math id="inf99"><mml:mi>x</mml:mi></mml:math></inline-formula> could be expressed as the product:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>∗</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> ’s describe individual patches of size <inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> within <inline-formula><mml:math id="inf102"><mml:mi>x</mml:mi></mml:math></inline-formula>. The independence assumption is problematic, however, since <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> ’s are far from independently sampled natural images: they need to be combined into a single coherent large image. Using this approach to approximate a prior would create block artifacts at the patch boundaries.</p><p>The basic idea above, however, can be extended heuristically to solve the block artifact problem by allowing <inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ’s to overlap with each other. The degree of overlap can be viewed as an additional parameter of the prior, which we refer to here as the stride. This effectively implements a convolutional form of the sparse coding prior (<xref ref-type="bibr" rid="bib42">Gu et al., 2015</xref>). Again, for example, consider a large image <inline-formula><mml:math id="inf105"><mml:mi>x</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf106"><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></inline-formula> A stride of 1 will tile through all <inline-formula><mml:math id="inf107"><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mi>*</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> possible patches of size <inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> within <inline-formula><mml:math id="inf109"><mml:mi>x</mml:mi></mml:math></inline-formula>, yielding a prior distribution of the form:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Although this form of prior is still an approximation, we have found it to work well in practice, and using it does not lead to visible block artifacts as long as the stride parameter is sufficiently smaller than <inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> .</p></sec><sec id="s4-4"><title>Maximum a posteriori estimation</title><p>To reconstruct the image <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> given a pattern of cone excitation <inline-formula><mml:math id="inf112"><mml:mi>m</mml:mi></mml:math></inline-formula>, we find the maximum a posteriori estimate: <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> . In practice, this optimization is usually expressed in terms of its logarithmic counterpart: <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> .</p><p>For the Poisson likelihood and sparse coding prior, the equation above becomes:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo>∗</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> ’s are individual patches of size <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> within <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each <inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is of length <inline-formula><mml:math id="inf121"><mml:mi>K</mml:mi></mml:math></inline-formula> and there are a total of <inline-formula><mml:math id="inf122"><mml:mi>J</mml:mi></mml:math></inline-formula> (overlapping) patches. Lastly, <inline-formula><mml:math id="inf123"><mml:mi>c</mml:mi></mml:math></inline-formula> is a constant that does not depend on <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In principle, the value of <inline-formula><mml:math id="inf125"><mml:mi>γ</mml:mi></mml:math></inline-formula> can be analytically derived based on the parametric form of the prior. However, due to the approximate nature of our prior, introduced especially by the aggregation over patches, we left <inline-formula><mml:math id="inf126"><mml:mi>γ</mml:mi></mml:math></inline-formula> as a free parameter. Treating <inline-formula><mml:math id="inf127"><mml:mi>γ</mml:mi></mml:math></inline-formula> as a free parameter also provides some level of robustness against misspecification of the prior more generally. For most of the reconstruction results presented in this paper, the value of <inline-formula><mml:math id="inf128"><mml:mi>γ</mml:mi></mml:math></inline-formula> was determined by maximizing reconstruction performance with a cross-validation procedure (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). We also found that the optimal <inline-formula><mml:math id="inf129"><mml:mi>γ</mml:mi></mml:math></inline-formula> values were similar across the two loss functions we considered. Note that the additional flexibility provided by this <inline-formula><mml:math id="inf130"><mml:mi>γ</mml:mi></mml:math></inline-formula> parameter also provides us with a parametric way to manipulate and isolate the relative contribution of the log-likelihood and log-prior terms to the reconstruction (e.g. <xref ref-type="fig" rid="fig2">Figure 2</xref>; also compare <xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>The optimization problem required to obtain <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> can be solved efficiently using the MATLAB function <italic>fmincon</italic> by providing the analytical gradient to the minimization function:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="italic">l</mml:mi><mml:mi mathvariant="italic">o</mml:mi><mml:mi mathvariant="italic">g</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>∘</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="italic">l</mml:mi><mml:mi mathvariant="italic">o</mml:mi><mml:mi mathvariant="italic">g</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf132"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes element-wise product between two vectors, <inline-formula><mml:math id="inf134"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">λ</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> is the element-wise inverse of vector <inline-formula><mml:math id="inf135"><mml:mi mathvariant="normal">λ</mml:mi></mml:math></inline-formula>, and<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5"><title>RGB image dataset</title><p>We used the ImageNet ILSVRC (<xref ref-type="bibr" rid="bib81">Russakovsky et al., 2015</xref>) as our dataset for natural RGB images. Fifty randomly sampled images were reserved as the evaluation set, and the rest of the images were used for learning the prior and for cross-validation. For the sparse prior, we constructed a basis set size of <inline-formula><mml:math id="inf136"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:math></inline-formula>, on image patches of size <inline-formula><mml:math id="inf137"><mml:mn>16</mml:mn><mml:mi>*</mml:mi><mml:mn>16</mml:mn></mml:math></inline-formula> sampled from the training set, and used a stride of 4 when tiling larger images. We randomly sampled 20 patches from each one of the 5000 images in the training set for learning the prior (ICA analysis), and 500 images for the cross-validation procedure to determine the <inline-formula><mml:math id="inf138"><mml:mi>γ</mml:mi></mml:math></inline-formula> parameter.</p><p>In our work, we simulate display of the RGB images on idealized monitor to generate spectral radiance as a linear combination of the monitor’s RGB channel spectra. Thus, a prior over the linear RGB pixels values induces a full spatial-spectral prior. To make sure the constraints introduced by RGB images together with the monitor do not influence our results, we also conducted a control analysis using hyperspectral images directly, as described in the following section.</p></sec><sec id="s4-6"><title>Hyperspectral images</title><p>As a control analysis, we developed priors and reconstructed images directly on small patches of hyperspectral images. The development is essentially the same as above, with the generalization being to increase the number of channels in the images from 3 to <inline-formula><mml:math id="inf139"><mml:mi>N</mml:mi></mml:math></inline-formula>. In addition, since our algorithm treats images as high-dimensional vectors, it can be directly applied to reconstruct hyperspectral images. Here, we used images from <xref ref-type="bibr" rid="bib67">Nascimento et al., 2002</xref> and <xref ref-type="bibr" rid="bib26">Chakrabarti and Zickler, 2011</xref>. The dataset of <xref ref-type="bibr" rid="bib67">Nascimento et al., 2002</xref> was pre-processed following the instructions provided by the authors, and the images of <xref ref-type="bibr" rid="bib26">Chakrabarti and Zickler, 2011</xref> were converted to spectral radiance using the hyperspectral camera calibration data provided in that work. We further resampled the combined image dataset with a patch size of <inline-formula><mml:math id="inf140"><mml:mn>18</mml:mn><mml:mi>*</mml:mi><mml:mn>18</mml:mn></mml:math></inline-formula> and 15 uniformly spaced wavelengths between 420 nm and 700 nm for a dataset of ∼5000 patches. We retained 300 of them as the evaluation set, and the rest for prior learning and cross-validation. The remaining of the analysis (i.e. prior and reconstruction algorithm) followed the same procedures as those used for the RGB images, using number of basis functions <inline-formula><mml:math id="inf141"><mml:mi>K</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mn>4860</mml:mn></mml:math></inline-formula> and applied directly to each small image without the patchwise procedure.</p><p>See Code and data availability for the curated RGB and hyperspectral image dataset, as well learned basis functions for each sparse prior.</p></sec><sec id="s4-7"><title>Gaussian prior for synthetic images</title><p>We also reconstructed multivariate Gaussian distributed synthetic images with known chromatic and spatial correlations that we can explicitly manipulate (<xref ref-type="fig" rid="fig5">Figure 5</xref>). To construct these signals <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , where <inline-formula><mml:math id="inf143"><mml:mi>x</mml:mi></mml:math></inline-formula> is RGB image of size <inline-formula><mml:math id="inf144"><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mn>3</mml:mn></mml:math></inline-formula> (<inline-formula><mml:math id="inf145"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>36</mml:mn></mml:math></inline-formula> in our current analysis), we set <inline-formula><mml:math id="inf146"><mml:mi mathvariant="normal">μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, and used a separable <inline-formula><mml:math id="inf147"><mml:mi>Σ</mml:mi></mml:math></inline-formula> along its two spatial dimensions and one chromatic dimension. That is:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the chromatic covariance matrix of size <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>∗</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the spatial covariance matrix of size <inline-formula><mml:math id="inf151"><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In the covariance matrix constructions, <inline-formula><mml:math id="inf152"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:math></inline-formula> index into entries of <inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mtext>-th</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> row and <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mtext>-th</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> column. Here <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⊗</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the Kronecker product, thus producing the signal covariance matrix <inline-formula><mml:math id="inf158"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:mn>3</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib16">Brainard et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Manning and Brainard, 2009</xref>).</p><p>The parameters <inline-formula><mml:math id="inf160"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> determine the overall variance of the signal, which are fixed across all simulations, whereas by changing the value of <inline-formula><mml:math id="inf162"><mml:msub><mml:mrow><mml:mi mathvariant="normal">ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:msub><mml:mrow><mml:mi mathvariant="normal">ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , we manipulate the degree of spatial and chromatic correlation presented in the synthetic images (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>We introduce an additional simplification for the case of reconstructions with respect to the synthetic Gaussian prior: We approximated the Poisson likelihood function with a Gaussian distribution with fixed variance. Thus, the reconstruction problem can be written as:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>R</mml:mi><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf164"><mml:mi>R</mml:mi></mml:math></inline-formula> is the render matrix, and <inline-formula><mml:math id="inf165"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mi mathvariant="normal">Λ</mml:mi><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> .</p><p>The reconstruction problem with Gaussian prior and Gaussian noise matches the ridge regression formulation, and can be solved analytically by the regularized normal equations, applied directly to each small image without the patchwise procedure. Denote the design matrix <inline-formula><mml:math id="inf166"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> :<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>m</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note that the <inline-formula><mml:math id="inf167"><mml:mi>γ</mml:mi></mml:math></inline-formula> parameter here is also determined through a cross-validation routine. We adopted this simplification (using Gaussian noise) for the simulation results in <xref ref-type="fig" rid="fig5">Figure 5</xref>, in order to make it computationally feasible to evaluate the average reconstruction error across a large number of synthetic image datasets.</p></sec><sec id="s4-8"><title>Variations in retinal cone mosaic</title><p>To simulate a dichromatic observer, we constructed retinal mosaics with only two classes of cones but with similar spatial configuration. To simulate the deuteranomalous observer, we shifted the M cone spectral sensitivity function, setting its peak at 550 nm instead of the typical 530 nm. In both cases, the likelihood function (i.e. render matrix <inline-formula><mml:math id="inf168"><mml:mi>R</mml:mi></mml:math></inline-formula>) was computed using the procedure described above and the same Bayesian algorithm was applied to obtain the reconstructed images.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref>, we also present the results of two comparison methods for visualizing dichromacy, those of <xref ref-type="bibr" rid="bib20">Brettel et al., 1997</xref> and <xref ref-type="bibr" rid="bib47">Jiang et al., 2016</xref>, both are implemented as part of ISETBio routine. To determine the corresponding dichromatic images, we first computed the LMS trichromatic stimulus coordinates of the linear RGB value of each pixel of the input image, based on the parameters of the simulated CRT display. LMS coordinates were computed with respect to the Stockman-Sharpe 2 deg cone fundamentals (<xref ref-type="bibr" rid="bib95">Stockman and Sharpe, 2000</xref>). The ISETBio function <italic>lms2lmsDichromat</italic> was then used to transform these LMS coordinates according to the two methods (see a brief description in the main text). Lastly, the transformed LMS coordinates were converted back to linear RGB values, and gamma corrected before rendering.</p><p>To simulate retinal mosaics at different eccentricities, we constructed retinal mosaics with the appropriate photoreceptor size, density (<xref ref-type="bibr" rid="bib32">Curcio et al., 1990</xref>), and physiological optics (<xref ref-type="bibr" rid="bib74">Polans et al., 2015</xref>), and computed their corresponding render matrices. The same Bayesian algorithm was applied to obtain the reconstructed images.</p><p>To simulate the interferometric experimental conditions of <xref ref-type="bibr" rid="bib109">Williams, 1985</xref>, we used diffraction-limited optics without longitudinal chromatic aberration (LCA) for the computation of the cone excitations, but used the likelihood function with normal optics for the reconstruction. This models subjects whose perceptual systems are matched to their normal optics and assumes there is no substantial adaptation within the short time span of the experiment.</p></sec><sec id="s4-9"><title>Contrast sensitivity function</title><p>We compared the spatial Contrast Sensitivity Function (CSF) between a standard, Poisson 2AFC ideal observer, and an image reconstruction-based observer.</p><p>We simulated stimulus modulations in two chromatic contrast directions, L + M and L - M. Contrast was measured as the vector length in the L and M cone contrast plane at 5 spatial frequencies, <inline-formula><mml:math id="inf169"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>32</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> cycles per degree. For each chromatic direction and spatial frequency combination, the sensitivity is defined as the inverse of threshold contrast.</p><p>We used the QUEST+ procedure (<xref ref-type="bibr" rid="bib106">Watson, 2017</xref>) as implemented in MATLAB by Brainard (mQUESTPlus; <ext-link ext-link-type="uri" xlink:href="https://github.com/BrainardLab/mQUESTPlus">https://github.com/BrainardLab/mQUESTPlus</ext-link>; <xref ref-type="bibr" rid="bib19">Brainard, 2022</xref>) for estimating the simulated threshold efficiently as follows: We initialized the procedure with the contrast near the middle of a pre-defined possible stimulus range. For each contrast, we first generated a null template <inline-formula><mml:math id="inf170"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> , which is the noise-free, average excitations of a 0.5 deg foveal mosaic with <inline-formula><mml:math id="inf171"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>cones</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> cones to a uniform background stimulus; and a target template <inline-formula><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> , which is the noise-free, average cone excitations to a grating stimulus at that contrast level. We then simulated 128 two alternative forced choice (TAFC) trials at this contrast. For each trial, two Poisson-noise corrupted observed sets of cone excitations <inline-formula><mml:math id="inf173"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf174"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> , are generated based on <inline-formula><mml:math id="inf175"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> , respectively. We determine the accuracy of for TAFC trials with the target in the first interval. Based on the observer responses, the QUEST+ procedure chooses the next test contrast according to an information-maximization criterion (<xref ref-type="bibr" rid="bib106">Watson, 2017</xref>). The process is repeated 15 times, for a total of 15 * 128 = 1920 trials.</p><p>For the Poisson TAFC observer, we directly compute the likelihood ratio for the two possible orderings of the null and target stimulus:<disp-formula id="equ18">.<mml:math id="m18"><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi> </mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi> </mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Taking the logarithm of the equation above, the decision rule simplifies to the following:<disp-formula id="equ19"><mml:math id="m19"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>cones</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where ο denotes element-wise product between two vectors. The simulated observer correctly chooses target in first interval when <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and incorrectly test in second when <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Because of symmetry, we only need to simulated one of the two TAFC orders.</p><p>For the image reconstruction-based observer, given the cone responses, it first applies the reconstruction algorithm to obtain the image template <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from <inline-formula><mml:math id="inf181"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> , and also noisy image instances <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by applying the same algorithm to <inline-formula><mml:math id="inf185"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> . We then perform a template-matching decision rule as follows:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the <inline-formula><mml:math id="inf188"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> norm of a vector. The template observer correctly chooses target in first interval when <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and incorrectly target in second interval when <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We choose the template matching procedure for computational convenience. Note that because the variability in the reconstructed images is not independent across pixels, this procedure is not ideal.</p></sec><sec id="s4-10"><title>Code and data availability</title><p>The MATLAB code used for this paper is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/isetbio/ISETImagePipeline">https://github.com/isetbio/ISETImagePipeline</ext-link>, (copy archieved at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e26fdcfd81b4c40051c6e5694151353d2af45c65;origin=https://github.com/isetbio/ISETImagePipeline;visit=swh:1:snp:244afc95dd1bfbfeb729d9e21ee2002d3bba8d96;anchor=swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427">swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427</ext-link>, <xref ref-type="bibr" rid="bib115">Zhang, 2022</xref>).</p><p>In addition, the curated RGB and hyperspectral image datasets, parameters used in the simulation including display and cone mosaic setup, as well as the intermediate results such as the learned sparse priors, likelihood functions (i.e. render matrices), are available through: <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/26r92c8y">https://tinyurl.com/26r92c8y</ext-link>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>Funding provided by Facebook Reality Labs</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Methodology, Software, Validation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Validation, Writing – review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-71132-transrepform1-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The MATLAB code used for this paper is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/isetbio/ISETImagePipeline">https://github.com/isetbio/ISETImagePipeline</ext-link>, (copy archieved at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e26fdcfd81b4c40051c6e5694151353d2af45c65;origin=https://github.com/isetbio/ISETImagePipeline;visit=swh:1:snp:244afc95dd1bfbfeb729d9e21ee2002d3bba8d96;anchor=swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427">swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427</ext-link>). In addition, the curated RGB and hyperspectral image datasets, parameters used in the simulation including display and cone mosaic setup, as well as the intermediate results such as the learned sparse priors, likelihood functions (i.e., render matrices), are available through: <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/26r92c8y">https://tinyurl.com/26r92c8y</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Chakrabarti</surname><given-names>A</given-names></name><name><surname>Zickler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><data-title>Real-World Hyperspectral Images Database</data-title><source>Harvard School of Engineering and Applied Sciences</source><pub-id pub-id-type="accession" xlink:href="http://vision.seas.harvard.edu/hyperspec/download.html">hyperspectral-realworld</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ala-Laurila</surname><given-names>P</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cone photoreceptor contributions to noise and correlations in the retinal output</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1309</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1038/nn.2927</pub-id><pub-id pub-id-type="pmid">21926983</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>SJ</given-names></name><name><surname>Mullen</surname><given-names>KT</given-names></name><name><surname>Hess</surname><given-names>RF</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Human peripheral spatial resolution for achromatic and chromatic stimuli: limits imposed by optical and retinal factors</article-title><source>The Journal of Physiology</source><volume>442</volume><fpage>47</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1991.sp018781</pub-id><pub-id pub-id-type="pmid">1798037</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>AG</given-names></name><name><surname>Ratnam</surname><given-names>K</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>High-acuity vision from retinal image motion</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.7.34</pub-id><pub-id pub-id-type="pmid">32735342</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angueyra</surname><given-names>JM</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Origin and effect of phototransduction noise in primate cone photoreceptors</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1692</fpage><lpage>1700</lpage><pub-id pub-id-type="doi">10.1038/nn.3534</pub-id><pub-id pub-id-type="pmid">24097042</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Redlich</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Understanding Retinal Color Coding from First Principles</article-title><source>Neural Computation</source><volume>4</volume><fpage>559</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.4.559</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Could information theory provide an ecological theory of sensory processing?</article-title><source>Network</source><volume>22</volume><fpage>4</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.3109/0954898X.2011.638888</pub-id><pub-id pub-id-type="pmid">22149669</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>Fred</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><volume>61</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1037/h0054663</pub-id><pub-id pub-id-type="pmid">13167245</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Bennett</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The physical limits of grating visibility</article-title><source>Vision Research</source><volume>27</volume><fpage>1915</fpage><lpage>1924</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(87)90057-5</pub-id><pub-id pub-id-type="pmid">3447346</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible Principles Underlying the Transformation of Sensory Messages</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group><source>Sensory Communication</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>1</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.001.0001</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name><name><surname>Földiàgk</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>Adaptation and Decorrelation in the Cortex</source><publisher-name>University of Cambridge Press</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>JO</given-names></name></person-group><year iso-8601-date="1985">1985</year><source>Statistical Decision Theory and Bayesian Analysis</source><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4757-4286-2</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghuis</surname><given-names>BG</given-names></name><name><surname>Ratliff</surname><given-names>CP</given-names></name><name><surname>Smith</surname><given-names>RG</given-names></name><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Design of a neuronal array</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>3178</fpage><lpage>3189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5259-07.2008</pub-id><pub-id pub-id-type="pmid">18354021</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosse</surname><given-names>S</given-names></name><name><surname>Maniry</surname><given-names>D</given-names></name><name><surname>Muller</surname><given-names>K-R</given-names></name><name><surname>Wiegand</surname><given-names>T</given-names></name><name><surname>Samek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment</article-title><source>IEEE Transactions on Image Processing</source><volume>27</volume><fpage>206</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1109/TIP.2017.2760518</pub-id><pub-id pub-id-type="pmid">29028191</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosten</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The known unknowns of anomalous trichromacy</article-title><source>Current Opinion in Behavioral Sciences</source><volume>30</volume><fpage>228</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.10.015</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bracewell</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>The Fourier Transform and Its Applications</source><publisher-loc>McGraw-Hill New York</publisher-loc><publisher-name>Stanford University</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Williams</surname><given-names>DR</given-names></name><name><surname>Hofer</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Trichromatic reconstruction from the interleaved cone mosaic: Bayesian model and the color appearance of small spots</article-title><source>Journal of Vision</source><volume>8</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1167/8.5.15</pub-id><pub-id pub-id-type="pmid">18842086</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Color and the Cone Mosaic</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>519</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035341</pub-id><pub-id pub-id-type="pmid">28532367</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Color, Pattern, and the Retinal Cone Mosaic</article-title><source>Current Opinion in Behavioral Sciences</source><volume>30</volume><fpage>41</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.05.005</pub-id><pub-id pub-id-type="pmid">34109261</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>BrainardLab/mQUESTPlus</data-title><version designator="swh:1:rev:2482d0481732f16a32e0c42a6f130116e7d9ee63">swh:1:rev:2482d0481732f16a32e0c42a6f130116e7d9ee63</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:cfcdeaff04c5caaa1639345e22b5b7594309621f;origin=https://github.com/BrainardLab/mQUESTPlus;visit=swh:1:snp:bb59c15efbe01a9284730be46eaa46994e02b0cb;anchor=swh:1:rev:2482d0481732f16a32e0c42a6f130116e7d9ee63">https://archive.softwareheritage.org/swh:1:dir:cfcdeaff04c5caaa1639345e22b5b7594309621f;origin=https://github.com/BrainardLab/mQUESTPlus;visit=swh:1:snp:bb59c15efbe01a9284730be46eaa46994e02b0cb;anchor=swh:1:rev:2482d0481732f16a32e0c42a6f130116e7d9ee63</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brettel</surname><given-names>H</given-names></name><name><surname>Viénot</surname><given-names>F</given-names></name><name><surname>Mollon</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Computerized simulation of color appearance for dichromats</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>14</volume><fpage>2647</fpage><lpage>2655</lpage><pub-id pub-id-type="doi">10.1364/josaa.14.002647</pub-id><pub-id pub-id-type="pmid">9316278</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Rokni</surname><given-names>U</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian model of dynamic image stabilization in the visual system</article-title><source>PNAS</source><volume>107</volume><fpage>19525</fpage><lpage>19530</lpage><pub-id pub-id-type="doi">10.1073/pnas.1006076107</pub-id><pub-id pub-id-type="pmid">20937893</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Optimal defocus estimation in individual natural images</article-title><source>PNAS</source><volume>108</volume><fpage>16849</fpage><lpage>16854</lpage><pub-id pub-id-type="doi">10.1073/pnas.1108491108</pub-id><pub-id pub-id-type="pmid">21930897</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal disparity estimation in natural stereo images</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/14.2.1</pub-id><pub-id pub-id-type="pmid">24492596</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Image-Computable Ideal Observers for Tasks with Natural Stimuli</article-title><source>Annual Review of Vision Science</source><volume>6</volume><fpage>491</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-030320-041134</pub-id><pub-id pub-id-type="pmid">32580664</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>LT</given-names></name><name><surname>Krishna</surname><given-names>V</given-names></name><name><surname>Hladnik</surname><given-names>TC</given-names></name><name><surname>Guilbeault</surname><given-names>NC</given-names></name><name><surname>Juntti</surname><given-names>SA</given-names></name><name><surname>Thiele</surname><given-names>TR</given-names></name><name><surname>Arrenberg</surname><given-names>AB</given-names></name><name><surname>Cooper</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Visual statistics of aquatic environments in the natural habitats of zebrafish</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>433</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.11.433</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chakrabarti</surname><given-names>A</given-names></name><name><surname>Zickler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Statistics of real-world hyperspectral images</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>193</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2011.5995660</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaparro</surname><given-names>A</given-names></name><name><surname>Stromeyer</surname><given-names>CF</given-names></name><name><surname>Huang</surname><given-names>EP</given-names></name><name><surname>Kronauer</surname><given-names>RE</given-names></name><name><surname>Eskew</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Colour is what the eye sees best</article-title><source>Nature</source><volume>361</volume><fpage>348</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1038/361348a0</pub-id><pub-id pub-id-type="pmid">8426653</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chin</surname><given-names>BM</given-names></name><name><surname>Burge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predicting the Partition of Behavioral Variability in Speed Perception with Naturalistic Stimuli</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>864</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1904-19.2019</pub-id><pub-id pub-id-type="pmid">31772139</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coletta</surname><given-names>NJ</given-names></name><name><surname>Williams</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Psychophysical estimate of extrafoveal cone spacing</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>4</volume><fpage>1503</fpage><lpage>1513</lpage><pub-id pub-id-type="doi">10.1364/josaa.4.001503</pub-id><pub-id pub-id-type="pmid">3625330</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Ding</surname><given-names>X</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A computational-observer model of spatial contrast sensitivity: Effects of wave-front-based optics, cone-mosaic structure, and inference engine</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/19.4.8</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A computational observer model of spatial contrast sensitivity: Effects of photocurrent encoding, fixational eye movements, and inference engine</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>17</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.7.17</pub-id><pub-id pub-id-type="pmid">32692826</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname><given-names>CA</given-names></name><name><surname>Sloan</surname><given-names>KR</given-names></name><name><surname>Kalina</surname><given-names>RE</given-names></name><name><surname>Hendrickson</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Human photoreceptor topography</article-title><source>The Journal of Comparative Neurology</source><volume>292</volume><fpage>497</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1002/cne.902920402</pub-id><pub-id pub-id-type="pmid">2324310</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davila</surname><given-names>KD</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The relative contributions of pre-neural and neural factors to areal summation in the fovea</article-title><source>Vision Research</source><volume>31</volume><fpage>1369</fpage><lpage>1380</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(91)90058-d</pub-id><pub-id pub-id-type="pmid">1891825</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derrico</surname><given-names>JB</given-names></name><name><surname>Buchsbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A computational model of spatiochromatic image coding in early vision</article-title><source>Journal of Visual Communication and Image Representation</source><volume>2</volume><fpage>31</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/1047-3203(91)90033-C</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engström</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>Cone Types and Cone Arrangement in the Retina of Some Cyprinids</article-title><source>Acta Zoologica</source><volume>41</volume><fpage>277</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1111/j.1463-6395.1960.tb00481.x</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>4</volume><fpage>2379</fpage><lpage>2394</lpage><pub-id pub-id-type="doi">10.1364/josaa.4.002379</pub-id><pub-id pub-id-type="pmid">3430225</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Jepson</surname><given-names>LH</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Gunning</surname><given-names>DE</given-names></name><name><surname>Mathieson</surname><given-names>K</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional connectivity in the retina at the resolution of photoreceptors</article-title><source>Nature</source><volume>467</volume><fpage>673</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1038/nature09424</pub-id><pub-id pub-id-type="pmid">20930838</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrigan</surname><given-names>P</given-names></name><name><surname>Ratliff</surname><given-names>CP</given-names></name><name><surname>Klein</surname><given-names>JM</given-names></name><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Design of a trichromatic cone array</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000677</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000677</pub-id><pub-id pub-id-type="pmid">20168996</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Sequential ideal-observer analysis of visual discriminations</article-title><source>Psychological Review</source><volume>96</volume><fpage>267</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.96.2.267</pub-id><pub-id pub-id-type="pmid">2652171</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contributions of ideal observer theory to vision research</article-title><source>Vision Research</source><volume>51</volume><fpage>771</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.09.027</pub-id><pub-id pub-id-type="pmid">20920517</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Psychometric functions of uncertain template matching observers</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/18.2.1</pub-id><pub-id pub-id-type="pmid">29392276</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>S</given-names></name><name><surname>Zuo</surname><given-names>W</given-names></name><name><surname>Xie</surname><given-names>Q</given-names></name><name><surname>Meng</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Convolutional Sparse Coding for Image Super-Resolution</article-title><conf-name>2015 IEEE International Conference on Computer Vision</conf-name><conf-loc>Santiago, Chile</conf-loc><pub-id pub-id-type="doi">10.1109/ICCV.2015.212</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guleryuz</surname><given-names>OG</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising--Part I: Theory</article-title><source>IEEE Transactions on Image Processing</source><volume>15</volume><fpage>539</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1109/tip.2005.863057</pub-id><pub-id pub-id-type="pmid">16519342</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>T</given-names></name><name><surname>Pracejus</surname><given-names>L</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Color perception in the intermediate periphery of the visual field</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>26</elocation-id><pub-id pub-id-type="doi">10.1167/9.4.26</pub-id><pub-id pub-id-type="pmid">19757935</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harmening</surname><given-names>WM</given-names></name><name><surname>Tuten</surname><given-names>WS</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name><name><surname>Sincich</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the perceptual grain of the human retina</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>5667</fpage><lpage>5677</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5191-13.2014</pub-id><pub-id pub-id-type="pmid">24741057</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hofer</surname><given-names>H</given-names></name><name><surname>Carroll</surname><given-names>J</given-names></name><name><surname>Neitz</surname><given-names>J</given-names></name><name><surname>Neitz</surname><given-names>M</given-names></name><name><surname>Williams</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Organization of the human trichromatic cone mosaic</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>9669</fpage><lpage>9679</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2414-05.2005</pub-id><pub-id pub-id-type="pmid">16237171</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>J</given-names></name><name><surname>Wandell</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A Spectral Estimation Theory for Color Appearance Matching</article-title><source>Electronic Imaging</source><volume>2016</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.2352/ISSN.2470-1173.2016.20.COLOR-329</pub-id><pub-id pub-id-type="pmid">26770219</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>Q</given-names></name><name><surname>Farrell</surname><given-names>J</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning the Image Processing Pipeline</article-title><source>IEEE Transactions on Image Processing</source><volume>26</volume><fpage>5032</fpage><lpage>5042</lpage><pub-id pub-id-type="doi">10.1109/TIP.2017.2713942</pub-id><pub-id pub-id-type="pmid">28613172</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kadkhodaie</surname><given-names>Z</given-names></name><name><surname>Simoncelli</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Stochastic Solutions for Linear Inverse Problems Using the Prior Implicit in a Denoiser</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Karklin</surname><given-names>Y</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>999</fpage><lpage>1007</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Burge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Natural scene statistics predict how humans pool information across space in surface tilt estimation</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007947</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007947</pub-id><pub-id pub-id-type="pmid">32579559</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>Yuille</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Introduction: A Bayesian Formulation of Visual Perception</article-title><source>Perception as Bayesian Inference</source><volume>1</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1017/CBO9780511984037</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet Classification with Deep Convolutional Neural Networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Osorio</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Colour vision: colouring the dark</article-title><source>Current Biology</source><volume>13</volume><elocation-id>R83-5</elocation-id><pub-id pub-id-type="doi">10.1016/s0960-9822(03)00031-9</pub-id><pub-id pub-id-type="pmid">12573231</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Animal Eyes</source><publisher-loc>Oxford, New York</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199581139.001.0001</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Le</surname><given-names>Q</given-names></name><name><surname>Karpenko</surname><given-names>A</given-names></name><name><surname>Ngiam</surname><given-names>J</given-names></name><name><surname>Ng</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lennie</surname><given-names>P</given-names></name><name><surname>Haake</surname><given-names>PW</given-names></name><name><surname>Williams</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1991">1991</year><chapter-title>The Design of Chromatically Opponent Receptive Fields</chapter-title><person-group person-group-type="editor"><name><surname>Lennie</surname><given-names>P</given-names></name></person-group><source>Computational Models of Visual Processing</source><publisher-loc>Cambridge</publisher-loc><publisher-name>The MIT Press</publisher-name><fpage>71</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.7551/mitpress/2002.003.0010</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Levin</surname><given-names>A</given-names></name><name><surname>Freeman</surname><given-names>WT</given-names></name><name><surname>Durand</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><chapter-title>Understanding Camera Trade-Offs through a Bayesian Analysis of Light Field Projections</chapter-title><person-group person-group-type="editor"><name><surname>Forsyth</surname><given-names>D</given-names></name><name><surname>Torr</surname><given-names>P</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><source>Computer Science and Artificial Intelligence Laboratory</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>88</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-88693-8_7</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>A</given-names></name><name><surname>Zhaoping</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Are cone sensitivities determined by natural color statistics?</article-title><source>Journal of Vision</source><volume>6</volume><fpage>285</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1167/6.3.8</pub-id><pub-id pub-id-type="pmid">16643096</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Lian</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>PhD Thesis: Vision Modeling Tools for Evaluating Next-Generation Displays</article-title><publisher-loc>Stanford University</publisher-loc><publisher-name>ProQuest Dissertations Publishing</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.proquest.com/openview">https://www.proquest.com/openview</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>D</given-names></name><name><surname>Hutchinson</surname><given-names>L</given-names></name><name><surname>Brown</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unique yellow and other special colors seen by deuteranomalous trichromats</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>1249</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.11.1249</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manning</surname><given-names>JR</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Optimal design of photoreceptor mosaics: Why we do not see color at night</article-title><source>Visual Neuroscience</source><volume>26</volume><fpage>5</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1017/S095252380808084X</pub-id><pub-id pub-id-type="pmid">19193250</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marimont</surname><given-names>DH</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Matching color images: the effects of axial chromatic aberration</article-title><source>Journal of the Optical Society of America A</source><volume>11</volume><elocation-id>3113</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.11.003113</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Conde</surname><given-names>S</given-names></name><name><surname>Macknik</surname><given-names>SL</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The role of fixational eye movements in visual perception</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>229</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1038/nrn1348</pub-id><pub-id pub-id-type="pmid">14976522</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname><given-names>K. T</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>The contrast sensitivity of human colour vision to red-green and blue-yellow chromatic gratings</article-title><source>The Journal of Physiology</source><volume>359</volume><fpage>381</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1985.sp015591</pub-id><pub-id pub-id-type="pmid">3999044</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname><given-names>KT</given-names></name><name><surname>Kingdom</surname><given-names>FAA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Losses in peripheral colour sensitivity predicted from “hit and miss” post-receptoral cone connections</article-title><source>Vision Research</source><volume>36</volume><fpage>1995</fpage><lpage>2000</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00261-8</pub-id><pub-id pub-id-type="pmid">8759439</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nascimento</surname><given-names>SM</given-names></name><name><surname>Ferreira</surname><given-names>FP</given-names></name><name><surname>Foster</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Statistics of spatial cone-excitation ratios in natural scenes</article-title><source>Journal of the Optical Society of America A</source><volume>19</volume><elocation-id>1484</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.19.001484</pub-id><pub-id pub-id-type="pmid">12152688</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian Reconstruction of Natural Images from Human Brain Activity</article-title><source>Neuron</source><volume>63</volume><fpage>902</fpage><lpage>915</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.006</pub-id><pub-id pub-id-type="pmid">19778517</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neitz</surname><given-names>A</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Kuchenbecker</surname><given-names>JA</given-names></name><name><surname>Domdei</surname><given-names>N</given-names></name><name><surname>Harmening</surname><given-names>W</given-names></name><name><surname>Yan</surname><given-names>H</given-names></name><name><surname>Yeonan-Kim</surname><given-names>J</given-names></name><name><surname>Patterson</surname><given-names>SS</given-names></name><name><surname>Neitz</surname><given-names>M</given-names></name><name><surname>Neitz</surname><given-names>J</given-names></name><name><surname>Coates</surname><given-names>DR</given-names></name><name><surname>Sabesan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Effect of cone spectral topography on chromatic detection sensitivity</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>37</volume><fpage>A244</fpage><lpage>A254</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.382384</pub-id><pub-id pub-id-type="pmid">32400553</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolan</surname><given-names>JM</given-names></name><name><surname>Stringham</surname><given-names>JM</given-names></name><name><surname>Beatty</surname><given-names>S</given-names></name><name><surname>Snodderly</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatial profile of macular pigment and its relationship to foveal architecture</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>49</volume><fpage>2134</fpage><lpage>2142</lpage><pub-id pub-id-type="doi">10.1167/iovs.07-0933</pub-id><pub-id pub-id-type="pmid">18436846</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>N</given-names></name><name><surname>Batty</surname><given-names>E</given-names></name><name><surname>Falcon</surname><given-names>W</given-names></name><name><surname>Rutten</surname><given-names>T</given-names></name><name><surname>Mohit Rajpal</surname><given-names>EJC</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons</article-title><conf-name>Advances in Neural Information Processing Systems 30</conf-name><pub-id pub-id-type="doi">10.1101/153759</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Uncertainty explains many aspects of visual contrast detection and discrimination</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>2</volume><fpage>1508</fpage><lpage>1532</lpage><pub-id pub-id-type="doi">10.1364/josaa.2.001508</pub-id><pub-id pub-id-type="pmid">4045584</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polans</surname><given-names>J</given-names></name><name><surname>Jaeken</surname><given-names>B</given-names></name><name><surname>McNabb</surname><given-names>RP</given-names></name><name><surname>Artal</surname><given-names>P</given-names></name><name><surname>Izatt</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Wide-field optical model of the human eye with asymmetrically tilted and decentered lens that reproduces measured ocular aberrations</article-title><source>Optica</source><volume>2</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.1364/OPTICA.2.000124</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portilla</surname><given-names>J</given-names></name><name><surname>Strela</surname><given-names>V</given-names></name><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Image denoising using scale mixtures of Gaussians in the wavelet domain</article-title><source>IEEE Transactions on Image Processing</source><volume>12</volume><fpage>1338</fpage><lpage>1351</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.818640</pub-id><pub-id pub-id-type="pmid">18244692</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Putnam</surname><given-names>CM</given-names></name><name><surname>Bland</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Macular pigment optical density spatial distribution measured in a subject with oculocutaneous albinism</article-title><source>Journal of Optometry</source><volume>7</volume><fpage>241</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.optom.2014.03.001</pub-id><pub-id pub-id-type="pmid">25323647</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratnam</surname><given-names>K</given-names></name><name><surname>Domdei</surname><given-names>N</given-names></name><name><surname>Harmening</surname><given-names>WM</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Benefits of retinal image motion at the limits of spatial vision</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>30</elocation-id><pub-id pub-id-type="doi">10.1167/17.1.30</pub-id><pub-id pub-id-type="pmid">28129414</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romano</surname><given-names>Y</given-names></name><name><surname>Elad</surname><given-names>M</given-names></name><name><surname>Milanfar</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Little Engine That Could: Regularization by Denoising (RED)</article-title><source>SIAM Journal on Imaging Sciences</source><volume>10</volume><fpage>1804</fpage><lpage>1844</lpage><pub-id pub-id-type="doi">10.1137/16M1102884</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Iovin</surname><given-names>R</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Santini</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Miniature eye movements enhance fine spatial detail</article-title><source>Nature</source><volume>447</volume><fpage>851</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1038/nature05866</pub-id><pub-id pub-id-type="pmid">17568745</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushton</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Visual pigments in man</article-title><source>Scientific American</source><volume>207</volume><fpage>120</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1038/scientificamerican1162-120</pub-id><pub-id pub-id-type="pmid">13975605</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ambiguity and invariance: two fundamental challenges for visual processing</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>382</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.04.013</pub-id><pub-id pub-id-type="pmid">20545020</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabesan</surname><given-names>R</given-names></name><name><surname>Schmidt</surname><given-names>BP</given-names></name><name><surname>Tuten</surname><given-names>WS</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The elementary representation of spatial and color vision in the human retina</article-title><source>Science Advances</source><volume>2</volume><elocation-id>e1600797</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.1600797</pub-id><pub-id pub-id-type="pmid">27652339</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sampat</surname><given-names>N</given-names></name><name><surname>Tezaur</surname><given-names>R</given-names></name><name><surname>Wüller</surname><given-names>D</given-names></name><name><surname>Tian</surname><given-names>Q</given-names></name><name><surname>Blasinski</surname><given-names>H</given-names></name><name><surname>Lansel</surname><given-names>S</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Fukunishi</surname><given-names>M</given-names></name><name><surname>Farrell</surname><given-names>JE</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatically designing an image processing pipeline for a five-band camera prototype using the local, linear, learned (L 3) method</article-title><conf-name>Proceedings of SPIE - The International Society for Optical Engineering 9404</conf-name><pub-id pub-id-type="doi">10.1117/12.2083435</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>BP</given-names></name><name><surname>Boehm</surname><given-names>AE</given-names></name><name><surname>Tuten</surname><given-names>WS</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spatial summation of individual cones in human color vision</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0211397</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0211397</pub-id><pub-id pub-id-type="pmid">31344029</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekiguchi</surname><given-names>N</given-names></name><name><surname>Williams</surname><given-names>DR</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Efficiency in detection of isoluminant and isochromatic interference fringes</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>10</volume><fpage>2118</fpage><lpage>2133</lpage><pub-id pub-id-type="doi">10.1364/josaa.10.002118</pub-id><pub-id pub-id-type="pmid">8229351</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Toward a universal law of generalization for psychological science</article-title><source>Science</source><volume>237</volume><fpage>1317</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1126/science.3629243</pub-id><pub-id pub-id-type="pmid">3629243</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>4.7 Statistical Modeling of Photographic Images</chapter-title><person-group person-group-type="editor"><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><source>Handbook of Video and Image Processing</source><publisher-loc>Cambridge, United States</publisher-loc><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1016/B978-012119792-6/50089-9</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient coding explains the universal law of generalization in human perception</article-title><source>Science</source><volume>360</volume><fpage>652</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1126/science.aaq1118</pub-id><pub-id pub-id-type="pmid">29748284</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>V</given-names></name><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Heasly</surname><given-names>BS</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Burge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Computational luminance constancy from naturalistic images</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1167/18.13.19</pub-id><pub-id pub-id-type="pmid">30593061</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>AW</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Stavenga</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Information capacity of eyes</article-title><source>Vision Research</source><volume>17</volume><fpage>1163</fpage><lpage>1175</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(77)90151-1</pub-id><pub-id pub-id-type="pmid">595380</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>AW</given-names></name><name><surname>Bossomaier</surname><given-names>TR</given-names></name><name><surname>Hughes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Optical image quality and the cone mosaic</article-title><source>Science</source><volume>231</volume><fpage>499</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1126/science.3941914</pub-id><pub-id pub-id-type="pmid">3941914</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Noise characteristics and prior expectations in human visual speed perception</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>578</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/nn1669</pub-id><pub-id pub-id-type="pmid">16547513</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stockman</surname><given-names>A</given-names></name><name><surname>Sharpe</surname><given-names>LT</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The spectral sensitivities of the middle- and long-wavelength-sensitive cones derived from measurements in observers of known genotype</article-title><source>Vision Research</source><volume>40</volume><fpage>1711</fpage><lpage>1737</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(00)00021-3</pub-id><pub-id pub-id-type="pmid">10814758</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stockman</surname><given-names>A</given-names></name><name><surname>Sharpe</surname><given-names>LT</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Into the twilight zone: the complexities of mesopic vision and luminous efficiency</article-title><source>Ophthalmic &amp; Physiological Optics</source><volume>26</volume><fpage>225</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1111/j.1475-1313.2006.00325.x</pub-id><pub-id pub-id-type="pmid">16684149</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tjan</surname><given-names>BS</given-names></name><name><surname>Legge</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The viewpoint complexity of an object-recognition task</article-title><source>Vision Research</source><volume>38</volume><fpage>2335</fpage><lpage>2350</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00255-1</pub-id><pub-id pub-id-type="pmid">9798003</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkacik</surname><given-names>G</given-names></name><name><surname>Prentice</surname><given-names>JS</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Local statistics in natural scenes predict the saliency of synthetic textures</article-title><source>PNAS</source><volume>107</volume><fpage>18149</fpage><lpage>18154</lpage><pub-id pub-id-type="doi">10.1073/pnas.0914916107</pub-id><pub-id pub-id-type="pmid">20923876</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tregillus</surname><given-names>KEM</given-names></name><name><surname>Isherwood</surname><given-names>ZJ</given-names></name><name><surname>Vanston</surname><given-names>JE</given-names></name><name><surname>Engel</surname><given-names>SA</given-names></name><name><surname>MacLeod</surname><given-names>DIA</given-names></name><name><surname>Kuriki</surname><given-names>I</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Color Compensation in Anomalous Trichromats Assessed with fMRI</article-title><source>Current Biology</source><volume>31</volume><fpage>936</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.11.039</pub-id><pub-id pub-id-type="pmid">33326771</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ulyanov</surname><given-names>D</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep Image Prior</article-title><conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><fpage>9446</fpage><lpage>9454</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00984</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Spatial, temporal and spectral pre-processing for colour vision</article-title><source>Proceedings. Biological Sciences</source><volume>251</volume><fpage>61</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1098/rspb.1993.0009</pub-id><pub-id pub-id-type="pmid">8094566</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Venkatakrishnan</surname><given-names>SV</given-names></name><name><surname>Bouman</surname><given-names>CA</given-names></name><name><surname>Wohlberg</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Plug-and-Play priors for model based reconstruction</article-title><conf-name>IEEE Global Conference on Signal and Information Processing</conf-name><pub-id pub-id-type="doi">10.1109/GlobalSIP.2013.6737048</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virsu</surname><given-names>V</given-names></name><name><surname>Rovamo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Visual resolution, contrast sensitivity, and the cortical magnification factor</article-title><source>Experimental Brain Research</source><volume>37</volume><fpage>475</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1007/BF00236818</pub-id><pub-id pub-id-type="pmid">520438</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Walls</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="1942">1942</year><source>The Vertebrate Eye and Its Adaptive Radiation</source><publisher-loc>Bloomfield Hills, Mich</publisher-loc><publisher-name>Cranbrook Institute of Science</publisher-name><pub-id pub-id-type="doi">10.5962/bhl.title.7369</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Sheikh</surname><given-names>HR</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Image Quality Assessment: From Error Visibility to Structural Similarity</article-title><source>IEEE Transactions on Image Processing</source><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>QUEST+: A general multidimensional Bayesian adaptive psychometric method</article-title><source>Journal of Vision</source><volume>17</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1167/17.3.10</pub-id><pub-id pub-id-type="pmid">28355623</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Mollon</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Adaptation and the color statistics of natural images</article-title><source>Vision Research</source><volume>37</volume><fpage>3283</fpage><lpage>3298</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00125-9</pub-id><pub-id pub-id-type="pmid">9425544</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X-X</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Bayesian observer model constrained by efficient coding can explain “anti-Bayesian” percepts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1038/nn.4105</pub-id><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Aliasing in human foveal vision</article-title><source>Vision Research</source><volume>25</volume><fpage>195</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(85)90113-0</pub-id><pub-id pub-id-type="pmid">4013088</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>DR</given-names></name><name><surname>Sekiguchi</surname><given-names>N</given-names></name><name><surname>Haake</surname><given-names>W</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Packer</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1991">1991</year><chapter-title>The Cost of Trichromacy for Spatial Vision</chapter-title><person-group person-group-type="editor"><name><surname>Valberg</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>BB</given-names></name></person-group><source>In From Pigments to Perception: Advances in Understanding Visual Processes, NATO ASI Series</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer US</publisher-name><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/978-1-4615-3718-2</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wool</surname><given-names>LE</given-names></name><name><surname>Crook</surname><given-names>JD</given-names></name><name><surname>Troy</surname><given-names>JB</given-names></name><name><surname>Packer</surname><given-names>OS</given-names></name><name><surname>Zaidi</surname><given-names>Q</given-names></name><name><surname>Dacey</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nonselective Wiring Accounts for Red-Green Opponency in Midget Ganglion Cells of the Primate Retina</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1520</fpage><lpage>1540</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1688-17.2017</pub-id><pub-id pub-id-type="pmid">29305531</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yellott</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Spectral consequences of photoreceptor sampling in the rhesus retina</article-title><source>Science</source><volume>221</volume><fpage>382</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1126/science.6867716</pub-id><pub-id pub-id-type="pmid">6867716</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A spatial extension of CIELAB for digital color-image reproduction</article-title><source>Journal of the Society for Information Display</source><volume>5</volume><elocation-id>61</elocation-id><pub-id pub-id-type="doi">10.1889/1.1985127</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Mou</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FSIM: a feature similarity index for image quality assessment</article-title><source>IEEE Transactions on Image Processing</source><volume>20</volume><fpage>2378</fpage><lpage>2386</lpage><pub-id pub-id-type="doi">10.1109/TIP.2011.2109730</pub-id><pub-id pub-id-type="pmid">21292594</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L-Q</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>ISETImagePipeline</data-title><version designator="swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427">swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e26fdcfd81b4c40051c6e5694151353d2af45c65;origin=https://github.com/isetbio/ISETImagePipeline;visit=swh:1:snp:244afc95dd1bfbfeb729d9e21ee2002d3bba8d96;anchor=swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427">https://archive.softwareheritage.org/swh:1:dir:e26fdcfd81b4c40051c6e5694151353d2af45c65;origin=https://github.com/isetbio/ISETImagePipeline;visit=swh:1:snp:244afc95dd1bfbfeb729d9e21ee2002d3bba8d96;anchor=swh:1:rev:72e7296dcaf8ebdcca35776d7a98026c8f041427</ext-link></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>MJY</given-names></name><name><surname>Nevala</surname><given-names>NE</given-names></name><name><surname>Yoshimatsu</surname><given-names>T</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name><name><surname>Nilsson</surname><given-names>D-E</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Zebrafish Differentially Process Color across Visual Space to Match Natural Scenes</article-title><source>Current Biology</source><volume>28</volume><fpage>2018</fpage><lpage>2032</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.075</pub-id><pub-id pub-id-type="pmid">29937350</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71132.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meister</surname><given-names>Markus</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.06.02.446829" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.06.02.446829"/></front-stub><body><p>This rigorous computational study simulates the sampling of the visual image by cone photoreceptors in the human eye, and explains how the image content can be reconstructed from those cone signals. The authors show that a number of properties of the human retina and of human perception are predicted from these simulations. Their modeling framework also serves to unify previous treatments and invites extension to subsequent stages of visual processing.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71132.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meister</surname><given-names>Markus</given-names></name><role>Reviewing Editor</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Meister</surname><given-names>Markus</given-names></name><role>Reviewer</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.06.02.446829">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.06.02.446829v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;An Image Reconstruction Framework for Characterizing Early Vision&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Markus Meister as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Tirin Moore as the Senior Editor.</p><p>The reviewers have discussed their comments, and the Reviewing Editor has drafted this consensus review to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Main recommendations</p><p>1. Sensor model: The cone mosaic is assumed to be totally regular (a triangular array) with irregularity in cone type. What happens to reconstruction when the array is not a perfect triangular array?</p><p>2. Image statistics model: Please comment on the role of fixational eye movements during human vision. Under normal viewing conditions the retinal image doesn't hold still for the 50 ms integration time used here (line 357). In reality the image drifts so fast that any given cone looks at different image pixels every 5 ms (e.g. doi.org/10.1073/pnas.1006076107). Please discuss how this might affect the conclusions derived from an assumption of static images.</p><p>3. The natural scenes prior: This is a prominent component of the algorithm. Please elaborate how important the inclusion that prior term is for producing the results. For example:</p><p>3.1. Figures6, 7, and 8: Some supplemental comparison images with no-prior or weak-prior estimates would be helpful to visualize the effect that including the prior term has on the results presented here.</p><p>3.2. Figure 9: How important is the natural scenes prior for replicating the gratings psychophysics results? If you used just an MLE estimate, or reduced the weight on the prior term, would the results change dramatically?</p><p>4. Extrafoveal vision: At the eccentricities considered in Figure 8 the circuitry of the retina already pools over many cones. Why is a reconstruction based on differentiable cones still relevant here? Generally some more discussion of post-receptor vision would be helpful, or at least a justification for not considering it.</p><p>5. Please offer some interpretation for the visualizations produced by the Bayesian model, for example:</p><p>5.1. In Figure 6, the protanopia images have a reddish hue, and the images generated using reference methods do not.</p><p>5.2. In Figure 7, the images tend to get more speckled as light intensity decreases, which doesn't seem to match up with perception during natural vision.</p><p>5.3. In Figure 8, we might expect from human vision that chromatic saturation would increase as we move to the periphery, but the example images don't show that.</p><p>6. Relation to prior work:</p><p>6.1. Discuss how the current assumptions differ from Garrigan et al., (2010).</p><p>6.2. Discuss relation to the Plug and Play Bayesian image reconstruction and image restoration methods (e.g. doi: 10.1109/TCI.2016.2629286, doi: 10.1109/TPAMI.2021.3088914). These methods are also optimization-based MAP estimation algorithms, and are conceptually quite similar to the approach taken in the paper.</p><p>6.3. Repeatedly the results of this new approach end up consistent with earlier work that operated with simpler analysis (lines 318, 435, 522). In the discussion, please give a crisp summary of what new insights came from the more complex approach.</p><p>6.4. When introducing ideas that are part of conventional wisdom, a broader list of citations would help the reader, for example: the notion that multi-chromatic receptors are less useful in dim light (line 347); the optimal allocation of spectral types given the spectra of natural scenes (line 235 ff); the importance of prior distributions in evaluating visual system design (line 277).</p><p>Other suggestions</p><p>7. Title and elsewhere: “Early vision” is often interpreted as “everything up to V1” (see textbooks and e.g. doi.org/10.1523/JNEUROSCI.3726-05.2005). Here the signal hasn’t even emerged from the receptors. None of the post-receptoral circuitry is included, which ultimately comes to dominate visual perception. Please consider a title that is more specific to the article.</p><p>8. Figure 5:</p><p>8.1. Maybe plot all curves on the same y-scale. Could be easier to see the systematic variation.</p><p>8.2. Maybe color the symbol nearest the minimum of each function.</p><p>9. Figure7:</p><p>9.1. Please include the original images, since in those panels the reader is trying to compare image degradation (also in S5).</p><p>9.2. What if at twilight the goal is to reconstruct the gray scale image and not the RBG image? Would the reconstruction be more spatially accurate and less noisy?</p><p>10. Lines 525-533. Other species like zebrafish have a much more limited range of tasks to perform than humans. Is image reconstruction still the appropriate cost function in those cases?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71132.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Main recommendations</p><p>1. Sensor model: The cone mosaic is assumed to be totally regular (a triangular array) with irregularity in cone type. What happens to reconstruction when the array is not a perfect triangular array?</p></disp-quote><p>We appreciate this comment, which revealed a lacuna in our exposition. Our calculations were actually performed with more realistic mosaics, but we did not describe this. We have now expanded the Methods to clarify, and to provide a reference to the procedure we used for mosaic generation.</p><p>Page 29, Line 792. “A stochastic procedure was used to generate approximately hexagonal mosaics with eccentricity-varying cone density matched to that of the human retina (Curcio et al., 1990). See Cottaris et al., (2019) for a detailed description of the algorithm.”</p><disp-quote content-type="editor-comment"><p>2. Image statistics model: Please comment on the role of fixational eye movements during human vision. Under normal viewing conditions the retinal image doesn't hold still for the 50 ms integration time used here (line 357). In reality the image drifts so fast that any given cone looks at different image pixels every 5 ms (e.g. doi.org/10.1073/pnas.1006076107). Please discuss how this might affect the conclusions derived from an assumption of static images.</p></disp-quote><p>How fixational eye movements interact with reconstruction is an interesting and topical question, and a full exploration within the ISETBio framework would represent a paperlength project in its own right. We agree that some discussion is warranted here, however, and have added the following to the discussion:</p><p>“Our current model also does not take into account fixational eye movements, which displace the retinal image at a time scale shorter than the integration period we have used here (Martinez-Conde, Macknik, and Hubel 2004; Burak et al., 2010). […] Future work should be able to extend our current results through the study of dynamic reconstruction algorithms within ISETBio.”</p><disp-quote content-type="editor-comment"><p>3. The natural scenes prior: This is a prominent component of the algorithm. Please elaborate how important the inclusion that prior term is for producing the results. For example:</p><p>3.1. Figures6, 7, and 8: Some supplemental comparison images with no-prior or weak-prior estimates would be helpful to visualize the effect that including the prior term has on the results presented here.</p></disp-quote><p>We agree that further elaboration on the role of the prior is a good idea. We have added a short summary of the importance of the natural image prior early on, and now follow that in the paper with additional analyses and comment.</p><p>Page 5, Line 138. “In the context of our current study, the role of the natural image prior comes in several forms, as we will demonstrate in Results. First, since the reconstruction problem is underdetermined, the prior is a regularizer, providing a unique MAP estimate; Second, the prior acts as a denoiser, counteracting the Poisson noise in the cone excitation; Lastly, the prior guides the spatial and spectral demosaicing of the signals provided via the discrete sampling of the retinal image by the cone mosaic.”</p><p>As we previously demonstrated in Figures 2 and 3, due to the presence of cone noise, estimation without a prior (maximum likelihood estimation) is highly subject to the effects of noise fluctuations – the reconstruction tracks the noise. This observation applies to all of our analyses but is particularly pertinent to the results shown in Figure 7, since those center around the effects of varying the signal-to-noise ratio of the cone excitations. We have elaborated on this result in response to Comment 5.2 below with the addition of Figure 7-S1, please refer to that response for more detail.</p><p>In addition, we conducted additional analyses associated with Figure 8, where we explored the effect of the prior on reconstruction in the absence of cone noise, by providing MLE reconstructions for comparison with those in Figure 8. We think this analysis provides valuable additional insight. The newly added figure is Figure 8-S4. We added the following to the main text:</p><p>Page 18, Line 452. “Lastly, to emphasize the importance of the natural image prior, we performed a set of maximum likelihood reconstructions with no explicit prior constraint, which resulted in images with less coherent spatial structure and lower fidelity color appearance (Figure 8-S4). Thus, the prior here is critical for the proper demosaicing and interpolation of the information provided by the sparse cone sampling at these peripheral locations.”</p><p>However, an important caveat here is that for the reconstruction problem we consider, the MLE estimate is not unique: variations within the null space of the render matrix do not influence the likelihood (Figure 3). This ambiguity is particularly pertinent to the dichromatic reconstructions, as large differences in color appearance can occur within the null space of a dichromatic mosaic’s render matrix. In fact, we have verified numerically that the original color image, an MLE estimate from the dichromatic mosaic (without cone noise), and linear mixtures of the two under the constraint that the mixture weights sum to one, all have the same (maximum) likelihood value (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). Thus, having a prior is crucial for obtaining well-defined dichromatic reconstructions. Rather than adding the Reviewer Figure to the paper, however, we have added the following prose to the discussion of Figure 6: Page 14, Line 329. “Note that in the case where there is no simulated cone noise (as in Figure 6), the original image has a likelihood at least as high as the reconstruction obtained via our method. Thus, the difference between the original images and each of the corresponding dichromatic reconstructions is driven by the image prior. On the other hand, the difference in the reconstructions across the three types of dichromacy illustrates how the different dichromatic likelihood functions interact with the prior.”</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>A set of six images with the same (maximum) likelihood for a deuteranopic cone mosaic.</title><p>The top-left image is the original image, the bottom-right image is one MLE estimate for the dichromatic mosaic (without cone noise), and the other four images are produced as linear mixtures of the two, with the mixture weights summing to one. Without an explicit prior constraint, all these 6 images (and many others with pixel differences in the null space of the render matrix) provide a valid MLE solution to the reconstruction problem.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-sa2-fig1-v3.tif"/></fig><p>Also see our response to Comment 5.1 for a discussion of the three methods we compared for visualizing dichromacy.</p><disp-quote content-type="editor-comment"><p>3.2. Figure 9: How important is the natural scenes prior for replicating the gratings psychophysics results? If you used just an MLE estimate, or reduced the weight on the prior term, would the results change dramatically?</p></disp-quote><p>This is also an excellent question. We have now added a supplementary figure (Figure 10-S1) showing that an observer that makes decisions based on a maximum-likelihood reconstruction using the same type of template-based decision rule as we used for the reconstruction-based CSF in Figure 10 will produce contrast sensitivity similar to the Poisson ideal observer, albeit at a lower overall sensitivity level (since the template based decision rule does not handle noise as well as the ideal observer). That is, the prior matters quite a bit here.</p><p>Page 22, Line 545. “We attribute these effects to the role of the image prior in the reconstructions, which leads to selective enhancement/attenuation of different image components. In support of this idea, we also found that an observer based on maximum likelihood reconstruction without the explicit prior term produced CSFs similar in shape to the Poisson ideal observer (Figure 10-S1).”</p><disp-quote content-type="editor-comment"><p>4. Extrafoveal vision: At the eccentricities considered in Figure 8 the circuitry of the retina already pools over many cones. Why is a reconstruction based on differentiable cones still relevant here? Generally some more discussion of post-receptor vision would be helpful, or at least a justification for not considering it.</p></disp-quote><p>We agree with the reviewer that post-receptoral factors are important to consider, both for foveal and for peripheral vision. In this regard, we are eager to expand our current model to include retinal ganglion cells. Nevertheless, we believe that there is considerable value of the analysis as we have developed and presented it here. Indeed, the current analysis elucidates what phenomenon can or cannot be attributed to factors up to and including the cone mosaic, and thus clarifies what phenomena require explanation by later stages of processing. We have added to the discussion on this point:</p><p>Page 26, Line 687. “Our current model only considers the representation up to and including the excitations of the cone mosaic. […] When we apply such an algorithm to, for example, the output of retinal ganglion cells, we shift the division. Our view is that analyses at multiple stages are of interest, and eventual comparisons between them are likely to shed light on the role of each stage.”</p><disp-quote content-type="editor-comment"><p>5. Please offer some interpretation for the visualizations produced by the Bayesian model, for example:</p><p>5.1. In Figure 6, the protanopia images have a reddish hue, and the images generated using reference methods do not.</p></disp-quote><p>Thanks for the question. We have now extended the text to discuss the similarities and differences among the three methods in terms of how the color in the visualization is determined as follows (also see the last section of our response to Comment 3.1):</p><p>Page 14, Line 340. “To determine an image based on the excitations of only two classes of cones, any method will need to rely on a set of regularizing assumptions to resolve the ambiguity introduced by the dichromatic retinas. Brettel et al., (1997) started with the trichromatic cone excitations of each image pixel, and projected these onto a biplanar surface, with each plane defined by the neutral color axis and an anchoring stimulus identified through color appearance judgments made across the two eyes of unilateral dichromats. […] We find the general agreement between the reconstruction-based methods and the one based subject reports an encouraging sign that the reconstruction approach can be used to predict aspects of appearance.”</p><p>In addition, we also found that in our previous analysis, the assumed display when rendering the two comparison methods was a generic sRGB display, not the CRT display we have used in the reconstruction routine. We have fixed this and updated Figure 6, although this results in no noticeable difference in the visualization as far as we can tell. We expanded the Methods section to include the details of the implementation:</p><p>Page 35, Line 1050. “In Figure 6 we also present the results of two comparison methods for visualizing dichromacy, those of Brettel et al., (1997) and Jiang et al., (2016), both are implemented as part of ISETBio routine. To determine the corresponding dichromatic images, we first computed the LMS trichromatic stimulus coordinates of the linear RGB value of each pixel of the input image, based on the parameters of the simulated CRT display. LMS coordinates were computed with respect to the Stockman-Sharpe 2-deg cone fundamentals (Stockman and Sharpe 2000). The ISETBio function <italic>lms2lmsDichromat</italic> was then used to transform these LMS coordinates according to the two methods (see a brief description in the main text). Lastly, the transformed LMS coordinates were converted back to linear RGB values, and γ corrected before rendering.”</p><disp-quote content-type="editor-comment"><p>5.2. In Figure 7, the images tend to get more speckled as light intensity decreases, which doesn't seem to match up with perception during natural vision.</p></disp-quote><p>Thanks for this insightful observation, which led us to recheck our calculations. In the original simulations, there was an error where the value of prior weight λ we used was too small, thus leading to an overly weak prior. We redid these calculations with the weight correctly chosen via our cross-validation procedure and updated Figure 7. In the corrected version, the increase in noise reduces the amount of spatial detail in the reconstructed images due to the denoising effect of the image prior, but the images do not get more “speckled”. This is more consistent with intuition. The text has been updated as following:</p><p>Page 16, Line 387. “At lower intensities, however, the deuteranomalous reconstruction lacks chromatic content still present in the normal reconstruction (second and third row). The increase in noise also reduces the amount of spatial detail in the reconstructed images, due to the denoising effect driven by the image prior. Furthermore, a loss of chromatic content is also seen for the reconstruction from the normal mosaic at the lowest light level (last row).”</p><p>Further, we have included as a supplementary figure the simulations done with the original lower λ value, as the comparison demonstrates the effect of cone noise when the prior is underweighted, which is a useful point to make in response to Comment 3.1 above:</p><p>Page 17, Line 407. “The prior weight parameter in these set of simulations was set based on a cross-validation procedure that minimizes RMSE (λ = 0.05). To highlight interaction between noise and the prior, we have also included a set of reconstructions with the prior weight set to a much lower level (λ = 0.001), see Figure 7-S1.”</p><disp-quote content-type="editor-comment"><p>5.3. In Figure 8, we might expect from human vision that chromatic saturation would increase as we move to the periphery, but the example images don't show that.</p></disp-quote><p>Our reading is that previous literature tends to find a decrease in chromatic sensitivity at peripheral visual eccentricities, at least for the red-green axis of color perception and some stimulus spatial configurations. Thus, we think our simulation is consistent with the literature in that a desaturation of the reconstructed images is qualitatively akin to a decrease in chromatic sensitivity, albeit with the degree of desaturation depending on the details of the prior, optical blur, and cone mosaic. We have added the following additional text to the paper:</p><p>Page 18, Line 437. “In the image of the dragonfly, for example, the reconstructed colors are desaturated at intermediate eccentricities (e.g., Figure 8C, D), compared with the fovea (Figure 8A) and more eccentric locations (Figure 8E, F). The desaturation is qualitatively consistent with the literature that indicates a decrease in chromatic sensitivity at peripheral visual eccentricities, at least for the red-green axis of color perception and for some stimulus spatial configurations (Virsu and Rovamo 1979; Mullen and Kingdom 1996; but see Hansen, Pracejus, and Gegenfurtner 2009).”</p><p>On this general point, also see our response to Comment 4 above.</p><disp-quote content-type="editor-comment"><p>6. Relation to prior work:</p><p>6.1. Discuss how the current assumptions differ from Garrigan et al., (2010).</p></disp-quote><p>Thanks for the suggestion. We have elaborated the Discussion section on differences between our method and the approach taken by Garrigan et al., (2010).</p><p>Page 24, Line 601. “Previous work (Garrigan et al., 2010) conducted a similar analysis with consideration of natural scene statistics, physiological optics, and cone spectral sensitivity, using an information maximization criterion. One advance enabled by our work is that we are able to fully simulate a 1-deg mosaic with naturalistic input, as opposed to the information-theoretical measures used by Garrigan et al., which became intractable as the size of the mosaic and the dimensionality of the input increased. In fact, Garrigan et al., (2010) approximated by estimating the exact mutual information for small mosaic size (N = 1 … 6 cones) and then extrapolated to larger cone mosaics using a scaling law (Borghuis et al., 2008). The fact that the two theories corroborate each other well is reassuring and suggests that the results are robust to the details of the analysis.”</p><disp-quote content-type="editor-comment"><p>6.2. Discuss relation to the Plug and Play Bayesian image reconstruction and image restoration methods (e.g. doi: 10.1109/TCI.2016.2629286, doi: 10.1109/TPAMI.2021.3088914). These methods are also optimization-based MAP estimation algorithms, and are conceptually quite similar to the approach taken in the paper.</p></disp-quote><p>Plug-and-Play and other related techniques (e.g., Alain and Bengio 2014; Romano, Elad, and Milanfar 2017), including one we cited previously (Kadkhodaie and Simoncelli, 2021), are related methods (see Introduction in Kadkhodaie and Simoncelli 2021 for a brief review) that enable transfer of the prior implicit in an image denoiser to other domains. We think these techniques represent a promising direction that should allow us to take advantage of the image priors learned by denoising convolution neural networks and apply them to our image reconstruction problem. We have expanded the Discussion section on these related techniques:</p><p>Page 28, Line 750. “However, the ability of neural networks to represent more complex natural image priors (Ulyanov, Vedaldi, and Lempitsky 2018; Kadkhodaie and Simoncelli 2021) is of great interest. […] We think this represents a promising direction, and in the future plan to incorporate more sophisticated priors, to evaluate the robustness of our conclusions to variations and improvements in the image prior.”</p><disp-quote content-type="editor-comment"><p>6.3. Repeatedly the results of this new approach end up consistent with earlier work that operated with simpler analysis (lines 318, 435, 522). In the discussion, please give a crisp summary of what new insights came from the more complex approach.</p></disp-quote><p>We agree that such a summary is useful. At a broad level, an important contribution of our work is that it unifies treatment of a diverse set of issues that have been studied in separate, although related ways. In this regard, the comparisons between our results and previous ones serves as an important validation of our approach. For novel results, we have included in the Discussion section a summary as follows:</p><p>Page 24, Line 573. “Our method enables both quantification and visualization of information loss due to various factors in the initial encoding, and unifies the treatment of a diverse set of issues that have been studied in separate, albeit related, ways. In several cases, we were able to extend previous studies by eliminating simplifying assumptions (e.g., by the use of realistic, large cone mosaics that operate on high-dimensional, naturalistic image input). To summarize succinctly, we highlight here the following novel results and substantial extensions of previous findings: (1) When considering the allocation of different cone types on the human retina, we demonstrated the importance of the spatial and spectral correlation structure of the image prior; (2) As we examined reconstructions as a way to visualize information loss, we observed rich interactions in how the appearances of the reconstruction vary with mosaic sampling, physiological optics, and the SNR of the cone excitations; (3) We found that the reconstructions are consistent with empirical reports of retinal spatial aliasing obtained with interferometric stimuli, adding an explicit image prior component and extending consideration of the interleaved nature of the trichromatic retinal cone mosaic relative to the previous treatment of these phenomena; (4) We linked image reconstructions to spatio-chromatic contrast sensitivity functions by applying a computational observer for psychophysical discrimination to the reconstructions. Below, we provide an extended discussion of key findings, as well as of some interesting open questions and future directions.”</p><p>The above noted, another important contribution of our work is that it allows for predictions of novel experiments. We have expanded on this point, just a little, in the discussion:</p><p>Page 26, Line 666. “Our method could also be applied to such questions, and also to a wider range of adaptive optics (AO) experiments (e.g., Schmidt et al., 2019; Neitz et al., 2020), to help understand the extent to which image reconstruction can capture perceptual behavior. More speculatively, it may be possible to use calculations performed within the image reconstruction framework to synthesize stimuli that will maximally discriminate between different hypothesis about how the excitations of sets of cones are combined to form percepts, particularly with the emergence of technology that enables precise experimental control over the stimulation of individual cones in human subjects (Harmening et al., 2014; Sabesan et al., 2016; Schmidt et al., 2019).”</p><disp-quote content-type="editor-comment"><p>6.4. When introducing ideas that are part of conventional wisdom, a broader list of citations would help the reader, for example: the notion that multi-chromatic receptors are less useful in dim light (line 347); the optimal allocation of spectral types given the spectra of natural scenes (line 235 ff); the importance of prior distributions in evaluating visual system design (line 277).</p></disp-quote><p>Thanks for the suggestions. We have included a broader list of citations at the three places mentioned above (Note that the line numbers have shifted from those in the comment, due to the revisions in the manuscript):</p><p>Page 9, Line 249. “Our results are in agreement with a previous analysis in showing that the empirically observed allocation of retinal photoreceptor type is consistent with the principle of optimal design (Garrigan et al., 2010; also see Levin et al., 2008; Manning and Brainard 2009; Tian et al., 2015; Jiang et al., 2017).”</p><p>Page 12, Line 294. “This analysis highlights the importance of considering visual system design in context of the statistical properties (prior distribution) of natural images, as it shows that the conclusions drawn can vary with these properties (Barlow 1961; Derrico and Buchsbaum 1991; Barlow and Földiàgk 1989; Atick, Li, and Redlich 1992; Lewis and Li 2006; Levin et al., 2008; Borghuis et al., 2008; Garrigan et al., 2010; Tkačik et al., 2010; Atick 2011; Burge 2020).”</p><p>Page 16, Line 392. “This observation may be connected to the fact that biological visual systems that operate at low light levels are typically monochromatic, potentially to increase the SNR of spatial vision at the cost of completely disregarding color (e.g., the monochromatic human rod system; see Manning and Brainard 2009 for a related and more detailed treatment; also see Walls 1942; Rushton 1962; Van Hateren 1993; Land and Osorio 2003).”</p><disp-quote content-type="editor-comment"><p>Other suggestions</p><p>7. Title and elsewhere: “Early vision” is often interpreted as “everything up to V1” (see textbooks and e.g. doi.org/10.1523/JNEUROSCI.3726-05.2005). Here the signal hasn’t even emerged from the receptors. None of the post-receptoral circuitry is included, which ultimately comes to dominate visual perception. Please consider a title that is more specific to the article.</p></disp-quote><p>We agree with this comment, and have replaced all occurrences of “early vision” in the paper with either “the initial visual encoding” or “initial encoding”.</p><disp-quote content-type="editor-comment"><p>8. Figure 5:</p><p>8.1. Maybe plot all curves on the same y-scale. Could be easier to see the systematic variation.</p><p>8.2. Maybe color the symbol nearest the minimum of each function.</p></disp-quote><p>We have modified Figure 5 to explicitly mark the areas that are close to the minimum, which improves the presentation. We have also included a new supplementary figure for the same data but with matched y-axis. The main text is changed as follows:</p><p>Page 12, Line 286. “The dependence of the average reconstruction error on the L-cone proportion decreases as the chromatic correlation in the signal increases (Figure 5). A decrease of spatial correlation has little impact on the shape of the curves, but increases the overall magnitude of reconstruction error (Figure 5; to highlight the shape, the scale of the y-axis is different across rows and columns. See Figure 5-S1 for the same plot with matched y-axis scale). When both the chromatic and spatial correlation are high, there is a large margin of L-cone proportion within which the reconstruction error is close to the optimal (minimal) point (Figure 5, shaded area).”</p><disp-quote content-type="editor-comment"><p>9. Figure7:</p><p>9.1. Please include the original images, since in those panels the reader is trying to compare image degradation (also in S5).</p></disp-quote><p>Thanks for the suggestions. We have now added the original images to these two figures to facilitate the comparison. Also note that S5 is now Figure 8-S3.</p><disp-quote content-type="editor-comment"><p>9.2. What if at twilight the goal is to reconstruct the gray scale image and not the RBG image? Would the reconstruction be more spatially accurate and less noisy?</p></disp-quote><p>We conducted an initial analysis to explore the possibility raised by this question. More specifically, we constrained the search space of the reconstruction algorithm to be grayscale images only (R = G = B at each pixel) and obtained the MAP estimate under this constraint. The prior weight was set to the same levels as Figure 7 (λ = 0.05) and Figure 7-S1 (λ = 0.001) in the main text. Visual examination did not reveal improvements in the quality of the reconstructed images (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>), with the most salient difference being the loss of the residual color in the images reconstructed under the grayscale constraint.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Grayscale image reconstruction from a normal trichromatic mosaic at twilight level, given two different prior weights.</title><p>Compare to Figure 7 and Figure 7-S1 in the main text, we did not find meaningful improvements in the quality of the reconstructed images.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71132-sa2-fig2-v3.tif"/></fig><p>Finding the MAP estimate under the grayscale constraint is simple and numerically feasible. A more sophisticated method would involve first marginalizing the posterior. Concretely:</p><p>Define p(x) as a posterior over RGB images x given a pattern of cone excitations.</p><p>Define G as the transformation between any x and its corresponding grayscale image y (i.e. G could simply add the R, G, B values at each pixel location and divide by 3). Then, the posterior over is computed as:<disp-formula id="sa2equ1"><mml:math id="sa2m1"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where δ(⋅) is the vector-valued delta function. It is possible that the MAP estimate under this marginalized posterior would yield improved grayscale reconstructions.</p><p>Another quite interesting approach would be to provide an explicit loss function, and rather than choosing the MAP reconstruction, choose the reconstruction that minimizes the expected (over the posterior loss). The marginalization approach may be thought of as a special case of the loss function approach, where the loss function is set to be sensitive only to grayscale reconstruction error (e.g. <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>G</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We did introduce the idea of an explicit loss function in the paper (see Page 9), and now have added a note indicating the MAP estimate does not in general minimize the expected loss in Footnote 3 of Page 9.</p><p>The challenge of implementing the more sophisticated approaches described above, however, is that the integration over the high-dimensional p(x) is computationally intractable. Although various approximations exist in the literature, exploring those is beyond the scope of the current paper.</p><p>Manning and Brainard (2009, as cited in the manuscript) do treat in detail the closely related issue of how the optimal choice of photoreceptor mosaic varies with overall SNR, for a simplified model system that allowed exhaustive computational exploration using a reconstruction approach. Their conclusion is that the reason nocturnal visual systems typically utilize a single photoreceptor class is that one class of receptor will inevitably have better SNR than the others, and that as the overall SNR drops, the benefit of utilizing multiple receptor types to provide color vision is outweighed by the benefit of having all of the photoreceptors be of the class that has the best SNR. Conversely, at higher SNR a visual system can afford to intersperse additional receptor classes with lower SNR to gain the benefits of color vision. We think the reviewer may find that paper of interest, although it does not directly address the specific question raised here, of what would happen if the goal of vision changed as a function of how well that goal could be accomplished.</p><p>As interesting as we find this topic, in terms of the manuscript we have chosen to expand our discussion only slightly and point to a larger set of references (this passage also referenced in response to Comment 6.4 above), as we think going further will take the reader too far afield.</p><p>Page 16, Line 392. “This observation may be connected to the fact that biological visual systems that operate at low light levels are typically monochromatic, potentially to increase the SNR of spatial vision at the cost of completely disregarding color (e.g., the monochromatic human rod system; see Manning and Brainard 2009 for a related and more detailed treatment; also see Walls 1942; Rushton 1962; Van Hateren 1993; Land and Osorio 2003).”</p><disp-quote content-type="editor-comment"><p>10. Lines 525-533. Other species like zebrafish have a much more limited range of tasks to perform than humans. Is image reconstruction still the appropriate cost function in those cases?</p></disp-quote><p>We agree that cross-species differences in the tasks supported by visual perception are likely an important consideration. We think an interesting way to approach this in the long run would be to incorporate an explicit loss function into the formulation, and then consider what loss function might be most appropriate for each species under consideration (see discussion of loss functions in response to Comment 9.2 above). Beyond the computational challenges involved, doing this would also require detailed investigation about what the right loss function for a zebrafish is, and how that differs from the corresponding human loss function.</p><p>We have expanded the related Discussion section:</p><p>Page 25, Line 620. “Further study that characterizes in detail the natural scene statistics of the zebrafish’s environment might help us to better understand this question (Zimmermann et al., 2018; Cai et al., 2020). It would also be interesting to incorporate into the formulation an explicit specification of how the goal of vision might vary across species. One extension to the current approach to incorporate this would be to specify an explicit loss function for each species and find the reconstruction that minimizes the expected (over the posterior of images) loss (Berger 1985), although implementing this approach would be computationally challenging. Related is the task-specific accuracy maximization analysis formulation (Burge and Geisler 2011; see Burge 2020 for a review).”</p></body></sub-article></article>