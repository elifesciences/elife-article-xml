<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">101747</article-id><article-id pub-id-type="doi">10.7554/eLife.101747</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101747.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dynamics of striatal action selection and reinforcement learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0930-7327</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Markowitz</surname><given-names>Jeffrey</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2362-1937</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Gillis</surname><given-names>Winthrop F</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Datta</surname><given-names>Sandeep R</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2422-6576</contrib-id><email>a.litwin-kumar@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Kavli Institute for Brain Science, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zkghx44</institution-id><institution>Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology and Emory University</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Neurobiology, Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Langdon</surname><given-names>Angela</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>08</day><month>05</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP101747</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-08-16"><day>16</day><month>08</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-07-25"><day>25</day><month>07</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.14.580408"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-06"><day>06</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101747.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-04-08"><day>08</day><month>04</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101747.2"/></event></pub-history><permissions><copyright-statement>© 2024, Lindsey et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Lindsey et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-101747-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-101747-figures-v1.pdf"/><abstract><p>Spiny projection neurons (SPNs) in dorsal striatum are often proposed as a locus of reinforcement learning in the basal ganglia. Here, we identify and resolve a fundamental inconsistency between striatal reinforcement learning models and known SPN synaptic plasticity rules. Direct-pathway (dSPN) and indirect-pathway (iSPN) neurons, which promote and suppress actions, respectively, exhibit synaptic plasticity that reinforces activity associated with elevated or suppressed dopamine release. We show that iSPN plasticity prevents successful learning, as it reinforces activity patterns associated with negative outcomes. However, this pathological behavior is reversed if functionally opponent dSPNs and iSPNs, which promote and suppress the current behavior, are simultaneously activated by efferent input following action selection. This prediction is supported by striatal recordings and contrasts with prior models of SPN representations. In our model, learning and action selection signals can be multiplexed without interference, enabling learning algorithms beyond those of standard temporal difference models.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>synaptic plasticity</kwd><kwd>basal ganglia</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011671</institution-id><institution>Mathers Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000861</institution-id><institution>Burroughs Wellcome Fund</institution></institution-wrap></funding-source><award-id>Career Award at the Scientific Interface</award-id><principal-award-recipient><name><surname>Markowitz</surname><given-names>Jeffrey</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Sloan Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Markowitz</surname><given-names>Jeffrey</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000008</institution-id><institution>David and Lucile Packard Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Markowitz</surname><given-names>Jeffrey</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RF1AG073625</award-id><principal-award-recipient><name><surname>Datta</surname><given-names>Sandeep R</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS114020</award-id><principal-award-recipient><name><surname>Datta</surname><given-names>Sandeep R</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U24NS109520</award-id><principal-award-recipient><name><surname>Datta</surname><given-names>Sandeep R</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014370</institution-id><institution>Simons Foundation Autism Research Initiative</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Datta</surname><given-names>Sandeep R</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000872</institution-id><institution>McKnight Endowment Fund for Neuroscience</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A theory of striatal synaptic plasticity separates activity related to learning and action execution into non-interfering subspaces.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Numerous studies have proposed that the basal ganglia is a reinforcement learning system (<xref ref-type="bibr" rid="bib32">Joel et al., 2002</xref>; <xref ref-type="bibr" rid="bib49">Niv, 2009</xref>; <xref ref-type="bibr" rid="bib30">Ito and Doya, 2011</xref>). Reinforcement learning algorithms use experienced and predicted rewards to learn to predict the expected future reward associated with an organism’s current state, and the action to select, in order to maximize this reward (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 2018</xref>). Spiny projection neurons (SPNs) in the striatum are well-positioned to take part in such an algorithm, as they receive diverse contextual information from the cerebral cortex and are involved in both action selection (in dorsal striatum; <xref ref-type="bibr" rid="bib51">Packard and Knowlton, 2002</xref>; <xref ref-type="bibr" rid="bib58">Seo et al., 2012</xref>; <xref ref-type="bibr" rid="bib3">Balleine et al., 2007</xref>) and value prediction (in ventral striatum; <xref ref-type="bibr" rid="bib9">Cardinal et al., 2002</xref>; <xref ref-type="bibr" rid="bib48">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib50">O’Doherty et al., 2004</xref>). Moreover, plasticity of SPN input synapses is modulated by midbrain dopamine release (<xref ref-type="bibr" rid="bib67">Wickens et al., 1996</xref>; <xref ref-type="bibr" rid="bib8">Calabresi et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Contreras-Vidal and Schultz, 1999</xref>). A variety of studies support the view that this dopamine release reflects reward prediction error (<xref ref-type="bibr" rid="bib57">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib48">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib27">Houk and Adams, 1995</xref>), which in many reinforcement learning algorithms is the key quantity used to modulate learning (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib49">Niv, 2009</xref>).</p><p>Despite these links, several aspects of striatal physiology are difficult to reconcile with reinforcement learning models. SPNs are classified in two main types – direct-pathway (dSPNs) and indirect-pathway (iSPNs). These two classes of SPNs exert opponent effects on action based on perturbation data (<xref ref-type="bibr" rid="bib36">Kravitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib25">Freeze et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Lee and Sabatini, 2021</xref>), but also exhibit highly correlated activity (<xref ref-type="bibr" rid="bib14">Cui et al., 2013</xref>). Moreover, dSPNs and iSPNs express different dopamine receptors (D1- and D2-type) and thus undergo synaptic plasticity according to different rules. In particular, dSPN inputs are potentiated when coincident pre- and post-synaptic activity is followed by above-baseline dopamine activity, while iSPN inputs are potentiated when coincident pre- and post-synaptic activity is followed by dopamine suppression (<xref ref-type="bibr" rid="bib59">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="bib24">Frank, 2005</xref>; <xref ref-type="bibr" rid="bib29">Iino et al., 2020</xref>).</p><p>Prior studies have proposed that dSPNs learn from positive reinforcement to promote actions, and iSPNs learn from negative reinforcement to suppress actions (<xref ref-type="bibr" rid="bib13">Cruz et al., 2022</xref>; <xref ref-type="bibr" rid="bib11">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib31">Jaskir and Frank, 2023</xref>; <xref ref-type="bibr" rid="bib65">Varin et al., 2023</xref>; <xref ref-type="bibr" rid="bib44">Mikhael and Bogacz, 2016</xref>; <xref ref-type="bibr" rid="bib17">Dunovan et al., 2019</xref>). However, we will show that a straightforward implementation of such a model fails to yield a functional reinforcement learning algorithm, as the iSPN learning rule assigns blame for negative outcomes to the wrong actions. Correct learning in this scenario requires a mechanism to selectively update corticostriatal weights corresponding to the chosen action, which is absent in prior models (see Discussion).</p><p>In this work, we begin by rectifying this inconsistency between standard reinforcement learning models of the striatum and known SPN plasticity rules. The iSPN learning rule reported in the literature reinforces patterns of iSPN activity that are associated with dopamine suppression, increasing the likelihood of repeating decisions that previously led to negative outcomes. We show that this pathological behavior is reversed if, after action selection, opponent dSPNs and iSPNs receive correlated efferent input encoding the animal’s selected action. A central contribution of our model is a decomposition of SPN activity into separate modes for action selection and for learning, the latter driven by this efferent input. This decomposition provides an explanation for the apparent paradox that the activities of dSPNs and iSPNs are positively correlated despite their opponent causal functions (<xref ref-type="bibr" rid="bib14">Cui et al., 2013</xref>), and provides a solution to the problem of multiplexing signals related to behavioral execution and learning. The model also makes predictions about the time course of SPN activity, including that dSPNs and iSPNs that are responsible for regulating the same behavior (promoting and suppressing it, respectively) should be co-active following action selection. This somewhat counterintuitive prediction contrasts with prior proposals that dSPNs that promote an action are co-active with iSPNs that suppress different actions (<xref ref-type="bibr" rid="bib46">Mink, 1996</xref>; <xref ref-type="bibr" rid="bib55">Redgrave et al., 1999</xref>). We find support for this prediction in experimental recordings of dSPNs and iSPNs during spontaneous behavior.</p><p>Next, we show that the nonuniformity of dSPN and iSPN plasticity rules enables more sophisticated learning algorithms than can be achieved in models with a single plasticity rule. In particular, it enables the striatum to implement so-called <italic>off-policy</italic> reinforcement learning algorithms, in which the corticostriatal pathway learns from the the outcomes of actions that are driven by other neural pathways. Off-policy algorithms are commonly used in state-of-the-art machine learning models, as they dramatically improve learning efficiency by facilitating learning from expert demonstrations, mixture-of-experts models, and replayed experiences (<xref ref-type="bibr" rid="bib1">Arulkumaran et al., 2017</xref>). Following the implications of this model further, we show that off-policy algorithms require a dopaminergic signal in dorsal striatum that combines classic state-based reward prediction error with a form of action prediction error. We confirm a key signature of this prediction in recent dopamine data collected from dorsolateral striatum (DLS) during spontaneous behavior.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In line with previous experimental (<xref ref-type="bibr" rid="bib67">Wickens et al., 1996</xref>; <xref ref-type="bibr" rid="bib8">Calabresi et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Contreras-Vidal and Schultz, 1999</xref>) and modeling (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib49">Niv, 2009</xref>) studies, we model plasticity of corticostriatal synapses using a three-factor learning rule, dependent on coincident presynaptic activity, post-synaptic activity, and dopamine release (<xref ref-type="fig" rid="fig1">Figure 1A, B</xref>). Concretely, we model plasticity of the weight <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> of a synapse from a cortical neuron with activity <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> onto a dSPN or iSPN with activity <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Corticostriatal action selection circuits and plasticity rules.</title><p>(<bold>A</bold>) Left, diagram of cortical inputs to striatal populations. Right, illustration of action selection architecture. Populations of dSPNs (blue) and iSPNs (red) in dorsolateral striatum (DLS) are responsible for promoting and suppressing specific actions, respectively. Active neurons (shaded circles) illustrate a pattern of activity consistent with typical models of striatal action selection, in which dSPNs that promote a chosen action and iSPNs that suppress other actions are active. (<bold>B</bold>) Illustration of three-factor plasticity rules at spiny projection neuron (SPN) input synapses, in which adjustments to corticostriatal synaptic weights depend on pre-synaptic cortical activity, SPN activity, and dopamine release. (<bold>C</bold>) Illustration of different models of the dopamine-dependent factor <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> in dSPN (blue) and iSPN (red) plasticity rules.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig1-v1.tif"/></fig><p>where <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula> represents dopamine release relative to baseline, and the functions <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> model the dependence of the two plasticity rules on dopamine concentration.</p><p>For dSPNs, the propensity of input synapses to potentiate increases with increasing dopamine concentration, while for iSPNs the opposite is true. This observation is corroborated by converging evidence from observations of dendritic spine volume, intracellular PKA measurements, and spike-timing-dependent plasticity protocols (<xref ref-type="bibr" rid="bib59">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="bib26">Gurney et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Iino et al., 2020</xref>; <xref ref-type="bibr" rid="bib37">Lee et al., 2021</xref>). For the three-factor plasticity rule above, these findings imply that <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is an increasing function of <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula> while <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is a decreasing function. Prior modeling studies have proposed specific plasticity rules that correspond to different choices of <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, some examples of which are shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</p><sec id="s2-1"><title>iSPN plasticity rule impedes successful reinforcement learning</title><p>Prior work has proposed that dSPNs activate when actions are performed and iSPNs activate when actions are suppressed (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). When an animal selects among multiple actions, subpopulations of dSPNs are thought to promote the selected action, while other subpopulations of iSPNs inhibit the unchosen actions (<xref ref-type="bibr" rid="bib46">Mink, 1996</xref>; <xref ref-type="bibr" rid="bib55">Redgrave et al., 1999</xref>). We refer to this general description as the ‘canonical action selection model’ of SPN activity and show that this model, when combined with the plasticity rules above, fails to produce a functional reinforcement learning algorithm. This failure is specifically due to the iSPN plasticity rule. Later, we also show that the SPN representation predicted by the canonical action selection model is inconsistent with recordings of identified dSPNs and iSPNs. We begin by analyzing a toy model of an action selection task with two actions, one of which is rewarded. In the model, the probability of selecting an action is increased when the dSPN corresponding to that action is active and decreased when the corresponding iSPN is active. After an action is taken, dopamine activity reports the reward prediction error, increasing when reward is obtained and decreasing when it is not.</p><p>It is easy to see that the dSPN plasticity rule in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is consistent with successful reinforcement learning (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Suppose action 1 is selected, leading to reward (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, center). The resulting dopamine increase potentiates inputs to the action 1 dSPN from cortical neurons that are active during the task, making action 1 more likely to be selected in the future (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Consequences of the canonical action selection model of spiny projection neuron (SPN) activity.</title><p>(<bold>A</bold>) Example in which dSPN plasticity produces correct learning. Left: cortical inputs to the dSPN and iSPN are equal prior to learning. Shading of corticostriatal connections indicates synaptic weight, and shading of blue and red circles denotes dSPN/iSPN activity. Middle: action 1 is selected, corresponding to elevated activity in the dSPN that promotes action 1 and the iSPN that suppresses action 2. In this example, action 1 leads to reward and increased DA activity. This potentiates the input synapse to the action 1-promoting dSPN and (depending on the learning rule, see <xref ref-type="fig" rid="fig1">Figure 1</xref>) depresses the input to the action 2-suppressing iSPN. Right: in a subsequent trial, cortical input to the action 1-promoting dSPN is stronger, increasing the likelihood of selecting action 1. Here, the dSPN-mediated effect of increasing action 1’s probability overcomes the iSPN-mediated effect of decreasing action 2’s probability. (<bold>B</bold>) Example in which iSPN plasticity produces incorrect learning. Same as A, but in a scenario in which action 2 is selected leading to punishment and a corresponding decrease in DA activity. As a result, the input synapse to the action 2-promoting dSPN is (depending on the learning rule) depressed, and the input to the action 1-suppressing iSPN is potentiated. On a subsequent trial, the probability of selecting action 2 rather than action 1 is greater, despite action 2 being punished. Note that the dSPN input corresponding to action 2 is (potentially) weakened, which correctly decreases the probability of selecting action 2, but this effect is not sufficient to overcome the strengthened action 1 iSPN activity. (<bold>C</bold>) Performance of a simulated striatal reinforcement learning system in go/no-go tasks with different reward contingencies. (<bold>D</bold>) Same as C, but for action selection tasks with two cortical input states, two available actions, and one correct action per state, under different reward protocols.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Go/no-go task.</title><p>(<bold>A</bold>) Example in which dSPN plasticity produces correct learning behavior in a go/no-go task. Left: cortical inputs to the dSPN and iSPN are equal prior to learning. Shading of corticostriatal connections indicates synaptic weight, and shading of blue and red circles denotes dSPN/iSPN activity. Middle: the ‘go’ response is selected, corresponding to elevated dSPN activity. In this example, the ‘go’ response is rewarded, leading to elevated DA activity and thus potentiation of the dSPN input synapse. Right: in a subsequent trial, cortical input to the dSPN is stronger, increasing the likelihood of selecting the ‘go’ response. (<bold>B</bold>) Example in which iSPN plasticity produces incorrect learning behavior in a go/no-go task. Left: same as panel A, left. Middle: the ‘no-go’ response is selected, corresponding to elevated iSPN activity. In this example, the ‘no-go’ response is punished, leading to decreased DA activity and thus potentiation of the iSPN input synapse. Right: in a subsequent trial, cortical input to the iSPN is stronger, decreasing the likelihood of selecting the ‘go’ response. (<bold>C</bold>) Illustration of the efference model in a go/no-go task. Left: feedforward SPN activity driven by cortical inputs. Right: once the ‘go’ response is selected, the dSPN and iSPN are both excited by efferent input, which is combined with their original input. As a result, both the dSPN and iSPN are more active than prior to action selection, but the dSPN is still more active than the iSPN.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig2-figsupp1-v1.tif"/></fig></fig-group><p>At first glance, it may seem that a similar logic would apply to iSPNs, since their suppressive effect on behavior and reversed dependence on dopamine concentration are both opposite to dSPNs. However, a more careful examination reveals that the iSPN plasticity rule in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> does not promote successful learning. In the canonical action selection model, dSPNs promoting a selected action and iSPNs inhibiting unselected actions are active. If a negative outcome is encountered leading to a dopamine decrease, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> predicts that inputs to iSPNs corresponding to unselected actions are strengthened (LTP in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, center). This makes the action that led to the negative outcome <italic>more</italic> rather than less likely to be taken when the same cortical inputs are active in the future (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right). More generally, the model demonstrates that, while the plasticity rule of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> correctly reinforces dSPN activity patterns that lead to positive outcomes, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> incorrectly reinforces iSPN activity patterns that lead to negative outcomes. The function of iSPNs in inhibiting action does not change the fact that such reinforcement is undesirable.</p><p>We note that, depending on the learning rule (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), inputs to dSPNs that promote the selected action may be weakened (LTD in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, left), which correctly disincentivizes the action that led to a negative outcome. However, this dSPN effect competes with the pathological behavior of the iSPNs and is often unable to overcome it. We also note that, if dopamine increases lead to depression of iSPN inputs (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, center, right), positive outcomes will lead to actions that were correctly being inhibited by iSPNs to be less inhibited in the future. Thus, both positive and negative outcomes may cause incorrect iSPN learning. Some sources suggest that while dopamine suppression increases D2 receptor activation, dopamine increase has little effect on D2 receptors (<xref ref-type="bibr" rid="bib16">Dreyer et al., 2010</xref>), corresponding to the rectified model of <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, left). In this case, pathological iSPN plasticity behavior still manifests when dopamine activity is suppressed (as in the examples of <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>We simulated learning of multiple tasks with the three-factor plasticity rules above, with dopamine activity modeled as reward prediction error obtained using a temporal difference (TD) learning rule. In a go/no-go task with one cue in which the ‘go’ action is rewarded (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), the system learns the wrong behavior when negative performance feedback is provided on no-go trials, and thus iSPN plasticity is the main driver of learning (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We also simulated a two-alternative forced choice task in which there are two cues (corresponding to different cortical input patterns), each with a corresponding target action. When performance feedback consists of rewards for correct actions, the system learns the task, as dSPNs primarily drive the learning. However, when instead performance feedback consists of giving punishments for incorrect actions, the system does not learn the task, as iSPNs primarily drive the learning (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). We note that, in principle, this problem could be avoided if the learning rate of iSPNs were very small compared to that of dSPNs, ensuring that reinforcement learning is always primarily driven by the dSPN pathway (leaving iSPNs to potentially perform a different function). However, this alternative would be inconsistent with prior studies indicating a significant role for the indirect pathway in reinforcement learning (<xref ref-type="bibr" rid="bib53">Peak et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Lee and Sabatini, 2021</xref>). The model we introduce below makes use of contributions to learning from both pathways.</p></sec><sec id="s2-2"><title>Efferent activity in SPNs enables successful reinforcement learning</title><p>We have shown that the canonical action selection model, when paired with <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>, produces incorrect learning. What pattern of SPN activity would produce correct learning? In the model, the probability of selecting an action is determined by the ‘difference mode’ <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are the activities of dSPN and iSPN neurons associated with that action. We analyzed how the plasticity rule of <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> determines changes to this difference mode. In the simplest case in which the SPN firing rate is a linear function of cortical input (i.e., <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and plasticity’s dependence on dopamine concentration is also linear (i.e., <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mo>±</mml:mo><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1C</xref>, center), the change in the probability of selecting an action due to learning is<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Changes to the ‘difference mode’ <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are therefore driven by the ‘sum mode’ <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. This implies that the activity pattern that leads to correct learning about an action’s outcome is different from the activity pattern that selects the action. To promote or inhibit, respectively, an action that leads to a dopamine increase or decrease, this analysis predicts that both dSPNs that promote and iSPNs that inhibit the action should be co-active. A more general argument applies for other learning rules and firing rate nonlinearities: as long as <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is an increasing function of total input current, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> has positive slope, and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> has negative slope, changes in difference mode activity will be positively correlated with sum mode activity (see Appendix).</p><p>The key insight of the above argument is that the pattern of SPN activity needed for learning involves simultaneous excitation of dSPNs that promote the current behavior and iSPNs that inhibit it. This differs from the pattern of activity needed to drive selection of that behavior in the first place. We therefore propose a model in which SPN activity contains a substantial <italic>efferent</italic> component that follows action selection and promotes learning, but has no causal impact on behavior. In the model, feedforward corticostriatal inputs initially produce SPN activity whose difference mode causally influences action selection, consistent with the canonical model (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, left). When an action is performed, both dSPNs and iSPNs responsible for promoting or inhibiting that action receive efferent excitatory input, producing sum mode activity. Following this step, SPN activity reflects both contributions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, center). The presence of sum mode activity leads to correct synaptic plasticity and learning (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right). Unlike the canonical action selection model (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), this model thus predicts an SPN representation in which, after an action is selected, the most highly active neurons are those responsible for regulating that behavior and not other behaviors.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The efference model of spiny projection neuron (SPN) activity.</title><p>(<bold>A</bold>) Illustration of the efference model in an action selection task. Left: feedforward SPN activity driven by cortical inputs. Center: once action 2 is selected, efferent inputs excite the dSPN and iSPN responsible for promoting and suppressing action 2. Efferent activity is combined with feedforward activity, such that the action 2-associated dSPNs and iSPNs are both more active than the action 1 dSPNs and iSPNs, but the relative dSPN and iSPN activity for each action remains unchanged. This produces strong LTD and LTP in the action 2-associated dSPNs and iSPNs upon a reduction in dopamine activity. Right: in a subsequent trial, this plasticity correctly reduces the likelihood of selecting action 2. (<bold>B</bold>) The activity levels of the dSPN and iSPN populations that promote and suppress a given action can be plotted in a two-dimensional space. The difference mode influences the probabiility of taking that action, while the sum mode drives future changes to activity in the difference mode via plasticity. Efferent activity excites the sum mode. (<bold>C</bold>) Performance of a striatal RL system using the efference model on the tasks of <xref ref-type="fig" rid="fig2">Figure 2C</xref>. (<bold>D</bold>) Performance of a striatal RL system using the efference model on the tasks of <xref ref-type="fig" rid="fig2">Figure 2D</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Comparison of canonical action selection and efference models with a distributed action code.</title><p>Performance of striatal RL models with a distributed code for actions on a task with 10 cortical input states, 10 available actions, and one correct action for each input state (see Appendix).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig3-figsupp1-v1.tif"/></fig></fig-group><p>In SPN activity space, the sum and difference modes are orthogonal to one another. This orthogonality has two consequences. First, it implies that encoding the action in the difference mode (as in the canonical action selection model) produces synaptic weight changes that do not promote learning, consistent with the competing effects of dSPN and iSPN plasticity that we previously described. Second, it implies that adding efferent activity along the sum mode, which produces correct learning, has no effect on action selection. The model thus provides a solution to the problem of interference between ‘forward pass’ (action selection) and ‘backward pass’ (learning) activity, a common issue in models of biologically plausible learning algorithms (see Discussion).</p><p>In simulations, we confirm that unlike the canonical action selection model, this efference model solves go/no-go (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) and action selection (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) tasks regardless of the reward protocol. Although the derivation above assumes linear SPN responses and linear dependences of plasticity on dopamine concentration, our model enables successful learning even using a nonlinear model of SPN responses and a variety of plasticity rules (<xref ref-type="fig" rid="fig3">Figure 3C, D</xref>; see Appendix for a derivation that explains this general success). Finally, we also confirmed that our results apply to cases in which actions are associated with distributed modes of dSPN and iSPN activity, and with a larger action space (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This success arises from the ability to form orthogonal subspaces for action selection and learning in this distributed setting. Although we describe the qualitative behavior of our model using discrete action spaces for illustrative purposes, we expect distributed representations to be more faithful to neural recordings.</p></sec><sec id="s2-3"><title>Temporal dynamics of the efference model</title><p>We simulated a two-alternative forced choice task using a firing rate model of SPN activity. This allowed us to directly visualize dynamics in the sum and difference modes and verify that the efference model prevents interference between them. In each trial of the forced choice task, one of two stimuli is presented and one of two actions is subsequently selected (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top row). The selected action is determined by the difference mode activity of action-encoding SPNs during the first half of the stimulus presentation period. The sum mode is activated by efferent input during the second half of this period. Reward is obtained if the correct action is selected in a trial, and each stimulus has a different corresponding correct action. Plasticity of cortical weights encoding stimulus identity onto SPNs is governed by <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Temporal dynamics of the efference model in a two-alternative forced choice task.</title><p>(<bold>A</bold>) Top row: in each trial, either stimulus 1 (magenta) or stimulus 2 (green) is presented for 2 s. After 1 s, either action 1 (magenta) or action 2 (green) is selected based on spiny projection neuron (SPN) activity. A correct trial is one in which action 1 (resp. 2) is selected after stimulus 1 (resp. 2) is presented. Second row: firing rates of four SPNs. Dark and light colors denote SPNs that represent actions 1 and 2, respectively. Third and fourth rows: projection of SPN activity onto difference and sum modes for actions 1 and 2. (<bold>B</bold>) Same as A, but illustrating the first trial, in which stimulus 2 is presented and action 1 is incorrectly selected. (<bold>C</bold>) Same as B, but illustrating the last trial, in which stimulus 1 is presented and action 1 is correctly selected.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig4-v1.tif"/></fig><p>The model learned the correct policy in about 10 trials. Early in learning, difference mode activity is small and primarily driven by noise, leading to random action selection (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). However, sum mode activity is strongly driven after an action is selected (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, bottom). As learning progresses, the magnitude of the difference mode activity evoked by the stimulus increases (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, third row). Late in learning, dSPN and iSPN firing rates are more separable during stimulus presentation, leading to correct action selection (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, second row). Both difference and sum mode activity is evident late in learning, with the former leading the latter (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom two rows).</p><p>Throughout the learning process, difference and sum mode activity for the two actions are separable and non-interfering, even when both are present simultaneously. As a result, action selection is not disrupted by efferent feedback. We conclude that the efference model multiplexes action selection and learning signals without separate learning phases or gated plasticity rules. While we illustrated this in a task with sequential trials for visualization purposes, this non-interference enables learning based on delayed reward and efferent feedback from past actions even as the selection of subsequent actions unfolds.</p></sec><sec id="s2-4"><title>Efference model predicts properties of SPN activity</title><p>Thus far, we have provided theoretical arguments and model simulations that suggest that simultaneous efferent input to opponent dSPNs and iSPNs is necessary for reinforcement learning, given known plasticity rules. We next sought to test this prediction in neural data. We predict these dynamics to be particularly important in scenarios where the action space is large and actions are selected continuously, without a clear trial structure. We therefore used data from a recent study which recorded bulk and cellular dSPN and iSPN activity in spontaneously behaving mice (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>). As no explicit rewards or task structure were provided during recording sessions, we adopted a modeling approach that makes minimal assumptions about the inputs to SPNs besides the core prediction of efferent activity. Specifically, we used a network model in which (1) populations of dSPNs and iSPNs promote or suppress different actions, (2) the feedforward inputs to all SPNs are random, (3) actions are sampled with log-likelihoods scaled according to the associated dSPN and iSPN difference mode, and (4) efferent activity excites the sum mode corresponding to the chosen action.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Comparisons of model predictions about bulk dSPN and iSPN activity to experimental data.</title><p>(<bold>A</bold>) Schematic of experimental setup, taken from <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>. Neural activity and kinematics of spontaneously behaving mice are recorded, and behavior is segmented into stereotyped ‘behavioral syllables’ using the MoSeq pipeline. (<bold>B</bold>) In simulation of efference model with random feedforward cortical inputs, cross-correlation of total dSPN and iSPN activity. (<bold>C</bold>) Cross-correlation between fiber photometry recordings of bulk dSPN and iSPN activity in freely behaving mice, using the data from <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>. Line thickness indicates standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Reversed indicator analysis.</title><p>Same as <xref ref-type="fig" rid="fig5">Figure 5C</xref>, but performing the analysis on subjects with reversed assignment of indicators to spiny projection neuron (SPN) types.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig5-figsupp1-v1.tif"/></fig></fig-group><p>In this model, difference mode dSPN and iSPN activity drives behaviors, and those behaviors cause efferent activation of the corresponding sum mode. As a result, on average, dSPN activity tends to lead to increased future iSPN activity, while iSPN activity leads to decreased future dSPN activity. Consequently, the temporal cross-correlation between total dSPN and iSPN activity is asymmetric, with present dSPN activity correlating more strongly with future iSPN activity than with past iSPN activity (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Such asymmetry is not predicted by the canonical action selection model, or models that assume dSPNs and iSPNs are co-active. Computing the temporal cross-correlation in the bulk two-color photometry recordings of dSPN and iSPN activity, we find a very similar skewed relationship in the data (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). We confirmed this result is not an artifact of the use of different indicators for dSPN and iSPN activity by repeating the analysis on data from mice where the indicators were reversed and finding the same result (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>Our model makes even stronger predictions about SPN population activity and its relationship to action selection. First, it predicts that both dSPNs and iSPNS exhibit similar selectivities in their tuning to actions. This contrasts with implementations of the canonical action selection model in which iSPNs are active whenever their associated action is not being performed and thus are more broadly tuned than dSPNs (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Second, it also predicts that efferent activity excites dSPNs that promote the currently performed action and iSPNs that suppress the currently performed action. As a result, dSPNs whose activity increases during the performance of a given action should tend to be above baseline shortly prior to the performance of that action. By contrast, iSPNs whose activity increases during an action should tend to be below baseline during the same time interval (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, left; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). Moreover, this effect should be action-specific: the dSPNs and iSPNs whose activity increases during a given action should display negligible average fluctuations around the onset of other actions (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, right). These predictions can also be reinterpreted in terms of the sum and difference modes. The difference mode activity associated with an action is elevated prior to selection of that action, while the sum mode activity is excited following action selection (<xref ref-type="fig" rid="fig4">Figures 4C</xref> and <xref ref-type="fig" rid="fig6">6B</xref>). These two phases of difference and sum mode activity are not predicted by the canonical action selection model.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparisons of model predictions about action-tuned spiny projection neuron (SPN) subpopulations to experimental data.</title><p>(<bold>A</bold>) Activity of dSPNs (blue) and iSPNs (red) around the onset of their associated action (left) or other actions (right) in the simulation from <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>B</bold>) Same information as A, but plotting activity of the sum (dSPN + iSPN) and difference (dSPN − iSPN) modes. (<bold>C</bold>) For an example experimental session, dSPN activity modes associated with each of the behavioral syllables, in <italic>z</italic>-scored firing rate units. (<bold>D</bold>) Correlation between identified dSPN and iSPN activity modes in two random subsamples of the data, for shuffled (left, circles) and real (right, x’s) data. (<bold>E</bold>) Projection of dSPN (blue) and iSPN (red) activity onto the syllable-associated modes identified in panel C, around the onset of the associated syllable (left panel) or other syllables (right panel) averaged across all syllables. Error bars indicate standard error of the mean across syllables. (<bold>F</bold>) Same as panel E, restricting the analysis to mice in which dSPNs and iSPNs were simultaneously recorded. (<bold>G</bold>) Same data as panel F, but plotting activity of the sum (dSPN + iSPN) and difference (dSPN − iSPN) modes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Comparison of dSPN and iSPN tuning selectivity.</title><p>Violin plots indicate the distribution of selectivity values across all neurons computed using <xref ref-type="disp-formula" rid="equ23">Equation 23</xref>, using either unsigned (left) or rectified (right) <italic>z</italic>-scored activity as the raw measure of a neuron’s tuning to a behavioral syllable. Horizontal lines indicate the 0, 25, 75, and 100 percentile values of the distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig6-figsupp1-v1.tif"/></fig></fig-group><p>To test these hypotheses, we used calcium imaging data collected during spontaneous mouse behavior (<xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>). The behavior of the mice was segmented into consistent, stereotyped kinematic motifs referred to as ‘behavioral syllables’, as in previous studies (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We regard these behavioral syllables as the analogs of actions in our model. First, we examined the tuning of dSPNs and iSPNs to different actions and found that, broadly consistent with what our model predicts, both subpopulations exhibit similar selectivities (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Next, to test our predictions about dynamics before and after action selection (<xref ref-type="fig" rid="fig6">Figure 6A, B</xref>), we identified, for each syllable, dSPN and iSPN population activity vectors (modes) that increased the most during performance of that syllable (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). We confirmed that these modes are meaningful by checking that modes identified using two disjoint subsets of the data are correlated (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). We then plotted the activity of these modes around the time of onset of the corresponding syllable, and averaged the result across the choice of syllables (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). The result displays remarkable agreement with the model prediction in <xref ref-type="fig" rid="fig6">Figure 6A</xref>.</p><p>The majority of the above data consisted of recordings of either dSPNs or iSPNs from a given mouse. However, in a small subset (<italic>n</italic> = 4) of mice, dSPNs and iSPNs were simultaneously recorded and identified. We repeated the analysis above on these sessions, and found the same qualitative results (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). The simultaneous recordings further allowed us to visualize the sum and difference mode activity (<xref ref-type="fig" rid="fig6">Figure 6G</xref>), which also agrees with the predictions of our model (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p></sec><sec id="s2-5"><title>Efference model enables off-policy reinforcement learning</title><p>Prior studies have argued for the importance of motor efference copies during basal ganglia learning, in particular when action selection is influenced by other brain regions (<xref ref-type="bibr" rid="bib21">Fee, 2014</xref>; <xref ref-type="bibr" rid="bib39">Lindsey and Litwin-Kumar, 2022</xref>). Indeed, areas such as the motor cortex and cerebellum drive behavior independent of the basal ganglia (<xref ref-type="bibr" rid="bib18">Exner et al., 2002</xref>; <xref ref-type="bibr" rid="bib68">Wildgruber et al., 2001</xref>; <xref ref-type="bibr" rid="bib2">Ashby et al., 2010</xref>; <xref ref-type="bibr" rid="bib61">Silveri, 2021</xref>; <xref ref-type="bibr" rid="bib6">Bostan and Strick, 2018</xref>). Actions taken by an animal may therefore at times differ from those most likely to be selected by striatal outputs (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), and it may be desirable for corticostriatal synapses to learn about the consequences of these actions.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The efference model enables off-policy reinforcement learning.</title><p>(<bold>A</bold>) Illustration of the efference model when the striatum shares control of behavior with other pathways. In this example, striatal activity biases the action selection toward choosing action 2, but other neural pathways override the striatum and cause action 1 to be selected instead (left). Following action selection, efferent activity excites the dSPN and iSPN associated with action 1. However, the action probability readouts of the striatal population remain unchanged. (<bold>B</bold>) Performance of RL models in a simulated action selection task (10 cortical states, 10 available actions, in each state one of the actions results in a reward of 1 and the others result in zero reward). Control is shared between the striatal RL circuit and another pathway that biases action selection toward the correct action. Different lines indicate different strength of striatal control relative to the strength of the other pathway. Line style (dashed or solid) indicates the efference model: off-policy efference excites spiny projection neurons (SPNs) associated with the selected action, while on-policy efference excites SPNs associated with the action most favored by the striatum. (<bold>C</bold>) Schematic of different reinforcement learning models of dopamine activity. The standard temporal difference (TD) error model predicts that dopamine activity is sensitive to reward, the predicted value of the current state, and the predicted value of the previous state. The Q-learning error model predicts sensitivity to reward, the predicted value of the current state, and the predicted value of the previous state–action pair. (<bold>D</bold>) In the task of panel B using the off-policy efference model, comparison between different models of dopamine activity as striatal control is varied (the Q-learning error model was used in panel B). (<bold>E</bold>) Correlation between predicted and actual syllable-to-syllable transition matrix. Predictions were made according to different models of the relationship between dopamine activity and behavior, using observed average dopamine activity associated with syllable transitions in the data of <xref ref-type="bibr" rid="bib43">Markowitz et al., 2023</xref>. Each dot indicates a different experimental session.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Comparison to counterfactual model in which iSPNs use the same plasticity rule as dSPNs.</title><p>(<bold>A</bold>) Left: performance of simulated striatal RL system using efference model with the opponent dSPN/iSPN plasticity rules used elsewhere in the paper (black, same as <xref ref-type="fig" rid="fig3">Figure 3E</xref>), and a system using the canonical action selection model and identical dSPN and iSPN plasticity rules (green). Right: same as left panel, but in an off-policy setting in which another pathway controls behavior during and always chooses the correct action, and the performance of the striatal RL system is evaluated over time. Here, the Q-learning model of dopamine activity is used. (<bold>B</bold>) In the counterfactual model in which iSPNs use the same plasticity rule as dSPNs, activity in the difference mode (dSPN − iSPN) influences (via plasticity) changes in future difference mode activity that affect decision-making.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101747-fig7-figsupp1-v1.tif"/></fig></fig-group><p>In the reinforcement learning literature, this kind of learning is known as an ‘off-policy’ algorithm, as the reinforcement learning system (in our model, the striatum) learns from actions that follow a different policy than its own. Off-policy learning has been observed experimentally, for instance in the consolidation of cortically driven behaviors into subcortical circuits including DLS (<xref ref-type="bibr" rid="bib33">Kawai et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Hwang et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Mizes et al., 2023</xref>). Such learning requires efferent activity in SPNs that reflects the actions being performed, rather than the action that would be performed based on the striatum’s influence alone.</p><p>We modeled this scenario by assuming that action selection is driven by weighted contributions from both the striatum and other motor pathways and that the ultimately selected action drives efferent activity (<xref ref-type="fig" rid="fig7">Figure 7A</xref>; see Methods). We found that when action selection is not fully determined by the striatum, such efferent activity is critical for successful learning (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Notably, in our model, efferent activity has no effect on striatal action selection, due to the orthogonality of the sum and difference modes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In a hypothetical alternative model in which the iSPN plasticity rule is the same as that of dSPNs, the efferent activity needed for learning is not orthogonal to the output of the striatum, impairing off-policy learning (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Thus, efferent excitation of opponent dSPNs/iSPNs is necessary both to implement correct learning updates given dSPN and iSPN plasticity rules, and to enable off-policy reinforcement learning.</p></sec><sec id="s2-6"><title>Off-policy reinforcement learning predicts relationship between dopamine activity and behavior</title><p>We next asked whether other properties of striatal dynamics are consistent with off-policy reinforcement learning. We focused on the dynamics of dopamine release, as off-policy learning makes specific predictions about this signal. Standard TD learning models of dopamine activity (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, top) determine the expected future reward (or ‘value’) <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> associated with each state <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> using the following algorithm:<disp-formula id="equ4">,<label>(4)</label><mml:math id="m4"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo stretchy="false">←</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> indicate current and previous states, <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> indicates the currently received reward, <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> is a learning rate factor, and <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the TD error thought to be reflected in phasic dopamine responses. These dopaminergic responses can be used as the learning signal for a updating action selection in dorsal striatum (<xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>), an arrangement commonly referred to as an ‘actor-critic’ architecture (<xref ref-type="bibr" rid="bib49">Niv, 2009</xref>).</p><p>TD learning of a value function <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is an on-policy algorithm, in that the value associated with each state is calculated under the assumption that the system’s future actions will be similar to those taken during learning. Hence, such algorithms are is poorly suited to training an action selection policy in the striatum in situations where the striatum does not fully control behavior, as the values <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> will not reflect the expected future reward associated with a state if the striatum were to dictate behavior on its own. Off-policy algorithms such as Q-learning solve this issue by learning an action-dependent value function <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, which indicates the expected reward associated with taking action <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> in action <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, bottom), via the following algorithm:<disp-formula id="equ6">,<label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This algorithm predicts that the dopamine response <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is action-dependent. The significance of on- versus off-policy learning algorithms can be demonstrated in simulations of operant conditioning tasks in which control of action selection is shared between the striatum and another ‘tutor’ pathway that biases responses toward the correct action. When the striatal contribution to decision-making is weak, it is unable to learn the appropriate response when dopamine activity is modeled as a TD error (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). On the other hand, a Q-learning model of dopamine activity enables efficient striatal learning even when control is shared with another pathway.</p><p>For the spontaneous behavior paradigm we analyzed previously (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), Q-learning but not TD learning of <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> predicts sensitivity of dopamine responses to the likelihood of the previous syllable-to-syllable transition. Using recordings of dopamine activity in the DLS in this paradigm (<xref ref-type="bibr" rid="bib43">Markowitz et al., 2023</xref>), we tested whether a Q-learning model could predict the relationship between dopamine activity and behavioral statistics, comparing it to TD learning of <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and other alternatives (see Appendix). The Q-learning model matches the data significantly better than alternatives (<xref ref-type="fig" rid="fig7">Figure 7E</xref>), providing support for a model of dorsal striatum as an off-policy reinforcement learning system.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have presented a model of reinforcement learning in the dorsal striatum in which efferent activity excites dSPNs and iSPNs that promote and suppress, respectively, the currently selected action. Thus, following action selection, iSPN activity counteruintively represents the action that is inhibited by the currently active iSPN population. This behavior contrasts with previous proposals in which iSPN activity reflects actions being inhibited. This model produces updates to corticostriatal synaptic weights given the known opposite-sign plasticity rules in dSPNs and iSPNs that correctly implement a form of reinforcement learning (<xref ref-type="fig" rid="fig3">Figure 3</xref>), which in the absence of such efferent activity produce incorrect weight updates (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The model makes several novel predictions about SPN activity which we confirmed in experimental data (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>). It also enables multiplexing of action selection signals and learning signals without interference. This facilitates more sophisticated learning algorithms such as off-policy reinforcement learning, which allows the striatum to learn from actions that were driven by other neural circuits. Off-policy reinforcement learning requires dopamine to signal action-sensitive reward predictions errors, which agrees better with experimental recordings of striatal dopamine activity than alternative models (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><sec id="s3-1"><title>Other models of striatal action selection</title><p>Prior models have modeled the opponent effects of dopamine on dSPN and iSPN plasticity (<xref ref-type="bibr" rid="bib24">Frank, 2005</xref>; <xref ref-type="bibr" rid="bib11">Collins and Frank, 2014</xref>; <xref ref-type="bibr" rid="bib31">Jaskir and Frank, 2023</xref>). In these models, dSPNs come to represent the positive outcomes and iSPNs the negative outcomes associated with a stimulus–action pair. Such models can also represent uncertainty in reward estimates (<xref ref-type="bibr" rid="bib44">Mikhael and Bogacz, 2016</xref>). Appropriate credit assignment in these models requires that only corticostriatal weights associated with SPNs encoding the chosen action are updated. Our model clarifies how the neural activity required for such selective weight updates can be multiplexed with the neural activity required for action selection, without requiring separate phases for action selection and learning.</p><p><xref ref-type="bibr" rid="bib5">Bariselli et al., 2019</xref> also argue against the canonical action selection model and propose a competitive role for dSPNs and iSPNs that is consistent with our model. However, the role of efferent activity and distinctions between action- and learning-related signals are not discussed.</p><p>Our model is related to these prior proposals but identifies motor efference as key for appropriate credit assignment across corticostriatal synapses. It also provides predictions concerning the temporal dynamics of such signals (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and a verification of these using physiological data (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p></sec><sec id="s3-2"><title>Other models of efferent inputs to the striatum</title><p>Prior work has pointed out the need for efference copies of decisions to be represented in the striatum, particularly for actions driven by other circuits (<xref ref-type="bibr" rid="bib21">Fee, 2014</xref>). <xref ref-type="bibr" rid="bib24">Frank, 2005</xref> proposes a model in which premotor cortex outputs collateral signals to the striatum that represent the actions under consideration, with the striatum potentially biasing the decision based on prior learning. Through bidirectional feedback (premotor cortex projecting to striatum, and striatum projecting to premotor cortex indirectly through the thalamus) a decision is collectively made by the combined circuit, and the selected action is represented in striatal activity, facilitating learning about the outcome of the action. While similar to our proposal in some ways, this model implicitly assumes that the striatal activity necessary for decision-making is also what is needed to facilitate learning. As we point out in this work, due to the opponent plasticity rules in dSPNs and iSPNs, a post hoc efferent signal that is not causally relevant to the decision-making process is necessary for appropriate learning.</p><p>Other authors have proposed models in which efferent activity is used for learning. In the context of vocal learning in songbirds, <xref ref-type="bibr" rid="bib19">Fee and Goldberg, 2011</xref> proposed that the variability-generating area LMAN, which projects to the song motor pathway, sends collateral projections to Area X, which undergoes dopamine-modulated plasticity. In this model, the efferent inputs to Area X allow it to learn which motor commands are associated with better song performance (signaled by dopamine). Similar to our model, this architecture implements off-policy reinforcement learning in Area X, with HVC inputs to Area X being analogous to corticostriatal projections in our model. However, in our work, the difference in plasticity rules between dSPNs and iSPNs is key to avoiding interference between efferent learning-related activity and feedforward action selection-related activity. A similar architecture was proposed in <xref ref-type="bibr" rid="bib20">Fee, 2012</xref> in the context of oculomotor learning, in which oculomotor striatum receives efferent collaterals from the superior colliculus and/or cortical areas which generate exploratory variability. <xref ref-type="bibr" rid="bib40">Lisman, 2014</xref> also propose a high-level model of striatal efferent inputs similar to ours, and also point out the issue with the iSPN plasticity rule assigning credit to inappropriate actions without efferent inputs. <xref ref-type="bibr" rid="bib56">Rubin et al., 2021</xref> argue that sustained efferent input is necessary for temporal credit assignment when reward is delayed relative to action selection.</p><p>Our model is consistent with these prior proposals, but describes how efferent input must be targeted to opponent SPNs. In our work, the distinction between dSPN and iSPN plasticity rules is key to enable multiplexing of action selection and efferent learning signals without interference. Previous authors have proposed other mechanisms to avoid interference. For instance, <xref ref-type="bibr" rid="bib21">Fee, 2014</xref> proposes that efferent inputs might influence plasticity without driving SPN spiking by synapsing preferentially onto dendritic shafts rather than spines. To avoid action selection-related spikes interfering with learning, the system may employ spike-timing-dependent plasticity rules that are tuned to match the latency at which efferent inputs excite SPNs. While these hypotheses are not mutually exclusive to ours, our model requires no additional circuitry or assumptions beyond the presence of appropriately tuned efferent input (see below) and opposite-sign plasticity rules in dSPNs and iSPNs, due to the orthogonality of the sum and difference modes. An important capability enabled by our model is that action selection and efferent inputs can be multiplexed simultaneously, unlike the works cited above, which posit the existence of temporally segregated action selection and learning phases of SPN activity.</p></sec><sec id="s3-3"><title>Biological substrates of striatal efferent inputs</title><p>Efferent inputs to the striatum must satisfy two important conditions for our model to learn correctly. Neither of these has been conclusively demonstrated, and the two conditions thus represent predictions or assumptions necessary for our model to function. First, they must be appropriately targeted: when an action is performed, dSPNs and iSPNs associated with that action must be excited, but other dSPNs and iSPNs must not be. The striatum receives topographically organized inputs from cortex (<xref ref-type="bibr" rid="bib54">Peters et al., 2021</xref>) and thalamus (<xref ref-type="bibr" rid="bib62">Smith et al., 2004</xref>), with neurons in some thalamic nuclei exhibiting long-latency responses (<xref ref-type="bibr" rid="bib45">Minamimoto et al., 2005</xref>). SPNs tuned to the same behavior tend to be located nearby in space (<xref ref-type="bibr" rid="bib4">Barbera et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Shin et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Klaus et al., 2017</xref>). This anatomical organization could enable action-specific efferent inputs. We note that this does not require a spatially specific dopaminergic signal (<xref ref-type="bibr" rid="bib66">Wärnberg and Kumar, 2023</xref>). In our models, we assume that dopamine conveys a global, scalar prediction error. Another possibility is that targeting of efferent inputs could be tuned via plasticity during development. For instance, if a dSPN promotes a particular action, reward-independent Hebbian plasticity of its efferent inputs would potentiate those inputs that encode the promoted action. Reward-independent anti-Hebbian plasticity would serve an analogous function for iSPNs. Alternatively, if efferent inputs are fixed, plasticity downstream of striatum could adapt the causal effect of SPNs to match their corresponding efferent input.</p><p>A second key requirement of our model is that efferent input synapses should not be adjusted according to the same reward-modulated plasticity rules as the feedforward corticostriatal inputs, as these rules would disrupt the targeting of efferent inputs to the corresponding SPNs. This may be achieved in multiple ways. One possibility is that efferent inputs project from different subregions or cell types than feedforward inputs and are subject to different forms of plasticity. Alternatively, efferent input synapses may have been sufficiently reinforced that they exist in a less labile, ‘consolidated’ synaptic state. A third possibility is that the system may take advantage of latency in efferent activity. Spike timing dependence in SPN input plasticity has been observed in several studies (<xref ref-type="bibr" rid="bib59">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Fino et al., 2005</xref>; <xref ref-type="bibr" rid="bib52">Pawlak and Kerr, 2008</xref>; <xref ref-type="bibr" rid="bib23">Fisher et al., 2017</xref>). This timing dependence could make plasticity sensitive to paired activity in state inputs and SPNs while being insensitive to paired activity in efferent inputs and SPNs. Investigating the source of efferent inputs to SPNs and how it is differentiated from other inputs is an important direction for future work.</p></sec><sec id="s3-4"><title>Extensions and future work</title><p>We have assumed that the striatum selects among a finite set of actions, each of which corresponds to mutually uncorrelated patterns of SPN activity. In reality, there is evidence that the striatal code for action is organized such that kinematically similar behaviors are encoded by similar SPN activity patterns (<xref ref-type="bibr" rid="bib34">Klaus et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>). Other work has shown that the DLS can exert influence over detailed kinematics of learned motor behaviors, rather than simply select among categorically distinct actions (<xref ref-type="bibr" rid="bib15">Dhawale et al., 2021</xref>). A more continuous, structured code for action in DLS is useful in allowing reinforcement learning to generalize between related actions. The ability afforded by our model to multiplex arbitrary action selection and learning signals may facilitate these more sophisticated coding schemes. For instance, reinforcement learning in continuous-valued action spaces requires a three-factor learning rule in which the post-synaptic activity factor represents the discrepancy between the selected action and the action typically selected in the current behavioral state (<xref ref-type="bibr" rid="bib39">Lindsey and Litwin-Kumar, 2022</xref>), which in our model would be represented by efferent activity in SPNs. Investigating such extensions to our model and their consequences for SPN tuning is an interesting future direction.</p><p>In this work, we find strong empirical evidence for our model of efferent activity in SPNs and show that in principle it enables off-policy reinforcement learning capabilities. A convincing experimental demonstration of off-policy learning capabilities would require a way of identifying the causal contribution of SPN activity to action selection, in order to distinguish between actions that are consistent (on-policy) or inconsistent (off-policy) with SPN outputs. This could be achieved through targeted stimulation of SPN populations, or by recording SPN activity during behaviors that are known to be independent of striatal influence (<xref ref-type="bibr" rid="bib47">Mizes et al., 2023</xref>). Simultaneous recordings in SPNs and other brain regions would also facilitate distinguishing between actions driven by striatum from those driven by other pathways. Our model predicts that the relative strength of fluctuations in difference mode versus sum mode activity should be greatest during striatum-driven actions. Such experimental design would also enable a stronger test of the Q-learning model of dopamine activity: actions driven by other regions should lead to increased dopamine activity, as they will be predicted according to the striatum’s learned action values to have low value.</p><p>In our model, the difference between dSPN and iSPN plasticity rules is key to enabling multiplexing of action selection and learning-related activity without interference. Observed plasticity rules elsewhere in the brain are also heterogeneous; for instance, both Hebbian and anti-Hebbian weight changes are observed in cortico-cortical connections (<xref ref-type="bibr" rid="bib35">Koch et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Chindemi et al., 2022</xref>). It is an interesting question whether a similar strategy may be employed outside the striatum, and in other contexts besides reinforcement learning, to allow simultaneous encoding of behavior and learning-related signals without interference.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Numerical simulations</title><p>Code implementing the model is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/alitwinkumar/lindsey_etal_striatal_dynamics">https://github.com/alitwinkumar/lindsey_etal_striatal_dynamics</ext-link>, copy archived at <xref ref-type="bibr" rid="bib41">Litwin-Kumar, 2025</xref>).</p></sec><sec id="s4-2"><title>Data availability</title><p>We reanalyzed data from <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref> and <xref ref-type="bibr" rid="bib43">Markowitz et al., 2023</xref>. Data from <xref ref-type="bibr" rid="bib43">Markowitz et al., 2023</xref> is available at <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.5281/zenodo.7274802">https://dx.doi.org/10.5281/zenodo.7274802</ext-link>.</p></sec><sec id="s4-3"><title>Basic model architecture</title><p>In our simulated learning tasks, we used networks with the following architecture. SPNs receive inputs from cortical neurons. In our simulated go/no-go tasks, there is a single cortical input neuron (representing a task cue) with activity equal to 1 on each trial. In simulated tasks with multiple different task cues (such as the two-alternative forced choice task), there is a population of cortical input neurons, each of which is active with activity 1 when the corresponding task cue is presented and 0 otherwise. The task cue is randomly chosen with uniform probability each trial.</p><p>For each of the <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula> actions available to the model, there is an assigned dSPN and iSPN. We choose to use a single neuron per action for simplicity of the model, but our model could easily be generalized to use population activity to encode actions. The activities of the dSPN and iSPN associated with action <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> are denoted as <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, respectively. Each dSPN and iSPN receives inputs from <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> cortical neurons, and the synaptic input weights from cortical neuron <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula> to the dSPN or iSPN associated with action <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> are denoted as <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>. Feedforward SPN activity is given by<disp-formula id="equ8">,<label>(8)</label><mml:math id="m8"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula> is a nonlinear activation function. We choose <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula> to be the rectified linear function: <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><p>Action selection depends on SPN activity in the following manner. The log-likelihood of an action <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> being performed is proportional to <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>. That is, dSPN activity increases the likelihood of taking the action and iSPN activity decreases the likelihood of taking the action. Concretely, the probability of action <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> being taken is:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> is a parameter controlling the degree of stochasticity in action selection (higher <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> corresponds to more deterministic choices), and <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the probability that no action is taken. In the simulated go/no-go tasks we choose <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and in the tasks involving selection among multiple actions we choose <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>. Except where otherwise noted we used <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>10.0</mml:mn></mml:mstyle></mml:math></inline-formula> in all task simulations.</p></sec><sec id="s4-4"><title>Models of SPN activity following action selection</title><p>In the ‘canonical action selection model’ (<xref ref-type="fig" rid="fig1">Figure 1</xref>), following action selection, the activity of the dSPN associated with the selected action and the activity of all iSPNs associated with unselected actions are set to 1. Biologically, this activity pattern can be implemented via effective mutual inhibition between SPNs with opponent functions (dSPNs tuned to different actions, iSPNs tuned to different actions, and dSPN/iSPN pairs tuned to the same action) and mutual excitation between SPNs with complementary functions (dSPNs tuned to one action and iSPNs to another) (<xref ref-type="bibr" rid="bib7">Burke et al., 2017</xref>).</p><p>In the proposed efference model, following selection of an action <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, activity of the SPNs associated with action <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is updated as follows:<disp-formula id="equ11">,<label>(11)</label><mml:math id="m11"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> equals 1 for <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and 0 otherwise. The parameter <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the strength of efferent excitation.</p></sec><sec id="s4-5"><title>Learning rules</title><p>In all models, SPN input weights are initialized at 1 and weight updates proceed according to the plasticity rules given below:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <italic>α</italic> is a learning rate, set to 0.05 throughout all learning simulations except the tutoring simulations of <xref ref-type="fig" rid="fig7">Figure 7</xref> where it is set to 0.01. In the paper we experiment with various choices of <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mspace width="0pt"/></mml:mtd><mml:mtd><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>δ</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mtext>(Linear)</mml:mtext></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>(Rectified)</mml:mtext></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>δ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>δ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mtext>(Offset sigmoid)</mml:mtext></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with the offset sigmoid parameters chosen as <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>3.5</mml:mn><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>11.5</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> (taken from <xref ref-type="bibr" rid="bib13">Cruz et al., 2022</xref>). The quantity <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula> indicates an estimate of reward prediction error. In our experiments in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> we use TD learing to compute <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ18">  <label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mi>δ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a learning rate, set to 0.05 throughout all learning simulations (except the tutoring simulations of <xref ref-type="fig" rid="fig7">Figure 7</xref> where it is set to 0.25) and <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> indicates the cortical input state (indicating which cue is being presented). <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is initialized at 0.</p><p>In our experiments in <xref ref-type="fig" rid="fig7">Figure 7</xref> we use Q-learning to enable off-policy learning, corresponding to the following value for <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> indicates the action that was just taken in response to state <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is taken to be equal to the striatal output <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> in response to the state <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-6"><title>Firing rate simulations</title><p>In each trial of the two-alternative forced choice task (<xref ref-type="fig" rid="fig4">Figure 4</xref>), one of two stimuli is presented for 2 s. Cortical activity <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> representing the stimulus is encoded in a one-hot vector. Four SPNs are modeled, one dSPN and one iSPN for each of two actions. The dynamics of SPN <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> follows:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes positive rectification, <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represent corticostriatal weights initialized following a Gaussian distribution with mean 0 and standard deviation 1 Hz, <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is an Ornstein–Uhlenbeck noise process with time constant 600 ms and variance 1/60 Hz<sup>2</sup>, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes efferent input, and <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> Hz is a bias term. Simulations were performed with <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ms.</p><p>On each trial, an action is selected based on the average difference mode activity for the two actions during the first 1 s of stimulus presentation. In the second half of the stimulus presentation period, efferent input is provided to the dSPN and iSPN corresponding to the chosen action by setting <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>7.5</mml:mn></mml:mstyle></mml:math></inline-formula> Hz for these neurons. Learning proceeds according to<disp-formula id="equ22"> <label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where in the second half of the stimulus presentation period <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> for dSPNs after a correct action is taken and iSPNs after an incorrect action is taken, and –1 otherwise, and <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> ms<sup>−1</sup>.</p></sec><sec id="s4-7"><title>Experimental prediction simulations</title><p>For the model predictions of <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>, we used the following parameters: <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, and we set <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> such that the no-action option was chosen 50% of the time. Feedforward SPN activity was generated from a Gaussian process with kernel <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (exponentially decaying autocorrelation with a time constant of 10 timesteps). Efference activity also decayed exponentially with a time constant of 10 timesteps. Action selection occured every 10 timesteps based on the SPN activity at the preceding timestep.</p></sec><sec id="s4-8"><title>Fiber photometry data</title><p>Adeno-associated viruses (AAVs) expressing Cre-On jRCaMP1b and Cre-Off GCaMP6s were injected into the DLS of <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula> <italic>Drd1a-Cre</italic> mice to measure bulk dSPN (red) and iSPN (green) activity via multicolor photometry. Activity of each indicator was recorded at a rate of 30 Hz using an optical fiber implanted in the right DLS. Data was collected during spontaneous behavior in a circular open field, for five to six sessions of 20 min each for each mouse. In the reversed indicator experiments of <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. <italic>A2a-Cre</italic> mice were injected with a mixture of the same AAVs, labeling iSPNs with jRCaMP1b (red) and dSPNs with GCaMP6s (green). More details are reported in <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>.</p><p>In our data analyses in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, for each session (<inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>48</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mstyle></mml:math></inline-formula>, respectively) we computed the autocorrelation and cross-correlation of the dSPN and iSPN indicator activity across the entire session.</p></sec><sec id="s4-9"><title>Miniscope data</title><p><italic>Drd1a-Cre</italic> AAVs expressing GCaMP6f were injected into the right DLS of <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mstyle></mml:math></inline-formula> <italic>Drd1a-Cre</italic> mice (to label dSPNs) and <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mstyle></mml:math></inline-formula> <italic>A2a-Cre</italic> mice (to label iSPNs). A head-mounted single-photon microscope was coupled to a gradient index lens implanted into the dorsal striatum above the injection site. Recordings were made, as for the photometry data, during spontaneous behavior in a circular open field. Calcium activity was recorded from a total of 653 dSPNs and 794 iSPNs for these mice, with the number of neurons per mouse ranging from 27 to 336. To enable simultaneous recording of dSPNs and iSPNs in the same mice, a different protocol was used: <italic>Drd1a-Cre</italic> mice were injected with an AAV mixture which labeled both dSPNs and iSPNS with GCaMP6s, but additionally selectively labeled dSPNS with nuclear-localized dTomato. This procedure enabled (in <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mstyle></mml:math></inline-formula> mice) cell-type identification of dSPNs versus iSPNs with a two-photon microscope which was cross-referenced with the single-photon microscope recordings. More details are given in <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>. In our analyses, these data were used for the simultaneous-recording analyses in <xref ref-type="fig" rid="fig6">Figure 6F,G</xref> and were also combined with the appropriate single-pathway data in the analyses of <xref ref-type="fig" rid="fig6">Figure 6D,E</xref>.</p></sec><sec id="s4-10"><title>Behavioral data</title><p>Mouse behavior in the circular open field was recorded as follows: 3D pose information was recorded using a depth camera at a rate of 30 Hz. The videos were preprocessed to center the mouse and align the nose-to-tail axis across frames and remove occluding objects. The videos were then fed through PCA to reduce the dimensinoality of the data and fed into the MoSeq algorithm (<xref ref-type="bibr" rid="bib70">Wiltschko et al., 2015</xref>) which fits a generative model to the video data that automatically infers a set of behavioral ‘syllables’ (repeated, stereotyped behavioral kinematics) and assigns each frame of the video to one of these syllables. More details on MoSeq are given in <xref ref-type="bibr" rid="bib70">Wiltschko et al., 2015</xref> and more details on its application to this dataset are given in <xref ref-type="bibr" rid="bib42">Markowitz et al., 2018</xref>. There were 89 syllables identified by MoSeq that appear across all the sessions. We restricted our analysis to the set of 62 syllables that appear at least 5 times in each behavioral session.</p></sec><sec id="s4-11"><title>Syllable-tuned SPN activity mode analysis</title><p>In our analysis, we first <italic>z</italic>-scored the activity of each neuron across the data collected for each mouse. We divided the data by the boundaries of behavioral syllables and split it into two equally sized halves (based on whether the timestamp, rounded to the nearest second, of the behavioral syllable was even or odd). To compute the activity modes associated with each behavioral syllable, we first computed the average change in activity for each neuron during each syllable and fit a linear regression model to predict this increase from a one-hot vector indicating the syllable identity. The resulting coefficients of this regression indicate the directions (modes) in activity space that increase the most during performance of each of the behavioral syllables. We linearly time-warped the data in each session based on the boundaries of each MoSeq-identified behavioral syllable, such that in the new time coordinates each behavioral syllable lasted 10 timesteps. The time course of the projection of SPN activity along the modes associated with each behavioral syllable was then computed around the onset of that syllable, or around all other sllables. As a way of cross-validating the analysis, we performed the regression on one half of the data and plotted the average mode activity on the other half of the data (in both directions, and averaged the results). We averaged the resulting time courses of mode activity across all choices of behavioral syllables. This analysis was performed for each mouse and the results in <xref ref-type="fig" rid="fig6">Figure 6</xref> show means and standard errors across mice.</p></sec><sec id="s4-12"><title>Comparison of selectivity of dSPNs and iSPNs</title><p>To test whether dSPNs or iSPNs exhibit greater or less specificity in their tuning to behaviors, we computed the selectivity of each neuron in the imaging data (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). For each neuron, we computed its average <italic>z</italic>-scored activity <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in response to each of the behavioral syllables <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> in the dataset. Common measures of selectivity require a nonnegative measurement of a neuron’s tuning to a given condition. Thus, we conducted the analysis in two ways, using either the unsigned activity <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> or the rectified activity <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> as the measure of the neuron’s tuning <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to syllable <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. The selectivity was then computed using the following expression introduced in prior work (<xref ref-type="bibr" rid="bib64">Treves and Rolls, 1991</xref>; <xref ref-type="bibr" rid="bib69">Willmore and Tolhurst, 2001</xref>):<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>A</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>A</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>This value ranges from 0 to 1, and a higher value indicates that fluctuations in a neuron’s activity are driven primarily by one or a few behavioral syllables. The results are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The selectivity values are fairly modest (consistent with a distributed code for actions) and comparable between dSPNs and iSPNs.</p></sec><sec id="s4-13"><title>Dopamine activity data and analysis</title><p>For <xref ref-type="fig" rid="fig7">Figure 7E</xref>, we used data from <xref ref-type="bibr" rid="bib43">Markowitz et al., 2023</xref>. Mice (<inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mstyle></mml:math></inline-formula>) virally expressing the dopamine reporter dLight1.1 in the DLS were recorded with a fiber cannula implanted above the injection site. Mice were placed in a circular open field for 30-min sessions and allowed to behave freely while spontaneous dLight activity was recorded. MoSeq (described above) was used to infer a set of <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>57</mml:mn></mml:mstyle></mml:math></inline-formula> behavioral syllables observed across all sessions. As in <xref ref-type="bibr" rid="bib43">Markowitz et al., 2023</xref>, the data were preprocessed by computing the maximum dLight value during each behavioral syllable. These per-syllable dopamine values were <italic>z</italic>-scored across each session and used as our measure of dopamine activity during each syllable. We then computed an <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mstyle></mml:math></inline-formula> table of the average dopamine activity during each syllable <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> conditioned on the previous syllable having been syllable <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, denoted as <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We also computed the <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> table of probabilities of transitioning from syllable <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> to syllable <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> across the dataset, denoted as <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. These tables were computed separately for each mouse. In <xref ref-type="fig" rid="fig7">Figure 7E</xref>, we report the Pearson correlation coefficient between the predicted and actual values of <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We then experimented with several alternative models, described below, that predict <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> based on <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig7">Figure 7E</xref>, we report the Pearson correlation coefficient between the predicted and actual values of <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><sec id="s4-13-1"><title>Q-learning model</title><p>In the Q-learning model, the mouse maintains an internal estimate of the value <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> of each transition between syllables. In the absence of explicit rewards, the dopamine activity associated with a syllable transition is predicted to be <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We inferred a set of <italic>Q</italic>-values by initializing a Q-table with all zero values and running gradient descent on the Q-table to minimize the mean squared error between the predicted and empirical values of <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. These inferred <italic>Q</italic>-values were used to predict behavioral transition probabilities according to <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. We did not fit the value of <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> but rather chose it to be the reciprocal of the standard deviation of <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> across all <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, to ensure a reasonable dynamic range in predicted transition probabilities.</p></sec><sec id="s4-13-2"><title><italic>V(s)</italic> TD learning model</title><p>In this model, the mouse maintains an internal estimate of the value <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> of each syllable, and the predicted dopamine activity at each transition is <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We fit the vector of values <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to minimize the mean squared error of predicted and empirical <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The predicted transition probabilities in this model (which are independent of the previous syllable <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) are <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> chosen to normalize the <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to have standard deviation 1, as in the previous models.</p></sec><sec id="s4-13-3"><title>Action value model</title><p>In this model, we assume that dopamine activity simply reflects the probability of each transition rather than encoding a prediction error; that is, we assume <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-13-4"><title>State value model</title><p>In this model, we assume that dopamine activity simply reflects the probability of each behavioral syllable being chosen and is independent of the previous syllable. That is, we compute the average dopamine activity <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> associated with each syllable <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>, and predict <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:math></inline-formula>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>S.R.D. sits on the scientific advisory boards of Neumora and Gilgamesh Therapeutics, which have licensed or sub-licensed the MoSeq technology</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Investigation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-101747-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Code implementing the model is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/alitwinkumar/lindsey_etal_striatal_dynamics">https://github.com/alitwinkumar/lindsey_etal_striatal_dynamics</ext-link>, copy archived at <xref ref-type="bibr" rid="bib41">Litwin-Kumar, 2025</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Jay</surname><given-names>M</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>R</given-names></name><name><surname>Cieszkowski</surname><given-names>R</given-names></name><name><surname>Scott</surname><given-names>R</given-names></name><name><surname>Brann</surname><given-names>D</given-names></name><name><surname>Koveal</surname><given-names>D</given-names></name><name><surname>Kula</surname><given-names>T</given-names></name><name><surname>Weinreb</surname><given-names>C</given-names></name><name><surname>Osman</surname><given-names>MA</given-names></name><name><surname>Pinto</surname><given-names>SR</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Linderman</surname><given-names>S</given-names></name><name><surname>Sabatini</surname><given-names>B</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Spontaneous behaviour is structured by reinforcement without explicit reward</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7274803</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Jaeeon Lee for providing the initial inspiration for this project, Sean Escola for fruitful discussions, and Steven A Siegelbaum for comments on the manuscript. JL is supported by the Mathers Foundation and the Gatsby Charitable Foundation. JM is supported by a Career Award at the Scientific Interface from the Burroughs Wellcome Fund, a fellowship from the Sloan Foundation, and a fellowship from the David and Lucille Packard Foundation. SRD is supported by NIH grants RF1AG073625, R01NS114020, and U24NS109520, the Simons Foundation Autism Research Initiative, and the Simons Collaboration on Plasticity and the Aging Brain. ALK is supported by the Mathers Foundation, the Burroughs Wellcome Foundation, the McKnight Endowment Fund, and the Gatsby Charitable Foundation.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arulkumaran</surname><given-names>K</given-names></name><name><surname>Deisenroth</surname><given-names>MP</given-names></name><name><surname>Brundage</surname><given-names>M</given-names></name><name><surname>Bharath</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep reinforcement learning: a brief survey</article-title><source>IEEE Signal Processing Magazine</source><volume>34</volume><fpage>26</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1109/MSP.2017.2743240</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Turner</surname><given-names>BO</given-names></name><name><surname>Horvitz</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical and basal ganglia contributions to habit learning and automaticity</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>208</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.02.001</pub-id><pub-id pub-id-type="pmid">20207189</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balleine</surname><given-names>BW</given-names></name><name><surname>Delgado</surname><given-names>MR</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The role of the dorsal striatum in reward and decision-making</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>8161</fpage><lpage>8165</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1554-07.2007</pub-id><pub-id pub-id-type="pmid">17670959</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbera</surname><given-names>G</given-names></name><name><surname>Liang</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Culurciello</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spatially compact neural clusters in the dorsal striatum encode locomotion relevant information</article-title><source>Neuron</source><volume>92</volume><fpage>202</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.037</pub-id><pub-id pub-id-type="pmid">27667003</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bariselli</surname><given-names>S</given-names></name><name><surname>Fobbs</surname><given-names>WC</given-names></name><name><surname>Creed</surname><given-names>MC</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A competitive model for striatal action selection</article-title><source>Brain Research</source><volume>1713</volume><fpage>70</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2018.10.009</pub-id><pub-id pub-id-type="pmid">30300636</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bostan</surname><given-names>AC</given-names></name><name><surname>Strick</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The basal ganglia and the cerebellum: nodes in an integrated network</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>338</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0002-7</pub-id><pub-id pub-id-type="pmid">29643480</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>DA</given-names></name><name><surname>Rotstein</surname><given-names>HG</given-names></name><name><surname>Alvarez</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Striatal local circuitry: a new framework for lateral inhibition</article-title><source>Neuron</source><volume>96</volume><fpage>267</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.019</pub-id><pub-id pub-id-type="pmid">29024654</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calabresi</surname><given-names>P</given-names></name><name><surname>Gubellini</surname><given-names>P</given-names></name><name><surname>Centonze</surname><given-names>D</given-names></name><name><surname>Picconi</surname><given-names>B</given-names></name><name><surname>Bernardi</surname><given-names>G</given-names></name><name><surname>Chergui</surname><given-names>K</given-names></name><name><surname>Svenningsson</surname><given-names>P</given-names></name><name><surname>Fienberg</surname><given-names>AA</given-names></name><name><surname>Greengard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dopamine and cAMP-regulated phosphoprotein 32 kDa controls both striatal long-term depression and long-term potentiation, opposing forms of synaptic plasticity</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>8443</fpage><lpage>8451</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-22-08443.2000</pub-id><pub-id pub-id-type="pmid">11069952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardinal</surname><given-names>RN</given-names></name><name><surname>Parkinson</surname><given-names>JA</given-names></name><name><surname>Hall</surname><given-names>J</given-names></name><name><surname>Everitt</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>26</volume><fpage>321</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1016/S0149-7634(02)00007-6</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chindemi</surname><given-names>G</given-names></name><name><surname>Abdellah</surname><given-names>M</given-names></name><name><surname>Amsalem</surname><given-names>O</given-names></name><name><surname>Benavides-Piccione</surname><given-names>R</given-names></name><name><surname>Delattre</surname><given-names>V</given-names></name><name><surname>Doron</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>A</given-names></name><name><surname>Jaquier</surname><given-names>AT</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Monney</surname><given-names>C</given-names></name><name><surname>Perin</surname><given-names>R</given-names></name><name><surname>Rössert</surname><given-names>C</given-names></name><name><surname>Tuncel</surname><given-names>AM</given-names></name><name><surname>Van Geit</surname><given-names>W</given-names></name><name><surname>DeFelipe</surname><given-names>J</given-names></name><name><surname>Graupner</surname><given-names>M</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Muller</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A calcium-based plasticity model for predicting long-term potentiation and depression in the neocortex</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>3038</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-30214-w</pub-id><pub-id pub-id-type="pmid">35650191</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Opponent actor learning (OpAL): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title><source>Psychological Review</source><volume>121</volume><fpage>337</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1037/a0037015</pub-id><pub-id pub-id-type="pmid">25090423</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Contreras-Vidal</surname><given-names>JL</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A predictive reinforcement model of dopamine neurons for learning approach behavior</article-title><source>Journal of Computational Neuroscience</source><volume>6</volume><fpage>191</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1023/a:1008862904946</pub-id><pub-id pub-id-type="pmid">10406133</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cruz</surname><given-names>BF</given-names></name><name><surname>Guiomar</surname><given-names>G</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Motiwala</surname><given-names>A</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Action suppression reveals opponent parallel control via striatal circuits</article-title><source>Nature</source><volume>607</volume><fpage>521</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-04894-9</pub-id><pub-id pub-id-type="pmid">35794480</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>G</given-names></name><name><surname>Jun</surname><given-names>SB</given-names></name><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Pham</surname><given-names>MD</given-names></name><name><surname>Vogel</surname><given-names>SS</given-names></name><name><surname>Lovinger</surname><given-names>DM</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Concurrent activation of striatal direct and indirect pathways during action initiation</article-title><source>Nature</source><volume>494</volume><fpage>238</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/nature11846</pub-id><pub-id pub-id-type="pmid">23354054</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Wolff</surname><given-names>SBE</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The basal ganglia control the detailed kinematics of learned motor skills</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1256</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00889-3</pub-id><pub-id pub-id-type="pmid">34267392</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dreyer</surname><given-names>JK</given-names></name><name><surname>Herrik</surname><given-names>KF</given-names></name><name><surname>Berg</surname><given-names>RW</given-names></name><name><surname>Hounsgaard</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Influence of phasic and tonic dopamine release on receptor activation</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>14273</fpage><lpage>14283</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1894-10.2010</pub-id><pub-id pub-id-type="pmid">20962248</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunovan</surname><given-names>K</given-names></name><name><surname>Vich</surname><given-names>C</given-names></name><name><surname>Clapp</surname><given-names>M</given-names></name><name><surname>Verstynen</surname><given-names>T</given-names></name><name><surname>Rubin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reward-driven changes in striatal pathway competition shape evidence evaluation in decision-making</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006998</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006998</pub-id><pub-id pub-id-type="pmid">31060045</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Exner</surname><given-names>C</given-names></name><name><surname>Koschack</surname><given-names>J</given-names></name><name><surname>Irle</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The differential role of premotor frontal cortex and basal ganglia in motor sequence learning: evidence from focal basal ganglia lesions</article-title><source>Learning &amp; Memory</source><volume>9</volume><fpage>376</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1101/lm.48402</pub-id><pub-id pub-id-type="pmid">12464697</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname><given-names>MS</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A hypothesis for basal ganglia-dependent reinforcement learning in the songbird</article-title><source>Neuroscience</source><volume>198</volume><fpage>152</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.069</pub-id><pub-id pub-id-type="pmid">22015923</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Oculomotor learning revisited: a model of reinforcement learning in the basal ganglia incorporating an efference copy of motor actions</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><elocation-id>38</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00038</pub-id><pub-id pub-id-type="pmid">22754501</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The role of efference copy in striatal learning</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>194</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.012</pub-id><pub-id pub-id-type="pmid">24566242</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fino</surname><given-names>E</given-names></name><name><surname>Glowinski</surname><given-names>J</given-names></name><name><surname>Venance</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Bidirectional activity-dependent plasticity at corticostriatal synapses</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>11279</fpage><lpage>11287</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4476-05.2005</pub-id><pub-id pub-id-type="pmid">16339023</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>SD</given-names></name><name><surname>Robertson</surname><given-names>PB</given-names></name><name><surname>Black</surname><given-names>MJ</given-names></name><name><surname>Redgrave</surname><given-names>P</given-names></name><name><surname>Sagar</surname><given-names>MA</given-names></name><name><surname>Abraham</surname><given-names>WC</given-names></name><name><surname>Reynolds</surname><given-names>JNJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reinforcement determines the timing dependence of corticostriatal synaptic plasticity in vivo</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>334</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00394-x</pub-id><pub-id pub-id-type="pmid">28839128</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>51</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1162/0898929052880093</pub-id><pub-id pub-id-type="pmid">15701239</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeze</surname><given-names>BS</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name><name><surname>Hammack</surname><given-names>N</given-names></name><name><surname>Berke</surname><given-names>JD</given-names></name><name><surname>Kreitzer</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Control of basal ganglia output by direct and indirect pathway projection neurons</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18531</fpage><lpage>18539</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1278-13.2013</pub-id><pub-id pub-id-type="pmid">24259575</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurney</surname><given-names>KN</given-names></name><name><surname>Humphries</surname><given-names>MD</given-names></name><name><surname>Redgrave</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A new framework for cortico-striatal plasticity: behavioural theory meets in vitro data at the reinforcement-action interface</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002034</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002034</pub-id><pub-id pub-id-type="pmid">25562526</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Houk</surname><given-names>JC</given-names></name><name><surname>Adams</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>A model of how the basal ganglia generate and use neural signals that predict reinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Houk</surname><given-names>JC</given-names></name></person-group><source>Models of Information Processing in the Basal Ganglia</source><publisher-name>MIT Press</publisher-name><fpage>249</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.7551/mitpress/4708.003.0020</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>EJ</given-names></name><name><surname>Dahlen</surname><given-names>JE</given-names></name><name><surname>Hu</surname><given-names>YY</given-names></name><name><surname>Aguilar</surname><given-names>K</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Mukundan</surname><given-names>M</given-names></name><name><surname>Mitani</surname><given-names>A</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Disengagement of motor cortex from movement control during long-term learning</article-title><source>Science Advances</source><volume>5</volume><elocation-id>eaay0001</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aay0001</pub-id><pub-id pub-id-type="pmid">31693007</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iino</surname><given-names>Y</given-names></name><name><surname>Sawada</surname><given-names>T</given-names></name><name><surname>Yamaguchi</surname><given-names>K</given-names></name><name><surname>Tajiri</surname><given-names>M</given-names></name><name><surname>Ishii</surname><given-names>S</given-names></name><name><surname>Kasai</surname><given-names>H</given-names></name><name><surname>Yagishita</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dopamine D2 receptors in discrimination learning and spine enlargement</article-title><source>Nature</source><volume>579</volume><fpage>555</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2115-1</pub-id><pub-id pub-id-type="pmid">32214250</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiple representations and algorithms for reinforcement learning in the cortico-basal ganglia circuit</article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>368</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.04.001</pub-id><pub-id pub-id-type="pmid">21531544</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaskir</surname><given-names>A</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>On the normative advantages of dopamine and striatal opponency for learning and choice</article-title><source>eLife</source><volume>12</volume><elocation-id>e85107</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.85107</pub-id><pub-id pub-id-type="pmid">36946371</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joel</surname><given-names>D</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Ruppin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Actor–critic models of the basal ganglia: new anatomical and computational perspectives</article-title><source>Neural Networks</source><volume>15</volume><fpage>535</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(02)00047-3</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawai</surname><given-names>R</given-names></name><name><surname>Markman</surname><given-names>T</given-names></name><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Fantana</surname><given-names>AL</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Motor cortex is required for learning but not for executing a motor skill</article-title><source>Neuron</source><volume>86</volume><fpage>800</fpage><lpage>812</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.024</pub-id><pub-id pub-id-type="pmid">25892304</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klaus</surname><given-names>A</given-names></name><name><surname>Martins</surname><given-names>GJ</given-names></name><name><surname>Paixao</surname><given-names>VB</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The spatiotemporal organization of the striatum encodes action space</article-title><source>Neuron</source><volume>95</volume><fpage>1171</fpage><lpage>1180</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.015</pub-id><pub-id pub-id-type="pmid">28858619</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>G</given-names></name><name><surname>Ponzo</surname><given-names>V</given-names></name><name><surname>Di Lorenzo</surname><given-names>F</given-names></name><name><surname>Caltagirone</surname><given-names>C</given-names></name><name><surname>Veniero</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hebbian and anti-Hebbian spike-timing-dependent plasticity of human cortico-cortical connections</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>9725</fpage><lpage>9733</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4988-12.2013</pub-id><pub-id pub-id-type="pmid">23739969</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>AV</given-names></name><name><surname>Freeze</surname><given-names>BS</given-names></name><name><surname>Parker</surname><given-names>PRL</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Thwin</surname><given-names>MT</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Kreitzer</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Regulation of parkinsonian motor behaviours by optogenetic control of basal ganglia circuitry</article-title><source>Nature</source><volume>466</volume><fpage>622</fpage><lpage>626</lpage><pub-id pub-id-type="doi">10.1038/nature09159</pub-id><pub-id pub-id-type="pmid">20613723</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SJ</given-names></name><name><surname>Lodder</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Patriarchi</surname><given-names>T</given-names></name><name><surname>Tian</surname><given-names>L</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cell-type-specific asynchronous modulation of PKA by dopamine in learning</article-title><source>Nature</source><volume>590</volume><fpage>451</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03050-5</pub-id><pub-id pub-id-type="pmid">33361810</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Striatal indirect pathway mediates exploration via collicular competition</article-title><source>Nature</source><volume>599</volume><fpage>645</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04055-4</pub-id><pub-id pub-id-type="pmid">34732888</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Action-modulated midbrain dopamine activity arises from distributed control policies</article-title><source>Advances in Neural Information Processing Systems</source><volume>35</volume><fpage>5535</fpage><lpage>5548</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two-phase model of the basal ganglia: implications for discontinuous control of the motor system</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>369</volume><elocation-id>20130489</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0489</pub-id><pub-id pub-id-type="pmid">25267829</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Lindsey_etal_striatal_dynamics</data-title><version designator="swh:1:rev:d9e671e39714975ef04ee2949cdcd0ff4496933c">swh:1:rev:d9e671e39714975ef04ee2949cdcd0ff4496933c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:010802e1f866a679f4581313ff4ec46402c05039;origin=https://github.com/alitwinkumar/lindsey_etal_striatal_dynamics;visit=swh:1:snp:b752b1e3591726b6106c884e91ec698e13817d49;anchor=swh:1:rev:d9e671e39714975ef04ee2949cdcd0ff4496933c">https://archive.softwareheritage.org/swh:1:dir:010802e1f866a679f4581313ff4ec46402c05039;origin=https://github.com/alitwinkumar/lindsey_etal_striatal_dynamics;visit=swh:1:snp:b752b1e3591726b6106c884e91ec698e13817d49;anchor=swh:1:rev:d9e671e39714975ef04ee2949cdcd0ff4496933c</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Beron</surname><given-names>CC</given-names></name><name><surname>Neufeld</surname><given-names>SQ</given-names></name><name><surname>Robertson</surname><given-names>K</given-names></name><name><surname>Bhagat</surname><given-names>ND</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Peterson</surname><given-names>E</given-names></name><name><surname>Hyun</surname><given-names>M</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The striatum organizes 3D behavior via moment-to-moment action selection</article-title><source>Cell</source><volume>174</volume><fpage>44</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.04.019</pub-id><pub-id pub-id-type="pmid">29779950</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Jay</surname><given-names>M</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>RW</given-names></name><name><surname>Cieszkowski</surname><given-names>R</given-names></name><name><surname>Scott</surname><given-names>R</given-names></name><name><surname>Brann</surname><given-names>D</given-names></name><name><surname>Koveal</surname><given-names>D</given-names></name><name><surname>Kula</surname><given-names>T</given-names></name><name><surname>Weinreb</surname><given-names>C</given-names></name><name><surname>Osman</surname><given-names>MAM</given-names></name><name><surname>Pinto</surname><given-names>SR</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spontaneous behaviour is structured by reinforcement without explicit reward</article-title><source>Nature</source><volume>614</volume><fpage>108</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05611-2</pub-id><pub-id pub-id-type="pmid">36653449</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning reward uncertainty in the basal ganglia</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005062</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005062</pub-id><pub-id pub-id-type="pmid">27589489</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minamimoto</surname><given-names>T</given-names></name><name><surname>Hori</surname><given-names>Y</given-names></name><name><surname>Kimura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Complementary process to response bias in the centromedian nucleus of the thalamus</article-title><source>Science</source><volume>308</volume><fpage>1798</fpage><lpage>1801</lpage><pub-id pub-id-type="doi">10.1126/science.1109154</pub-id><pub-id pub-id-type="pmid">15961671</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mink</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The basal ganglia: focused selection and inhibition of competing motor programs</article-title><source>Progress in Neurobiology</source><volume>50</volume><fpage>381</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/s0301-0082(96)00042-1</pub-id><pub-id pub-id-type="pmid">9004351</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizes</surname><given-names>KGC</given-names></name><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Escola</surname><given-names>GS</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dissociating the contributions of sensorimotor striatum to automatic and visually guided motor sequences</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>1791</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01431-3</pub-id><pub-id pub-id-type="pmid">37667040</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname><given-names>PR</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id><pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reinforcement learning in the brain</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>139</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2008.12.005</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>J</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title><source>Science</source><volume>304</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1126/science.1094285</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packard</surname><given-names>MG</given-names></name><name><surname>Knowlton</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Learning and memory functions of the Basal Ganglia</article-title><source>Annual Review of Neuroscience</source><volume>25</volume><fpage>563</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.25.112701.142937</pub-id><pub-id pub-id-type="pmid">12052921</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pawlak</surname><given-names>V</given-names></name><name><surname>Kerr</surname><given-names>JND</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dopamine receptor activation is required for corticostriatal spike-timing-dependent plasticity</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>2435</fpage><lpage>2446</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4402-07.2008</pub-id><pub-id pub-id-type="pmid">18322089</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peak</surname><given-names>J</given-names></name><name><surname>Chieng</surname><given-names>B</given-names></name><name><surname>Hart</surname><given-names>G</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Striatal direct and indirect pathway neurons differentially control the encoding and updating of goal-directed learning</article-title><source>eLife</source><volume>9</volume><elocation-id>e58544</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58544</pub-id><pub-id pub-id-type="pmid">33215609</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>AJ</given-names></name><name><surname>Fabre</surname><given-names>JMJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Striatal activity topographically reflects cortical activity</article-title><source>Nature</source><volume>591</volume><fpage>420</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03166-8</pub-id><pub-id pub-id-type="pmid">33473213</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redgrave</surname><given-names>P</given-names></name><name><surname>Prescott</surname><given-names>TJ</given-names></name><name><surname>Gurney</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The basal ganglia: a vertebrate solution to the selection problem?</article-title><source>Neuroscience</source><volume>89</volume><fpage>1009</fpage><lpage>1023</lpage><pub-id pub-id-type="doi">10.1016/S0306-4522(98)00319-4</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>JE</given-names></name><name><surname>Vich</surname><given-names>C</given-names></name><name><surname>Clapp</surname><given-names>M</given-names></name><name><surname>Noneman</surname><given-names>K</given-names></name><name><surname>Verstynen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The credit assignment problem in cortico-basal ganglia-thalamic networks: A review, A problem and A possible solution</article-title><source>The European Journal of Neuroscience</source><volume>53</volume><fpage>2234</fpage><lpage>2253</lpage><pub-id pub-id-type="doi">10.1111/ejn.14745</pub-id><pub-id pub-id-type="pmid">32302439</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Action selection and action value in frontal-striatal circuits</article-title><source>Neuron</source><volume>74</volume><fpage>947</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.037</pub-id><pub-id pub-id-type="pmid">22681697</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>W</given-names></name><name><surname>Flajolet</surname><given-names>M</given-names></name><name><surname>Greengard</surname><given-names>P</given-names></name><name><surname>Surmeier</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title><source>Science</source><volume>321</volume><fpage>848</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1160575</pub-id><pub-id pub-id-type="pmid">18687967</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>JH</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Paik</surname><given-names>S-B</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatial organization of functional clusters representing reward and movement information in the striatal direct and indirect pathways</article-title><source>PNAS</source><volume>117</volume><fpage>27004</fpage><lpage>27015</lpage><pub-id pub-id-type="doi">10.1073/pnas.2010361117</pub-id><pub-id pub-id-type="pmid">33055217</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silveri</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Contribution of the cerebellum and the basal ganglia to language production: speech, word fluency, and sentence construction-evidence from pathology</article-title><source>Cerebellum</source><volume>20</volume><fpage>282</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1007/s12311-020-01207-6</pub-id><pub-id pub-id-type="pmid">33120434</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>Y</given-names></name><name><surname>Raju</surname><given-names>DV</given-names></name><name><surname>Pare</surname><given-names>JF</given-names></name><name><surname>Sidibe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The thalamostriatal system: a highly specific network of the basal ganglia circuitry</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>520</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.07.004</pub-id><pub-id pub-id-type="pmid">15331233</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treves</surname><given-names>A</given-names></name><name><surname>Rolls</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>What determines the capacity of autoassociative memories in the brain?</article-title><source>Network</source><volume>2</volume><fpage>371</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1088/0954-898X/2/4/004</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varin</surname><given-names>C</given-names></name><name><surname>Cornil</surname><given-names>A</given-names></name><name><surname>Houtteman</surname><given-names>D</given-names></name><name><surname>Bonnavion</surname><given-names>P</given-names></name><name><surname>de Kerchove d’Exaerde</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The respective activation and silencing of striatal direct and indirect pathway neurons support behavior encoding</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>4982</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-40677-0</pub-id><pub-id pub-id-type="pmid">37591838</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wärnberg</surname><given-names>E</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Feasibility of dopamine as a vector-valued feedback signal in the basal ganglia</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2221994120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2221994120</pub-id><pub-id pub-id-type="pmid">37527344</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickens</surname><given-names>JR</given-names></name><name><surname>Begg</surname><given-names>AJ</given-names></name><name><surname>Arbuthnott</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dopamine reverses the depression of rat corticostriatal synapses which normally follows high-frequency stimulation of cortex In vitro</article-title><source>Neuroscience</source><volume>70</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(95)00436-M</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Ackermann</surname><given-names>H</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Differential contributions of motor cortex, basal ganglia, and cerebellum to speech motor control: effects of syllable repetition rate evaluated by fMRI</article-title><source>NeuroImage</source><volume>13</volume><fpage>101</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0672</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Characterizing the sparseness of neural codes</article-title><source>Network</source><volume>12</volume><fpage>255</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1080/net.12.3.255.270</pub-id><pub-id pub-id-type="pmid">11563529</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>JM</given-names></name><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Abraira</surname><given-names>VE</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping sub-second structure in mouse behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Relationship between sum mode activity and future difference mode activity</title><p>In the main text, we provided an argument for why sum mode activity drives changes to future difference mode activity, assuming a linear <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and linear neural activation functions. Here, we generalize this argument to more general learning rules and activation functions <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula>, assuming only that <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is monotonically increasing, <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is monotonically increasing, and <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is monotonically increasing. We have that <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, in the limit of small small weight updates, we can write:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≈</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>x</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∝</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are nonnegative because <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is always nonnegative by assumption. Since by assumption <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are increasing/decreasing, respectively, the first term of the above sum has nonnegative correlation with <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and the second term has nonnegative correlation with <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. Thus, changes <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to difference mode activity are always nonnegatively correlated with sum mode activity. If we assume that efferent excitation is always sufficiently strong that <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> are positive, and that there are no values of <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula> for which <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> both have zero derivative, we can further guarantee that changes to difference mode activity will always be <italic>positively</italic> correlated with sum mode activity.</p></sec><sec sec-type="appendix" id="s9"><title>Generalizing the model to a distributed code for actions</title><p>In our model simulations in the main text, we assumed for convenience that there is a single dSPN and iSPN that promote and suppress each available action, respectively. It is more realistic to model the code for action as distributed among many SPNs. Our model generalizes easily to this case; all that is necessary is for the efferent activity following action selection to excite the vectors (for both dSPNs and iSPNs) in population activity space corresponding to that action. To demonstrate this, we conducted a simulation with <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mstyle></mml:math></inline-formula> dSPNs and iSPNs each, <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula> input cues (one-hot input vectors), and <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula> actions, with one correct action for each input state. Feedforward SPN activity is given by<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The log-likelihood of an action <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> being performed is proportional to<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are randomly sampled uniformly in the interval [0, 1] and then normalized so that each vector <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> has norm 1. Thus, the contribution of each dSPN/iSPN to the promotion/suppression of each action is randomly distributed.</p><p>In the efference model, following selection of an action <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, activities of the SPNs associated with action <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are updated as follows, so that efference excites the modes <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> associated with the selected action:<disp-formula id="equ28">,<label>(28)</label><mml:math id="m28"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We also experiment with a generalization of the canonical action selection model to this distributed action tuning architecture, in which following action selection, SPN activity is set to<disp-formula id="equ30">,<label>(30)</label><mml:math id="m30"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munder><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ζ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In this model, dSPNs are excited in proportion to their contribution to the currently selected action and iSPNs are suppressed in proportion to their degree of inhibition of the currently selected action.</p><p>The plasticity rules used are the same as in the main text.</p><p>We find that the results of the main text – that the canonical action selection model fails to learn from negative rewards, while the efference model successully learns from both reward protocols – is replicated (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec sec-type="appendix" id="s10"><title>Alternative model with shared plasticity rule among all SPNs</title><p>The issues identified in <xref ref-type="fig" rid="fig2">Figure 2</xref> with the canonical action selection model are a consequence of the iSPN plasticity rule. From a normative perspective, it is interesting to consider why the empirically observed iSPN plasticity rule might be advantageous, compared to an alternative model in which iSPNs share the same plasticity rule as dSPNs. For instance, this alternative model can solve the two-alternative forced choice task of <xref ref-type="fig" rid="fig2">Figure 2</xref> with both positive and negative reward protocols (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, left). However, the limitations of this alternative model are revealed in the off-policy learning setting, where the Q-learning algorithm is required. In this case, SPN activity must encode <italic>Q</italic>-values associated with each action, but in the canonical action selection model, these values are disrupted by the updates to SPN activity following action selection. This is because the activity updates in the canonical action selection model modify difference mode activity, which (when dSPN and iSPN plasticity rules are the same) is needed for learning (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1B</xref>). As a result, the predicted <italic>Q</italic>-values are inaccurate, and the model has difficulty learning the true value of each action. We demonstrate this in the two-alternative forced task in an off-policy learning protocol where an oracle chooses the correct action on each trial, and the striatal pathway’s ability to solve the task independently is evaluated. The efference activity model has no issue due to the orthogonality of the efferent activity and difference modes as described above, but the canonical action selection model fails to solve the task (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>, right).</p><p>We note that non-orthogonality of the activity mode used for learning and behavior could cause other problems besides impairing the system’s ability to implement off-policy learning algorithms; for instance, even in an on-policy setting, it could interfere with sequential action selection at short timescales.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101747.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Langdon</surname><given-names>Angela</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>National Institute of Mental Health</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Fundamental</kwd></kwd-group></front-stub><body><p>The authors present a biologically plausible framework for action selection and learning in the striatum that is a <bold>fundamental</bold> advance in our understanding of possible neural implementations of reinforcement learning in the basal ganglia. They provide <bold>compelling</bold> evidence that their model can reconcile realistic neural plasticity rules with the distinct functional roles of the direct and indirect spiny projection neurons of the striatum, recapitulating experimental findings regarding the activity profiles of these distinct neural populations and explaining a key aspect of striatal function.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101747.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors propose a new model of biologically realistic reinforcement learning in the direct and indirect pathway spiny projection neurons in the striatum. These pathways are widely considered to provide a neural substrate for reinforcement learning in the brain. However, we do not yet have a full understanding of mechanistic learning rules that would allow successful reinforcement learning like computations in these circuits. The authors outline some key limitations of current models and propose an interesting solution by leveraging learning with efferent inputs of selected actions. They show that the model simulations are able to recapitulate experimental findings about the activity profile in these populations in mice during spontaneous behavior. They also show how their model is able to implement off-policy reinforcement learning.</p><p>Strengths:</p><p>The manuscript has been very clearly written and the results have been presented in a readily digestible manner. The limitations of existing models, that motive the presented work, have been clearly presented and the proposed solution seems very interesting. The novel contribution in the proposed model is the idea that different patterns of activity drive current action selection and learning. Not only does this allow the model is able to implement reinforcement learning computations well, this suggestion may have interesting implications regarding why some processes selectively affect ongoing behavior and others affect learning. The model is able to recapitulate some interesting experimental findings about various activity characteristics of dSPN and iSPN pathway neuronal populations in spontaneously behaving mice. The authors also show that their proposed model can implement off-policy reinforcement learning algorithms with biologically realistic learning rules. This is interesting since off-policy learning provides some unique computational benefits and it is very likely that learning in neural circuits may, at least to some extent, implement such computations.</p><p>Weaknesses:</p><p>A weakness in this work is that it isn't clear how a key component in the model - an efferent copy of selected actions - would be accessible to these striatal populations. The authors propose several plausible candidates, but future work may clarify the feasibility of this proposal.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101747.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The basal ganglia is often understood within a reinforcement learning (RL) framework, where dopamine neurons convey a reward prediction error which modulates cortico-striatal connections onto spiny projection neurons (SPNS) in the striatum. However, current models of plasticity rules are inconsistent with learning in a reinforcement learning framework.</p><p>This paper proposes a new model that describes how distinct learning rules in direct and indirect pathway striatal neurons allows them to implement reinforcement learning models. It proposes that two distinct component of striatal activity affect action selection and learning. They show that the proposed implementation allows learning in simple tasks and is consistent with experimental data from calcium imaging data in direct and indirect SPNs in freely moving mouse.</p><p>Strengths:</p><p>Despite the success of reward prediction errors at characterizing the responses of dopamine neurons as the temporal difference error within an RL framework, the implementation of RL algorithms in the rest of the basal ganglia has been unclear. A key missing aspect has been the lack of a RL implementation that is consistent with the distinction of direct- and indirect SPNs. This paper proposes a new model that is able to learn successfully in simple RL tasks and explains recent experimental results.</p><p>The author shows that their proposed model, unlike previous implementations, this model can perform well in RL tasks. The new model allows them to make experimental predictions. They test some of these predictions and show that the dynamics of dSPNs and iSPNs correspond to model predictions.</p><p>More generally, this new model can be used to understand striatal dynamics across direct and indirect SPNs in future experiments.</p><p>Weaknesses:</p><p>The authors could characterize better the reliability of their experimental predictions and the description of the parameters of some of the simulations</p><p>The authors propose some ideas about how the specificity of the striatal efferent inputs but should highlight better that this is a key feature of the model whose anatomical implementation has yet to be resolved.</p><p>Comments on revisions:</p><p>I thank the authors for their response to public and private reviews and for the clarifications and changes to the manuscript which have strengthened it. I understand the inability to implement some of the proposed additional simulation due to authors having left academia and the request for a version of record.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101747.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper points out an inconsistency of the roles of the striatal spiny neurons projecting to the indirect pathway (iSPN) and the synaptic plasticity rule of those neurons expressing dopamine D2 receptors, and proposes a novel, intriguing mechanisms that iSPNs are activated by the efference copy of the chosen action that they are supposed to inhibit.</p><p>The proposed model was supported by simulations and analysis of the neural recording data during spontaneous behaviors.</p><p>Strengths:</p><p>Previous models suggested that the striatal neurons learn action values functions, but how the information about the chosen action is fed back to the striatum for learning was not clear. The author pointed out that this is a fundamental problem for iSPNs that are supposed to inhibit specific actions and its synaptic inputs are potentiated with dopamine dips.</p><p>The authors proposes a novel hypothesis that iSPNs are activated by efference copy of the selected action which they are supposed to inhibit during action selection. Even though intriguing and seemingly unnatural, the authors demonstrated that the model based on the hypothesis can circumvent the problem of iSPNs learning to disinhibit the actions associated with negative reward errors. They further showed by analyzing the cell-type specific neural recording data by Markowitz et al. (2018) that iSPN activities tend to be anti-correlated before and after action selection.</p><p>Weaknesses:</p><p>(1) It is not correct to call the action value learning using the externally-selected action as &quot;off-policy.&quot; Both off-policy algorithm Q-learning and on-policy algorithm SARSA update the action value of the chosen action, which can be different from the greedy action implicated by the present action values. In standard reinforce learning terminology, on-policy or off-policy is regarding the actions in the subsequent state, whether to use the next action value of (to be) chosen action or that of greedy choice as in equation (7).</p><p>It is worth noting that this paper suggested that dopamine neurons encode on-policy TD errors: Morris G, Nevet A, Arkadir D, Vaadia E, Bergman H (2006). Midbrain dopamine neurons encode decisions for future action. Nat Neurosci, 9, 1057-63. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1743">https://doi.org/10.1038/nn1743</ext-link></p><p>(2) It is also confusing to contract TD learning and Q-learning, as the latter is considered as on type of TD learning. In the TD error signal by state value function (6) is dependent on the chosen action a_{t-1} implicitly in r_t and s_t based on the reward and state transition function.</p><p>(3) It is not clear why interferences of the activities for action selection and learning can be avoided, especially when actions are taken with short intervals or even temporal overlaps. How can the efference copy activation for the previous action be dissociated with the sensory cued activation for the next action selection?</p><p>(4) Although it may be difficult to single out the neural pathway that carries the efference copy signal to the striatum, it is desired to consider their requirements and difference possibilities. A major issue is that the time delay from actions to reward feedback can be highly variable.</p><p>An interesting candidate is the long-latency neurons in the CM thalamus projecting to striatal cholinergic interneurons, which are activated following low-reward actions:</p><p>Minamimoto T, Hori Y, Kimura M (2005). Complementary process to response bias in the centromedian nucleus of the thalamus. Science, 308, 1798-801. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1109154">https://doi.org/10.1126/science.1109154</ext-link></p><p>(5) In the paragraph before Eq. (3), Eq (1) should be Eq. (2) for the iSPN.</p><p>Here are comments back to the authors' replies with the revised version:</p><p>(1) I do not agree on the use of inaccurate technical terms. On-policy does not require that the policy is greedy with respect to the actions values, as authors seem to assume here.</p><p>In fact, the policy (10) is just a standard soft-max action selection based on the action values by the difference of dSPN and iSPN outputs.</p><p>Furthermore, in the immediate reward setting tested in this paper, action values are independent of the policy, so there is no distinction between on-policy vs. off-policy. This is also apparent from the &quot;TD&quot; errors in (19) and (21), where there is no TD.</p><p>(2) To really compare the different forms of TD, multi-step RL tasks should be used.</p><p>(3) This fundamental limitation should be explicitly documented in the manuscript. This is not just the same as any RL algorithms. Having two action representations within each action step make temporal credit assignment more difficult.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101747.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Markowitz</surname><given-names>Jeffrey</given-names></name><role specific-use="author">Author</role><aff><institution>Georgia Tech</institution><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Gillis</surname><given-names>Winthrop F</given-names></name><role specific-use="author">Author</role><aff><institution>Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Datta</surname><given-names>Sandeep R</given-names></name><role specific-use="author">Author</role><aff><institution>Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>Summary:</p><p>The authors propose a new model of biologically realistic reinforcement learning in the direct and indirect pathway spiny projection neurons in the striatum. These pathways are widely considered to provide a neural substrate for reinforcement learning in the brain. However, we do not yet have a full understanding of mechanistic learning rules that would allow successful reinforcement learning like computations in these circuits. The authors outline some key limitations of current models and propose an interesting solution by leveraging learning with efferent inputs of selected actions. They show that the model simulations are able to recapitulate experimental findings about the activity profile in these populations of mice during spontaneous behavior. They also show how their model is able to implement off-policy reinforcement learning.</p><p>Strengths:</p><p>The manuscript has been very clearly written and the results have been presented in a readily digestible manner. The limitations of existing models, that motivate the presented work, have been clearly presented and the proposed solution seems very interesting. The novel contribution of the proposed model is the idea that different patterns of activity drive current action selection and learning. Not only does this allow the model is able to implement reinforcement learning computations well, but this suggestion may have interesting implications regarding why some processes selectively affect ongoing behavior and others affect learning. The model is able to recapitulate some interesting experimental findings about various activity characteristics of dSPN and iSPN pathway neuronal populations in spontaneously behaving mice. The authors also show that their proposed model can implement off-policy reinforcement learning algorithms with biologically realistic learning rules. This is interesting since off-policy learning provides some unique computational benefits and it is very likely that learning in neural circuits may, at least to some extent, implement such computations.</p></disp-quote><p>We thank the reviewer for the positive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>A weakness in this work is that it isn’t clear how a key component in the model - an efferent copy of selected actions - would be accessible to these striatal populations. The authors propose several plausible candidates, but future work may clarify the feasibility of this proposal.</p></disp-quote><p>We agree that the biological substrate of the efference copy remains a key open question. We discuss potential pathways in the Discussion section of our manuscript and hope that future experimental studies clarify the question.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>Summary:</p><p>The basal ganglia is often understood within a reinforcement learning (RL) framework, where dopamine neurons convey a reward prediction error that modulates cortico-striatal connections onto spiny projection neurons (SPNS) in the striatum. However, current models of plasticity rules are inconsistent with learning in a reinforcement learning framework.</p><p>This paper proposes a new model that describes how distinct learning rules in direct and indirect pathway striatal neurons allow them to implement reinforcement learning models. It proposes that two distinct components of striatal activity affect action selection and learning. They show that the proposed implementation allows learning in simple tasks and is consistent with experimental data from calcium imaging data in direct and indirect SPNs in freely moving mice.</p><p>Strengths:</p><p>Despite the success of reward prediction errors at characterizing the responses of dopamine neurons as the temporal difference error within an RL framework, the implementation of RL algorithms in the rest of the basal ganglia has been unclear. A key missing aspect has been the lack of a RL implementation that is consistent with the distinction of direct- and indirect SPNs. This paper proposes a new model that is able to learn successfully in simple RL tasks and explains recent experimental results.</p><p>The author shows that their proposed model, unlike previous implementations, this model can perform well in RL tasks. The new model allows them to make experimental predictions. They test some of these predictions and show that the dynamics of dSPNs and iSPNs correspond to model predictions.</p><p>More generally, this new model can be used to understand striatal dynamics across direct and indirect SPNs in future experiments.</p></disp-quote><p>We thank the reviewer for the positive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The authors could characterize better the reliability of their experimental predictions and the description of the parameters of some of the simulations.</p></disp-quote><p>In addition to the descriptions in the Methods, we have provided code implementing the key features of our simulations, which should contribute to reproducibility of our results.</p><disp-quote content-type="editor-comment"><p>The authors propose some ideas about how the specificity of the striatal efferent inputs but should highlight better that this is a key feature of the model whose anatomical implementation has yet to be resolved.</p></disp-quote><p>We have clarified in the Discussion section “Biological substrates of striatal efferent inputs” that these represent assumptions or predictions that have not yet been demonstrated experimentally.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Summary:</p><p>This paper points out an inconsistency of the roles of the striatal spiny neurons projecting to the indirect pathway (iSPN) and the synaptic plasticity rule of those neurons expressing dopamine D2 receptors and proposes a novel, intriguing mechanisms that iSPNs are activated by the efference copy of the chosen action that they are supposed to inhibit.</p><p>The proposed model was supported by simulations and analysis of the neural recording data during spontaneous behaviors.</p><p>Strengths:</p><p>Previous models suggested that the striatal neurons learn action-value functions, but how the information about the chosen action is fed back to the striatum for learning was not clear. The author pointed out that this is a fundamental problem for iSPNs that are supposed to inhibit specific actions and its synaptic inputs are potentiated with dopamine dips.</p><p>The authors propose a novel hypothesis that iSPNs are activated by efference copy of the selected action which they are supposed to inhibit during action selection. Even though intriguing and seemingly unnatural, the authors demonstrated that the model based on the hypothesis can circumvent the problem of iSPNs learning to disinhibit the actions associated with negative reward errors. They further showed by analyzing the cell-type specific neural recording data by Markowitz et al. (2018) that iSPN activities tend to be anti-correlated before and after action selection.</p></disp-quote><p>We thank the reviewer for the positive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>It is not correct to call the action value learning using the externally-selected action as “offpolicy.” Both off-policy algorithm Q-learning and on-policy algorithm SARSA update the action value of the chosen action, which can be different from the greedy action implicated by the present action values. In standard reinforcement learning terminology, on-policy or off-policy is regarding the actions in the subsequent state, whether to use the next action value of (to be) chosen action or that of greedy choice as in equation (7).</p><p>It is worth noting that this paper suggested that dopamine neurons encode on-policy TD errors: Morris G, Nevet A, Arkadir D, Vaadia E, Bergman H (2006). Midbrain dopamine neurons encode decisions for future action. Nat Neurosci, 9, 1057-63. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1743">https://doi.org/10.1038/nn1743</ext-link>.</p></disp-quote><p>We regret that we do not completely follow the reviewer’s comment. We use “off-policy” to refer to the fact that, considered in isolation, the basal ganglia reinforcement learning system that we model learns a target policy that may be distinct from the behavioral policy of the organism as a whole.</p><disp-quote content-type="editor-comment"><p>It is also confusing to contract TD learning and Q-learning, as the latter is considered as one type of TD learning. In the TD error signal by state value function (6) is dependent on the chosen action <italic>at</italic>−1 implicitly in <italic>rt</italic> and <italic>st</italic> based on the reward and state transition function.</p></disp-quote><p>We agree that this was confusing. We have therefore changed the places in our paper where we intended to refer to “TD learning of a value function <italic>V</italic> (<italic>s</italic>)” to specifically mention <italic>V</italic> (<italic>s</italic>), rather than just “TD learning.”</p><disp-quote content-type="editor-comment"><p>It is not clear why interferences of the activities for action selection and learning can be avoided, especially when actions are taken with short intervals or even temporal overlaps. How can the efference copy activation for the previous action be dissociated with the sensory cued activation for the next action selection?</p></disp-quote><p>The non-interference arises from the orthogonality of the difference (action selection) and sum (efference copy) modes, as described in Figure 3. However, we agree with the reviewer that the problem of temporal credit assignment, when many actions are taken before reward feedback is obtained, is present in our model, as in any standard RL model.</p><disp-quote content-type="editor-comment"><p>Although it may be difficult to single out the neural pathway that carries the efference copy signal to the striatum, it is desired to consider their requirements and difference possibilities. A major issue is that the time delay from actions to reward feedback can be highly variable.</p><p>An interesting candidate is the long-latency neurons in the CM thalamus projecting to striatal cholinergic interneurons, which are activated following low-reward actions: Minamimoto T, Hori Y, Kimura M (2005). Complementary process to response bias in the centromedian nucleus of the thalamus. Science, 308, 1798-801. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1109154">https://doi.org/10.1126/science.1109154</ext-link>.</p></disp-quote><p>We are grateful for the interesting suggestion and reference, which we have added to the manuscript. However, we note that the issue of delayed reward feedback may also be partially addressed by using a sufficiently long eligibility trace.</p><disp-quote content-type="editor-comment"><p>In the paragraph before Eq. (3), Eq. (1) should be Eq. (2) for the iSPN.</p></disp-quote><p>Corrected.</p></body></sub-article></article>