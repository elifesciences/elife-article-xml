<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">92184</article-id><article-id pub-id-type="doi">10.7554/eLife.92184</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92184.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Protein language model-embedded geometric graphs power inter-protein contact prediction</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-333172"><name><surname>Si</surname><given-names>Yunda</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-333171"><name><surname>Yan</surname><given-names>Chengfei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2010-6668</contrib-id><email>chengfeiyan@hust.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00p991c53</institution-id><institution>School of Physics, Huazhong University of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Wuhan</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution></institution-wrap><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>04</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP92184</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-04"><day>04</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-24"><day>24</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.07.523121"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-12"><day>12</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92184.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-14"><day>14</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92184.2"/></event></pub-history><permissions><copyright-statement>© 2023, Si and Yan</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Si and Yan</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-92184-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-92184-figures-v2.pdf"/><abstract><p>Accurate prediction of contacting residue pairs between interacting proteins is very useful for structural characterization of protein–protein interactions. Although significant improvement has been made in inter-protein contact prediction recently, there is still a large room for improving the prediction accuracy. Here we present a new deep learning method referred to as PLMGraph-Inter for inter-protein contact prediction. Specifically, we employ rotationally and translationally invariant geometric graphs obtained from structures of interacting proteins to integrate multiple protein language models, which are successively transformed by graph encoders formed by geometric vector perceptrons and residual networks formed by dimensional hybrid residual blocks to predict inter-protein contacts. Extensive evaluation on multiple test sets illustrates that PLMGraph-Inter outperforms five top inter-protein contact prediction methods, including DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter, by large margins. In addition, we also show that the prediction of PLMGraph-Inter can complement the result of AlphaFold-Multimer. Finally, we show leveraging the contacts predicted by PLMGraph-Inter as constraints for protein–protein docking can dramatically improve its performance for protein complex structure prediction.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>protein–protein interaction</kwd><kwd>inter-protein contact prediction</kwd><kwd>protein complex structure prediction</kwd><kwd>protein language models</kwd><kwd>geometric deep learning</kwd><kwd>protein–protein docking</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32101001</award-id><principal-award-recipient><name><surname>Yan</surname><given-names>Chengfei</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003397</institution-id><institution>Huazhong University of Science and Technology</institution></institution-wrap></funding-source><award-id>3004012167</award-id><principal-award-recipient><name><surname>Yan</surname><given-names>Chengfei</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Integrating multiple protein language models using protein geometric graphs can dramatically improve the model performance for predicting the contacting residue pairs between interacting proteins.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Protein–protein interactions (PPIs) are essential activities of most cellular processes (<xref ref-type="bibr" rid="bib1">Alberts, 1998</xref>; <xref ref-type="bibr" rid="bib38">Spirin and Mirny, 2003</xref>). Structure characterization of PPIs is important for mechanistic investigation of these cellular processes and therapeutic development (<xref ref-type="bibr" rid="bib9">Goodsell and Olson, 2000</xref>). However, currently experimental structures of many important PPIs are still missing as experimental methods to resolve complex structures such as X-ray crystallography, nuclear magnetic resonance, and cryo-electron microscopy are costly and time-consuming (<xref ref-type="bibr" rid="bib3">Berman et al., 2000</xref>). Therefore, it is necessary to develop computational methods to predict protein complex structures (<xref ref-type="bibr" rid="bib4">Bonvin, 2006</xref>). Predicting contacting residue pairs between interacting proteins can be considered an intermediate step for protein complex structure prediction (<xref ref-type="bibr" rid="bib14">Hopf et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Ovchinnikov et al., 2014</xref>) as the predicted contacts can be integrated into protein–protein docking algorithms to assist protein complex structure prediction (<xref ref-type="bibr" rid="bib7">Dominguez et al., 2003</xref>; <xref ref-type="bibr" rid="bib23">Li and Huang, 2021</xref>; <xref ref-type="bibr" rid="bib41">Sun et al., 2020</xref>). Besides, the predicted contacts can also be very useful to guide protein interfacial design (<xref ref-type="bibr" rid="bib25">Martino et al., 2021</xref>) and the inter-protein contact prediction methods can be further extended to predict novel PPIs (<xref ref-type="bibr" rid="bib5">Cong et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Green et al., 2021</xref>).</p><p>Based on the fact that contacting residue pairs often vary co-operatively during evolution, coevolutionary analysis methods <xref ref-type="bibr" rid="bib47">Weigt et al., 2009</xref> have been used in previous studies to predict inter-protein contacts (<xref ref-type="bibr" rid="bib14">Hopf et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Ovchinnikov et al., 2014</xref>). However, coevolutionary analysis methods do have certain limitations. For example, effective coevolutionary analysis requires a large number of interolog sequences, which are often difficult to obtain, especially for heteromeric PPIs (<xref ref-type="bibr" rid="bib14">Hopf et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Ovchinnikov et al., 2014</xref>), and it is difficult to distinguish inter-protein and intra-protein coevolutionary signals for homomeric PPIs (<xref ref-type="bibr" rid="bib43">Uguzzoni et al., 2017</xref>). Inspired by its great success in intra-protein contact prediction (<xref ref-type="bibr" rid="bib12">Hanson et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Ju et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Li et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Si and Yan, 2021</xref>; <xref ref-type="bibr" rid="bib46">Wang et al., 2017</xref>), deep learning has also been applied to predict inter-protein contacts (<xref ref-type="bibr" rid="bib11">Guo et al., 2022</xref>; <xref ref-type="bibr" rid="bib32">Roy et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Xie and Xu, 2022</xref>; <xref ref-type="bibr" rid="bib51">Yan and Huang, 2021</xref>; <xref ref-type="bibr" rid="bib54">Zeng et al., 2018</xref>). ComplexContact (<xref ref-type="bibr" rid="bib54">Zeng et al., 2018</xref>), to the best of our knowledge, the first deep learning method for inter-protein contact prediction, has significantly improved the prediction accuracy over coevolutionary analysis methods. However, its performance on eukaryotic PPIs is still quite limited, partly due to the difficulty of accurately inferring interologs for eukaryotic PPIs. In a later study, coming from the same group as ComplexContact, Xie et al. developed GLINTER (<xref ref-type="bibr" rid="bib50">Xie and Xu, 2022</xref>), another deep learning method for inter-protein contact prediction. Compared with ComplexContact, GLINTER leverages structures of interacting monomers, from which their rotational invariant graph representations are used as additional input features. GLINTER outperforms ComplexContact in the prediction accuracy, although there is still large room for improvement, especially for heteromeric PPIs. It is worth mentioning that CDPred (<xref ref-type="bibr" rid="bib11">Guo et al., 2022</xref>), a recently developed method, further surpasses GLINTER in prediction accuracy with 2D attention-based neural networks. Apart from these methods developed to predict inter-protein contacts for both homomeric and heteromeric PPIs, inter-protein contact prediction methods specifically for homomeric PPIs were also developed (<xref ref-type="bibr" rid="bib32">Roy et al., 2022</xref>; <xref ref-type="bibr" rid="bib48">Wu et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Yan and Huang, 2021</xref>) as predicting the inter-protein contacts for homomeric PPIs is generally much easier due to the symmetric restriction, relatively larger interfaces and the trivialness of interologs identification. For example, Yan et al. developed DeepHomo (<xref ref-type="bibr" rid="bib51">Yan and Huang, 2021</xref>), a deep learning method specifically to predict inter-protein contacts of homomeric PPIs, which also significantly outperforms coevolutionary analysis-based methods. However, DeepHomo requires docking maps calculated from structures of interacting monomers, which is computationally expensive and is also sensitive to the quality of monomeric structures. Besides, coming from the same group, <xref ref-type="bibr" rid="bib24">Lin et al., 2023</xref> further developed DeepHomo2 for inter-protein contact prediction for homomeric PPIs by including the multiple sequence alignment (MSA) embeddings and attentions from an MSA-based protein language model (PLM) (MSA transformer) (<xref ref-type="bibr" rid="bib30">Rao et al., 2021b</xref>) in their prediction model, which further improved the prediction performance. At almost the same time as DeepHomo2, we proved that embeddings from PLMs (<xref ref-type="bibr" rid="bib29">Rao et al., 2021a</xref>; <xref ref-type="bibr" rid="bib31">Rives et al., 2021</xref>) are very effective features in predicting inter-protein contacts for both homomeric and heteromeric PPIs, and we further show the sequence embeddings (ESM-1b [<xref ref-type="bibr" rid="bib31">Rives et al., 2021</xref>]), MSA embeddings (ESM-MSA-1b [<xref ref-type="bibr" rid="bib30">Rao et al., 2021b</xref>] and Position-Specific Scoring Matrix [PSSM]), and the inter-protein coevolutionary information complement each other in the prediction, with which we developed DRN-1D2D_Inter (<xref ref-type="bibr" rid="bib36">Si and Yan, 2023</xref>). Extensive benchmark results show that DRN-1D2D_Inter significantly outperforms DeepHomo and GLINTER in inter-protein contact prediction, although DRN-1D2D_Inter makes the prediction purely from sequences.</p><p>In this study, we developed a structure-informed method to predict inter-protein contacts. Given the structures of two interacting proteins, we first build rotationally and translationally (SE(3)) invariant geometric graphs from the two monomeric structures, which encode both the inter-residue distance and orientation information of the monomeric structures. We further embedded the single-sequence embeddings (ESM-1b), MSA embeddings (ESM-MSA-1b and PSSM), and structure embeddings (ESM-IF [<xref ref-type="bibr" rid="bib15">Hsu et al., 2022</xref>]) from PLMs in the graph nodes of the corresponding residues to build the PLM-embedded geometric graphs, which are then transformed by graph encoders formed by geometric vector perceptrons (GVPs) to generate graph embeddings for interacting monomers. The graph embeddings are further combined with inter-protein pairwise features and transformed by residual networks formed by dimensional hybrid residual blocks (residual block hybridizing 1D and 2D convolutions) to predict inter-protein contacts. The developed method referred to as PLMGraph-Inter was extensively benchmarked on multiple tests with application of either experimental or predicted structures of interacting monomers as the input. The result shows that in both cases PLMGraph-Inter outperforms other top prediction methods, including DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter by large margins. In addition, we also compared the prediction results of PLMGraph-Inter with the protein complex structures generated by AlphaFold-Multimer (<xref ref-type="bibr" rid="bib8">Evans et al., 2022</xref>). The result shows that for many targets that AlphaFold-Multimer made poor predictions, PLMGraph-Inter yielded better results. Finally, we show leveraging the contacts predicted by PLMGraph-Inter as constraints for protein–protein docking can dramatically improve its performance for protein complex structure prediction.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Overview of PLMGraph-Inter</title><p>The method of PLMGraph-Inter is summarized in <xref ref-type="fig" rid="fig1">Figure 1a</xref>. PLMGraph-Inter consists of three modules: the graph representation module (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), the graph encoder module (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), and the residual network module (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). Each interacting monomer is first transformed into a PLM-embedded graph by the graph representation module, then the graph is passed through the graph encoder module to obtain a 1D representation of each protein. The two protein representations are transformed into 2D pairwise features through outer concatenation (horizontal and vertical tiling followed by concatenation) and further concatenated with other 2D pairwise features including the inter-protein attention maps and the inter-protein coevolution matrices, which are then transformed by the residual network module to obtain the predicted inter-protein contact map.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of PLMGraph-Inter.</title><p>(<bold>a</bold>) The network architecture of PLMGraph-Inter. (<bold>b</bold>) The graph representation module. (<bold>c</bold>) The graph encoder module, s denotes scalar features, v denotes vector features. (<bold>d</bold>) The dimensional hybrid residual block (‘IN’ denotes Instance Normalization).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>The graph representation of protein structures.</title><p>(<bold>a</bold>) Dihedral angles of the protein backbone. (<bold>b</bold>) The local coordinate system of each amino acid. (<bold>c</bold>) The scalar (distances) and vector (directions) of the edge i-&gt;j.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig1-figsupp1-v2.tif"/></fig></fig-group><sec id="s2-1-1"><title>The graph representation module</title><p>The first step of the graph representation module is to represent the protein 3D structure as a geometric graph, where each residue is represented as a node, and an edge is defined if the <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atom distance between two residues is less than 18 Å. For each node and edge, we use scalars and vectors extracted from the 3D structures as their geometric features. To make the geometric graph SE(3) invariant, we use a set of local coordinate systems to extract the geometric vectors. The SE(3) invariance of representation of each interacting monomer is important as in principle the inter-protein contact prediction result should not depend on the initial positions and orientations of protein structures. A detailed description can be found in the ‘Methods’ section. The second step is to integrate the single-sequence embedding from ESM-1b (<xref ref-type="bibr" rid="bib31">Rives et al., 2021</xref>), the MSA embedding from ESM-MSA-1b (<xref ref-type="bibr" rid="bib29">Rao et al., 2021a</xref>), the PSSM calculated from the MSA, and structure embedding from ESM-IF (<xref ref-type="bibr" rid="bib15">Hsu et al., 2022</xref>) for each interacting monomer using its corresponding geometric graph. Where ESM-1b and ESM-MSA-1b are pretrained PLMs learned from large datasets of sequences and MSAs respectively with masked language modeling tasks, and ESM-IF is a supervised PLM trained from 12 million protein structures predicted by AlphaFold2 (<xref ref-type="bibr" rid="bib20">Jumper et al., 2021</xref>) for fixed backbone design. The embeddings from these models contain high-dimensional representations of each residue in the protein, which are concatenated and further combined with the PSSM to form additional features of each node in the geometric graph. Since the sequence embeddings, MSA embeddings, PSSM, and structure embeddings are all SE(3) invariant, the PLM-embedded geometric graph of each protein is also SE(3) invariant.</p></sec><sec id="s2-1-2"><title>The graph encoder module</title><p>The graph encoder module is formed by GVP and GVP convolutional layer (GVPConv) (<xref ref-type="bibr" rid="bib16">Jing et al., 2021a</xref>; <xref ref-type="bibr" rid="bib17">Jing et al., 2021b</xref>). Where GVP is a graph neural network module consisting of a scalar track and a vector track, which can perform rotationally invariant transformations on scalar features and rotationally equivariant on vector features of nodes and edges; GVPConv follows the message passing paradigm of graph neural network and mainly consists of GVP, which updates the embedding of each node by passing information from its neighboring nodes and edges. A detailed description of GVP and GVPConv can be found in the ‘Methods’ section and also in the work of GVP (<xref ref-type="bibr" rid="bib16">Jing et al., 2021a</xref>; <xref ref-type="bibr" rid="bib17">Jing et al., 2021b</xref>). For each protein graph, we first use a GVP module to reduce the dimension of the scalar features of each node from 2586 to 256, which is then transformed successively by three GVPConv layers. Finally, we stitch the scalar features and the vector features of each node to form the 1D representation of the protein. Since the input protein graph is SE(3) invariant and the GVP and GVPConv transformations are rotationally equivariant, the 1D representation of each interacting monomer is also SE(3) invariant.</p></sec><sec id="s2-1-3"><title>The residual network module</title><p>The residual network module is mainly formed by nine-dimensional hybrid residual blocks to transform the 2D feature maps to obtain the predicted inter-protein contact map. Our previous study illustrated the effective receptive field can be enlarged with the application of the dimensional hybrid residual block, thus helping in improving the model performance (<xref ref-type="bibr" rid="bib34">Si and Yan, 2021</xref>). A more detailed description of the transforming procedure can be found in the ‘Methods’ section.</p></sec></sec><sec id="s2-2"><title>Evaluation of PLMGraph-Inter on HomoPDB and HeteroPDB test sets</title><p>We first evaluated PLMGraph-Inter on two self-built test sets which are non-redundant to the training dataset of PLMGraph-Inter: HomoPDB and HeteroPDB. Where HomoPDB is the test set for homomeric PPIs containing 400 homodimers and HeteroPDB is the test set for heteromeric PPIs containing 200 heterodimers. For comparison, we also evaluated DeepHomo, GLINTER, DeepHomo2, CDPred, and DRN-1D2D_Inter on the same datasets. Since DeepHomo and DeepHomo2 were developed to predict inter-protein contacts only for homomeric PPIs, their evaluation was only performed on HomoPDB. It should be noted that since HomoPDB and HeteroPDB are not de-redundant with the training sets of DeepHomo, DeepHomo2, CDPred, and GLINTER, the performances of the four methods may be overestimated.</p><p>In all the evaluations, the structural-related features were drawn from experimental structures of interacting monomers separated from complex structures of PPIs after randomizing their initial positions and orientations (DRN-1D2D_Inter does not use structural information). Besides, we also used the AlphaFold2 predicted monomeric structures as the input, considering experimental structures of interacting monomers often do not exist. Since the interacting monomers in HomoPDB and HeteroPDB are not de-redundant with the training set of AlphaFold2, using default settings of AlphaFold2 may overestimate its performance. To mimic the performance of AlphaFold2 in real practice and produce predicted monomeric structures with more diverse qualities, we only used the MSA searched from Uniref100 ( <xref ref-type="bibr" rid="bib42">Suzek et al., 2015</xref>) protein sequence database as the input of AlphaFold2 and set to not use the template. The predicted structures yielded a mean TM-score of 0.88, which is close to the performance of AlphaFold2 for CASP14 targets (mean TM-score 0.85) (<xref ref-type="bibr" rid="bib24">Lin et al., 2023</xref>).</p><p><xref ref-type="table" rid="table1">Table 1</xref> shows the mean precision of each method on the HomoPDB and HeteroPDB when the top (5, 10, 50, L/10, L/5) predicted inter-protein contacts are considered, where L denotes the sequence length of the shorter protein in the PPI. (Note that GLINTER encountered errors for 81 [5 when using the predicted monomeric structures] targets in HomoPDB and 15 [3 when using the predicted monomeric structures] targets in HeteroPDB at run time and did not produce predictions, thus we removed these targets in the evaluation of the performance of GLINTER. The performances of HomoPDB and HeteroPDB for these methods after the removal of these targets which GLINTER failed in any case are shown in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.) As can be seen from the table, whenever the experimental or the predicted monomeric structures were used as the input, the mean precision of PLMGraph-Inter far exceeds those of other algorithms in each metric for both datasets. Particularly, the mean precision of PLMGraph-Inter is substantially improved in each metric on each dataset compared to our previous method DRN-1D2D_Inter, which used most features of PLMGraph-Inter except those drawn from the structures of the interacting monomers, illustrating the importance of the inclusion of structural information. Besides, GLINTER, CDPred, and DeepHomo2 also use structural information and PLMs, but have much lower performance than PLMGraph-Inter, illustrating the efficacy of our deep learning framework.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred, and PLMGraph-Inter on the HomoPDB and HeteroPDB test sets using experimental structures (AlphaFold2 predicted structures).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Methods</th><th align="left" valign="bottom" colspan="5">HomoPDB (precision %)</th><th align="left" valign="bottom" colspan="5">HeteroPDB (precision %)</th></tr><tr><th align="left" valign="bottom">L/5</th><th align="left" valign="bottom">L/10</th><th align="left" valign="bottom">50</th><th align="left" valign="bottom">10</th><th align="left" valign="bottom">5</th><th align="left" valign="bottom">L/5</th><th align="left" valign="bottom">L/10</th><th align="left" valign="bottom">50</th><th align="left" valign="bottom">10</th><th align="left" valign="bottom">5</th></tr></thead><tbody><tr><td align="left" valign="bottom">DeepHomo</td><td align="left" valign="bottom">43.2<break/>(39.3)</td><td align="left" valign="bottom">46.7<break/>(42.7)</td><td align="left" valign="bottom">42.4<break/>(38.8)</td><td align="left" valign="bottom">48.5<break/>(44.8)</td><td align="char" char="." valign="bottom">49.9<break/>(46.2)</td><td align="left" valign="bottom" colspan="5"/></tr><tr><td align="left" valign="bottom">GLINTER</td><td align="left" valign="bottom">42.9<break/>(47.3)</td><td align="left" valign="bottom">45.0<break/>(50.1)</td><td align="left" valign="bottom">42.2<break/>(52.1)</td><td align="left" valign="bottom">46.4<break/>(51.9)</td><td align="char" char="." valign="bottom">48.5<break/>(53.6)</td><td align="left" valign="bottom">23.9<break/>(25.1)</td><td align="left" valign="bottom">24.7<break/>(27.0)</td><td align="left" valign="bottom">20.9<break/>(21.9)</td><td align="left" valign="bottom">25.5<break/>(25.8)</td><td align="left" valign="bottom">26.7<break/>(26.2)</td></tr><tr><td align="left" valign="bottom">DRN-1D2D_Inter</td><td align="left" valign="bottom">52.5</td><td align="left" valign="bottom">55.2</td><td align="left" valign="bottom">51.3</td><td align="left" valign="bottom">56.6</td><td align="char" char="." valign="bottom">57.6</td><td align="left" valign="bottom">34.9</td><td align="left" valign="bottom">37.1</td><td align="left" valign="bottom">32.6</td><td align="left" valign="bottom">38.1</td><td align="left" valign="bottom">38.5</td></tr><tr><td align="left" valign="bottom">DeepHomo2</td><td align="left" valign="bottom">55.6<break/>(52.4)</td><td align="left" valign="bottom">58.1<break/>(53.9)</td><td align="left" valign="bottom">55.0<break/>(51.7)</td><td align="left" valign="bottom">59.4<break/>(55.7)</td><td align="char" char="." valign="bottom">61.3<break/>(56.7)</td><td align="left" valign="bottom" colspan="5"/></tr><tr><td align="left" valign="bottom">CDPred</td><td align="left" valign="bottom">59.4<break/>(54.7)</td><td align="left" valign="bottom">61.3<break/>(56.2)</td><td align="left" valign="bottom">58.4<break/>(54.1)</td><td align="left" valign="bottom">62.4<break/>(57.1)</td><td align="char" char="." valign="bottom">62.9<break/>(57.7)</td><td align="left" valign="bottom">30.0<break/>(30.2)</td><td align="left" valign="bottom">31.0<break/>(31.7)</td><td align="left" valign="bottom">27.6<break/>(27.3)</td><td align="left" valign="bottom">32.0<break/>(32.2)</td><td align="left" valign="bottom">32.1<break/>(32.7)</td></tr><tr><td align="left" valign="bottom">PLMGraph-Inter</td><td align="left" valign="bottom"><bold>68.6</bold><break/>(<bold>61.8</bold>)</td><td align="left" valign="bottom"><bold>70.4</bold><break/>(<bold>63.6</bold>)</td><td align="left" valign="bottom"><bold>67.3</bold><break/>(<bold>60.9</bold>)</td><td align="left" valign="bottom"><bold>71.6</bold><break/>(<bold>65.0</bold>)</td><td align="char" char="." valign="bottom"><bold>72.1</bold><break/>(<bold>65.25</bold>)</td><td align="left" valign="bottom"><bold>45.9</bold><break/>(<bold>41.9</bold>)</td><td align="left" valign="bottom"><bold>48.6</bold><break/>(<bold>43.6</bold>)</td><td align="left" valign="bottom"><bold>41.4</bold><break/>(<bold>37.8</bold>)</td><td align="left" valign="bottom"><bold>49.1</bold><break/>(<bold>44.1</bold>)</td><td align="left" valign="bottom"><bold>51.6</bold><break/>(<bold>45.0</bold>)</td></tr></tbody></table><table-wrap-foot><fn><p>The highest mean precision (%) in each column is highlighted in bold.</p></fn></table-wrap-foot></table-wrap><p>It can also be seen from the result that all the methods tend to have better performances on HomoPDB than those on HeteroPDB. One possible reason is that the complex structures of homodimers are generally C2 symmetric, which largely restricts the configurational spaces of PPIs, making the inter-protein contact prediction a relatively easier task (e.g., the inter-protein contact maps for homodimers are also symmetric). Besides, compared with heteromeric PPIs, we may be more likely to successfully infer the inter-protein coevolutionary information for homomeric PPIs to assist the contact prediction for two reasons: first, it is straightforward to pair sequences in the MSAs for homomeric PPIs, thus the paired MSA for homomeric PPIs may have higher qualities; second, homomeric PPIs may undergo stronger evolutionary constraints, as homomeric PPIs are generally permanent interactions, but many heteromeric PPIs are transient interactions.</p><p>In addition to using the mean precision on each test set to evaluate the performance of each method, the performance comparisons between PLMGraph-Inter and other models on the top 50 predicted contacts for each individual target in HomoPDB and HeteroPDB are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> (separate comparisons are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref>). Specifically, when the experimental (predicted) structures were used as the input, PLMGraph-Inter achieved the best performance for 60% (53%) of the targets in HomoPDB and 58% (51%) of the targets in HeteroPDB. We further group targets in each dataset according to their inter-protein contact densities defined as <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">#</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and the normalized number of the effective sequences <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of paired MSAs. We found that all the methods tend to have lower performances for targets with lower contact densities (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), which is reasonable since obviously it is more challenging to identify the true contacts when their ratio is lower. We also found when the <inline-formula><mml:math id="inf4"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is low (log <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> &lt; 3), the prediction performances of all methods tend to improve with <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, but when <inline-formula><mml:math id="inf7"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> reaches certain thresholds (log <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> &gt; 4), the performances of all the methods tend to fluctuate with <inline-formula><mml:math id="inf9"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). However, PLMGraph-Inter consistently achieved the best performances in most categories.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The performances of PLMGraph-Inter and other methods on the HomoPDB and HeteroPDB test sets.</title><p>(<bold>a, b</bold>) The head-to-head comparison of the precisions (%) of the top 50 contacts predicted by PLMGraph-Inter and other methods for each target in (<bold>a</bold>) HomoPDB and (<bold>b</bold>) HeteroPDB using experimental structures. (<bold>c, d</bold>) The head-to-head comparison of the precisions (%) of the top 50 contacts predicted by PLMGraph-Inter and other methods for each target in (<bold>c</bold>) HomoPDB and (<bold>d</bold>) HeteroPDB using AlphaFold2 predicted structures.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>The head-to-head comparison of the precisions (%) of the top 50 contacts predicted by PLMGraph-Inter and other methods (<bold>a</bold>: DRN-1D2D_Inter; <bold>b</bold>: DeepHomo; <bold>c</bold>: GLINTER ; <bold>d</bold>: CDPred; <bold>e</bold>: DeepHomo2) for each target in HomoPDB and HeteroPDB using experimental structures.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>The head-to-head comparison of the precisions (%) of the top 50 contacts predicted by PLMGraph-Inter and other methods (<bold>a</bold>: DRN-1D2D_Inter; <bold>b</bold>: DeepHomo; <bold>c</bold>: GLINTER ; <bold>d</bold>: CDPred; <bold>e</bold>: DeepHomo2) for each target in HomoPDB and HeteroPDB using AlphaFold2 predicted structures.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>The mean precision versus contact density for the top 50 contacts predicted by PLMGraph-Inter, GLINTER, DeepHomo, DeepHomo2, CDPred, and DRN-1D2D_Inter on the HomoPDB test set (<bold>a, c</bold>) and HeteroPDB test set (<bold>b, d</bold>) using experimental structures (first row) and AlphaFold2 predicted structures (second row).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>The mean precision versus log (<inline-formula><mml:math id="inf10"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) for the top 50 contacts predicted by PLMGraph-Inter, GLINTER, DeepHomo, DeepHomo2, and CDPred, DRN-1D2D_Inter on the HomoPDB test set (<bold>a, c</bold>) and HeteroPDB test set (<bold>b, d</bold>) using experimental structures (first row) and AlphaFold2 predicted structures (second row).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig2-figsupp4-v2.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Impact of the monomeric structure quality on contact prediction</title><p>We further analyzed the performance difference of PLMGraph-Inter when using the AlphaFold2 predicted structures and using the experimental structures as the inputs. As shown in <xref ref-type="fig" rid="fig3">Figure 3a and b</xref>, when the predicted structures were used by PLMGraph-Inter for inter-protein contact prediction, mean precisions of the predicted inter-protein contacts in each metric on both HomoPDB and HeteroPDB test sets decreased by about 5% (also see <xref ref-type="table" rid="table1">Table 1</xref>), indicating qualities of the input structures do have a certain impact on the prediction performance.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The performances of PLMGraph-Inter when using experimental and AlphaFold2 predicted structures as the input.</title><p>(<bold>a, b</bold>) The performance comparison of PLMGraph-Inter when using experimental structures and AlphaFold2 predicted structures as the input on (<bold>a</bold>) HomoPDB and (<bold>b</bold>) HeteroPDB. (<bold>c</bold>) The performance gaps (measured as the difference of the mean precision of the top 50 predicted contacts) of PLMGraph-Inter with the application of AlphaFold2 predicted structures and experimental structures as the input when the protein–protein interactions (PPIs) are within different intervals of DTM-score. The upper panel shows the percentage of the total number of PPIs in each interval. (<bold>d</bold>) The comparison of the precision of the top 50 contacts predicted by PLMGraph-Inter for each target when using experimental structures and AlphaFold2 predicted structures as the input.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig3-v2.tif"/></fig><p>We further explored the impact of the monomeric structure quality on the inter-protein contact prediction performance of PLMGraph-Inter. Specifically, for a given PPI, the TM-score (<xref ref-type="bibr" rid="bib55">Zhang and Skolnick, 2004</xref>) was used to evaluate the quality of the predicted structure for each interacting monomer, and the TM-score of the predicted structure with lower quality was used to evaluate the overall monomeric structure prediction quality for the PPI, denoted as ‘DTM-score’. In <xref ref-type="fig" rid="fig3">Figure 3c</xref>, we show the performance gaps (using the mean precisions of the top 50 predicted contacts as the metric) between applying the predicted structures and applying the experimental structures in the inter-protein contact prediction, in which we grouped targets according to DTM-scores of their monomeric structure prediction, and in <xref ref-type="fig" rid="fig3">Figure 3d</xref>, we show the performance comparison for each specific target. From <xref ref-type="fig" rid="fig3">Figure 3c and d</xref>, we can clearly see that when the DTM-score is lower, the prediction using the prediction structure tends to have lower accuracy. However, when the DTM-score is ≥0.8, there is almost no difference between applying the predicted structures and applying the experimental structure, which shows the robustness of PLMGraph-Inter to the structure quality.</p></sec><sec id="s2-4"><title>Ablation study</title><p>To explore the contribution of each input component to the performance of PLMGraph-Inter, we conducted an ablation study on PLMGraph-Inter. The graph representation from the structure of each interacting proteins is the base feature of PLMGraph-Inter, so we first trained the baseline model using only the geometric graphs as the input feature, denoted as model a. Our previous study in DRN-1D2D_Inter has shown that the single-sequence embeddings, MSA 1D features (including the MSA embeddings and PSSMs), and 2D pairwise features from the paired MSA play important roles in the model performance. To further explore the importance of these features when integrated with the geometric graphs, we trained models b–d separately (model b: geometric graphs + sequence embeddings; model c: geometric graphs + sequence embeddings + MSA 1D features; model d: geometric graphs + sequence embeddings + MSA 1D features + 2D features). Finally, we included the structure embeddings as additional features to train the model e (model e uses all the input features of PLMGraph-Inter). All the five models were trained using the same protocol as PLMGraph-Inter on the same training and validation partition without cross-validation. We further evaluated performances of models a–e together with PLMGraph-Inter (i.e., model f: model e + cross-validation) on HomoPDB and HeteroPDB using experimental structures of interacting monomers respectively.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4a</xref>, we show the mean precisions of the top 50 predicted contacts by models a–f on HomoPDB and HeteroPDB, respectively. It can be seen from <xref ref-type="fig" rid="fig4">Figure 4a</xref> that including the sequence embeddings in the geometric graphs has a very good boost to the model performance (model b versus model a), while the additional introduction of MSA 1D features and 2D pairwise features can further improve the model performance (model d versus model c versus model b). DRN-1D2D_Inter also uses the same set of sequence embeddings, MSA 1D features, and 2D pairwise features as the input, and our model d shows a significant performance improvement over DRN-1D2D_Inter (single model) (the model trained on the same training and validation partition without cross-validation) on both HomoPDB and HeteroPDB (the mean precision improvement: HomoPDB: 14%; HeteroPDB: 5.6%), indicating that the introduced graph representation is important for the model performance. The head-to-head comparison of model d and DRN-1D2D_Inter (single model) on each specific target in <xref ref-type="fig" rid="fig4">Figure 4b</xref> further demonstrates the value of the graph representation. Besides, the additionally introduced structure embeddings from ESM-IF can further improve the mean precisions of the predicted contacts by 3–4% on both HomoPDB and HeteroPDB (model e versus model d) and the application of the cross-validation can also improve the precisions by 1.0% on HomoPDB and 2.7% on HeteroPDB (model f versus model d) (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The ablation study of PLMGraph-Inter on the HomoPDB and HeteroPDB test sets.</title><p>(<bold>a</bold>) The mean precisions of the top 50 contacts predicted by different ablation models on the HomoPDB and HeteroPDB test sets. (<bold>b</bold>) The head-to-head comparisons of mean precisions of the top 50 contacts predicted by model d and DRN-1D2D_Inter (single model) for each target in HomoPDB and HeteroPDB. (<bold>c</bold>) The head-to-head comparison of mean precisions of the top 50 contacts predicted by the model using our geometric graphs and the geometric vector perceptron (GVP) geometric graphs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig4-v2.tif"/></fig><p>To demonstrate the efficacy of our proposed graph representation of protein structures, we also trained a model using the structural representation proposed in the work of GVP (<xref ref-type="bibr" rid="bib16">Jing et al., 2021a</xref>; <xref ref-type="bibr" rid="bib17">Jing et al., 2021b</xref>) (denoted as ‘GVP Graph’), as a control. Our structural representation differs significantly from GVP Graph. For example, we extracted inter-residue distances and orientations between five atoms (C,O,<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>,N, and a virtual <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) from the structure as the geometric scalar and vector features, in which the vector features are calculated in a local coordinate system. However, GVP Graph only uses the distances and orientations between <inline-formula><mml:math id="inf13"><mml:mi>C</mml:mi></mml:math></inline-formula> atoms as the geometric scalar and vector features and the vector features are calculated in a global coordinate system. In addition, after the geometric graph is transformed by the graph encoder module, GVP Graph only uses the scalar features of each node as the node representation, while we concatenate the scalar and vector features of the node as the node representation. In <xref ref-type="fig" rid="fig4">Figure 4c</xref>, we show the performance comparison between this model and our base model (model a). From <xref ref-type="fig" rid="fig4">Figure 4c</xref>, we can clearly see that our base model significantly outperforms the GVP Graph-based model on both HomoPDB and HeteroPDB, illustrating the high efficacy of our proposed graph representation.</p><p>We also explored the performance of PLMGraph-Inter on the HomoPDB and HeteroPDB test sets when using different protocols to further remove potential redundancies between the training and the test sets. Specifically, although the ‘40% sequence identity’ used in our study is a widely used threshold to remove redundancy when evaluating deep learning-based PPI and protein complex structure prediction methods (<xref ref-type="bibr" rid="bib8">Evans et al., 2022</xref>; <xref ref-type="bibr" rid="bib37">Sledzieski et al., 2021</xref>), it is worth testing whether PLMGraph-Inter can keep its performance when more stringent threshold is applied. Besides, it is also worth evaluating whether PLMGraph-Inter can keep its performance on targets for which the folds of their interacting monomers are different from the targets in the training set (i.e., non-redundant in main chain structures of interacting monomers). To the best of our knowledge, all the previous studies failed to remove potential redundancies in folds of the interacting monomers when evaluating their methods.</p><p>In <xref ref-type="table" rid="table2">Table 2</xref>, we show mean precisions of the contacts (top 50) predicted by PLMGraph-Inter on the HomoPDB and HeteroPDB when various sequence identity thresholds (with MMseq2 [<xref ref-type="bibr" rid="bib39">Steinegger and Söding, 2018</xref>]) and fold similarity thresholds (with TMalign [<xref ref-type="bibr" rid="bib56">Zhang and Skolnick, 2005</xref>]) were further used in the de-redundancy (see section ‘Further potential redundancies removal between the training and the test’). It can be seen from that table that when using more stringent sequence identity thresholds for de-redundancy, the performance of PLMGraph-Inter on both the HomoPDB and HeteroPDB datasets decreases very little. For example, even when using ‘10% sequence identity’ for de-redundancy, mean precisions of the predicted contacts only decrease by 2–4%. Whereas when using fold similarities of the interacting monomers for de-redundancy, although the performance of PLMGraph-Inter on HeteroPDB decreases very little (only 3–4% when TM-score 0.5 is used as the threshold), the performance of PLMGraph-Inter on HomoPDB decreases significantly (17–19% when TM-score 0.5 is used as the threshold). One possible reason for the performance decrease on HomoPDB is that the binding mode of the homomeric PPI is largely determined by the fold of its monomer, thus the model does not generalize well on targets whose folds have never been seen during the training.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>The performance of PLMGraph-Inter when using different sequence identity and fold similarity thresholds to further remove potential redundancies in HomoPDB and HeteroPDB.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">HomoPDB</th><th align="left" valign="bottom" colspan="2">HeteroPDB</th></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><bold>Count</bold></td><td align="left" valign="bottom"><bold>Precision</bold><break/><bold>(Top 50 [%])</bold></td><td align="left" valign="bottom"><bold>Count</bold></td><td align="left" valign="bottom"><bold>Precision</bold><break/><bold>(Top 50 [%])</bold></td></tr><tr><td align="left" valign="bottom" rowspan="5">Sequence identity<break/>(MMSeqs2)</td><td align="left" valign="bottom">Original</td><td align="char" char="." valign="bottom">400</td><td align="char" char="." valign="bottom">67.3 (60.9)</td><td align="char" char="." valign="bottom">200</td><td align="char" char="." valign="bottom">41.4 (37.8)</td></tr><tr><td align="char" char="." valign="bottom">40%</td><td align="char" char="." valign="bottom">341</td><td align="char" char="." valign="bottom">68.7 (62.5)</td><td align="char" char="." valign="bottom">160</td><td align="char" char="." valign="bottom">38.6 (35.6)</td></tr><tr><td align="char" char="." valign="bottom">30%</td><td align="char" char="." valign="bottom">257</td><td align="char" char="." valign="bottom">64.7 (58.7)</td><td align="char" char="." valign="bottom">144</td><td align="char" char="." valign="bottom">38.1 (35.1)</td></tr><tr><td align="char" char="." valign="bottom">20%</td><td align="char" char="." valign="bottom">211</td><td align="char" char="." valign="bottom">63.2 (56.3)</td><td align="char" char="." valign="bottom">138</td><td align="char" char="." valign="bottom">38.5 (35.3)</td></tr><tr><td align="char" char="." valign="bottom">10%</td><td align="char" char="." valign="bottom">211</td><td align="char" char="." valign="bottom">63.2 (56.3)</td><td align="char" char="." valign="bottom">138</td><td align="char" char="." valign="bottom">38.5 (35.3)</td></tr><tr><td align="left" valign="bottom" rowspan="5">Fold similarity<break/>(TM-align)</td><td align="char" char="." valign="bottom">0.9</td><td align="char" char="." valign="bottom">370</td><td align="char" char="." valign="bottom">65.2 (58.3)</td><td align="char" char="." valign="bottom">185</td><td align="char" char="." valign="bottom">39.7 (35.7)</td></tr><tr><td align="char" char="." valign="bottom">0.8</td><td align="char" char="." valign="bottom">281</td><td align="char" char="." valign="bottom">61.8 (53.4)</td><td align="char" char="." valign="bottom">153</td><td align="char" char="." valign="bottom">38.1 (34.1)</td></tr><tr><td align="char" char="." valign="bottom">0.7</td><td align="char" char="." valign="bottom">179</td><td align="char" char="." valign="bottom">56.5 (45.8)</td><td align="char" char="." valign="bottom">126</td><td align="char" char="." valign="bottom">38.8 (34.6)</td></tr><tr><td align="char" char="." valign="bottom">0.6</td><td align="char" char="." valign="bottom">124</td><td align="char" char="." valign="bottom">50.4 (39.9)</td><td align="char" char="." valign="bottom">102</td><td align="char" char="." valign="bottom">37.4 (34.1)</td></tr><tr><td align="char" char="." valign="bottom">0.5</td><td align="char" char="." valign="bottom">70</td><td align="char" char="." valign="bottom">49.6 (41.3)</td><td align="char" char="." valign="bottom">83</td><td align="char" char="." valign="bottom">36.5 (34.5)</td></tr></tbody></table><table-wrap-foot><fn><p>The results using experimental structures are shown outside the parentheses, and the results using the AlphaFold2 predicted structures are shown inside the parentheses.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-5"><title>Evaluation of PLMGraph-Inter on DHTest and DB5.5 test sets</title><p>We further evaluated PLMGraph-Inter on DHTest and DB5.5. The DHTest test set was formed by removing PPIs redundant to our training set from the original test set of DeepHomo, which contains 130 homomeric PPIs. The DB5.5 test set was formed by removing PPIs redundant to our training dataset from the heterodimers in Protein-protein Docking Benchmark 5.5, which contains 59 heteromeric PPIs. Still, both the experimental structures and the predicted structures (generated using the same protocol as in HomoPDB and HeteroPDB) of the interacting monomers were used respectively in the inter-protein contact prediction. It should be noted that since DHTest and DB5.5 are not de-redundant with the training sets of CDPred and GLINTER, particularly, all PPIs in the DHTest test set are included in the training set of CDPred, thus the performances of the two methods may be overestimated.</p><p>As shown in <xref ref-type="fig" rid="fig5">Figure 5a and b</xref>, when using the experimental structures in the prediction, the mean precisions of the top 50 contacts predicted by PLMGraph-Inter are 71.9% on DHTest and 29.5% on DB5.5 (also see <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>), which are dramatically higher than DeepHomo, GLINTER, DeepHomo2, and DRN-1D2D_Inter. (Note that GLINTER encountered errors for 47 targets in DHTest and 3 targets in DB5.5 at run time and did not produce predictions, thus we removed these targets in the evaluation of the performance of GLINTER. The performances of other methods on DHTest and DB5.5 after the removal of these targets are shown in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>.) We can also see that although PLMGraph-Inter achieved significantly better performance than CDPred on DB5.5, its performance on DHTest is quite close to CDPred. However, it should be noted that the performance of CDPred on DHTest might be grossly overestimated since PPIs in DHTest are fully included in the training set of CDPred. The distributions of the precisions of the top 50 predicted contacts by different methods on DHTest and DB5.5 are shown in <xref ref-type="fig" rid="fig5">Figure 5d and e</xref>, from which we can clearly see that PLMGraph-Inter can make high-quality predictions for more targets on both DHTest and DB5.5.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The performances of PLMGraph-Inter and other methods on the DHTest and DB5.5 test sets.</title><p>(<bold>a, b</bold>) The mean precisions of the top 50 contacts predicted by PLMGraph-Inter, GLINTER, DeepHomo2, CDPred, and DeepHomo on (<bold>a</bold>) DHTest and (<bold>b</bold>) DB5.5 when using experimental structures and AlphaFold2 predicted structures as the input, where the green lines indicate the performance of DRN-1D2D_Inter. (<bold>c</bold>) The performance gaps (measured as the difference of the mean precision of the top 50 predicted contacts) of PLMGraph-Inter with the application of AlphaFold2 predicted structures and experimental structures as the input when the protein–protein interactions (PPIs) are within different intervals of DTM-score. The upper panel shows the percentage of the total number of PPI’s in each interval. (<bold>d, e</bold>) The distributions of precisions of the top 50 contacts predicted by PLMGraph-Inter and other methods for PPIs in (<bold>d</bold>) DHTest and (<bold>e</bold>) DB5.5. (<bold>f</bold>) The mean precisions of the top 50 contacts predicted by PLMGraph-Inter on PPIs within different intervals of contact densities in DB5.5. The upper panel shows the percentage of the total number of PPIs in each interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig5-v2.tif"/></fig><p>When using the predicted structures in the prediction, the mean precisions of the top 50 contacts predicted by PLMGraph-Inter show a reasonable decrease to 61.1% on DHTest and 23.8% on DB5.5, respectively (see <xref ref-type="fig" rid="fig5">Figure 5a and b</xref> and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). We also analyzed the impact of the monomeric structure quality on the inter-protein contact prediction performance of PLMGraph-Inter. As shown in <xref ref-type="fig" rid="fig5">Figure 5c</xref>, when the DTM-score is ≥0.7, there is almost no difference between applying the predicted structures and the experimental structures, which is consistent with our analysis in HomoPDB and HeteroPDB.</p><p>We noticed that the performance of PLMGraph-Inter on the DB5.5 is significantly lower than that on HeteroPDB, and so are the performances of other methods. That the targets in DB5.5 have relatively lower mean contact densities (1.01% vs 1.29%) may partly explain this phenomenon. In <xref ref-type="fig" rid="fig5">Figure 5f</xref>, we show the variations of the precisions of predicted contacts with the variation of contact density. As can be seen from <xref ref-type="fig" rid="fig5">Figure 5f</xref>, as the contact density increases, precisions of predicted contacts tend to increase regardless of whether the experimental structures or predicted structures are used in the prediction. 37.29% targets in DB5.5 are with inter-protein contact densities lower than 0.5%, for which precisions of predicted contacts are generally very low, making the overall inter-protein contact prediction performance on DB5.5 relatively low.</p></sec><sec id="s2-6"><title>Comparison of PLMGraph-Inter with AlphaFold-Multimer</title><p>After the development of AlphaFold2, DeepMind also released AlphaFold-Multimer, as an extension of AlphaFold2 for protein complex structure prediction. The inter-protein contacts can also be extracted from the complex structures generated by AlphaFold-Multimer. It is worth making a comparison between the performances of AlphaFold-Multimer and PLMGraph-Inter on inter-protein contact prediction. Therefore, we also employed AlphaFold-Multimer (version 2.2) with its default settings to generate complex structures for all the PPIs in the four datasets which we used to evaluate PLMGraph-Inter. We then selected the 50 inter-protein residue pairs with the shortest heavy atom distances in each generated protein complex structures as the predicted inter-protein contacts. It should be noted that AlphaFold-Multimer used all protein complex structures in the Protein Data Bank deposited before April 30, 2018, in the model training, thus these PPIs may have a large overlap with the training set of AlphaFold-Multimer. Therefore, there is no doubt that the performance of AlphaFold-Multimer would be overestimated here. It should also be noted that although AlphaFold-Multimer makes the prediction from sequences, it automatically searches templates of the interacting monomers. When we checked our AlphaFold-Multimer runs, we noticed for 99% of the targets (including all the targets in the four datasets: HomoPDB, HeteroPDB, DHTest, and DB5.5), at least 20 templates were identified (AlphaFold-Multimer only employed the top 20 templates), and AlphaFold-Multimer employed the native template (i.e., the template which has the same PDB id with the target) for 87.8% of the targets. Besides, AlphaFold-Multimer employs multiple sequence databases including a huge metagenomics database (<xref ref-type="bibr" rid="bib20">Jumper et al., 2021</xref>), but PLMGraph-Inter only employs the UniRef100, thus the comparison is not on the same footing.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6a</xref>, we show the relationship between the quality of the generated protein complex structure (evaluated with DockQ) and the precision of the top 50 inter-protein contacts extracted from the protein complex structure predicted by AlphaFold-Multimer for each PPI in the homomeric PPI (DHTest + HomoPDB) and heteromeric PPI (DB5.5 + HeteroPDB) datasets. As can be seen from the figure, the precision of the predicted contacts is highly correlated with the quality of the generated structure. Especially when the precision of the contacts is higher than 50%, most of the generated complex structures have at least acceptable qualities (DockQ ≥ 0.23), in contrast, almost all the generated complex structures are incorrect (DockQ &lt; 0.23) when the precision of the contacts is below 50%. Therefore, 50% can be considered as a critical precision threshold for inter-protein contact prediction (the top 50 contacts).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The comparison of PLMGraph-Inter with AlphaFold-Multimer.</title><p>(<bold>a</bold>) The head-to-head comparison between the qualities of the protein complex structures generated by AlphaFold-Multimer (evaluated with DockQ) and the precision of the top 50 inter-protein contacts extracted from the generated protein complex structures. The red horizontal lines represent the threshold (DockQ = 0.23) to determine whether the complex structure prediction is successful or not. (<bold>b</bold>) The head-to-head comparisons of precisions of the top 50 inter-protein contacts predicted by PLMGraph-Inter and AlphaFold-Multimer for each target in the homomeric protein–protein interaction (PPI) and heteromeric PPI datasets. (<bold>c, d</bold>) The mean precisions of top 50 inter-protein contacts predicted by PLMGraph-Inter and AlphaFold-Multimer on the PPI subsets from (<bold>c</bold>) ‘DHTest + HomoPDB’ and (<bold>d</bold>) 'DB5.5 + HeteroPDB’ in which the precision of the top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of the complex structure predicted by AlphaFold-Multimer is lower than 0.23 or the ‘iptm + ptm’ of the complex structure predicted by AlphaFold-Multimer is lower than 0.5.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>The comparison of PLMGraph-Inter with AlphaFold-Multimer.</title><p>(<bold>b</bold>) The head-to-head comparisons of precisions of the top 50 inter-protein contacts predicted by PLMGraph-Inter (using AlphaFold2 predicted structures) and AlphaFold-Multimer for each target in the homomeric protein–protein interaction (PPI) and heteromeric PPI datasets. (<bold>c, d</bold>) The mean precisions of top 50 inter-protein contacts predicted by PLMGraph-Inter(using AlphaFold2 predicted structures as input) and AlphaFold-Multimer on the PPI subsets from (<bold>c</bold>) ‘DHTest + HomoPDB’ and (<bold>d</bold>) ‘DB5.5 + HeteroPDB’ in which the precision of the top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of the complex structure predicted by AlphaFold-Multimer is lower than 0.23 or the ‘iptm + ptm’ of the complex structure predicted by AlphaFold-Multimer is lower than 0.5.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig6-figsupp1-v2.tif"/></fig></fig-group><p>In <xref ref-type="fig" rid="fig6">Figure 6b</xref>, we show the comparison of the precisions of top 50 contacts predicted by AlphaFold-Multimer and PLMGraph-Inter for each target when using the experimental monomeric structures as the input for PLMGraph-Inter respectively (also see <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>, and the comparison when using the AlphaFold2 predicted structures is shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>). It can be seen from the figure that although for most of the targets AlphaFold-Multimer yielded better results, but for a significant number of the targets AlphaFold-Multimer made poor predictions (precision &lt;50%), the results of PLMGraph-Inter can have certain improvement over the AlphaFold-Multimer predictions.</p><p>We further explored the performance of PLMGraph-Inter on the PPIs which AlphaFold-Multimer failed to make correct predictions. Specifically, we denoted a PPI for which the precision of the top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of protein complex structure predicted by AlphaFold-Multimer is less than 0.23 as ‘precision-Failed’, ‘DockQ-Failed’. The ‘iptm + ptm’ metrics output by AlphaFold-Multimer for each target also has a certain ability to characterize the quality of the predicted complexes. Our DockQ versus ‘iptm + ptm’ analysis shows that 0.5 can be reasonably chosen as the cutoff of ‘iptm + ptm’ to evaluate whether the prediction of AlphaFold-Multimer is successful or not (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a</xref>), so we denoted a prediction for which the ‘iptm + ptm’ of the prediction is lower than 0.5 as ‘pTM-Failed’. Then the mean precisions of top 50 contacts predicted by PLMGraph-Inter and AlphaFold-Multimer on the ‘precision-Failed’, ‘pTM-Failed’, and ‘DockQ-Failed’ sub-test sets from ‘DHTest + HomoPDB’ and ‘DB5.5 + HeteroPDB’ are shown in <xref ref-type="fig" rid="fig6">Figure 6c and d</xref> (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1c and d</xref> for the comparison when using the AlphaFold2 predicted structures). From <xref ref-type="fig" rid="fig6">Figure 6c and d</xref> we can see that the mean precisions of contacts predicted by PLMGraph-Inter are higher than the mean precisions of contacts predicted by AlphaFold-Multimer, further demonstrating that PLMGraph-Inter can complement AlphaFold-Multimer in certain cases.</p></sec><sec id="s2-7"><title>PLMGraph-Inter can significantly improve protein–protein docking performance</title><p>Prior to AlphaFold-Multimer, protein–protein docking is generally used for protein complex structure prediction. HADDOCK (<xref ref-type="bibr" rid="bib13">Honorato et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">van Zundert et al., 2016</xref>) is a widely used information-driven protein–protein docking approach to model complex structures of PPIs, which allows us to encode predicted inter-protein contacts as constraints to drive the docking. In this study, we used HADDOCK (version 2.4) to explore the contribution of PLMGraph-Inter to protein complex structure prediction.</p><p>We prepared the test set of homomeric PPIs by merging HomoPDB and DHTest and the test set of heteromeric PPIs by merging HeteroPDB and DB5.5, where the monomeric structures generated previously by AlphaFold2 were used as the input to HADDOCK for protein–protein docking, in which the top 50 contacts predicted by PLMGraph-Inter with the application of the predicted monomeric structures were used as the constraints. Since HADDOCK generally cannot model large conformational changes in protein–protein docking, we filtered PPIs in which either of the AlphaFold2-generated interacting monomeric structure has a TM-score lower than 0.8. Finally, the homomeric PPI test set contains 462 targets, denoted as homodimer, and the heteromeric PPI test set contains 174 targets, denoted as heterodimer.</p><p>For each PPI, we used the top 50 contacts predicted by PLMGraph-Inter and other methods as ambiguous distance restraints between the alpha carbons （<inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>）of residues (distance = 8 Å, lower bound correction = 8 Å, upper-bound correction = 4 Å) to drive the protein–protein docking. All other parameters of HADDOCK were set as the default parameters. In each protein–protein docking, HADDOCK output 200 predicted complex structures ranked by the HADDOCK scores. As a control, we also performed protein–protein docking with HADDOCK in ab initio docking mode (center of mass restraints and random ambiguous interaction restraints definition). Besides, for homomeric PPIs, we additionally added the C2 symmetry constraint in both cases.</p><p>As shown in <xref ref-type="fig" rid="fig7">Figure 7a and c</xref>, the success rate of docking on homodimer and heterodimer test sets can be significantly improved when using the PLMGraph-Inter predicted inter-protein contacts as restraints. Whereas on the Homodimer test set, the success rate (<inline-formula><mml:math id="inf15"><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>Q</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.23</mml:mn></mml:math></inline-formula>) of the top 1 (top 10) prediction of HADDOCK in ab initio docking mode is 15.37% (39.18%), and when predicted by HADDOCK with PLMGraph-Inter predicted contacts, the success rate of the top 1 (top 10) prediction is 57.58% (61.04%). On the heterodimer test set, the success rate of top 1 (top 10) predictions of HADDOCK in ab initio docking mode is only 1.72% (6.32%), and when predicted by HADDOCK with PLMGraph-Inter predicted contacts, the success rate of top 1 (top 10) prediction is 29.89% (37.93%). From <xref ref-type="fig" rid="fig7">Figure 7a and b</xref> we can also see that integrating PLMGraph-Inter predicted contacts with HADDOCK not only allows for a higher success rate, but also more high-quality models in the docking results.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Protein–protein docking performances on the homodimer and heterodimer test sets.</title><p>(<bold>a, b</bold>) The protein–protein docking performance comparison between HADDOCK with and without (ab initio) using PLMGraph-Inter predicted contacts as restraints on (<bold>a</bold>) homodimer and (<bold>b</bold>) heterodimer. The left side of each column shows the performance when the top 1 predicted model for each protein–protein interaction (PPI) is considered, and the right side shows the performance when the top 10 predicted models for each PPI are considered. (<bold>c</bold>) The head-to-head comparison of qualities of the top 1 model predicted by HADDOCK with and without using PLMGraph-Inter predicted contacts as restraints for each target PPI. The red lines represent the threshold (DockQ = 0.23) to determine whether the complex structure prediction is successful or not. (<bold>d</bold>) The success rates (the top 1 model) for protein complex structure prediction when only including targets for which precisions of the predicted contacts are higher than certain thresholds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>3D structure of the homodimer (PDB: 3DFU).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>The comparison of HADDOCK (with PLMGraph-Inter contact constraints) with AlphaFold-Multimer in protein complex structure prediction.</title><p>(<bold>a</bold>) The binding configurations predicted by HADDOCK (orange, DockQ: 0.375), predicted by AlphaFold-Multimer (pink, DockQ: 0) and the native binding configuration (blue) for chain B of the protein complex structure in PDB 5HPS. Chain A is shown in the protein surface mode (green). (<bold>b, c</bold>) The head-to-head comparison of qualities of the (<bold>a</bold>) top 1 or (<bold>b</bold>) top 10 model predicted by HADDOCK with using PLMGraph-Inter predicted contacts as restraints and AlphaFold-Multimer for each target protein–protein interaction (PPI).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-fig7-figsupp2-v2.tif"/></fig></fig-group><p>We further explored the relationship between the precision of the top 50 contacts predicted by PLMGraph-Inter and the success rate of the top prediction of HADDOCK with PLMGraph-Inter predicted contacts. It can be clearly seen from <xref ref-type="fig" rid="fig7">Figure 7d</xref> that the success rate of protein–protein docking increases with the precision of contact prediction. Especially, when the precision of the predicted contacts reaches 50%, the docking success rate of both homologous and heterologous complexes can reach 80%, which is consistent with our finding in AlphaFold-Multimer. Therefore, we think this threshold can be used as a critical criterion for inter-protein contact prediction. It is important to emphasize that for some targets, although precisions of predicted contacts are very high, HADDOCK still failed to produce acceptable models. We manually checked these targets and found that many of these targets have at least one chain totally entangled by another chain (e.g., PDB 3DFU in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). We think large structural rearrangements may exist in forming the complex structures, which is difficult to model by traditional protein–protein docking approach.</p><p>Finally, we compared the qualities of the complex structures predicted by HADDOCK (with PLMGraph-Inter predicted contacts) and AlphaFold-Multimer. Although for some targets (e.g., PDB 5HPS in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2a</xref>), the qualities of the structures predicted by HADDOCK were higher than those by AlphaFold-Multimer, for most targets, AlphaFold-Multimer generated higher quality structures (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2b and c</xref>). Several reasons can account for the performance gap. First, precisions of the PLMGraph-Inter predicted contacts are still not enough, especially for heteromeric PPIs; second, HADDOCK cannot model large structural rearrangements in protein–protein docking, as we can see that for some targets, HADDOCK made poor predictions with high precisions of contact constraints (<xref ref-type="fig" rid="fig7">Figure 7d</xref>); and third, it is difficult to provide an objective evaluation of the true performance of AlphaFold-Multimer since many targets have already been included in the training set of AlphaFold-Multimer.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we proposed a new method to predict inter-protein contacts, denoted as PLMGraph-Inter. PLMGraph-Inter is based on the SE(3) invariant geometric graphs obtained from the structures of interacting proteins which are embedded with multiple PLMs. The predicted inter-protein contacts are obtained by successively transforming the PLM-embedded geometric graphs with graph encoders and residual networks. Benchmarking results on four test datasets show that PLMGraph-Inter outperforms five state-of-the-art inter-protein contact prediction methods including GLINTER, DeepHomo, CDPred, DeepHomo2, and DRN-1D2D_Inter by large margins, regardless of whether the experimental or predicted monomeric structures are used in building the geometric graphs. The ablation study further shows that the integration of the PLMs with the protein geometric graphs can dramatically improve the model performance, illustrating the efficacy of the PLM-embedded geometric graphs in protein representations. The protein representation framework proposed in this work can also be used to develop models for other tasks like protein function prediction, PPI prediction, etc. Recently, Fang et al. have also shown in their work that the incorporation of PLMs in geometric networks can significantly improve the model performances on a variety of protein-related tasks, including protein–protein interface prediction, model quality assessment, protein–protein rigid docking, and binding affinity prediction (<xref ref-type="bibr" rid="bib49">Wu et al., 2023</xref>), which further supports this claim. We further show that PLMGraph-Inter can complement the result of AlphaFold-Multimer and leveraging the inter-protein contacts predicted by PLMGraph-Inter as constraints in protein–protein docking implemented with HADDOCK can dramatically improve its performance for protein complex structure prediction.</p><p>We noticed that although PLMGraph-Inter has achieved remarkable progress in inter-protein contact prediction, there is still room for further improvement, especially for heteromeric PPIs. Using more advanced PLMs, larger training datasets, and explicitly integrating physicochemical features of interacting proteins are directions worthy of exploration. Besides, since protein–protein docking approach generally has difficulties in modeling large conformational changes in PPIs, developing new approaches to integrate the predicted inter-protein contacts in the more advanced folding-and-docking framework like AlphaFold-Multimer or directly incorporating an additional structural module for protein complex structure generation in the network architecture can also be the future research directions.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Training and test datasets</title><p>We used the training set and test sets prepared in our previous work DRN-1D2D_Inter (<xref ref-type="bibr" rid="bib36">Si and Yan, 2023</xref>) to train and evaluate PLMGraph-Inter. More details for the dataset generation can be found in the previous work. Specifically, we first prepared a non-redundant PPI dataset containing 4828 homomeric PPIs and 3134 heteromeric PPIs (with sequence identity 40% as the threshold), and after randomly selecting 400 homomeric PPIs (denoted as HomoPDB) and 200 heteromeric PPIs (denoted as HeteroPDB) as independent test sets, the remaining 7362 homomeric and heteromeric PPIs were used for training and validation.</p><p>DHTest and DB5.5 were also prepared in the work of DRN-1D2D_Inter by removing PPIs that are redundant (with sequence identity 40% as the threshold) to our training and validation set from the test set of DeepHomo and Docking Benchmark 5.5. DHTest contains 130 homomeric PPIs, and DB5.5 contains 59 heteromeric PPIs. Therefore, all the test sets used in this study are non-redundant (with sequence identity 40% as the threshold) to the dataset for the model development.</p></sec><sec id="s4-2"><title>Inter-protein contact definition</title><p>For a given PPI, two residues from the two interacting proteins are defined to be in contact if the distance of any two heavy atoms belonging to the two residues is smaller than 8 Å.</p></sec><sec id="s4-3"><title>Preparing the input features</title><sec id="s4-3-1"><title>Geometric graphs from structures of interacting monomers</title><p>We first represent the protein as a graph, where each residue is represented as a node, and an edge is defined if the <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> atom distance between two residues is less than 18 Å. (In our small-scale tests, increasing the cutoff used for defining edges can slightly increase the performance of the model. However, due to GPU memory limitations, we set the cutoff as 18 Å.) For each node and edge, we use scalars and vectors extracted from the 3D structures as their geometric features.</p><p>For each residue, we use its C,O,<inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>,N, and a virtual <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> atom coordinates to extract information, the virtual <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> coordinates are calculated using the following formula (<xref ref-type="bibr" rid="bib6">Dauparas et al., 2022</xref>): b = <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> – N, c = C – <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, a = cross(b, c), <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = –0.58273431 * a + 0.56802827 * b – 0.54067466 * c + <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>To achieve an SE(3) invariant graph representation, as shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>, we define a local coordinate system on each residue (<xref ref-type="bibr" rid="bib20">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Pagès et al., 2019</xref>). Specifically, for each residue, the unit vector in the <inline-formula><mml:math id="inf24"><mml:mi>C</mml:mi><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:math></inline-formula> direction is set as the <inline-formula><mml:math id="inf25"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> axis, the unit vector in the <inline-formula><mml:math id="inf26"><mml:mi>C</mml:mi><mml:mi>α</mml:mi></mml:math></inline-formula>-C-N plane and perpendicular <inline-formula><mml:math id="inf27"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> to is used as <inline-formula><mml:math id="inf28"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> , and the z-direction is obtained through the cross-product of <inline-formula><mml:math id="inf29"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> .</p><p>For the <italic>i</italic>th node, we use the three dihedral angles (ϕ, ψ, ω) of the corresponding residue as the scalar features of the node (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>), and the unit vectors between the <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of the corresponding residue and the <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of the forward residue and the <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of backward residue as the vector features of the node. In total, for each node, the dimension of the scalar features is 6 (each dihedral angle is encoded with its sine and cosine) and the dimension of the vector features is 50 *3.</p><p>For the edge between <italic>i</italic>th node and <italic>j</italic>th node, we use the distances and directions between the atoms of the two residues as the scalar features and vector features (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>). The distances between the <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of <italic>i</italic>th residue and the <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of the <italic>j</italic>th residue are used as scalar features after encoded with the 16 Gaussian radial basis functions (<xref ref-type="bibr" rid="bib17">Jing et al., 2021b</xref>). The position difference between <italic>i</italic> and <italic>j</italic> (<italic>j –</italic> i) is also used as a scalar feature after sinusoidal encoding (<xref ref-type="bibr" rid="bib45">Vaswani et al., 2017</xref>). The unit vectors between the <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of <italic>i</italic>th residue and the <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>C</mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> atoms of the <italic>j</italic>th residue are used as vector features. In total, for each edge, the dimension of the scalar features is 432 and the dimension of the vector features is 25 * 3.</p></sec><sec id="s4-3-2"><title>Embeddings of single sequence, MSA, and structure</title><p>The single-sequence embedding is obtained by feeding the sequence into ESM-1b, and the structure embedding is obtained by feeding the structure into ESM-IF. To obtain the MSA embedding, we first search the Uniref100 protein sequence database for the sequence using JACKHMMER (<xref ref-type="bibr" rid="bib28">Potter et al., 2018</xref>) with the parameter (--incT L/2) to obtain the MSA, which is then inputted to hhmake (<xref ref-type="bibr" rid="bib40">Steinegger et al., 2019</xref>) to get the HMM file, and to the LoadHMM.py script from RaptorX_Contact (<xref ref-type="bibr" rid="bib46">Wang et al., 2017</xref>) to obtain the PSSM. The number of sequences of MSA is limited to 256 by hhfilter (<xref ref-type="bibr" rid="bib40">Steinegger et al., 2019</xref>) and then input to ESM-MSA-1b to get the MSA embedding. The dimensions of the sequence embedding, PSSM, MSA embedding, and structural embeddings are 1280, 20, 768, and 512, respectively. After adding embeddings to the scalar features of the nodes, the dimension of the scalar features of each node is 2586.</p></sec><sec id="s4-3-3"><title>2D feature from paired MSA</title><p>For homomeric PPIs, the paired MSA is formed by concatenating two copies of the MSA. For heteromeric PPIs, the paired MSA is formed by pairing the MSAs through the phylogeny-based approach described in <ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank">https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank</ext-link> (<xref ref-type="bibr" rid="bib53">Yan, 2024</xref>; <xref ref-type="bibr" rid="bib35">Si and Yan, 2022</xref>). We input the paired MSA into CCMpred (<xref ref-type="bibr" rid="bib33">Seemayer et al., 2014</xref>) to get the evolutionary coupling matrix and into alnstats (<xref ref-type="bibr" rid="bib18">Jones et al., 2015</xref>) to get mutual information matrix, APC-corrected mutual information matrix, and contact potential matrix. The number of sequences of paired MSA is limited to 256 by hhfilter (<xref ref-type="bibr" rid="bib40">Steinegger et al., 2019</xref>) and then input to ESM-MSA-1b to get the attention maps. In total, the channel of 2D features is 148.</p></sec></sec><sec id="s4-4"><title>GVP and GVPConv</title><p>GVP is a two-track neural network module consisting of a scalar track and a vector track, which can perform SE(3) invariant transformations on scalar features and SE(3) equivariant transformations on vector features. A detailed description can be found in the work of GVP (<xref ref-type="bibr" rid="bib16">Jing et al., 2021a</xref>; <xref ref-type="bibr" rid="bib17">Jing et al., 2021b</xref>).</p><p>GVPConv is a message passing-based graph neural network, which mainly consists of a message function and a feedforward function. Where the message function contains a sequence of three GVP modules and the feedforward function contains a sequence of two GVP modules. GVPConv is used to transform the node features. Specifically, the input node features are first processed by the message function. We denote the features of node <italic>i</italic> by <inline-formula><mml:math id="inf66"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> , the feature of edge (<italic>j</italic> → <italic>i</italic>) by <inline-formula><mml:math id="inf67"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> , the set of nodes connected to node <italic>i</italic> by <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and the three GVP modules of the message function by <italic>gm</italic>, then the node features processed by the message function can be represented as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf69"><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> denotes the number of nodes connected to node <italic>i</italic>. After sequential normalization (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) and feedforward function (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), the features of node <italic>i</italic> updated by GVPConv Layer are obtained:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>LayerNorm</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mtext>Dropout</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>LayerNorm</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mtext>Dropout</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>gs</italic> denotes the two GVP modules of the feedforward function, and <inline-formula><mml:math id="inf70"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> denotes the outputs of GVPConv Layer.</p></sec><sec id="s4-5"><title>The transforming procedure in the residual network module</title><p>We first use a convolution layer with kernel size of 1 * 1 to reduce the number of channels of the input 2D feature maps from 1044 to 96, which are then transformed successively by nine-dimensional hybrid residual blocks and another convolution layer with kernel size of 1 * 1 for the channel reduction (from 96 to 1). Finally, we use the sigmoid function to transform the feature map to obtain the predicted inter-protein contact map.</p></sec><sec id="s4-6"><title>Training protocol</title><p>Our training set contains 7362 PPIs, and we used sevenfold cross-validation to train PLMGraph-Inter. Specifically, we randomly divided the training set into seven subsets, and each time, we selected six subsets as the training set and the remaining subset as the validation set. Seven models were trained in total, and the final prediction was the average of the predictions from the seven models. Each model was trained using AdamW optimizer with 0.001 as the initial learning rate, in which the singularity enhanced loss function proposed by us in our previous study (<xref ref-type="bibr" rid="bib34">Si and Yan, 2021</xref>) was used to calculate the training and validation loss. During training, if the validation loss did not decrease within two epochs, we would decay the learning rate by 0.1. The training stopped after the learning rate decayed twice and the model with the highest top 50 mean precision on the validation dataset was saved as the prediction model.</p><p>PLMGraph-Inter was implemented with pytorch (v.1.11) and trained on one NVIDIA TESLA A100 GPU with batch size equaling 1. Due to memory limitation of GPU, the length of each protein sequence was limited to 400. When a sequence was longer than 400, a fragment with sequence length equaling 400 was randomly selected in the model training.</p></sec><sec id="s4-7"><title>Quality assessment of the predicted protein complex structures</title><p>We evaluated the models generated by AlphaFold-Multimer and HADDOCK using DockQ (<xref ref-type="bibr" rid="bib2">Basu and Wallner, 2016</xref>), a score ranging between 0 and 1. Specifically, a model with DockQ &lt; 0.23 means that the prediction is incorrect; 0.23 ≤ DockQ &lt; 0.49 means the model is an acceptable prediction; 0.49 ≤ DockQ &lt; 0.8 corresponds to a medium-quality prediction; and 0.8 ≤ DockQ corresponds to a high-quality prediction.</p></sec><sec id="s4-8"><title>Further potential redundancies removal between the training and the test sets</title><sec id="s4-8-1"><title>Removing potential redundancies using different sequence similarity thresholds</title><p>CD-HIT (<xref ref-type="bibr" rid="bib21">Li et al., 2001</xref>) was originally used in removing redundancies between the training and test sets used in this study. Since the lowest sequence identity threshold accepted by CD-HIT is 40%, to use more stringent threshold in the redundancy removal, we further clustered all the monomer sequences from the training set and the test sets (HomoPDB, HeteroPDB) using MMSeq2 (<xref ref-type="bibr" rid="bib39">Steinegger and Söding, 2018</xref>) with different sequence identity thresholds (40%, 30%, 20%, 10%). Under a certain threshold, each sequence is uniquely labeled by the cluster (e.g., cluster 0, cluster 1, …) to which it belongs, from which each PPI can be marked with a pair of clusters (e.g., cluster 0–cluster 1). The PPIs belonging to the same cluster pair (note: cluster <italic>n</italic> – cluster <italic>m</italic> and cluster <italic>n –</italic> cluster <italic>m</italic> were considered as the same pair) were considered redundant with this sequence identity threshold. For each PPI in the test set, if the pair cluster it belongs to contains any PPI belonging to the training set, we remove that PPI from the test set.</p></sec><sec id="s4-8-2"><title>Removing potential redundancies using different fold similarity thresholds of interacting monomers</title><p>We used TM-align (<xref ref-type="bibr" rid="bib56">Zhang and Skolnick, 2005</xref>) to evaluate the fold similarities (in TM-scores) between the experimental structures of the interacting monomers in the training set and the test sets (HomoPDB, HeteroPDB). Specifically, for any two targets <italic>A–B</italic> and <italic>A’–B</italic>’ in the training set and test sets, respectively, where <italic>A, B, A</italic>’, and <italic>B</italic>’ represent the interacting monomers. We calculated the MTM-score defined as<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>M</mml:mi><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>min</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:msup><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>min</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:msup><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>between the two targets. The MTM-score higher than a certain value means that both the two interacting monomers in the two targets have fold similarity scores (TM-scores) higher than this value. When a threshold is chosen, we remove targets in the test tests if they have MTM-scores higher than this threshold when compared with any target in the training set. In this study, different thresholds, including 0.9, 0.8, 0.7, 0.6, and 0.5, were used in the study. 0.5 was chosen as the lowest threshold for protein pairs with TM-score &lt; 0.5 mainly not in the same fold.</p></sec></sec><sec id="s4-9"><title>Calculating the normalized number of the effective sequences of paired MSA</title><p>We define the normalized number of the effective sequences (<inline-formula><mml:math id="inf71"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) as follows:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mi>L</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>I</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>L</italic> is the length of the paired MSA, <italic>N</italic> is the number of sequences in the paired MSA, <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the sequence identity between the <italic>m</italic>th and <italic>n</italic>th sequences, and <italic>I</italic>[ ] represents the Iverson bracket, which means <inline-formula><mml:math id="inf73"><mml:mi>I</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> if <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ≥ 0.8 or 0 otherwise.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred, and PLMGraph-Inter on HomoPDB and HeteroPDB after the removal of targets which GLINTER failed to make the prediction using experimental structures (AlphaFold2 predicted structures).</title></caption><media xlink:href="elife-92184-supp1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>The performances of different ablation study models on the HomoPDB and HeteroPDB test sets.</title></caption><media xlink:href="elife-92184-supp2-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred, and PLMGraph-Inter on the DHTest and DB5.5 test sets using experimental structures (AlphaFold2 predicted structures).</title></caption><media xlink:href="elife-92184-supp3-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>The performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred, and PLMGraph-Inter on DHTest and DB5.5 after the removal of targets which GLINTER failed to make the prediction using experimental structures (AlphaFold2 predicted structures).</title></caption><media xlink:href="elife-92184-supp4-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>The performances of AlphaFold-Multimer and PLMGraph-Inter on the homodimer and heterodimer test sets.</title></caption><media xlink:href="elife-92184-supp5-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-92184-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript contains results which are computational. These are all presented and discussed in the main text. The datasets for training and testing PLMGraph-Inter are provided in <ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PLMGraph-Inter/tree/main/data">https://github.com/ChengfeiYan/PLMGraph-Inter/tree/main/data</ext-link>. The code for training and implementing PLMGraph-Inter is provided in <ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PLMGraph-Inter">https://github.com/ChengfeiYan/PLMGraph-Inter</ext-link> (copy archived at <xref ref-type="bibr" rid="bib52">Yan, 2023</xref>). The code for implementing PLMGraph-Inter is provided in <ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PLMGraph-Inter">https://github.com/ChengfeiYan/PLMGraph-Inter</ext-link>.</p></sec><ack id="ack"><title>Acknowledgements</title><p>The work was supported by the National Natural Science Foundation of China (32101001) and a new faculty startup grant (3004012167) from the Huazhong University of Science and Technology. The computation was completed in the HPC Platform of Huazhong University of Science and Technology.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alberts</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The cell as a collection of protein machines: preparing the next generation of molecular biologists</article-title><source>Cell</source><volume>92</volume><fpage>291</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/s0092-8674(00)80922-8</pub-id><pub-id pub-id-type="pmid">9476889</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basu</surname><given-names>S</given-names></name><name><surname>Wallner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>DockQ: A quality measure for protein-protein docking models</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0161879</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0161879</pub-id><pub-id pub-id-type="pmid">27560519</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>HM</given-names></name><name><surname>Westbrook</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Gilliland</surname><given-names>G</given-names></name><name><surname>Bhat</surname><given-names>TN</given-names></name><name><surname>Weissig</surname><given-names>H</given-names></name><name><surname>Shindyalov</surname><given-names>IN</given-names></name><name><surname>Bourne</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The protein data bank</article-title><source>Nucleic Acids Research</source><volume>28</volume><fpage>235</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id><pub-id pub-id-type="pmid">10592235</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonvin</surname><given-names>AMJJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Flexible protein-protein docking</article-title><source>Current Opinion in Structural Biology</source><volume>16</volume><fpage>194</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2006.02.002</pub-id><pub-id pub-id-type="pmid">16488145</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cong</surname><given-names>Q</given-names></name><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Protein interaction networks revealed by proteome coevolution</article-title><source>Science</source><volume>365</volume><fpage>185</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1126/science.aaw6718</pub-id><pub-id pub-id-type="pmid">31296772</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dauparas</surname><given-names>J</given-names></name><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Bennett</surname><given-names>N</given-names></name><name><surname>Bai</surname><given-names>H</given-names></name><name><surname>Ragotte</surname><given-names>RJ</given-names></name><name><surname>Milles</surname><given-names>LF</given-names></name><name><surname>Wicky</surname><given-names>BIM</given-names></name><name><surname>Courbet</surname><given-names>A</given-names></name><name><surname>de Haas</surname><given-names>RJ</given-names></name><name><surname>Bethel</surname><given-names>N</given-names></name><name><surname>Leung</surname><given-names>PJY</given-names></name><name><surname>Huddy</surname><given-names>TF</given-names></name><name><surname>Pellock</surname><given-names>S</given-names></name><name><surname>Tischer</surname><given-names>D</given-names></name><name><surname>Chan</surname><given-names>F</given-names></name><name><surname>Koepnick</surname><given-names>B</given-names></name><name><surname>Nguyen</surname><given-names>H</given-names></name><name><surname>Kang</surname><given-names>A</given-names></name><name><surname>Sankaran</surname><given-names>B</given-names></name><name><surname>Bera</surname><given-names>AK</given-names></name><name><surname>King</surname><given-names>NP</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Robust deep learning-based protein sequence design using ProteinMPNN</article-title><source>Science</source><volume>378</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1126/science.add2187</pub-id><pub-id pub-id-type="pmid">36108050</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dominguez</surname><given-names>C</given-names></name><name><surname>Boelens</surname><given-names>R</given-names></name><name><surname>Bonvin</surname><given-names>AMJJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>HADDOCK: A protein-protein docking approach based on biochemical or biophysical information</article-title><source>Journal of the American Chemical Society</source><volume>125</volume><fpage>1731</fpage><lpage>1737</lpage><pub-id pub-id-type="doi">10.1021/ja026939x</pub-id><pub-id pub-id-type="pmid">12580598</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>O’Neill</surname><given-names>M</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Antropova</surname><given-names>N</given-names></name><name><surname>Senior</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Blackwell</surname><given-names>S</given-names></name><name><surname>Yim</surname><given-names>J</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Protein Complex Prediction with AlphaFold-Multimer</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.10.04.463034</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodsell</surname><given-names>DS</given-names></name><name><surname>Olson</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Structural symmetry and protein function</article-title><source>Annual Review of Biophysics and Biomolecular Structure</source><volume>29</volume><fpage>105</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1146/annurev.biophys.29.1.105</pub-id><pub-id pub-id-type="pmid">10940245</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>AG</given-names></name><name><surname>Elhabashy</surname><given-names>H</given-names></name><name><surname>Brock</surname><given-names>KP</given-names></name><name><surname>Maddamsetti</surname><given-names>R</given-names></name><name><surname>Kohlbacher</surname><given-names>O</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Large-scale discovery of protein interactions at residue resolution using co-evolution calculated from genomic sequences</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1396</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-21636-z</pub-id><pub-id pub-id-type="pmid">33654096</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Skolnick</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Prediction of inter-chain distance maps of protein complexes with 2D attention-based deep neural networks</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>6963</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-34600-2</pub-id><pub-id pub-id-type="pmid">36379943</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanson</surname><given-names>J</given-names></name><name><surname>Paliwal</surname><given-names>K</given-names></name><name><surname>Litfin</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks</article-title><source>Bioinformatics</source><volume>34</volume><fpage>4039</fpage><lpage>4045</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty481</pub-id><pub-id pub-id-type="pmid">29931279</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honorato</surname><given-names>RV</given-names></name><name><surname>Koukos</surname><given-names>PI</given-names></name><name><surname>Jiménez-García</surname><given-names>B</given-names></name><name><surname>Tsaregorodtsev</surname><given-names>A</given-names></name><name><surname>Verlato</surname><given-names>M</given-names></name><name><surname>Giachetti</surname><given-names>A</given-names></name><name><surname>Rosato</surname><given-names>A</given-names></name><name><surname>Bonvin</surname><given-names>AMJJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Structural biology in the clouds: The WeNMR-EOSC ecosystem</article-title><source>Frontiers in Molecular Biosciences</source><volume>8</volume><elocation-id>729513</elocation-id><pub-id pub-id-type="doi">10.3389/fmolb.2021.729513</pub-id><pub-id pub-id-type="pmid">34395534</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>TA</given-names></name><name><surname>Schärfe</surname><given-names>CPI</given-names></name><name><surname>Rodrigues</surname><given-names>J</given-names></name><name><surname>Green</surname><given-names>AG</given-names></name><name><surname>Kohlbacher</surname><given-names>O</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Bonvin</surname><given-names>A</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sequence co-evolution gives 3D contacts and structures of protein complexes</article-title><source>eLife</source><volume>3</volume><elocation-id>e03430</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03430</pub-id><pub-id pub-id-type="pmid">25255213</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>C</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Hie</surname><given-names>B</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learning Inverse Folding from Millions of Predicted Structures</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.04.10.487779</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jing</surname><given-names>B</given-names></name><name><surname>Eismann</surname><given-names>S</given-names></name><name><surname>Soni</surname><given-names>PN</given-names></name><name><surname>Dror</surname><given-names>RO</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Equivariant graph neural networks for 3D macromolecular structure</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2106.03843">http://arxiv.org/abs/2106.03843</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jing</surname><given-names>B</given-names></name><name><surname>Eismann</surname><given-names>S</given-names></name><name><surname>Suriana</surname><given-names>P</given-names></name><name><surname>Townshend</surname><given-names>RJL</given-names></name><name><surname>Dror</surname><given-names>RO</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Learning from Protein Structure with Geometric Vector Perceptrons</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2009.01411">https://arxiv.org/abs/2009.01411</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DT</given-names></name><name><surname>Singh</surname><given-names>T</given-names></name><name><surname>Kosciolek</surname><given-names>T</given-names></name><name><surname>Tetchner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins</article-title><source>Bioinformatics</source><volume>31</volume><fpage>999</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu791</pub-id><pub-id pub-id-type="pmid">25431331</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ju</surname><given-names>F</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Shao</surname><given-names>B</given-names></name><name><surname>Kong</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>TY</given-names></name><name><surname>Zheng</surname><given-names>WM</given-names></name><name><surname>Bu</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>CopulaNet: Learning residue co-evolution directly from multiple sequence alignment for protein structure prediction</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2535</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22869-8</pub-id><pub-id pub-id-type="pmid">33953201</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Ballard</surname><given-names>AJ</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Nikolov</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>J</given-names></name><name><surname>Back</surname><given-names>T</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Reiman</surname><given-names>D</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Pacholska</surname><given-names>M</given-names></name><name><surname>Berghammer</surname><given-names>T</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Senior</surname><given-names>AW</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><pub-id pub-id-type="pmid">34265844</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Jaroszewski</surname><given-names>L</given-names></name><name><surname>Godzik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Clustering of highly homologous sequences to reduce the size of large protein databases</article-title><source>Bioinformatics</source><volume>17</volume><fpage>282</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/17.3.282</pub-id><pub-id pub-id-type="pmid">11294794</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>D-J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ResPRE: high-accuracy protein contact prediction by coupling precision matrix with deep residual neural networks</article-title><source>Bioinformatics</source><volume>35</volume><fpage>4647</fpage><lpage>4655</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btz291</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>SY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Protein–protein docking with interface residue restraints*</article-title><source>Chinese Physics B</source><volume>30</volume><elocation-id>018703</elocation-id><pub-id pub-id-type="doi">10.1088/1674-1056/abc14e</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>P</given-names></name><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>SY</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>DeepHomo2.0: improved protein–protein contact prediction of homodimers by transformer-enhanced deep learning</article-title><source>Briefings in Bioinformatics</source><volume>24</volume><elocation-id>bbac499</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbac499</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martino</surname><given-names>E</given-names></name><name><surname>Chiarugi</surname><given-names>S</given-names></name><name><surname>Margheriti</surname><given-names>F</given-names></name><name><surname>Garau</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mapping, structure and modulation of PPI</article-title><source>Frontiers in Chemistry</source><volume>9</volume><elocation-id>718405</elocation-id><pub-id pub-id-type="doi">10.3389/fchem.2021.718405</pub-id><pub-id pub-id-type="pmid">34692637</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Kamisetty</surname><given-names>H</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Robust and accurate prediction of residue-residue interactions across protein interfaces using evolutionary information</article-title><source>eLife</source><volume>3</volume><elocation-id>e02030</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.02030</pub-id><pub-id pub-id-type="pmid">24842992</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagès</surname><given-names>G</given-names></name><name><surname>Charmettant</surname><given-names>B</given-names></name><name><surname>Grudinin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Protein model quality assessment using 3D oriented convolutional neural networks</article-title><source>Bioinformatics</source><volume>35</volume><fpage>3313</fpage><lpage>3319</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btz122</pub-id><pub-id pub-id-type="pmid">30874723</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname><given-names>SC</given-names></name><name><surname>Luciani</surname><given-names>A</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>HMMER web server: 2018 update</article-title><source>Nucleic Acids Research</source><volume>46</volume><fpage>W200</fpage><lpage>W204</lpage><pub-id pub-id-type="doi">10.1093/nar/gky448</pub-id><pub-id pub-id-type="pmid">29905871</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Canny</surname><given-names>JF</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>MSA Transformer</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.02.12.430858</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Canny</surname><given-names>JF</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>MSA Transformer</article-title><conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name><fpage>8844</fpage><lpage>8856</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>PNAS</source><volume>118</volume><fpage>1</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id><pub-id pub-id-type="pmid">33876751</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>RS</given-names></name><name><surname>Quadir</surname><given-names>F</given-names></name><name><surname>Soltanikazemi</surname><given-names>E</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A deep dilated convolutional residual network for predicting interchain contacts of protein homodimers</article-title><source>Bioinformatics</source><volume>38</volume><fpage>1904</fpage><lpage>1910</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btac063</pub-id><pub-id pub-id-type="pmid">35134816</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seemayer</surname><given-names>S</given-names></name><name><surname>Gruber</surname><given-names>M</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>CCMpred--fast and precise prediction of protein residue-residue contacts from correlated mutations</article-title><source>Bioinformatics</source><volume>30</volume><fpage>3128</fpage><lpage>3130</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu500</pub-id><pub-id pub-id-type="pmid">25064567</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Si</surname><given-names>Y</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Improved protein contact prediction using dimensional hybrid residual networks and singularity enhanced loss function</article-title><source>Briefings in Bioinformatics</source><volume>22</volume><elocation-id>bbab341</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbab341</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Si</surname><given-names>Y</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Protein complex structure prediction powered by multiple sequence alignments of interologs from multiple taxonomic ranks and AlphaFold2</article-title><source>Briefings in Bioinformatics</source><volume>23</volume><elocation-id>bbac208</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbac208</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Si</surname><given-names>Y</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Improved inter-protein contact prediction using dimensional hybrid residual networks and protein language models</article-title><source>Briefings in Bioinformatics</source><volume>24</volume><elocation-id>bbad039</elocation-id><pub-id pub-id-type="doi">10.1093/bib/bbad039</pub-id><pub-id pub-id-type="pmid">36759333</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sledzieski</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Cowen</surname><given-names>L</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>D-SCRIPT translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein-protein interactions</article-title><source>Cell Systems</source><volume>12</volume><fpage>969</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2021.08.010</pub-id><pub-id pub-id-type="pmid">34536380</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spirin</surname><given-names>V</given-names></name><name><surname>Mirny</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Protein complexes and functional modules in molecular networks</article-title><source>PNAS</source><volume>100</volume><fpage>12123</fpage><lpage>12128</lpage><pub-id pub-id-type="doi">10.1073/pnas.2032324100</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Clustering huge protein sequence sets in linear time</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2542</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04964-5</pub-id><pub-id pub-id-type="pmid">29959318</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Mirdita</surname><given-names>M</given-names></name><name><surname>Vöhringer</surname><given-names>H</given-names></name><name><surname>Haunsberger</surname><given-names>SJ</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>HH-suite3 for fast remote homology detection and deep protein annotation</article-title><source>BMC Bioinformatics</source><volume>20</volume><elocation-id>473</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-019-3019-7</pub-id><pub-id pub-id-type="pmid">31521110</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Gong</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Review of multimer protein–protein interaction complex topology and structure prediction*</article-title><source>Chinese Physics B</source><volume>29</volume><elocation-id>108707</elocation-id><pub-id pub-id-type="doi">10.1088/1674-1056/abb659</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzek</surname><given-names>BE</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>McGarvey</surname><given-names>PB</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name><collab>UniProt Consortium</collab></person-group><year iso-8601-date="2015">2015</year><article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title><source>Bioinformatics</source><volume>31</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id><pub-id pub-id-type="pmid">25398609</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uguzzoni</surname><given-names>G</given-names></name><name><surname>John Lovis</surname><given-names>S</given-names></name><name><surname>Oteri</surname><given-names>F</given-names></name><name><surname>Schug</surname><given-names>A</given-names></name><name><surname>Szurmant</surname><given-names>H</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Large-scale identification of coevolution signals across homo-oligomeric protein interfaces by direct coupling analysis</article-title><source>PNAS</source><volume>114</volume><fpage>E2662</fpage><lpage>E2671</lpage><pub-id pub-id-type="doi">10.1073/pnas.1615068114</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Zundert</surname><given-names>GCP</given-names></name><name><surname>Rodrigues</surname><given-names>J</given-names></name><name><surname>Trellet</surname><given-names>M</given-names></name><name><surname>Schmitz</surname><given-names>C</given-names></name><name><surname>Kastritis</surname><given-names>PL</given-names></name><name><surname>Karaca</surname><given-names>E</given-names></name><name><surname>Melquiond</surname><given-names>ASJ</given-names></name><name><surname>van Dijk</surname><given-names>M</given-names></name><name><surname>de Vries</surname><given-names>SJ</given-names></name><name><surname>Bonvin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The HADDOCK2.2 Web Server: User-Friendly Integrative Modeling of Biomolecular Complexes</article-title><source>Journal of Molecular Biology</source><volume>428</volume><fpage>720</fpage><lpage>725</lpage><pub-id pub-id-type="doi">10.1016/j.jmb.2015.09.014</pub-id><pub-id pub-id-type="pmid">26410586</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accurate de novo prediction of protein contact map by ultra-deep learning model</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005324</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005324</pub-id><pub-id pub-id-type="pmid">28056090</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>White</surname><given-names>RA</given-names></name><name><surname>Szurmant</surname><given-names>H</given-names></name><name><surname>Hoch</surname><given-names>JA</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Identification of direct residue contacts in protein–protein interaction by message passing</article-title><source>PNAS</source><volume>106</volume><fpage>67</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1073/pnas.0805923106</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>T</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Gong</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Inter-chain contact map prediction for protein complex based on graph attention network and triangular multiplication update</article-title><conf-name>2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</conf-name><fpage>2143</fpage><lpage>2148</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>F</given-names></name><name><surname>Wu</surname><given-names>L</given-names></name><name><surname>Radev</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>SZ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Integration of pre-trained protein language models into geometric deep learning networks</article-title><source>Communications Biology</source><volume>6</volume><elocation-id>876</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-023-05133-1</pub-id><pub-id pub-id-type="pmid">37626165</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep graph learning of inter-protein contacts</article-title><source>Bioinformatics</source><volume>38</volume><fpage>947</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab761</pub-id><pub-id pub-id-type="pmid">34755837</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>SY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Accurate prediction of inter-protein residue–residue contacts for homo-oligomeric protein complexes</article-title><source>Briefings in Bioinformatics</source><volume>22</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1093/bib/bbab038</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Plmgraph-inter</data-title><version designator="swh:1:rev:b8afdd3dae15e1ddeec4b3d57f8d84d3668c7619">swh:1:rev:b8afdd3dae15e1ddeec4b3d57f8d84d3668c7619</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b462a9746bb7a2eed509375fedd35f613c2256dd;origin=https://github.com/ChengfeiYan/PLMGraph-Inter;visit=swh:1:snp:6ba464f147c15b7c528e1efeec52387c273f1770;anchor=swh:1:rev:b8afdd3dae15e1ddeec4b3d57f8d84d3668c7619">https://archive.softwareheritage.org/swh:1:dir:b462a9746bb7a2eed509375fedd35f613c2256dd;origin=https://github.com/ChengfeiYan/PLMGraph-Inter;visit=swh:1:snp:6ba464f147c15b7c528e1efeec52387c273f1770;anchor=swh:1:rev:b8afdd3dae15e1ddeec4b3d57f8d84d3668c7619</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Ppi_Msa-Taxonomy_Rank</data-title><version designator="1c783dd">1c783dd</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank">https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Zhou</surname><given-names>T</given-names></name><name><surname>Zhao</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Wu</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>ComplexContact: A web server for inter-protein contact prediction using deep learning</article-title><source>Nucleic Acids Research</source><volume>46</volume><fpage>W432</fpage><lpage>W437</lpage><pub-id pub-id-type="doi">10.1093/nar/gky420</pub-id><pub-id pub-id-type="pmid">29790960</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Skolnick</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Scoring function for automated assessment of protein structure template quality</article-title><source>Proteins</source><volume>57</volume><fpage>702</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1002/prot.20264</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Skolnick</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>TM-align: A protein structure alignment algorithm based on the TM-score</article-title><source>Nucleic Acids Research</source><volume>33</volume><fpage>2302</fpage><lpage>2309</lpage><pub-id pub-id-type="doi">10.1093/nar/gki524</pub-id><pub-id pub-id-type="pmid">15849316</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92184.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution><country>Switzerland</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Useful</kwd></kwd-group></front-stub><body><p>This study presents a <bold>useful</bold> deep learning-based inter-protein contact prediction method named PLMGraph-Inter which combines protein language models and geometric graphs. The evidence supporting the claims of the authors is <bold>solid</bold>. The authors show that their approach may be used in cases where AlphaFold-Multimer performs poorly. This work will be of interest to researchers working on protein complex structure prediction, particularly when accurate experimental structures are available for one or both of the monomers in isolation.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92184.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Given knowledge of the amino acid sequence and of some version of the 3D structure of two monomers that are expected to form a complex, the authors investigate whether it is possible to accurately predict which residues will be in contact in the 3D structure of the expected complex. To this effect, they train a deep learning model which takes as inputs the geometric structures of the individual monomers, per-residue features (PSSMs) extracted from MSAs for each monomer, and rich representations of the amino acid sequences computed with the pre-trained protein language models ESM-1b, MSA Transformer, and ESM-IF. Predicting inter-protein contacts in complexes is an important problem. Multimer variants of AlphaFold, such as AlphaFold-Multimer, are the current state of the art for full protein complex structure prediction, and if the three-dimensional structure of a complex can be accurately predicted then the inter-protein contacts can also be accurately determined. By contrast, the method presented here seeks state-of-the-art performance among models that have been trained end-to-end for inter-protein contact prediction.</p><p>Strengths:</p><p>The paper is carefully written and the method is very well detailed. The model works both for homodimers and heterodimers. The ablation studies convincingly demonstrate that the chosen model architecture is appropriate for the task. Various comparisons suggest that PLMGraph-Inter performs substantially better, given the same input, than DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter.</p><p>The authors control for some degree of redundancy between their training and test sets, both using sequence and structural similarity criteria. This is more careful than can be said of most works in the field of PPI prediction.</p><p>As a byproduct of the analysis, a potentially useful heuristic criterion for acceptable contact prediction quality is found by the authors: namely, to have at least 50% precision in the prediction of the top 50 contacts.</p><p>Weaknesses:</p><p>The authors check for performance drops when the test set is restricted to pairs of interacting proteins such that the chain pair is not similar *as a pair* (in sequence or structure) to a pair present in the training set. A more challenging test would be to restrict the test set to pairs of interacting proteins such that *none* of the chains are separately similar to monomers present in the training set. In the case of structural similarity (TM-scores), this would amount to replacing the two &quot;min&quot;s with &quot;max&quot;s in Eq. (4). In the case of sequence similarity, one would simply require that no monomer in the test set is in any MMSeqs2 cluster observed in the training set. This may be an important check to make, because a protein may interact with several partners, and/or may use the same sites for several distinct interactions, contributing to residual data leakage in the test set.</p><p>The training set of AFM with v2 weights has a global cutoff of 30 April 2018, while that of PLMGraph-Inter has a cutoff of March 7 2022. So there may be structures in the test set for PLMGraph-Inter that are not in the training set of AFM with v2 weights (released between May 2018 and March 2022). The &quot;Benchmark 2&quot; dataset from the AFM paper may have a few additional structures not in the training or test set for PLMGraph-Inter. I realize there may be only few structures that are in neither training set, but still think that showing the comparison between PLMGraph-Inter and AFM there would be important, even if no statistically significant conclusions can be drawn.</p><p>Finally, the inclusion of AFM confidence scores is very good. A user would likely trust AFM predictions when the confidence score is high, but look for alternative predictions when it is low. The authors' analysis (Figure 6, panels c and d) seems to suggest that, in the case of heterodimers, when AFM has low confidence, PLMGraph-Inter improves precision by (only) about 3% on average. By comparison, the reported gains in the &quot;DockQ-failed&quot; and &quot;precision-failed&quot; bins are based on knowledge of the ground truth final structure, and thus are not actionable in a real use-case.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92184.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This work introduces PLMGraph-Inter, a new deep learning approach for predicting inter-protein contacts, which is crucial for understanding protein-protein interactions. Despite advancements in this field, especially driven by AlphaFold, prediction accuracy and efficiency in terms of computational cost still remains an area for improvement. PLMGraph-Inter utilizes invariant geometric graphs to integrate the features from multiple protein language models into the structural information of each subunit. When compared against other inter-protein contact prediction methods, PLMGraph-Inter shows better performance which indicates that utilizing both sequence embeddings and structural embeddings is important to achieve high-accuracy predictions with relatively smaller computational costs for the model training.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92184.3.sa3</article-id><title-group><article-title>Author Response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Si</surname><given-names>Yunda</given-names></name><role specific-use="author">Author</role><aff><institution>Huazhong University of Science and Technology</institution><addr-line><named-content content-type="city">Wuhan</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Yan</surname><given-names>Chengfei</given-names></name><role specific-use="author">Author</role><aff><institution>Huazhong University of Science and Technology</institution><addr-line><named-content content-type="city">Wuhan</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the current reviews.</p><p>Overall Response</p><p>We thank the reviewers for reviewing our manuscript, recognizing the significance of our study, and offering valuable suggestions. Based on the reviewer’s comments and the updated eLife assessment, we would like to chose the current version of our manuscript as the Version of Record of our manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>Given knowledge of the amino acid sequence and of some version of the 3D structure of two monomers that are expected to form a complex, the authors investigate whether it is possible to accurately predict which residues will be in contact in the 3D structure of the expected complex. To this effect, they train a deep learning model which takes as inputs the geometric structures of the individual monomers, per-residue features (PSSMs) extracted from MSAs for each monomer, and rich representations of the amino acid sequences computed with the pre-trained protein language models ESM-1b, MSA Transformer, and ESM-IF. Predicting inter-protein contacts in complexes is an important problem. Multimer variants of AlphaFold, such as AlphaFold-Multimer, are the current state of the art for full protein complex structure prediction, and if the three-dimensional structure of a complex can be accurately predicted then the inter-protein contacts can also be accurately determined. By contrast, the method presented here seeks state-of-the-art performance among models that have been trained end-to-end for inter-protein contact prediction.</p><p>Strengths:</p><p>The paper is carefully written and the method is very well detailed. The model works both for homodimers and heterodimers. The ablation studies convincingly demonstrate that the chosen model architecture is appropriate for the task. Various comparisons suggest that PLMGraph-Inter performs substantially better, given the same input, than DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter.</p><p>The authors control for some degree of redundancy between their training and test sets, both using sequence and structural similarity criteria. This is more careful than can be said of most works in the field of PPI prediction.</p><p>As a byproduct of the analysis, a potentially useful heuristic criterion for acceptable contact prediction quality is found by the authors: namely, to have at least 50% precision in the prediction of the top 50 contacts.</p></disp-quote><p>We thank the reviewer for recognizing the strengths of our work!</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The authors check for performance drops when the test set is restricted to pairs of interacting proteins such that the chain pair is not similar <italic>as a pair</italic> (in sequence or structure) to a pair present in the training set. A more challenging test would be to restrict the test set to pairs of interacting proteins such that <italic>none</italic> of the chains are separately similar to monomers present in the training set. In the case of structural similarity (TM-scores), this would amount to replacing the two &quot;min&quot;s with &quot;max&quot;s in Eq. (4). In the case of sequence similarity, one would simply require that no monomer in the test set is in any MMSeqs2 cluster observed in the training set. This may be an important check to make, because a protein may interact with several partners, and/or may use the same sites for several distinct interactions, contributing to residual data leakage in the test set.</p></disp-quote><p>We thank the reviewer for the suggestion! In the case of protein-protein prediction (“0D prediction”) or protein-protein interfacial residue prediction(“1D prediction”), we think making none of the chains in the test set separately similar to monomers in the training set is necessary, as the reviewer pointed out that a protein may interact with several partners, and may even use the same sites for the interactions. Since the task of this study is predicting the inter-protein residue-residue contacts (“2D prediction”), even though a protein uses the same site to interact with different partners, as long as the interacting partners are different, the inter-protein contact maps would be different. Therefore, we don’t think that in our task, making this restriction to the test set is necessary.</p><disp-quote content-type="editor-comment"><p>The training set of AFM with v2 weights has a global cutoff of 30 April 2018, while that of PLMGraph-Inter has a cutoff of March 7 2022. So there may be structures in the test set for PLMGraph-Inter that are not in the training set of AFM with v2 weights (released between May 2018 and March 2022). The &quot;Benchmark 2&quot; dataset from the AFM paper may have a few additional structures not in the training or test set for PLMGraph-Inter. I realize there may be only few structures that are in neither training set, but still think that showing the comparison between PLMGraph-Inter and AFM there would be important, even if no statistically significant conclusions can be drawn.</p></disp-quote><p>We thank the reviewer for the suggestion! It is not enough to only use the date cutoff to remove the redundancy, since similar structures can be deposited in the PDB in different dates. Because AFM does not release the PDB codes of its training set, it is difficult for us to totally remove the redundancy. Therefore, we think no rigorous conclusion can be drawn by including these comparisons in the manuscript. Besides, the main point of this study is to demonstrate that the integration of multiple protein language models using protein geometric graphs can dramatically improve the model performance for inter-protein contact prediction, which can provide some important enlightenments for the future development of more powerful protein complex structure prediction methods beyond AFM, rather than providing a tool which can beat AFM at this moment. We think including too many stuffs in the comparison with AFM may distract the readers. Therefore, we choose to not include these comparisons in the manuscript.</p><disp-quote content-type="editor-comment"><p>Finally, the inclusion of AFM confidence scores is very good. A user would likely trust AFM predictions when the confidence score is high, but look for alternative predictions when it is low. The authors' analysis (Figure 6, panels c and d) seems to suggest that, in the case of heterodimers, when AFM has low confidence, PLMGraph-Inter improves precision by (only) about 3% on average. By comparison, the reported gains in the &quot;DockQ-failed&quot; and &quot;precision-failed&quot; bins are based on knowledge of the ground truth final structure, and thus are not actionable in a real use-case.</p></disp-quote><p>We agree with the reviewer that more studies are needed for providing a model which can well complement or even beat AFM. The main point of this study is to demonstrate that the integration of multiple protein language models using protein geometric graphs can dramatically improve the model performance for inter-protein contact prediction, which can provide some important enlightenments for the future development of more powerful protein complex structure prediction methods beyond AFM.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>This work introduces PLMGraph-Inter, a new deep learning approach for predicting inter-protein contacts, which is crucial for understanding proteinprotein interactions. Despite advancements in this field, especially driven by AlphaFold, prediction accuracy and efficiency in terms of computational cost still remains an area for improvement. PLMGraph-Inter utilizes invariant geometric graphs to integrate the features from multiple protein language models into the structural information of each subunit. When compared against other inter-protein contact prediction methods, PLMGraph-Inter shows better performance which indicates that utilizing both sequence embeddings and structural embeddings is important to achieve high-accuracy predictions with relatively smaller computational costs for the model training.</p></disp-quote><p>We thank the reviewer for recognizing the strengths of our work!</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><list list-type="bullet"><list-item><p>I recommend renaming the section &quot;Further potential redundancies removal between the training and the test&quot; to &quot;Further potential redundancies removal between the training and the test sets&quot;</p></list-item></list></disp-quote><p>Changed.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>In lines 768-769, the sentence seems to end prematurely in &quot;to use more stringent threshold in the redundancy removal&quot;</p></list-item></list></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>In Eq. (4), line 789, there are many instances of dashes that look like minus signs, creating some confusion.</p></list-item></list></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>I think I may have mixed up figure references in my first review. When I said (Recommendations to the authors): &quot;p. 22, line 2: from the figure, I would have guessed &quot;greater than or equal to 0.7&quot;, not 0.8&quot;, I think I was referring to what is now lines 423-424, referring to what is now Figure 5c. The point stands there, I think.</p></list-item></list></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>A couple of new grammatical mishaps have been introduced in the revision. These could be rectified.</p></list-item></list></disp-quote><p>We carefully rechecked our revisions, and corrected the grammatical issues we found.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Most of my concerns were resolved through the revision. I have only one suggestion for the main figure.</p><p>The current scatter plots in Figure 2 are hard to understand as too many different methods are abstracted into a single plot with multiple colors. I would suggest comparing their performances using box plot or violin plot for the figure 2.</p></disp-quote><p>We thank the reviewer for the suggestion! In the revision, we tried violin plot, but it does not look good since too many different methods are included in the plot. Besides, we chose the scatter plot as it can provide much more details. We also provided the individual head-to-head scatter plots as supplementary figures, we think which can also be helpful for the readers to capture the information of the figures.</p><p>The following is the authors’ response to the original reviews.</p><p>Overall Response</p><p>We would like to thank the reviewers for reviewing our manuscript, recognizing the significance of our study, and offering valuable suggestions. We have carefully revised the manuscript to address all the concerns and suggestions raised by the reviewers.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>Given knowledge of the amino acid sequence and of some version of the 3D structure of two monomers that are expected to form a complex, the authors investigate whether it is possible to accurately predict which residues will be in contact in the 3D structure of the expected complex. To this effect, they train a deep learning model that takes as inputs the geometric structures of the individual monomers, per-residue features (PSSMs) extracted from MSAs for each monomer, and rich representations of the amino acid sequences computed with the pre-trained protein language models ESM-1b, MSA Transformer, and ESM-IF. Predicting inter-protein contacts in complexes is an important problem. Multimer variants of AlphaFold, such as AlphaFold-Multimer, are the current state of the art for full protein complex structure prediction, and if the three-dimensional structure of a complex can be accurately predicted then the inter-protein contacts can also be accurately determined. By contrast, the method presented here seeks state-of-the-art performance among models that have been trained end-to-end for inter-protein contact prediction.</p><p>Strengths:</p><p>The paper is carefully written and the method is very well detailed. The model works both for homodimers and heterodimers. The ablation studies convincingly demonstrate that the chosen model architecture is appropriate for the task. Various comparisons suggest that PLMGraph-Inter performs substantially better, given the same input than DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter. As a byproduct of the analysis, a potentially useful heuristic criterion for acceptable contact prediction quality is found by the authors: namely, to have at least 50% precision in the prediction of the top 50 contacts.</p></disp-quote><p>We thank the reviewer for recognizing the strengths of our work!</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>My biggest issue with this work is the evaluations made using <italic>bound</italic> monomer structures as inputs, coming from the very complexes to be predicted.Conformational changes in protein-protein association are the key element of the binding mechanism and are challenging to predict. While the GLINTER paper (Xie &amp; Xu, 2022) is guilty of the same sin, the authors of CDPred (Guo et al., 2022) correctly only report test results obtained using predicted unbound tertiary structures as inputs to their model. Test results using experimental monomer structures in bound states can hide important limitations in the model, and thus say very little about the realistic use cases in which only the unbound structures (experimental or predicted) are available. I therefore strongly suggest reducing the importance given to the results obtained using bound structures and emphasizing instead those obtained using predicted monomer structures as inputs.</p></disp-quote><p>We thank the reviewer for the suggestion! In the revision, to emphasize the performance of PLMGraph-Inter using the predicted monomer structures, we moved the evaluation results based on the predicted monomer from the supplementary to the main text (see the new Table 1 and Figure 2 in the revised manuscript) and re-organized the two subsections “Evaluation of PLMGraph-Inter on HomoPDB and HeteroPDB test sets” and “Impact of the monomeric structure quality on contact prediction” in the main text.</p><disp-quote content-type="editor-comment"><p>In particular, the most relevant comparison with AlphaFold-Multimer (AFM) is given in Figure S2, <italic>not</italic> Figure 6. Unfortunately, it substantially shrinks the proportion of structures for which AFM fails while PLMGraph-Inter performs decently. Still, it would be interesting to investigate why this occurs. One possibility would be that the predicted monomer structures are of bad quality there, and PLMGraph-Inter may be able to rely on a signal from its language model features instead. Finally, AFM multimer confidence values (&quot;iptm + ptm&quot;) should be provided, especially in the cases in which AFM struggles.</p></disp-quote><p>We thank the reviewer for the suggestion! It is worth noting that AFM automatically searches monomer templates in the prediction, and when we checked our AFM runs, we found that 99% of the targets in our study (including all the targets in the four datasets: HomoPDB, HeteroPDB, DHTest and DB5.5) at least 20 templates were identified (AFM employed the top 20 templates in the prediction), and 87.8% of the targets employed the native templates (line 455-462 in page 25 in the subsection of “Comparison of PLMGraph-Inter with AlphaFold-Multimer”). Therefore, we think Figure 6 not Figure S5 (the original Figure S2) shows a fairer comparison. Besides, it is also worth noting the targets used in this study would have a large overlap with the training set of AlphaFold-Multimer, since AFM used all protein complex structures in PDB deposited before 2018-04-30 in the model training, which would further cause the overestimation of the performance of AFM (line 450-455 in page 24-25 in the subsection of “Comparison of PLMGraph-Inter with AlphaFold-Multimer”).</p><p>To mimic the performance of AlphaFold2 in real practice and produce predicted monomeric structures with more diverse qualities, we only used the MSA searched from Uniref100 protein sequence database as the input to AlphaFold2 and set to not use the template (line 203~210 in page 12 in the subsection of “Evaluation of PLMGraph-Inter on HomoPDB and HeteroPDB test sets”). Since some of the predicted monomer structures are of bad quality, it is reasonable that the performance of PLMGraph-Inter drops when the predicted monomeric structures are used in the prediction. We provided a detailed analysis of the impact of the monomeric structure quality on the prediction performance in the subsection “Impact of the monomeric structure quality on contact prediction” in the main text.</p><p>We provided the analysis of the AFM multimer confidence values (“iptm + ptm”) in the revision (Figure 6, Figure S5 and line 495-501 in page 27 in the subsection of“Comparison of PLMGraph-Inter with AlphaFold-Multimer”).</p><disp-quote content-type="editor-comment"><p>Besides, in cases where <italic>any</italic> experimental structures - bound or unbound - are available and given to PLMGraph-Inter as inputs, they should also be provided to AlphaFold-Multimer (AFM) as templates. Withholding these from AFM only makes the comparison artificially unfair. Hence, a new test should be run using AFM templates, and a new version of Figure 6 should be produced. Additionally, AFM's mean precision, at least for top-50 contact prediction, should be reported so it can be compared with PLMGraph-Inter's.</p></disp-quote><p>We thank the reviewers for the suggestion, and we are sorry for the confusion! In the AFM runs to predict protein complex structures, we used the default setting of AFM which automatically searches monomer templates in the prediction. When we checked our AFM runs, we found that 99% of the targets in our study (including all the targets in the four datasets: HomoPDB, HeteroPDB, DHTest and DB5.5) employed at least 20 templates in their predictions (AFM only used the top 20 templates), and 87.8% of the targets employed the native template. We further clarified this in the revision (line 455462 in page 25 in the subsection of “Comparison of PLMGraph-Inter with AlphaFoldMultimer”). We also included the mean precisions of AFM (top-50 contact prediction) in the revision (Table S5 and line 483-484 in page 26 in the subsection of “Comparison of PLMGraph-Inter with AlphaFold-Multimer”).</p><disp-quote content-type="editor-comment"><p>It's a shame that many of the structures used in the comparison with AFM are actually in the AFM v2 training set. If there are any outside the AFM v2 training set and, ideally, not sequence- or structure-homologous to anything in the AFM v2 training set, they should be discussed and reported on separately. In addition, why not test on structures from the &quot;Benchmark 2&quot; or &quot;Recent-PDB-Multimers&quot; datasets used in the AFM paper?</p></disp-quote><p>We thank the reviewer for the suggestion! The biggest challenge to objectively evaluate AFM is that as far as we known, AFM does not release the PDB ids of its training set and the “Recent-PDB-Multimers” dataset. “Benchmark 2” only includes 17 heterodimer proteins, and the number would be further decreased after removing targets redundant to our training set. We think it is difficult to draw conclusions from such a small number of targets.</p><disp-quote content-type="editor-comment"><p>It is also worth noting that the AFM v2 weights have now been outdated for a while, and better v3 weights now exist, with a training cutoff of 2021-09-30.</p></disp-quote><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>The head-to-head comparison of qualities of complex predicted by AlphaFold-Multimer (2.2.0) and AlphaFold-Multimer (2.3.2) for each target PPI.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92184-sa3-fig1-v2.tif"/></fig><p>We thank the reviewer for reminding the new version of AFM. The only difference between AFM V3 and V2 is the cutoff date of the training set. During the revision, we also tested the new version of AFM on the datasets of HomoPDB and HeteroPDB, but we found the performance difference between the two versions of AFM is actually very little (see the figure above, not shown in the main text). One reason might be that some targets in HomoPDB and HeteroPDB are redundant with the training sets of the two version of AFM. Since our test sets would have more overlaps with the training set of AFM V3, we keep using the AFM V2 weights in this study.</p><disp-quote content-type="editor-comment"><p>Another weakness in the evaluation framework: because PLMGraph-Inter uses structural inputs, it is not sufficient to make its test set non-redundant in sequence to its training set. It must also be non-redundant in structure. The Benchmark 2 dataset mentioned above is an example of a test set constructed by removing structures with homologous templates in the AF2 training set. Something similar should be done here.</p></disp-quote><p>We thank the reviewer for the suggestion! In the revision, we explored the performance of PLMGraph-Inter when using different thresholds of fold similarity scores of interacting monomers to further remove potential redundancies between the training and test sets (i.e. redundancy in structure ) (line 353-386 in page 19-21 in the subsection “Ablation study”; line 762-797 in page 41-43 in the subsection “Further potential redundancies removal between the training and the test”). We found that for heteromeric PPIs (targets in HeteroPDB), the further removal of potential redundancy in structure has little impact on the model performance (~3%, when TM-score 0.5 is used as the threshold). However, for homomeric PPIs (targets in HomoPDB), the further removal of potential redundancy in structure significantly reduce the model performance (~18%, when TM-score 0.5 is used as the threshold) (see Table 2). One possible reason for this phenomenon is that the binding mode of the homomeric PPI is largely determined by the fold of its monomer, thus the does not generalize well on targets whose folds have never been seen during the training.</p><p>Whether the deep learning model can generalize well on targets with novel folds is a very interesting and important question. We thank the reviewer for pointing out this!However, to the best of our knowledge, this question has rarely been addressed by previous studies including AFM. For example, the Benchmark 2 dataset is prepared by ClusPro TBM (bioRxiv 2021.09.07.459290; Proteins 2020, 88:1082-1090) which uses a sequence-based approach (HHsearch) to identify templates not structure-based.Therefore, we don’t think this dataset is non-redundant in structure.</p><disp-quote content-type="editor-comment"><p>Finally, the performance of DRN-1D2D for top-50 precision reported in Table 1 suggests to me that, in an ablation study, language model features alone would yield better performance than geometric features alone. So, I am puzzled why model &quot;a&quot; in the ablation is a &quot;geometry-only&quot; model and not a &quot;LM-only&quot; one.</p></disp-quote><p>Using the protein geometric graph to integrate multiple protein language models is the main idea of PLMGraph-Inter. Comparing with our previous work (DRN-1D2D_Inter), we consider the building of the geometric graph as one major contribution of this work. To emphasize the efficacy of this geometric graph, we chose to use the “geometry-only” model as the base model.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Some sections of the paper use technical terminology which limits accessibility to a broad audience. An obvious example is in the section &quot;Results &gt; Overview of PLMGraph-Inter &gt; The residual network module&quot;: the average eLife reader is not a machine learning expert and might not be familiar with a &quot;convolution with kernel size of 1 * 1&quot;. In general, the &quot;Overview of PLMGraph-Inter&quot; is a bit heavy with technical details, and I suggest moving many of these to Methods. This overview section can still be there but it should be shorter and written using less technical language.</p></disp-quote><p>We thank the reviewer for the suggestion! We moved some technical details to the Methods section in the revision (line 184-185 in page 11; line 729-735 in page 39).</p><disp-quote content-type="editor-comment"><p>List of typos and minor issues (page number according to merged PDF):</p><list list-type="bullet"><list-item><p>p. 3. line -3: remove &quot;to&quot;</p></list-item></list></disp-quote><p>Corrected (line 36, page 3)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 5, line 7: &quot;GINTER&quot; should be &quot;GLINTER&quot;</p></list-item></list></disp-quote><p>Corrected (line 64, page 5)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 6, line -4: &quot;Given structures&quot; -&gt; &quot;Given the structures&quot;</p></list-item></list></disp-quote><p>Corrected (line 95, page 6)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 6, line -2: &quot;with which encoded&quot;... ?</p></list-item></list></disp-quote><p>We rephrased this sentence in revision. (line 97, page 6)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 9, line 1: &quot;principal&quot; -&gt; &quot;principle&quot;</p></list-item></list></disp-quote><p>Corrected (line 142, page 9)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 13, line 1: &quot;has&quot; -&gt; &quot;but have&quot;</p></list-item></list></disp-quote><p>Corrected (line 231, page 13)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 14, lines 6-7: &quot;As can be seen from the figure that the predicted&quot; -&gt; &quot;As can be seen from the figure, the predicted&quot;</p></list-item></list></disp-quote><p>We rephrased this paragraph, and the sentence was deleted in the revision (line 257-259 in page 15).</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 18, line 1: the &quot;five models&quot; are presumably models a-e? If so, say &quot;of models a-e&quot;</p></list-item></list></disp-quote><p>Corrected (line 310, page 17)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 22, line 2: from the figure, I would have guessed &quot;greater than or equal to 0.7&quot;, not 0.8</p></list-item></list></disp-quote><p>Based the Figure 3C, we think 0.8 is a more appropriate cutoff, since the precision drops significantly when the DTM-score is within 0.7~0.8.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 23, lines 2-3: &quot;worth to making&quot; -&gt; &quot;worth making&quot;</p></list-item></list></disp-quote><p>Corrected (line 443, page 24)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p. 24, line -5: &quot;predict&quot; -&gt; &quot;predicted&quot;</p></list-item></list></disp-quote><p>Corrected (line 484, page 26)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>p 28, line -5: Please clarify what you mean by &quot;We doubt&quot;: are you saying that you don't think these rearrangements exist in nature? If not, then reword.</p></list-item></list></disp-quote><p>Corrected (line 566, page 30)</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figure 2, panel c, &quot;DCPred&quot; in the legend should be &quot;CDPred&quot;</p></list-item></list></disp-quote><p>Corrected</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figures 3 and 5: Please improve the y-axis title in panel C. &quot;Percent&quot; of what?</p></list-item></list></disp-quote><p>We changed the “Percent” to “% of targets” in the revision.</p><p>We thank the reviewer for carefully reading our manuscript!</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>This work introduces PLMGraph-Inter, a new deep-learning approach for predicting inter-protein contacts, which is crucial for understanding proteinprotein interactions. Despite advancements in this field, especially driven by AlphaFold, prediction accuracy and efficiency in terms of computational cost still remains an area for improvement. PLMGraph-Inter utilizes invariant geometric graphs to integrate the features from multiple protein language models into the structural information of each subunit. When compared against other inter-protein contact prediction methods, PLMGraph-Inter shows better performance which indicates that utilizing both sequence embeddings and structural embeddings is important to achieve high-accuracy predictions with relatively smaller computational costs for the model training.</p><p>The conclusions of this paper are mostly well supported by data, but test examples should be revisited with a more strict sequence identity cutoff to avoid any potential information leakage from the training data. The main figures should be improved to make them easier to understand.</p></disp-quote><p>We thank the reviewer for recognizing the significance of our work! We have carefully revised the manuscript to address the reviewer’s concerns.</p><disp-quote content-type="editor-comment"><p>(1) The sequence identity cutoff to remove redundancies between training and test set was set to 40%, which is a bit high to remove test examples having homology to training examples. For example, CDPred uses a sequence identity cutoff of 30% to strictly remove redundancies between training and test set examples. To make their results more solid, the authors should have curated test examples with lower sequence identity cutoffs, or have provided the performance changes against sequence identities to the closest training examples.</p></disp-quote><p>We thank the reviewer for the valuable suggestion! The “40 sequence identity” is a widely used threshold to remove redundancy when evaluating deep-learning based protein-protein interaction and protein complex structure prediction methods, thus we also chose this threshold in our study (bioRxiv 2021.10.04.463034, Cell Syst. 2021 Oct 20;12(10):969-982.e6). In the revision, we explored whether PLMGraph-inter can keep its performance when more stringent thresholds (30%,20%,10%) is applied (line 353386 in page 20-21 in the subsection of “Ablation study” and line 762-780 in page 40 in the subsection of “Further potential redundancies removal between the training and the test”). The result shows that even when using “10% sequence identity” as the threshold, mean precisions of the predicted contacts only decreases by ~3% (Table 2).</p><disp-quote content-type="editor-comment"><p>(2) Figures with head-to-head comparison scatter plots are hard to understand as scatter plots because too many different methods are abstracted into a single plot with multiple colors. It would be better to provide individual head-tohead scatter plots as supplementary figures, not in the main figure.</p></disp-quote><p>We thank the reviewer for the suggestion! We will include the individual head-to-head scatter plots as supplementary figures in the revision (Figure S1 and Figure S2 in the supplementary).</p><disp-quote content-type="editor-comment"><p>(3) The authors claim that PLMGraph-Inter is complementary to AlphaFoldmultimer as it shows better precision for the cases where AlphaFold-multimer fails. To strengthen the point, the qualities of predicted complex structures via protein-protein docking with predicted contacts as restraints should have been compared to those of AlphaFold-multimer structures.</p></disp-quote><p>We thank the reviewer for the suggestion! We included this comparison in the revision(Figure S7).</p><disp-quote content-type="editor-comment"><p>(4) It would be interesting to further analyze whether there is a difference in prediction performance depending on the depth of multiple sequence alignment or the type of complex (antigen-antibody, enzyme-substrates, single species PPI, multiple species PPI, etc).</p></disp-quote><p>We thank the reviewer for the suggestion! We analyzed the relationship between the prediction performance and the depth of MSA in the revision (Figure S4 and Line 253264 in page 15 in the subsection of “Evaluation of PLMGraph-Inter on HomoPDB and HeteroPDB test sets” and line 798-806 in page 42 in the subsection of “Calculating the normalized number of the effective sequences of paired MSA”).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>I have the following suggestions in addition to the public review.</p><p>(1) Overall, the manuscript is well-written; however, I recommend a careful review for minor grammar corrections to polish the final text.</p></disp-quote><p>We carefully checked the manuscript and corrected all the grammar issues and typos we found in the revision.</p><disp-quote content-type="editor-comment"><p>(2) It would be better to indicate that single sequence embeddings, MSA embeddings, and structure embeddings are ESM-1b, ESM-MSA &amp; PSSM, and ESM-IF when they are first mentioned in the manuscript e.g. single sequence embeddings from ESM-1b, MSA embeddings from ESM-MSA and PSSM, and structural embeddings from ESM-IF.</p></disp-quote><p>We revised the manuscript according to the reviewer’s suggestion (line 86-88 in page 6; line 99-101 in page 7).</p><disp-quote content-type="editor-comment"><p>(3) I don't think &quot;outer concatenation&quot; is commonly used. Please specify whether it's outer sum, outer product, or horizontal &amp; vertical tiling followed by concatenation.</p></disp-quote><p>It is horizontal &amp; vertical tiling followed by concatenation. We clarified this in the revision (line 129-130 in page 8).</p><disp-quote content-type="editor-comment"><p>(4) 10th sentence on the page where the Results section starts, please briefly mention what are the other 2D pairwise features.</p></disp-quote><p>We clarified this in the revision (line 131-132 in page 8).</p><disp-quote content-type="editor-comment"><p>(5) In the result section, it states edges are defined based on Ca distances, but in the method section, it says edges are determined based on heavy atom distances. Please correct one of them.</p></disp-quote><p>It should be Ca distances. We are sorry for the carelessness, and we corrected this in the revision (line 646 in page 35).</p><disp-quote content-type="editor-comment"><p>(6) For the sentence, &quot;Where ESM-1b and ESM-MSA-1b are pretrained PLMs learned from large datasets of sequences and MSAs respectively without label supervision,&quot;, I'd suggest replacing &quot;without label supervision&quot; with &quot;with masked language modeling tasks&quot; for clarity.</p></disp-quote><p>We revised the manuscript according to the reviewer’s suggestion (line 150-151 in page9).</p><disp-quote content-type="editor-comment"><p>(7) It would be better to briefly explain what is the dimensional hybrid residual block when it first mentioned.</p></disp-quote><p>We explained the dimensional hybrid residue block when it first mentioned in the revision (line 107 in page 7).</p><disp-quote content-type="editor-comment"><p>(8) Please include error bars for the bar plots and standard deviations for the tables.</p></disp-quote><p>We thank the reviewer for the suggestion! Our understanding is the error bars and standard deviations are very informative for data which follow gaussian-like distributions, but our data (precisions of the predicted contacts) are obviously not this type. Most previous studies in protein contact prediction and inter-protein contact prediction also did not include these in their plots or tables. In our case, including these elements requires a dramatic change of the styles of our figures and tables, but we would like to not change our figures and tables too much in the revision.</p><disp-quote content-type="editor-comment"><p>(9) Please indicate whether the chain break is considered to generate attention map features from ESM-MSA-1b. If it's considered, please specify how.</p></disp-quote><p>The paired sequences were directly concatenated without using any letter to connect them, which means we did not consider chain break in generating the attention maps from ESM-MSA-1b.</p></body></sub-article></article>