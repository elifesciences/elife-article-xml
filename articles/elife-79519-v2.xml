<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79519</article-id><article-id pub-id-type="doi">10.7554/eLife.79519</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Cell Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>DetecDiv, a generalist deep-learning platform for automated cell division tracking and survival analysis</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-242319"><name><surname>Aspert</surname><given-names>Théo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2957-0683</contrib-id><email>theo.aspert@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-271217"><name><surname>Hentsch</surname><given-names>Didier</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-16661"><name><surname>Charvin</surname><given-names>Gilles</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6852-6952</contrib-id><email>charvin@unistra.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0015ws592</institution-id><institution>Department of Developmental Biology and Stem Cells, Institut de Génétique et de Biologie Moléculaire et Cellulaire</institution></institution-wrap><addr-line><named-content content-type="city">Strasbourg</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>Centre National de la Recherche Scientifique</institution></institution-wrap><addr-line><named-content content-type="city">Strasbourg</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>Institut National de la Santé et de la Recherche Médicale</institution></institution-wrap><addr-line><named-content content-type="city">Strasbourg</named-content></addr-line><country>France</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00pg6eq24</institution-id><institution>Université de Strasbourg</institution></institution-wrap><addr-line><named-content content-type="city">Strasbourg</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Smal</surname><given-names>Ihor</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/018906e22</institution-id><institution>Erasmus University Medical Center</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Isales</surname><given-names>Carlos</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/012mef835</institution-id><institution>Medical College of Georgia at Augusta University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>17</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e79519</elocation-id><history><date date-type="received" iso-8601-date="2022-04-15"><day>15</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-08-16"><day>16</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-10-05"><day>05</day><month>10</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.10.05.463175"/></event></pub-history><permissions><copyright-statement>© 2022, Aspert et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Aspert et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79519-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-79519-figures-v2.pdf"/><abstract><p>Automating the extraction of meaningful temporal information from sequences of microscopy images represents a major challenge to characterize dynamical biological processes. So far, strong limitations in the ability to quantitatively analyze single-cell trajectories have prevented large-scale investigations to assess the dynamics of entry into replicative senescence in yeast. Here, we have developed DetecDiv, a microfluidic-based image acquisition platform combined with deep learning-based software for high-throughput single-cell division tracking. We show that DetecDiv can automatically reconstruct cellular replicative lifespans with high accuracy and performs similarly with various imaging platforms and geometries of microfluidic traps. In addition, this methodology provides comprehensive temporal cellular metrics using time-series classification and image semantic segmentation. Last, we show that this method can be further applied to automatically quantify the dynamics of cellular adaptation and real-time cell survival upon exposure to environmental stress. Hence, this methodology provides an all-in-one toolbox for high-throughput phenotyping for cell cycle, stress response, and replicative lifespan assays.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>deep learning</kwd><kwd>image processing</kwd><kwd>microfluidics</kwd><kwd>replicative aging</kwd><kwd>stress response</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>S. cerevisiae</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-LABX-0030-INRT</award-id><principal-award-recipient><name><surname>Charvin</surname><given-names>Gilles</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-IDEX-0002-02</award-id><principal-award-recipient><name><surname>Charvin</surname><given-names>Gilles</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A deep learning-based image classification pipeline unleashes automated division counting and replicative lifespan analyses of single cells growing in microfluidic cellular traps.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Epigenetic processes that span several division cycles are ubiquitous in biology and underlie essential biological functions, such as cellular memory phenomena (<xref ref-type="bibr" rid="bib7">Caudron and Barral, 2013</xref>; <xref ref-type="bibr" rid="bib6">Bheda et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Kundu et al., 2007</xref>), differentiation, and aging (<xref ref-type="bibr" rid="bib11">Denoth Lippuner et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Janssens and Veenhoff, 2016</xref>). In budding yeast, mother cells undergo about 20–30 asymmetric divisions before entering senescence and dying (<xref ref-type="bibr" rid="bib42">Mortimer and Johnson, 1959</xref>). Over the last decades, this simple unicellular has become a reference model for understanding the fundamental mechanisms that control longevity (<xref ref-type="bibr" rid="bib11">Denoth Lippuner et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Janssens and Veenhoff, 2016</xref>). Several independent mechanistic models have been proposed to explain entry into replicative senescence, including asymmetric accumulation of extrachromosomal rDNA circles (ERCs) (<xref ref-type="bibr" rid="bib51">Sinclair and Guarente, 1997</xref>), protein aggregates (<xref ref-type="bibr" rid="bib1">Aguilaniu et al., 2003</xref>), signaling processes associated with loss of vacuole acidity (<xref ref-type="bibr" rid="bib25">Hughes and Gottschling, 2012</xref>), or loss of chromatin silencing (<xref ref-type="bibr" rid="bib44">Pal and Tyler, 2016</xref>). Classical replicative lifespan (RLS) assays by microdissection, combined with genetic perturbations, have been decisive in identifying and characterizing genetic factors and pathways that influence longevity in budding yeast (<xref ref-type="bibr" rid="bib40">McCormick et al., 2015</xref>). Similarly, enrichment techniques of aged mother cells in a batch provided further understanding of the physiology of cellular senescence in this model organism (<xref ref-type="bibr" rid="bib38">Lindstrom and Gottschling, 2009</xref>; <xref ref-type="bibr" rid="bib27">Janssens et al., 2015</xref>).</p><p>However, how the appearance of markers of aging is coordinated temporally and causally remains poorly understood (<xref ref-type="bibr" rid="bib13">Dillin et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">He et al., 2018</xref>). In part, this is due to the difficulty of directly characterizing the sequence of events that constitute the senescence entry scenario: RLS assays by microdissection generally give no information other than the replicative age upon cell death; old cells enrichment techniques ignore the well-known large cell-cell variability in the progression to senescence, which may blur the sequence of individual cellular events.</p><p>Based on pioneering work in yeast (<xref ref-type="bibr" rid="bib49">Ryley and Pereira-Smith, 2006</xref>) and bacteria (<xref ref-type="bibr" rid="bib58">Wang et al., 2010</xref>), the development of microfluidics-based mother cell traps has partially alleviated these limitations by allowing continuous observation of individual cell divisions and relevant fluorescent cellular markers under the microscope from birth to death (<xref ref-type="bibr" rid="bib34">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib59">Xie et al., 2012</xref>; <xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>). In these studies, monitoring individual cells over time in a microfluidic device has demonstrated the unique potential to quantitatively characterize the heterogeneity in cellular dynamics during aging. Recent years have seen a wide diversification of microfluidic devices aimed at improving both experimental throughput and cell retention rates (<xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Liu et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Li et al., 2017</xref>). These new developments have helped to highlight the existence of independent trajectories leading to cell death (<xref ref-type="bibr" rid="bib35">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib41">Morlot et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Li et al., 2020</xref>) and to better understand the physiopathology of the senescent state (<xref ref-type="bibr" rid="bib43">Neurohr et al., 2018</xref>).</p><p>However, the hype surrounding these emerging microfluidic techniques has so far masked a key problem associated with high-throughput time-lapse imaging, namely the difficulty of extracting quantitative information in an efficient and standardized manner due to the manual aspect of the analysis (<xref ref-type="bibr" rid="bib24">Huberts et al., 2014</xref>). In theory, expanding the number of individual cell traps and chambers on a microfluidic system makes it possible to test the effect of a large number of genetic and/or environmental perturbations on aging. Yet, in practice, this is out of reach since lifespan analyses require manual division counting and frequent corrections in cell segmentation. This problem has largely limited the interest of the ‘arms race’ observed in recent years for the temporal tracking of individual cells during aging. This has also made it very difficult to cross-validate the results obtained by different laboratories, which is yet essential to advance our understanding of the mechanisms involved in aging.</p><p>Fortunately, the rapid development of powerful deep learning-based image processing methods in biology using convolutional neural networks (CNN) (<xref ref-type="bibr" rid="bib33">Laine et al., 2021</xref>) suggests a way to overcome this important technical barrier. Recently, a study showed the potential of image classification by a CNN or a capsule network to classify the state of dividing yeast cells (i.e. budded, unbudded, etc.) trapped in a microfluidic device (<xref ref-type="bibr" rid="bib17">Ghafari et al., 2021</xref>). However, due to the limited accuracy of the model, it has not demonstrated its ability to perform an automated division counting, let alone determine the RLS of individual cells. This is likely due to the fact that the misclassification of a single frame during the lifespan can dramatically compromise the accuracy of the RLS measurement.</p><p>Here, we report the development of DetecDiv, an integrated platform that combines high-throughput observation of cell divisions using a microfluidic device, a simple benchtop image acquisition system, and a deep learning-based image processing software with several image classification frameworks. Using this methodology, one can accurately track successive cell divisions in an automated manner and reconstruct RLS without human intervention, saving between 2 and 3 orders of magnitude on the analysis time. By combining this pipeline with additional deep-learning models for time-series classification and semantic segmentation, we provide a comprehensive toolset for an in-depth quantification of single-cell trajectories (i.e. division rate, mortality, size, and fluorescence) during entry into senescence and adaptation to environmental stress.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Building an improved microfluidic device and a minimal image acquisition system for replicative lifespan analyses</title><p>The primary scope of our present study was to overcome the current limitations inherent to the analysis of large-scale replicative lifespan assays by taking advantage of deep-learning image processing methods. Yet, we took this opportunity to provide improvements to individual mother cell trapping devices, in order to maximize the robustness of RLS data acquisition. Based on a design similar to that reported in previous studies (<xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Crane et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Liu et al., 2015</xref>), we added small jaws on the top of the trap to better retain the mother cells in the traps (especially the old ones <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1G</xref>). In addition, we reduced the wall thickness of the traps to facilitate their deformation and thus avoid strong mechanical constraints when the cells become too big (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D,G</xref> and supplementary text for details). Finally, we added a microfluidic barrier that filters cells coming from microcolonies located upstream of the trap matrix, which eventually clog the device and thus compromise the experiment after typically 24 h of culture. Altogether, the microfluidic device features 16 independent chambers with 2000 traps each, eliciting multiple conditions and strains to be analyzed in parallel.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>DetecDiv workflow Left: Sketch of the analysis pipeline used to track divisions at the single-cell level.</title><p>Left: A microfluidic device, featuring 16 independent channels with 2000 individual cell traps in each (depicted with a zoom on the trap array (scale bar: 20 µm) and zoom on one trap containing a budding yeast (scale bar: 5 µm)), is imaged using time-lapse microscopy. Middle-left: Typical temporal sequence of brightfield field of views obtained with the setup (scale bar: 60 µm). Regions Of Interest (ROI) representing the traps are automatically detected using XY cross-correlation processing, and the temporal sequence of each ROI (trap) is extracted and saved. Top-right: Sketch of the training and validation pipeline of DetecDiv classifiers. A set of ROIs is picked from one (or several) experiments and annotated to form a groundtruth dataset. It is then split into a training set, used to train the corresponding classifier, and a test set used to validate the trained classifier. Bottom-right: Example of signals extracted from ROIs using DetecDiv classifiers. An image classifier can be used to extract oscillations of classes describing the size of the bud, from dividing cells, and thus the occurrence of new cell cycles (more details in <xref ref-type="fig" rid="fig2">Figure 2</xref>). A sequence classifier can be used to detect changes in cell-cycle frequency, such as a cell-cycle slowdown (<underline>S</underline>enescence <underline>E</underline>ntry <underline>P</underline>oint, SEP; more details in <xref ref-type="fig" rid="fig4">Figure 4</xref>). A pixel classifier can be used to segment the mother cell from other cells, and from the background (more details in <xref ref-type="fig" rid="fig5">Figure 5</xref>). Using these classifiers on the same ROIs allows extracting quantitative metrics from dividing cells, at the single-cell and population level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Experimental setup and microfluidic device.</title><p>(<bold>A</bold>) Schematics of the custom imaging setup built for DetecDiv (see Methods for details). (<bold>B</bold>) Picture of the imaging setup. (<bold>C</bold>) Design of the microfluidic device with 16 independent channels. Each channel has one inlet, a dust filter, and one outlet. (<bold>D</bold>) Left: Schematics of the array of cell traps. Inset represents close-ups on indicated areas. Dimensions are in µm. Right: Brightfield image of a typical field of view and close-up on one trap (scale bar: 60 µm and 5 µm). (<bold>E</bold>) Principle of the cell barrier used to prevent the cells from moving towards the inlet when loading the cells from the outlet, since any cell upstream of the cell array may lead to the formation of colonies hence clog the device over time. (<bold>F</bold>) Principle of the automated dissection of daughter cells: the mother is retained within the trap but their successive daughters are flushed away due to constant medium flow. Daughters may either exit the trap from the top of the bottom opening. (<bold>G</bold>) Unlike previous cell trap geometries (‘classical’), the current design (‘new’) features shallow PDMS walls that can be deformed by large cells, hence ensuring the long-term retention of the cells. Two small claws on each side of the trap entrance further enhance retention. The retention, measured as the number of cells staying inside the trap before their death (or more than 5000 min, i.e. the duration of the experiment), is displayed for both type of traps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Next, we built a custom benchtop microscope (referred to as the ‘RAMM system’ in the following, see methods for details) using simple optical parts to demonstrate that high-throughput division counting and quantitative RLS assays do not require any expensive fully-automated or high-magnification commercial microscopy systems. For this, we used a simple rigid frame with inverted epifluorescence optics, a fixed dual-band GFP/mCherry filter set, a brightfield illumination column, a camera, and a motorized stage, for a total cost of fewer than 40 k euros (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A-B</xref>). Image acquisition, illumination, and stage control were all interfaced using the open-source Micromanager software (<xref ref-type="bibr" rid="bib14">Edelstein et al., 2014</xref>). Using a ×20 magnification objective, this ‘minimal’ microscope allowed us to follow the successive divisions and the entry into senescence of typically 30,000 individual cells in parallel with a 5 min resolution (knowing that there are ~500 traps per field of view using the ×20 objective).</p></sec><sec id="s2-2"><title>An image sequence classification model for automated division counting and lifespan reconstruction</title><p>This image acquisition system generates a large amount of cell division data (on the Terabytes scale depending on the number of channels, frames, and fields of view), only a tiny part of which can be manually curated in a reasonable time. In particular, the determination of replicative lifespans requires counting successive cell divisions until death, hence, reviewing all images acquired for each cell in each field of view over time. In addition, automating the division counting process is complicated by the heterogeneity in cell fate (i.e. cell-cycle durations and cell shape), especially during the entry into senescence.</p><p>To overcome this limitation, we have developed an image classification pipeline to count successive generations and reconstruct the entire lifespan of individual cells dividing in the traps (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). For this, we have trained a convolutional neural network (CNN) based on the ‘Inception v1’ architecture (<xref ref-type="bibr" rid="bib53">Szegedy et al., 2015</xref>) to predict the budding state of the trapped cells by assigning one of six possible classes (unbudded, small-budded, large-budded, dead, empty trap, and clogged trap) to each frame (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, Top). In this framework, the alternation between the 'large budded' or ‘unbudded’ and the ’small budded' states reveals bud emergences. The cell cycle durations can be deduced by measuring the time interval between successive budding events, and the occurrence of the ‘dead’ class determines the end of the cell’s lifespan (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, Bottom). We selected this classification scheme - namely, the prediction of the budding state of the cell - over the direct assessment of cell division or budding (e.g. ‘division’ versus ‘no division’) because division and budding events can only be assessed by comparing successive frames, which is impossible using a classical CNN architecture dedicated to image classification, which takes a single frame as input. To train and evaluate the performance of the classifier, we generated a manually annotated dataset (referred to as ‘groundtruth’ in the following) by arbitrarily selecting 250 traps (split into a training and a test set, see Methods) containing situations representative of all cellular states from different fields of view and independent experimental replicates.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>DetecDiv cell-cycle duration predictions and RLS reconstruction pipeline.</title><p>(<bold>A</bold>) Principles of the DetecDiv division tracking and lifespan reconstruction pipeline; Brightfield images are processed by a convolutional neural network (CNN) to extract representative image features. The sequence of image features is then processed by a long short-term memory network (LSTM) that assigns one of the 6 predefined classes (‘unbud’, ‘small’, ‘large’, ‘dead’, ‘clog’, ‘empty’), taking into account the time dependencies. Temporal oscillations between ‘large’ and ‘small’ or ‘large’ and ‘unbudded’ indicate the beginning of a new generation (i.e. cell-cycle). The appearance of the ‘dead’ class marks the end of the lifespan. For scale reference, each image is 19.5µm wide. (<bold>B</bold>) Comparison of the different methods used for six sample cells. The gray bars represent the groundtruth data made from manually annotated image sequences. Colored lines indicate the corresponding predictions made by CNN+LSTM (orange), the CNN+post-processing (magenta), and the CNN (blue) networks (see Methods and supplementary text for details). The red segments indicate the position of new generation events. (<bold>C</bold>) Left: histogram of cell-cycle durations representing groundtruth data and predictions using different processing pipelines. The p-value indicates the results of a rank-sum test comparing the predictions to the groundtruth for the different pipeline variants. The total number of generations annotated in the groundtruth or detected by the networks is indicated in the legend. Right: Scatter plot in log scale representing the correlation between groundtruth-calculated cell-cycle durations and those predicted by the CNN+LSTM network. R<sup>2</sup> represents the coefficient of correlation between the two datasets. Precision and recall are defined in the Methods section. (<bold>D</bold>) Left: cumulative distribution showing the survival of cells as a function of the number of generations (N=50 cells). The numbers in the legend indicate the median replicative lifespans. The p-value indicates the results from a statistical log-rank test. Right: Scatter plot representing the correlation of the replicative lifespans of 50 individual cells obtained from the groundtruth with that predicted by the CNN+LSTM architecture. Inset: same as the main plot, but for the CNN and CNN+Post-Processing pipelines. R<sup>2</sup> indicates the coefficient of correlation between the two datasets. (<bold>E</bold>) Replicative lifespans obtained using the CNN+LSTM network for longevity mutants (solid colored lines, genotype indicated). The shading represents the 95% confidence interval calculated using the Greenwood method (<xref ref-type="bibr" rid="bib46">Pokhrel et al., 2008</xref>). The median RLS and the number of cells analyzed are indicated in the legend. The dashed lines with shading represent the hazard rate (i.e. the instantaneous rate of cell mortality in the population of cells at a given replicative age) and its standard deviation estimated with a bootstrap test (N=100). Results from log-rank tests (comparing WT and mutant distributions) are indicated on the left of the legend. (<bold>F</bold>) Same as E but for WT cells grown in 2% glucose or 2% galactose (colored lines). Inset: Same as C - Left but with the same conditions as the main panel.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Parameter values used for training the CNN+LSTM classifier.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-79519-fig2-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Principles of division tracking and lifespan reconstruction using a CNN-based image classification.</title><p>(<bold>A</bold>) In this framework, the sequence of images is processed by a GoogleNet CNN that processes each image separately. The CNN extracts image features that are used to assign a label to each image among six possible classes (see supplementary methods for details). As with the CNN+LSTM architecture described in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the sequence of labels is used to assign new generation events and the occurrence of cell death. For scale reference, each image is 19.5µm wide. (<bold>B</bold>) Typical sequence of label (Top) and the associated generations extracted from it (Bottom), from the image sequence of a ROI. The groundtruth is depicted in black and gray and the output of the CNN-based image classification is shown in purple. The yellow arrows indicate prediction errors. A large-to-small error leads to a false positive new generation, while a false death leads to a precocious shortening the sequence of generations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Image classification benchmarks obtained with the CNN and the CNN+LSTM architecture.</title><p>(<bold>A</bold>) Confusion matrix obtained with a test dataset (50 trapped cells followed over 1000 frames) using the CNN image classifier. Each number in the matrix represents the number of detected events. (<bold>B</bold>) Same as A, but for the combined CNN+LSTM architecture (<bold>C</bold>) Bar plot showing the recall, precision, and F<sub>1</sub>-score metrics obtained on each class for the CNN image classifier on the test dataset. (<bold>D</bold>) Same as C, but for the combined CNN+LSTM architecture.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Example of image classification correctly labeling the state of the mother cell, despite the presence of surrounding cells with potentially different states.</title><p>Top: Four images of a mother cell in contact with another cell, annotated (groundtruth) as ‘Small’, and classified as such. The presence of this cell does not affect the classification of the budding state of the mother cell. The green arrows indicate the actual small bud from the mother cell. Bottom: Four images of a mother cell in contact with another cell, annotated (groundtruth) as ‘Large’, and classified as such. The presence of this cell does not affect the classification of the budding state of the mother cell. The red arrows indicate a small bud from the neighbor cell that could have misled the classifier. For scale reference, each image is 19.5µm wide.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Image classification benchmarks, cell-cycle duration and RLS prediction using different CNNs.</title><p>(<bold>A</bold>) Confusion matrix obtained with a test dataset (50 trapped cells followed over 1000 frames) using a InceptionV3 CNN combined with an LSTM. Each number in the matrix represents the number of detected events. (<bold>B</bold>) Same as A, but using an InceptionResnetV2 combined with an LSTM. (<bold>C</bold>) Bar plot showing the recall, precision, and F<sub>1</sub>-score metrics obtained on each class using a InceptionV3 CNN combined with an LSTM on the test dataset. (<bold>D</bold>) Same as C, but using an InceptionResnetV2 combined with an LSTM. (<bold>E</bold>) Scatter plot representing the correlation of the replicative lifespans of 50 individual cells obtained from the groundtruth with that predicted using a InceptionV3 CNN combined with an LSTM. (<bold>F</bold>) Same as E, but using an InceptionResnetV2 combined with an LSTM. (<bold>G</bold>) Scatter plot in log-scale representing the correlation between groundtruth-calculated cell-cycle durations and those predicted using a InceptionV3 CNN combined with an LSTM. R<sup>2</sup> represents the coefficient of correlation between the two datasets. Precision and recall are defined in the Methods section. (<bold>H</bold>) Same as G, but using an InceptionResnetV2 combined with an LSTM (<bold>I</bold>) Cumulative distribution showing the survival of cells as a function of the number of generations (N=50 cells) as determined manually or as predicted by different CNNs combined with an LSTM. The numbers in the legend indicate the median replicative lifespans. The p-value indicates the results from a statistical log-rank test. (<bold>J</bold>) Average classification time of an image sequence from a ROI (1000 frames) using three different CNNs combined with an LSTM. N=1000 image sequences. (<bold>K</bold>) Histogram of cell-cycle durations representing groundtruth data and predictions by different CNNs combined with an LSTM. The p-value indicates the results of a rank-sum test comparing the predictions to the groundtruth for the different networks. The total number of generations annotated in the groundtruth or detected by the networks is indicated in the legend.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Validation of RLS and cell-cycle durations predictions, for mutants and galactose conditions.</title><p>(<bold>A</bold>) Scatter plot representing the correlation of the replicative lifespans of individual cells from <italic>sir2Δ</italic> or <italic>fob1Δ</italic> longevity mutants obtained from the groundtruth with that predicted by the CNN+LSTM architecture (N=35). (<bold>B</bold>) Histogram of cell-cycle durations representing groundtruth data and predictions, for cells growing in 2% galacose media. The p-value indicates the results of a rank-sum test comparing the predictions to the groundtruth. The total number of generations annotated in the groundtruth or detected by the networks is indicated in the legend. (<bold>C</bold>) Scatter plot in log scale representing the correlation between groundtruth-calculated cell-cycle durations and those predicted by the CNN+LSTM network, for cells growing in 2% galacose media. R<sup>2</sup> represents the coefficient of correlation between the two datasets. Precision and recall are defined in the supplementary text. (<bold>D</bold>) Cumulative distribution showing the survival of cells growing in 2% galacose media as a function of the number of generations (N=35 cells). The numbers in the legend indicate the median replicative lifespans. The p-value indicates the results from a statistical log-rank test. (<bold>E</bold>) Scatter plot representing the correlation of the replicative lifespans of individual cells growing in 2% galacose media, obtained from the groundtruth with that predicted by the CNN+LSTM architecture (N=35). R<sup>2</sup> indicates the coefficient of correlation between the two datasets.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig2-figsupp5-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79519-fig2-video1.mp4" id="fig2video1"><label>Figure 2—video 1.</label><caption><title>Comparison of the groundtruth with the CNN+LSTM classifier predictions for the cellular state.</title><p>The left column represents the class predictions made by the CNN+LSTM classifier, while the right column represents the groundtruth (determined by manual annotation). The two numbers represent the number of buds generated by the cells according to the classifier predictions and manual annotation, respectively. For scale reference, each image is 19.5µm wide.</p></caption></media></fig-group><p>Benchmarking the classifier consisted of three steps: first, we computed the confusion matrices (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>) as well as the classical metrics of precision (i.e. the fraction of correct predictions among all predictions for each class), recall (i.e. the fraction of detected observations among all observations for each class), and F<sub>1</sub>-score (i.e. the harmonic mean of precision and recall). The F<sub>1</sub>-score was found to be higher than 85% for all classes (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref>). Next, the predictions of budding events were compared to the manually annotated data. Despite a good visual match between the groundtruth and the CNN predictions, the distribution of cell-cycle durations revealed that the model tends to predict ‘ghost’ divisions of abnormally short duration (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In addition, sporadic misclassification could falsely assign a cell to the ‘dead’ state, thus decreasing the number of total generations predicted based on the test dataset (N=1127 for the groundtruth versus N=804 for the CNN model, see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). Last, by comparing the lifespan predictions to the corresponding groundtruth data, we observed a striking underestimate of the overall survival (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), due to the sporadic misassignments of the ‘dead’ class (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>).</p><p>These problems could be partially alleviated by post-processing the predictions made by the CNN (see ‘CNN+PP’ in <xref ref-type="fig" rid="fig2">Figure 2B–D</xref> and supplementary text for details). Indeed, by ignoring isolated frames with a ‘dead’ class, we could greatly reduce the number of cases with premature cell death prediction, yet we failed to efficiently remove ghost divisions, hence leading to an overestimate of the RLS and a large number of short cell-cycles (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>).</p><p>An inherent limitation to this approach is that images are individually processed without taking the temporal context into account. Although a more complex post-processing routine could be designed to improve the robustness of the predictions, it would come at the expense of adding more ad hoc parameters, hence decreasing the generality of the method. Therefore, to circumvent this problem, we decided to combine the CNN image classification with a long short-term memory network (LSTM) (<xref ref-type="bibr" rid="bib57">Venugopalan et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Hochreiter and Schmidhuber, 1997</xref>), to take into account the time-dependencies between images (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, Middle). In this framework, the CNN was first trained on the individual images taken from the training set similarly as above. Then, the CNN network activations computed from the temporal sequences of images were used as inputs to train an LSTM network (see supplementary text for details). Following this training procedure, the assembled CNN+LSTM network was then benchmarked similarly as described above. We obtained only a marginal increase in the classification metrics compared to the CNN network (about 90–95% precision and recall for all classes, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A-B</xref>). Yet, strikingly, the quantification of cell-cycle durations and cellular lifespan both revealed considerable improvements in the accuracy: ‘ghost’ divisions were drastically reduced, and the distribution of cell-cycle duration was indistinguishable from that of the groundtruth (p=0.45, <xref ref-type="fig" rid="fig2">Figure 2C</xref>), and the difference between the number of generations predicted by the network and the actual number was less than 2% (N=1147 and N=1127, respectively, see left panel on <xref ref-type="fig" rid="fig2">Figure 2C</xref>). In addition, the Pearson correlation coefficient for groundtruth vs prediction was very high (R²=0.996, see right panel on <xref ref-type="fig" rid="fig2">Figure 2C</xref>). This indicates that mild classification errors may be buffered and hence do not affect the accuracy in the measurements of cell-cycle durations. Moreover, it suggests that the network was robust enough to ignore the budding status of the daughter cells surrounding the mother cell of interest (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Similarly, the predicted survival curve was almost identical to that computed from the groundtruth (p=0.74, <xref ref-type="fig" rid="fig2">Figure 2D</xref> and <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>) and the corresponding Pearson correlation reached 0.991 (vs 0.8 and 0.1 for the CNN+PP and CNN, respectively). Last, in order to determine if the performances of the classifier could be further improved using a more complex CNN, we did a similar analysis using the inception v3 (<xref ref-type="bibr" rid="bib54">Szegedy et al., 2016</xref>) and the inception-resnet v2 (<xref ref-type="bibr" rid="bib55">Szegedy et al., 2017</xref>) networks. We did not observe any increase in classification accuracy (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), while the classification times increased with the CNN complexity (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4J</xref>).</p><p>Altogether, these benchmarks indicated that only the combined CNN+LSTM architecture provided the necessary robustness to provide an accurate prediction of individual cellular lifespan based on image sequence classification.</p><p>Following its validation, we deployed this model to classify all the ROIs from several fields of view extracted from three independent experiments. We were thus able to obtain a survival curve with N=1880 lifespans in a remarkably short time (<xref ref-type="fig" rid="fig2">Figure 2E</xref>): less than 3.5 s were necessary to classify the 1000 images in one lifespan using 8 Tesla K80 GPUs (see Methods for details). This is to be compared with manual annotation of images which takes 5–10 min per cell lifespan depending on the number of generations (i.e. computing is ~100 times faster than manual annotation). Conversely, it would have taken a human being between 7 and 14 days, working 24 h a day, to manually annotate 2000 cells (vs. 2 h for the computer). To further apply the classification model trained on images of wild-type (WT) cells, we measured the large-scale RLS in two classical longevity mutants. Remarkably, we recapitulated the increase (resp. decrease) in longevity observed in the <italic>fob1Δ</italic> (resp. <italic>sir2Δ</italic>) mutant (<xref ref-type="bibr" rid="bib10">Defossez et al., 1999</xref>; <xref ref-type="bibr" rid="bib37">Lin et al., 2000</xref>) and we could compute the related death rate with a high-confidence interval thanks to this large-scale dataset (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Model predictions were further evaluated by comparing the predicted replicative lifespans to manually generated test sets for each of the mutants (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5A</xref>). In addition, using glucose and galactose as carbon sources, we performed comparative measurements of cell-cycle durations (N=38,205 events and N=15,495 events for glucose and galactose, see <xref ref-type="fig" rid="fig2">Figure 2F</xref> inset) and RLS (median = 26 generations, N=1174 events and median=24, N=565 events, for glucose and galactose, respectively, <xref ref-type="fig" rid="fig2">Figure 2F</xref>). Our results were in line with previous measurements (<xref ref-type="bibr" rid="bib39">Liu et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">Frenk et al., 2017</xref>). To further check the performance of the model, we used an additional manually generated test set obtained with cells growing in galactose to compare to the corresponding predictions (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5B-D</xref>). This evaluation demonstrated that the model initially trained with cells growing under glucose conditions could be successfully applied to data obtained with another carbon source, which is known to affect the cell-cycle duration and the general physiology of the cell.</p><p>Altogether, our study shows that our classification pipeline can successfully detect cell divisions, perform lifespan replicative analysis with high throughput, and is robust enough to be employed with different strain backgrounds and under various environmental conditions, even though the training has only been performed on WT data and in glucose conditions.</p></sec><sec id="s2-3"><title>Application of the division counting and lifespan prediction model to different imaging platforms and microfluidic devices</title><p>To further test the robustness of our analysis pipeline, we proceeded to the analysis of several datasets obtained under various imaging conditions. First, we performed experiments with the same microfluidic system but using a commercial microscope with ×60 magnification. After training the classifier on 80 ROIs and testing on 40 independent ROIs, we observed similar results to those obtained with the RAMM system and a ×20 objective (compare the ‘specialist’ columns for the panels in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref>): the classification benchmarks were greater than 90%, the error rate on the number of generations detected was a few percents, and the cell-cycle length distributions were similar between prediction and groundtruth. This first demonstrated that neither the RAMM imaging system nor the ×20 magnification is required to guarantee successful division counting and lifespan reconstruction with our analysis pipeline.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Classification benchmarks and performances of the divison detection of a CNN+LSTM image classifier, trained on time-lapses images from different microfluidic devices and imaging setups.</title><p>A specialist classifier was trained independently for each source, while a generalist classifier was trained on a mixed dataset generated from all the sources. (<bold>A</bold>) Cell trap and imaging setup developed in this study, with a framerate of 1 frame/5 min. (<bold>B</bold>) Cell trap developed in this study imaged with a ×60 objective mounted on a commercial imaging system with a framerate of 1 frame/5 min. (<bold>C</bold>) Cell trap from the Acar lab (<xref ref-type="bibr" rid="bib39">Liu et al., 2015</xref>) imaged with a ×40 objective mounted on a commercial imaging system with a framerate of 1 frame/10 min. (<bold>D</bold>) Cup-shaped trap similar to <xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref>, imaged with a ×60 phase-constrast objective mounted on a commercial imaging system with a framerate of 1 frame/10 min. (E) Cell trap from the Swain lab (<xref ref-type="bibr" rid="bib9">Crane et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Granados et al., 2018</xref>) imaged with a ×60 objective mounted on a commercial imaging system with a framerate of 1 frame/2.5 min. Scale bars: 5µm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Top: Time-lapse images of traps from the Acar lab <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/dXijjd/S7NEB">(Liu et al., 2015)</ext-link> from which the automated division detection can be impaired due to multiple cells in the trap (orange arrows).</title><p>Bottom: Time-lapse images of cup-shaped traps (<xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref>) from which the automated RLS analysis can be impaired due to mother/daughter replacement (red arrows).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig3-figsupp1-v2.tif"/></fig></fig-group><p>In addition, we gathered time-lapse microscopy datasets from several laboratories using microfluidic cell trapping systems with different geometries and various imaging conditions (<xref ref-type="fig" rid="fig3">Figure 3C and E</xref>; <xref ref-type="bibr" rid="bib9">Crane et al., 2014</xref>; <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/dXijjd/tAdkp+S7NEB+BIGnk">Liu et al., 2015;</ext-link> <xref ref-type="bibr" rid="bib19">Granados et al., 2018</xref>). We also included data generated in our lab based on a device similar to that used in <xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref> (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). For each trap geometry, we manually evaluated the retention rate of a mother cell during a lifespan. Indeed, high retention is key to getting a reliable measurement of the RLS (i.e. to ensure that mother cells are not eventually replaced by their daughters). This analysis revealed that a ‘semi-open’ geometry (as in the design shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1G</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) did not prevent large mother cells from being sporadically expelled during the budding of their daughters, unlike other cell trap shapes. Of note, the geometry proposed by <xref ref-type="bibr" rid="bib9">Crane et al., 2014</xref>; <xref ref-type="fig" rid="fig3">Figure 3E</xref> was not tested on an entire lifespan, but only on about even generations, hence leading to an overestimation of the retention rate (it was reported to be below 50% in the original paper).</p><p>For each dataset, we trained a specific classifier (or ‘specialist’) on 80 ROIs and validated it on 40 independent ROIs. The different benchmarks (i.e<italic>.</italic> classification performance and division predictions) showed that each specialist performed very well on each specific test dataset, thus confirming further that our analysis pipeline is robust and applicable to different cell trapping and imaging configurations.</p><p>Last, instead of training the classifiers separately on each dataset, we asked whether a unique classifier would have sufficient capacity to handle the pooled datasets with all imaging conditions and trap geometries used in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Strikingly, this &quot;generalist&quot; model showed comparable performance to the different specialists. This approach thus further highlighted the versatility of our methodology and demonstrated the interest in aggregating data sets to ultimately build a standardized reference model for counting divisions, independently of the specific imaging conditions.</p></sec><sec id="s2-4"><title>Automated quantification of cellular physiological decline upon entry into senescence</title><p>Aging yeast cells have long been reported to undergo a cell-cycle slowdown when approaching senescence (<xref ref-type="bibr" rid="bib42">Mortimer and Johnson, 1959</xref>), a phenomenon that we have since quantified and referred to as the <underline>S</underline>enescence <underline>E</underline>ntry <underline>P</underline>oint or SEP (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>). More recently, we have demonstrated that this quite abrupt physiological decline in the cellular lifespan is concomitant with the accumulation of extrachromosomal rDNA circles (ERCs) (<xref ref-type="bibr" rid="bib41">Morlot et al., 2019</xref>), a long described marker of aging in yeast (<xref ref-type="bibr" rid="bib51">Sinclair and Guarente, 1997</xref>). Therefore, precise identification of the turning point from healthy to pathological state (named pre-SEP and post-SEP in the following, respectively) is essential to capture the dynamics of entry into senescence, and even more so since the large cell-cell variability in cell death makes trajectory alignment from cell birth irrelevant (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Morlot et al., 2019</xref>). Yet, the noise in cell-cycle durations, especially beyond the SEP, can make the determination of this transition error-prone if based on a simple analysis (e.g. thresholding) of the cell-cycle durations. Hence, to achieve a reliable determination of the SEP in an automated manner, we sought to develop an additional classification scheme as follows: we trained a simple LSTM sequence-to-sequence classifier to assign a ‘pre-SEP’ or ‘post-SEP’ label (before or after the SEP, respectively) to each frame, using the sequence of cellular state probabilities (i.e. the output of the CNN+LSTM image classifier described in <xref ref-type="fig" rid="fig2">Figure 2A</xref>) as input (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The groundtruth was generated by visual inspection using a graphical user interface representing the budding status of a given cell over time. Same as above, we used 200 manually annotated ROIs for the training procedure and reserved 47 additional ones that were never ‘seen’ by the network to evaluate the predictions. Comparing the predictions to the groundtruth revealed that we could successfully identify the transition to a slow division mode (R²=0.93, see <xref ref-type="fig" rid="fig4">Figure 4B–C</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Hence, we could recapitulate the rapid increase in the average cell-cycle durations after aligning individual trajectories from that transition (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), as described before (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>). These results show that complementary classifiers can be used to process time series output by other classification models, allowing further exploitation of relevant dynamic information, such as the entry into senescence.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Deep learning-based measurement of the dynamics of entry into senescence.</title><p>(<bold>A</bold>) Sketch depicting the detection of the Senescence Entry Point (SEP). The temporal sequence of classes probabilities (i.e. unbud, small, large, dead) is fed into an LSTM network that predicts the SEP by assigning one of the two predefined classes pre-SEP or post-SEP to each frame. (<bold>B</bold>) Correlogram showing the correlation between the SEP predicted by the LSTM network and the groundtruth data, obtained as previously described (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>). The gray level coded data points indicate the local density of the points using arbitrary units as indicated by the gray level bar. (<bold>C</bold>) Sample trajectories indicating the successive generations of individual cells (red lines) along with the cell-cycle duration (color-coded as indicated). (<bold>D</bold>) Average cell-cycle duration versus generation index after aligning all individual trajectories from the SEP (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>). Each point represents an average over up to 200 cell trajectories. The error bar represents the standard error-on-mean.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Parameter values used for training the SEP detection classifier.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-79519-fig4-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Classification benchmarks for the detection of the onset of Senescence Entry using an LSTM sequence-to-sequence classification.</title><p>(<bold>A</bold>) Confusion matrix obtained with a test dataset (50 time-series based on the cellular state probabilities output by the CNN+LSTM classifier) using a trained LSTM classifier. Each number in the matrix represents the number of detected events. (<bold>B</bold>) Bar plot showing the precision, recall, and F<sub>1</sub>-score metrics obtained on each class for the LSTM classifier on the test dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig4-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Cell contour determination and fluorescence image quantification by semantic segmentation</title><p>Quantifying the dynamics of successive divisions is an indispensable prerequisite for capturing phenomena that span multiple divisions such as replicative aging. However, in order to make the most of the possibilities offered by photonic microscopy, it is necessary to develop complementary cytometry tools. For this purpose, semantic segmentation based on the classification of pixels has seen a growing interest recently to process biomedical images since the pioneering development of the U-Net architecture (<xref ref-type="bibr" rid="bib48">Ronneberger et al., 2015</xref>). U-Net networks feature an encoding network that extracts meaningful image information and a decoding part that reconstructs a segmented image with a user-defined number of classes (e.g. background, cell, etc.). Recently, the original U-NET architecture has been employed for segmentation in yeast (<xref ref-type="bibr" rid="bib12">Dietler et al., 2020</xref>). More generally, more sophisticated versions have been released allowing the segmentation of objects with low contrast and/or in dense environments, such as Stardist (<xref ref-type="bibr" rid="bib50">Schmidt et al., 2018</xref>) and Cellpose (<xref ref-type="bibr" rid="bib52">Stringer et al., 2021</xref>).</p><p>Here, since the complexity of images with individual cell traps is limited, we have used an encoder/decoder network based on the DeepLabV3+ architecture (<xref ref-type="bibr" rid="bib8">Chen et al., 2018</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), to segment brightfield images (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>, and Methods). Briefly, DeepLabV3+ features an encoder/decoder architecture similar to U-Net, but is more versatile by allowing to process images of arbitrary size. In the following, we chose the Resnet50 network (<xref ref-type="bibr" rid="bib20">He et al., 2016</xref>) as the CNN encoder, which we found to outperform the Inception model for this task. We trained the model on ~1,400 manually segmented brightfield images using three output classes (i.e. ‘background,’ ‘mother cell,’ ‘other cell’) in order to discriminate the mother cell of interest from the surrounding cellular objects. We used a separate test dataset containing ~500 labeled images to evaluate the performance of the classifier (see Methods for details about the generation of the groundtruth data sets). Our results revealed that mother cell contours could be determined accurately with a trained classifier (<xref ref-type="fig" rid="fig5">Figure 5A–C</xref> and <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2A-D</xref>). In addition, we used a cross-validation procedure based on random partitioning of training and test datasets that highlighted the robustness of the classification (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2E</xref>). Overall, this segmentation procedure allowed us to quantify the dynamics of volume increase of the mother cell during replicative aging (<xref ref-type="fig" rid="fig5">Figure 5C–D</xref>), as previously reported (<xref ref-type="bibr" rid="bib41">Morlot et al., 2019</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Deep learning-based semantic segmentation of cells and nuclei.</title><p>(<bold>A</bold>) Principles of semantic cell contours segmentation based on brightfield images; Top and middle row: Individual brightfield images were processed by the DeeplabV3+ network that was trained to perform pixel classification using three predefined classes representing the background (black), the mother cell of interest (orange), or any other cell in the image (blue). Bottom row: overlay of brightfield images with segmented cellular contours. For scale reference, each image is 19.5µm wide. (<bold>B</bold>) Correlogram showing the correlation between individual cell area predicted by the segmentation pipeline and the groundtruth data, obtained by manual annotation of the images. The color code indicates the local density of the points using arbitrary units. (<bold>C</bold>) Sample trajectories indicating the successive generations of individual cells (red lines) along with the cell surface area (color-coded as indicated). (<bold>D</bold>) Average mother cell surface area versus generation index after aligning all individual trajectories from the SEP (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>). Each point represents an average of up to 200 cell trajectories. The error bar represents the standard error-on-mean. (<bold>E</bold>) Principles of semantic cell nuclei segmentation based on fluorescent images of cells expressing a histone-Neongreen fusion. The semantic segmentation network was trained to classify pixels between two predefined classes (‘background’ in black, ‘nucleus’ in green). For scale reference, each image is 19.5µm wide. (<bold>F</bold>) Same as B but for nuclear surface area. (<bold>G</bold>) Same as C but for total nuclear fluorescence (<bold>H</bold>) Same as in D but for total nuclear fluorescence.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Parameter values used for training the classifier dedicated to cell segmentation.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-79519-fig5-data1-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Parameter values used for training the classifier dedicated to nucleus segmentation.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-79519-fig5-data2-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Principles of the pipeline used for semantic segmentation with DeepLabV3+.</title><p>Brightfield or fluorescence images are separately processed by the DeepLabV3+ encoder/decoder network (<xref ref-type="bibr" rid="bib8">Chen et al., 2018</xref>) that has been modified to classify image pixels according to user-defined classes (mother/other/background and nucleus/background for brightfield and fluorescence images, respectively). For scale reference, each image is 19.5µm wide. A weighted classification layer is used to deal with class imbalance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Benchmarks for the semantic segmentation of brightfield images.</title><p>(<bold>A</bold>) Precision/Recall tradeoff plot obtained by varying the output prediction threshold for the class “mother” using a test dataset that contains 50 image sequences with 1000 frames. The yellow dot indicates the point that maximizes the F<sub>1</sub>-score. (<bold>B</bold>) Evolution of F<sub>1</sub>-score as a function of the output prediction threshold computed on the [0.2 : 0.95] interval. A threshold value of 0.9 maximizes the F<sub>1</sub>-score (90%). (<bold>C</bold>) Confusion matrix obtained with the test dataset using a 0.9 prediction threshold. (<bold>D</bold>) Bar plot showing the recall, precision, and F<sub>1</sub>-score metrics obtained on each class for the pixel classifier on the test dataset. (<bold>E</bold>) Cross-validation of the classification model used for semantic segmentation; Class-averaged recall, precision, and F<sub>1</sub>-score plotted as a function of the index of the draws performed, as indicated on the legend (200 ROIs and 50 ROIs are randomly selected for training and testing upon each draw, respectively).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Benchmarks for the semantic segmentation of fluorescence images.</title><p>(<bold>A</bold>) Precision/Recall tradeoff plot obtained by varying the output prediction threshold for the class “nucleus” using a test dataset that contains 25 image sequences with 1000 frames. The orange dot indicates the point that maximizes the F<sub>1</sub>-score. (<bold>B</bold>) Evolution of F<sub>1</sub>-score as a function of the output prediction threshold computed on the [0.2 : 0.95] interval. A threshold value of 0.35 maximizes the F<sub>1</sub>-score (91%). (<bold>C</bold>) Confusion matrix obtained with the test dataset using a 0.35 prediction threshold. (<bold>D</bold>) Bar plot showing the recall, precision and F<sub>1</sub>-score metrics obtained on each class for the pixel classifier on the test dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig5-figsupp3-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79519-fig5-video1.mp4" id="fig5video1"><label>Figure 5—video 1.</label><caption><title>Sample movies of individual cells following cellular state classification, cell, and nuclear contour segmentation.</title><p>The left column represents the cellular state according to the prediction made by the CNN+LSTM classifier. The middle column shows the brightfield image along with mother cell contours obtained by a semantic segmentation classifier. The right column displays the Htb2-NeonGreen fluorescence channel, along with cells contours and nuclear contours obtained by a semantic segmentation classifier. For scale reference, each image is 19.5µm wide.</p></caption></media></fig-group><p>Last, a similar training procedure with ~3000 fluorescence images with a nuclear marker (using a strain carrying a histone-Neongreen fusion) yielded accurate nuclei contours (<xref ref-type="fig" rid="fig5">Figure 5E–F</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). It successfully recapitulated the sharp burst in nuclear fluorescence that follows the Senescence Entry Point (<xref ref-type="fig" rid="fig5">Figure 5G–H</xref>; <xref ref-type="bibr" rid="bib41">Morlot et al., 2019</xref>).</p></sec><sec id="s2-6"><title>Automated quantitative measurements of the physiological adaptation to hydrogen peroxide</title><p>Beyond replicative longevity analyses, we wondered if this automated pipeline could be applied to other biological contexts, in which cell proliferation and cell death need to be accurately quantified over time. Hence, we sought to measure the dynamics of the physiological adaptation of yeast cells subjected to hydrogen peroxide stress.</p><p>For this purpose, young cells were abruptly exposed to different stress concentrations, ranging from 0 to 0.8 mM H<sub>2</sub>O<sub>2</sub>, and observed over about 15 h (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We used a strain carrying the Tsa1-GFP fusion protein (<italic>TSA1</italic> encodes a peroxiredoxin, a major cytosolic antioxidant overexpressed in response to oxidative stress) as a fluorescent reporter of the cellular response to this stress (<xref ref-type="bibr" rid="bib18">Goulev et al., 2017</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Automated analysis of the stress response to H<sub>2</sub>O<sub>2</sub> using DetecDiv.</title><p>(<bold>A</bold>) Successive brightfield and Tsa1-GFP images of three representative cells submitted to 0.3 mM of H<sub>2</sub>O<sub>2</sub> and corresponding to a different fate. The orange contour of the cell is determined using the segmentation described in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and the total GFP fluorescence inside it is depicted as a function of time, where red bars indicates a new generation and the purple dotted bar indicated the onset of H<sub>2</sub>O<sub>2</sub>. For scale reference, each image is 19.5µm wide. (<bold>B</bold>) Scatter plot of automatically detected cell-cycle durations versus time of 500 cells submitted to different doses of H<sub>2</sub>O<sub>2</sub>. The purple area indicates the presence of the indicated dose of H<sub>2</sub>O<sub>2</sub>. (<bold>C</bold>) Fraction of dead cells versus time as automatically detected by the CNN+LSTM classifier, under different H<sub>2</sub>O<sub>2</sub> doses. The purple area indicates the presence of the indicated dose of H<sub>2</sub>O<sub>2</sub>. N=500. (<bold>D</bold>) Mean Tsa1-GFP fluorescence from cells submitted to different doses of H<sub>2</sub>O<sub>2</sub>. The purple area indicates the presence of the indicated dose of H<sub>2</sub>O<sub>2</sub>. N=500.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Validation of predicted time of death and death status at the end of the experiment.</title><p>(<bold>A</bold>) Scatter plot representing the correlation of the time of death of 35 individual cells exposed to a 0.05 mM H<sub>2</sub>O<sub>2</sub> stress, between the groundtruth and the prediction by the CNN+LSTM architecture. R<sup>2</sup> indicates the coefficient of correlation between the two datasets. (<bold>B</bold>) Confusion matrix between the survival status (‘Dead’ or ‘Alive’) of cells at the end of the experiment between the groundtruth and the prediction by the CNN+LSTM. Each number in the matrix represents the number of cells.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-fig6-figsupp1-v2.tif"/></fig></fig-group><p>In this context, we first sought to characterize the dynamics of the cell cycle by using the classifier reported in <xref ref-type="fig" rid="fig2">Figure 2</xref> - without doing any retraining - to detect divisions during the experiment (using N=250 ROIs). Our automated analysis revealed different possible cell fates, whose proportions varied according to the stress concentration (<xref ref-type="fig" rid="fig6">Figure 6A and B</xref>): in the absence of stress (0 mM), cells maintained a constant division rate throughout the experiment; in contrast, at 0.3 mM, the population partitioned between cells that recovered a normal division rate after experiencing a delay (see the ‘adapted cells’ in <xref ref-type="fig" rid="fig6">Figure 6B</xref>) and others that seemed unable to resume a normal cell-cycle (see the ‘slowly dividing cells’ in <xref ref-type="fig" rid="fig6">Figure 6B</xref>), in agreement with previous results (<xref ref-type="bibr" rid="bib18">Goulev et al., 2017</xref>).</p><p>Higher doses of stress (0.5 mM and 0.8 mM) saw these populations gradually disappear, indicating that very few divisions occur at these elevated doses. To check this further, we exploited further the outputs of the classifier to score the onset of cell death for each trapped cell (<xref ref-type="fig" rid="fig6">Figure 6A and C</xref>). Our analysis revealed a progressive, dose-dependent increase in the fraction of dead cells over time, which was confirmed by a comparison between groundtruth data and network predictions of the time of cell death (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). These results thus demonstrated the possibility to perform real-time and quantitative measurement of the cell death rate in response to an environmental insult, which is rarely precisely done due to the difficulty of precisely scoring dead cells in a population of cells without any additional viability marker.</p><p>Finally, we used our semantic segmentation model (reported in <xref ref-type="fig" rid="fig5">Figure 5</xref>) to quantify cytoplasmic fluorescence over time in the stress response experiment (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The population-averaged mean cytoplasmic fluorescence revealed a significant increase at 0.3 mM H<sub>2</sub>O<sub>2</sub> due to the transcriptional upregulation of antioxidant genes, as previously described (<xref ref-type="bibr" rid="bib18">Goulev et al., 2017</xref>). However, this average upregulation of Tsa1 was lessened at higher doses, an effect we attribute to the large fraction of cell death observed in these conditions (e.g.: bottom cell in <xref ref-type="fig" rid="fig6">Figure 6A</xref>). Altogether, these results indicate that DetecDiv used with single-cell traps provides a highly suitable method for quantifying both cell division rate and mortality in real-time under variable environmental conditions.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we have developed a pipeline based on the combined use of two architectures, namely a CNN+LSTM network for the exploitation of temporal information and semantic segmentation (DeepLabV3+) for the quantification of spatial information. We demonstrate that it can successfully characterize the dynamics of multi-generational phenomena, using the example of the entry into replicative senescence in yeast, a difficult case study that has long resisted any automated analysis. We also successfully used our classification model to score cellular adaptation and mortality in the context of the physiological stress response to hydrogen peroxide. Furthermore, we have developed a graphical user interface to make this method accessible for the community without requiring any programming knowledge. We envision that this methodology will unleash the potential of microfluidic cell trapping devices to quantify temporal physiological metrics in a high-throughput and single-cell manner.</p><p>The major novelty of this work lies in the development of an analysis method to automatically obtain survival curves and cytometric measurements during the entry into senescence from raw image sequences. Nevertheless, we also focused our efforts on improving traps to increase the efficiency of RLS assays in microfluidics. Also, we have built a minimal optical system (yet with a motorized stage) assembled from simple optical components (i.e. no filter wheel, fixed objective), for a price of about one-third that of a commercial automated microscope, which can be made accessible to a larger community of researchers. Although many laboratories use common imaging platforms with shared equipment, it is important to note that the cost of an hour of microscopy is around 10–20 euros in an imaging facility. As an RLS assay typically lasts 60–80 hours, these experiments may not be affordable. Developing a simple system can therefore quickly pay off if the lab does not have its own microscope.</p><p>Using this experimental setup, we showed that our analysis pipeline works perfectly even with a low optical resolution (i.e. the theoretical resolution of our imaging system with a ×20, N.A. 0.45 objective is ~0.7 µm), and without any contrast-enhancing method. In practice, it might be desirable for some applications to use higher magnification to better preserve spatial information and analyze the detailed localization of fluorescent markers. Yet, using the same microfluidic device described here, we showed that DetecDiv works similarly with higher magnification objectives and different imaging systems. In addition, we demonstrated that division detection can also be performed with cells growing in traps with different geometries (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1G</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Furthermore, a unique classifier trained on a large collection of images obtained under broad imaging contexts can also achieve accurate division detection. This may be instrumental to standardize the quantitative analysis of replicative lifespan data in the yeast aging community.</p><p>However, one limitation to applying our analysis pipeline with a broad range of trap geometries is that the accuracy of RLS measurements may be affected when using designs with a low retention rate. Although lifespan trajectories can be marked as ‘censored’ when the mother cell leaves the traps (as proposed in a recently released preprint <xref ref-type="bibr" rid="bib56">Thayer et al., 2022</xref>), our method is currently unable to systematically detect when a mother cell is replaced by its daughter (e.g. cell traps in <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Therefore, we believe that retention is an essential feature to consider when designing the geometry of a trap.</p><p>An important advantage of individual cell trapping is that it makes image analysis much simpler than using chambers filled with two-dimensional cell microcolonies. Indeed, individual traps behave as a ‘hardware-based cell tracking’ method, thus alleviating the need to identify and track objects spatially, a procedure that provides an additional source of errors. Because the cells of interest are located in the middle of the traps, the learning process can focus the attention of the classifier on the state of the mother cell only (e.g. small-budded, large-budded, etc.), hence the specific state of the few cells surrounding it may not influence the reliability of the classification of the mother (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for specific examples). In addition, a powerful feature of whole image classification is that it can easily be coupled to a recurrent neural network (such as an LSTM network), thus opening the door to more accurate analyses that exploit temporal dependencies between images, as demonstrated in our study.</p><p>Beyond the tracking of successive divisions, complementary methods are necessary to characterize the evolution of cell physiology over time. In our study, we used semantic segmentation to delineate the contours of cell bodies over time. Same as above, the ability to discriminate the mother cell of interest from the surrounding cells results is facilitated by the conserved position of the mother at the center of the trap. However, a limitation of our classification scheme is that the buds that arise from the mother cell can not be identified, and further work is necessary to assess the requirements (e.g. the size of the training set) to achieve their successful segmentation, such as using a separate ‘bud’ class. Thus, it is currently impossible to measure the growth rate (in volume) of the mother cell over time (most of the biomass created during the budded period goes into the bud) and it precludes analyzing fluorescent markers that would localize into the bud. Future work may explore how the use of additional segmentation classes or the use of tracking methods could complement our pipeline to alleviate this limitation, as recently shown (<xref ref-type="bibr" rid="bib45">Pietsch et al., 2022</xref>). Alternatively, the development of an instance segmentation method (<xref ref-type="bibr" rid="bib21">He et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Prangemeier et al., 2022</xref>) could also facilitate the identification and separation of different cell bodies in the image.</p><p>Unlike classical image analysis methods, which require complex parameterization and are highly dependent on the problem being addressed, the well-known advantage of machine learning is the versatility of the models, which can be used for a variety of tasks. Here, we show that our division counting/lifespan reconstruction classifier can readily be used to quantify cellular dynamics and mortality in response to hydrogen peroxide stress. We envision that DetecDiv could be further applied in different contexts without additional development - yet with potential retraining of the classifier with complementary data, and/or following the definition of new classes. For example, it could be useful to develop a classifier able to identify different cell fates during aging based on image sequences (e.g. <italic>petite</italic> cells (<xref ref-type="bibr" rid="bib15">Fehrmann et al., 2013</xref>), or mode 1 versus mode 2 aging trajectories <xref ref-type="bibr" rid="bib29">Jin et al., 2019</xref>), as well as during induced (<xref ref-type="bibr" rid="bib4">Bagamery et al., 2020</xref>) or undergone (<xref ref-type="bibr" rid="bib26">Jacquel et al., 2021</xref>) metabolic changes. More generally, the rationalization of division rate measurements in a system where there is no competition between cells offers a unique framework to study the heterogeneity of cell behaviors in response to environmental changes (stress, chemical drugs, etc.), as demonstrated in our study and evidenced by the rise of high-throughput quantitative studies in bacteria (<xref ref-type="bibr" rid="bib5">Bakshi et al., 2021</xref>). Mechanistic studies of the cell-cycle could also benefit from a precise and standardized phenotypic characterization of the division dynamics. Along this line, beyond the classification models described in this study, we have integrated additional frameworks, such as image and image sequence regressions (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for details), which could be useful to score fluorescent markers quantitatively and over time (e.g. mitotic spindle length inference, scoring of the mitochondrial network shape, etc.). We envision that the kind of approach described here may be easily transferred to other cellular models to characterize heterogeneous and complex temporal patterns in biological signals.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Strains</title><p>All strains used in this study are congenic to S288C (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for the list of strains). See the next section for detailed protocols for cell culture.</p></sec><sec id="s4-2"><title>Cell culture</title><p>For each experiment, freshly thawed cells were grown overnight, diluted in the morning, and allowed to perform several divisions (~5 hours at 30 °C) before injection into the microfluidic device. Yeast extract Peptone Dextrose (YPD) medium was used throughout the experiments.</p></sec><sec id="s4-3"><title>Microfabrication</title><p>The designs were created on AutoCAD to produce chrome photomasks (jd-photodata, UK). The microfluidic master molds were then made by standard photolithography processes. The designs were created on AutoCAD (see <ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/DetecDiv_Data">https://github.com/TAspert/DetecDiv_Data</ext-link>; <xref ref-type="bibr" rid="bib2">Aspert, 2021</xref> to download the design) to produce chrome photomasks (jd-photodata, UK). Then, the microfluidic master molds were made using two rounds of classical photolithography steps.</p><p>The array of 2000 traps was created from a 5.25 µm deposit by spinning (WS650 spin coater, Laurell, France) 3 mL of SU8-2005 negative photoresist at 2500 rpm for 30 s on a 3” wafer (Neyco, FRANCE). Then, a soft bake of 3 min at 95 °C on heating plates (VWR) was performed, followed by exposure to 365 nm UVs at 120 mJ/cm² with a mask aligner (UV-KUB3, Kloé, FRANCE). Finally, a post-exposure bake identical to the soft bake was performed before development using SU-8 developer (Microchem, USA).</p><p>The second layer with channel motifs was made of a 30 µm deposit of SU8-2025, by spinning it at 2500 rpm for 30 s. Subsequently, a soft bake of 3 min at 65 °C and 6 min at 95 °C was performed. The wafer was then aligned with the mask containing the motif of the second layer before a 120mJ/cm² exposure. A post-exposure bake similar to the soft bake was then performed.</p><p>After each layer, we performed a hard bake at 150 °C for 15 min to anneal potential cracks and stabilize the photoresist. Finally, the master molds were treated with chlorotrimethylsilane to passivate the surface.</p></sec><sec id="s4-4"><title>Microfluidics chip design, fabrication, and handling</title><p>The microfluidic device is composed of an array of 2048 microstructures able to trap a mother cell while removing successive daughter cells, similarly to previously designed (<xref ref-type="bibr" rid="bib49">Ryley and Pereira-Smith, 2006</xref>; <xref ref-type="bibr" rid="bib60">Zhang et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Crane et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Liu et al., 2015</xref>). The traps are composed of two symmetrical structures separated by 3 µm (see <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D-G</xref>), in such a way that only one cell can be trapped and remain in between the structures. We have measured that 94% of the cells that underwent at least five divisions in the trap would stay inside until their death. This is striking contrast with the results obtained with a device with a semi-open trap geometry (see <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for details). Moreover, a particle filter with a cutoff size of 15 µm is present before each array of traps, preventing dust particles or debris from clogging the chip (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). In addition, a cell filter with a cutoff size of 1.5µm is placed upstream of each trapping area to prevent contamination of the inlet by cells during the seeding phase (see next paragraph).</p><p>The microfluidic devices were fabricated using soft-lithography by pouring polydimethylsiloxane (PDMS, Sylgard 184, Dow Chemical, USA) with its curing agent (10:1 mixing ratio) on the different molds. The chips were punched with a 1 mm biopsy tool (KAI, Japan) and covalently bound to a 24×50 mm coverslip using plasma surface activation (Diener Zepto, Germany). The assembled chips were baked for 30 min at 60 °C to consolidate covalent bonds between glass and PDMS. The chip was then plugged using a 1 mm Outer Diameter (O.D.) PTFE tubing (Adtech, UK) and the channels were primed using culture media for 5 min. After that, cells were injected through the outlet using a 5 mL syringe and a 26 G needle for approximately 1 min per channel by applying very gentle pressure. The cell filter placed upstream of the trapping area prevented the cells from entering the tubing connected to the inlet (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1 E</xref>). Then, the inlet of each microfluidic channel was connected to a peristaltic pump (Ismatec, Switzerland) with a 5 µL/min rate to ensure a constant replenishment of the media and dissection of the daughter cells (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>). This procedure avoids potential contamination by cells forming colonies upstream of the trapping area, which would induce the clogging of the device after 1–2 days of experiment, therefore making long-lasting experiments more robust.</p></sec><sec id="s4-5"><title>Microscopy</title><p>The microscope was built from a modular microscope system with a motorized stage (ASI, USA, see the supplementary text for the detailed list of components), a ×20 objective 0.45 (Nikon, Japan) lens, and an sCMOS camera (ORCA flash 4.0, Hamamatsu, Japan). A dual-band filter (#59022, Chroma Technology, Germany) coupled with a two-channel LED system (DC4104 and LED4D067, Thorlabs, USA). The sample temperature was maintained at 30 °C thanks to a heating system based on an Indium Thin Oxide coated glass and an infrared sensor coupled to an Arduino-based regulatory loop.</p><sec id="s4-5-1"><title>Microscope</title><p>The microscope was built from a modular microscope system (RAMM, ASI, USA) with trans- (Oly-Trans-Illum, ASI, USA) and epi- (Mim-Excite-Cond20N-K, ASI, USA) illumination. This microscope frame provides a cost-effective solution to build a minimal microscopy apparatus to perform robust image acquisition over several days (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>).</p><p>It is equipped with a motorized XY stage (S551-2201B, ASI, USA), a stage controller (MS200, ASI, USA), and a stepper motor to drive the ×20 N.A. 0.45 Plan Fluor objective (Nikon, Japan) and an sCMOS camera (ORCA flash 4.0, Hamamatsu, Japan) with 2048 pixels × 2048 pixels (i.e. 650 µm × 650 µm field of view at ×20 magnification). We used a dual-band filter (#59022, Chroma Technology, Germany) coupled with two-channel LED illumination (DC4104 and LED4D067, Thorlabs, USA), which allows fast imaging of GFP and mCherry without any filter switching.</p></sec><sec id="s4-5-2"><title>Sample holder and temperature control</title><p>We designed a custom 3D-printed sample holder (by extruding PLA material with a MK3S+printer, Prusa Research, Czech republic) for the microfluidic device to ensure the mechanical stability of the microfluidic device (design available on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/ITO_heating_device">https://github.com/TAspert/ITO_heating_device</ext-link>; <xref ref-type="bibr" rid="bib3">Aspert, 2022</xref>).</p><p>In addition, we developed a custom temperature control system to maintain a constant temperature (30 °C) and guarantee optimal cell growth throughout the experiment. Briefly, we used an Indium Tin Oxide (ITO) Coated glass, an electrically conductive and transparent material, in direct contact with the PDMS chip. Therefore, applying a voltage to the ITO glass heat the glass and the adjacent PDMS chip by Joule effect (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>).</p><p>To achieve a temperature control loop, we used an infrared sensor attached to the objective and facing towards the bottom glass coverslip in contact with the cells. The sensor allowed in situ temperature measurement and was used in an Arduino-based PID control loop to regulate the heating power to maintain the setpoint temperature (the circuit diagram is available on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/ITO_heating_device">https://github.com/TAspert/ITO_heating_device</ext-link>; <xref ref-type="bibr" rid="bib3">Aspert, 2022</xref>). About 0.3 W was sufficient to maintain a 30 °C temperature at room temperature. Notably, the temperature profile obtained with this method was homogenous and constant throughout the experiment. Furthermore, the glass is fully transparent to visible light. In addition, it does not interfere with fluorescent light when using an inverted microscope since it is located at the end of the optical path, after the sample.</p></sec><sec id="s4-5-3"><title>Software and time-lapse acquisition parameters</title><p>Micromanager v2.0. was used to drive the camera, the light source, the XYZ controller, and the LED light source for fluorescence epi-illumination. We developed a specific program in order to drive the temperature controller from the Arduino (The source code is available on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/ITO_heating_device">https://github.com/TAspert/ITO_heating_device</ext-link>; <xref ref-type="bibr" rid="bib3">Aspert, 2022</xref>).</p><p>Unless specified otherwise, the interval between two brightfield frames for all the experiments was 5 min, and images were recorded over 1000 frames (i.e. ~3 days). We used three z-stack for brightfield imaging (spaced by 1.35 µm) to ease the detection of small buds during the image classification process for cell state determination. Fluorescent images were acquired with a 10 min interval using 470 nm illumination for 50ms. Up to 80 fields of view were recorded over the 5 min interval.</p></sec><sec id="s4-5-4"><title>Autofocusing</title><p>To keep a stable focus through the whole experiment, we developed a custom software-based autofocus routine that finds the sharpest image on the first field of view and then applies the focus correction to the rest of the positions (<ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/DetecDiv_Data">https://github.com/TAspert/DetecDiv_Data</ext-link>; <xref ref-type="bibr" rid="bib2">Aspert, 2021</xref>). This method provides faster scanning of all fields of view in a reasonable time. Nevertheless, it is almost as efficient as performing autofocusing on each position since the primary source of defocusing in our setup is the thermal drift, which applies identically to all the positions.</p></sec><sec id="s4-5-5"><title>Time-lapse routine</title><p>Micromanager v2.0 (<xref ref-type="bibr" rid="bib14">Edelstein et al., 2014</xref>) was used to drive all hardware, including the camera, the light sources, and the stage and objective motors. We could image approximately 80 fields of view (0.65mm×0.65mm) in brightfield and fluorescence (using a dual-band GFP-mCherry filter) with this interval. In the H<sub>2</sub>O<sub>2</sub> stress response experiments, cells were exposed abruptly to a medium containing the desired concentration (from 0.3mM to 0.8 mM) and fluorescence was acquired every 15 min.</p></sec></sec><sec id="s4-6"><title>Additional datasets for the comparative study of division detection</title><p>Time-lapse image datasets of individual mother cells trapped in microfluidic devices were obtained from the Murat Acar and Peter Swain lab. The datasets were used to compare the performance of our cell division tracking pipeline, as described in the main text. Data from the Acar lab were generated on a Nikon Ti Eclipse using ×40 brightfield imaging with a 10min-interval and a single z-stack, as previously described (<xref ref-type="bibr" rid="bib39">Liu et al., 2015</xref>). Data from the Swain lab were obtained using a Nikon Ti Eclipse microscope using ×60 brightfield imaging, a 2.5min-interval (<xref ref-type="bibr" rid="bib9">Crane et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Granados et al., 2018</xref>), and 5 z-stacks combined into a single RGB image and used as input to the classifier. We also used a separate trap design from our own lab that is similar to a previously reported design (<xref ref-type="bibr" rid="bib30">Jo et al., 2015</xref>) which was imaged on a Nikon Ti Eclipse microscope using a ×60 phase-contrast objective.</p></sec><sec id="s4-7"><title>Image processing</title><sec id="s4-7-1"><title>DetecDiv software</title><p>We developed Matlab software with a graphical user interface, DetecDiv, which provides different classification models: image classification, image sequence classification, time series classification, and pixel classification (semantic segmentation), see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for details. DetecDiv was developed using Matlab, and additional toolboxes (TB), such as the Computer Vision TB, the Deep-learning TB, and the Image Processing TB. A graphical user interface (GUI) was designed to facilitate the generation of the training sets. The DetecDiv software is available for download on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/gcharvin/DetecDiv">https://github.com/gcharvin/DetecDiv</ext-link> (<xref ref-type="bibr" rid="bib2">Aspert, 2021</xref>).</p><p>DetecDiv can be used with an arbitrarily large number of classes, image channels, types, and sizes. Indeed, several classification models can be defined to process the images: (1) Image classification using a convolutional network (CNN, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>); (2) a combined CNN+LSTM classifier (<xref ref-type="fig" rid="fig2">Figure 2A</xref>); (3) an LSTM network to perform sequence-to-sequence (as in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) or sequence-to-one classification; (4) An encoder/decoder classifier to perform pixel classification (semantic segmentation) based on the DeeplabV3+ architecture (<xref ref-type="bibr" rid="bib8">Chen et al., 2018</xref>), (<xref ref-type="fig" rid="fig5">Figure 5</xref>); (5) Similar routines as in 1–4, but for regression analyses (not used in the present study, see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for more details). DetecDiv allows the user to choose among several CNNs -such as GoogleNet or Resnet50 for all image classifications/segmentations applications.</p><p>DetecDiv also provides a graphical user interface to generate the groundtruth required for both training and testing the classifiers used in the image classification, pixel classification, and time-series classification pipelines. Furthermore, we paid attention to making this step as user-friendly as possible. For instance, we used keyboard shortcuts to assign labels to individual frames (it takes about 5–10 min to annotate 1000 frames in the case of cell state assignment). Similarly, direct ‘painting’ of objects with a mouse or a graph pad can be used to label images before launching the training procedure for pixel classification.</p><p>DetecDiv training and validation procedures are run either at the command line, which allows using remote computing resources, such as a CPU/GPU cluster, or using a Matlab GUI application. All the relevant training parameters can be easily defined by the user. Moreover, we designed generic routines to benchmark the trained classifiers that allow an in-depth evaluation of the classifiers’ performances (<xref ref-type="bibr" rid="bib33">Laine et al., 2021</xref>). Trained classifiers can be exported to user-defined repositories and classified data can be further processed using custom Matlab scripts, and images sequences can be exported as.mat or.avi video files.</p><p>Last, DetecDiv provides additional post-processing routines to extract cell-cycle, lifespan and pixel-related (volume, signal intensity, etc) data for further analysis, as performed in the present study.</p></sec><sec id="s4-7-2"><title>Convolutional Neural Networks (CNN) for classification of the cellular budding status and death</title><p>We used an image classifier to assess the state of cells in the cell cycle (small, large-budded, etc.) using brightfield images of individual traps. For each frame, we combined the three z-stack images described above into a single RGB image, which was used as input for the classifier.</p><p>We defined six classes, four of which represent the state of the cell, that is, unbudded (‘unbud’), small-budded (‘small’), large-budded (‘large’), dead (‘dead’), as shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Two additional classes are related to the state of the trap: trap with no cell (‘empty’) and clogged trap (‘clog’).</p><p>The small class was attributed to images on which the mother cell displayed a bud below a certain threshold size. This size was defined to represent approximately half of the cell-cycle images, but also to stay below the smallest daughter cells.</p><p>The large class was attributed to images on which the mother cell displayed a bud above the aforementioned threshold size. If no bud was visible on the mother’s surface after the cytokinesis, the image was labeled as &quot;large&quot; if the last daughter cell remained in contact with the mother, or &quot;unbudded&quot; if the mother cell was left alone in the trap.</p><p>An image was labeled as ‘dead’ when the mother cells appeared as dead. This includes an unambiguous, very abrupt (within one frame), and strong change in contrast and appearance of the cell.</p><p>The empty class was attributed to images on which no cell was present in the ~three first quarters (from the bottom) part of the trap.</p><p>Finally, the clogged class was used for images where more than ~50% of the outer space of the trap was filled with cells.</p><p>We trained a pre-trained Inception v1 (also known as GoogleNet) convolutional neural network (<xref ref-type="bibr" rid="bib53">Szegedy et al., 2015</xref>) to classify images according to these six classes using a training set of 200,000 representative manually annotated brightfield images (i.e. 200 ROIs monitored during 1000 frames). Of note, the number of images used for training results from a trade-off between the desired accuracy of the model and the time required to build the training set.</p><p>The training of the classifier was achieved using Adaptive Moment estimation (Adam) optimizer (<xref ref-type="bibr" rid="bib31">Kingma and Ba, 2015</xref>). Specific parameters used can be found in the <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p>After the training procedure, we tested the classifier using a dataset composed of 50 independent ROIs (i.e. ~50,000 images) that were manually annotated and used for benchmarking (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p></sec><sec id="s4-7-3"><title>Cell-cycle duration measurements and replicative lifespan (RLS) reconstruction based on classification results</title><p>As the image classifier outputs a label for each frame corresponding to one of the ix classes defined above, we used the sequence of labels to reveal the successive generations of the cells: the oscillations between the ‘large’ and ‘small’ or ‘unbudded’ and ‘small’ classes captured the entry into a new cell cycle (i.e. a budding event).</p><p>The first occurrence of one of the four following rules was used as a condition to stop the lifespan: (1) the occurrence of a ‘Dead’ class; (2) division arrest for more than 10 h; (3) occurrence of a ‘Clogged’ class; (4) the occurrence of an ‘Empty’ class (the mother left the trap <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1G</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), which was a rare case. Premature lifespan arrests due to clogging or mother cell removal (3 and 4) were not considered further for lifespan analyses.</p><p>This set of rules was used to compute the cell-cycle duration and the RLS of each individual cell when using either the CNN or the combined CNN+LSTM architecture (see below). However, in order to improve the accuracy of the method based only on the CNN, we implemented an additional ‘post-processing’ step (referred to as PP in <xref ref-type="fig" rid="fig2">Figure 2</xref>), namely that two consecutive frames with a ‘dead’ label are necessary to consider a cell as dead.</p></sec><sec id="s4-7-4"><title>Image sequence classification using combined CNN and a long short-term memory network (LSTM)</title><p>To provide a more accurate classification of the image according to the cellular state, we added a bidirectional long short-term memory (LSTM) network with 150 hidden units to the CNN network (<xref ref-type="bibr" rid="bib23">Hochreiter and Schmidhuber, 1997</xref>). The LSTM network takes the whole sequence of images as input (instead of independent images in the case of the CNN). 200 ROIs (with 1000 frames each) were used to train the LSTM network independently of the CNN network (see training parameters in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref> and benchmarks on <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B-D</xref>). The CNN and the LSTM network were then assembled as described in <xref ref-type="fig" rid="fig2">Figure 2A</xref> in order to output a sequence of labels for each sequence of images. The assembled network was then benchmarked using a set of 50 independent annotated ROIs, as described above.</p><p>The training and test datasets are available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6078462">doi.org/10.5281/zenodo.6078462</ext-link>.</p><p>The trained network is available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5553862">doi.org/10.5281/zenodo.5553862</ext-link>.</p></sec><sec id="s4-7-5"><title>Assessment of cell-cycle slowdown using an LSTM network</title><p>We designed a time series classification method to identify when the cell cycle starts to slow down (<underline>S</underline>enescence <underline>E</underline>ntry <underline>P</underline>oint, or SEP, see Results section). For this, we trained a bidirectional LSTM network with 150 hidden units to classify all the frames in each lifespan, into two classes, ‘pre-SEP and ‘post-SEP’, using a manually annotated dataset containing 200 ROIs. To achieve these annotations, we designed a custom annotation GUI allowing us to monitor the successive states of a mother cell of interest over time, as output by the CNN+LSTM network above. This tool was convenient to detect the cell cycle slow down occurring upon entry into senescence. Then, the LSTM network was trained using class probabilities from the previously described CNN+LSTM (unbudded, small, large, dead), see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for benchmarking results on a test set with 47 ROIs.</p><p>The training and test datasets are available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6075691">doi.org/10.5281/zenodo.6075691</ext-link>.</p><p>The trained network is available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5553829">doi.org/10.5281/zenodo.5553829</ext-link>.</p></sec></sec><sec id="s4-8"><title>Brightfield and fluorescence images semantic segmentation using DeepLabV3+</title><p>Cells and nuclei contours were determined based on brightfield and fluorescence images, respectively, using the deep learning-based semantic segmentation architecture DeepLabV3+ (<xref ref-type="bibr" rid="bib8">Chen et al., 2018</xref>). To generate the groundtruth data required to feed both the training and test datasets, we developed a graphical user-interfaced routine to ‘paint’ the input images, a process which took about 15–30 s per image depending on the number of cells in a 60×60 pixel-large field of view. We trained the network using a training set containing 1400 and 3000 images for brightfield and fluorescence images, respectively (see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> and <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref> for benchmarking results). Specific parameters used can be found in the <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref> (cell segmentation) and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref> (nucleus segmentation). In addition, we have implemented a cross-validation routine to test the sensitivity of the classifier used for cell segmentation to the training and test datasets. For this purpose, we have performed 30 successive random draws of 200 annotated ROIs to be used as a training set and 50 annotated ROIs for testing the classifier (the total number of manually annotated ROIs is 250). For each draw, we have measured the performance of the classifier (i.e. precision, recall, F<sub>1</sub>-score), see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2E</xref>.</p><p>The training and test datasets are available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6077125">doi.org/10.5281/zenodo.6077125</ext-link>.</p><p>The trained network is available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5553851">doi.org/10.5281/zenodo.5553851</ext-link>.</p></sec><sec id="s4-9"><title>Classifier benchmarking</title><p>We used standard benchmarking to estimate the efficiency of image and pixel classifiers. For each classifier, we computed the confusion matrix obtained by comparing the groundtruth of manually annotated images (or time series) taken from a test set unseen by the network during training to the predictions made by the classifier. We computed the precision, recall, and F<sub>1</sub>-score for each class (see the corresponding definitions in the Results section). In the specific case of pixel classification (semantic segmentation), we computed these benchmarks for different values of prediction thresholds used to assign the ‘mother’ and ‘nucleus’ classes, as reported in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplements 2</xref> and <xref ref-type="fig" rid="fig5s3">3</xref>. Then, we performed the segmentation of images using the threshold value that maximizes the F<sub>1</sub>-score (0.9 and 0.35 for brightfield and fluorescence image classification, respectively).</p><p>To benchmark the detection of new generations, we used a custom pairing algorithm to detect false positive and false negative new generation events. Using this, we could compute the precision and recall of the detection of new generation events, and plot the correlation between paired new generations events (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p></sec><sec id="s4-10"><title>Statistics</title><p>All experiments have been replicated at least twice. Error bars and ± represent the standard error-on-mean unless specified otherwise. Results of specific statistical tests are indicated in the figure legends.</p></sec><sec id="s4-11"><title>Computing time</title><p>Image processing was performed on a computing server with 8 Intel Xeon E5-2620 processors and 8 co-processing GPU units (Nvidia Tesla K80, released in 2014), each of them with 12Go RAM. Under these conditions, the classification of the time series of 1000 frames from a single trap (roughly 60 pixels × 60 pixels) took 3.5 s to the CNN+LSTM classifier. For the image segmentation, the DeepLabV3 +network took about 20 s to classify 1000 images. Alternatively, we have used a personal notebook with an Nvidia Quadro T2000 card (similar to a GTX 1650, released in 2019) to set up and troubleshoot the code. Under the conditions, training procedures could be achieved within a few hours or overnight depending on the size of the training set, and classification times were similar to those obtained with a Telsa K80. However, using bigger CNNs, such as the inception v3 or the inception-resnet v2, lead to a large increase in computing times (see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4J</xref>), which makes them cumbersome when using limited computing resources, as in our study.</p></sec><sec id="s4-12"><title>Materials availability statement</title><sec id="s4-12-1"><title>Datasets</title><p>Annotated datasets and trained classifiers used in this study are available for download as indicated:</p><list list-type="bullet"><list-item><p>Lifespan analyses:</p><p>○ Training and test datasets: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6078462">doi.org/10.5281/zenodo.6078462</ext-link></p><p>○ Trained Network (CNN+LSTM): <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5553862">doi.org/10.5281/zenodo.5553862</ext-link></p></list-item><list-item><p>Brightfield image segmentation:</p><p>○ Training and test datasets: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6077125">doi.org/10.5281/zenodo.6077125</ext-link></p><p>○ Trained Network (Encoder-Decoder DeeplabV3+): <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5553851">doi.org/10.5281/zenodo.5553851</ext-link></p></list-item><list-item><p>Cell-cycle slowdown (Senescence Entry Point) detection:</p><p>○ Training &amp; test datasets: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6075691">doi.org/10.5281/zenodo.6075691</ext-link></p><p>○ Trained Network (LSTM): <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5553829">doi.org/10.5281/zenodo.5553829</ext-link></p></list-item></list><p>Information regarding the design of the microfluidic device and of the custom imaging system are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/DetecDiv_Data">https://github.com/TAspert/DetecDiv_Data</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:41bb8b573b3af8ded2089f6d7711de3958e40546;origin=https://github.com/TAspert/DetecDiv_Data;visit=swh:1:snp:1678d72d8146e36ff7608e2c0ba92e98c8f1aaf9;anchor=swh:1:rev:ab95660be5e0677dba69247d27492036c33e08c1">swh:1:rev:ab95660be5e0677dba69247d27492036c33e08c1</ext-link>; <xref ref-type="bibr" rid="bib2">Aspert, 2021</xref>).</p></sec></sec><sec id="s4-13"><title>Code</title><p>The custom MATLAB software DetecDiv, used to analyze imaging data with deep-learning algorithms, is available on <ext-link ext-link-type="uri" xlink:href="https://github.com/gcharvin/DetecDiv">https://github.com/gcharvin/DetecDiv</ext-link>.</p><p>This software distribution features a tutorial on how to use the graphical user interface: <ext-link ext-link-type="uri" xlink:href="https://github.com/gcharvin/DetecDiv/blob/master/Tutorial/GUI_tutorial.md">https://github.com/gcharvin/DetecDiv/blob/master/Tutorial/GUI_tutorial.md</ext-link>.</p><p>It also provides the main commands to use the DetecDiv pipeline in custom user-defined scripts: <ext-link ext-link-type="uri" xlink:href="https://github.com/gcharvin/DetecDiv/blob/master/Tutorial/commandline_tutorial.md">https://github.com/gcharvin/DetecDiv/blob/master/Tutorial/commandline_tutorial.md</ext-link>.</p><p>A demo project that contains all the necessary files to learn how to use DetecDiv can be downloaded from zenodo: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5771536">https://doi.org/10.5281/zenodo.5771536</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Investigation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79519-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Strain list.</title></caption><media xlink:href="elife-79519-supp1-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>List of classification models.</title></caption><media xlink:href="elife-79519-supp2-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Software and documentation is fully available via Github. Data used for training classifiers is available on Zenodo. A demo detecdiv project that contains all information to train users on detecdiv is available on zenodo. All the links are provided in the manuscript file and dataset list.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Annotated images from yeast cell lifespans - Training &amp; Test sets - DetecDiv (id01)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.6078462</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Trained network for classification of images from yeast cell lifespans - DetecDiv (id01)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5553862</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Annotated pixels from brightfield images from yeast cell in microfluidic traps - Training and Test sets - DetecDiv (id02)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.6077125</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset4"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Trained network for segmentation of yeast cell from brightfield images in microfluidic traps - DetecDiv (id02)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5553851</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset5"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Annotated timeseries from yeast cell lifespans - Training and Test sets - DetecDiv (id03)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.6075691</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset6"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Trained network for cell-cycle slowdown detection - DetecDiv (id03)</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5553829</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset7"><person-group person-group-type="author"><name><surname>Charvin</surname><given-names>G</given-names></name><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>DetecDiv Demo Project files</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5771536</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Audrey Matifas for constant technical support throughout this work, Sophie Quintin and Nacho Molina for carefully reading the manuscript. We are very grateful to Murat Acar and David Moreno Fortuño, as well as Peter Swain and Julian Pietsch for providing the additional time-lapse datasets analyzed in this study. We thank Olivier Tassy for their insightful discussions. We thank Denis Fumagalli at the IGBMC Mediaprep facility for media preparation. We are grateful to the IT service for efficient support and providing the computing resources. We thank the Charvin lab members, Bertrand Vernay, Jerome Mutterer, Serge Taubert, and the IGBMC imaging facility for discussions and technical support. This work was supported by the Agence Nationale pour la Recherche (T.A. and G.C.), the grant ANR-10-LABX-0030-INRT, a French State fund managed by the Agence Nationale de la Recherche under the frame program Investissements d'Avenir ANR-10-IDEX-0002–02.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguilaniu</surname><given-names>H</given-names></name><name><surname>Gustafsson</surname><given-names>L</given-names></name><name><surname>Rigoulet</surname><given-names>M</given-names></name><name><surname>Nyström</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Asymmetric inheritance of oxidatively damaged proteins during cytokinesis</article-title><source>Science</source><volume>299</volume><fpage>1751</fpage><lpage>1753</lpage><pub-id pub-id-type="doi">10.1126/science.1080418</pub-id><pub-id pub-id-type="pmid">12610228</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>DetecDiv_Data</data-title><version designator="swh:1:rev:ab95660be5e0677dba69247d27492036c33e08c1">swh:1:rev:ab95660be5e0677dba69247d27492036c33e08c1</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/DetecDiv_Data">https://github.com/TAspert/DetecDiv_Data</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Aspert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>ITO_heating_device</data-title><version designator="swh:1:rev:b3d70c528c741876c88e5300e82a6f85f4201afd">swh:1:rev:b3d70c528c741876c88e5300e82a6f85f4201afd</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://github.com/TAspert/ITO_heating_device">https://github.com/TAspert/ITO_heating_device</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bagamery</surname><given-names>LE</given-names></name><name><surname>Justman</surname><given-names>QA</given-names></name><name><surname>Garner</surname><given-names>EC</given-names></name><name><surname>Murray</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A putative bet-hedging strategy buffers budding yeast against environmental instability</article-title><source>Current Biology</source><volume>30</volume><fpage>4563</fpage><lpage>4578</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.08.092</pub-id><pub-id pub-id-type="pmid">32976801</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakshi</surname><given-names>S</given-names></name><name><surname>Leoncini</surname><given-names>E</given-names></name><name><surname>Baker</surname><given-names>C</given-names></name><name><surname>Cañas-Duarte</surname><given-names>SJ</given-names></name><name><surname>Okumus</surname><given-names>B</given-names></name><name><surname>Paulsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Tracking bacterial lineages in complex and dynamic environments with applications for growth control and persistence</article-title><source>Nature Microbiology</source><volume>6</volume><fpage>783</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1038/s41564-021-00900-4</pub-id><pub-id pub-id-type="pmid">34017106</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bheda</surname><given-names>P</given-names></name><name><surname>Aguilar-Gómez</surname><given-names>D</given-names></name><name><surname>Becker</surname><given-names>NB</given-names></name><name><surname>Becker</surname><given-names>J</given-names></name><name><surname>Stavrou</surname><given-names>E</given-names></name><name><surname>Kukhtevich</surname><given-names>I</given-names></name><name><surname>Höfer</surname><given-names>T</given-names></name><name><surname>Maerkl</surname><given-names>S</given-names></name><name><surname>Charvin</surname><given-names>G</given-names></name><name><surname>Marr</surname><given-names>C</given-names></name><name><surname>Kirmizis</surname><given-names>A</given-names></name><name><surname>Schneider</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Single-cell tracing dissects regulation of maintenance and inheritance of transcriptional reinduction memory</article-title><source>Molecular Cell</source><volume>78</volume><fpage>915</fpage><lpage>925</lpage><pub-id pub-id-type="doi">10.1016/j.molcel.2020.04.016</pub-id><pub-id pub-id-type="pmid">32392469</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caudron</surname><given-names>F</given-names></name><name><surname>Barral</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A super-assembly of whi3 encodes memory of deceptive encounters by single cells during yeast courtship</article-title><source>Cell</source><volume>155</volume><fpage>1244</fpage><lpage>1257</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2013.10.046</pub-id><pub-id pub-id-type="pmid">24315096</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Schroff</surname><given-names>F</given-names></name><name><surname>Adam</surname><given-names>H</given-names></name></person-group><article-title>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</article-title><conf-name>ECCV 2018</conf-name><year iso-8601-date="2018">2018</year><ext-link ext-link-type="uri" xlink:href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper.pdf</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crane</surname><given-names>MM</given-names></name><name><surname>Clark</surname><given-names>IBN</given-names></name><name><surname>Bakker</surname><given-names>E</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name><name><surname>Swain</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A microfluidic system for studying ageing and dynamic single-cell responses in budding yeast</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e100042</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0100042</pub-id><pub-id pub-id-type="pmid">24950344</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Defossez</surname><given-names>PA</given-names></name><name><surname>Prusty</surname><given-names>R</given-names></name><name><surname>Kaeberlein</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>SJ</given-names></name><name><surname>Ferrigno</surname><given-names>P</given-names></name><name><surname>Silver</surname><given-names>PA</given-names></name><name><surname>Keil</surname><given-names>RL</given-names></name><name><surname>Guarente</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Elimination of replication block protein fob1 extends the life span of yeast mother cells</article-title><source>Molecular Cell</source><volume>3</volume><fpage>447</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/s1097-2765(00)80472-4</pub-id><pub-id pub-id-type="pmid">10230397</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denoth Lippuner</surname><given-names>A</given-names></name><name><surname>Julou</surname><given-names>T</given-names></name><name><surname>Barral</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Budding yeast as a model organism to study the effects of age</article-title><source>FEMS Microbiology Reviews</source><volume>38</volume><fpage>300</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1111/1574-6976.12060</pub-id><pub-id pub-id-type="pmid">24484434</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietler</surname><given-names>N</given-names></name><name><surname>Minder</surname><given-names>M</given-names></name><name><surname>Gligorovski</surname><given-names>V</given-names></name><name><surname>Economou</surname><given-names>AM</given-names></name><name><surname>Joly</surname><given-names>DAHL</given-names></name><name><surname>Sadeghi</surname><given-names>A</given-names></name><name><surname>Chan</surname><given-names>CHM</given-names></name><name><surname>Koziński</surname><given-names>M</given-names></name><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Bitbol</surname><given-names>A-F</given-names></name><name><surname>Rahi</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A convolutional neural network segments yeast microscopy images with high accuracy</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-19557-4</pub-id><pub-id pub-id-type="pmid">33184262</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dillin</surname><given-names>A</given-names></name><name><surname>Gottschling</surname><given-names>DE</given-names></name><name><surname>Nyström</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The good and the bad of being connected: the integrons of aging</article-title><source>Current Opinion in Cell Biology</source><volume>26</volume><fpage>107</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.ceb.2013.12.003</pub-id><pub-id pub-id-type="pmid">24529252</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelstein</surname><given-names>AD</given-names></name><name><surname>Tsuchida</surname><given-names>MA</given-names></name><name><surname>Amodaj</surname><given-names>N</given-names></name><name><surname>Pinkard</surname><given-names>H</given-names></name><name><surname>Vale</surname><given-names>RD</given-names></name><name><surname>Stuurman</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Advanced methods of microscope control using μmanager software</article-title><source>Journal of Biological Methods</source><volume>1</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.14440/jbm.2014.36</pub-id><pub-id pub-id-type="pmid">25606571</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fehrmann</surname><given-names>S</given-names></name><name><surname>Paoletti</surname><given-names>C</given-names></name><name><surname>Goulev</surname><given-names>Y</given-names></name><name><surname>Ungureanu</surname><given-names>A</given-names></name><name><surname>Aguilaniu</surname><given-names>H</given-names></name><name><surname>Charvin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Aging yeast cells undergo a sharp entry into senescence unrelated to the loss of mitochondrial membrane potential</article-title><source>Cell Reports</source><volume>5</volume><fpage>1589</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2013.11.013</pub-id><pub-id pub-id-type="pmid">24332850</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frenk</surname><given-names>S</given-names></name><name><surname>Pizza</surname><given-names>G</given-names></name><name><surname>Walker</surname><given-names>RV</given-names></name><name><surname>Houseley</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Aging yeast gain a competitive advantage on non-optimal carbon sources</article-title><source>Aging Cell</source><volume>16</volume><fpage>602</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1111/acel.12582</pub-id><pub-id pub-id-type="pmid">28247585</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghafari</surname><given-names>M</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>H-B</given-names></name><name><surname>Yu</surname><given-names>R</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Dang</surname><given-names>W</given-names></name><name><surname>Qin</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Complementary performances of convolutional and capsule neural networks on classifying microfluidic images of dividing yeast cells</article-title><source>PLOS ONE</source><volume>16</volume><elocation-id>e0246988</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0246988</pub-id><pub-id pub-id-type="pmid">33730031</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goulev</surname><given-names>Y</given-names></name><name><surname>Morlot</surname><given-names>S</given-names></name><name><surname>Matifas</surname><given-names>A</given-names></name><name><surname>Huang</surname><given-names>B</given-names></name><name><surname>Molin</surname><given-names>M</given-names></name><name><surname>Toledano</surname><given-names>MB</given-names></name><name><surname>Charvin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Nonlinear feedback drives homeostatic plasticity in H2O2 stress response</article-title><source>eLife</source><volume>6</volume><elocation-id>e23971</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23971</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granados</surname><given-names>AA</given-names></name><name><surname>Pietsch</surname><given-names>JMJ</given-names></name><name><surname>Cepeda-Humerez</surname><given-names>SA</given-names></name><name><surname>Farquhar</surname><given-names>IL</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Swain</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distributed and dynamic intracellular organization of extracellular information</article-title><source>PNAS</source><volume>115</volume><fpage>6088</fpage><lpage>6093</lpage><pub-id pub-id-type="doi">10.1073/pnas.1716659115</pub-id><pub-id pub-id-type="pmid">29784812</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Gkioxari</surname><given-names>G</given-names></name><name><surname>Dollar</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mask R-CNN</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV</conf-name><conf-loc>Venice</conf-loc><pub-id pub-id-type="doi">10.1109/ICCV.2017.322</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>C</given-names></name><name><surname>Zhou</surname><given-names>C</given-names></name><name><surname>Kennedy</surname><given-names>BK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The yeast replicative aging model</article-title><source>Biochimica et Biophysica Acta. Molecular Basis of Disease</source><volume>1864</volume><fpage>2690</fpage><lpage>2696</lpage><pub-id pub-id-type="doi">10.1016/j.bbadis.2018.02.023</pub-id><pub-id pub-id-type="pmid">29524633</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberts</surname><given-names>D</given-names></name><name><surname>González</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>SS</given-names></name><name><surname>Litsios</surname><given-names>A</given-names></name><name><surname>Hubmann</surname><given-names>G</given-names></name><name><surname>Wit</surname><given-names>EC</given-names></name><name><surname>Heinemann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Calorie restriction does not elicit a robust extension of replicative lifespan in <italic>Saccharomyces cerevisiae</italic></article-title><source>PNAS</source><volume>111</volume><fpage>11727</fpage><lpage>11731</lpage><pub-id pub-id-type="doi">10.1073/pnas.1410024111</pub-id><pub-id pub-id-type="pmid">25071164</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>AL</given-names></name><name><surname>Gottschling</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An early age increase in vacuolar ph limits mitochondrial function and lifespan in yeast</article-title><source>Nature</source><volume>492</volume><fpage>261</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1038/nature11654</pub-id><pub-id pub-id-type="pmid">23172144</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacquel</surname><given-names>B</given-names></name><name><surname>Aspert</surname><given-names>T</given-names></name><name><surname>Laporte</surname><given-names>D</given-names></name><name><surname>Sagot</surname><given-names>I</given-names></name><name><surname>Charvin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Monitoring single-cell dynamics of entry into quiescence during an unperturbed life cycle</article-title><source>eLife</source><volume>10</volume><elocation-id>e73186</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.73186</pub-id><pub-id pub-id-type="pmid">34723791</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janssens</surname><given-names>GE</given-names></name><name><surname>Meinema</surname><given-names>AC</given-names></name><name><surname>González</surname><given-names>J</given-names></name><name><surname>Wolters</surname><given-names>JC</given-names></name><name><surname>Schmidt</surname><given-names>A</given-names></name><name><surname>Guryev</surname><given-names>V</given-names></name><name><surname>Bischoff</surname><given-names>R</given-names></name><name><surname>Wit</surname><given-names>EC</given-names></name><name><surname>Veenhoff</surname><given-names>LM</given-names></name><name><surname>Heinemann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Protein biogenesis machinery is a driver of replicative aging in yeast</article-title><source>eLife</source><volume>4</volume><elocation-id>e08527</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08527</pub-id><pub-id pub-id-type="pmid">26422514</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janssens</surname><given-names>GE</given-names></name><name><surname>Veenhoff</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evidence for the hallmarks of human aging in replicatively aging yeast</article-title><source>Microbial Cell</source><volume>3</volume><fpage>263</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.15698/mic2016.07.510</pub-id><pub-id pub-id-type="pmid">28357364</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>O’Laughlin</surname><given-names>R</given-names></name><name><surname>Bittihn</surname><given-names>P</given-names></name><name><surname>Pillus</surname><given-names>L</given-names></name><name><surname>Tsimring</surname><given-names>LS</given-names></name><name><surname>Hasty</surname><given-names>J</given-names></name><name><surname>Hao</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Divergent aging of isogenic yeast cells revealed through single-cell phenotypic dynamics</article-title><source>Cell Systems</source><volume>8</volume><fpage>242</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2019.02.002</pub-id><pub-id pub-id-type="pmid">30852250</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jo</surname><given-names>MC</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Gu</surname><given-names>L</given-names></name><name><surname>Dang</surname><given-names>W</given-names></name><name><surname>Qin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>High-throughput analysis of yeast replicative aging using a microfluidic system</article-title><source>PNAS</source><volume>112</volume><fpage>9364</fpage><lpage>9369</lpage><pub-id pub-id-type="doi">10.1073/pnas.1510328112</pub-id><pub-id pub-id-type="pmid">26170317</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980v9">https://arxiv.org/abs/1412.6980v9</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kundu</surname><given-names>S</given-names></name><name><surname>Horn</surname><given-names>PJ</given-names></name><name><surname>Peterson</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>SWI/SNF is required for transcriptional memory at the yeast GAL gene cluster</article-title><source>Genes &amp; Development</source><volume>21</volume><fpage>997</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1101/gad.1506607</pub-id><pub-id pub-id-type="pmid">17438002</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laine</surname><given-names>RF</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Henriques</surname><given-names>R</given-names></name><name><surname>Jacquemet</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Avoiding a replication crisis in deep-learning-based bioimage analysis</article-title><source>Nature Methods</source><volume>18</volume><fpage>1136</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01284-3</pub-id><pub-id pub-id-type="pmid">34608322</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SS</given-names></name><name><surname>Avalos Vizcarra</surname><given-names>I</given-names></name><name><surname>Huberts</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>LP</given-names></name><name><surname>Heinemann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Whole lifespan microscopic observation of budding yeast aging through a microfluidic dissection platform</article-title><source>PNAS</source><volume>109</volume><fpage>4916</fpage><lpage>4920</lpage><pub-id pub-id-type="doi">10.1073/pnas.1113505109</pub-id><pub-id pub-id-type="pmid">22421136</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Jin</surname><given-names>M</given-names></name><name><surname>O’Laughlin</surname><given-names>R</given-names></name><name><surname>Bittihn</surname><given-names>P</given-names></name><name><surname>Tsimring</surname><given-names>LS</given-names></name><name><surname>Pillus</surname><given-names>L</given-names></name><name><surname>Hasty</surname><given-names>J</given-names></name><name><surname>Hao</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multigenerational silencing dynamics control cell aging</article-title><source>PNAS</source><volume>114</volume><fpage>11253</fpage><lpage>11258</lpage><pub-id pub-id-type="doi">10.1073/pnas.1703379114</pub-id><pub-id pub-id-type="pmid">29073021</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Paxman</surname><given-names>J</given-names></name><name><surname>O’Laughlin</surname><given-names>R</given-names></name><name><surname>Klepin</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Pillus</surname><given-names>L</given-names></name><name><surname>Tsimring</surname><given-names>LS</given-names></name><name><surname>Hasty</surname><given-names>J</given-names></name><name><surname>Hao</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A programmable fate decision landscape underlies single-cell aging in yeast</article-title><source>Science</source><volume>369</volume><fpage>325</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1126/science.aax9552</pub-id><pub-id pub-id-type="pmid">32675375</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>SJ</given-names></name><name><surname>Defossez</surname><given-names>PA</given-names></name><name><surname>Guarente</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Requirement of NAD and SIR2 for life-span extension by calorie restriction in <italic>Saccharomyces cerevisiae</italic></article-title><source>Science</source><volume>289</volume><fpage>2126</fpage><lpage>2128</lpage><pub-id pub-id-type="doi">10.1126/science.289.5487.2126</pub-id><pub-id pub-id-type="pmid">11000115</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindstrom</surname><given-names>DL</given-names></name><name><surname>Gottschling</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The mother enrichment program: A genetic system for facile replicative life span analysis in <italic>Saccharomyces cerevisiae</italic></article-title><source>Genetics</source><volume>183</volume><fpage>413</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1534/genetics.109.106229</pub-id><pub-id pub-id-type="pmid">19652178</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>P</given-names></name><name><surname>Young</surname><given-names>TZ</given-names></name><name><surname>Acar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Yeast replicator: A high-throughput multiplexed microfluidics platform for automated measurements of single-cell aging</article-title><source>Cell Reports</source><volume>13</volume><fpage>634</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2015.09.012</pub-id><pub-id pub-id-type="pmid">26456818</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>MA</given-names></name><name><surname>Delaney</surname><given-names>JR</given-names></name><name><surname>Tsuchiya</surname><given-names>M</given-names></name><name><surname>Tsuchiyama</surname><given-names>S</given-names></name><name><surname>Shemorry</surname><given-names>A</given-names></name><name><surname>Sim</surname><given-names>S</given-names></name><name><surname>Chou</surname><given-names>AC-Z</given-names></name><name><surname>Ahmed</surname><given-names>U</given-names></name><name><surname>Carr</surname><given-names>D</given-names></name><name><surname>Murakami</surname><given-names>CJ</given-names></name><name><surname>Schleit</surname><given-names>J</given-names></name><name><surname>Sutphin</surname><given-names>GL</given-names></name><name><surname>Wasko</surname><given-names>BM</given-names></name><name><surname>Bennett</surname><given-names>CF</given-names></name><name><surname>Wang</surname><given-names>AM</given-names></name><name><surname>Olsen</surname><given-names>B</given-names></name><name><surname>Beyer</surname><given-names>RP</given-names></name><name><surname>Bammler</surname><given-names>TK</given-names></name><name><surname>Prunkard</surname><given-names>D</given-names></name><name><surname>Johnson</surname><given-names>SC</given-names></name><name><surname>Pennypacker</surname><given-names>JK</given-names></name><name><surname>An</surname><given-names>E</given-names></name><name><surname>Anies</surname><given-names>A</given-names></name><name><surname>Castanza</surname><given-names>AS</given-names></name><name><surname>Choi</surname><given-names>E</given-names></name><name><surname>Dang</surname><given-names>N</given-names></name><name><surname>Enerio</surname><given-names>S</given-names></name><name><surname>Fletcher</surname><given-names>M</given-names></name><name><surname>Fox</surname><given-names>L</given-names></name><name><surname>Goswami</surname><given-names>S</given-names></name><name><surname>Higgins</surname><given-names>SA</given-names></name><name><surname>Holmberg</surname><given-names>MA</given-names></name><name><surname>Hu</surname><given-names>D</given-names></name><name><surname>Hui</surname><given-names>J</given-names></name><name><surname>Jelic</surname><given-names>M</given-names></name><name><surname>Jeong</surname><given-names>K-S</given-names></name><name><surname>Johnston</surname><given-names>E</given-names></name><name><surname>Kerr</surname><given-names>EO</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Kirkland</surname><given-names>K</given-names></name><name><surname>Klum</surname><given-names>S</given-names></name><name><surname>Kotireddy</surname><given-names>S</given-names></name><name><surname>Liao</surname><given-names>E</given-names></name><name><surname>Lim</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>MS</given-names></name><name><surname>Lo</surname><given-names>WC</given-names></name><name><surname>Lockshon</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>HA</given-names></name><name><surname>Moller</surname><given-names>RM</given-names></name><name><surname>Muller</surname><given-names>B</given-names></name><name><surname>Oakes</surname><given-names>J</given-names></name><name><surname>Pak</surname><given-names>DN</given-names></name><name><surname>Peng</surname><given-names>ZJ</given-names></name><name><surname>Pham</surname><given-names>KM</given-names></name><name><surname>Pollard</surname><given-names>TG</given-names></name><name><surname>Pradeep</surname><given-names>P</given-names></name><name><surname>Pruett</surname><given-names>D</given-names></name><name><surname>Rai</surname><given-names>D</given-names></name><name><surname>Robison</surname><given-names>B</given-names></name><name><surname>Rodriguez</surname><given-names>AA</given-names></name><name><surname>Ros</surname><given-names>B</given-names></name><name><surname>Sage</surname><given-names>M</given-names></name><name><surname>Singh</surname><given-names>MK</given-names></name><name><surname>Smith</surname><given-names>ED</given-names></name><name><surname>Snead</surname><given-names>K</given-names></name><name><surname>Solanky</surname><given-names>A</given-names></name><name><surname>Spector</surname><given-names>BL</given-names></name><name><surname>Steffen</surname><given-names>KK</given-names></name><name><surname>Tchao</surname><given-names>BN</given-names></name><name><surname>Ting</surname><given-names>MK</given-names></name><name><surname>Vander Wende</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Welton</surname><given-names>KL</given-names></name><name><surname>Westman</surname><given-names>EA</given-names></name><name><surname>Brem</surname><given-names>RB</given-names></name><name><surname>Liu</surname><given-names>X-G</given-names></name><name><surname>Suh</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Kaeberlein</surname><given-names>M</given-names></name><name><surname>Kennedy</surname><given-names>BK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A comprehensive analysis of replicative lifespan in 4,698 single-gene deletion strains uncovers conserved mechanisms of aging</article-title><source>Cell Metabolism</source><volume>22</volume><fpage>895</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1016/j.cmet.2015.09.008</pub-id><pub-id pub-id-type="pmid">26456335</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morlot</surname><given-names>S</given-names></name><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Léger-Silvestre</surname><given-names>I</given-names></name><name><surname>Matifas</surname><given-names>A</given-names></name><name><surname>Gadal</surname><given-names>O</given-names></name><name><surname>Charvin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Excessive rdna transcription drives the disruption in nuclear homeostasis during entry into senescence in budding yeast</article-title><source>Cell Reports</source><volume>28</volume><fpage>408</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.06.032</pub-id><pub-id pub-id-type="pmid">31291577</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mortimer</surname><given-names>RK</given-names></name><name><surname>Johnson</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Life span of individual yeast cells</article-title><source>Nature</source><volume>183</volume><fpage>1751</fpage><lpage>1752</lpage><pub-id pub-id-type="doi">10.1038/1831751a0</pub-id><pub-id pub-id-type="pmid">13666896</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neurohr</surname><given-names>GE</given-names></name><name><surname>Terry</surname><given-names>RL</given-names></name><name><surname>Sandikci</surname><given-names>A</given-names></name><name><surname>Zou</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Amon</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deregulation of the G1/S-phase transition is the proximal cause of mortality in old yeast mother cells</article-title><source>Genes &amp; Development</source><volume>32</volume><fpage>1075</fpage><lpage>1084</lpage><pub-id pub-id-type="doi">10.1101/gad.312140.118</pub-id><pub-id pub-id-type="pmid">30042134</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>S</given-names></name><name><surname>Tyler</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Epigenetics and aging</article-title><source>Science Advances</source><volume>2</volume><elocation-id>e1600584</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.1600584</pub-id><pub-id pub-id-type="pmid">27482540</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pietsch</surname><given-names>JMJ</given-names></name><name><surname>Muñoz</surname><given-names>AF</given-names></name><name><surname>Adjavon</surname><given-names>DYA</given-names></name><name><surname>Farquhar</surname><given-names>I</given-names></name><name><surname>Clark</surname><given-names>IBN</given-names></name><name><surname>Swain</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Label-Free Method to Track Individuals and Lineages of Budding Cells</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.11.491488</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pokhrel</surname><given-names>A</given-names></name><name><surname>Dyba</surname><given-names>T</given-names></name><name><surname>Hakulinen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A greenwood formula for standard error of the age-standardised relative survival ratio</article-title><source>European Journal of Cancer</source><volume>44</volume><fpage>441</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.ejca.2007.10.026</pub-id><pub-id pub-id-type="pmid">18053707</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prangemeier</surname><given-names>T</given-names></name><name><surname>Wildner</surname><given-names>C</given-names></name><name><surname>Françani</surname><given-names>AO</given-names></name><name><surname>Reich</surname><given-names>C</given-names></name><name><surname>Koeppl</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Yeast cell segmentation in microstructured environments with deep learning</article-title><source>Bio Systems</source><volume>211</volume><elocation-id>104557</elocation-id><pub-id pub-id-type="doi">10.1016/j.biosystems.2021.104557</pub-id><pub-id pub-id-type="pmid">34634444</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-net: convolutional networks for biomedical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>de Bruijne</surname><given-names>M</given-names></name></person-group><source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</source><publisher-name>Springer International Publishing</publisher-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryley</surname><given-names>J</given-names></name><name><surname>Pereira-Smith</surname><given-names>OM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Microfluidics device for single cell gene expression analysis in <italic>Saccharomyces cerevisiae</italic></article-title><source>Yeast</source><volume>23</volume><fpage>1065</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1002/yea.1412</pub-id><pub-id pub-id-type="pmid">17083143</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>U</given-names></name><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Broaddus</surname><given-names>C</given-names></name><name><surname>Myers</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Cell detection with star-convex polygons</chapter-title><person-group person-group-type="editor"><name><surname>Frangi</surname><given-names>A</given-names></name></person-group><source>In Medical Image Computing and Computer Assisted Intervention – MICCAI</source><publisher-name>Springer International Publishing</publisher-name><fpage>265</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-00934-2_30</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinclair</surname><given-names>DA</given-names></name><name><surname>Guarente</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Extrachromosomal rdna circles--a cause of aging in yeast</article-title><source>Cell</source><volume>91</volume><fpage>1033</fpage><lpage>1042</lpage><pub-id pub-id-type="doi">10.1016/s0092-8674(00)80493-6</pub-id><pub-id pub-id-type="pmid">9428525</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cellpose: A generalist algorithm for cellular segmentation</article-title><source>Nature Methods</source><volume>18</volume><fpage>100</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id><pub-id pub-id-type="pmid">33318659</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Rabinovich</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Going deeper with convolutions</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Wojna</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rethinking the Inception Architecture for Computer Vision</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><fpage>2818</fpage><lpage>2826</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.308</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>AA</given-names></name></person-group><article-title>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</article-title><conf-name>Thirty-First AAAI Conference on Artificial Intelligence</conf-name><year iso-8601-date="2017">2017</year><ext-link ext-link-type="uri" xlink:href="https://ojs.aaai.org/index.php/AAAI/article/view/11231">https://ojs.aaai.org/index.php/AAAI/article/view/11231</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Thayer</surname><given-names>NH</given-names></name><name><surname>Robles</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Schinski</surname><given-names>EL</given-names></name><name><surname>Hotz</surname><given-names>M</given-names></name><name><surname>Keyser</surname><given-names>R</given-names></name><name><surname>Millett-Sikking</surname><given-names>A</given-names></name><name><surname>Okreglak</surname><given-names>V</given-names></name><name><surname>Rogers</surname><given-names>JV</given-names></name><name><surname>Waite</surname><given-names>AJ</given-names></name><name><surname>Wranik</surname><given-names>BJ</given-names></name><name><surname>York</surname><given-names>AG</given-names></name><name><surname>McIsaac</surname><given-names>RS</given-names></name><name><surname>Gottschling</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The Yeast Lifespan Machine: A Microfluidic Platform for Automated Replicative Lifespan Measurements</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.02.14.480146</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Venugopalan</surname><given-names>S</given-names></name><name><surname>Rohrbach</surname><given-names>M</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Saenko</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sequence to Sequence -- Video to Text</article-title><conf-name>2015 IEEE International Conference on Computer Vision (ICCV</conf-name><conf-loc>Santiago, Chile</conf-loc><pub-id pub-id-type="doi">10.1109/ICCV.2015.515</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Robert</surname><given-names>L</given-names></name><name><surname>Pelletier</surname><given-names>J</given-names></name><name><surname>Dang</surname><given-names>WL</given-names></name><name><surname>Taddei</surname><given-names>F</given-names></name><name><surname>Wright</surname><given-names>A</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Robust growth of <italic>Escherichia coli</italic></article-title><source>Current Biology</source><volume>20</volume><fpage>1099</fpage><lpage>1103</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.04.045</pub-id><pub-id pub-id-type="pmid">20537537</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Zou</surname><given-names>K</given-names></name><name><surname>Brandman</surname><given-names>O</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Ouyang</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Molecular phenotyping of aging in single yeast cells using a novel microfluidic device</article-title><source>Aging Cell</source><volume>11</volume><fpage>599</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1111/j.1474-9726.2012.00821.x</pub-id><pub-id pub-id-type="pmid">22498653</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Zou</surname><given-names>K</given-names></name><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Brandman</surname><given-names>O</given-names></name><name><surname>Ouyang</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Single cell analysis of yeast replicative aging using a new generation of microfluidic device</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e48275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0048275</pub-id><pub-id pub-id-type="pmid">23144860</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79519.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Smal</surname><given-names>Ihor</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/018906e22</institution-id><institution>Erasmus University Medical Center</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.10.05.463175" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.05.463175"/></front-stub><body><p>In this work, the authors describe a novel method, based on deep learning, to analyze large numbers of yeast cells dividing in a controlled environment. The method builds on existing yeast cell trapping microfluidic devices that have been used for replicative lifespan assay. The authors demonstrate how an optimized microfluidic device can be coupled with deep learning methods to perform automatic cell division tracking and single cell trajectories quantification. The overall performance of the method is impressive: it allows to deal with large image datasets generated by timelapse microscopy several order of magnitudes faster than what manual annotation would require.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79519.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Smal</surname><given-names>Ihor</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/018906e22</institution-id><institution>Erasmus University Medical Center</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Hersen</surname><given-names>Pascal</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.05.463175">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.05.463175v4">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;DetecDiv, a generalist deep-learning platform for automated cell division tracking and survival analysis&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 4 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Carlos Isales as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Pascal Hersen (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>In addition to the suggestions made by all the reviewers, which we hope are easily addressable, the essential revisions would include (for the authors):</p><p>1) Comparison with more recent Inception model, suggested by Reviewer 1.</p><p>2) Application of the proposed techniques to mutants and media conditions with known effects on survival rates and show that the results qualitatively match the expectations, as well as answering if similar conclusions are also be acquired with traditional/manual analysis (Reviewer 1).</p><p>3) Results of training the whole CNN+LSTM architecture in an end-to-end fashion, not separating training for two networks (Reviewer 2).</p><p>4) Comparison with the straightforward prediction of actual division times using that CNN+LSTM combination (Reviewer 2).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. I understand that the study was performed from the point of an experimentalist who wants to obtain a functional tool. Still, I think the manuscript could be strengthened by including comparison with alternative choices for initial classification. For example, for the &quot;Inception&quot; CNN, two updates (Inception V2 and V3) have since been published by the authors. By testing different approaches, it may become apparent whether there is still (major) room for improvement.</p><p>2. The authors rigorously evaluate the performance of their approach compared to manual analysis, and even evaluate its performance on a broad range of data obtained with different experimental setups. To validate that the final, fully automated, pipeline can be used to extract meaningful biological information, they then apply it to mutants and media conditions with known effects on survival rates (Figure 2E and F) and show that the results qualitatively match the expectations. I think this could be further strengthened by comparing to what would be obtained with traditional 'manual' analysis. Would it be feasible to also create some ground-truth data for the mutants, to check whether the automated approach recapitulates not only qualitative but also quantitative effects? Similarly, a quantitative comparison to ground-truth analysis of Figure 5H and (at least parts of) Figure 6D would be good.</p><p>3. For the identification of the SEP, it is not entirely clear to me what the manual annotation for the ground-truth was based on. Is it only the duration of cell cycle phases? If so, would a simple 'algorithmic criteria' (such as a threshold cell cycle duration) recapitulating the manual choice then not achieve the same as the classifier?</p><p>4. To give a better assessment of the quality of the volume measurements based on semantic segmentation shown in Figure 4, it would be helpful to see the dependence of an object-wise accuracy as a function of the IoU threshold used to define correctly identified objects.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Even though the paper is well written, it does contain lots of information on all possible aspects of the study, which in my opinion slightly lacks the structure. Lots of ideas are presented, for example in terms of neural networks, but it reads like a report, where the authors just present chronologically how they developed the ideas. The same feeling is also about the experimental section, where experiments are just there to show what the pipeline can achieve and not to answer very well described initial research question(s). This is probably something which is difficult to address, but making some things more concise and more structured would help. Several ideas/descriptions/and statements are present in the main text (while describing those things) but then also in the discussion and methods. It is not exactly a repetition, which is good, but it is the same &quot;information&quot; that overlaps. For example the parts on the neural network are split between the Results and Method, and in both cases are very extensive.</p><p>Additionally to the above mentioned comments, a more interesting one is about the training of the CNN and LSTM. In the end the authors use both network and describe how they came to this decision, but why not show what the end-to-end training of the combined architecture CNN+LSTM would produce? It would be interesting how it compares to separate training, taking into account that having separate classification with a CNN and then do a kind of temporal analysis, independently from the first step, is suboptimal.</p><p>Another concern, which the authors can address is going back to prediction of actual division times. The authors argue in the beginning that &quot;We selected this classification scheme – namely, the prediction of the budding state of the cell – over the direct assessment of cell division or budding (e.g., &quot;division&quot; versus &quot;no division&quot;) because division and budding events can only be assessed by comparing successive frames, which is impossible using a classical CNN architecture dedicated to image classification, which takes a single frame as input. &quot;</p><p>It was a good explanation in the beginning, before the authors went to LSTMs but then, in the end, they arrived at CNN+LSTM architecture that can easily predict the division times. Indeed the performance might be worse (or not) compared to DetecDiv, but even in that case, it is good to have a benchmark and show what the performance of such straightforward approach is, so to have a sort of a baseline.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79519.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>In addition to the suggestions made by all the reviewers, which we hope are easily addressable, the essential revisions would include (for the authors):</p><p>1) Comparison with more recent Inception model, suggested by Reviewer 1.</p></disp-quote><p>To address this request, for the division counting model, we have added two variants of the inception model, namely the inception v3 (Szegedy et al., 2015) and the latest inception-resnet v2 (Szegedy et al., 2016), in comparison to the original inception model v1 that we used in the manuscript (also known as GoogleNet, see Szegedy et al., 2014). Based on the training set and training procedures used in our study, we haven’t found any improvement neither with inception v3 nor with the larger inception-resnet v2 (see new Figure 2 —figure supplement 4 in the revised version). The agreement between ground truth and prediction is actually slightly less good than with the original inception model. This could be due to a tendency of the bigger models to overfit more.</p><p>However, because these two neural networks are bigger than the original inception one, we observed a ~3-fold (respectively ~6-fold) increase in classification times, see Figure</p><p>2 —figure supplement 4J. This increase in computing times is also reported here: https://fr.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks. html.</p><p>Under these conditions, we think these more sophisticated models do not provide any substantial benefit for division counting yet are less practical to use for the developer and the end-user due to increased computing times. Nevertheless, these newer models are now fully compatible with the DetecDiv GUI and they could prove to be useful for future applications beyond the scope of this study.</p><disp-quote content-type="editor-comment"><p>2) Application of the proposed techniques to mutants and media conditions with known effects on survival rates and show that the results qualitatively match the expectations, as well as answering if similar conclusions are also be acquired with traditional/manual analysis (Reviewer 1).</p></disp-quote><p>We fully agree with the reviewer that this comparison is relevant to assess how well the model can deal with various genetic contexts that differ from the condition used for training. To answer this point, we have analyzed further the raw data to build ground truth (GT) datasets in the following contexts:</p><p>– Lifespan analysis: we have made a groundtruth (GT) with 35 cells for the <italic>fob1delta</italic> and <italic>sir2delta</italic> mutants and compared these manually annotated data to the network predictions – importantly, the network was only trained using WT data. The results indicate a very good agreement between prediction and GT, as displayed in the new Figure 2 —figure supplement 5A.</p><p>– We have manually annotated 35 cells grown in galactose to evaluate the agreement between GT and predictions using the same network as previously, which was trained on glucose data only. Here again, we observed a very good matching between the two, showing that the model can readily be applied to galactose data (and potentially other sugar sources) without further training, see new Figure 2 —figure supplement 5B-E.</p><p>– Stress survival analysis: we have made a GT with 35 cells to evaluate the time of death of cells exposed to 0.5mM H2O2 compared to the predictions of the classifier. The onset of cell death was evaluated by visual inspection by looking at the appearance (i.e., the refractive index, the presence of large vacuoles, etc.) of the cells. The comparison (i.e. the correlation between predicted death time and actual death time, and the confusion matrix for the time of death at the end of the experiment) is now displayed as a new Figure 6 —figure supplement 1. Here, again, there is a very good agreement between predictions and GT.</p><p>Overall, these new analyses suggest that the model is able to generalize very well when exposed to the new contexts that were investigated. Of note, the <italic>sir2delta</italic> mutant has a significantly bigger size than the WT strain, suggesting that the model is somewhat robust to cell size variations.</p><disp-quote content-type="editor-comment"><p>3) Results of training the whole CNN+LSTM architecture in an end-to-end fashion, not separating training for two networks (Reviewer 2).</p></disp-quote><p>To address the reviewer’s request, we have tried to train the CNN+LSTM in an end-to-end fashion. Yet, unfortunately, we have never been able to obtain an accuracy comparable to that obtained after separately training the CNN and the LSTM. Part of the reason was that training the network under these conditions was found to be very inefficient and demanding in terms of computing resources.</p><p>The LSTM training procedure uses feature vectors as inputs which are calculated only once for each sequence of images, based on the upstream CNN activations. In contrast, training the whole CNN+LSTM network uses image sequences as input that need to be processed by the whole architecture for each optimization step, hence is more intensive computationally. Also, because this method attempts to optimize CNN and LSTM layers weights within the same procedure, it is not clear how weights tuning is partitioned between the two parts of the network, knowing that they may have very different numbers of parameters and optimization requirements (the LSTM is naive, whereas the CNN is pre-trained).</p><p>Last, following a rapid survey of the literature, we point out that training video classification models based on such a CNN+LSTM architecture is usually performed using a sequential optimization of both parts of the network, as performed in our study.</p><disp-quote content-type="editor-comment"><p>4) Comparison with the straightforward prediction of actual division times using that CNN+LSTM combination (Reviewer 2).</p></disp-quote><p>We understand the point raised by the reviewer that an end-to-end training would alleviate the current limitation that we pointed out in the manuscript, namely, that the CNN cannot be trained to distinguish transitions between successive frames (i.e. onset of cell division or budding). However, as explained above, such an end-to-end training appeared to be quite unpractical, hence we did not proceed further to build a model that would attempt to distinguish ‘div’ vs ‘no div’ frames.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. I understand that the study was performed from the point of an experimentalist who wants to obtain a functional tool. Still, I think the manuscript could be strengthened by including comparison with alternative choices for initial classification. For example, for the &quot;Inception&quot; CNN, two updates (Inception V2 and V3) have since been published by the authors. By testing different approaches, it may become apparent whether there is still (major) room for improvement.</p></disp-quote><p>Please see above our detailed reply to the editor regarding this point.</p><disp-quote content-type="editor-comment"><p>2. The authors rigorously evaluate the performance of their approach compared to manual analysis, and even evaluate its performance on a broad range of data obtained with different experimental setups. To validate that the final, fully automated, pipeline can be used to extract meaningful biological information, they then apply it to mutants and media conditions with known effects on survival rates (Figure 2E and F) and show that the results qualitatively match the expectations. I think this could be further strengthened by comparing to what would be obtained with traditional 'manual' analysis. Would it be feasible to also create some ground-truth data for the mutants, to check whether the automated approach recapitulates not only qualitative but also quantitative effects? Similarly, a quantitative comparison to ground-truth analysis of Figure 5H and (at least parts of) Figure 6D would be good.</p></disp-quote><p>We have addressed this point above, see the point-by-point reply to the editor.</p><disp-quote content-type="editor-comment"><p>3. For the identification of the SEP, it is not entirely clear to me what the manual annotation for the ground-truth was based on. Is it only the duration of cell cycle phases? If so, would a simple 'algorithmic criteria' (such as a threshold cell cycle duration) recapitulating the manual choice then not achieve the same as the classifier?</p></disp-quote><p>Thanks for this suggestion. However, the main issue here is that the post-SEP period of the lifespan is characterized by large variability in cell cycle duration (see Fehmann et al., Cell Rep, 2013). Therefore, unfortunately, using simple thresholding to assess the position of the SEP may not be reliable, since a punctual long cell-cycle could be followed by another one that would fall below the threshold.</p><p>In this context, we have previously introduced a piecewise linear fit based on Chi2 minimization to determine the onset of the SEP (see Fehrmann et al., Cell Rep, 2013, see also Morlot et al., Cell Rep 2019), as shown in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> (taken from Morlot et al., 2019).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79519-sa2-fig1-v2.tif"/></fig><p>This method, which looks somewhat reliable by visual inspection, may still strongly mislocalize the position of the SEP on some occasions (e.g. short post-SEP period, erratic post-SEP divisions, etc.). Therefore, we thought that using an appropriate classifier may be useful to overcome these limitations. In this context, We have generated the groundtruth by inspecting the cell cycle frequency vs time plot for each cell, using the piecewise linear fit as a visual aid.Beyond the determination of the SEP, which is quite successful, we believe that more generally, this classifier provides an interesting application of an LSTM classifier to identify a specific transition in image sequence datasets. In the revised version, we have added a sentence to better justify the need to use such an approach.</p><disp-quote content-type="editor-comment"><p>4. To give a better assessment of the quality of the volume measurements based on semantic segmentation shown in Figure 4, it would be helpful to see the dependence of an object-wise accuracy as a function of the IoU threshold used to define correctly identified objects.</p></disp-quote><p>Unless we misunderstand this statement, we think that such analysis is already present in Figure 5 —figure supplement 2A for cell contours and Figure 5 —figure supplement 3A for nucleus contours. These two figures display how precision and recall vary as we increase the ‘cell’ (and ‘nucleus’, respectively) class assignment threshold on the [0.2; 0.95] interval. Based on this analysis, we have chosen the threshold that maximizes the F1-score (Panel B in the same figures). We agree with the reviewer that this benchmarking provides a useful evaluation of the robustness of the classifiers.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Even though the paper is well written, it does contain lots of information on all possible aspects of the study, which in my opinion slightly lacks the structure. Lots of ideas are presented, for example in terms of neural networks, but it reads like a report, where the authors just present chronologically how they developed the ideas. The same feeling is also about the experimental section, where experiments are just there to show what the pipeline can achieve and not to answer very well described initial research question(s). This is probably something which is difficult to address, but making some things more concise and more structured would help. Several ideas/descriptions/and statements are present in the main text (while describing those things) but then also in the discussion and methods. It is not exactly a repetition, which is good, but it is the same &quot;information&quot; that overlaps. For example the parts on the neural network are split between the Results and Method, and in both cases are very extensive.</p></disp-quote><p>In order to improve readability, we have made the following changes to the manuscript:</p><p>– We have moved some experimental/methodological details to the methods section whenever possible, in order to guide the reader through the main results of the paper. In this way, we have removed part of the redundancy pointed out by the reviewer.</p><p>– We have introduced one more section header to separate the hardware part from the description of the image processing pipeline.</p><p>– We have better justified the choices we made regarding the network architecture, and the procedures used throughout the paper.</p><p>We hope that these changes makes the revised manuscript easier to read.</p><disp-quote content-type="editor-comment"><p>Additionally to the above mentioned comments, a more interesting one is about the training of the CNN and LSTM. In the end the authors use both network and describe how they came to this decision, but why not show what the end-to-end training of the combined architecture CNN+LSTM would produce? It would be interesting how it compares to separate training, taking into account that having separate classification with a CNN and then do a kind of temporal analysis, independently from the first step, is suboptimal.</p></disp-quote><p>We thank the reviewer for this suggestion. We have addressed this point above in the reply to the editor (point #3).</p><disp-quote content-type="editor-comment"><p>Another concern, which the authors can address is going back to prediction of actual division times. The authors argue in the beginning that &quot;We selected this classification scheme – namely, the prediction of the budding state of the cell – over the direct assessment of cell division or budding (e.g., &quot;division&quot; versus &quot;no division&quot;) because division and budding events can only be assessed by comparing successive frames, which is impossible using a classical CNN architecture dedicated to image classification, which takes a single frame as input. &quot;</p><p>It was a good explanation in the beginning, before the authors went to LSTMs but then, in the end, they arrived at CNN+LSTM architecture that can easily predict the division times. Indeed the performance might be worse (or not) compared to DetecDiv, but even in that case, it is good to have a benchmark and show what the performance of such straightforward approach is, so to have a sort of a baseline.</p></disp-quote><p>We thank the reviewer for this suggestion. We provide a reply to this above in the reply to the editor (point #4).</p></body></sub-article></article>