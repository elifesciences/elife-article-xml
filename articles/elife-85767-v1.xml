<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">85767</article-id><article-id pub-id-type="doi">10.7554/eLife.85767</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Changing the incentive structure of social media platforms to halt the spread of misinformation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-301686"><name><surname>Globig</surname><given-names>Laura K</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0612-0594</contrib-id><email>laura.globig@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-302846"><name><surname>Holtz</surname><given-names>Nora</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-244099"><name><surname>Sharot</surname><given-names>Tali</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8384-6292</contrib-id><email>t.sharot@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Affective Brain Lab, Department of Experimental Psychology, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>The Max Planck UCL Centre for Computational Psychiatry and Ageing Research, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>06</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e85767</elocation-id><history><date date-type="received" iso-8601-date="2022-12-22"><day>22</day><month>12</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-04-21"><day>21</day><month>04</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-09-26"><day>26</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/26j8w"/></event></pub-history><permissions><copyright-statement>© 2023, Globig et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Globig et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-85767-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-85767-figures-v1.pdf"/><abstract><p>The powerful allure of social media platforms has been attributed to the human need for social rewards. Here, we demonstrate that the spread of misinformation on such platforms is facilitated by existing social ‘carrots’ (e.g., ‘likes’) and ‘sticks’ (e.g., ‘dislikes’) that are dissociated from the veracity of the information shared. Testing 951 participants over six experiments, we show that a slight change to the incentive structure of social media platforms, such that social rewards and punishments are contingent on information veracity, produces a considerable increase in the discernment of shared information. Namely, an increase in the proportion of true information shared relative to the proportion of false information shared. Computational modeling (i.e., drift-diffusion models) revealed the underlying mechanism of this effect is associated with an increase in the weight participants assign to evidence consistent with discerning behavior. The results offer evidence for an intervention that could be adopted to reduce misinformation spread, which in turn could reduce violence, vaccine hesitancy and political polarization, without reducing engagement.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>In recent years, the amount of untrue information, or ‘misinformation’, shared online has increased rapidly. This can have profound effects on society and has been linked to violence, political extremism, and resistance to climate action.</p><p>One reason for the spread of misinformation is the lack of incentives for users to share true content and avoid sharing false content. People tend to select actions that they believe will lead to positive feedback (‘carrots’) and try to avoid actions that lead to negative feedback (‘sticks’). On most social media sites, these carrots and sticks come in the form of ‘like’ and ‘dislike’ reactions, respectively. Stories that users think will attract ‘likes’ are most likely to be shared with other users. However, because the number of likes a post receives is not representative of how accurate it is, users share information even if they suspect it may not be accurate. As a result, misinformation can spread rapidly.</p><p>Measures aimed at slowing the spread of misinformation have been introduced to some social media sites, such as removing a few virulent spreaders of falsities and flagging misleading content. However, measures that change the incentive structure of sites so that positive and negative feedback is based on the trustworthiness of the information have not yet been explored.</p><p>To test this approach, Globig et al. set up a simulated social media site that included ‘trust’ and ‘distrust’ buttons, as well as the usual ‘like’ and ‘dislike’ options. The site featured up to one hundred news stories, half of which were untrue. More than 900 participants viewed the news posts and could react using the new buttons as well as repost the stories.</p><p>The experiment showed that participants used the ‘trust’ and ‘distrust’ buttons to differentiate between true and false posts more than the other options. As a result, to receive more ‘trust’ responses and less ‘distrust’ responses from other users, participants were more likely to repost true stories than false ones. This led to a large reduction in the amount of misinformation being spread. Computational modeling revealed that the participants were paying more attention to how reliable a news story appeared to be when deciding whether to repost it.</p><p>Globig et al. showed that adding buttons to highlight the trustworthiness of posts on social media sites reduces the spread of misinformation, without reducing user engagement. This measure could be easily incorporated into existing social media sites and could have a positive impact on issues that are often fuelled by misinformation, such as vaccine hesitancy and resistance to climate action.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>misinformation</kwd><kwd>trust</kwd><kwd>belief</kwd><kwd>reward</kwd><kwd>DDM</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>214268/Z/18/Z</award-id><principal-award-recipient><name><surname>Sharot</surname><given-names>Tali</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Offering users reaction buttons that convey reliability (e.g., ‘trust’, ‘distrust’) increases discernment and significantly reduces the spread of misinformation on a social media platform.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In recent years, the spread of misinformation online has skyrocketed, increasing polarization, racism and resistance to climate action and vaccines (<xref ref-type="bibr" rid="bib4">Barreto et al., 2021</xref>; <xref ref-type="bibr" rid="bib34">Rapp and Salovich, 2018</xref>; <xref ref-type="bibr" rid="bib43">Tsfati et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Van Bavel et al., 2021</xref>). Existing measures to halt the spread, such as flagging posts, have had limited impact (e.g., <xref ref-type="bibr" rid="bib9">Chan et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Grady et al., 2021</xref>; <xref ref-type="bibr" rid="bib24">Lees et al., 2022</xref>).</p><p>We hypothesize that the spread of misinformation on social media platforms is facilitated by the existing incentive structure of those platforms, where social rewards (in the form of ‘likes’ and ‘shares’) are dissociated from the veracity of the information (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, left panel, <xref ref-type="bibr" rid="bib39">Sharot, 2021</xref>). The rationale for this hypothesis is as follows: users can discern true from false content to a reasonable degree (<xref ref-type="bibr" rid="bib1">Allen et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Pennycook and Rand, 2019</xref>). Yet, because misinformation generates no less retweets and ‘likes’ than reliable information (<xref ref-type="bibr" rid="bib23">Lazer et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Vosoughi et al., 2018</xref>), and online behavior conforms to a reinforcement-learning model by which users are reacting to social rewards (<xref ref-type="bibr" rid="bib27">Lindström et al., 2021</xref>; <xref ref-type="bibr" rid="bib5">Brady et al., 2021</xref>) users have little reason to use their discernment to guide their sharing behavior. Thus, people will share misinformation even when they do not trust it (<xref ref-type="bibr" rid="bib31">Pennycook et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Ren et al., 2021</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Theoretical framework.</title><p>(<bold>a</bold>) The current incentive structure (blue) is such that the veracity of shared information is dissociated from rewards (‘carrots’) and punishments (‘sticks’). That is, true information and misinformation may lead to roughly equal number of rewards and punishments. An optimal incentive structure (orange) is such that sharing true information is rewarded with more ‘carrots’ than sharing misinformation, which in turn is penalized with more ‘sticks’ than true information. To create an optimal environment, an intervention is needed by which the number of rewards and punishments are directly associated with the veracity of information. (<bold>b</bold>) We test one such possible intervention (Experiment 1). In particular, we allow people to engage with posts using ‘trust’ reaction buttons and ‘distrust’ reaction buttons (orange). The rationale is that they will use these reactions to discern true from false information more so than ‘like’ and ‘dislike’ reaction buttons. (<bold>c</bold>) As a result, to obtain a greater number of ‘trust’ carrots and a smaller number of ‘distrust’ sticks in response to a post, people in the optimal environment (orange) will share more true than misinformation compared to those in the suboptimal environment which includes no feedback at all (gray), and those in an environment where the association between veracity of information and number of carrots and sticks is weak (blue). This second step is tested in Experiments 2 and 3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig1-v1.tif"/></fig><p>To halt the spread, an incentive structure is needed where ‘carrots’ and ‘sticks’ are directly associated with accuracy (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, right panel, <xref ref-type="bibr" rid="bib39">Sharot, 2021</xref>). Such a system will work with the natural human tendency to select actions that lead to the greatest reward and avoid those that lead to punishment (<xref ref-type="bibr" rid="bib40">Skinner, 1966</xref>). Scientists have tested different strategies to reduce the spread of misinformation, including educating people about fake news (<xref ref-type="bibr" rid="bib17">Guess et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Traberg et al., 2022</xref>), using a prompt to direct attention to accuracy (<xref ref-type="bibr" rid="bib22">Kozyreva et al., 2020</xref>; <xref ref-type="bibr" rid="bib31">Pennycook et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Pennycook et al., 2020</xref>) and limiting how widely a post can be shared (<xref ref-type="bibr" rid="bib21">Jackson et al., 2022</xref>). Surprisingly, possible interventions in which the incentive structure of social media platforms is altered to reduce misinformation have been overlooked.</p><p>Here, we test the efficacy of such a structure by slightly altering the engagement options offered to users. Specifically, we add an option to react to posts using ‘trust’ and ‘distrust’ buttons (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). We selected these buttons because trust by definition is related to veracity – it is defined as ‘a firm belief in the reliability, truth, ability, or strength of someone or something’ (Oxford Dictionary).</p><p>We hypothesize that (1) people will use the ‘trust’ and ‘distrust’ buttons to discern true from misinformation more so than the commonly existing engagement options (such as a ‘like’ button; <xref ref-type="fig" rid="fig1">Figure 1b</xref>, top panel). By ‘discernment’ we mean that true posts will receive more ‘trusts’ reactions than ‘distrusts’ reactions and vice versa for false posts. This will create an environment in which rewards (‘trusts’) and punishments (‘distrusts’) are more directly associated with the veracity of information. Thus, (2) when exposed to this environment, users will start sharing more true information and less false information in order to obtain more ‘trust’ carrots and fewer ‘distrust’ sticks (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, bottom panel). The new feedback options could both reinforce user behavior that generates trustworthy material and signal to others that the post is dependable.</p><p>We also test environments in which participants receive only ‘trusts’ (a different number of <italic>trust</italic> for different posts) or only ‘distrusts’ (a different number of <italic>distrust</italic> for different posts) to examine if and how the impact of small vs large positive feedback (‘trust’) on discernment differs from the impact of small vs large negative feedback (distrust’). It has been proposed that the possibility of reward is more likely to reinforce action than the possibility of punishment, while the possibility of punishment is more likely to reinforce inaction (<xref ref-type="bibr" rid="bib20">Guitart-Masip et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Guitart-Masip et al., 2011</xref>; <xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>). This may translate to a large number of ‘trust’ selectively increasing sharing of true information without decreasing sharing of misinformation and vice versa for large number of ‘distrust’. Further, being mindful of potential differences in sharing behavior across political parties (e.g., <xref ref-type="bibr" rid="bib16">Grinberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Guess et al., 2020</xref>) we test participants from both sides of the political divide.</p><p>To that end, over six experiments 951 participants engaged in simulated social media platforms where they encountered true and false information. In Experiment 1, we examined whether participants would use ‘trust’ and ‘distrust’ buttons to discern true from false information more so than existing ‘like’ and ‘dislike' buttons (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, replication: Experiment 4). In Experiments 2 and 3, we tested whether new groups of participants would share more true than false information in social media platforms that introduce real ‘trust’ and ‘distrust’ feedback from other participants (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, replication: Experiments 5 and 6). The intuition is that ‘trust’ and ‘distrust’ reactions will naturally be used to indicate veracity and thus provide a reward structure contingent on accuracy, thereby reducing the sharing of misinformation and generating a healthier information ecosystem. Using computational modeling we provide insights into the specific mechanism by which our intervention improves sharing discernment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Participants use ‘trust’ and ‘distrust’ buttons to discern true from false information (Experiment 1)</title><p>In a first step, we examined whether participants used ‘trust’ and ‘distrust’ reactions to discern true from false information more so than ‘like’ and ‘dislike’ reactions. In Experiment 1, participants saw 100 news posts taken from the fact-checking website Politifact (<ext-link ext-link-type="uri" xlink:href="https://www.politifact.com">https://www.politifact.com</ext-link>; see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Half of the posts were true, and half were false. Participants were given the opportunity to react to each post using ‘like’, ‘dislike’, ‘trust’, and ‘distrust’ reaction buttons. They could select as many buttons as they wished or none at all (skip). Five participants were excluded according to pre-determined criteria (see Materials and methods for details). Thus, 106 participants (52 Democrats, 54 Republican, <italic>M</italic><sub>age</sub> = 40.745, SD<sub>age</sub> ± 14.479; female = 54, male = 52) were included in the analysis. See <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for full instructions.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Task (Experiment 1).</title><p>Participants observed a series of 100 posts in random order (50 true, 50 false). Their task was to react using one or more of the ‘like’, ‘dislike’, ‘trust’, or ‘distrust’ buttons or to skip. The task was self-paced.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Instructions for Experiment 1.</title><p>At the start of the experiment participants received extensive instructions explaining the task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig2-figsupp1-v1.tif"/></fig></fig-group><p>We then examined whether participants used the different reaction buttons to discern true from false information. Discernment was calculated as follows, such that high numbers always indicate better discernment:</p><p>For ‘like’:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>For ‘dislike’:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>For ‘trust’:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>For ‘distrust’:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>With Prop indicating the proportion of that response out of all true posts, or out of all false posts, as indicated.</p><p>These discernment scores were calculated for each participant separately and then entered into a 2 (<italic>type of reaction</italic>: ‘trust’ and ‘distrust’/‘like’ and ‘dislike’) by 2 (<italic>valence of reaction</italic>: positive, i.e., ‘like’, ‘trust’/negative, i.e., ‘dislike’, ‘distrust’) within-subject analysis of variance (ANOVA). Political orientation was also added as a between-subject factor (Republican/Democrat), allowing for an interaction of political orientation and type of reaction to assess whether participants with differing political beliefs used the reaction buttons in different ways.</p><p>The results reveal that participants’ use of ‘(Dis)Trust’ reaction buttons (<italic>M</italic> = 0.127; SE = 0.007) was more discerning than their use of ‘(Dis)Like’ reaction buttons (<italic>M</italic> = 0.047; SE = 0.005; <italic>F</italic>(1,104) = 95.832, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.48, <xref ref-type="fig" rid="fig3">Figure 3</xref>). We additionally observed an effect of valence (<italic>F</italic>(1,105) = 17.33, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.14), with negatively valenced reaction buttons (e.g., ‘dislike’ and ‘distrust’, <italic>M</italic> = 0.095, SE = 0.007) being used in a more discerning manner than positively valenced reaction buttons (e.g., ‘like’ and ‘trust’, <italic>M</italic> = 0.087, SE = 0.005) and an effect of political orientation (<italic>F</italic>(1,104) = 25.262, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.2), with Democrats (<italic>M</italic> = 0.115, SE = 0.007) being more discerning than Republicans (<italic>M</italic> = 0.06, SE = 0.005). There was also an interaction of type of reaction and political orientation (<italic>F</italic>(1,104) = 24.084, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.19), which was characterized by Democrats showing greater discernment than Republicans in their use of ‘(Dis)Trust’ reaction buttons (<italic>F</italic>(1,104) = 33.592, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.24), but not in their use of ‘(Dis)Like’ reaction buttons (<italic>F</italic>(1,104) = 2.255, p = 0.136, partial <italic>η</italic><sup>2</sup> = 0.02). Importantly, however, both Democrats (<italic>F</italic>(1,51) = 93.376, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.65) and Republicans (<italic>F</italic>(1,53) = 14.715, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.22) used the ‘(Dis)Trust’ reaction buttons in a more discerning manner than the ‘(Dis)Like’ reaction buttons.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Participants use ‘trust’ and ‘distrust’ reactions to discern true from false information.</title><p><bold>‘</bold>Distrust’ and ‘trust’ reactions were used in a more discerning manner than ‘like’ and ‘dislike’ reactions. <italic>Y</italic> axis shows discernment between true and false posts. For positive reactions (e.g., ‘<italic>likes</italic>’ and ‘<italic>trusts</italic>’), discernment is equal to the proportion of positive reactions for true information minus false information, and vice versa for negative reactions (‘<italic>dislikes</italic>’ and ‘<italic>distrusts</italic>’). <italic>X</italic> axis shows reaction options. Data are plotted as box plots for each reaction button, in which horizontal lines indicate median values, boxes indicate 25/75% interquartile range and whiskers indicate 1.5 × interquartile range. Diamond shape indicates the mean discernment per reaction. Individuals’ mean discernment data are shown separately as gray dots. Symbols above each box plot indicate significance level compared to 0 using a t-test. N=106, ***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Participants’ use ‘(Dis)Trust’ buttons to discern true from false information (Experiment 4).</title><p>Experiment 4 is a replication of Experiment 1, in which participants (N=50) observe posts (half true half false) and can respond by clicking all, none or some of the following buttons: ‘like’, ‘dislike’, ‘trust’, and ‘distrust’ (see Materials and methods for details). <italic>Y</italic> axis shows discernment. For positive reactions (e.g., ‘<italic>likes</italic>’ and ‘<italic>trusts</italic>’) discernment is equal to the proportion of those reactions in response to true posts minus false posts, and vice versa for negative reactions (‘<italic>dislikes</italic>’ and ‘<italic>distrusts</italic>’). A 2 (<italic>type of reaction</italic>: ‘trust’ and ‘distrust’/‘like’ and ‘dislike’) by 2 (<italic>valence</italic>: positive, i.e., ‘like’, ‘trust’/negative, i.e., ‘dislike’, ‘distrust’) within-subject analysis of variance (ANOVA) revealed an effect of type of feedback (<italic>F</italic>(1,49) = 51.996, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.51): participants used ‘(Dis)Trust’ (<italic>M</italic> = 0.111; SE = 0.01) in a more discerning manner than ‘(Dis)Like’ (<italic>M</italic> = 0.03; SE = 0.006). There was also an effect of valence (<italic>F</italic>(1,49) = 7.147, p = 0.01, partial <italic>η</italic><sup>2</sup> = 0.13): participants used negative reactions (<italic>M</italic> = 0.082; SE = 0.011) in a more discerning manner than positive reactions (<italic>M</italic> = 0.06, SE = 0.006). Participants’ used all reaction buttons, except ‘dislike’, to discern between true and false posts (‘like’: <italic>M</italic> = 0.053; SE = 0.008; <italic>t</italic>(49) = 6.982, p &lt; 0.001, Cohen’s <italic>d</italic> = 0.987; ‘trust’: <italic>M</italic> = 0.066; SE = 0.01; <italic>t</italic>(49) = 6.641, p &lt; 0.001, Cohen’s <italic>d</italic> = 0.939; ‘dislike’: <italic>M</italic> = 0.007; SE = 0.008; <italic>t</italic>(49) = 0.883, p = 0.381, Cohen’s <italic>d</italic> = 0.125; ‘distrust’: <italic>M</italic> = 0.157; SE = 0.014; <italic>t</italic>(49) = 11.312, p &lt; 0.001, Cohen’s <italic>d</italic> = 1.6). Data are plotted as box plots for each reaction, in which horizontal lines indicate median values, boxes indicate 25/75% interquartile range, and whiskers indicate 1.5 × interquartile range. Diamond shape indicates the mean discernment per reaction. Individuals’ mean discernment data are shown separately as gray dots. Symbols above each box plot indicate significance level compared to 0 using a t-test. ***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig3-figsupp1-v1.tif"/></fig></fig-group><p>One-sample <italic>t</italic>-tests against zero further revealed that participants’ use of each reaction button discerned true from false information (‘like’: <italic>M</italic> = 0.06; SE = 0.006; <italic>t</italic>(105) = 10.483, p &lt; 0.001, Cohen’s <italic>d</italic> = 1.018; ‘trust’: <italic>M</italic> = 0.099; SE = 0.01; <italic>t</italic>(105) = 9.744, p &lt; 0.001, Cohen’s <italic>d</italic> = 0.946; ‘dislike’: <italic>M</italic> = 0.034; SE = 0.007; <italic>t</italic>(105) = 4.76, p &lt; 0.001, Cohen’s <italic>d</italic> = 0.462; ‘distrust’: <italic>M</italic> = 0.156; SE = 0.01; <italic>t</italic>(105) = 15.872, p &lt; 0.001, Cohen’s <italic>d</italic> = 1.542).</p><p>Thus far, we have shown that participants use ‘(Dis)Trust’ reaction buttons in a more discerning manner than ‘(Dis)Like’ reaction buttons. As social media platforms care about overall engagement not only its quality, we examined how frequently participants used the different reaction buttons. An ANOVA with the same specifications as above was conducted, but this time submitting frequency of reaction as the dependent variable. We found that participants used ‘(Dis)Trust’ reaction buttons more often than ‘(Dis)Like’ reaction buttons (percentage use of reaction out of all trials: ‘trust’: <italic>M</italic> = 28.057%; ‘distrust’: <italic>M</italic> = 34.085%; ‘like’: <italic>M</italic> = 18.604%; ‘dislike’: <italic>M</italic> = 23.745%; <italic>F</italic>(1,104) = 36.672, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.26). In addition, negative reaction buttons (‘distrust’ and ‘dislike’: <italic>M</italic> = 28.915%, SE = 1.177) were used more frequently than positive reaction buttons (‘trust’ and ‘like’: <italic>M</italic> = 23.33%, SE = 1.133; <italic>F</italic>(1,105) = 16.96, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.07). No other effect was significant. Interestingly, we also found that participants who skipped more posts were less discerning (<italic>R</italic> = −0.414, p &lt; 0.001). Together, the results show that the new reaction options increase engagement.</p><p>The results hold when controlling for demographics, when not including political orientation in the analysis, and allowing for an interaction between type of reaction and valence (see <xref ref-type="supplementary-material" rid="supp1 supp2">Supplementary files 1 and 2</xref>). The results also replicate in an independent sample (Experiment 4, see Materials and methods for details; and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p></sec><sec id="s2-2"><title>‘Trust’ and ‘distrust’ incentives improve discernment in sharing behavior (Experiment 2)</title><p>Thus far, we have shown that participants use ‘(Dis)Trust’ reaction buttons in a more discerning manner than ‘(Dis)Like’ reaction buttons. Thus, an environment which offers ‘(Dis)Trust’ feedback is one where the number of ‘carrots’ (in the form of ‘trusts’) and the number of ‘sticks’ (in the form of ‘distrusts’) are directly associated with the veracity of the posts. It then follows that submitting participants to such an environment will increase their sharing of true information (to receive ‘trusts’) and reduce their sharing of misinformation (to avoid ‘distrusts’).</p><p>To test this, we ran a second experiment. A new group of participants (<italic>N</italic> = 320) were recruited to engage in a simulated social media platform. They observed the same 100 posts (50 true, 50 false) shown to the participants in Experiment 1, but this time instead of reacting to the posts they could either share the post or skip it (see <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref> and <xref ref-type="fig" rid="fig4s2">2</xref> for full instructions). They were told that if they chose to share a post, it would be shared to their feed such that the other participants would be able to see the post and would then be able to react to it in real time (<italic>feedback</italic>). Depending on the environment participants were in, which varied between subjects, they could receive feedback in the form of the number of users who (1) ‘<italic>disliked</italic>’, (2) ‘<italic>liked</italic>’, (3) ‘<italic>distrusted</italic>’, or (4) ‘<italic>trusted</italic>’ their posts. We also included a (5) baseline condition, in which participants received no feedback. If participants selected to skip, they would observe a white screen asking them to click continue. Data of 32 participants were not analyzed according to pre-determined criteria (see Materials and methods for details). Two-hundred and eighty-eight participants (146 Democrats, 142 Republicans, <italic>M</italic><sub>age</sub> = 38.073, SD<sub>age</sub> ± 13.683; female = 147, male = 141) were included in the analysis (see Materials and methods for details).<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Task.</title><p>In Experiment 2 on each of 100 trials participants observed a post (50 true, 50 false content). They then chose whether to share it or skip (self-paced). They were told that if they chose to share a post, it would be shared to their feed such that other participants would be able to see the post and react to it in real time (<italic>feedback</italic>). Depending on the environment participants were in, they could either observe the number of (1) ‘<italic>dislikes</italic>’ (<italic>N</italic> = 45), (2) ‘<italic>likes</italic>’ (<italic>N</italic> = 89), (3) ‘<italic>distrusts</italic>’ (<italic>N</italic> = 49), or (4) ‘trusts’ (<italic>N</italic> = 46) feedback. The feedback was in fact the number of reactions gathered from Experiment 1, though the participants believed the reactions were in real time as indicated by a rotating cogwheel (1 s). Once the feedback appeared, participants could then click continue. If participants selected to skip, they would observe a white screen asking them to click continue (self-paced). In the <italic>Baseline</italic> environment (<italic>N</italic> = 59) participants received no feedback. Experiment 3 was identical to Experiment 2 with two distinctions: (1) Depending on the environment participants were in, they could either observe the number of (i) both ‘<italic>dislikes</italic>’ and ‘<italic>likes</italic>’ (<italic>N</italic> = 128), (ii) both ‘<italic>distrusts</italic>’ and ‘trusts’ (<italic>N</italic> = 137), or (iii) no feedback (Baseline, <italic>N</italic> = 126). (2) In Experiment 3, we selected 40 posts (20 true, 20 false) to which Republicans and Democrats had on average reacted to similarly using the ‘trust’ button in Experiment 1. Discernment was calculated for each participant by subtracting the proportion of sharing false information from the proportion of sharing true information. High discernment indicates greater sharing of true than false information.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Instructions for Experiment 2.</title><p>At the start of the experiment participants received extensive instructions explaining the task. Depending on which condition they were assigned to participants would receive Instructions for either the (<bold>a</bold>) Baseline environment, (<bold>b</bold>) Like environment, (<bold>c</bold>) Dislike environment, (<bold>d</bold>) Trust environment, or (<bold>e</bold>) Distrust environment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Instructions for Experiment 3.</title><p>At the start of the experiment participants received extensive instructions explaining the task. Depending on which condition they were assigned to participants would receive Instructions for either the (<bold>a</bold>) Baseline environment, (<bold>b</bold>) Like &amp; Dislike environment, or (<bold>c</bold>) Trust &amp; Distrust environment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig4-figsupp2-v1.tif"/></fig></fig-group><p>These scores were submitted into a between-subject ANOVA with <italic>type of feedback</italic> (‘trust’ and ‘distrust’/‘like’ and ‘dislike’/Baseline), <italic>valence</italic> (positive, i.e., ‘like’, ‘trust’/negative, i.e., ‘dislike’, ‘distrust’ vs neutral/no feedback) and political orientation (Republican/Democrat) as factors. We also allowed for an interaction of political orientation and type of feedback.</p><p>We observed an effect of type of feedback (<italic>F</italic>(1,281) = 15.2, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.051), such that participants shared more true than false information in the ‘(Dis)Trust’ environments (<italic>M</italic> = 0.18, SE = 0.018) than the ‘(Dis)Like’ environments (<italic>M</italic> = 0.085, SE = 0.019, <italic>F</italic>(1,225) = 14.249, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.06) and Baseline environment (<italic>M</italic> = 0.084, SE = 0.025; <italic>F</italic>(1,150) = 10.906, p = 0.001, partial <italic>η</italic><sup>2</sup> = 0.068, <xref ref-type="fig" rid="fig5">Figure 5a</xref>). Moreover, participants who received ‘trust’ feedback (<italic>M</italic> = 0.176, SE = 0.026) were more discerning in their sharing behavior than those who received ‘like’ feedback (<italic>M</italic> = 0.081, SE = 0.021, <italic>F</italic>(1,131) = 10.084, p = 0.002, partial <italic>η</italic><sup>2</sup> = 0.071). Those who received ‘distrust’ feedback (<italic>M</italic> = 0.175, SE = 0.026) were more discerning than those who received ‘dislike’ feedback (<italic>M</italic> = 0.092, SE = 0.039, <italic>F</italic>(1,90) = 5.003, p = 0.028, partial <italic>η</italic><sup>2</sup> = 0.053). We further observed a trend interaction between type of feedback and political orientation (<italic>F</italic>(1,281) = 2.939, p = 0.055, partial <italic>η</italic><sup>2</sup> = 0.02). While Democrats (<italic>M</italic> = 0.213; SE = 0.014) were generally more discerning than Republicans (<italic>M</italic> = 0.017; SE = 0.016; <italic>F</italic>(1,281) = 77.392, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.216), this difference was smaller in those who received ‘(Dis)Trust’ feedback (<italic>M</italic> = 0.082, SE = 0.034) compared to those who received ‘(Dis)Like’ feedback (<italic>M</italic> = 0.23, SE = 0.03; <italic>F</italic>(1,224) = 4.879, p = 0.028, partial <italic>η</italic><sup>2</sup> = 0.021) and by trend smaller than those who received no feedback (<italic>M</italic> = 0.229, SE = 0.045; <italic>F</italic>(1,149) = 3.774, p = 0.054, partial <italic>η</italic><sup>2</sup> = 0.025). There was no difference between the latter two (<italic>F</italic>(1,188) = 0.00, p = 0.988, partial <italic>η</italic><sup>2</sup> = 0.00). No other effects were significant. Overall engagement, measured as percentage of posts shared out of all trials, did not differ across environments (<italic>F</italic>(1,281) = 1.218, p = 0.271, partial <italic>η</italic><sup>2</sup>=0.004; Mean % posts shared out of all trials: Baseline = 27.712%; Dislike = 35.889%; Like = 33.258%; Distrust = 32.51%; Trust = 30.435%; see <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> for means for true and false posts).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Altering the incentive structure of social media environments increases discernment of information shared.</title><p>(<bold>a</bold>) Participants (N=288) operating in an environment where ‘(Dis)Trust’ feedback was introduced shared more true information relative to false information than participants operating in an environment where only ‘(Dis)Like’ feedback was available, or no feedback at all (Baseline) <italic>Y</italic> axis shows discernment, that is, proportion of true posts shared minus proportion of false posts shared. <italic>X</italic> axis shows the group environment (type of feedback). (<bold>b</bold>) This was the case regardless of the topic of the post (politics, science, health, environment, society, other). Bubble size corresponds to number of the posts included in the study. Diagonal dashed line indicates point of equivalence, where discernment in equal across the ‘(Dis)Like’ and ‘(Dis)Trust’ environments. As can be seen, all circles are above the dashed line indicating that in all cases discernment is greater in an environment that offers ‘(Dis)Trust’ feedback. <italic>Y</italic> axis shows discernment in the ‘(Dis)Trust’ environment, <italic>X</italic> axis shows discernment in the ‘(Dis)Like’ environment. (<bold>c</bold>) Experiment 3 (N=391) showed the same results as Experiment 2. Data are plotted as box plots for each reaction, in which horizontal lines indicate median values, boxes indicate 25/75% interquartile range, and whiskers indicate 1.5 × interquartile range. Diamond shape indicates the mean discernment per reaction. Individuals’ mean discernment data are shown separately as gray dots; symbols above each box plot indicate significance level compared to 0 using a t-test.***p &lt; 0.001, **p &lt; 0.01.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>‘(Dis)Trust’ feedback improves discernment in sharing behavior (Experiments 5 and 6).</title><p>(<bold>a</bold>) Experiment 5 and (<bold>b</bold>) Experiment 6 are replications of Experiments 2 and 3. (<bold>a</bold>) In Experiment 5 participants (N=261) observed the same 100 posts (50 true, 50 false) shown to participants in Experiment 4, but instead of reacting to the posts they could either share the post or skip it. Depending on their group, participants would then receive feedback from other participants in the form of the number of either (1) ‘<italic>dislikes</italic>’, or (2) ‘<italic>likes</italic>’, or (3) ‘<italic>distrusts</italic>’, or (4) ‘<italic>trusts</italic>’. The <italic>Baseline</italic> group received no feedback (see Materials and methods for details). On the <italic>Y</italic> axis is ‘Discernment’ = proportion of sharing true information − proportion of sharing false information. A between-subject analysis of variance (ANOVA) on discernment with <italic>type of feedback</italic> (‘trust’ and ‘distrust’/‘like’ and ‘dislike’/Baseline) and <italic>valence</italic> (positive, i.e., ‘like’, ‘trust’/negative, i.e., ‘dislike’, ‘distrust’ vs neutral/no feedback) revealed an effect of type of feedback (<italic>F</italic>(1,257) = 8.112, p = 0.005, partial <italic>η</italic><sup>2</sup> = 0.031): discernment was greater in the ‘(Dis)Trust’ conditions (<italic>M</italic> = 0.236, SE = 0.019) than the ‘(Dis)Like’ (<italic>M</italic> = 0.111, SE = 0.022; <italic>F</italic>(1,212) = 7.682, p = 0.006, partial <italic>η</italic><sup>2</sup>=0.035) and Baseline (<italic>M</italic> = 0.102, SE = 0.026; <italic>F</italic>(1,163) = 16.246, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.087) conditions. No other effects were significant. (<bold>b</bold>) Experiment 6 (N=150) was the same as Experiment 5, but with three groups: Baseline group (receiving no feedback), 'Trust &amp; Distrust’ group, in which participants received both the number of <italic>Trust</italic> and <italic>Distrust</italic> feedback, and a ‘Like &amp; Dislike’ environment, in which participants received the number of <italic>Like</italic> and <italic>Dislike</italic> feedback (see Materials and methods and for details). Once again, an ANOVA revealed a main effect of type of feedback (<italic>F</italic>(1,147) = 11.150, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.132): participants in the ‘Trust and Distrust’ group were more discerning (<italic>M</italic> = 0.264, SE = 0.023) than those in the ‘Like and Dislike’ (<italic>M</italic> = 0.147, SE = 0.027; <italic>F</italic>(1,101) = 11.122, p = 0.001, partial <italic>η</italic><sup>2</sup> = 0.099) and baseline (<italic>M</italic> = 0.106, SE = 0.026; <italic>F</italic>(1,101) = 21.141, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.173) groups. Data are plotted as box plots for each reaction, in which horizontal lines indicate median values, boxes indicate 25/75% interquartile range, and whiskers indicate 1.5 × interquartile range. Diamond shape indicates the mean discernment per reaction. Individuals’ mean discernment data are shown separately as gray dots; symbols above each box plot indicate significance level compared to 0 using a t-test . ***p &lt; 0.001, **p &lt; 0.01.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Results hold when controlling for demographics, when not including political orientation in the analysis, and allowing for an interaction between type of reaction and valence (see <xref ref-type="supplementary-material" rid="supp5 supp6">Supplementary files 5 and 6</xref>). Results replicate in an independent sample (Experiment 5, see Materials and methods for details; and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="supplementary-material" rid="supp7">Supplementary file 7</xref>).</p><p>To recap – participants in Experiment 2 decided whether to share content or skip. They then observed the reaction of other participants to their post (they believed this was happening in real time, but for simplicity we fed them reactions of participants from Experiment 1). Each participant in Experiment 2 observed only one type of feedback. For example, only ‘distrusts’. How is it that observing ‘distrusts’ alone increases discernment? The rationale behind this design is that for any given post, true or false, some users will distrust the post. However, true posts will receive fewer ‘distrusts’ than false posts. It is the number of ‘distrusts’ per post that matters. The participants are motivated to minimize the average number of ‘distrusts’ they receive. To achieve this, they should post more true posts and fewer false posts. Of course, if the participants were simply trying of minimize the <italic>total</italic> number of distrusts, they would just skip on every trial. Participants do not do that, however. Potentially because sharing in and of itself is rewarding (<xref ref-type="bibr" rid="bib41">Tamir and Mitchell, 2012</xref>). The results indicate that participants are sensitive to the number of ‘distrusts’ per posts not just to the total number of ‘distrusts’ over all posts.</p><p>The same rationale holds for the participants that only observe ‘trusts’. They receive more ‘trusts’ for true than false posts. It is the magnitude of ‘trusts’ that is associated with veracity. This motivates participants to post more true posts and fewer false posts in order to maximize the average number of ‘trusts’ per post. Of course, if participants were simply trying of maximize the total number of ‘trusts’, they would just share on every trial. Participants do not do that, however. This indicates that they are sensitive to the number of ‘trusts’ per post not just to total number over all posts. Any user of social media platforms could relate to this; when posting a tweet, for example, many people will be disappointed with only a handful of ‘hearts’. The user’s goal is to maximize positive feedback per post. The same rationale as above holds for ‘likes’ and ‘dislikes’ except that those are less associated with veracity, thus impact discernment less.</p><p>The posts included in the experiment covered a range of topics including politics, science, health, environment, and society. As observed in <xref ref-type="fig" rid="fig5">Figure 5b</xref>, the effect of ‘(Dis)Trust’ environment on discernment is observed regardless of content type.</p><p>Thus far, our results show that changing the incentive structure of social media platforms by coupling the number of ‘carrots’ and ‘sticks’ with information veracity could be a valuable tool to reduce the spread of misinformation. If feedback promotes discernment in sharing behavior, it is plausible that it may in turn improve belief accuracy. To test this, we asked participants at the end of the experiment to indicate how accurate they thought a post was on a scale from inaccurate (0) to accurate (100). Participants’ error in estimating whether a post was true or false was calculated as follows: for false posts error was equal to the participants’ accuracy rating and for true posts it was equal to 100 minus their rating. Participants’ average error scores were entered into a between-subject ANOVA with type of feedback and valence of feedback, as well as political orientation and its interaction with feedback type. We observed an effect of type of feedback (<italic>F</italic>(1,281) = 7.084, p = 0.008, partial <italic>η</italic><sup>2</sup> = 0.025), such that participants were more accurate (less errors) when they received ‘(Dis)Trust’ feedback (<italic>M</italic> = 47.24, SE = 0.938) compared to ‘(Dis)Like’ feedback (<italic>M</italic> = 50.553, SE = 0.851, <italic>F</italic>(1,224) = 7.024, p = 0.009, partial <italic>η</italic><sup>2</sup> = 0.03). We further observed an effect of political orientation (<italic>F</italic>(1,281) = 11.402, p &lt; 0.001, <italic>η</italic><sup>2</sup> = 0.039), with Democrats (<italic>M</italic> = 47.264, SE = 0.773) being more accurate than Republicans (<italic>M</italic> = 51.117, SE = 0.802). No other effects were significant. All results hold when controlling for demographics, when not including political orientation in the analysis, and allowing for an interaction between type of feedback and valence (see <xref ref-type="supplementary-material" rid="supp8">Supplementary file 8</xref>). We replicated these results in Experiment 5. We again, observed an effect of feedback type (<italic>F</italic>(1,258) = 4.179, p = 0.042, partial <italic>η</italic><sup>2</sup> = 0.016), such that participants were more accurate (less errors) when they received ‘(Dis)Trust’ feedback (<italic>M</italic> = 35.717, SE = 0.65) compared to ‘(Dis)Like’ feedback (<italic>M</italic> = 37.63, SE = 0.767; <italic>F</italic>(1,212) = 3.955, p = 0.048, partial <italic>η</italic><sup>2</sup> = 0.018) and also more accurate than those who received no feedback (Baseline, <italic>M</italic> = 39.73, SE = 0.886; <italic>F</italic>(1,162) = 11.759, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.068). No other effects were significant. These results hold when allowing for an interaction between type of feedback and valence (see <xref ref-type="supplementary-material" rid="supp9">Supplementary file 9</xref>).</p></sec><sec id="s2-3"><title>‘Trust’ and ‘distrust’ incentives together improve discernment in sharing behavior (Experiment 3)</title><p>Given that Experiment 2 revealed that receiving ‘trust’ or ‘distrust’ feedback separately improves discernment, it is likely that the coupled presentation of both will jointly also improve discernment. To test this, we ran a third experiment with a new group of participants. The task was identical to Experiment 2 (see <xref ref-type="fig" rid="fig4">Figure 4</xref>), but this time we included three between-subject environments: a <italic>Baseline</italic> environment, in which participants received no feedback, a ‘Trust &amp; Distrust’ environment, in which participants observed both the number of <italic>trust</italic> and the number of <italic>distrust</italic> feedback, and a ‘Like &amp; Dislike’ environment, in which participants observed both the number of <italic>like</italic> and the number of <italic>dislike</italic> feedback.</p><p>Additionally, to ensure posts align equally with Democratic and Republican beliefs, in Experiment 3 we selected 40 posts (20 true, 20 false) in response to which Republicans and Democrats utilized the ‘trust’ button in a similar manner in Experiment 1 (see Materials and methods). Data of 18 participants were not analyzed according to pre-determined criteria (see Materials and methods for details). Analysis of Experiment 3 (<italic>N</italic> = 391, 194 Democrats, 197 Republican, <italic>M</italic><sub>age</sub> = 35.304, SD<sub>age</sub> ± 11.089; female = 196, male = 192, other = 3) was the same as in Experiment 2 except that there were three environments (Baseline, ‘Like &amp; Dislike’, and ‘Trust &amp; Distrust’) and no valence of feedback, because all environments either include both positive and negative feedbacks or no feedback.</p><p>Discernment was submitted to a between-subject ANOVA with <italic>type of feedback</italic> (Baseline/‘Like &amp; Dislike’/‘Trust &amp; Distrust’), political orientation and their interaction as factors. Again, we observed an effect of type of feedback (<italic>F</italic>(1,385) = 11.009, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.054, <xref ref-type="fig" rid="fig5">Figure 5c</xref>), with participants in the ‘Trust &amp; Distrust’ feedback group posting more true relative to false information (<italic>M</italic> = 0.101, SE = 0.015) than those in the ‘Like &amp; Dislike’ group (<italic>M</italic> = 0.042, SE = 0.013; <italic>F</italic>(1,261) = 8.478, p = 0.00, partial <italic>η</italic><sup>2</sup> = 0.031) or those who received no feedback at all (<italic>M</italic> = 0.008, SE = 0.014, <italic>F</italic>(1,259) = 20.142, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.0724). By contrast there was no difference between the latter two groups (<italic>F</italic>(1,250) = 2.981, p = 0.085, partial <italic>η</italic><sup>2</sup> = 0.012). As observed in Experiment 2, Democrats (<italic>M</italic> = 0.073, SE = 0.011) were more discerning than Republicans (<italic>M</italic> = 0.031, SE = 0.012; <italic>F</italic>(1,385) = 6.409, p = 0.012, partial <italic>η</italic><sup>2</sup> = 0.016). No other effects were significant.</p><p>Interestingly participants shared more frequently in the ‘Trust &amp; Distrust’ environment compared to the other two environments (% of all trials: ‘Trust &amp; Distrust’ = 36.2%, ‘Like &amp; Dislike’ = 30.41%; Baseline = 25.853%; <italic>F</italic>(1,385) = 8.7692, p &lt; 0.001, partial <italic>η</italic><sup>2</sup> = 0.044). This illustrates that ‘(Dis)Trust’ feedback improves discernment without reducing engagement. No other effects were significant.</p><p>All results hold when controlling for demographics, when not including political orientation in the analysis, and allowing for an interaction between type of reaction and valence (see <xref ref-type="supplementary-material" rid="supp10 supp11">Supplementary files 10 and 11</xref>). Results replicate in an independent sample (Experiment 6, see Materials and methods for details; and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>At the end of Experiment 3, we again asked participants to indicate how accurate they thought a post was. Participants’ average error scores were calculated as in Experiment 2 and entered into a between-subject ANOVA with type of feedback, political orientation and their interaction as factors. Democrats (<italic>M</italic> = 40.591; SE = 6.371) were more accurate than Republicans (<italic>M</italic> = 42.056; SE = 5.633; <italic>F</italic>(1,385) = 5.723, p = 0.017, partial <italic>η</italic><sup>2</sup> = 0.015). No other effects were significant (for results controlling for demographics not including political orientation see <xref ref-type="supplementary-material" rid="supp12">Supplementary file 12</xref>). Note, that in the replication study (Experiment 6) we did observe an effect of type of feedback (<italic>F</italic>(1,147) = 4.596, p = 0.012, partial <italic>η</italic><sup>2</sup> = 0.059), with ‘(Dis)Trust condition being most accurate. Thus, we see accuracy effects in three (Experiments 2, 5, and 6) out of our four experiments.</p><p>Taken together these findings suggest that changing the incentive structure of social media platforms, such that ‘carrots’ and ‘sticks’ are strongly associated with veracity promotes discernment in sharing behavior, thereby reducing the spread of misinformation.</p></sec><sec id="s2-4"><title>‘(Dis)Trust’ incentives improve discernment in sharing behavior by increasing the relative importance of evidence consistent with discerning behavior</title><p>Next, we set out to characterize the mechanism by which the new incentive structure increased discernment. Imagine you observe a post on social media, and you need to decide whether to share it – how do you make this decision? First, you examine the post. Second, you retrieve existing knowledge. For example, you may think about what you already know about the topic, what you heard others say, you may try to estimate how others will react to the post if you share it, and so on. This process is called ‘evidence accumulation’ – you gradually accumulate and integrate external evidence and internal evidence (memories, preferences, etc.) to decide. Some of the evidence you retrieve will push you toward a ‘good’ response that promotes veracity (i.e., posting a true post and skipping a false post) and some will push you toward a ‘bad’ response that obstructs veracity (i.e., posting a false post and skipping a true post). We can think of the evidence that pushes you toward a response that promotes veracity as ‘signal’. Using computational modeling it is possible to estimate how much a participant is influenced (‘pushed’) by signal relative to noise, by calculating a parameter known as a ‘drift rate’ in a class of models known as drift-diffusion models (DDM). One possibility then is that in the ‘(Dis)Trust’ environment evidence toward responses that promote veracity is given more weight than toward responses that obstruct veracity (i.e., the drift rate is larger), thus people make more discerning decisions.</p><p>Another, non-exclusive possibility, is that in the ‘(Dis)Trust’ environment participants are more careful about their decisions. They require more evidence before making a decision. For example, they may spend more time deliberating the post. In DDM, this is estimated by calculating what is known as the distance between the decision thresholds (i.e., how much total distance do I need to be ‘pushed’ in one direction or the other to finally make a choice).</p><p>To test the above possible mechanisms, we modeled our data using the DDM (<xref ref-type="bibr" rid="bib35">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib36">Ratcliff and McKoon, 2008</xref>, see also <xref ref-type="bibr" rid="bib26">Lin et al., 2023</xref>). We modeled participants’ responses (‘veracity-promoting’ vs ‘veracity-obstructing’ choice) separately for each type of feedback (‘(Dis)Trust’, ‘(Dis)Like’, Baseline) and each experiment (Experiments 2 and 3). The following parameters were included: (1) <italic>t</italic>(0) – the amount of non-decision time, capturing encoding and motor response time; (2) <italic>α</italic> – the distance between decision thresholds (‘veracity-promoting’ response vs ‘veracity-obstructing’ response); (3) <italic>z</italic> – starting point of the accumulation process; and (4) <italic>v</italic> – the drift rate (see Materials and methods for details; <xref ref-type="bibr" rid="bib35">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib36">Ratcliff and McKoon, 2008</xref>; <xref ref-type="bibr" rid="bib46">Voss et al., 2013</xref>).</p><p>We next examined which of the parameters were different in the different environments (see <xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref>, and <xref ref-type="supplementary-material" rid="supp13 supp14">Supplementary files 13 and 14</xref> for highest density interval [HDI] comparisons). To that end, we calculated the difference in posterior distributions of each parameter for each pair of incentive environments (‘(Dis)Trust’ vs ‘(Dis)Like’, ‘(Dis)Trust’ vs Baseline, ‘(Dis)Like’ vs Baseline) and report the 95% HDI of the difference. If the 95% HDI of the distribution does not overlap with zero, we infer a credible difference between the two incentive environments.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Group estimates for drift-diffusion model (DDM) in Experiment 2.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">Baseline</th><th align="left" valign="bottom">‘(Dis)Like’</th><th align="left" valign="bottom">‘(Dis)Trust’</th></tr></thead><tbody><tr><td align="left" valign="bottom">Distance between decision thresholds (<italic>α</italic>)</td><td align="left" valign="bottom">2.153<break/>95% CI [2.09; 2.214]</td><td align="left" valign="bottom">2.373<break/>95% CI [2.281; 2.466]</td><td align="left" valign="bottom">2.403<break/>95% CI [2.280; 2.529]</td></tr><tr><td align="left" valign="bottom">Non-decision time (<italic>t</italic>0)</td><td align="left" valign="bottom">7.025<break/>95% CI [6.898; 7.154]</td><td align="left" valign="bottom">6.936<break/>95% CI [6.802; 7.071]</td><td align="left" valign="bottom">6.681<break/>95% CI [6.425; 6.94]</td></tr><tr><td align="left" valign="bottom">Starting point (<italic>z</italic>)</td><td align="left" valign="bottom">0.497<break/>95% CI [0.486; 0.508]</td><td align="left" valign="bottom">0.491<break/>95% CI [0.483; 0.50]</td><td align="left" valign="bottom">0.48<break/>95% CI [0.471; 0.49]</td></tr><tr><td align="left" valign="bottom">Drift rate (<italic>v</italic>)</td><td align="left" valign="bottom">0.098<break/>95% CI [0.039; 0.158]</td><td align="left" valign="bottom">0.10<break/>95% CI [0.056; 0.145]</td><td align="left" valign="bottom">0.216<break/>95% CI [0.17; 0.262]</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Group estimates for drift-diffusion model (DDM) in Experiment 3.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Estimate</th><th align="left" valign="top">Baseline</th><th align="left" valign="top">‘(Dis)Like’</th><th align="left" valign="top">‘(Dis)Trust’</th></tr></thead><tbody><tr><td align="left" valign="top">Distance between decision thresholds (<italic>α</italic>)</td><td align="left" valign="top">2.238<break/>95% CI [2.153; 2.328]</td><td align="left" valign="top">2.207<break/>95% CI [2.132; 2.286]</td><td align="left" valign="top">2.209<break/>95% CI [2.134; 2.286]</td></tr><tr><td align="left" valign="top">Non-decision time (<italic>t</italic>0)</td><td align="left" valign="top">6.9<break/>95% CI [6.762; 7.04]</td><td align="left" valign="top">7.051<break/>95% CI [6.918; 7.186]</td><td align="left" valign="top">7.076<break/>95% CI [6.944; 7.208]</td></tr><tr><td align="left" valign="top">Starting point (<italic>z</italic>)</td><td align="left" valign="top">0.5<break/>95% CI [0.49; 0.51]</td><td align="left" valign="top">0.5<break/>95% CI [0.49; 0.511]</td><td align="left" valign="top">0.489<break/>95% CI [0.476; 0.5]</td></tr><tr><td align="left" valign="top">Drift rate (<italic>v</italic>)</td><td align="left" valign="top">0.006<break/>95% CI [−0.027; 0.037]</td><td align="left" valign="top">0.037<break/>95% CI [0.002; 0.069]</td><td align="left" valign="top">0.12<break/>95% CI [0.086; 0.155]</td></tr></tbody></table></table-wrap><p>For both Experiment 2 (see <xref ref-type="fig" rid="fig6">Figure 6a</xref>) and Experiment 3 (see <xref ref-type="fig" rid="fig6">Figure 6c</xref>) we observed a meaningful difference in the drift rate. In particular, in the ‘(Dis)Trust’ environments the drift rate was larger (Experiment 2: <italic>v</italic> = 0.216; Experiment 3: <italic>v</italic> = 0.12) than in the‘(Dis)Like’ environments (Experiment 2: <italic>v</italic> = 0.01; 95% HDI of difference [0.048; 0.183], Experiment 3: <italic>v</italic> = 0.037; 95% HDI of difference [0.032; 0.135]) or no feedback environment (Experiment 2: <italic>v</italic> = 0.098; 95% HDI of difference [0.041; 0.195]; Experiment 3: <italic>v</italic> = 0.006; 95% HDI of difference [0.061; 0.167]). The Baseline and ‘(Dis)Like’ environments did not differ for drift rate (Experiment 2: 95% HDI of difference: [−0.075; 0.08]; Experiment 3: 95% HDI of difference [−0.016; 0.079]). This suggests that relative to the other environments, in the ‘(Dis)Trust’ environments evidence consistent with a ‘veracity-promoting’ response is weighted more than ‘evidence’ consistent with a ‘veracity-obstructing’ response. We replicate these results in Experiments 5 and 6 (see <xref ref-type="supplementary-material" rid="supp15 supp16 supp17 supp18">Supplementary files 15–18</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>‘(Dis)Trust’ feedback increases the drift rate.</title><p>Displayed are the posterior distributions of parameter estimates for the Baseline environment, the ‘(Dis)Like’ environment and the ‘(Dis)Trust’ environment. Dashed vertical lines indicate respective group means. In both (<bold>a</bold>) Experiment 2 (N=288) and (<bold>c</bold>) Experiment 3 (N=391) highest density interval (HDI) comparison revealed that participants had a larger drift rate in the ‘(Dis)Trust’ environments than in the other environments. No credible difference was observed between the latter two environments. Recovered model parameter estimates reproduced experimental results for both (<bold>b</bold>) Experiment 2 and (<bold>d</bold>) Experiment 3. * indicates credible difference between environments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig6-v1.tif"/></fig><p>While in Experiment 2 the decision threshold in the Baseline environment was lower than the other two environments, and non-decision time (t0) higher than in the ‘(Dis)Trust’, these differences are not replicated in Experiment 3. More importantly, neither decision threshold nor non-decision time differed between the ‘(Dis)Trust’ and ‘(Dis)Like’ environments (see <xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref> and <xref ref-type="supplementary-material" rid="supp13 supp14">Supplementary files 13 and 14</xref> for HDI comparisons).</p><p>Model parameters could be successfully recovered with data simulated using group-level parameters from Experiments 2 and 3 separately (for details, see Materials and methods, see <xref ref-type="fig" rid="fig6">Figure 6b, d</xref>, <xref ref-type="supplementary-material" rid="supp19 supp20">Supplementary files 19 and 20</xref>). This was done by fitting the model to simulated data, in the same way as for the experimental data. We sampled 2000 times from the posteriors, discarding the first 500 as burn in. The same pattern of results was reproduced with the simulated data as with real participants’ data (<xref ref-type="fig" rid="fig7">Figure 7</xref>). For each experiment, we ran two separate one-way ANOVAs to assess the effect of type of feedback on discernment: one for the real data and one for the simulated data. We remind the reader that we entered responses into our DDM as either ‘veracity-promoting’ (true post shared or false post skipped) or ‘veracity-obstructing’ (false post shared or true post skipped). Thus, discernment here is calculated as:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Which is equal to:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>As expected, we observed an effect of type of feedback in both the simulated (Experiment 2: <italic>F</italic>(1,285) = 3.795, p = 0.024, <italic>η</italic><sup>2</sup> = 0.026; Experiment 3: <italic>F</italic>(1,388) = 7.843, p = 0.001, <italic>η</italic><sup>2</sup> = 0.039, <xref ref-type="fig" rid="fig7">Figure 7b, d</xref>), and the experimental data (Experiment 2: <italic>F</italic>(1,287) = 7.049, p = 0.001, <italic>η</italic><sup>2</sup> = 0.047; Experiment 3: <italic>F</italic>(1,388) = 11.166, p &lt; 0.001, <italic>η</italic><sup>2</sup> = 0.054). That is, discernment was higher in ‘(Dis)Trust’ environments relative to ‘(Dis)Like’ environments or no feedback environments (<xref ref-type="fig" rid="fig7">Figure 7a, c</xref>, see <xref ref-type="supplementary-material" rid="supp21 supp22">Supplementary files 21 and 22</xref> for pairwise comparisons and <xref ref-type="supplementary-material" rid="supp23">Supplementary file 23</xref> for correlations between real and recovered individual-level parameters).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Simulated data reproduced experimental findings.</title><p>One-way ANOVAs revealed that In both (<bold>a</bold>) Experiment 2 (N=288) and (<bold>c</bold>) Experiment 3 (N=391) participants who received ‘(Dis)Trust’ feedback were more discerning than participants in the ‘(Dis)Like’ and Baseline environments. Simulated data reproduced these findings (<bold>b, d</bold>). <italic>Y</italic> axis shows discernment, that is, proportion of true posts shared and false posts skipped minus the proportion of true posts skipped and false posts shared. <italic>X</italic> axis shows feedback group. Data are plotted as box plots for each reaction, in which horizontal lines indicate median values, boxes indicate 25/75% interquartile range and whiskers indicate 1.5 × interquartile range. Diamond shape indicates the mean discernment per reaction. Individuals’ mean discernment data are shown separately as gray dots; symbols above each box plot indicate significance level compared to 0 using a t-test. ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85767-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we created a novel incentive structure that significantly reduced the spread of misinformation and provide insights into the cognitive mechanisms that make it work. This structure can be adopted by social media platforms at no cost. The key was to offer reaction buttons (social ‘carrots’ and ‘sticks’) that participants were likely to use in a way that discerned between true and false information. Users who found themselves in such an environment, shared more true than false posts in order to receive more ‘carrots’ and less ‘sticks’.</p><p>In particular, we offered ‘trust’ and ‘distrust’ reaction buttons, which in contrast to ‘likes’ and ‘dislikes’, are by definition associated with veracity. For example, a person may dislike a post about Joe Biden winning the election, however this does not necessarily mean that they think it is untrue. Indeed, in our study participants used ‘distrust’ and ‘trust’ reaction buttons in a more discerning manner than ‘dislike’ and ‘like’ reaction buttons. This created an environment in which the number of social rewards (‘carrots’) and punishments (‘sticks’) were strongly associated with the veracity of the information shared. Participants who were submitted to this new environment were more discerning in their sharing behavior compared to those in traditional environments who saw either no feedback or ‘dislike’ and/or ‘like’ feedback. The result was a reduction in sharing of misinformation without a reduction in overall engagement. All the effects were replicated and effect sizes of misinformation reduction were large to medium.</p><p>Using computational modeling we were able to pin-point the changes to participants’ decision-making process. In particular, drift-diffusion modeling revealed that participants in the new environment assigned more weight to evidence consistent with discerning than non-discerning behavior relative to traditional environments. In other words, the possibility of receiving rewards that are consistent with accuracy led to an increase in the weight participants assigned to accuracy-consistent evidence when making a decision. ‘Evidence’ likely includes external information that can influence the decision to share a post (such as the text and photo associated with the post) as well as internal information (e.g., retrieval of associated knowledge and memories).</p><p>Our results held when the potential feedback was only negative (‘distrust’), only positive (‘trust’), or both (‘trust’ and ‘distrust’). While negative reaction buttons were used in a more discerning manner and more frequently than positive reaction buttons, we did not find evidence for a differential strength of positively or negatively valenced feedback on discernment of sharing behavior itself.</p><p>The findings also held across a wide range of different topics (e.g., politics, health, science, etc.) and a diverse sample of participants, suggesting that the intervention is not limited to a set group of topics or users, but instead relies more broadly on the underlying mechanism of associating veracity and social rewards. Indeed, we speculate that these findings would hold for different ‘carrots’ and ‘sticks’ (beyond ‘trust’ and ‘distrust’), as long as people use these ‘carrots’ and ‘sticks’ to reward true information and punish false information. However, we speculate that the incentives were especially powerful due to being provided by fellow users and easily quantifiable (just as existing buttons including ‘like’ and ‘heart’). This may contrast with incentives which are either provided by the platform itself and/or not clearly quantified such as verification marks (<xref ref-type="bibr" rid="bib10">Edgerly and Vraga, 2019</xref>) or flagging false news (<xref ref-type="bibr" rid="bib6">Brashier et al., 2021</xref>; <xref ref-type="bibr" rid="bib9">Chan et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Grady et al., 2021</xref>; <xref ref-type="bibr" rid="bib24">Lees et al., 2022</xref>). Interestingly, a trust button has also been shown to increase sharing of private information (<xref ref-type="bibr" rid="bib3">Bălău and Utz, 2016</xref>).</p><p>Finally, we observed that feedback not only promotes discernment in sharing behavior but may also increase the accuracy of beliefs. Though, while we see an increase in accuracy of beliefs in three of the four experiments, we did not observe this effect in Experiment 3. Thus, the new incentive structure reduces the spread of misinformation and may help in correcting false beliefs. It does so without drastically diverging from the existing incentive structure of social media networks by relying on user engagement. Thus, this intervention may be a powerful addition to existing intervention such as educating users on how to detect misinformation (e.g., <xref ref-type="bibr" rid="bib25">Lewandowsky and van der Linden, 2021</xref>; <xref ref-type="bibr" rid="bib28">Maertens et al., 2021</xref>; <xref ref-type="bibr" rid="bib33">Pilditch et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Roozenbeek and van der Linden, 2019</xref>; <xref ref-type="bibr" rid="bib42">Traberg et al., 2022</xref>) or prompting users to think about accuracy before they engage in the platform (e.g., <xref ref-type="bibr" rid="bib7">Capraro and Celadin, 2022</xref>; <xref ref-type="bibr" rid="bib12">Fazio, 2020</xref>; <xref ref-type="bibr" rid="bib32">Pennycook and Rand, 2022</xref>). Over time, these incentives may help users build better habits online (e.g., <xref ref-type="bibr" rid="bib2">Anderson and Wood, 2021</xref>; <xref ref-type="bibr" rid="bib8">Ceylan et al., 2023</xref>).</p><p>As real-world platforms are in the hands of private entities, studying changes to existing platforms requires testing simulated platforms. The advantage of this approach is the ability to carefully isolate the effects of different factors. However, ‘real world’ networks are more complex and involve additional features which may interact with the tested factors. Our hope is that the science described here will eventually impact how privately owned platforms are designed, which will reveal whether the basic mechanisms reported here hold in more complex scenarios.</p><p>This study lays the groundwork for integration of the new incentive structure into existing (and future) social media platforms to further test the external validity of the findings. Rather than removing existing forms of engagement, we suggest an addition that complements the existing system and could be adopted by social media platforms at no cost. The new structure could subsequentially help reduce violence, vaccine hesitancy and political polarization, without reducing user engagement.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental design</title><sec id="s4-1-1"><title>Power calculations</title><p>Sample sizes for all experiments were computed based on our pilot study (see Experiments 4–6). Power calculations were performed using g*Power (<xref ref-type="bibr" rid="bib11">Faul et al., 2009</xref>) to achieve power of 0.8 (beta = 0.2, alpha = 0.05; Experiment 1: partial <italic>η</italic><sup>2</sup> = 0.51; Experiment 2: Cohen’s <italic>d</italic> = 0.33; Experiment 3: Cohen’s <italic>d</italic> = 0.327).</p></sec><sec id="s4-1-2"><title>Participants (Experiment 1)</title><p>One-hundred and eleven participants residing in the USA completed the task on <italic>Prolific Academic</italic>. Exclusion criteria were pre-established. Data of four participants who failed more than two memory checks were excluded from further analysis (see below). Thus, data of 107 participants were analyzed (52 Democrats, 54 Republican, 1 Other, <italic>M</italic><sub>age</sub> = 40.579, SD<sub>age</sub> ± 14.512; female = 55, male = 52; Non-White = 20, White = 87). Participants received £7.50 per hour for their participation in addition to a memory test performance-related bonus. For all experiments presented in this article, ethical approval was provided by the Research Ethics Committee at University College London (#3990/003) and all participants gave informed consent. All experiments were performed in accordance with the principles expressed in the Declaration of Helsinki. All samples were politically balanced for Democrats and Republicans. All experiments were replicated using biological replicates (Experiments 4–6).</p></sec><sec id="s4-1-3"><title>Participants (Experiment 2)</title><p>Three-hundred and twenty participants completed the task on <italic>Prolific Academic</italic>. Data of four participants who failed more than two memory checks were excluded from further analysis (see below for details). Thus, data of 316 participants were analyzed (146 Democrats, 142 Republican, 28 Other, <italic>M</italic><sub>age</sub> = 37.598, SD<sub>age</sub> ± 13.60; female = 157, male = 157, other = 2, Non-White = 77, White = 239). Participants received £7.50 per hour for the participation in addition to a memory test performance-related bonus.</p></sec><sec id="s4-1-4"><title>Participants (Experiment 3)</title><p>Four-hundred and nine participants completed the task on <italic>Prolific Academic</italic>. Data of three participants who failed more than two memory checks were excluded from further analysis (see Participants Experiment 1 for details). Further data of three participants who suspected that the feedback provided did not stem from real participants were excluded. Thus, data of four-hundred and three participants were analyzed (194 Democrats, 197 Republican, 12 Other, <italic>M</italic><sub>age</sub> = 35.179, SD<sub>age</sub> ± 11.051; female = 204, male = 194, other = 4, Non-White = 85, White = 218). Participants received £7.50 per hour for their participation in addition to a memory test performance-related bonus.</p></sec><sec id="s4-1-5"><title>Participants (Experiment 4)</title><p>Fifty participants residing in the USA completed the task on <italic>Prolific Academic</italic> (25 Democrats, 8 Republican, 17 Other, M<sub>age</sub> = 33.16, SD<sub>age</sub> ±9.804; females = 24, male = 25, other = 1, Non-White = 15, White = 35). No participants failed the attention checks. Participants received £7.50 per hour for their participation in addition to a memory test performance-related bonus.</p></sec><sec id="s4-1-6"><title>Participants (Experiment 5)</title><p>Two-hundred and sixty-one participants completed the task on <italic>Prolific Academic</italic> (132 Democrats, 90 Republican, 39 Other, <italic>M</italic><sub>age</sub> = 34.824, SD<sub>age</sub> ± 12.632; females = 122, males = 131, others = 8, Non-White = 84, White = 177). Participants received £7.50 per hour for their participation in addition to a memory test performance-related bonus.</p></sec><sec id="s4-1-7"><title>Participants (Experiment 6)</title><p>One-hundred and fifty participants completed the task on <italic>Prolific Academic</italic> (74 Democrats, 14 Republican, 62 Other, <italic>M</italic><sub>age</sub> = 34.2, SD<sub>age</sub> ± 12.489; females = 70, males = 77, others = 3, Non-White = 39, White = 150). Participants received £7.50 per hour for their participation in addition to a memory test performance-related bonus.</p></sec><sec id="s4-1-8"><title>Task (Experiment 1)</title><p>Participants engaged in a simulated social media platform where they saw 100 news posts, each consisting of an image and a headline (see <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="supplementary-material" rid="supp24">Supplementary file 24</xref> for stimuli and ratings). Half of the posts were true, and half were false. They covered a range of different topics including COVID-19, environmental issues, politics, health, and society. They were all extracted from fact-checking website Politifact (<ext-link ext-link-type="uri" xlink:href="https://www.politifact.com">https://www.politifact.com</ext-link>). For each post, participants had the option to either ‘like’, ‘dislike’, ‘trust’, or ‘distrust’ the post, or they could choose to ‘skip’ the post. They could press as many options as they wished (i.e., ‘like’ and ‘distrust’ for example). Participants were informed that if they chose to react to a post other users would be able to see their reactions. They were asked to treat the platform as they would any other social media network. The order in which reaction buttons appeared on screen was counterbalanced across participants. Participants also indicated their age, gender, ethnicity, and political orientation. The task was coded using the <italic>Qualtrics</italic> online platform (<ext-link ext-link-type="uri" xlink:href="https://www.qualtrics.com">https://www.qualtrics.com</ext-link>).</p></sec><sec id="s4-1-9"><title>Memory/attention check</title><p>At the end of the experiment, participants were presented with five posts and had to indicate whether these were old or new. This is to ensure that participants were attentive during the experiment. Participants who failed more than two of the memory checks were excluded from the analysis.</p></sec><sec id="s4-1-10"><title>Task (Experiment 2)</title><p>In Experiment 2, participants engaged in a simulated social media platform where they saw the same 100 posts (50 true, 50 false) shown to participants in Experiment 1. Participants had to either ‘repost’ or ‘skip’ each post (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). They were told that if they decided to repost, then the post would be shared to their feed, and they would observe other participants’ reaction to it. We used a between-subject design with five environments. Depending on the environment participants were randomly assigned to, they could either see (1) how many people <italic>disliked</italic> the post, (2) how many people <italic>liked</italic> the post, (3) how many people <italic>distrusted</italic> the post, or (4) how many people <italic>trusted</italic> the post. We also included a <italic>Baseline</italic> environment, in which participants received no feedback. Due to logistic constraints, the feedback was not collected in real time but was instead taken from participants’ reactions in Experiment 1. The participants, however, believed the reactions were provided in real time as indicated by a rotating cogwheel (1 s). If participants selected to skip, they would also observe a rotating cogwheel (1 s) and then a screen asking them to click continue. The average duration of the white screen (<italic>M</italic> = 2.351 s; SE = 0.281) was not different from the average duration of feedback (<italic>M</italic> = 2.625 s; SE = 0.245; <italic>t</italic>(233) = 0.853, p = 0.395, Cohen’s <italic>d</italic> = 0.056). Though the duration of trials in which participants chose to skip (<italic>M</italic> = 9.046, SE = 0.38) was slightly shorter than those in which they chose to share (<italic>M</italic> = 9.834, SE = 0.358; <italic>t</italic>(233) = 2.044, p = 0.042, Cohen’s <italic>d</italic> = 0.134). Thereafter, participants were presented with all the posts again and asked to indicate if they believed the post was accurate or inaccurate on a continuous scale from <italic>0 = inaccurate</italic> to <italic>100 = accurate</italic>. Finally, participants completed a short demographic questionnaire assessing age, gender, ethnicity, and political orientation. The task was self-paced. The task was coded using <italic>JsPsych</italic> and <italic>Javascript</italic>.</p></sec><sec id="s4-1-11"><title>Task (Experiment 3)</title><p>Experiment 3 (see <xref ref-type="fig" rid="fig4">Figure 4</xref>) was identical to the task used in Experiment 2 with three exceptions:</p><list list-type="order"><list-item><p>We selected 40 posts (20 true, 20 false), in which there was no significant difference in the way Republicans and Democrats reacted to them using the trust button during Experiment 1. This was done by entering participants’ trust responses (0/1) into a vector for Democrats and Republicans for each post. We then performed Pearson chi-square tests for each of the 100 posts to identify whether Democrats and Republicans used the trust button differently. Posts where no significant difference was observed were included in Experiment 3.</p></list-item><list-item><p>Three environments were included: a <italic>Baseline</italic> environment, in which participants received no feedback, a ‘Trust &amp; Distrust’ environment, in which participants received both <italic>Trust</italic> and <italic>Distrust</italic> feedback whenever they chose to share a post, and a ‘Like &amp; Dislike’ environment, in which participants received <italic>Like</italic> and <italic>Dislike</italic> feedback whenever they chose to share a post.</p></list-item><list-item><p>At the end of the experiment, we asked participants: (1) ‘What do you think the purpose of this experiment is?’ (2) ‘Did you, at any point throughout the experiment, think that the experimenter had deceived you in any way? If yes, please specify.’</p></list-item></list></sec><sec id="s4-1-12"><title>Task (Experiments 4–6)</title><p>The tasks and analysis in Experiments 4–6 were identical to those used in Experiments 1–3 except for the following differences:</p><list list-type="order"><list-item><p>In Experiment 4, a ‘repost’ button was included in addition to ‘skip’, ‘(Dis)Like’, and ‘(Dis)Trust’ options.</p></list-item><list-item><p>In Experiment 5, feedback symbols were colored – ‘distrusts’ and ‘dislikes’ in red and ‘trusts’ and ‘likes’ in green, instead of black and white.</p></list-item><list-item><p>Experiment 6 contained all 100 posts instead of a selection of 40 posts and did not contain final questions to assess whether participants believed the feedback was real.</p></list-item></list></sec></sec><sec id="s4-2"><title>Statistical analysis</title><sec id="s4-2-1"><title>Statistical analysis (Experiment 1)</title><p>We examined whether participants used the different reaction buttons to discern true from false information. For positive reactions (e.g., ‘<italic>likes</italic>’ and ‘<italic>trusts</italic>’) discernment is equal to the proportion of those reactions for true information minus false information, and vice versa for negative reactions (‘<italic>dislikes</italic>’ and ‘<italic>distrusts</italic>’). Proportions were calculated for each participant and then entered into a 2 (<italic>type of reaction</italic>: ‘trust’ and ‘distrust’/‘like’ and ‘dislike’) by 2 (<italic>valence</italic>: positive, i.e., ‘like’, ‘trust’/negative, i.e., ‘dislike’, ‘distrust’) within-subject ANOVA. Political orientation was also added as a between-subject factor (Republican/Democrat), allowing for an interaction of political orientation and type of reaction to assess whether participants with differing political beliefs used the reaction buttons in different ways. We performed one-sample <italic>t</italic>-tests to compare discernment (equal to the proportion of those reactions for true information minus false information, and vice versa for negative reactions) against zero to assess whether each reaction discerned between true and false information. To examine whether participants’ frequency of use of each reaction option differed we again ran a within-subject ANOVA, but this time with percentage frequency of reaction option used as the dependent variable. We computed a Pearson’s correlation across participants between frequency of skips and discernment.</p><p>One participant selected ‘other’ for political orientations. This participant was not included in the analysis because political orientation was included in analyses, and such small group sizes could heavily skew results. All statistical tests conducted in the present article are two sided. Analysis was conducted using IBM SPSS 27 and R Studio (Version 1.3.1056). All statistical tests conducted in the present article are two sided. All results of interest hold when controlling for demographics (age, gender, and ethnicity; see <xref ref-type="supplementary-material" rid="supp9 supp10 supp11 supp12 supp13 supp14 supp15 supp16">Supplementary files 9–16</xref>).</p></sec><sec id="s4-2-2"><title>Discernment analysis (Experiments 2 and 3)</title><p>Discernment is calculated for each participant by subtracting the proportion of sharing false information from the proportion of sharing true information. High discernment indicates greater sharing of true than false information. In Experiment 2, scores were submitted into an ANOVA with type of feedback (‘(Dis)Trust’ vs ‘(Dis)Like’ vs Baseline), valence of feedback (positive, i.e., ‘like’, ‘trust’ vs negative, i.e., ‘dislike’, ‘distrust’), political orientation and an interaction of political orientation and type of feedback. To assess whether frequency of posts shared differed we used the same ANOVA, this time with percentage of posts shared out of all trials as the dependent variable.</p><p>To test whether ‘(Dis)Trust’ feedback improves belief accuracy, we transformed participants’ belief ratings (which were given on a scale from post is accurate (100) or post is inaccurate (0)) to indicate error. If the post was false (inaccurate) error was equal to the rating itself, if the post was true (accurate) error was equal to 100 minus the rating. Participants’ average error scores were then entered into a between-subject ANOVA with type of feedback (Baseline, ‘(Dis)Trust’, ‘(Dis)Like’), valence of feedback, political orientation, and an interaction of political orientation and type of feedback.</p><p>Analysis of Experiment 3 followed that of Experiment 2 with the difference being that we had three type of feedback environments (Baseline, ‘Like &amp; Dislike’, and ‘Trust &amp; Distrust’) and of course no valence of feedback (as all environments were mixed valence or no valence). Data of participants who selected ‘other’ for political orientations (Experiment 2 = 28, Experiment 3 = 12) were not analyzed, because political orientation was included in the analyses variable, and small group sizes of ‘other’ could heavily skew results.</p></sec><sec id="s4-2-3"><title>Drift-diffusion modeling (Experiments 2 and 3)</title><p>To assess whether being exposed to an environment with ‘(Dis)Trust’ feedback impacted the parameters of the evidence accumulation process in our data compared to Baseline and ‘(Dis)Like’ feedback we analyzed our data using drift-diffusion modeling. To that end we ran three separate models – one for each type of feedback and included the following parameters: (1) <italic>t</italic>(0), amount of non-accumulation/non-decision time; (2) <italic>α</italic>, distance between decision thresholds; (3) <italic>z</italic>, starting point of the accumulation process; and (4) <italic>v</italic>, drift rate, is the rate of evidence accumulation.</p><p>We used the HDDM software toolbox (<xref ref-type="bibr" rid="bib47">Wiecki et al., 2013</xref>) to estimate the parameters of our models. The HDDM package employs hierarchical Bayesian parameter estimation, using Markov chain Monte Carlo (MCMC) methods to sample the posterior probability density distributions for the estimated parameter values. We estimated both group- and individual-level parameters. Parameters for individual participants were assumed to be randomly drawn from a group-level distribution. Participants’ parameters both contributed to and were constrained by the estimates of group-level parameters. In fitting the models, we used priors that assigned equal probability to all possible values of the parameters. Models were fit to log-transformed RTs. We sampled 20,000 times from the posteriors, discarding the first 5000 as burn in and thinning set at 5. MCMCs are guaranteed to reliably approximate the target posterior density as the number of samples approaches infinity. To test whether the MCMC converged within the allotted time, we used Gelman–Rubin statistic (<xref ref-type="bibr" rid="bib13">Gelman and Rubin, 1997</xref>) on five chains of our sampling procedure. The Gelman–Rubin diagnostic evaluates MCMC convergence by analyzing the difference between multiple Markov chains. The convergence is assessed by comparing the estimated between- and within-chain variances for each model parameter. In each case, the Gelman–Rubin statistic was close to one (&lt;1.1), suggesting that MCMC were able to converge.</p><p>We then compared parameter estimates using 95% HDI. Specifically, for each comparison (‘(Dis)Trust’ vs ‘(Dis)Like’, ‘(Dis)Trust’ vs Baseline, ‘(Dis)Like’ vs Baseline) we calculated the difference in the posterior distributions and reported the 95% HDI of the difference. If this HDI did not include zero, we consider there to be a meaningful difference between the two feedback types compared. To validate the winning model, we used each group’s parameters obtained from participants’ data to simulate log-transformed response times and responses separately for each feedback type. We used the exact number of subjects and number of trials as in the experiments. Simulated data were then used to (1) perform model recovery analysis and (2) to compare the pattern of participants’ response to the pattern of simulated responses, separately for each group. We sampled 2000 times from the posteriors, discarding the first 500 as burn in. Simulation and model recovery analysis were performed using the HDDM software toolbox (<xref ref-type="bibr" rid="bib47">Wiecki et al., 2013</xref>). One-way ANOVAs were computed to examine if simulated data reproduced the behavioral pattern from experimental data. To that end, discernment was entered into a one-way ANOVA with type of feedback as the independent variable for Experiments 2 and 3 separately. Note, that as we did not enter veracity of the post into our DDM and instead entered responses as either ‘veracity-promoting’ (true post shared or false post skipped) or ‘veracity-obstructing’ (false post shared or true post skipped). Thus, discernment was calculated as the proportion of true posts shared and false posts skipped minus the proportion of true posts skipped and false posts shared.</p></sec><sec id="s4-2-4"><title>Analysis (Experiments 4–6)</title><p>The analyses were identical to those in Experiment 1–3 except that the samples were not politically balanced (see Participants Experiments 4–6), as such analysis did not take into account political orientation.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Project administration</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Writing – review and editing, Methodology</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>For all experiments presented in this article, ethical approval was provided by the Research Ethics Committee at University College London and all participants gave informed consent (#3990/003).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Discernment of reactions (Experiment 1).</title></caption><media xlink:href="elife-85767-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>% Reactions out of all posts (Experiment 1).</title></caption><media xlink:href="elife-85767-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Discernment of reactions (Experiment 4, including type x valence of reaction interaction).</title></caption><media xlink:href="elife-85767-supp3-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>% true and false posts shared out of all true or false posts in that feedback condition (Experiment 2).</title></caption><media xlink:href="elife-85767-supp4-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Discernment of sharing behavior (Experiment 2).</title></caption><media xlink:href="elife-85767-supp5-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp6"><label>Supplementary file 6.</label><caption><title>% posts shared out of all posts (Experiment 2).</title></caption><media xlink:href="elife-85767-supp6-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp7"><label>Supplementary file 7.</label><caption><title>Discernment of sharing behavior (Experiment 5).</title></caption><media xlink:href="elife-85767-supp7-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp8"><label>Supplementary file 8.</label><caption><title>Belief Accuracy (Experiment 2).</title></caption><media xlink:href="elife-85767-supp8-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp9"><label>Supplementary file 9.</label><caption><title>Belief Accuracy (Experiment 5).</title></caption><media xlink:href="elife-85767-supp9-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp10"><label>Supplementary file 10.</label><caption><title>Discernment of sharing behavior (Experiment 3).</title></caption><media xlink:href="elife-85767-supp10-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp11"><label>Supplementary file 11.</label><caption><title>% posts shared out of all posts (Experiment 3).</title></caption><media xlink:href="elife-85767-supp11-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp12"><label>Supplementary file 12.</label><caption><title>Belief Accuracy (Experiment 3).</title></caption><media xlink:href="elife-85767-supp12-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp13"><label>Supplementary file 13.</label><caption><title>Mean difference in posterior distributions and 95% HDI Comparison (Experiment 2).</title></caption><media xlink:href="elife-85767-supp13-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp14"><label>Supplementary file 14.</label><caption><title>Mean difference in posterior distributions and 95% HDI Comparison (Experiment 3).</title></caption><media xlink:href="elife-85767-supp14-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp15"><label>Supplementary file 15.</label><caption><title>Group estimates for DDM (Experiment 5).</title></caption><media xlink:href="elife-85767-supp15-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp16"><label>Supplementary file 16.</label><caption><title>Mean difference in posterior distributions and 95% HDI Comparison (Experiment 5).</title></caption><media xlink:href="elife-85767-supp16-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp17"><label>Supplementary file 17.</label><caption><title>Group estimates for DDM (Experiment 6).</title></caption><media xlink:href="elife-85767-supp17-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp18"><label>Supplementary file 18.</label><caption><title>Mean difference in posterior distributions and 95% HDI Comparison (Experiment 6).</title></caption><media xlink:href="elife-85767-supp18-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp19"><label>Supplementary file 19.</label><caption><title>Recovered Group estimates for DDM based on simulated data (Experiment 2).</title></caption><media xlink:href="elife-85767-supp19-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp20"><label>Supplementary file 20.</label><caption><title>Recovered Group estimates for DDM based on simulated data (Experiment 3).</title></caption><media xlink:href="elife-85767-supp20-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp21"><label>Supplementary file 21.</label><caption><title>Pairwise Comparisons for Discernment (Experiment 2).</title></caption><media xlink:href="elife-85767-supp21-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp22"><label>Supplementary file 22.</label><caption><title>Pairwise Comparisons for Discernment (Experiment 3).</title></caption><media xlink:href="elife-85767-supp22-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp23"><label>Supplementary file 23.</label><caption><title>Correlations between participants’ real and recovered DDM estimates (Experiment 2 and Experiment 3).</title></caption><media xlink:href="elife-85767-supp23-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp24"><label>Supplementary file 24.</label><caption><title>Individual Ratings per Stimulus (Experiment 1).</title></caption><media xlink:href="elife-85767-supp24-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-85767-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Code and anonymized data are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/affective-brain-lab/Changing-the-Incentive-Structure-of-Social-Media-Platforms">https://github.com/affective-brain-lab/Changing-the-Incentive-Structure-of-Social-Media-Platforms</ext-link> (copy archived at <xref ref-type="bibr" rid="bib14">Globig, 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Valentina Vellani, Bastien Blain, Irene Cogliati Dezza, Moshe Glickman, Sarah Zheng, India Pinhorn, Christopher Kelly, and Hadeel Haj-Ali for comments on previous versions of the manuscript. TS is funded by a Wellcome Trust Senior Research Fellowship 214268/Z/18/Z.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>J</given-names></name><name><surname>Arechar</surname><given-names>AA</given-names></name><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Scaling up fact-checking using the wisdom of crowds</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabf4393</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abf4393</pub-id><pub-id pub-id-type="pmid">34516925</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>IA</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Habits and the electronic herd: The psychology behind social media’s successes and failures</article-title><source>Consumer Psychology Review</source><volume>4</volume><fpage>83</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1002/arcp.1063</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bălău</surname><given-names>N</given-names></name><name><surname>Utz</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Exposing information sharing as strategic behavior: Power as responsibility and &quot;trust&quot; buttons</article-title><source>Journal of Applied Social Psychology</source><volume>46</volume><fpage>593</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1111/jasp.12388</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barreto</surname><given-names>M</given-names></name><name><surname>Caram</surname><given-names>C</given-names></name><name><surname>dos Santos</surname><given-names>JLG</given-names></name><name><surname>de Souza</surname><given-names>RR</given-names></name><name><surname>Goes</surname><given-names>HL</given-names></name><name><surname>Marcon</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fake news about the COVID-19 pandemic: Perception of health professionals and their families</article-title><source>Revista Da Escola de Enfermagem Da USP</source><volume>55</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1590/1980-220x-reeusp-2021-0007</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>WJ</given-names></name><name><surname>McLoughlin</surname><given-names>K</given-names></name><name><surname>Doan</surname><given-names>TN</given-names></name><name><surname>Crockett</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How social learning Amplifies moral outrage expression in Online social networks</article-title><source>Science Advances</source><volume>7</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1126/sciadv.abe5641</pub-id><pub-id pub-id-type="pmid">34389534</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brashier</surname><given-names>NM</given-names></name><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Berinsky</surname><given-names>AJ</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Timing matters when correcting fake news</article-title><source>PNAS</source><volume>118</volume><fpage>2</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1073/pnas.2020043118</pub-id><pub-id pub-id-type="pmid">33495336</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Capraro</surname><given-names>V</given-names></name><name><surname>Celadin</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>Endorsing Accuracy Decreases the Sharing of Fake News and Increases the Sharing of Real News</source><publisher-name>Personality and Social Psychology Bulletin</publisher-name><pub-id pub-id-type="doi">10.31234/osf.io/s3q5n</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceylan</surname><given-names>G</given-names></name><name><surname>Anderson</surname><given-names>IA</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Sharing of misinformation is habitual, not just lazy or biased</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2216614120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2216614120</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>CR</given-names></name><name><surname>Hall Jamieson</surname><given-names>K</given-names></name><name><surname>Albarracín</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Debunking: A meta-analysis of the psychological efficacy of messages countering misinformation</article-title><source>Psychological Science</source><volume>28</volume><fpage>1531</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1177/0956797617714579</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edgerly</surname><given-names>S</given-names></name><name><surname>Vraga</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The blue check of credibility: does account verification matter when evaluating news on Twitter?</article-title><source>Cyberpsychology, Behavior and Social Networking</source><volume>22</volume><fpage>283</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1089/cyber.2018.0475</pub-id><pub-id pub-id-type="pmid">30848675</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name><name><surname>Lang</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Statistical power analyses using G* power 3.1: Tests for correlation and regression analyses</article-title><source>Behavior Research Methods</source><volume>41</volume><fpage>1149</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.3758/BRM.41.4.1149</pub-id><pub-id pub-id-type="pmid">19897823</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fazio</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pausing to consider why a headline is true or false can help reduce the sharing of false news</article-title><source>Harvard Kennedy School Misinformation Review</source><volume>1</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.37016/mr-2020-009</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical Science</source><volume>7</volume><fpage>457</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Globig</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Changing-the-incentive-structure-of-social-media-platforms</data-title><version designator="swh:1:rev:55ba6cbbcd4be1c826e34c73afd4c659ed780501">swh:1:rev:55ba6cbbcd4be1c826e34c73afd4c659ed780501</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3b44cc260b1a654ba478a9892f8a9bc4e059b62c;origin=https://github.com/affective-brain-lab/Changing-the-Incentive-Structure-of-Social-Media-Platforms;visit=swh:1:snp:520ec8f24d4fa1b7f920b455bfc69d590458300f;anchor=swh:1:rev:55ba6cbbcd4be1c826e34c73afd4c659ed780501">https://archive.softwareheritage.org/swh:1:dir:3b44cc260b1a654ba478a9892f8a9bc4e059b62c;origin=https://github.com/affective-brain-lab/Changing-the-Incentive-Structure-of-Social-Media-Platforms;visit=swh:1:snp:520ec8f24d4fa1b7f920b455bfc69d590458300f;anchor=swh:1:rev:55ba6cbbcd4be1c826e34c73afd4c659ed780501</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grady</surname><given-names>RH</given-names></name><name><surname>Ditto</surname><given-names>PH</given-names></name><name><surname>Loftus</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Nevertheless, partisanship persisted: Fake news warnings help briefly, but bias returns with time</article-title><source>Cognitive Research</source><volume>6</volume><elocation-id>52</elocation-id><pub-id pub-id-type="doi">10.1186/s41235-021-00315-z</pub-id><pub-id pub-id-type="pmid">34297248</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grinberg</surname><given-names>N</given-names></name><name><surname>Joseph</surname><given-names>K</given-names></name><name><surname>Friedland</surname><given-names>L</given-names></name><name><surname>Swire-Thompson</surname><given-names>B</given-names></name><name><surname>Lazer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fake news on Twitter during the 2016 US presidential election</article-title><source>Science</source><volume>363</volume><fpage>374</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1126/science.aau2706</pub-id><pub-id pub-id-type="pmid">30679368</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guess</surname><given-names>AM</given-names></name><name><surname>Lerner</surname><given-names>M</given-names></name><name><surname>Lyons</surname><given-names>B</given-names></name><name><surname>Montgomery</surname><given-names>JM</given-names></name><name><surname>Nyhan</surname><given-names>B</given-names></name><name><surname>Reifler</surname><given-names>J</given-names></name><name><surname>Sircar</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Digital media literacy intervention increases discernment between mainstream and false news in the United States and India</article-title><source>PNAS</source><volume>117</volume><fpage>15536</fpage><lpage>15545</lpage><pub-id pub-id-type="doi">10.1073/pnas.1920498117</pub-id><pub-id pub-id-type="pmid">32571950</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Fuentemilla</surname><given-names>L</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Action dominates Valence in anticipatory representations in the human striatum and dopaminergic Midbrain</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>7867</fpage><lpage>7875</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6376-10.2011</pub-id><pub-id pub-id-type="pmid">21613500</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Fuentemilla</surname><given-names>L</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Go and No-Go learning in reward and punishment: interactions between affect and effect</article-title><source>NeuroImage</source><volume>62</volume><fpage>154</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.024</pub-id><pub-id pub-id-type="pmid">22548809</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Action versus Valence in decision making</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>194</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.003</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>MO</given-names></name><name><surname>Malladi</surname><given-names>S</given-names></name><name><surname>McAdams</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learning through the grapevine and the impact of the breadth and depth of social networks</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2205549119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2205549119</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozyreva</surname><given-names>A</given-names></name><name><surname>Lewandowsky</surname><given-names>S</given-names></name><name><surname>Hertwig</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Citizens versus the Internet: confronting digital challenges with cognitive tools</article-title><source>Psychological Science in the Public Interest</source><volume>21</volume><fpage>103</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1177/1529100620946707</pub-id><pub-id pub-id-type="pmid">33325331</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazer</surname><given-names>DMJ</given-names></name><name><surname>Baum</surname><given-names>MA</given-names></name><name><surname>Benkler</surname><given-names>Y</given-names></name><name><surname>Berinsky</surname><given-names>AJ</given-names></name><name><surname>Greenhill</surname><given-names>KM</given-names></name><name><surname>Menczer</surname><given-names>F</given-names></name><name><surname>Metzger</surname><given-names>MJ</given-names></name><name><surname>Nyhan</surname><given-names>B</given-names></name><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Rothschild</surname><given-names>D</given-names></name><name><surname>Schudson</surname><given-names>M</given-names></name><name><surname>Sloman</surname><given-names>SA</given-names></name><name><surname>Sunstein</surname><given-names>CR</given-names></name><name><surname>Thorson</surname><given-names>EA</given-names></name><name><surname>Watts</surname><given-names>DJ</given-names></name><name><surname>Zittrain</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The science of fake news</article-title><source>Science</source><volume>359</volume><fpage>1094</fpage><lpage>1096</lpage><pub-id pub-id-type="doi">10.1126/science.aao2998</pub-id><pub-id pub-id-type="pmid">29590025</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lees</surname><given-names>J</given-names></name><name><surname>McCarter</surname><given-names>A</given-names></name><name><surname>Sarno</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Twitter’s disputed tags may be ineffective at reducing belief in fake news and only reduce intentions to share fake news among Democrats and independents</article-title><source>Journal of Online Trust and Safety</source><volume>1</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.54501/jots.v1i3.39</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowsky</surname><given-names>S</given-names></name><name><surname>van der Linden</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Countering misinformation and fake news through inoculation and Prebunking</article-title><source>European Review of Social Psychology</source><volume>32</volume><fpage>348</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1080/10463283.2021.1876983</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Thinking more or thinking differently? using drift-diffusion modeling to illuminate why accuracy prompts decrease misinformation sharing</article-title><source>Cognition</source><volume>230</volume><elocation-id>105312</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105312</pub-id><pub-id pub-id-type="pmid">36334467</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindström</surname><given-names>B</given-names></name><name><surname>Bellander</surname><given-names>M</given-names></name><name><surname>Schultner</surname><given-names>DT</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Amodio</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A computational reward learning account of social media engagement</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1802</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22067-6</pub-id><pub-id pub-id-type="pmid">33727545</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maertens</surname><given-names>R</given-names></name><name><surname>Roozenbeek</surname><given-names>J</given-names></name><name><surname>Basol</surname><given-names>M</given-names></name><name><surname>van der Linden</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Long-term effectiveness of inoculation against misinformation: Three longitudinal experiments</article-title><source>Journal of Experimental Psychology. Applied</source><volume>27</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1037/xap0000315</pub-id><pub-id pub-id-type="pmid">33017160</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fighting misinformation on social media using crowdsourced judgments of news source quality</article-title><source>PNAS</source><volume>116</volume><fpage>2521</fpage><lpage>2526</lpage><pub-id pub-id-type="doi">10.1073/pnas.1806781116</pub-id><pub-id pub-id-type="pmid">30692252</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>McPhetres</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>JG</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fighting COVID-19 misinformation on social media: experimental evidence for a scalable accuracy-nudge intervention</article-title><source>Psychological Science</source><volume>31</volume><fpage>770</fpage><lpage>780</lpage><pub-id pub-id-type="doi">10.1177/0956797620939054</pub-id><pub-id pub-id-type="pmid">32603243</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Epstein</surname><given-names>Z</given-names></name><name><surname>Mosleh</surname><given-names>M</given-names></name><name><surname>Arechar</surname><given-names>AA</given-names></name><name><surname>Eckles</surname><given-names>D</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shifting attention to accuracy can reduce misinformation online</article-title><source>Nature</source><volume>592</volume><fpage>590</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03344-2</pub-id><pub-id pub-id-type="pmid">33731933</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennycook</surname><given-names>G</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Accuracy prompts are a replicable and generalizable approach for reducing the spread of misinformation</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>2333</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-30073-5</pub-id><pub-id pub-id-type="pmid">35484277</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pilditch</surname><given-names>TD</given-names></name><name><surname>Roozenbeek</surname><given-names>J</given-names></name><name><surname>Madsen</surname><given-names>JK</given-names></name><name><surname>van der Linden</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Psychological inoculation can reduce susceptibility to misinformation in large rational agent networks</article-title><source>Royal Society Open Science</source><volume>9</volume><elocation-id>211953</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.211953</pub-id><pub-id pub-id-type="pmid">35958086</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rapp</surname><given-names>DN</given-names></name><name><surname>Salovich</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Can’t we just disregard fake news? the consequences of exposure to inaccurate information</article-title><source>Policy Insights from the Behavioral and Brain Sciences</source><volume>5</volume><fpage>232</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1177/2372732218785193</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Drift diffusion decision model: theory and data</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1016/j.biotechadv.2011.08.021.Secreted</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>Z</given-names></name><name><surname>Dimant</surname><given-names>E</given-names></name><name><surname>Schweitzer</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>Social motives for sharing conspiracy theories</source><publisher-name>SSRN Electronic Journal</publisher-name><pub-id pub-id-type="doi">10.2139/ssrn.3919364</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roozenbeek</surname><given-names>J</given-names></name><name><surname>van der Linden</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The fake news game: Actively inoculating against the risk of misinformation</article-title><source>Journal of Risk Research</source><volume>22</volume><fpage>570</fpage><lpage>580</lpage><pub-id pub-id-type="doi">10.1080/13669877.2018.1443491</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharot</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>To quell misinformation, use carrots—not just sticks</article-title><source>Nature</source><volume>591</volume><elocation-id>347</elocation-id><pub-id pub-id-type="doi">10.1038/d41586-021-00657-0</pub-id><pub-id pub-id-type="pmid">33731950</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skinner</surname><given-names>BF</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>The Behavior of Organisms: An Experimental Analysis</source><publisher-name>Appleton-Century-Crofts</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamir</surname><given-names>DI</given-names></name><name><surname>Mitchell</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disclosing information about the self is intrinsically rewarding</article-title><source>PNAS</source><volume>109</volume><fpage>8038</fpage><lpage>8043</lpage><pub-id pub-id-type="doi">10.1073/pnas.1202129109</pub-id><pub-id pub-id-type="pmid">22566617</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Traberg</surname><given-names>CS</given-names></name><name><surname>Roozenbeek</surname><given-names>J</given-names></name><name><surname>van der Linden</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Psychological inoculation against misinformation: Current evidence and future directions</article-title><source>The ANNALS of the American Academy of Political and Social Science</source><volume>700</volume><fpage>136</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1177/00027162221087936</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsfati</surname><given-names>Y</given-names></name><name><surname>Boomgaarden</surname><given-names>HG</given-names></name><name><surname>Strömbäck</surname><given-names>J</given-names></name><name><surname>Vliegenthart</surname><given-names>R</given-names></name><name><surname>Damstra</surname><given-names>A</given-names></name><name><surname>Lindgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Causes and consequences of mainstream media dissemination of fake news: Literature review and synthesis</article-title><source>Annals of the International Communication Association</source><volume>44</volume><fpage>157</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1080/23808985.2020.1759443</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Bavel</surname><given-names>JJ</given-names></name><name><surname>Rathje</surname><given-names>S</given-names></name><name><surname>Harris</surname><given-names>E</given-names></name><name><surname>Robertson</surname><given-names>C</given-names></name><name><surname>Sternisko</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How social media shapes polarization</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>913</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.07.013</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vosoughi</surname><given-names>S</given-names></name><name><surname>Roy</surname><given-names>D</given-names></name><name><surname>Aral</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The spread of true and false news Online</article-title><source>Science</source><volume>359</volume><fpage>1146</fpage><lpage>1151</lpage><pub-id pub-id-type="doi">10.1126/science.aap9559</pub-id><pub-id pub-id-type="pmid">29590045</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname><given-names>A</given-names></name><name><surname>Nagler</surname><given-names>M</given-names></name><name><surname>Lerche</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Diffusion models in experimental psychology: A practical introduction</article-title><source>Experimental Psychology</source><volume>60</volume><fpage>385</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1027/1618-3169/a000218</pub-id><pub-id pub-id-type="pmid">23895923</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname><given-names>TV</given-names></name><name><surname>Sofer</surname><given-names>I</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>HDDM: Hierarchical Bayesian estimation of the drift-diffusion model in python</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><pub-id pub-id-type="pmid">23935581</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85767.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group></front-stub><body><p>This important paper outlines a novel method for reducing the spread of misinformation on social media platforms. A compelling series of experiments and replications support the main claims, which could have significant real-world societal impact.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85767.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Gillan</surname><given-names>Claire M</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Changing the Incentive Structure of Social Media Platforms to Halt the Spread of Misinformation&quot; for consideration by <italic>eLife</italic> – we enjoyed reading it and think it could have significant real-world impact.</p><p>Your article has been reviewed by 3 peer reviewers, including Claire M Gillan as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Michael Frank as the Senior Editor. The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Clarity and consistency of analysis. While all the reviewers applauded the replications and comprehensiveness of the study, it was felt that some important analyses were omitted. Of note, the valence by response type interaction was not carried out, though there were valence effects. There are also queries about the use of political orientation in these main analyses given it is not the focus of the paper.</p><p>2) More detail of the design of the study, including the instructions provided to participants and reporting of the ratings of trust/distrust, like/dislike from study 1 (which are used for study 2). Means are reported, but distributions of ratings for each post would be helpful to understand the feedback received in study 2.</p><p>3) The DDM is not essential to the main results, but there were questions about the omission of the starting bias and in general the methods could have been better fleshed out.</p><p>4) Although some reviewers felt the question was more applied, it was suggested there be more emphasis on theoretical motivations for the study. So you might consider striking a balance here.</p><p><italic>Reviewer #1:</italic></p><p>This is a very comprehensive and compelling piece of research that investigates a novel method for reducing the spread of misinformation online. The authors introduce trust/distrust options into a (analogue) social media environment and illustrate that it improves discernment (sensitivity of users to the factual basis of content they share). Pairing cross-sectional, correlational designs with causal manipulation and replication across multiple independent studies, this is a tour de force. There is immediate real-world impact from the work; the data present a compelling case that misinformation would be spread less often if social media environments included peer-assessments of the trustworthiness of content. The authors additionally showed that trust/distrust options actually increase engagement with social media over the existing like/dislike buttons, which creates an interesting additional incentive for private companies in this space to implement this. Finally, they showed that these social incentives around trust actually increase belief accuracy in users.</p><p>The authors compliment these insights with mechanistic explanations by modelling the 'sharing of posts' data using a drift diffusion model and comparing parameters across environments where trust/distrust vs like/dislike were differentially available. They find that across two studies drift rates are larger in the trust/distrust conditions, suggesting users accumulate more information before making a decision.</p><p>The authors note that the complexities of real world social media environments may work differently to the tightly controlled laboratory analogues described here and this would be an important next step. Other papers have investigated methods to promote the sharing of true information online, but they are typically active training conditions that would be difficult to roll out and achieve engagement. In contrast, the approach here is naturalistic and gamified in the same sense like/dislikes are on these platforms and demonstrably increases engagement.</p><p>I really have no major suggestions at all. The paper is very clear and extremely comprehensive. It builds nicely on this recent PNAS paper (https://www.pnas.org/doi/epdf/10.1073/pnas.2216614120), though no need to cite it necessarily, it highlights the need for methods that can promote discernment via their reward systems – the present paper appears to do just that.</p><p><italic>Reviewer #2:</italic></p><p>This study presents important data that changing the incentive structure in social platform can decrease the spread of misinformation. If implemented in the real world, these findings may help solve an important problem in our society. The evidence supporting the conclusions is solid but more clarity in the way the analyses were conducted would strengthen the study.</p><p>One strength of the study is that it tests a clear hypothesis, namely that changing the incentive structure in social media platforms (so that rewards and punishment that users receive reflect the veracity of the information they share) will decrease the spread of misinformation. The authors suggest changing the current like/dislike feedback by trust/distrust feedback, the latter implying a judgement about veracity of the statement.</p><p>The study involves 3 different experiments. One strength of the study is that the main fundings are replicated in 3 independent experiments. In the first experiment, the authors demonstrate that participants discern true from false information when deciding to trust or distrust a new post to a higher degree than when deciding to like or dislike a post. This claim seems to be supported by the data. In the second and the third studies, the authors show that participants are more likely to share true information when their decisions to share a post result in trust or distrust feedback in comparison to like or dislike feedback or no feedback at all. Moreover, seeing how other participants trust/distrust the posts increase the accuracy of the beliefs that participants hold at the end of the experiment. These claims also seem to be supported by the data.</p><p>These results are clearly relevant and provide a clear and easy to implement solution to a very hot problem in our contemporary society, namely spread of misinformation via social media that contributes to polarization and consolidation of conspiracy theories. In this sense, these results might be very valuable. However, I missed information about what participants were told about the experiment and what were they instructed to do. This information has implications to understand the possible generalization of the results outside of the experimental setting. For example, the authors show that participants engage more with the trust/distrust than with the like/dislike buttons. It is not clear how this result may be influenced by the fact that participants are doing an experiment about trustworthiness of social media posts. The authors rightly acknowledge that their simulated social network is a simplification of a more complex real system. Hence, the importance of knowing every detail of the experimental paradigm and instructions given to the participants.</p><p>The authors used DDM to understand the underlying cognitive mechanism of the observed effects. The standard DMM as implemented in this study assumes that the decision-making process is at equilibrium and does not account for changes due to learning during the experiment. This is a drawback considering that the main effect of changing the incentive structure is expected to be achieved through a learning process. Nevertheless, the DDM could still disentangle between alternative cognitive/computational explanations of the differences between the experimental conditions. But the current implementation is difficult to fully interpret. It is unclear why the starting point is fixed and not a free parameter. Theoretically, it is equally likely that the experimental manipulation induces a bias towards discernment in repost behavior. And if there is a good reason to fix the starting point, it is unclear how it was chosen and how this decision impacts the rest of the DMM results.</p><p>Although strongly mitigated by the fact that the main results are replicated, one possible drawback of the study is that it was not preregistered. The analysis pipeline involves a few researchers' degrees of freedom. I list 3 examples:</p><p>1) It is unclear why the ANOVA in experiment 1 did not include an interaction between type of response and valence. The data seems to show a pattern towards this direction whereby the main behavioral effect is driven by a difference between dislike and distrust whereas the difference between like and trust seems much smaller or non-significant. The omission of the interaction might have affected the reported results.</p><p>2) I am not sure that the use of political orientation in the analysis is fully adequate. The authors report an interaction with the main effect in experiment 1 driven by stronger results among democrats when compared to republicans. However, in experiment 2, the interaction is only marginally significant. Despite this, the authors still explore it further to show a difference in discernment between democrats and republicans. In experiment 3, the authors only used a subset of posts that were equally trusted by democrats and republicans. Importantly, the data shows that effect of trust/distrust on repost discernment is unaffected, which strengthens the main results without doubt. However, this rises the possibility that the interaction in experiment 1 is driven by the selection of material. It is also unclear to what extend the results in experiment 1 and 2 are dependent on the inclusion of political orientation in the models. Also, the inclusion of this factors justifies the exclusion of a significant number of participants in experiment 2 and 3.</p><p>3) In experiment 2 and 3, the main results are analyzed using a GLM approach. It is unclear why an ANOVA is not used here and how the main regressor was built. The latter may have implications for interpretation of the results.</p><p>Try to increase clarity in some points of the text. The use of the term discernment in the abstract is not clear as the term is only defined in the text. State clearly in the Results section that experiment 2 and 3 are between subject (now it is only stated in the figure) along with the number of participants per group, including the number of excluded participants.</p><p>Include the interaction between type of response and valence in study 1. Report the results with and without political orientation to show that the results are independent of the inclusion of this factor.</p><p>I would suggest that the DDM analysis either includes all model parameters as free or that the authors try to implement different variations (with different free parameters) and perform model comparison to test which is the most parsimonious fit to the data. In the recoverability analysis, I would report the correlation between the simulated and recovered parameters. And if different models are tested, I would include a confusion matrix of the recoverability of the models.</p><p>When presenting the DDM results, I would avoid the systematic use of &quot;good&quot; and &quot;bad&quot; labels for the responses. It is fine to use this metaphor in one instance, but it sounds like an oversimplification when it is repeated many times.</p><p>In the first paragraph of the discussion, the authors state &quot;Users who found themselves in such an environment began sharing more true than false posts in order to receive carrots and avoid sticks.&quot; I would reformulate this statement not to imply learning. Although learning is implicit by the fact that the effects are a consequence of changing the incentive structure, the behavioral data analysis is aggregated across the whole experiment.</p><p>Figure 7 is missing</p><p><italic>Reviewer #3:</italic></p><p>This paper suggests that changing the incentive structure on social platforms may influence people's discernment in their sharing decisions. Particularly, in experiment 1, the authors suggest that participants' use of trust-distrust buttons were more discerning than their use of like-dislike button. In experiments 2 and 3, the authors suggest that operating in a trust-distrust environment makes people's sharing more discerning than operating in a like-dislike or no-feedback environments. They also test a drift rate and claim that people place more weight to accuracy when provided with the distrust-trust feedback.</p><p>Strengths:</p><p>– The paper examines a novel mechanism to prevent misinformation spread on social platforms.</p><p>– The authors' idea of offering buttons that are better linked to headline veracity is a practical solution that could be easily implemented.</p><p>Weaknesses:</p><p>– The conceptualization could be better articulated with wider coverage of the misinformation literature.</p><p>– There is a disconnect between the theory and the empirical studies.</p><p>– Fake and true headlines may be different from one another on other factors (e.g., emotions) that affect people's trust judgments.</p><p>– The design of experiments 2 and 3 fails to accurately connect trust judgments to the veracity of the headlines.</p><p>Major Comments</p><p>Conceptualization:</p><p>Conceptually, authors suggest that adding buttons should make people's evaluations of headlines (in experiment 1) and the presence of much wider variety of feedback will make their sharing decisions better (in experiments 2 and 3).</p><p>1. The reader would like to see better build-up of the theoretical framework. Some questions are:</p><p>a. Whereas social media sharing (especially sharing 40-100 headlines) seem to be repetitive and can lead participants to develop habits (Anderson and Wood 2021; Ceylan et al., 2023). Do habits play a role? With the change in incentive structure, do participants develop different sharing habits?</p><p>b. Would we expect differential evaluation and sharing as a function of valence of the feedback button? The authors claim in many places that &quot;… humans naturally seek social rewards and are motivated to avoid social punishments (Behrens, Hunt, Woolrich, and Rushworth, 2008; Bhanji and Delgado, 2014).&quot; Do they test and find evidence for this? (More discussion and suggestions are below).</p><p>2. Figure 1b and 1c are rather confusing than clarifying. If I understand correctly, in experiment 2, participants receive one type of feedback that is disconnected from the actual veracity of the information. If people receive just distrust feedback for all the headlines (true and false), how is this in line with the idea that authors align carrots with truth and sticks with falsity?</p><p>Empirics:</p><p>My overall feedback is that authors should better connect their conceptualization to their empirics.</p><p>3. Why is the participant the evaluator of the news headline in experiment 1 and then the sharer of the headline in experiments 2 and 3?</p><p>4. The reader wonders where the action comes from: is it from reduction in rejecting false information or increasing in endorsing true information? In the model authors shared in the Supplemental Appendix Table 2, can the authors run a mixed effect model on the % of headlines engaged (not skipped) as a function of type of reaction and valence of reaction? This will help authors test their theory more robustly.</p><p>a. The same for Supplemental Appendix Tables 5 and 7. Authors can test their theory by including an interaction term for type of reaction and valence of that reaction.</p><p>5. In experiment 1, how did &quot;skip&quot; reaction impact people's discernment? Were those who skipped more discerning than those who used like-dislike buttons?</p><p>6. In experiments 2 and 3, the authors provide participants with 100 headlines. Participants make a skip or repost decision. Then, participants receive one of the types of feedback depending on their condition. One problem with this design and what authors suggest in Figure 1 is that participants receive one type of feedback in their respective condition. For instance, in the distrust condition, participants receive distrust feedback no matter what the veracity of the headline is. This is problematic because in this model, people do not have an opportunity to learn and connect veracity with trust feedback. I suggest authors could collect new data by cleaning up this mapping. The reader wonders how the results would change if this mapping actually happens and participants receive trust feedback when they share accurate information and distrust feedback when they share false information.</p><p>a. This may explain why the authors do not find a differential effect between trust and distrust conditions. Trust feedback that is disconnected from the actual veracity of the headline would not enhance people's discernment differentially when this feedback is negative or positive. It seems like participants were just operating in an environment knowing that the feedback will be about trust but no matter what they share the valence of the trust would not change.</p><p>7. Further, in the current execution, participants learn about the reward and use this learning in their sharing decision all at one stage. However, the execution of the study can be cleaner if the authors separate the learning stage from the test stage (see Study 4 in Ceylan et al., 2023).</p><p>8. One concern is that the authors stripped the headlines from any factor that could contribute people's truth and trust judgments. For instance, source credibility or consensus are factors that aid people's judgments on social platforms such as Facebook and Twitter. One may wonder how people make their truth judgments in the absence of these cues. Can people discern true information from false information? In experiment 2, authors measure the discernment, but I did not see the means between true and false headlines. Also, do people's trust and like responses correlate with the actual veracity of the information?</p><p>9. Trust judgments also bear on other factors. One such factor is emotions (Dunn and Schweitzer, 2005). The reader wonders what the valence of the emotions is between the true and false headlines. A pretest that shows differences in emotions between false and true headlines would be helpful to the reader.</p><p>10. I am not very familiar with the computational analysis calculating drift rate. For readers who will not be very familiar with this method will need more detail about the analysis. Also, it seems like the results from experiments 2 and 3 do not show similar trends. Specifically, in experiment 2, the distance between the thresholds is the highest in the trust-distrust conditions and the non-decision time is the lowest. In experiment 3, there is no difference in distance between the thresholds across all three conditions. This time, the non-decision time was the highest in the trust-distrust conditions. How do the authors reconcile these differences?</p><p>References</p><p>Anderson, I. A., and Wood, W. (2021). Habits and the electronic herd: The psychology behind social media's successes and failures. Consumer Psychology Review, 4(1), 83-99.</p><p>Ceylan, G., Anderson, I. A., and Wood, W. (2023). Sharing of misinformation is habitual, not just lazy or biased. Proceedings of the National Academy of Sciences, 120(4), e2216614120.</p><p>Dunn, J. R., and Schweitzer, M. E. (2005). Feeling and believing: the influence of emotion on trust. Journal of personality and social psychology, 88(5), 736.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85767.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Clarity and consistency of analysis. While all the reviewers applauded the replications and comprehensiveness of the study, it was felt that some important analyses were omitted. Of note, the valence by response type interaction was not carried out, though there were valence effects. There are also queries about the use of political orientation in these main analyses given it is not the focus of the paper.</p></disp-quote><p>We thank the reviewers for this positive evaluation of our study. We have repeated all the main analyses including valence and type of reaction/feedback interactions, as well as not including political orientation as a factor. The results show once again that discernment is greater in the ‘(Dis)Trust’ condition than the other conditions (Supplementary files 1-12).</p><disp-quote content-type="editor-comment"><p>2) More detail of the design of the study, including the instructions provided to participants and reporting of the ratings of trust/distrust, like/dislike from study 1 (which are used for study 2). Means are reported, but distributions of ratings for each post would be helpful to understand the feedback received in study 2.</p></disp-quote><p>We now provide more details pertaining to the design of the study. In particular, we now clarify the between-subject design nature of Experiment 2 and 3 (pg.13, 21, 35-36) and provide the full instructions for all studies (Figure 2 – —figure supplement 1, Figure 4 – —figure supplement 1 and 2). We also report the exact ratings for each post in Experiment 1 (Supplementary file 24).</p><disp-quote content-type="editor-comment"><p>3) The DDM is not essential to the main results, but there were questions about the omission of the starting bias and in general the methods could have been better fleshed out.</p></disp-quote><p>We now follow the reviewer’s recommendation and include the starting point as a free parameter (see pg. 23-28, Figure 6 and 7, Tables 1-2). The findings remain the same. In particular we find that the drift rate in the ‘(Dis)Trust’ environments (Experiment 2: v=0.216; Experiment 3: v=0.12) was meaningfully higher than the drift rate in both the ‘(Dis)Like’ (Experiment 2: v=0.01; 95% HDI of difference [0.048; 0.183], Experiment 3: 0.034; 95% HDI of difference [0.033;0.139]) and Baseline environments (Experiment 2: v=0.098; 95% HDI of difference [0.041; 0.195]; Experiment 3: v=0.007; 95% HDI of difference [0.062; 0.165]).</p><disp-quote content-type="editor-comment"><p>4) Although some reviewers felt the question was more applied, it was suggested there be more emphasis on theoretical motivations for the study. So you might consider striking a balance here.</p></disp-quote><p>Thank you for the suggestion, we have now added more details regarding the theory and rationale on pg. 4-7, 16-17, 30-31.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This is a very comprehensive and compelling piece of research that investigates a novel method for reducing the spread of misinformation online. The authors introduce trust/distrust options into a (analogue) social media environment and illustrate that it improves discernment (sensitivity of users to the factual basis of content they share). Pairing cross-sectional, correlational designs with causal manipulation and replication across multiple independent studies, this is a tour de force. There is immediate real-world impact from the work; the data present a compelling case that misinformation would be spread less often if social media environments included peer-assessments of the trustworthiness of content. The authors additionally showed that trust/distrust options actually increase engagement with social media over the existing like/dislike buttons, which creates an interesting additional incentive for private companies in this space to implement this. Finally, they showed that these social incentives around trust actually increase belief accuracy in users.</p><p>The authors compliment these insights with mechanistic explanations by modelling the 'sharing of posts' data using a drift diffusion model and comparing parameters across environments where trust/distrust vs like/dislike were differentially available. They find that across two studies drift rates are larger in the trust/distrust conditions, suggesting users accumulate more information before making a decision.</p><p>The authors note that the complexities of real world social media environments may work differently to the tightly controlled laboratory analogues described here and this would be an important next step. Other papers have investigated methods to promote the sharing of true information online, but they are typically active training conditions that would be difficult to roll out and achieve engagement. In contrast, the approach here is naturalistic and gamified in the same sense like/dislikes are on these platforms and demonstrably increases engagement.</p></disp-quote><p>We thank the reviewer for this positive evaluation.</p><disp-quote content-type="editor-comment"><p>I really have no major suggestions at all. The paper is very clear and extremely comprehensive. It builds nicely on this recent PNAS paper (https://www.pnas.org/doi/epdf/10.1073/pnas.2216614120), though no need to cite it necessarily, it highlights the need for methods that can promote discernment via their reward systems – the present paper appears to do just that.</p></disp-quote><p>We thank the reviewer for this positive evaluation.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>This study presents important data that changing the incentive structure in social platform can decrease the spread of misinformation. If implemented in the real world, these findings may help solve an important problem in our society. The evidence supporting the conclusions is solid but more clarity in the way the analyses were conducted would strengthen the study.</p><p>One strength of the study is that it tests a clear hypothesis, namely that changing the incentive structure in social media platforms (so that rewards and punishment that users receive reflect the veracity of the information they share) will decrease the spread of misinformation. The authors suggest changing the current like/dislike feedback by trust/distrust feedback, the latter implying a judgement about veracity of the statement.</p><p>The study involves 3 different experiments. One strength of the study is that the main fundings are replicated in 3 independent experiments. In the first experiment, the authors demonstrate that participants discern true from false information when deciding to trust or distrust a new post to a higher degree than when deciding to like or dislike a post. This claim seems to be supported by the data. In the second and the third studies, the authors show that participants are more likely to share true information when their decisions to share a post result in trust or distrust feedback in comparison to like or dislike feedback or no feedback at all. Moreover, seeing how other participants trust/distrust the posts increase the accuracy of the beliefs that participants hold at the end of the experiment. These claims also seem to be supported by the data.</p><p>These results are clearly relevant and provide a clear and easy to implement solution to a very hot problem in our contemporary society, namely spread of misinformation via social media that contributes to polarization and consolidation of conspiracy theories. In this sense, these results might be very valuable.</p></disp-quote><p>We thank the reviewer for this summary of the findings and for their positive assessment.</p><disp-quote content-type="editor-comment"><p>However, I missed information about what participants were told about the experiment and what were they instructed to do. This information has implications to understand the possible generalization of the results outside of the experimental setting. For example, the authors show that participants engage more with the trust/distrust than with the like/dislike buttons. It is not clear how this result may be influenced by the fact that participants are doing an experiment about trustworthiness of social media posts. The authors rightly acknowledge that their simulated social network is a simplification of a more complex real system. Hence, the importance of knowing every detail of the experimental paradigm and instructions given to the participants.</p></disp-quote><p>We thank the reviewer for prompting us to provide the exact instructions. These are now available in full in Figure 2 – —figure supplement 1, Figure 4 – —figure supplement 1 and 2.</p><disp-quote content-type="editor-comment"><p>The authors used DDM to understand the underlying cognitive mechanism of the observed effects. The standard DMM as implemented in this study assumes that the decision-making process is at equilibrium and does not account for changes due to learning during the experiment. This is a drawback considering that the main effect of changing the incentive structure is expected to be achieved through a learning process. Nevertheless, the DDM could still disentangle between alternative cognitive/computational explanations of the differences between the experimental conditions. But the current implementation is difficult to fully interpret. It is unclear why the starting point is fixed and not a free parameter. Theoretically, it is equally likely that the experimental manipulation induces a bias towards discernment in repost behavior. And if there is a good reason to fix the starting point, it is unclear how it was chosen and how this decision impacts the rest of the DMM results.</p></disp-quote><p>Following the reviewer’s recommendation, we now include the starting point as a free parameter. Again, we find that the drift rate in the ‘(Dis)Trust’ environments (Experiment 2: v=0.216; Experiment 3: v=0.12) was meaningfully higher than the drift rate in both the ‘(Dis)Like’ (Experiment 2: v=0.01; 95% HDI of difference [0.048; 0.183], Experiment 3: 0.037; 95% HDI of difference [0.032;0.135]) and Baseline environments (Experiment 2: v=0.098; 95% HDI of difference [0.041; 0.195]; Experiment 3: v=0.006; 95% HDI of difference [0.061; 0.167]). We do not find a difference in starting point bias across conditions in Experiment 2 and 3 (see pg. 23-28, Figure 6 and 7, Tables 1-2), though in the replications studies, the baseline starting point is greater than in the ‘(Dis)Trust’ and/or ‘(Dis)Like’ conditions (see Supplementary files 15-18).</p><disp-quote content-type="editor-comment"><p>Although strongly mitigated by the fact that the main results are replicated, one possible drawback of the study is that it was not preregistered. The analysis pipeline involves a few researchers' degrees of freedom. I list 3 examples:</p><p>1) It is unclear why the ANOVA in experiment 1 did not include an interaction between type of response and valence. The data seems to show a pattern towards this direction whereby the main behavioral effect is driven by a difference between dislike and distrust whereas the difference between like and trust seems much smaller or non-significant. The omission of the interaction might have affected the reported results.</p></disp-quote><p>Following the reviewer’s suggestion, we have now added the valence x type of reaction interaction to the analysis of Experiment 1. Once again, we find a main effect of type of reaction (F(1,106)=80.936, p&lt;0.001, partial η2=0.43) and valence (F(1,106)=18.26, p&lt;0.001, partial η2=0.15). There was also an interaction of valence x type of reaction (F(1,106)=51.489, p&lt;0.001, partial η2=0.33), which was characterized by the ‘distrust’ button (M=0.157, SE=0.008) being used in a more discerning manner than the ‘trust’ button (M=0.099, SE=0.008; t(106)=9.338, p&lt;0.001, Cohen’s d=0.903), while the ‘like’ button (M=0.06, SE=0.008) was used in a more discerning manner than the ‘dislike’ button (M=0.034, SE=0.008; t(106)=3.474, p&lt;0.001, Cohen’s d=0.336). These results are reported in Supplementary file 1.</p><disp-quote content-type="editor-comment"><p>2) I am not sure that the use of political orientation in the analysis is fully adequate. The authors report an interaction with the main effect in experiment 1 driven by stronger results among democrats when compared to republicans. However, in experiment 2, the interaction is only marginally significant. Despite this, the authors still explore it further to show a difference in discernment between democrats and republicans. In experiment 3, the authors only used a subset of posts that were equally trusted by democrats and republicans. Importantly, the data shows that effect of trust/distrust on repost discernment is unaffected, which strengthens the main results without doubt. However, this rises the possibility that the interaction in experiment 1 is driven by the selection of material. It is also unclear to what extend the results in experiment 1 and 2 are dependent on the inclusion of political orientation in the models. Also, the inclusion of this factors justifies the exclusion of a significant number of participants in experiment 2 and 3.</p></disp-quote><p>Following the reviewer’s comment, we repeated all our analyses without including political orientation as a factor and thus not excluding participants who were ‘independent’. Again, we find in all studies that discernment is greater in the ‘(Dis)Trust’ conditions than the other conditions (Supplementary files 1-12).</p><p>We now also clarify our motivation for including political orientation in the main text. In particular, we included political orientation in our analysis because previous studies suggest a political asymmetry in both the sharing of misinformation online (e.g., Grinberg et al., 2019; Guess et al., 2019) as well as in the subsequent efficacy of interventions aimed at reducing misinformation (e.g., Roozenbeek et al., 2020; Pennycook et al., 2021). For an intervention to be effective it is helpful that it would work across both sides of the political spectrum. As such it is common practice to include political orientation in analyses assessing the efficacy of novel interventions (e.g., Roozenbeek et al., 2022, Pennycook and Rand, 2022; see pg. 5).</p><disp-quote content-type="editor-comment"><p>3) In experiment 2 and 3, the main results are analyzed using a GLM approach. It is unclear why an ANOVA is not used here and how the main regressor was built. The latter may have implications for interpretation of the results.</p></disp-quote><p>We agree an ANOVA is appropriate here and thus report ANOVAs in Experiment 2 and 3 (pg. 12-23). The results remain the same.</p><disp-quote content-type="editor-comment"><p>Try to increase clarity in some points of the text. The use of the term discernment in the abstract is not clear as the term is only defined in the text.</p></disp-quote><p>We thank the reviewer for pointing this out. We now include a definition of discernment in the abstract (pg. 2).</p><disp-quote content-type="editor-comment"><p>State clearly in the Results section that experiment 2 and 3 are between subject (now it is only stated in the figure) along with the number of participants per group, including the number of excluded participants.</p></disp-quote><p>We now state that Experiment 2 and 3 are between-subject (pg.13, 21, 35-36). We also include the number of participants per group and excluded participants (pg. 7, 13-14, 21).</p><disp-quote content-type="editor-comment"><p>Include the interaction between type of response and valence in study 1. Report the results with and without political orientation to show that the results are independent of the inclusion of this factor.</p></disp-quote><p>Following the reviewer’s suggestion, we have repeated all the analyses without political orientation and include an interaction term for valence x type where valence is a factor. Once again, we find that discernment is greater in the ‘(Dis)Trust’ conditions than the other conditions (Supplementary files 1-12).</p><disp-quote content-type="editor-comment"><p>I would suggest that the DDM analysis either includes all model parameters as free or that the authors try to implement different variations (with different free parameters) and perform model comparison to test which is the most parsimonious fit to the data. In the recoverability analysis, I would report the correlation between the simulated and recovered parameters. And if different models are tested, I would include a confusion matrix of the recoverability of the models.</p></disp-quote><p>We now follow the reviewer’s recommendation and include all model parameters, including the starting point, as free. Once again, we find that the drift rate in the ‘(Dis)Trust’ environments (Experiment 2: v=0.216; Experiment 3: v=0.12) was meaningfully higher than the drift rate in both the ‘(Dis)Like’ (Experiment 2: v=0.01; 95% HDI of difference [0.048; 0.183], Experiment 3: 0.037; 95% HDI of difference [0.032;0.135]) and Baseline environments (Experiment 2: v=0.098; 95% HDI of difference [0.041; 0.195]; Experiment 3: v=0.006; 95% HDI of difference [0.061; 0.167]). By contrast there is no difference in drift rate between the latter two environments (Experiment 2: 95% HDI of difference: [-0.075; 0.08]; Experiment 3: 95% HDI of difference [-0.016; 0.079]). We do not find a difference in starting point bias across conditions in the main studies (Experiment 2 and 3) (see pg. 2328, Figure 6 and 7, Tables 1-2), though in the replications studies the baseline starting point is greater than in the ‘(Dis)Trust’ and/or ‘(Dis)Like’ conditions (see Supplementary files 15-18). In addition, as requested, we now report the correlations between real and recovered parameters (see Supplementary file 23).</p><disp-quote content-type="editor-comment"><p>When presenting the DDM results, I would avoid the systematic use of &quot;good&quot; and &quot;bad&quot; labels for the responses. It is fine to use this metaphor in one instance, but it sounds like an oversimplification when it is repeated many times.</p></disp-quote><p>As recommended by the reviewer we now changed the labels to “veracity promoting” and “veracity obstructing” (pg. 23-28 and pg. 41-42).</p><disp-quote content-type="editor-comment"><p>In the first paragraph of the discussion, the authors state &quot;Users who found themselves in such an environment began sharing more true than false posts in order to receive carrots and avoid sticks.&quot; I would reformulate this statement not to imply learning. Although learning is implicit by the fact that the effects are a consequence of changing the incentive structure, the behavioral data analysis is aggregated across the whole experiment.</p></disp-quote><p>Following the reviewer’s suggestion, we now rephrased this sentence to <italic>“Users who found themselves in such an environment, shared more true than false posts in order to receive carrots and avoid sticks</italic>” (pg. 29)</p><disp-quote content-type="editor-comment"><p>Figure 7 is missing</p></disp-quote><p>We thank the reviewer for bringing this to our attention. Figure 7 was incorrectly labelled as Figure 8. We have now rectified this (pg. 27-28).</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This paper suggests that changing the incentive structure on social platforms may influence people's discernment in their sharing decisions. Particularly, in experiment 1, the authors suggest that participants' use of trust-distrust buttons were more discerning than their use of like-dislike button. In experiments 2 and 3, the authors suggest that operating in a trust-distrust environment makes people's sharing more discerning than operating in a like-dislike or no-feedback environments. They also test a drift rate and claim that people place more weight to accuracy when provided with the distrust-trust feedback.</p><p>Strengths:</p><p>– The paper examines a novel mechanism to prevent misinformation spread on social platforms.</p><p>– The authors' idea of offering buttons that are better linked to headline veracity is a practical solution that could be easily implemented.</p></disp-quote><p>We thank the reviewer for pointing out these strengths.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>– The conceptualization could be better articulated with wider coverage of the misinformation literature.</p></disp-quote><p>In the revised manuscript we better articulate the conceptualization (pg. 4-6) and cover more of the misinformation literature (pg. 5, 30-32).</p><disp-quote content-type="editor-comment"><p>– There is a disconnect between the theory and the empirical studies.</p></disp-quote><p>We find that the empirical findings follow our theory closely. In particular, our theory is that (i) people use ‘(Dis)Trust’ reaction buttons to discern true from false posts more so than the existing reaction buttons such as ‘(Dis)Like’. This is because trust by definition is the belief in reliability of information; and (ii) once users are exposed to an environment in which they can receive more ‘trusts’ carrots and fewer ‘distrust’ sticks for true than false posts they will share more true and less false information. Our empirical results show exactly that; (i) participants use ‘(Dis)Trust’ reaction buttons to discern true from false posts more than the existing ‘(Dis)Like’ reaction button options (Figure 3, pg. 10); and (ii) participants who are exposed to an environment in which they can receive more ‘trusts’ carrots and fewer ‘distrust’ sticks for true than false posts share more true and less false information than participants in other environments (Figure 5, pg. 18).</p><disp-quote content-type="editor-comment"><p>– Fake and true headlines may be different from one another on other factors (e.g., emotions) that affect people's trust judgments.</p></disp-quote><p>The reviewer raises the possibility that false and true headlines may differ systematically on factors such as emotion, and this difference may drive a trust judgement. We do not see this possibility as contradicting our conclusions. If people use a signal that is associated with veracity on social media platforms, such as emotion, to guide their discernment, that will still lead to a successful intervention in reducing misinformation spread. We note however, that there is no a-priori reason to assume that factors such as emotion will impact ‘(Dis)Trust’ reactions more so than ‘(Dis)Like’ reactions. Thus, we find this possibility less likely.</p><disp-quote content-type="editor-comment"><p>– The design of experiments 2 and 3 fails to accurately connect trust judgments to the veracity of the headlines.</p></disp-quote><p>‘Trust’ and ‘distrust’ judgments are directly connected to the veracity of the headlines. As described in the manuscript, participants use both ‘trust’ and ‘distrust’ buttons to discern between true and false posts (pg.9-10). Participants in Experiment 1 selected the ‘trust’ button more for true (M=18.897, SE=1.228) than false posts (M=9.037, SE=0.78; t(106)=9.846, p&lt;0.001, Cohen’s d=0.952). They also selected the ‘distrust’ button more for false (M=24.953, SE=1.086) than true posts (M=9.252. SE=0.731; t(106)=16.019, p&lt;0.001, Cohen’s d=1.549). These numbers of ‘trusts’ and distrusts’ per posts, which were gathered in Experiment 1, were then fed directly to the new groups of participants in Experiment 2 and Experiment 3 (see Figure 4, pg. 13, 36). Thus, they receive more ‘trusts’ for true than false posts, and more ‘distrusts’ for false than true posts.</p><disp-quote content-type="editor-comment"><p>Major Comments</p><p>Conceptualization:</p><p>Conceptually, authors suggest that adding buttons should make people's evaluations of headlines (in experiment 1) and the presence of much wider variety of feedback will make their sharing decisions better (in experiments 2 and 3).</p><p>1. The reader would like to see better build-up of the theoretical framework. Some questions are:</p><p>a. Whereas social media sharing (especially sharing 40-100 headlines) seem to be repetitive and can lead participants to develop habits (Anderson and Wood 2021; Ceylan et al., 2023). Do habits play a role? With the change in incentive structure, do participants develop different sharing habits?</p></disp-quote><p>Yes, changing the incentive structure could lead users to develop new sharing habits. We now note this possibility in the discussion (pg. 31).</p><disp-quote content-type="editor-comment"><p>b. Would we expect differential evaluation and sharing as a function of valence of the feedback button? The authors claim in many places that &quot;… humans naturally seek social rewards and are motivated to avoid social punishments (Behrens, Hunt, Woolrich, and Rushworth, 2008; Bhanji and Delgado, 2014).&quot; Do they test and find evidence for this? (More discussion and suggestions are below).</p></disp-quote><p>Yes, we expect and find that participants share differently based on the valence of the feedback button. Participants are expected to behave in a way that reflects a desire to attain more ‘trusts’ and ‘likes’ per post and fewer ‘distrust’ and ‘dislikes’ per posts. This is exactly what we find. In Experiment 1 more participants pressed ‘trust’/’like’ reaction buttons for true than false posts and more participants pressed ‘distrust’/‘dislike’ reaction buttons for false than true posts. In Experiment 2 participants were then fed the feedback which was gathered from (different) participants in Experiment 1. This means that participants in Experiment 2 would be able to maximize the average number of ‘trusts’/’likes’ they received per post by sharing more true posts and fewer false posts. They would also be able to minimize the average number of ‘distrusts’/’dislikes’ they received per post by sharing fewer false posts and more true posts. This is exactly what they do (Figure 5, pg. 18). This is true in Experiment 2 where only positively or only negatively valenced feedback is provided and also in Experiment 3 where both positively and negatively valenced feedback is provided together. The key is reducing the magnitude of negative feedback per post and increasing the magnitude of positive feedback per post. Just like in real life where a user does not simply want a ‘heart’ they want many ‘hearts’ (such as on Twitter) and they want the minimum number of ‘dislikes’ (such as on Youtube).</p><disp-quote content-type="editor-comment"><p>2. Figure 1b and 1c are rather confusing than clarifying. If I understand correctly, in experiment 2, participants receive one type of feedback that is disconnected from the actual veracity of the information. If people receive just distrust feedback for all the headlines (true and false), how is this in line with the idea that authors align carrots with truth and sticks with falsity?</p></disp-quote><p>Following the reviewer’s comment, it became apparent that the design and rationale behind Experiment 2 (pg.12-20) was unclear. The feedback participants receive is not disconnected from the actual veracity of the information. Let’s take the ‘distrust’ condition as an example. For any given post, true or false, some users will distrust the post. However, true posts receive fewer ‘distrusts’ than false posts. It is the number of ‘distrusts’ per post that matters. The participants in Experiment 2 (we assume) are motivated to minimize the average number of ‘distrusts’ they receive. To achieve this, they should post more true posts and fewer false posts. The same rationale holds for the group of participants that only receive ‘trusts’. They will receive more ‘trusts’ for true than false posts. It is the magnitude of ‘trusts’ that is associated with veracity. Again, this motivates participants to post more true and fewer false posts in order to maximize the average number of ‘trusts’ per post. Of course, if participants were simply trying of maximizing the total number of ‘trusts’ in Experiment 2, they would just repost on every trial. Participants do not do that. This indicates that they are sensitive to the number of ‘trusts’ per posts not just to the total number over all posts. Any user of social media platforms could relate to this. When posting a tweet, for example, many people will be disappointed with only a handful of ‘hearts’. The user’s goal is to maximize positive feedback per post. Similarly, if participants were simply trying of minimize the total number of ‘distrusts’ in Experiment 2 they would just skip on every trial. Participants do not do that, presumably because humans find the act of sharing information rewarding (Tamir and Mitchell, 2012). Thus, participants are motivated to share, but to do so while minimizing ‘distrusts’ per post. We now add this clarification to the revised manuscript (pg. 16-17).</p><disp-quote content-type="editor-comment"><p>Empirics:</p><p>My overall feedback is that authors should better connect their conceptualization to their empirics.</p><p>3. Why is the participant the evaluator of the news headline in experiment 1 and then the sharer of the headline in experiments 2 and 3?</p></disp-quote><p>This is a misunderstanding. Participants are different in each experiment. The participants in Experiment 1 are not the same participants as in Experiment 2, which are not the same participants as in Experiment 3. This is now explicitly stated on pg. 6, 12, 21.</p><disp-quote content-type="editor-comment"><p>4. The reader wonders where the action comes from: is it from reduction in rejecting false information or increasing in endorsing true information? In the model authors shared in the Supplemental Appendix Table 2, can the authors run a mixed effect model on the % of headlines engaged (not skipped) as a function of type of reaction and valence of reaction? This will help authors test their theory more robustly.</p><p>a. The same for Supplemental Appendix Tables 5 and 7. Authors can test their theory by including an interaction term for type of reaction and valence of that reaction.</p></disp-quote><p>Following the reviewer’s request, we now add the interaction between type of reaction and valence of reaction to the above Tables. Note that the analysis is now an ANOVA rather than a mixed model, due to the request of Reviewer 2 to uses ANOVAs. What was Supplemental Appendix Table 2 (Experiment 1) is now Supplementary file 2. As a reminder, this table examined if the likelihood of clicking the different buttons was different for the different type of button options overall. Thus, the interaction asks if the likelihood of clicking the different button types differs as a function of valence. The answer is no. As can be seen, the interaction is not significant: F(1,106)=0.19, p = 0.64. We then do the same for what was Supplemental Appendix Tables 5 (Experiment 2), which is now Supplementary file 6. As a reminder, this Table originally asked if the likelihood of sharing posts was different across the different type of conditions. Thus, the interaction asks if the likelihood of sharing posts in the different conditions differs as a function of valence. The answer is no. The interaction is not significant: Experiment 2: F(1,311)=0.199, p = 0.656. Together the results suggest that the likelihood of reacting (Experiment 1) and the likelihood of reposting (Experiment 2) is not different for the different conditions as a function of valence options. As for what was Supplemental Appendix Table 7 (Experiment 3) valence of feedback does not exist as participants receive positive and negative feedback at the same time (‘Trust and Distrust’ and ‘Like and Dislike’), thus there is no interaction to be had.</p><disp-quote content-type="editor-comment"><p>5. In experiment 1, how did &quot;skip&quot; reaction impact people's discernment? Were those who skipped more discerning than those who used like-dislike buttons?</p></disp-quote><p>To test for a relationship between skipping and discernment we correlated across participants the percentage of posts skipped with discernment. We found that participants who skipped more posts were <italic>less</italic> discerning (R=-0.414, p&lt;0.001). We now report this on pg. 11.</p><disp-quote content-type="editor-comment"><p>6. In experiments 2 and 3, the authors provide participants with 100 headlines. Participants make a skip or repost decision. Then, participants receive one of the types of feedback depending on their condition. One problem with this design and what authors suggest in Figure 1 is that participants receive one type of feedback in their respective condition. For instance, in the distrust condition, participants receive distrust feedback no matter what the veracity of the headline is. This is problematic because in this model, people do not have an opportunity to learn and connect veracity with trust feedback. I suggest authors could collect new data by cleaning up this mapping. The reader wonders how the results would change if this mapping actually happens and participants receive trust feedback when they share accurate information and distrust feedback when they share false information.</p><p>a. This may explain why the authors do not find a differential effect between trust and distrust conditions. Trust feedback that is disconnected from the actual veracity of the headline would not enhance people's discernment differentially when this feedback is negative or positive. It seems like participants were just operating in an environment knowing that the feedback will be about trust but no matter what they share the valence of the trust would not change.</p></disp-quote><p>There seem to be a few misunderstandings here (see also response to comment 2 above). First, in Experiment 3 participants receive two types of feedback simultaneously, not one (either ‘trust’ and ‘distrust’ or ‘like’ and ‘dislike’). Second, the feedback in both Experiment 2 and Experiment 3 <italic>is dependent</italic> on veracity. What matters is the number of ‘trusts’ or ‘distrusts’ per post. True posts receive more ‘trusts’ (M=18.897, SE=1.228) than false posts (M=9.037, SE=0.78; t(106)=9.846, p&lt;0.001, Cohen’s d=0.952) and false posts (M=24.953, SE=1.086) receive more ‘distrusts’ than true posts (M=9.252. SE=0.731; t(106)=16.019, p&lt;0.001, Cohen’s d=1.549). This is the signal participants are picking up and learning from. Thus, participants indeed have an opportunity to learn veracity from ‘trust’ (and from ‘distrust’) feedback. Both ‘trusts’ and ‘distrusts’ are associated with veracity: greater number of ‘trusts’→ more likely to be true; smaller number of ‘distrusts’→ more likely to be true. There is no reason to apriori hypothesize that one condition (‘trust’ or ‘distrust’) will be better at driving discernment than the other. We try to make this clearer in the revised manuscript (pg. 16-17).</p><disp-quote content-type="editor-comment"><p>7. Further, in the current execution, participants learn about the reward and use this learning in their sharing decision all at one stage. However, the execution of the study can be cleaner if the authors separate the learning stage from the test stage (see Study 4 in Ceylan et al., 2023).</p></disp-quote><p>This is not entirely correct. First, participants make a decision to repost or skip a specific post (Post 1). This decision is final. Only then, do they receive feedback regarding Post 1. After this, they make a decision to repost or skip a completely different post (Post 2) and so on and so forth. Thus, they cannot use the feedback about Post 1 to make a decision about Post 1, because that decision has already been made. However, from the feedback about Post 1, they can learn about the consequences of posting true vs false posts and they can use this understanding to direct their next choice (Post 2). Our task is designed to mirror real life in this aspect. On Twitter and elsewhere, users do not spend a week (or a day) only making sharing decisions and another week (or day) only observing if others liked and retweeted their post. In any case, it is unclear why separating the stages will be cleaner or how exactly it may impact the conclusions.</p><disp-quote content-type="editor-comment"><p>8. One concern is that the authors stripped the headlines from any factor that could contribute people's truth and trust judgments. For instance, source credibility or consensus are factors that aid people's judgments on social platforms such as Facebook and Twitter.</p></disp-quote><p>Now that we have shown how an incentive structure can in principle reduce misinformation spread, it would be useful for future studies to test this intervention in real-world platforms and/or simulated platforms that include additional factors observed in real platforms. We make this point in the discussion (pg. 31).</p><disp-quote content-type="editor-comment"><p>One may wonder how people make their truth judgments in the absence of these cues. Can people discern true information from false information?</p></disp-quote><p>Yes, our results show that participants discern true from false information in the absence of cues such as the source of the information (see Figure 5, pg. 18 and Supplementary 5 – —figure supplement 1). The Y axis is discernment, which reflects the ability to discern true from false information. The (**) indicates that – yes – participants significantly discern true form false information in all studies when receiving ‘(Dis)Trust’ and ‘(Dis)Like’ feedback. In many cases we see they do so also when receiving no feedback. The thing to look out for is that the bars are significantly different from zero (Figure 5c, pg. 18). Crucially, they do so most when receiving ‘(Dis)Trust’ feedback.</p><disp-quote content-type="editor-comment"><p>In experiment 2, authors measure the discernment, but I did not see the means between true and false headlines.</p></disp-quote><p>We now report the means for true and false posts in Supplementary file 4.</p><disp-quote content-type="editor-comment"><p>Also, do people's trust and like responses correlate with the actual veracity of the information?</p></disp-quote><p>Yes, people trust and like true posts more than false posts. This can be observed by the corresponding bars measuring discernment in Experiment 1 (Figure 3, pg. 10). The fact that these bars are significantly different than zero indicated they like and trust true more than false posts. We also report this information in text on pg. 9 in the manuscript. Veracity is binary (true/false) rather than a continuum. Thus, the analysis is a t-test rather than a correlation.</p><disp-quote content-type="editor-comment"><p>9. Trust judgments also bear on other factors. One such factor is emotions (Dunn and Schweitzer, 2005). The reader wonders what the valence of the emotions is between the true and false headlines. A pretest that shows differences in emotions between false and true headlines would be helpful to the reader.</p></disp-quote><p>Whether and how emotion plays a role is an intriguing question. We hope this paper will prompt future studies to measure and test the role of emotion in the success of this intervention.</p><disp-quote content-type="editor-comment"><p>10. I am not very familiar with the computational analysis calculating drift rate. For readers who will not be very familiar with this method will need more detail about the analysis. Also, it seems like the results from experiments 2 and 3 do not show similar trends. Specifically, in experiment 2, the distance between the thresholds is the highest in the trust-distrust conditions and the non-decision time is the lowest. In experiment 3, there is no difference in distance between the thresholds across all three conditions. This time, the non-decision time was the highest in the trust-distrust conditions. How do the authors reconcile these differences?</p></disp-quote><p>In all sharing experiments (Experiment 2, Experiment 3, Experiment 5, Experiment 6), we observe a clear, significant and consistent difference in drift rate across conditions. This suggests that the rate of evidence accumulation toward ‘good judgments’ relative to ‘bad judgments’ is higher in the ‘(Dis)Trust’ environments than the other environments. This difference is consistently observed across all experiments and is thus the conclusion one should take away from the DDM analysis. As for other effects which are either observed as non-significant trends or are not replicable – we recommend caution in trying to interpret them. The reason we test the effects over and over, using slight differentiations across experiments, is that we wish to identify the most robust effects.</p></body></sub-article></article>