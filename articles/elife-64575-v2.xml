<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64575</article-id><article-id pub-id-type="doi">10.7554/eLife.64575</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Value representations in the rodent orbitofrontal cortex drive learning, not choice</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-213576"><name><surname>Miller</surname><given-names>Kevin J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3465-2512</contrib-id><email>kevinjmiller@deepmind.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-285635"><name><surname>Botvinick</surname><given-names>Matthew M</given-names></name><email>botvinick@deepmind.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-3699"><name><surname>Brody</surname><given-names>Carlos D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4201-561X</contrib-id><email>brody@princeton.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hx57361</institution-id><institution>Princeton Neuroscience Institute, Princeton University</institution></institution-wrap><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00971b260</institution-id><institution>DeepMind</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Department of Ophthalmology, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Gatsby Computational Neuroscience Unit, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hx57361</institution-id><institution>Howard Hughes Medical Institute and Department of Molecular Biology, Princeton University</institution></institution-wrap><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute on Drug Abuse, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>17</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e64575</elocation-id><history><date date-type="received" iso-8601-date="2020-11-03"><day>03</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2022-08-01"><day>01</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2018-01-10"><day>10</day><month>01</month><year>2018</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/245720"/></event></pub-history><permissions><copyright-statement>© 2022, Miller et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Miller et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64575-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-64575-figures-v2.pdf"/><abstract><p>Humans and animals make predictions about the rewards they expect to receive in different situations. In formal models of behavior, these predictions are known as value representations, and they play two very different roles. Firstly, they drive <italic>choice</italic>: the expected values of available options are compared to one another, and the best option is selected. Secondly, they support <italic>learning</italic>: expected values are compared to rewards actually received, and future expectations are updated accordingly. Whether these different functions are mediated by different neural representations remains an open question. Here, we employ a recently developed multi-step task for rats that computationally separates learning from choosing. We investigate the role of value representations in the rodent orbitofrontal cortex, a key structure for value-based cognition. Electrophysiological recordings and optogenetic perturbations indicate that these representations do not directly drive choice. Instead, they signal expected reward information to a learning process elsewhere in the brain that updates choice mechanisms.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>planning</kwd><kwd>orbitofrontal cortex</kwd><kwd>learning</kwd><kwd>decision making</kwd><kwd>electrophysiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T-32 MH065214</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Kevin J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006734</institution-id><institution>Princeton University</institution></institution-wrap></funding-source><award-id>Harold W Dodds Fellowship</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Kevin J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurons in the OFC signal expected reward specifically when this information is used for learning rather than for choosing, and silencing these neurons impairs use of this information to learn.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Representations of expected value play a critical role in human and animal cognition (<xref ref-type="bibr" rid="bib77">Sugrue et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Daw and O’Doherty, 2014</xref>). A key brain region involved in representing and using expected value information is the orbitofrontal cortex (OFC). An active and unresolved debate in the literature involves theoretical accounts of the OFC that emphasize roles in either choosing (<xref ref-type="bibr" rid="bib83">Wallis, 2007</xref>; <xref ref-type="bibr" rid="bib59">Padoa-Schioppa and Conen, 2017</xref>), in learning (<xref ref-type="bibr" rid="bib72">Schoenbaum et al., 2009</xref>; <xref ref-type="bibr" rid="bib86">Walton et al., 2011</xref>; <xref ref-type="bibr" rid="bib75">Song et al., 2017</xref>), or both (<xref ref-type="bibr" rid="bib55">O’Doherty, 2007</xref>; <xref ref-type="bibr" rid="bib84">Wallis, 2012</xref>; <xref ref-type="bibr" rid="bib67">Rudebeck and Murray, 2014</xref>; <xref ref-type="bibr" rid="bib87">Wilson et al., 2014</xref>; <xref ref-type="bibr" rid="bib76">Stalnaker et al., 2015</xref>). Studies perturbing OFC activity have variously reported behavioral effects consistent with either altered learning (<xref ref-type="bibr" rid="bib79">Takahashi et al., 2009</xref>; <xref ref-type="bibr" rid="bib85">Walton et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">McDannald et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Jones et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Gardner et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Gardner et al., 2020</xref>), or with altered choosing, including conditions involving choosing between multiple possible actions (<xref ref-type="bibr" rid="bib51">Murray et al., 2015</xref>; <xref ref-type="bibr" rid="bib4">Ballesta et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Kuwabara et al., 2020</xref>) and involving choosing the frequency with which to perform a single action (<xref ref-type="bibr" rid="bib28">Jones et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Gremel and Costa, 2013</xref>). Recording studies in many species have revealed neural correlates of expected value in the OFC (<xref ref-type="bibr" rid="bib80">Thorpe et al., 1983</xref>; <xref ref-type="bibr" rid="bib70">Schoenbaum et al., 1998</xref>; <xref ref-type="bibr" rid="bib22">Gottfried et al., 2003</xref>; <xref ref-type="bibr" rid="bib56">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib78">Sul et al., 2010</xref>), but it has not been clear whether these neural correlates of expected value are selective for roles in learning and in choosing. This, along with limitations inherent in neural perturbation studies, has made it difficult to determine whether the OFC plays a role in learning, choosing, or both.</p><p>Perturbation studies designed to separate learning from choosing have typically adopted a temporal strategy, attempting to behaviorally separate the time of learning from the time of choosing, and performing neural silencing experiments specifically at one time or the other. Some experiments of this type are consistent with a role for the OFC in learning: silencing the OFC specifically during the learning phase of either of two associative learning procedures (blocking or overexpectation) impairs subsequent behavior on an assay performed with the OFC intact (<xref ref-type="bibr" rid="bib79">Takahashi et al., 2009</xref>; <xref ref-type="bibr" rid="bib28">Jones et al., 2012</xref>). Other experiments of this type are consistent with a role for the OFC in choosing: silencing the OFC specifically during the post-learning assay of either of two associative learning procedures (sensory preconditioning or outcome devaluation) impairs expression of knowledge gained earlier (<xref ref-type="bibr" rid="bib28">Jones et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Gremel and Costa, 2013</xref>; <xref ref-type="bibr" rid="bib51">Murray et al., 2015</xref>).</p><p>One limitation of studies that attempt to separate learning and choosing temporally is that these are fundamentally cognitive events, internal to the brain, and their timing cannot be fully controlled by an experimenter. For example, it has been suggested that subjects faced with a choice might reconsider information that was presented in the past; cognitively, this might best be interpreted as a learning process happening during a putative choice epoch (<xref ref-type="bibr" rid="bib21">Gershman et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Lombrozo, 2017</xref>; <xref ref-type="bibr" rid="bib42">Ludvig et al., 2017</xref>). Conversely, subjects experiencing a surprising outcome might consider its implications for future choices that they expect to be faced with, and memorize decisions for later use (<xref ref-type="bibr" rid="bib44">McDaniel and Einstein, 2007</xref>) cognitively, this might best be interpreted as a choice process happening during a putative learning epoch. There is no a priori requirement for learning and choosing to be implemented in separate neural circuits. The approach we used here does not depend only on timing, but importantly also uses signals internal to the brain (firing rates, effect of OFC perturbations), to quantify the relative contributions of learning and choosing in the OFC. A second limitation is of previous studies is that they typically address forms of learning that unfold over many sessions, a very different timescale and behavioral regime to the one in which neural representations of value in the OFC are typically studied. Since different neural mechanisms may be at play in trial-by-trial vs. session-by-session learning, it is difficult to confidently interpret the results of neural recording experiments addressing the former in light of neural silencing experiments addressing the latter.</p><p>Studies characterizing neural correlates of expected value in the OFC have used a different set of behavioral methods. Typically, they adopt a task design which facilitates the analysis of neural recording by allowing data to be aggregated over many repeated trials. In each trial, the subject is presented with one or more options, chooses among them, and receives a reward associated with the chosen option. Some studies (<xref ref-type="bibr" rid="bib80">Thorpe et al., 1983</xref>; <xref ref-type="bibr" rid="bib82">Wallis and Miller, 2003</xref>; <xref ref-type="bibr" rid="bib78">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib11">Costa and Averbeck, 2020</xref>) present options that are novel or have repeatedly changing reward associations in order to elicit trial-by-trial learning, interleaving decision-making with learning by design. Other studies (<xref ref-type="bibr" rid="bib81">Tremblay and Schultz, 1999</xref>; <xref ref-type="bibr" rid="bib56">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib29">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib66">Rudebeck et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Blanchard et al., 2015</xref>; <xref ref-type="bibr" rid="bib10">Constantinople et al., 2019</xref>) present options whose reward associations are stable and well-learned, with the intention of isolating decision-making specifically. Although not incentivized by task design, trial-by-trial learning is frequently evident in these tasks as well (<xref ref-type="bibr" rid="bib58">Padoa-Schioppa, 2013</xref>; <xref ref-type="bibr" rid="bib10">Constantinople et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Lak et al., 2020</xref>). The fact that learning and choosing are intertwined in these tasks, operating at similar times and over the same set of items, has made it difficult to determine whether OFC value representations are selective for one or the other process, or whether they play a role in both.</p><p>Here, we adopt a computational approach that separates learning from choosing, even while using a repeated-trials task that facilitates analysis of neural recordings. This approach is based not only on <italic>when</italic> learning versus choosing happens, but also on <italic>what content</italic> they operate over. We use a multi-step decision task <xref ref-type="bibr" rid="bib13">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>, in which a choice made on a first step is linked probabilistically to an outcome that occurs on a second step, which in turn is linked probabilistically to reward (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Rats adopt a model-based planning strategy that respects this structure (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). On the first step of every trial rats make their choice based on values that are computed, not learned (‘Compute Choice Port Values’ and ‘Choose a Choice Port’ in <xref ref-type="fig" rid="fig2">Figure 2a</xref>), while on the second step they learn the values of the outcomes (‘Learn Outcome Port Values’ in <xref ref-type="fig" rid="fig2">Figure 2a</xref>) but are led to those outcomes without making a choice. This task structure, in which choices and outcomes are linked probabilistically rather than having a 1-to-1 relationship, combined with the planning strategy rats use to solve it, provides the critical separation necessary to differentiate learning from choosing.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Two-step task for rats.</title><p>(<bold>a</bold>) Rat two-step task. The rat initiates a trial by entering the top center port (<bold>i</bold>), then chooses to enter one of two choice ports (<bold>ii</bold>). This leads to a probabilistic transition (<bold>iii</bold>) to one of two possible paths. In both paths, the rat enters the bottom center port (<bold>v</bold>), causing one of two outcome ports to illuminate. The rat enters that outcome port (<bold>v</bold>), and receives a reward (<bold>vi</bold>). (<bold>b</bold>) Example behavioral session. At unpredictable intervals, outcome port reward probabilities flip synchronously between high (80%) and low (20%). The rat adjusts choices accordingly. (<bold>c</bold>) The fraction of trials on which each rat (n=19) selected the choice port whose common (80%) transition led to the outcome port with the currently higher reward probability, as a function of the number of trials that have elapsed since the last reward probability flip.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig1-v2.tif"/></fig><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Planning strategy separates learning, choosing.</title><p>(<bold>a</bold>) Schematic of the planning strategy. Agent maintains value estimates (<bold>V</bold>) for each outcome port, based on a history of recent rewards at that port; as well as value estimates (<bold>Q</bold>) for each choice port, which are computed on each trial based on the outcome values and the world model (<italic>P(o|c</italic>)). Choices are drawn probabilistically, based on a weighted combination of these values and of the influence of three other behavioral patterns: perseveration, novelty preference, and bias (see Methods for details). (<bold>b</bold>): Mixture weights of the different components of the cognitive model fit to rats’ behavioral data. shown for electrophysiology rats (n=6, squares), optogenetics rats (n=9, triangles), and sham optogenetics rats (n=4, diamonds). (<bold>c</bold>) Change in quality of model fit resulting from removing components from the model (red) or adding additional components (green). (<bold>d</bold>) Fit weights of the trial-history regression for an example rat (left) and averaged over all rats (right). (<bold>e</bold>) Definitions of the four behavioral indices in terms of the fit stay/switch regression weights. The planning index for a particular rat is defined as the sum of that rat’s common-reward and the uncommon-omission weights, minus the sum of its common-omission and uncommon-reward weights. The ‘stay’ index is defined as the sum of all weights. (<bold>f</bold>) Values of the four behavioral indices for all rats. The planning and stay indices are large and positive for all rats, while the common-stay/uncommon-switch and win-stay/lose-switch indices are smaller and inconsistent in sign.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Learning rate parameters.</title><p>Fit learning rate parameters from the mixture-of-agents model for the model-based planning agent and for the perseverative agent. Learning rates for perseveration are smaller for all rats, indicating that this agent takes into account a larger number of recent trials than the model-based agent does. Shapes of symbols indicate rats participating in the electrophysiology (squares), optogenetics (triangles), or sham optogenetics (diamonds) experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Weights of trial-history regression model, fit both to rats’ behavioral data and to synthetic datasets generated by our mixture-of-agents model fit separately to each rat.</title><p>The planning strategy is characterized by a greater tendency to repeat choices that lead to rewards following a common transition than following an uncommon transition (solid blue line above dotted blue line), as well as a greater tendency to repeat choices that lead to omissions following an uncommon transition than following a common transition (dotted red line above solid red line). All nineteen rats show evidence of such a strategy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig2-figsupp2-v2.tif"/></fig></fig-group><p>We asked whether OFC neuron firing rates were correlated with the expected values of the items being learned about, the values being chosen between, or both. We find that neurons in the OFC correlate significantly with the expected values of the items being learned about, but only weakly with the expected values of the items being chosen between. This indicates that, within a repeated-trials task, neural representations in the OFC carry value information that is largely selective for a role in learning, but only weakly carry value information selective for a role in choice. To causally probe whether OFC plays a role in learning, in choosing, or in both processes, we transiently silenced OFC activity, and found that the pattern of behavior induced by this silencing was reproduced in our computational model only when we disrupted only the role of expected value in learning. Disrupting the role of value in choice did not reproduce this effect. In this model, learning could still occur, but was impaired due to one of its key input signals, expected value, being disrupted.</p><p>These results suggest that, even when choosing and learning are commingled on the same trial-by-trial timescale, value representations in the rodent OFC do not drive choices directly, but instead support learning. Moreover, we identify a specific computational role for OFC within learning: it supplies information about expected outcomes to a separate learning process, elsewhere in the brain, that updates value estimates, which can then in turn be used to drive future behavior (<xref ref-type="bibr" rid="bib73">Schoenbaum et al., 2011</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Planning strategy in the two-step task separates choosing and learning</title><p>We trained rats to perform a multistep decision-making task in which they adopt a strategy of model-based planning (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>). The structure of the task was as follows: The rat initiated each trial by poking its nose into a neutral center port, and then selected one of two choice ports (<xref ref-type="fig" rid="fig1">Figure 1a</xref> <bold>i,ii</bold>). One choice caused a left outcome port to become available with probability 80% (‘common’ transition), and a right outcome port to become available with probability 20% (‘uncommon’ transition), while the opposite choice reversed these probabilities (<xref ref-type="fig" rid="fig1">Figure 1a</xref> <bold>iii</bold>). These transition probabilities were fixed for each rat, but counterbalanced across rats. Following the initial choice, an auditory cue informed the rat which of the two outcome ports had in fact become available on that trial and, after poking into a second neutral center port, the available outcome port was further indicated by a light (<xref ref-type="fig" rid="fig1">Figure 1a</xref> <bold>iv,v</bold>). The rat was required to poke into the available outcome port (no choice in this step), where it received a water reward with some probability (<xref ref-type="fig" rid="fig1">Figure 1a</xref> <bold>v,vi</bold>). The reward probability at each outcome port on each trial was either 80% or 20%, and these probabilities reversed at unpredictable intervals (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Earning a high reward rate on this task requires learning which outcome port currently has the higher reward probability, and choosing the choice port that is more likely to lead to that outcome port. Rats did this successfully, switching their average choices to the appropriate choice port within several trials of a reward probability reversal (<xref ref-type="fig" rid="fig1">Figure 1b and c</xref>).</p><p>A high reward rate on this task can be achieved by multiple cognitive strategies <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/SgyBce/MKfd+83q5">Daw et al., 2011;</ext-link> <xref ref-type="bibr" rid="bib32">Kool et al., 2016</xref>. In previous work (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>), we showed that rats solve the task using a particular strategy termed ‘model-based planning’ (<xref ref-type="bibr" rid="bib16">Dolan and Dayan, 2013</xref>; <xref ref-type="bibr" rid="bib49">Miller and Venditto, 2021</xref>), and presented a cognitive model implementing this strategy (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Model-based planning in our task involves separate representations of the expected value associated with the outcome ports and of the expected value associated with the choice ports, and very different computational roles for each type of value representation. Outcome port values, (labeled <italic>V(o</italic>) in our model), represent an estimate of the reward that can be expected following a visit to the corresponding outcome port (<italic>o</italic>). They are updated incrementally by a learning process that compares this expectation to the reward actually received on each trial (labeled <italic>R<sub>t</sub></italic>, <xref ref-type="fig" rid="fig2">Figure 2a</xref>, left: ‘Learn Outcome Port Values’). We used a symmetric update for the value of the outcome port that was not visited (see Methods). Choice port values (labeled <italic>Q(c</italic>) in our model) represent an estimate of the reward that can be expected following a visit to the corresponding choice port (<italic>c</italic>). They are computed based on the known transition probabilities between first-step choice and second-step outcome (the ‘world model’, <italic>P(o|c</italic>); <xref ref-type="fig" rid="fig2">Figure 2a</xref>, center: ‘Compute Choice Port Values’). These choice port values are then used to determine the next choice (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, right: ‘Choose a Choice Port’). The values of the choice ports therefore drive choice directly, while the learned values of the outcome ports support choice only indirectly, by directly supporting learning.</p><p>As in our previous study (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>), rat behavior was well-described by a cognitive model combining this model-based planning strategy with a mixture of three additional components. The first of these is ‘perseveration’, which reflects a tendency to repeat past choices, regardless of their outcomes (<xref ref-type="bibr" rid="bib1">Akaishi et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Miller et al., 2019</xref>). The second, which we term ‘novelty preference’, reflects a tendency to repeat (or to switch away from) choices that lead to an uncommon transition, regardless of whether or not they are rewarded. The third is a constant side bias, reflecting an overall tendency to prefer either the right or the left choice port. Each of these components is associated with a weighting parameter (β), reflecting the strength of its influence on the decision between the left and right choice port on each trial (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, right: ‘Choose a choice port’). Fitting these parameters to the dataset for each rat, we find that the planning and perseverative components earn weights that are large and positive (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), while the novelty preference and bias components earn weights that are generally smaller and differ in sign among rats (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The planning and perseveration components are each associated with a learning rate parameter (α), which reflects the relative influence of trials in the recent past vs the more distant past. Learning rates for the planning component were consistently larger than those for the perseverative component (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>We validate that our cognitive model provides a good description of rat behavior in two different ways. The first way is quantitative model comparison: we compute a quality-of-fit score for the model using cross-validated likelihood, and compare this score between our model and various alternatives (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Alternative models which are missing any of the four components perform substantially worse (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, red points), while alternative models adding various additional components do not perform substantially better (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, green points). Among these additional components we tested were several model-free reinforcement learning strategies, which have been reported to contribute to behavior on similar multi-step tasks <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/SgyBce/MKfd+SwFG+koca+5GJQ+zErE+ChKh">Daw et al., 2011;</ext-link> <xref ref-type="bibr" rid="bib26">Hasz and Redish, 2018</xref>; <xref ref-type="bibr" rid="bib15">Dezfouli and Balleine, 2019</xref>; <xref ref-type="bibr" rid="bib24">Groman et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Akam et al., 2020</xref>; <xref ref-type="bibr" rid="bib50">Miranda et al., 2020</xref>. That adding them to our model does not improve quality of fit suggests that model-free reinforcement learning does not contribute meaningfully to rat behavior in our task. This is important for our analysis because these strategies involve value representations of their own – that they are not part of our model means that model-based value representations are the only representations of expected reward that are present.</p><p>Our second way of validating the cognitive model makes use of a trial-history regression analysis (<xref ref-type="bibr" rid="bib37">Lau and Glimcher, 2005</xref>), which provides a theory-neutral way of characterizing the patterns present in a behavioral dataset. This analysis fits separate weights for each of the four possible outcome types (common-rewarded, common-omission, uncommon-rewarded, uncommon-omission). These weights will be positive if the rat tends to repeat choices that lead to that outcome, and negative if the rat tends to switch away from such choices. Behavioral datasets produced by different strategies will have different patterns of weights (<xref ref-type="bibr" rid="bib46">Miller et al., 2016</xref>). For example, a model-based planning strategy will show more positive weights for common-reward than for uncommon-reward (since a rewarding outcome port visited after a common transition is likely to be reached again by repeating the choice, while one reached after an uncommon transition is more likely to be reached by switching to the other choice port instead), as well as more negative weights for common-omission than for uncommon-omission (since a unrewarding outcome port visited after a common transition is likely to be avoided by switching the choice, while one reached after an uncommon transition is more likely to be avoided by repeating the choice). As in our previous study, rats trained for the present study universally show this qualitative pattern, and the quantitative patterns present in their weights are well-matched by fits of the cognitive model (example rat: <xref ref-type="fig" rid="fig2">Figure 2d</xref>; all rats: <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). To summarize the pattern and compare it to others in behavior, we define a ‘planning index’ as the linear combination of weights consistent with the planning strategy, as well as a ‘stay’ index, a ‘common-stay/uncommon-switch’ index, and a ‘win-stay/lose-switch’ index quantifying other patterns (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). All rats showed large values of the planning index and the stay index, and much smaller values of the common-stay/uncommon-switch and win-stay/lose-switch indices (<xref ref-type="fig" rid="fig2">Figure 2f</xref>).</p><p>This cognitive model allows us to probe the role of the OFC in two key ways. First, the model can be run using choices and rewards actually experienced by the rat to provide trial-by-trial timecourses for the expected values of the choice ports (<italic>Q</italic>) and outcome ports (<italic>V</italic>). We will use these timecourses (<xref ref-type="bibr" rid="bib12">Daw, 2011</xref>) as estimates of the value placed by the rat on the various ports, and look for correlates of them in the activity of OFC neurons. Second, the model can be altered selectively and run to generate synthetic behavioral datasets, providing predictions about the specific behavioral effects of particular cognitive impairments. We will selectively disrupt choice port value information (which directly drives choice) or outcome port value information (which participates in learning) within the model on a random subset of trials. We will compare these synthetic datasets to real behavioral datasets from rats in which we silence neural activity in the OFC.</p></sec><sec id="s2-2"><title>Neural activity in orbitofrontal cortex codes expected value of outcomes</title><p>We implanted a recording array into the left OFC (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) of each of six rats, and performed electrophysiological recordings during 51 behavioral sessions. This dataset yielded 477 activity clusters, including both single-unit and multi-unit recordings. We characterized whether the activity of each unit encoded three different types of value information, which we estimated trial-by-trial using fits of the cognitive model (<xref ref-type="bibr" rid="bib12">Daw, 2011</xref>). The first type was the difference in expected value between the left and the right choice ports, which we term ‘choice value difference’. Representations of this kind would be consistent with a role in driving choice. The second was the expected value of the choice port that the rat actually selected on that trial, termed ‘chosen value’. This signal has also been proposed to play a role in the choice process (<xref ref-type="bibr" rid="bib69">Rustichini and Padoa-Schioppa, 2015</xref>). The last was the expected value of the outcome port visited by the rat on that trial, which we term ‘outcome value’. Representations of outcome value are predictions about immediate reward probability, and are consistent with a role in learning. We sought to determine whether our neural recording data contained correlates of choice-related value signals, outcome-related value signals, or both.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>OFC units encode multiple correlated variables.</title><p>(<bold>a</bold>) Example unit whose firing rate differs both with the rat’s choice and with the difference in value between the two possible choices. An analysis using coefficient of partial determination (CPD) reveals choice coding, but no coding of expected value. (<bold>b</bold>) Example unit whose firing rate differs both with the rat’s choice and with the expected value of the choice port visited on that trial. CPD analysis reveals coding of both of these variables, though with different timecourses. (<bold>c</bold>) Example unit whose firing rate differs both with reward received and with the expected value of the outcome port visited. CPD analysis reveals coding of both of these variables, with different timecourses. (<bold>d</bold>) Approximate location of recording electrodes targeting OFC, which in rats is represented by regions LO and AIv (<xref ref-type="bibr" rid="bib60">Paxinos and Watson, 2006</xref>; <xref ref-type="bibr" rid="bib61">Price, 2007</xref>; <xref ref-type="bibr" rid="bib76">Stalnaker et al., 2015</xref>), estimated using histology images (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Histological verification of implant locations in OFC.</title><p>Brightfield image of a coronal section from a rat implanted with an electrode array targeting OFC. The locations of the electrode tips are visible, as is damage done when the array was removed post-mortem. Red box indicates the estimated location of electrode tips.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Correlations among predictors in the model used for analysis of electrophysiology data.</title><p>Correlations among regressors motivate the use of a coefficient of partial determination analysis to quantify the unique contribution of each predictor to explaining neural activity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig3-figsupp2-v2.tif"/></fig></fig-group><p>Determining whether a unit encodes these quantities requires separating their influence from that of other variables with which they may be correlated. For example, if the firing rate of a unit differs between left-choice and right-choice trials, it will also differ between trials in which the left port had a higher value and those in which the right port had a higher value (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). This happens because the rat is more likely to select the choice port that is currently higher-valued. Similarly, if the firing rate of a unit differs between rewarded and unrewarded trials, it will also differ between trials with a high and a low outcome port value (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). To quantify coding strength in light of these and other correlations (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), we used a multiple regression approach. Specifically, we fit a separate regression model to predict the spike counts of each unit in each of several 200ms time bins taken relative to the port entry events, using as regressors the potentially learning-related events from one trial (choice port, outcome port, reward, and their interactions, as well as outcome value) as well as the potentially choosing-related variables from the next trial (choice port, choice value difference, and chosen value). We then computed for each regressor the coefficient of partial determination <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/SgyBce/LN6N+dKA3/?prefix=CPD%2C%20also%20known%20as%20">(CPD, also known as ‘</ext-link><ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/SgyBce/LN6N+dKA3/?prefix=CPD%2C%20also%20known%20as%20">partial r-squared’</ext-link>; <xref ref-type="bibr" rid="bib7">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Kennerley et al., 2011</xref>), which quantifies the percentage of remaining variance explained by that regressor, once the influence of all other regressors has been accounted for (<xref ref-type="fig" rid="fig3">Figure 3abc</xref>, bottom row).</p><p>Coefficients of partial determination can be computed for a particular fit (one unit in one time bin), or for a collection of fits (aggregating variance over units, bins, or both). First, we considered coding in individual clusters (single- or multi-unit), aggregating data from time bins within a one-second window around the time of each nose port entry. We assess significance by constructing for each unit a set of null datasets by circularly permuting the trial labels, and comparing the CPDs in the true dataset to those in the null datasets. Using this method, we found that a large fraction of units significantly modulated their firing rate according to outcome-value, with the largest fraction doing so at outcome port entry (170/477, 36%; permutation test at p&lt;0.01). In contrast, a relatively small fraction of units modulated their firing rate according to choice-value-difference (largest at outcome port entry: 34/477, 7%), or to chosen-value (largest at choice port entry: 61/477, 13%). Furthermore, the magnitude of CPD was larger for outcome-value than for the other value regressors. Considering the port entry event with the strongest coding for each regressor, the mean cluster had CPD for outcome-value 3.5 x larger than for choice-value-difference (p=10<sup>–24</sup>, sign rank test; median unit 1.6 x larger; <xref ref-type="fig" rid="fig4">Figure 4a</xref>, note logarithmic axes), and 3.7 x larger than for chosen-value (p=10<sup>–14</sup>; median unit 1.7 x; <xref ref-type="fig" rid="fig4">Figure 4a</xref>). Considering each 200ms time bin separately, we find the fraction of units encoding outcome value rises shortly before step two initiation and peaks sharply around the time of outcome port entry, the fraction coding choice value difference peaks at outcome port entry as well, while the fraction coding chosen value peaks at the time of choice port entry (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Next, we considered coding at the population level, computing CPD over all clusters for each time bin and subtracting the average CPD from the null datasets (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). We found robust population coding of outcome-value, beginning at the time of entry into the bottom-center port, and peaking shortly after entry into the outcome port at 0.82%. In contrast, population coding of choice-value-difference and chosen-value was low in all time bins, reaching a maximum of only 0.29%. Population coding in the OFC was also present for other regressors in our model, especially for reward, choice, and outcome port (<xref ref-type="fig" rid="fig4">Figure 4c</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Coding of expected value of outcomes outweighs coding of expected value of choices.</title><p>(<bold>a</bold>) Left: Scatterplot showing CPD for each unit (n=477) for the outcome-value regressor against CPD for the choice-value-difference regressor, both computed in a one-second window centered on entry into the outcome port. Right: Scatterplot showing CPD for outcome-value, computed at outcome port entry, against CPD for the chosen-value regressor, computed at choice port entry. (<bold>b</bold>). Timecourse of population CPD for the three expected value regressors. We have subtracted from each CPD the mean CPD found in permuted datasets. (<bold>c</bold>) Timecourse of population CPD for the remaining regressors in the model, which reflect observable variables and interactions between them.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Fraction of units significantly encoding each regressor in each time bin.</title><p>Units were deemed significant if they earned a coefficient of partial determination larger than that of 99% of permuted datasets for that regressor in that time bin. Gray shading indicates a threshold for population-level significance (17/477 units; equivalent to p=2 × 10<sup>–6</sup> by a Bernoulli test uncorrected; p&lt;0.01 after Bonferroni correction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Coefficients of partial determination for value regressors, separately for single-unit and multi-unit clusters.</title><p>Above. Right: Scatterplot showing CPD for each single unit (n=251) for the outcome-value regressor against CPD for the choice-value-difference regressor, both computed in a one-second window centered on entry into the outcome port. Right: Scatterplot showing CPD for outcome-value, computed at outcome port entry, against CPD for the chosen-value regressor, computed at choice port entry. below. As above but for multi-unit clusters (n=226). In all panels, CPD for outcome value is greater than for choice-related value information (all p&lt;10<sup>–7</sup>, signrank test. See also <xref ref-type="fig" rid="fig4">Figure 4a</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Timecourse of population CPD for the regressors in our model, considering only single-unit clusters (above) or considering only multi-unit clusters (below).</title><p>See also <xref ref-type="fig" rid="fig2">Figure 2a and c</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Analysis considering each rat individually.</title><p>Above: Population CPD computed separately for units from each rat in the dataset. Below: Number of single-unit and number of multi-unit clusters recorded in each rat.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp4-v2.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Fraction of significant units, considering each rat individually.</title><p>Units were deemed significant if they earned a coefficient of partial determination larger than that of 99% of permuted datasets for that regressor in that time bin. Gray shading indicates a threshold for rat-level significance (p=2 × 10<sup>–6</sup> by a Bernoulli test uncorrected; p&lt;0.01 after Bonferroni correction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp5-v2.tif"/></fig><fig id="fig4s6" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 6.</label><caption><title>Analysis removing the outcome-by-reward interaction regressor.</title><p>The coefficient of partial determination earned by each regressor is sensitive to our choice of which other regressors to include in the model. The strength of this sensitivity depends on the correlation between the regressors (adding an orthogonal regressor will not affect CPD; adding a perfectly correlated regressor will reduce CPD to zero). The ‘choice value difference’ regressor in our model is highly correlated with one of our other regressors ‘outcome port by reward interaction’. This is because, in our model, outcome port and reward information are used to update outcome port values, which in turn update choice port values: a reward at the left outcome port or an omission at the right outcome port will increase the relative value of one choice port; a reward at the right outcome port or an omission at the left outcome port will increase the relative value of the other. This raises the possibility that our finding that the choice value difference regressor earns a relatively small CPD is an artifact of this correlation. To check this, we re-ran our regression analysis without the ‘outcome port by reward interaction’ regressor. Plots of individual clusters (<bold>a</bold>) and of the population timecourse (<bold>b</bold>) show that choice value difference still earns ambler CPDs than outcome value.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp6-v2.tif"/></fig><fig id="fig4s7" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 7.</label><caption><title>Analysis using alternative choice-related value regressors.</title><p>To compute choice-related value regressors, we used the expected values of the model-based planning component of our cognitive model (<italic>Q</italic>, see <xref ref-type="fig" rid="fig2">Figure 2a</xref>). However, the decision variable used by our model is not this expected value in isolation, but instead a weighted sum including expected value as well as variables related to perseveration and novelty preference (<italic>P</italic> and <italic>N</italic>, <xref ref-type="fig" rid="fig2">Figure 2a</xref>). Here, we compute analogs of choice value difference and of chosen value in terms of this decision variable instead of in terms of Q.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp7-v2.tif"/></fig><fig id="fig4s8" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 8.</label><caption><title>Correlates of reward prediction error.</title><p>It has been suggested that OFC causes value-guided learning in other brain regions via signaling of reward prediction errors (RPE; <xref ref-type="bibr" rid="bib5">Banerjee et al., 2020</xref>). RPE is defined as the difference between the reward that was actually experienced and the reward that was expected. <bold>Above</bold>: We tested whether our OFC units tended to carry such a signal by examining the fit weights from our regression model in the time bins immediately following reward, following the method of <xref ref-type="bibr" rid="bib78">Sul et al., 2010</xref>. A unit signaling RPE in our task is expected to have equal and opposite regression weights for reward and for outcome port expected value. Considering all units, we find that there is a weak but significant tendency for these weights to have opposite sign in two of the time bins (400ms: 257/477, 54%, p=0.04; 600ms: 262/477, 55%, p=0.01). Considering only units which correlate significantly with both reward and expected value, there was a similar trend (400ms: 37/62, 60%, p=0.05; 600ms: 39/66, 59%, p=0.05). These results are consistent with the idea that OFC contains units which correlate with RPE, with the caveats that it also contains a substantial number of units which correlate with its opposite (reward expected <italic>plus</italic> reward received), as well as units which correlate with expected reward without correlating with reward itself at all. <bold>Below</bold>: Example units illustrating these three patterns.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig4-figsupp8-v2.tif"/></fig></fig-group><p>Similar results were found when considering single- and multi-unit clusters separately (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>), as well as when separating the units recorded from individual rats (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>, <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). These results were robust to replacing the choice-related value regressors with analogs that consider the full decision variable, including contributions from perseveration and novelty preference as well as expected value (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6</xref>). They were also robust to removing a regressor (interaction of reward and outcome port) that was particularly highly correlated with choice value difference (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6</xref>). Across all of these variants a clear pattern was present: neural activity in OFC encodes the expected value of the visited outcome port more strongly than it encodes either type of value information about the choice ports. In computational models (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), this type of value information plays a role in supporting learning, but does not play a direct role in choice. Our neural recording results therefore suggest that the OFC may play a role in supporting learning, but cast doubt on the idea that they play a strong role in driving choice directly.</p></sec><sec id="s2-3"><title>Inactivations of OFC impair update process, not choice process</title><p>To assess the causal role of the OFC’s value signals, we silenced neural activity using the optogenetic construct halorhodopsin (eNpHR3.0; <xref ref-type="fig" rid="fig5">Figure 5b</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) during either the <italic>outcome period</italic> (beginning at entry into the outcome port and lasting until the end of reward consumption), the <italic>choice period</italic> (beginning at the end of reward consumption and lasting until entry into the choice port on the subsequent trial), or <italic>both periods</italic> (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) in an experimental group of nine rats (577 total sessions). Previous work (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>) had shown that whole-session pharmacological silencing of the OFC specifically attenuates the ‘planning index’ (<xref ref-type="fig" rid="fig2">Figure 2e</xref>, see Methods), which quantifies the extent to which the rats’ choices are modulated by past trials’ outcomes in a way consistent with planning. Here, we found that optogenetic inactivation spanning both the outcome and the reward period reproduced this effect, decreasing the planning index on the subsequent trial (p=0.007, t-test with n=9 rats; <xref ref-type="fig" rid="fig5">Figure 5c</xref>). Inactivation during the outcome period alone also decreased the planning index on the subsequent trial (p=0.0007, <xref ref-type="fig" rid="fig5">Figure 5c</xref>), but inactivation during the choice period alone did not (p=0.64). Comparing the strength of the effect across time periods, we found that both reward-period and both-periods inactivation produced effects that were similar to one another (p=0.5), but greater than the effect of choice-period inactivation (p=0.007, p=0.02). We repeated the experiment in a control group of four rats (109 total sessions) that were implanted with optical fibers but did not express halorhodopsin. This sham inactivation produced no significant effect on the planning index for any time period (all p&gt;0.15, t-test with n=4 rats; <xref ref-type="fig" rid="fig5">Figure 5c</xref>, grey diamonds), and experimental and control rats differed in the effects of outcome-period and both-period inactivation (p=0.02, p=0.02, two-sample t-tests). As in our previous study, inactivation of OFC did not significantly affect other regression-based behavioral indices (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Together, these results indicate that silencing the OFC at the time of the outcome (and therefore the time of peak outcome-value coding, <xref ref-type="fig" rid="fig4">Figure 4b</xref>) is sufficient to disrupt planning behavior.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Inactivation of OFC attenuates influence of outcome values.</title><p>(<bold>a</bold>) Three time periods of inactivation. Outcome-period inactivation began when the rat entered the outcome port, and continued until the rat exited the port, or for a minimum of two seconds. Choice-period inactivation began after this outcome period, and continued until the rat entered the choice port on the next trial, or for a maximum of 15 s. Both-period inactivation encompassed both of these periods. (<bold>b</bold>) Target location for optical fiber implants. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for estimated actual locations in individual rats. Coronal section modified from <xref ref-type="bibr" rid="bib60">Paxinos and Watson, 2006</xref>. (<bold>c</bold>) Effects of inactivation on the planning index on the subsequent trial for experimental rats (n=9, colored triangles) and sham-inactivation rats (n=4, gray diamonds). Bars indicate standard errors across rats. (<bold>d</bold>) Simplified schematic of the representations and computations that take place in our software agent between the delivery of the outcome on one trial and the choice on the next. Compare to <xref ref-type="fig" rid="fig2">Figure 2a</xref>. (<bold>e</bold>) Analysis of synthetic datasets created by disrupting different representations within the software agent on a subset of trials. Each panel shows the contribution to the planning index of trial outcomes at different lags on choices, both on control trials (black) and on trials following disruption of a representation (red). Bars indicate standard error across simulated rats (see Methods) (<bold>f</bold>) Same analysis as in c, applied to data from optogenetic inactivation of the OFC during the outcome period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Locations of optogenetics Implants.</title><p>Example brightfield and YFP images of a coronal section taken from a rat implanted with optical fibers and infected with AAV-halorhodopsin. The location of the optical fiber on the right is visible as a scar (optical fiber on the left is visible in a different coronal section in this rat). The area of expression is visible in the YFP channel. Below: Estimates of the locations of all recording electrodes and fiber tips, obtained by comparing histology images to the reference atlas (<xref ref-type="bibr" rid="bib60">Paxinos and Watson, 2006</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Periods of optogenetic inhibition.</title><p>The ‘outcome period’ on each trial was defined as beginning when the rat entered the outcome port, and ending either when the rat had left the outcome port for a continuous period of one second or more or after 2.5 s (unrewarded trials) or 5 s (rewarded trials). The ‘choice period’ was defined as beginning at the end of the outcome period, and ending when the rat entered the choice port on the subsequent trial. ‘Both periods’ inactivation began at entry into the outcome port and ended at entry into the choice port on the subsequent trial. For both ‘choice period’ and ‘both periods’ inactivation, an upper limit of 15 seconds was imposed. Trials on which inactivation ended due to this limit were not analyzed. <bold>Above</bold>: Histograms of the total duration of inactivation in each condition. <bold>Below</bold>: Relationship between the timebins used in electrophysiology analysis and the optogenetic inactivation periods. Line plot shows the population CPD for each of the three value-related regressors from the electrophysiology experiment (identical to <xref ref-type="fig" rid="fig4">Figure 4b</xref>). Red and green stripes show the probability of each timebin being in each of the optogenetic inactivation periods, computed using the optogenetics dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Effects of optogenetics on all behavioral indices.</title><p><bold>Above</bold>: Regression-based behavioral indices computed considering only trials that were not preceded by inactivation, shown separately for optogenetics rats (triangles) and sham optogenetics rats (diamonds). Behavior was similar in the two groups of rats. <bold>Below</bold>: Difference in behavioral indices between trials that were preceded by inactivation in each time period and those that were not preceded by inactivation. Only for the planning index were there significant differences between optogenetics and sham optogenetics rats (p-values shown from two-sample rank sum test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Average fit weights of the trial-history regression model to behavioral data from the optogenetics experiment.</title><p>Weights were estimated separately for trials preceded by (<bold>a</bold>) no inhibition, (<bold>b</bold>) inhibition during the reward period, (<bold>c</bold>) inhibition during the choice period, or (<bold>d</bold>) inhibition during both periods. Weights for individual rats are shown in <xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig5-figsupp4-v2.tif"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 5.</label><caption><title>Fit weights of the trial history regression model for individual rats in the optogenetics experiment.</title><p>Weights were estimated separately for trials preceded by no inhibition, inhibition during the reward period, inhibition during the choice period, or inhibition during both periods.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64575-fig5-figsupp5-v2.tif"/></fig></fig-group><p>To help understand which aspect of the behavior was affected by silencing the OFC, we used our cognitive model (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) to perform three different types of synthetic inactivation experiments. Each of these produced a distinct pattern of behavior, most clearly visible when we separately computed the contribution to the planning index of the past three trials’ outcomes (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). To simulate an effect of inactivation on the choice process, we decreased the value of the planning agent’s weighting parameter (β<italic><sub>q</sub></italic>; <xref ref-type="fig" rid="fig2">Figure 2a</xref>, ‘choose a choice port’) on synthetic inactivation trials, effectively reducing the influence of choice port value representations on choice. This resulted in choices that were more noisy in general, affecting the influence of all past outcomes (<xref ref-type="fig" rid="fig5">Figure 5e</xref>, left). To simulate effects of inactivation on the learning process, we reparameterized the agent’s learning equation (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, ‘update outcome port values’) to facilitate trial-by-trial alterations of learning parameters:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>α<sub>value</sub></italic> and <italic>α<sub>reward</sub></italic> are now separate learning rate parameters, and E[<italic>V</italic>] represents the expected reward of a random-choice policy. We made a corresponding change to the symmetric update for the outcome port that was not visited (see Methods). To simulate an effect of inactivation on the role of reward in learning, we decreased <italic>α<sub>reward</sub></italic> on synthetic inactivation trials. This resulted in a change specifically to the influence of the outcome during which we performed the synthetic inactivation on future choice (<xref ref-type="fig" rid="fig5">Figure 5e</xref>, middle). To simulate an effect of inactivation on the role of value in learning, we decreased <italic>α<sub>value</sub></italic> on synthetic inactivation trials. This resulted in a change to the influence of outcomes from <italic>previous</italic> trials (<xref ref-type="fig" rid="fig5">Figure 5e</xref> right) but not of the trial during which synthetic inactivation was actually performed. This is because value acts as a summarized memory of previous trials’ outcomes, and attenuating it affects the influence of all of these.</p><p>We performed the same analysis on the inactivation data from our rats (<xref ref-type="fig" rid="fig5">Figure 5f</xref>), and found that silencing OFC during the outcome period on a particular trial did not affect the influence of that trial’s outcome on the upcoming choice (p=0.2, paired t-test with n=9 rats), but that it did affect the influence of the previous two trials’ outcomes (p=0.004, p=0.02). This pattern was consistent with the synthetic dataset in which outcome value representations had been attenuated, but not the other synthetic datasets (compare <xref ref-type="fig" rid="fig5">Figure 5f</xref> to <xref ref-type="fig" rid="fig5">Figure 5e</xref>). We conclude that silencing the OFC in our task predominantly impairs the use of outcome port value information, needed for updating value expectations, but has little or no effect on the use of choice port value information, needed for driving choice.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In formal models of value-based cognition, value representations can play two very different computational roles. First, they can drive choosing, as expected values of different available options are compared to one another, and the best is selected. Secondly, they can drive learning, as the expected value of an outcome is compared to the reward actually received, and future expectations are updated. Value representations in the OFC have been reported in many tasks and species, but it is still unclear whether they drive one process, the other process, or both. The rat two-step task gave us the opportunity to separate the two roles, both in terms of coding in neural activity and in terms of the behavioral impact of silencing that activity. Using this approach, we find weak (though significant) representation of values associated with the available choices (‘choice values’), little or no effect of silencing OFC at the putative time of choice, and effects of silencing inconsistent with impairing choice values in a computational model. Instead, we find strong representation of values associated with immediately impending reward outcomes (‘outcome values’), a strong behavioral effect of silencing OFC at the time of those representations, and effects of silencing that are consistent with specifically impairing the use of outcome values for learning. This pattern of results suggests a much stronger role for OFC in learning than in choosing. More specifically, the results suggest that value information in the rodent OFC may not drive choice directly, but instead provide information about expected outcomes to a separate learning process. In the computational model most consistent with our data, OFC inactivation does not prevent learning, but instead impairs one of the critical inputs to the learning process, namely expected outcome value. Our results therefore are consistent with the idea that learning does not take place in the OFC itself, but in some other structure to which OFC supplies this key input.</p><p>These results clarify the role of a neural signal that has been reported over a wide variety of tasks and species (<xref ref-type="bibr" rid="bib55">O’Doherty, 2007</xref>; <xref ref-type="bibr" rid="bib84">Wallis, 2012</xref>; <xref ref-type="bibr" rid="bib67">Rudebeck and Murray, 2014</xref>; <xref ref-type="bibr" rid="bib76">Stalnaker et al., 2015</xref>). In decision tasks, several distinct types of value representation have been reported, but the most commonly observed is ‘chosen value’ (<xref ref-type="bibr" rid="bib82">Wallis and Miller, 2003</xref>; <xref ref-type="bibr" rid="bib78">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib30">Kennerley et al., 2011</xref>): the expected reward associated with the option that the subject chose. The role of this signal is not clear, it has been interpreted both as representing a post-decision signal that provides input to a learning process (<xref ref-type="bibr" rid="bib73">Schoenbaum et al., 2011</xref>), but also as an internal component of the choice process itself (<xref ref-type="bibr" rid="bib69">Rustichini and Padoa-Schioppa, 2015</xref>). These roles are difficult to fully disentangle using classic tasks, in which both learning and choosing operate over the same set of items. Rat behavior in the two-step task separates these possibilities. If ‘chosen value’ relates to the choice process, we would expect to see representations of the expected value of the port that was chosen (not learned about), while if it relates to the learning process, we would expect to see expected value of the port that was learned about (but not chosen). Our data predominantly provide evidence for the latter alternative in rats.</p><p>Our results build on prior work attempting to separate learning and choosing in trial-by-trial tasks in primates. One set of studies used a probabilistic reward-learning task (the ‘three-armed bandit’ task), along with detailed trial-by-trial analysis to quantify deficits in ‘credit assignment’, attributable to an impaired learning process, as well as deficits in decision-making (<xref ref-type="bibr" rid="bib85">Walton et al., 2010</xref>). Studies using this tool reported that both macaques and humans with lesions to the OFC showed impairments in credit assignment, while those with lesions to a neighboring region, ventromedial prefrontal cortex (vmPFC), showed impairments in decision-making (<xref ref-type="bibr" rid="bib53">Noonan et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Noonan et al., 2017</xref>). A methodological limitation of these studies is that they employed nonselective lesions, which damage both local neurons (from which electrophysiological signals are recorded) as well as fiber tracts connecting other brain regions (from which they typically are not). A subsequent study <xref ref-type="bibr" rid="bib68">Rudebeck et al., 2017</xref> used excitotoxic lesions, which do not damage passing fibers, and found that lesions spanning both OFC and vmPFC affected neither credit assignment nor decision making on this task. Instead, the study found that lesions of another neighboring region, ventrolateral prefrontal cortex (vlPFC), caused impairments in credit assignment (<xref ref-type="bibr" rid="bib68">Rudebeck et al., 2017</xref>). Together, these primate results suggest that neural activity in OFC itself is not necessary either for normal choosing or for normal learning on the three-armed bandit task, but that vlPFC may play a specific role in supporting learning (<xref ref-type="bibr" rid="bib52">Murray and Rudebeck, 2018</xref>). Mapping these findings to the rodent brain is not entirely straightforward. While the distinction in rodents between OFC and vmPFC is relatively clear, the distinction between OFC and vlPFC is much less so (<xref ref-type="bibr" rid="bib61">Price, 2007</xref>). Anatomically, many of the criteria that motivate the proposed homology between rodent OFC (typically taken to mean areas LO and AIv) and primate OFC (typically taken to mean areas 11 l and 13) may apply to parts of primate vlPFC (especially area 12) as well (<xref ref-type="bibr" rid="bib61">Price, 2007</xref>; <xref ref-type="bibr" rid="bib76">Stalnaker et al., 2015</xref>). Like in OFC, neurons in vlPFC have been shown to represent expected outcomes (<xref ref-type="bibr" rid="bib31">Kobayashi et al., 2010</xref>; <xref ref-type="bibr" rid="bib63">Rich and Wallis, 2014</xref>), though these signals have not been characterized as intensively as those in primate OFC. A possibility is that primate OFC and vlPFC are specialized for functions that in rodents are both performed by the OFC.</p><p>Our results also build on those of previous studies that attempt to separate learning and choosing by separating these functions in time, typically by tens of minutes or longer. Here, however, we investigated a very different timescale in that learning and choosing were interleaved, both occurring on each trial of the two-step task. Our inactivation results are consistent with previous studies suggesting a role for the OFC in learning (<xref ref-type="bibr" rid="bib79">Takahashi et al., 2009</xref>; <xref ref-type="bibr" rid="bib28">Jones et al., 2012</xref>), but in apparent tension with others finding that inactivation impairs performance on a post-learning assay (<xref ref-type="bibr" rid="bib28">Jones et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Gremel and Costa, 2013</xref>; <xref ref-type="bibr" rid="bib51">Murray et al., 2015</xref>). One possibility is that the OFC plays different roles for different timescales, or for these different types of behavior. Another possibility is that the OFC plays a common role across timescales and tasks, supporting a process that updates choice mechanisms – in long-timescale tasks this process may take place during either the learning or the probe session, while in our task it necessarily takes place between the outcome of one trial and the choice on the next.</p><p>Our results are consistent with the view that the OFC carries expectancy signals (<xref ref-type="bibr" rid="bib71">Schoenbaum and Roesch, 2005</xref>; <xref ref-type="bibr" rid="bib67">Rudebeck and Murray, 2014</xref>) indicating which outcomes are expected to follow from the current state. This view is distinct from the ‘chosen value’ view described earlier in that it claims that these signals indicate the particular identities of the expected outcomes (e.g. different types of foods) rather than the abstract ‘reward’ or ‘common currency’ value found in reinforcement learning or neuroeconomic theories. Our experiment does not speak to the difference between these views, because the only reward available in our task is a water droplet of a fixed size. Our ‘outcome value’ correlates might therefore reflect expectations of this reward (the water droplet) in particular, and play a role in updates based on this expectation. In formal models, this might be described as updating estimates of a state transition probability (e.g. from an ‘at the outcome port’ state to a ‘receive water droplet’ state). Alternatively, ‘outcome value’ might abstract over many different possible rewarding outcomes. In formal models, this might be described as updating estimates of the reward function (e.g. in an ‘at the outcome port’ state).</p><p>Our results are also in tension with those of some studies using economic choice tasks, in which subjects make decisions between pairs of well-learned stimuli leading to different quantities of differently flavored rewards. A pair of studies, one in mice (<xref ref-type="bibr" rid="bib35">Kuwabara et al., 2020</xref>) and one in primates (<xref ref-type="bibr" rid="bib4">Ballesta et al., 2020</xref>), uses methods that disrupt OFC neural activity on a particular subset of trials, and reports that subjects’ decisions are disrupted specifically on those trials. Recording studies in these tasks, both in primates and in mice (<xref ref-type="bibr" rid="bib56">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib35">Kuwabara et al., 2020</xref>), have reported neural correlates of the values of the individual options that are available. This ‘offer value’ correlate is distinct from the ‘chosen value’ correlate discussed earlier, and is suitable for a role in driving decisions directly. These results have led to the view that OFC value representations, at least in these tasks and species, play a key role in decision-making (<xref ref-type="bibr" rid="bib69">Rustichini and Padoa-Schioppa, 2015</xref>; <xref ref-type="bibr" rid="bib59">Padoa-Schioppa and Conen, 2017</xref>). Our results indicate that in the rat two-step task, OFC does not play this role, since choice-related value correlates are minimal, and inactivation effects are inconsistent with a direct role in choice. One possible resolution to this tension is a difference between tasks. It has been proposed that OFC plays a role specifically in choices between outcomes that offer different ‘goods’ (<xref ref-type="bibr" rid="bib57">Padoa-Schioppa, 2011</xref>; e.g. different flavors <xref ref-type="bibr" rid="bib35">Kuwabara et al., 2020</xref>). In primate tasks involving decisions between outcomes that differ in other properties – such as magnitude, probability, or delay – OFC value representations are typically more consistent with chosen-value than with offer-value coding (<xref ref-type="bibr" rid="bib82">Wallis and Miller, 2003</xref>; <xref ref-type="bibr" rid="bib65">Roesch and Olson, 2004</xref>; <xref ref-type="bibr" rid="bib64">Rich and Wallis, 2016</xref>). Casting doubt on this goods-based interpretation are a set of recent results from a flavor-based task in rats (<xref ref-type="bibr" rid="bib17">Gardner et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Gardner et al., 2020</xref>) showing that silencing the OFC does not affect choice. Another possible resolution is a difference between species. Consistent with either of these possibilities are recent results from a choice task in rats in which options differed in probability and magnitude (but not flavor): rat OFC in this task did not carry correlates of offer value, and silencing impaired trial-history effects rather than the influence of offers on choice (<xref ref-type="bibr" rid="bib10">Constantinople et al., 2019</xref>).</p><p>A final possibility is that OFC contributes to a model-based ‘policy update’ process that combines known information about the structure of the world with new information, in order to modify an animal’s behavioral strategy (<xref ref-type="bibr" rid="bib21">Gershman et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Ludvig et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Mattar and Daw, 2018</xref>). Such a process could happen at different times in different tasks. In our task, policy update might happen between the end of one trial and the beginning of the next, and be interpreted as learning. In economic choice tasks, policy update might happen immediately after the options are presented, but before a choice is made, and be interpreted as related to deliberation. Indeed, experiments analyzing response times suggest that such a deliberative process sometimes occurs during economic choice (<xref ref-type="bibr" rid="bib34">Krajbich et al., 2010</xref>), and may be related to moment-by-moment patterns of activity in the OFC (<xref ref-type="bibr" rid="bib64">Rich and Wallis, 2016</xref>).</p><p>Consistent with this idea are results from a series of recent studies using a flavor-based economic choice task for rats (<xref ref-type="bibr" rid="bib17">Gardner et al., 2017</xref>). If rats are pre-fed rewards of one flavor immediately before a task session begins, they normally alter their choice preferences during that session; silencing OFC activity impairs this update (<xref ref-type="bibr" rid="bib18">Gardner et al., 2019</xref>). If rats are presented with a novel pairing of flavors that have previously been experienced separately, their preferences are initially unstable, but normally converge over the course of a session; silencing OFC activity impairs this update as well (<xref ref-type="bibr" rid="bib19">Gardner et al., 2020</xref>). These results indicate that the OFC activity is crucial specifically at times when a behavioral strategy is being updated. They leave open the question of what computational role this activity plays. They are consistent with several possible roles, including identifying when the strategy should be updated, driving the update of that strategy directly, or driving decisions between recently updated alternatives. Our analysis of the coarse effects of inactivation in different time windows replicates this result. Our analysis of the fine-grained effects of inactivation builds on it, using a cognitive model to jointly interpret neural recordings and trial-by-trial behavioral effects of inactivation. They indicate that the detailed effects of inactivation are attributable in particular to degradation of the outcome value signal (and not, for example, the reward signal, which is stronger and carried by a larger fraction of clusters). This suggests that, at least in our task, the OFC plays a particular computational role in supporting update: it provides outcome value estimates to a learning system, elsewhere in the brain, that updates the behavioral policy.</p><p>Our data are also broadly consistent with the idea that the OFC represents a ‘cognitive map’ of task-relevant state information. A broad version of this account (<xref ref-type="bibr" rid="bib87">Wilson et al., 2014</xref>) was based largely on lesion and inactivation data, and leaves open the question of whether this cognitive mapping information is used to drive choice directly, to guide learning, or for both. A narrower version of this cognitive mapping account was based also on fMRI data in human subjects (<xref ref-type="bibr" rid="bib9">Chan et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Schuck et al., 2016</xref>), and proposes that activity in the OFC specifically represents the subject’s beliefs about the current unobservable state of the environment. In our task, the most relevant states are fully observable (indicated by which noseports are currently lit). However it is possible to view the current reward probability contingencies (right/left outcome ports rewarded at 80%/20% vs. 20%/80%) as an unobservable task-relevant state variable that the animal might make inferences about. In our model, the ‘choice value difference’ variable corresponds very closely to the current belief about this unobservable state. The narrow version of the cognitive mapping account might therefore predict that we would see this variable strongly represented in OFC throughout the entire trial. Our finding that it is represented only transiently and weakly puts our data in apparent tension with the narrower cognitive mapping account. A possible resolution to this tension lies in a difference in brain regions. The fMRI data (<xref ref-type="bibr" rid="bib9">Chan et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Schuck et al., 2016</xref>) find their strongest representations of unobservable state in very medial regions of the orbital surface and in regions of the medial surface, likely belonging to the medial network (areas 10 and 11 m; <xref ref-type="bibr" rid="bib61">Price, 2007</xref>) rather than in the orbital network regions (areas 11 l, 12/47, and 13) which are most plausibly homologous to the regions we investigated here (LO and AIv). A promising direction for future work would be to perform experiments analogous to ours in the regions of the rodent brain that are plausibly homologous to ventral parts of the medial network (VO and MO) as well as elsewhere in PFC.</p><p>In summary, we find that rat behavior on a multi-step decision task dissociates the computational role of expected value information in choosing from its role in learning. We investigate the role of expected value representations in the OFC, and find a pattern of results indicating that these representations do not directly drive choice in this task, but instead support a learning process that updates choice mechanisms elsewhere in the brain. In addition to clarifying the function of the rodent OFC, these results provide insight into the overall architecture of value-based cognition. Specifically, they reveal that it is modular: that value-based learning and value-based choosing depend on separate neural representations. This lends credence to computational models which separate these functions (<xref ref-type="bibr" rid="bib27">Joel et al., 2002</xref>; <xref ref-type="bibr" rid="bib75">Song et al., 2017</xref>). It also motivates a search for the neural modules in the rat which play the remaining computational roles, including the role of directly driving choice.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Subjects</title><p>All subjects were adult male Long-Evans rats (Taconic Biosciences; Hilltop Lab Animals), placed on a restricted water schedule to motivate them to work for water rewards. Rats were housed on a reverse 12 hr light cycle and trained during the dark phase of the cycle. Rats were pair housed during behavioral training and then single housed after being implanted with microwire arrays or optical fiber implants. All experimental procedures were performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health., and were approved by the Princeton University Institutional Animal Care and Use Committee (protocol #1853). No explicit power analysis was used to determine the number of rats; instead we aimed for a sample size consistent with previous studies using similar tools.</p></sec><sec id="s4-2"><title>Two-step behavioral task</title><p>Rats were trained on a two-step behavioral task, following a shaping procedure which has been previously described (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>). Rats performed the task in custom behavioral chambers containing six ‘nose ports’ arranged in two rows of three, each outfitted with a white LED for delivering visual stimuli, as well as an infrared LED and phototransistor for detecting rats’ nose entries into the port. The left and right ports in the bottom row also contained sipper tubes for delivering water rewards. The rat initiated each trial by entering the illuminated top center port, causing the two top side ports (‘choice ports’) to illuminate. The rat then made his choice by entering one of these ports. Immediately upon entry into a choice port, two things happened: the bottom center port light illuminateed, and one of two possible sounds began to play, indicating which of the two bottom side ports (‘outcome ports’) would eventually be illuminated. The rat then entered the bottom center port, which caused the appropriate outcome port to illuminate. Finally, the rat entered the outcome port which illuminated, and received either a water reward or an omission. Once the rat had consumed the reward, a trial-end sound played, and the top center port illuminated again to indicate that the next trial was ready.</p><p>The selection of each choice port led to one of the outcome ports becoming available with 80% probability (common transition), and to the other becoming available with 20% probability (uncommon transition). These probabilities were counterbalanced across rats, but kept fixed for each rat for the entirety of his experience with the task. The probability that entry into each bottom side port would result in reward switched in blocks. In each block one port resulted in reward 80% of the time, and the other port resulted in reward 20% of the time. Block shifts happened unpredictably, with a minimum block length of 10 trials and a 2% probability of block change on each subsequent trial.</p></sec><sec id="s4-3"><title>Analysis of behavioral data: Planning index and model-free index</title><p>We quantify the effect of past trials and their outcomes on future decisions using a logistic regression analysis based on previous trials and their outcomes (<xref ref-type="bibr" rid="bib37">Lau and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>). We define vectors for each of the four possible trial outcomes: common-reward (CR), common-omission (CO), uncommon-reward (UR), and uncommon-omission (UO), each taking on a value of +1 for trials of their type where the rat selected the left choice port, a value of –1 for trials of their type where the rat selected the right choice port, and a value of 0 for trials of other types. We use the following regression model:<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>U</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>U</mml:mi><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where β<italic><sub>cr</sub></italic>, β<italic><sub>co</sub></italic>, β<italic><sub>ur</sub></italic>, and β<italic><sub>uo</sub></italic> are vectors of regression weights which quantify the tendency to repeat on the next trial a choice that was made trials ago and resulted in the outcome of their type, and <italic>T</italic> is a hyperparameter governing the number of past trials used by the model to predict upcoming choice, which was set to 3 for all analyses.</p><p>We expect model-free agents to repeat choices which lead to reward and switch away from those which lead to omissions <ext-link ext-link-type="uri" xlink:href="https://paperpile.com/c/SgyBce/MKfd">Daw et al., 2011</ext-link>, so we define a model-free index for a dataset as the sum of the appropriate weights from a regression model fit to that dataset:<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>F</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We expect that planning agents will show the opposite pattern after uncommon transition trials, since the uncommon transition from one choice is the common transition from the other choice. We define a planning index:<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mi>P</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We also compute the main effect of past choices on future choice<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As well as an index quantifying the tendency to repeat choices the lead to common transitions and to switch away from those that lead to uncommon transitions:<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4"><title>Mixture-of-agents behavior model</title><p>We model behavior and obtain trial-by-trial estimates of value signals using an agent-based computational model similar to one that we have previously shown to provide a good explanation of rat behavior on the two-step task (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>). This model adopts the mixture-of-agents approach, in which each rat’s behavior is described as resulting from the influence of a weighted average of several different ‘agents’ implementing different behavioral strategies to solve the task. On each trial, each agent <italic>A</italic> computes a value, <italic>Q<sub>A</sub>(c),</italic> for each of the two available choices <italic>c</italic>, and the combined model makes a decision according to a weighted average of the various strategies’ values, <italic>Q<sub>total</sub>(c</italic>):<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the β’s are weighting parameters determining the influence of each agent, and (<italic>c</italic>) is the probability that the mixture-of-agents will select choice <italic>c</italic> on that trial. The model which we have previously shown to provide the best explanation of rat’s behavior contains four such agents: model-based temporal difference learning, novelty preference, perseveration, and bias. The model used here is identical to the one in our previous paper (<xref ref-type="bibr" rid="bib47">Miller et al., 2017</xref>), except that the perseverative agent is modified to allow it to consider many past trials rather than only the immediately previous trial (<xref ref-type="bibr" rid="bib48">Miller et al., 2019</xref>).</p><sec id="s4-4-1"><title>Model-based temporal difference learning</title><p>Model-based temporal difference learning is a planning strategy, which maintains separate estimates of the probability with which each action (selecting the left or the right choice port) will lead to each outcome (the left or the right outcome port becoming available), <italic>P(o|a</italic>), as well as the probability, <italic>R(o</italic>), with which each outcome will lead to reward. This strategy assigns values to the actions by combining these probabilities to compute the expected probability with which selection of each action will ultimately lead to reward:<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>At the beginning of each session, the reward estimate <italic>V(o</italic>) is initialized to 0.5 for both outcomes, and the transition estimate <italic>P(o|c</italic>) is set to the true transition function for the rat being modeled (0.8 for common and 0.2 for uncommon transitions). After each trial, the reward estimate for both outcomes is updated according to<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>o<sub>t</sub></italic> is the outcome that was observed on that trial, <italic>r<sub>t</sub></italic> is a binary variable indicating reward delivery, and α is a learning rate parameter constrained to lie between zero and one.</p></sec><sec id="s4-4-2"><title>Novelty preference</title><p>The novelty preference agent follows an ‘uncommon-stay/common switch’ pattern, which tends to repeat choices when they lead to uncommon transitions on the previous trial, and to switch away from them when they lead to common transitions. Note that some rats have positive values of the <italic>β<sub>np</sub></italic> parameter weighting this agent (novelty preferring) while others have negative values (novelty averse; see <xref ref-type="fig" rid="fig1">Figure 1e</xref>):<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="30pt"/><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-3"><title>Perseveration</title><p>Perseveration is a pattern which tends to repeat choices that have been made in the recent past, regardless of whether they led to a common or an uncommon transition, and regardless of whether or not they led to reward.<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-4"><title>Bias</title><p>Bias is a pattern which tends to select the same choice port on every trial. Its value function is therefore static, with the extent and direction of the bias being governed by the magnitude and sign of this strategy’s weighting parameter <italic>β<sub>bias</sub></italic>.<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s4-5"><title>Model fitting</title><p>We implemented the model described above using the probabilistic programming language Stan (<xref ref-type="bibr" rid="bib8">Carpenter, 2016</xref>; <xref ref-type="bibr" rid="bib38">Lau, 2017</xref>), and performed maximum-a-posteriori fits using weakly informative priors on all parameters (<xref ref-type="bibr" rid="bib20">Gelman et al., 2013</xref>) The prior over the weighting parameters <italic>β</italic> was normal with mean 0 and sd 0.5, and the prior over <italic>α</italic> was a beta distribution with <italic>a</italic>=<italic>b</italic> = 3. For ease of comparison, we normalize the weighting parameters <italic>β<sub>plan</sub></italic>, <italic>β<sub>np</sub></italic>, and <italic>β<sub>persev</sub></italic>, dividing each by the standard deviation of its agent’s associated values (<italic>Q<sub>plan</sub></italic>, <italic>Q<sub>np</sub></italic>, and <italic>Q<sub>persev</sub></italic>) taken across trials. Since each weighting parameter affects behavior only by scaling the value output by its agent, this technique brings the weights into a common scale and facilitates interpretation of their relative magnitudes, analogous to the use of standardized coefficients in regression models.</p></sec><sec id="s4-6"><title>Surgery: Microwire array implants</title><p>Six rats were implanted with microwire arrays (Tucker-David Technologies) targeting OFC unilaterally. Arrays contained tungsten microwires 4.5 mm long and 50 μm in diameter, cut at a 60° angle at the tips. Wires were arranged in four rows of eight, with spacing 250 μm within-row and 375 μm between rows, for a total of 32 wires in a 1.125 mm by 1.75 mm rectangle. Target coordinates for the implant with respect to bregma were 3.1–4.2 mm anterior, 2.4–4.2 mm lateral, and 5.2 mm ventral (~4.2 mm ventral to brain surface at the posterior-middle of the array).</p><p>In order to expose enough of the skull for a craniotomy in this location, the jaw muscle was carefully resected from the lateral skull ridge in the area near the target coordinates. Dimpling of the brain surface was minimized following procedures described in more detail elsewhere (<xref ref-type="bibr" rid="bib3">Akrami et al., 2017</xref>). Briefly, a bolus of petroleum jelly (Puralube, Dechra Veterinary Products) was placed in the center of the craniotomy to protect it, while cyanoacrylate glue (Vetbond, 3 M) was used to adhere the pia mater to the skull at the periphery. The petroleum jelly was then removed, and the microwire array inserted slowly into the brain. Rats recovered for a minimum of 1 week, with ad lib access to food and water, before returning to training.</p></sec><sec id="s4-7"><title>Electrophysiological recordings</title><p>Once rats had recovered from surgery, recording sessions were performed in a behavioral chamber outfitted with a 32 channel recording system (Neuralynx). Spiking data was acquired using a bandpass filter between 600 and 6000 Hz and a spike detection threshold of 30 μV. Clusters were manually cut (Spikesort 3D, Neuralynx), and both single- and multi-units were considered. All manually cut units were used for analysis. We observed that a small fraction of trials showed apparent artifacts in which some units appeared to have extremely high firing rates, which we suspect was due to motion of the implant or tether. We therefore excluded units from analysis on trials where the Median Absolute Deviation (<xref ref-type="bibr" rid="bib40">Leys et al., 2013</xref>) of their spike count exceeded a conservative threshold of three.</p></sec><sec id="s4-8"><title>Analysis of electrophysiology data</title><p>To determine the extent to which different variables were encoded in the neural signal, we fit a series of regression models to our spiking data. Models were fit to the spike counts emitted by each unit in 200ms time bins taken relative to the four noseport entry events that made up each trial. There were ten total regressors, defined relative to a pair of adjacent trials. Seven of them were binary (coded as +–1), and related to observable task variables: the choice port selected on the earlier trial(left or right), the outcome port visited (left or right), the reward received (reward or omission), the interaction between choice port and outcome port (common or uncommon transition), the interaction between choice port and reward, the interaction between outcome port and reward, and the choice port selected on the later trial. Three additional regressors were continuous and related to subjective reward expectation: the expected value of the outcome port visited (<italic>V</italic>) for the first trial, the value difference between the choice ports (<italic>Q(left) - Q(right</italic>)), and the value of the choice port selected (<italic>Q(chosen</italic>)) on the subsequent trial. These last three regressors were obtained using the agent-based computational model described above, with parameters fit separately to each rat’s behavioral data. Regressors were z-scored to facilitate comparison of fit regression weights. Models were fit using the Matlab function glmnet <xref ref-type="bibr" rid="bib62">Qian, 2013</xref> using a Poisson noise model. We fit each model both with no regularization and with L1 regularization (<italic>α</italic>=1, λ=0, 10<sup>–10</sup>, 10<sup>–9</sup>, …,10<sup>–1</sup>), using the model with the weakest regularization that still allowed all weights to be identifiable.</p><p>In our task, many of these regressors were correlated with one another (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), so we quantify encoding using the coefficient of partial determination (CPD; also known as partial r-squared) associated with each (<xref ref-type="bibr" rid="bib7">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Kennerley et al., 2011</xref>). This measure quantifies the fraction of variance explained by each regressor, once the variance explained by all other regressors has been taken account of:<disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mrow><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic>u</italic> refers to a particular unit, <italic>t</italic> refers to a particular time bin, and SSE(<italic>X<sub>all</sub></italic>) refers to the sum-squared-error of a regression model considering all eight regressors described above, and SSE(X<sub>-i</sub>) refers to the sum-squared-error of a model considering the seven regressors other than <italic>X<sub>i</sub></italic>. We compute total CPD for each unit by summing the SSE associated with the regression models for that unit for all time bins:<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mrow><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>We report this measure for individual example units showing all time bins (<xref ref-type="fig" rid="fig2">Figure 2</xref>, bottom). We report the CPD for each unit for particular port entry events (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), taking the sum over the five bins making up a 1 s time window centered on a particular port entry event (top neutral center port, choice port, bottom neutral center port, or outcome port). We report the ‘population CPD’ (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="fig" rid="fig3">Figure 3c</xref>) by aggregating over all units for a particular time bin:<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mrow><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:munder><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For each of these measures, we assess significance by comparing the CPD computed on the true dataset to a distribution of CPDs computed on surrogate datasets constructed by circularly permuting the trial labels within each session. We use permuted, rather than shuffled, labels in order to preserve trial-by-trial correlational structure. If the true CPD is larger than most of the CPDs from these surrogate datasets, we can reject the null hypothesis that the CPD is driven by correlational structure alone. We compute a permutation p-value by finding the percentile of the true CPD within the distribution of CPDs in surrogate datasets. In the main-text figure (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="fig" rid="fig3">Figure 3c</xref>) we additionally subtract the mean of the CPDs from the surrogate datasets, in order to give a measure that can be fairly compared to zero.</p></sec><sec id="s4-9"><title>Surgery: Optical fiber implant and virus injection</title><p>Rats were implanted with sharpened fiber optics and received virus injections following procedures similar to those described previously (<xref ref-type="bibr" rid="bib25">Hanks et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Kopec et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Akrami et al., 2017</xref>), and documented in detail on the Brody lab <ext-link ext-link-type="uri" xlink:href="http://brodywiki.princeton.edu/wiki/index.php/Etching_Fiber_Optics">website</ext-link>. A 50/125 μm LC-LC duplex fiber cable (Fiber Cables) was dissected to produce four blunt fiber segments with LC connectors. These segments were then sharpened by immersing them in hydroflouric acid and slowly retracting them using a custom-built motorized jig attached to a micromanipulator (Narashige International) holding the fiber. Each rat was implanted with two sharpened fibers, in order to target OFC bilaterally. Target coordinates with respect to bregma were 3.5 mm anterior, 2.5 mm lateral, 5 mm ventral. Fibers were angled 10 degrees laterally, to make space for female-female LC connectors which were attached to each and included as part of the implant.</p><p>Four rats were implanted with sharpened optical fibers only, but received no injection of virus. These rats served as uninfected controls.</p><p>Nine additional rats received both fiber implants as well as injections of a virus (AAV5-CaMKIIα-eNpHR3.0-eYFP; UNC Vector Core) into the OFC to drive expression of the light-activated inhibitory opsin eNpHR3.0. Virus was loaded into a glass micropipette mounted into a Nanoject III (Drummond Scientific), which was used for injections. Injections involved five tracks arranged in a plus-shape, with spacing 500 μm. The center track was located 3.5 mm anterior and 2.5 mm lateral to bregma, and all tracks extended from 4.3 to 5.7 mm ventral to bregma. In each track, 15 injections of 23 nL were made at 100 μm intervals, pausing for ten seconds between injections, and for 1 min at the bottom of each track. In total 1.7 μl of virus were delivered to each hemisphere over a period of about 20 min.</p><p>Rats recovered for a minimum of one week, with ad lib access to food and water, before returning to training. Rats with virus injections returned to training, but did not begin inactivation experiments until a minimum of 6 weeks had passed, to allow for virus expression.</p></sec><sec id="s4-10"><title>Optogenetic perturbation experiments</title><p>During inactivation experiments, rats performed the task in a behavioral chamber outfitted with a dual fiber optic patch cable connected to a splitter and a single-fiber commutator (Princetel) mounted in the ceiling. This fiber was coupled to a 200 mW 532 nm laser (OEM Laser Systems) under the control of a mechanical shutter (ThorLabs) by way of a fiber port (ThorLabs). The laser power was tuned such that each of the two fibers entering the implant received between 25 and 30 mW of light when the shutter was open.</p><p>Each rat received several sessions in which the shutter remained closed, in order to acclimate to performing the task while tethered. Once the rat showed behavioral performance while tethered that was similar to his performance before the implant surgery, inactivation sessions began. During these sessions, the laser shutter was opened (causing light to flow into the implant, activating the eNpHR3.0 and silence neural activity) on 7% of trials each in one of three time periods. ‘Outcome period’ inactivation began when the rat entered the bottom center port at the end of the trial, and ended either when the rat had left the port and remained out for a minimum of 500ms, or after 2.5 s. ‘Choice period’ inactivation began at the end of the outcome period and lasted until the rat entered the choice port on the following trial. ‘Both period’ inactivation encompassed both the outcome period and the choice period. The total duration of the inactivation therefore depended in part on the movement times of the rat, and was somewhat variable from trials to trial (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). If a scheduled inactivation would last more than 15 s, inactivation was terminated, and that trial was excluded from analysis. Due to constraints of the bControl software, inactivation was only performed on even-numbered trials.</p></sec><sec id="s4-11"><title>Analysis of optogenetic effects on behavior</title><p>We quantify the effects of optogenetic inhibition on behavior by computing separately the planning index for trials following inactivation of each type (outcome period, choice period, both periods) and for control trials. Specifically, we fit the trial history regression model of <xref ref-type="disp-formula" rid="equ2">Equation 1</xref> with a separate set of weights for trials following inactivation of each type:<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>U</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>U</mml:mi><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We used maximum a posteriori fitting in which the priors were Normal(0,1) for weights corresponding to control trials, and Normal(<italic>β<sub>X, cntrl</sub></italic>, 1) for weights corresponding to inactivation trials, where <italic>β<sub>X, cntrl</sub></italic> is the corresponding control trial weight – e.g. the prior for <italic>β<sub>CR, out</sub></italic>(1) is Normal(<italic>β<sub>CR, cntrl</sub></italic>(1), 1). This prior embodies the belief that the effect of inactivation on behavior is likely to be small, and that the direction of any effect is equally likely to be positive or negative. This ensures that our priors cannot induce any spurious differences between control and inactivation conditions into the parameter estimates. We then compute a planning index separately for the weights of each type, modifying <xref ref-type="disp-formula" rid="equ4">Equation 3</xref>:<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mrow><mml:mi>P</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We compute the relative change in planning index for each inactivation condition: (<italic>PlanningIndex<sub>i</sub> - PlanningIndex<sub>cntrl</sub></italic>) / <italic>PlanningIndex<sub>cntrl</sub></italic>, and report three types of significance tests on this quantity. First, we test for each inactivation condition the hypothesis that there was a significant change in the planning index, reporting the results of a one-sample t-test over rats. Next, we test the hypothesis that different inactivation conditions had effects of different sizes on the planning index, reporting a paired t-test over rats. Finally, we test the hypothesis for each condition that inactivation had a different effect than sham inactivation (conducted in rats which had not received virus injections to deliver eNpHR3.0), reporting a two-sample t-test.</p><p>To test the hypothesis that inactivation specifically impairs the effect of distant past outcomes on upcoming choice, we break down the planning index for each condition by the index of the weights the contribute to it:<disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:mrow><mml:mi>P</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We report these trial-lagged planning indices for each inactivation condition, and assess the significance of the difference between inactivation and control conditions at each lag using a paired t-test.</p></sec><sec id="s4-12"><title>Synthetic datasets</title><p>To generate synthetic datasets for comparison to optogenetic inactivation data, we generalized the behavioral model to separate the contributions of representations of expected value and of immediate reward. In particular, we replaced the learning equation within the model-based RL agent (<xref ref-type="disp-formula" rid="equ8">Equation 7</xref>) with the following:<disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>α<sub>value</sub></italic> and <italic>α<sub>reward</sub></italic> are separate learning rate parameters, constrained to be nonnegative and to have a sum no larger than one, and E[<italic>V</italic>] represents the expected reward of a random-choice policy on the task, which in the case of our task is equal to 0.5.</p><p>To generate synthetic datasets in which silencing the OFC impairs choice value representations, outcome value representations, or reward representations, we decrease the parameter <italic>β<sub>plan</sub></italic>, <italic>α<sub>value</sub></italic>, or <italic>α<sub>reward</sub></italic>, respectively. Specifically, we first fit the model to the dataset for each rat in the optogenetics experiment (n=9) as above (i.e. using <xref ref-type="disp-formula" rid="equ8">equation 7</xref> as the learning rule) to obtain maximum a posteriori parameters. We translated these parameters to the optogenetics (<xref ref-type="disp-formula" rid="equ19">equation 18</xref>) version of the model by setting <italic>α<sub>value</sub></italic> equal to the fit parameter <italic>α</italic> and <italic>α<sub>reward</sub></italic> equal to 1 - <italic>α</italic>. We then generated four synthetic datasets for each rat. For the control dataset, the fit parameters were used on trials of all types, regardless of whether inhibition of OFC was scheduled on that trial. For the ‘impaired outcome values’ dataset, <italic>α<sub>value</sub></italic> was decreased specifically for trials with inhibition scheduled during the outcome period or both periods, but not on trials with inhibition during the choice period or on control trials. For the ‘impaired reward processing’ dataset, <italic>α<sub>reward</sub></italic> was decreased on these trials instead. For the ‘impaired decision-making’ dataset, <italic>β<sub>plan</sub></italic> was decreased specifically on trials following inhibition. In all cases, the parameter to be decreased was multiplied by 0.3, and synthetic datasets consisted of 100,000 total trials per rat.</p></sec><sec id="s4-13"><title>Histological verification of targeting</title><p>We verified that surgical implants were successfully placed in the OFC using standard histological techniques. At experimental endpoint, rats with electrode arrays were anesthetized, and microlesions were made at the site of each electrode tip by passing current through the electrodes. Rats were then perfused transcardially with saline followed by formalin. Brains were sliced using a vibratome and imaged using an epifluorescent microscope. Recording sites were identified using these microlesions and the scars created by the electrodes in passing, as well dimples in the surface of the brain. Locations of optical fibers were identified using the scars created by their passage. Location of virus expression was identified by imaging the YFP conjugated to the eNpHR3.0 molecule <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><p>Data collected for the purpose of this paper will be posted on Figshare upon acceptance. Software used to analyze the data will be made available as a Github release. Software used for training rats and design files for constructing behavioral rigs are available on the Brody lab website.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures were performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health, and were approved by the Princeton University Institutional Animal Care and Use Committee (protocol #1853).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-64575-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data collected for the purpose of this paper are available on FigShare at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.20449140">https://doi.org/10.6084/m9.figshare.20449140</ext-link>. Software used to analyze the data is available on Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/kevin-j-miller/MBB2022-orbitofrontal-learning-choosing/">https://github.com/kevin-j-miller/MBB2022-orbitofrontal-learning-choosing/</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c2b9cc519a0287bf60d0284816f0bc2726dd9232;origin=https://github.com/kevin-j-miller/MBB2022-orbitofrontal-learning-choosing;visit=swh:1:snp:1ac43437532adf60eac44af8be5c11abf5ccd257;anchor=swh:1:rev:c65b4ca63e73927acbac07168f220741a02a4301">swh:1:rev:c65b4ca63e73927acbac07168f220741a02a4301</ext-link>. Software used for training rats and design files for constructing behavioral rigs are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kevin-j-miller/MBB2022-orbitofrontal-learning-choosing/tree/main/bControl_protocols">https://github.com/kevin-j-miller/MBB2022-orbitofrontal-learning-choosing/tree/main/bControl_protocols</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data from Value Representations in the Rodent Orbitofrontal Cortex Drive Learning, not Choice</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.20449140</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Code from Value Representations in the Rodent Orbitofrontal Cortex Drive Learning, not Choice</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.6974148</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Athena Akrami for assistance with array implant surgeries; Jovanna Teran, Klaus Osorio, Adrian Sirko, Samantha Stein, and Lillianne Teachen for assistance with animal training; and Peter Bibawi for help with histology. We thank Luca Mazzucato for identifying an error in an early version of our data processing pipeline. We thank Nathaniel Daw, Yael Niv, Geoffrey Schoenbaum, and Thomas Akam for helpful discussions, and Athena Akrami, Christine Constantinople, Nathaniel Daw, Cristina Domnisoru, Jeff Gauthier, Chuck Kopec, Olga Lositsky, Marcelo Mattar, Bas van Opheusden, Angela Radulescu, Ben Scott, Kim Stachenfeld, and Bob Wilson for helpful comments on the manuscript. KJM was supported by training grant NIH T-32 MH065214 to the Princeton Neuroscience Institute, and by a Harold W Dodds fellowship from Princeton University.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaishi</surname><given-names>R</given-names></name><name><surname>Umeda</surname><given-names>K</given-names></name><name><surname>Nagase</surname><given-names>A</given-names></name><name><surname>Sakai</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Autonomous mechanism of internal choice estimate underlies decision inertia</article-title><source>Neuron</source><volume>81</volume><fpage>195</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.018</pub-id><pub-id pub-id-type="pmid">24333055</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Rodrigues-Vaz</surname><given-names>I</given-names></name><name><surname>Marcelo</surname><given-names>I</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Pereira</surname><given-names>M</given-names></name><name><surname>Oliveira</surname><given-names>RF</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Anterior Cingulate Cortex Represents Action-State Predictions and Causally Mediates Model-Based Reinforcement Learning in a Two-Step Decision Task</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/126292</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name><name><surname>Brody</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Posterior Parietal Cortex Represents Sensory History and Mediates Its Effects on Behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/182246</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballesta</surname><given-names>S</given-names></name><name><surname>Shi</surname><given-names>W</given-names></name><name><surname>Conen</surname><given-names>KE</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Values encoded in orbitofrontal cortex are causally related to economic choices</article-title><source>Nature</source><volume>588</volume><fpage>450</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2880-x</pub-id><pub-id pub-id-type="pmid">33139951</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banerjee</surname><given-names>A</given-names></name><name><surname>Parente</surname><given-names>G</given-names></name><name><surname>Teutsch</surname><given-names>J</given-names></name><name><surname>Lewis</surname><given-names>C</given-names></name><name><surname>Voigt</surname><given-names>FF</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Value-guided remapping of sensory cortex by lateral orbitofrontal cortex</article-title><source>Nature</source><volume>585</volume><fpage>245</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2704-z</pub-id><pub-id pub-id-type="pmid">32884146</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>TC</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Bromberg-Martin</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Orbitofrontal cortex uses distinct codes for different choice attributes in decisions motivated by curiosity</article-title><source>Neuron</source><volume>85</volume><fpage>602</fpage><lpage>614</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.050</pub-id><pub-id pub-id-type="pmid">25619657</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Heterogeneous coding of temporally discounted values in the dorsal and ventral striatum during intertemporal choice</article-title><source>Neuron</source><volume>69</volume><fpage>170</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.11.041</pub-id><pub-id pub-id-type="pmid">21220107</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stan: A probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>SCY</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A probability distribution over latent causes, in the orbitofrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>7817</fpage><lpage>7828</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0659-16.2016</pub-id><pub-id pub-id-type="pmid">27466328</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinople</surname><given-names>CM</given-names></name><name><surname>Piet</surname><given-names>AT</given-names></name><name><surname>Bibawi</surname><given-names>P</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>C</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Lateral orbitofrontal cortex promotes trial-by-trial learning of risky, but not spatial, biases</article-title><source>eLife</source><volume>8</volume><elocation-id>e49744</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49744</pub-id><pub-id pub-id-type="pmid">31692447</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Primate orbitofrontal cortex codes information relevant for managing explore-exploit tradeoffs</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>2553</fpage><lpage>2561</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2355-19.2020</pub-id><pub-id pub-id-type="pmid">32060169</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Trial-by-trial data analysis using computational models</article-title><conf-name>Decision Making, Affect, and Learning</conf-name><fpage>3</fpage><lpage>38</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id><pub-id pub-id-type="pmid">21435563</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Chapter 21 - multiple systems for value learning</chapter-title><person-group person-group-type="editor"><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name></person-group><source>Neuroeconomics</source><edition>Second Edition</edition><publisher-loc>San Diego</publisher-loc><publisher-name>Academic Press</publisher-name><fpage>393</fpage><lpage>410</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dezfouli</surname><given-names>A</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning the structure of the world: the adaptive nature of state-space and action representations in multi-stage decision-making</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007334</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007334</pub-id><pub-id pub-id-type="pmid">31490932</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Goals and habits in the brain</article-title><source>Neuron</source><volume>80</volume><fpage>312</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.007</pub-id><pub-id pub-id-type="pmid">24139036</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Conroy</surname><given-names>JS</given-names></name><name><surname>Shaham</surname><given-names>MH</given-names></name><name><surname>Styer</surname><given-names>CV</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Lateral orbitofrontal inactivation dissociates devaluation-sensitive behavior and economic choice</article-title><source>Neuron</source><volume>96</volume><fpage>1192</fpage><lpage>1203</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.10.026</pub-id><pub-id pub-id-type="pmid">29154127</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Conroy</surname><given-names>JC</given-names></name><name><surname>Sanchez</surname><given-names>DC</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Real-time value integration during economic choice is regulated by orbitofrontal cortex</article-title><source>Current Biology</source><volume>29</volume><fpage>4315</fpage><lpage>4322</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.058</pub-id><pub-id pub-id-type="pmid">31813612</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Sanchez</surname><given-names>D</given-names></name><name><surname>Conroy</surname><given-names>JC</given-names></name><name><surname>Wikenheiser</surname><given-names>AM</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Processing in lateral orbitofrontal cortex is required to estimate subjective preference during initial, but not established, economic choice</article-title><source>Neuron</source><volume>108</volume><fpage>526</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.08.010</pub-id><pub-id pub-id-type="pmid">32888408</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Carlin</surname><given-names>JB</given-names></name><name><surname>Stern</surname><given-names>HS</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Bayesian Data Analysis</source><edition>Third Edition</edition><publisher-name>CRC Press</publisher-name><pub-id pub-id-type="doi">10.1201/b16018</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Markman</surname><given-names>AB</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Retrospective revaluation in sequential decision making: a tale of two systems</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>182</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1037/a0030844</pub-id><pub-id pub-id-type="pmid">23230992</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottfried</surname><given-names>JA</given-names></name><name><surname>O’Doherty</surname><given-names>J</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Encoding predictive reward value in human amygdala and orbitofrontal cortex</article-title><source>Science</source><volume>301</volume><fpage>1104</fpage><lpage>1107</lpage><pub-id pub-id-type="doi">10.1126/science.1087919</pub-id><pub-id pub-id-type="pmid">12934011</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gremel</surname><given-names>CM</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Orbitofrontal and striatal circuits dynamically encode the shift between goal-directed and habitual actions</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>2264</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms3264</pub-id><pub-id pub-id-type="pmid">23921250</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groman</surname><given-names>SM</given-names></name><name><surname>Massi</surname><given-names>B</given-names></name><name><surname>Mathias</surname><given-names>SR</given-names></name><name><surname>Curry</surname><given-names>DW</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neurochemical and behavioral dissections of decision-making in a rodent multistage task</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>295</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2219-18.2018</pub-id><pub-id pub-id-type="pmid">30413646</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct relationships of parietal and prefrontal cortices to evidence accumulation</article-title><source>Nature</source><volume>520</volume><fpage>220</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1038/nature14066</pub-id><pub-id pub-id-type="pmid">25600270</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasz</surname><given-names>BM</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deliberation and procedural automation on a two-step task for rats</article-title><source>Frontiers in Integrative Neuroscience</source><volume>12</volume><elocation-id>30</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2018.00030</pub-id><pub-id pub-id-type="pmid">30123115</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joel</surname><given-names>D</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Ruppin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Actor-critic models of the basal ganglia: new anatomical and computational perspectives</article-title><source>Neural Networks</source><volume>15</volume><fpage>535</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(02)00047-3</pub-id><pub-id pub-id-type="pmid">12371510</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JL</given-names></name><name><surname>Esber</surname><given-names>GR</given-names></name><name><surname>McDannald</surname><given-names>MA</given-names></name><name><surname>Gruber</surname><given-names>AJ</given-names></name><name><surname>Hernandez</surname><given-names>A</given-names></name><name><surname>Mirenzi</surname><given-names>A</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Orbitofrontal cortex supports behavior and learning using inferred but not cached values</article-title><source>Science</source><volume>338</volume><fpage>953</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1126/science.1227489</pub-id><pub-id pub-id-type="pmid">23162000</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennerley</surname><given-names>SW</given-names></name><name><surname>Dahmubed</surname><given-names>AF</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neurons in the frontal lobe encode the value of multiple decision variables</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1162</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21100</pub-id><pub-id pub-id-type="pmid">18752411</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennerley</surname><given-names>S.W.</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Double dissociation of value computations in orbitofrontal and anterior cingulate neurons</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1581</fpage><lpage>1589</lpage><pub-id pub-id-type="doi">10.1038/nn.2961</pub-id><pub-id pub-id-type="pmid">22037498</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Pinto de Carvalho</surname><given-names>O</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Adaptation of reward sensitivity in orbitofrontal neurons</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>534</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4009-09.2010</pub-id><pub-id pub-id-type="pmid">20071516</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Cushman</surname><given-names>FA</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When does model-based control pay off?</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005090</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005090</pub-id><pub-id pub-id-type="pmid">27564094</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical and subcortical contributions to short-term memory for orienting movements</article-title><source>Neuron</source><volume>88</volume><fpage>367</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.08.033</pub-id><pub-id pub-id-type="pmid">26439529</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Armel</surname><given-names>C</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual fixations and the computation and comparison of value in simple choice</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1292</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1038/nn.2635</pub-id><pub-id pub-id-type="pmid">20835253</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuwabara</surname><given-names>M</given-names></name><name><surname>Kang</surname><given-names>N</given-names></name><name><surname>Holy</surname><given-names>TE</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural mechanisms of economic choices in mice</article-title><source>eLife</source><volume>9</volume><elocation-id>e49669</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49669</pub-id><pub-id pub-id-type="pmid">32096761</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Hueske</surname><given-names>E</given-names></name><name><surname>Hirokawa</surname><given-names>J</given-names></name><name><surname>Masset</surname><given-names>P</given-names></name><name><surname>Ott</surname><given-names>T</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reinforcement biases subsequent perceptual decisions when confidence is low, a widespread behavioral phenomenon</article-title><source>eLife</source><volume>9</volume><elocation-id>e49834</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49834</pub-id><pub-id pub-id-type="pmid">32286227</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>B</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamic response-by-response models of matching behavior in rhesus monkeys</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>84</volume><fpage>555</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1901/jeab.2005.110-04</pub-id><pub-id pub-id-type="pmid">16596980</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>MatlabStan</data-title><version designator="5831c78">5831c78</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/brian-lau/MatlabStan/releases/tag/v2.15.1.0">https://github.com/brian-lau/MatlabStan/releases/tag/v2.15.1.0</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Seo</surname><given-names>H</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural basis of reinforcement learning and decision making</article-title><source>Annual Review of Neuroscience</source><volume>35</volume><fpage>287</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150512</pub-id><pub-id pub-id-type="pmid">22462543</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leys</surname><given-names>C</given-names></name><name><surname>Ley</surname><given-names>C</given-names></name><name><surname>Klein</surname><given-names>O</given-names></name><name><surname>Bernard</surname><given-names>P</given-names></name><name><surname>Licata</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Detecting outliers: do not use standard deviation around the mean, use absolute deviation around the median</article-title><source>Journal of Experimental Social Psychology</source><volume>49</volume><fpage>764</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2013.03.013</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lombrozo</surname><given-names>T.</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Learning by Thinking” in Science and in Everyday Life’, in The scientific imagination</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Mirian</surname><given-names>MS</given-names></name><name><surname>Kehoe</surname><given-names>EJ</given-names></name><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Associative Learning from Replayed Experience</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/100800</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prioritized memory access explains planning and hippocampal replay</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id><pub-id pub-id-type="pmid">30349103</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McDaniel</surname><given-names>MA</given-names></name><name><surname>Einstein</surname><given-names>GO</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Prospective Memory: An Overview and Synthesis of an Emerging Field</source><publisher-name>SAGE Publications</publisher-name><pub-id pub-id-type="doi">10.4135/9781452225913</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDannald</surname><given-names>MA</given-names></name><name><surname>Lucantonio</surname><given-names>F</given-names></name><name><surname>Burke</surname><given-names>KA</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Ventral striatum and orbitofrontal cortex are both required for model-based, but not model-free, reinforcement learning</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>2700</fpage><lpage>2705</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5499-10.2011</pub-id><pub-id pub-id-type="pmid">21325538</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Identifying Model-Based and Model-Free Patterns in Behavior on Multi-Step Tasks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/096339</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dorsal hippocampus contributes to model-based planning</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1269</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1038/nn.4613</pub-id><pub-id pub-id-type="pmid">28758995</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Habits without values</article-title><source>Psychological Review</source><volume>126</volume><fpage>292</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1037/rev0000120</pub-id><pub-id pub-id-type="pmid">30676040</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Venditto</surname><given-names>SJC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-step planning in the brain</article-title><source>Current Opinion in Behavioral Sciences</source><volume>38</volume><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.07.003</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miranda</surname><given-names>B</given-names></name><name><surname>Malalasekera</surname><given-names>WMN</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Combined model-free and model-sensitive reinforcement learning in non-human primates</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007944</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007944</pub-id><pub-id pub-id-type="pmid">32569311</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Moylan</surname><given-names>EJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Basile</surname><given-names>BM</given-names></name><name><surname>Turchi</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Specialized areas for value updating and goal selection in the primate orbitofrontal cortex</article-title><source>eLife</source><volume>4</volume><elocation-id>e11695</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11695</pub-id><pub-id pub-id-type="pmid">26673891</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Specializations for reward-guided decision-making in the primate ventral prefrontal cortex</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>404</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0013-4</pub-id><pub-id pub-id-type="pmid">29795133</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noonan</surname><given-names>MP</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex</article-title><source>PNAS</source><volume>107</volume><fpage>20547</fpage><lpage>20552</lpage><pub-id pub-id-type="doi">10.1073/pnas.1012246107</pub-id><pub-id pub-id-type="pmid">21059901</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noonan</surname><given-names>MAP</given-names></name><name><surname>Chau</surname><given-names>BKH</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Fellows</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contrasting effects of medial and lateral orbitofrontal cortex lesions on credit assignment and decision-making in humans</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7023</fpage><lpage>7035</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0692-17.2017</pub-id><pub-id pub-id-type="pmid">28630257</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Lights, camembert, action! the role of human orbitofrontal cortex in encoding stimuli, rewards, and choices</article-title><source>Annals of the New York Academy of Sciences</source><volume>1121</volume><fpage>254</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1196/annals.1401.036</pub-id><pub-id pub-id-type="pmid">17872386</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neurobiology of economic choice: a good-based model</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>333</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-061010-113648</pub-id><pub-id pub-id-type="pmid">21456961</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neuronal origins of choice variability in economic decisions</article-title><source>Neuron</source><volume>80</volume><fpage>1322</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.013</pub-id><pub-id pub-id-type="pmid">24314733</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Conen</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Orbitofrontal cortex: A neural circuit for economic decisions</article-title><source>Neuron</source><volume>96</volume><fpage>736</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.031</pub-id><pub-id pub-id-type="pmid">29144973</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paxinos</surname><given-names>G</given-names></name><name><surname>Watson</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The Rat Brain in Stereotaxic Coordinates: Hard Cover Edition</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Definition of the orbital cortex in relation to specific connections with limbic and visceral structures and other cortical regions</article-title><source>Annals of the New York Academy of Sciences</source><volume>1121</volume><fpage>54</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1196/annals.1401.008</pub-id><pub-id pub-id-type="pmid">17698999</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>Glmnet for matlab</data-title><version designator="GPL-2">GPL-2</version><source>Hastie</source><ext-link ext-link-type="uri" xlink:href="https://hastie.su.domains/glmnet_matlab/">https://hastie.su.domains/glmnet_matlab/</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>EL</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Medial-lateral organization of the orbitofrontal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>1347</fpage><lpage>1362</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00573</pub-id><pub-id pub-id-type="pmid">24405106</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>EL</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding subjective decisions from orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>973</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1038/nn.4320</pub-id><pub-id pub-id-type="pmid">27273768</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal activity related to reward value and motivation in primate frontal cortex</article-title><source>Science</source><volume>304</volume><fpage>307</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1126/science.1093223</pub-id><pub-id pub-id-type="pmid">15073380</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Mitz</surname><given-names>AR</given-names></name><name><surname>Chacko</surname><given-names>RV</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effects of amygdala lesions on reward-value coding in orbital and medial prefrontal cortex</article-title><source>Neuron</source><volume>80</volume><fpage>1519</fpage><lpage>1531</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.036</pub-id><pub-id pub-id-type="pmid">24360550</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudebeck</surname><given-names>P.H.</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The orbitofrontal oracle: cortical mechanisms for the prediction and evaluation of specific behavioral outcomes</article-title><source>Neuron</source><volume>84</volume><fpage>1143</fpage><lpage>1156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.049</pub-id><pub-id pub-id-type="pmid">25521376</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Lundgren</surname><given-names>DA</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Specialized representations of value in the orbital and ventrolateral prefrontal cortex: desirability versus availability of outcomes</article-title><source>Neuron</source><volume>95</volume><fpage>1208</fpage><lpage>1220</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.07.042</pub-id><pub-id pub-id-type="pmid">28858621</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rustichini</surname><given-names>A</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neuro-computational model of economic decisions</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>1382</fpage><lpage>1398</lpage><pub-id pub-id-type="doi">10.1152/jn.00184.2015</pub-id><pub-id pub-id-type="pmid">26063776</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Chiba</surname><given-names>AA</given-names></name><name><surname>Gallagher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Orbitofrontal cortex and basolateral amygdala encode expected outcomes during learning</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>155</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1038/407</pub-id><pub-id pub-id-type="pmid">10195132</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Roesch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Orbitofrontal cortex, associative learning, and expectancies</article-title><source>Neuron</source><volume>47</volume><fpage>633</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.07.018</pub-id><pub-id pub-id-type="pmid">16129393</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Takahashi</surname><given-names>YK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A new perspective on the role of the orbitofrontal cortex in adaptive behaviour</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>885</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1038/nrn2753</pub-id><pub-id pub-id-type="pmid">19904278</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Takahashi</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>T-L</given-names></name><name><surname>McDannald</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Does the orbitofrontal cortex signal value?</article-title><source>Annals of the New York Academy of Sciences</source><volume>1239</volume><fpage>87</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2011.06210.x</pub-id><pub-id pub-id-type="pmid">22145878</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Cai</surname><given-names>MB</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human orbitofrontal cortex represents a cognitive map of state space</article-title><source>Neuron</source><volume>91</volume><fpage>1402</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.019</pub-id><pub-id pub-id-type="pmid">27657452</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reward-based training of recurrent neural networks for cognitive and value-based tasks</article-title><source>eLife</source><volume>6</volume><elocation-id>e21492</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21492</pub-id><pub-id pub-id-type="pmid">28084991</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Cooch</surname><given-names>NK</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What the orbitofrontal cortex does not do</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>620</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1038/nn.3982</pub-id><pub-id pub-id-type="pmid">25919962</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugrue</surname><given-names>LP</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Choosing the greater of two goods: neural currencies for valuation and decision making</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>363</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/nrn1666</pub-id><pub-id pub-id-type="pmid">15832198</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sul</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Huh</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Distinct roles of rodent orbitofrontal and medial prefrontal cortex in decision making</article-title><source>Neuron</source><volume>66</volume><fpage>449</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.033</pub-id><pub-id pub-id-type="pmid">20471357</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Haney</surname><given-names>RZ</given-names></name><name><surname>Calu</surname><given-names>DJ</given-names></name><name><surname>Taylor</surname><given-names>AR</given-names></name><name><surname>Burke</surname><given-names>KA</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The orbitofrontal cortex and ventral tegmental area are necessary for learning from unexpected outcomes</article-title><source>Neuron</source><volume>62</volume><fpage>269</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.005</pub-id><pub-id pub-id-type="pmid">19409271</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>SJ</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Maddison</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The orbitofrontal cortex: neuronal activity in the behaving monkey</article-title><source>Experimental Brain Research</source><volume>49</volume><fpage>93</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1007/BF00235545</pub-id><pub-id pub-id-type="pmid">6861938</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname><given-names>L</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Relative reward preference in primate orbitofrontal cortex</article-title><source>Nature</source><volume>398</volume><fpage>704</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1038/19525</pub-id><pub-id pub-id-type="pmid">10227292</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuronal activity in primate dorsolateral and orbital prefrontal cortex during performance of a reward preference task</article-title><source>The European Journal of Neuroscience</source><volume>18</volume><fpage>2069</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02922.x</pub-id><pub-id pub-id-type="pmid">14622240</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Orbitofrontal cortex and its contribution to decision-making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>31</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094334</pub-id><pub-id pub-id-type="pmid">17417936</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cross-species studies of orbitofrontal cortex and value-based decision-making</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>13</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1038/nn.2956</pub-id><pub-id pub-id-type="pmid">22101646</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Separable learning systems in the macaque brain and the role of orbitofrontal cortex in contingent learning</article-title><source>Neuron</source><volume>65</volume><fpage>927</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.02.027</pub-id><pub-id pub-id-type="pmid">20346766</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Noonan</surname><given-names>MP</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Giving credit where credit is due: orbitofrontal cortex and valuation in an uncertain world</article-title><source>Annals of the New York Academy of Sciences</source><volume>1239</volume><fpage>14</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2011.06257.x</pub-id><pub-id pub-id-type="pmid">22145871</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal cortex as a cognitive map of task space</article-title><source>Neuron</source><volume>81</volume><fpage>267</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.005</pub-id><pub-id pub-id-type="pmid">24462094</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64575.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute on Drug Abuse, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>In this manuscript, Miller et al., use the two-step task, a task initially designed to discern model-free from model-based behavior, to probe OFC at specific times throughout the two stages of the task to understand OFC's role in choice and learning. The authors exploited an interesting feature of the two-step task that allows choice and learning to be separated into separate stages. They then used this feature to clearly show that OFC is not necessary for choice behavior in a well-learned choice task, although it is required for updating the model based on trial-by-trial reward information. The ability to separate learning from choice in a decision-making task is a unique and novel approach for probing OFC function, making this manuscript an important contribution to our understanding of OFC function.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64575.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute on Drug Abuse, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rudebeck</surname><given-names>Peter</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Mount Sinai NY</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Value Representations in the Rodent Orbitofrontal Cortex Drive Learning, not Choice&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Geoffrey Schoenbaum as the Reviewing Editor and Kate Wassum as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Peter Rudebeck (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this manuscript, Miller et al., use the two-step task, a task initially designed to discern model-free from model-based behavior, to probe OFC at specific times throughout the two stages of the task to understand OFC's role in choice and learning. The authors exploited an interesting feature of the two-step task that allows choice and learning to be separated into separate stages. They then used this feature to clearly show that OFC is not necessary for choice behavior in a well-learned choice task, although it is required for updating the model based on trial-by-trial reward information. The ability to separate learning from choice in a decision-making task is a unique and novel approach for probing OFC function, making this manuscript an important contribution to our understanding of OFC function.</p><p>Revisions:</p><p>In the discussion and the reviews below, each reviewer was enthusiastic about the results and felt they should be published without undue delay. While there were a number of questions raised, in the discussion it was felt that three key areas in particular were particularly important or essential to address in the revision. As the authors will see, these general areas encompass many of the specific requests in each of the reviews.</p><p>(1) It was felt that it was important for the authors to provide more information and specification of the modeling, so that the details would be more accessible to readers.</p><p>(2) Additional details regarding the single unit results, and possibly changes to bring it more into alignment with the epochs employed in the inactivation experiment, are necessary.</p><p>(3) Some softening of the hard claim that OFC is not necessary for choice; while the results of this specific study are clear, the possibility exists that in other settings this may not be true. This is discussed a bit in the current paper, but this issue was raised in each review and also in the discussion in various ways. These thoughts should be addressed in the revision.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In this manuscript, Miller et al., further dissect OFC's role in the two-step task, a task initially designed to discern model-free from model-based behavior. While their previous study showed that session-wide pharmacological inactivation of the OFC shifts rats to learn from rewards in a model-free way, they found that such full-session inactivation played no role in impairing choice behavior on individual trials. Here they used optogenetics to probe OFC at specific times throughout the two stages of the task to further understand OFC's role in choice and learning. Inactivation of OFC during the initial choice phase had no effect on behavior, although inactivation during the second phase, as well as across both phases, impaired learning of probabilistic outcomes. Comparing their behavioral results to a well-fit hybrid RL model of behavior, they showed that this change in the rats' behavior best corresponds with a loss of expected outcome value rather than an impairment of choice value. Electrophysiology recordings in OFC revealed that expected outcome value was substantially better coded for during the task than chosen value, consistent with the inactivation results.</p><p>The authors exploited an interesting feature of the two-step task that allows choice and learning to be separated into separate stages. They then used this feature to clearly show that OFC is not necessary for choice behavior in a well-learned choice task, although it is required for updating the model based on trial-by-trial reward information. The ability to separate learning from choice in a decision-making task is a unique and novel approach for probing OFC function, making this manuscript an important contribution to our understanding of OFC function.</p><p>Overall I think this is a terrific study. The approach is novel and creative, taking advantage of a unique task design, combined with unit recording and temporally specific manipulations, to put at risk an important set of hypotheses about an important brain region – the OFC. The paper is also exceptionally well-written and, to the authors credit, I think they fairly consider the array of results both in setting up the importance of their study and in understanding the significance of the results. I definitely have questions, but I do not mean them to detract from my enthusiastic support.</p><p>So one question I had was whether the task favors the result that they obtained. Specifically, it seems to me that prior work, starting with the original Daw report and including rodent work, shows that choice in this setting is driven by a combination of MB and MF information. Or at least that both types of value can support many of the choices, whereas the learning in the critical trials requires MB information. I think this leads to an asymmetry where choice is less dependent on MB information than the type of learning that is isolated; while perhaps this can be mitigated by which trials are analyzed, I wonder if this asymmetry might lead to some of the results. That is, the finding that OFC activity is only weakly related to and unnecessary for choice in the task could reflect the weaker dependence of choice on MB information. And vice versa for the involvement in learning. If the task were more symmetrical – as one might speculate probe test behavior in devaluation is for example – could a stronger role for OFC in driving choices appear? Note this would not affect the significance of the current results for settings like economic choice, at least as experimentally tested; it would simply be a weaker distinction I think in the function. In any event, the authors could comment on this; if they disagree with the idea of an asymmetry at least in the analysis, then a careful explanation of that to me and perhaps in the paper would be useful. Or perhaps this is what the authors are thinking? I was confused a bit where they note several times that some other brain area is functioning as a controller for behavior – do they mean a MF area like striatum? Or another MB controller?</p><p>Another question I have is how the authors think the update might occur. Specifically, do the authors suggest that this is in agreement with the credit assignment hypothesis, indicating that these are incremental changes in values of the second stage states? Or could this be modeled as updates in state transitions if the final outcome is modeled as states with different probabilities of reward. These possibilities might not be distinguishable with the task, but perhaps they should be mentioned as they pertain to different interpretations of OFC function.<italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This manuscript has the feel of one that has been well reviewed before and so my suggestions are really just to make the paper more accessible:</p><p>The computational modeling is really under specified/described in the paper. I realize that the authors have published the model elsewhere but without more information in this manuscript a reader is left a little unsure how to interpret the effects reported. The authors need to fix this lack of specificity in two places: (1) in the behavioral analyses of the data and (2) the modeling of the effects of optogenetic inhibition. In both places the model is introduced rather suddenly. Figures 1 and 4 have schematics for how the model generally works but more of the text should be dedicated to explaining how the model is implemented. Similarly, in the section analyzing the effects of optogenetic inhibition on behavior the section/paragraph using modeling is quite opaque. It is simply introduced as an &quot;artificial planning agent&quot; but without specifying how this works a reader is unable to evaluate this approach. The authors should take the time to explain in the text how they simulated the choices of this agent and how that in turn was used to generate the different effects on behavior. This is essential to ensuring that the sophisticated approach to modeling behavior that was taken here can be fully appreciated.</p><p>The analyses of the neurophysiology data are solid but lack detail. The authors should provide: (1) breakdowns of the numbers of neurons/MUAs recorded per rat. (2) Given the variability in regression coefficients between animals, the authors should also ensure that their neurophysiology effects are not simply being driven by a single or subset of animals. This could be done by adding subject as a random effect into their models, subject dropping, data splitting or similar. Either way, the authors should provide evidence that the effects are not solely driven by a single/few animals. Related to the above, the authors should also prove that this effect of animal is consistent for both the CPD analyses as well as the number of neurons signaling the different factors as shown in figure 3 – supplement 3.</p><p>While the analyses of behavior using the computational model is really elegant, only presenting the change in planning index as the primary measure of change in behavior after OFC inactivation feels very far removed from the data. The authors should provide more information in both text and figures on how inactivation of OFC alters subsequent choices or other metrics of behaviors such as reaction time. The authors could consider showing change in regression coefficients similar to supplemental Figure 1, win-stay/loose-shift behaviors etc. Further, analyses of reaction times/time to initiate choices after optogenetic inhibition could be another way to get insight into the effects of inactivating OFC. On trials after laser delivery, do rats slow down on the first or second stage of the two-step task? If they are slower to choose between left and right on the first step this might indicate a change in policy/decision-making whereas if they are slower to initiate/move based on the second stage it would indicate altered value expectations.<italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The central question in this study is whether value representations in the (rodent) OFC drive choice or whether they drive learning. The core question is an important one in neuroscience because the OFC is the focus of a great deal of long-standing debate. The authors of the study separate these possibilities using a well-known multi-step task. The task is powerful because it provides a computationally strong and elegant way to separate these things. Note that one innovative feature of the authors' research program is training rodents to perform this task – they have done so in previous work, and, here leverage that advance to begin to answer neuroscientific questions. Formally speaking, the authors use two complementary methods, unit recordings and optogenetic silencing. Results from both methods wind up pointing in the same direction, towards the learning account and away from the choice account.</p><p>I have a few thoughts that relate more to the presentation than to the data. I wonder how the frame structure of learning vs. choice relates to the existing larger literature on OFC. From my understanding, this frame skips cognitive mapping functions and the related expectancy signaling accounts, and also skips accounts in which OFC helps navigate complex of model-based choices. Im not saying that these accounts are all in conflict or must be addressed, but most readers will be familiar with them, and the authors should work through how they all relate. Are they saying these others are all part of a more general learning-based account? Additional explanation would help.</p><p>Relatedly, it would seem that the null hypothesis here is that OFC does both learning and choice – why not?? Is there any a priori reason it shouldn't do them? It seems reductive to insist a brain region can only have one function.</p><p>So for example, the authors find that OFC is more active for outcomes than for offers. But it is still active for offers. One might therefore conclude most parsimoniously that OFC is involved more strongly in learning, but still involved in choice as well. Indeed, I would look at the significant CPDs for choice variables and say the authors have presented clear evidence for both. It's not even clear that bigger CPDs for outcome than offer implies that OFC is more involved in learning than choice – there are several reasons you could imagine why you would see this effect (e.g. learning is more difficult, requires more attention, is more narrowly focused on OFC). These other possibilities are all consistent with the &quot;both&quot; theory, which the authors do not seem to consider. Like, these sentences, for example, seem to contain within them the assumption that weak but significant correlations are otiose… which seems unlikely…. &quot;We find that neurons in the OFC correlate with the expected values of the items being learned about, but only weakly with the expected values of the items being chosen between. This indicates that, within a repeated-trials task, neural representations in the OFC carry value information that is selective for a role in learning, but do not seem to carry value information selective for a role in choice.&quot; Moreover, the analysis is conservative – if I understand correctly it attributes all variance to other confounding variables and counts only remaining variance. I see why the authors would do it, but this would seem to artificially deflate responses. That itself might not be bad, if we keep it in mind when interpreting the data. But the result here is that the effects are smaller than expected.</p><p>Even so, the effect size numerically is not as big as the authors' rhetoric implies – 1.1% is the good number and 0.52% is the bad number. Is it so obvious that we should interpret these as the authors want us to – that 1.1% means OFC is all about learning and 0.52% means it has nothing to do with choice? Stronger results are reported a few sentences later (the ratio is 3.8 or 4.0). However, again, I think it is a step too far to consider this to be evidence that OFC does one thing and not another. I am reminded, for example, of the fact that nodulation by attention in visual area V4 and strong driving by visual stimuli – in those areas, the modulation ratio is about 5:1. That doesn't mean, however, that V4 doesn't participate in attention.</p><p>For the causal manipulation, I think a clearer case can be made about its novelty. The issue is a series of papers by Gardner and Schoenbaum using causal/optogenetic approaches to demonstrate the relevance of OFC to learning and learning-like processes. The authors here say that &quot;Our results build on this by pinpointing, out of all those possibilities, a specific computational role for OFC: it provides outcome value estimates to a learning system, elsewhere in the brain, that updates the estimates of these values.&quot; However, again, this claim is not as strong as it seems – there is no reason given to think that OFC only has one role or that its role in this task is the only role that it plays in cognition more generally. Reading this paper along with the Gardner papers, I would say that the authors' conclusion is wrong in that they do not reject alternative roles for OFC other than choice.</p><p>So what are we left with here? I believe the authors that learning-related activity is stronger than choice-related activity. And I think it's worth pointing out that this suggests OFC plays a more important role in learning than choice, although it's not necessarily further than a suggestion. The optogenetic silencing results are stronger, but the novelty of those results – especially relative to Schoenbaum's series of papers – is not clear.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64575.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Revisions:</p><p>In the discussion and the reviews below, each reviewer was enthusiastic about the results and felt they should be published without undue delay. While there were a number of questions raised, in the discussion it was felt that three key areas in particular were particularly important or essential to address in the revision. As the authors will see, these general areas encompass many of the specific requests in each of the reviews.</p><p>(1) It was felt that it was important for the authors to provide more information and specification of the modeling, so that the details would be more accessible to readers.</p></disp-quote><p>Thank you for this comment, we have added substantially more detail on the modeling and behavior analysis, to clarify them, in three places. The first is a new figure (Figure 2) with panels on the cognitive model and one the trial-history regression analysis. The second is a substantially expanded description in the section of Results that is devoted to behavior (“Planning Strategy in the Two-Step Task Separates Choosing and Learning”). The third is an expanded description of the synthetic inactivation experiments and analysis of synthetic data in the section of Results describing optogenetics experiments (“Inactivations of OFC impair update process, not choice process”). We hope this will successfully make the modeling more accessible to readers.</p><disp-quote content-type="editor-comment"><p>(2) Additional details regarding the single unit results, and possibly changes to bring it more into alignment with the epochs employed in the inactivation experiment, are necessary.</p></disp-quote><p>Following the individual reviewers’ requests, we have added additional detail regarding the electrophysiological recordings in several new supplemental figures. Two of these focus on robustness of results across rats (Figure 4-S4: ephys CPD data analysis separated by rat; 4-S5: ephys fraction of significant units separated by rat), as requested by R2. A third (Figure 4-S6) addresses the fact that CPD is a conservative measure when regressors are correlated, a concern raised by R3, by removing a regressor (interaction between outcome port and reward) that is highly correlated with choice-related value regressors. The results remain substantially the same, thus buttressing the analysis and conclusions in the main text. The last addition we made follows a request from R1 and adds a new analysis that addresses the fact that choices in our task are not driven exclusively by expected value, by using choice-related regressors that consider not just value but the total decision variable (Figure 4-S7).</p><p>We have also added additional detail clarifying how the epochs in the inactivation experiment relate to the timebins of analysis in the electrophysiology experiment. In Figure 5-S2, we show a timecourse of the probability distributions of being in each optogenetics epoch, aligned in the same way as our analysis of the electrophysiology data.</p><disp-quote content-type="editor-comment"><p>(3) Some softening of the hard claim that OFC is not necessary for choice; while the results of this specific study are clear, the possibility exists that in other settings this may not be true. This is discussed a bit in the current paper, but this issue was raised in each review and also in the discussion in various ways. These thoughts should be addressed in the revision.</p></disp-quote><p>We have softened these claims throughout the paper, and clarified the description in our discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Overall I think this is a terrific study. The approach is novel and creative, taking advantage of a unique task design, combined with unit recording and temporally specific manipulations, to put at risk an important set of hypotheses about an important brain region – the OFC. The paper is also exceptionally well-written and, to the authors credit, I think they fairly consider the array of results both in setting up the importance of their study and in understanding the significance of the results. I definitely have questions, but I do not mean them to detract from my enthusiastic support.</p></disp-quote><p>We are very grateful for the reviewer’s enthusiastic support!</p><disp-quote content-type="editor-comment"><p>So one question I had was whether the task favors the result that they obtained. Specifically, it seems to me that prior work, starting with the original Daw report and including rodent work, shows that choice in this setting is driven by a combination of MB and MF information. Or at least that both types of value can support many of the choices, whereas the learning in the critical trials requires MB information. I think this leads to an asymmetry where choice is less dependent on MB information than the type of learning that is isolated; while perhaps this can be mitigated by which trials are analyzed, I wonder if this asymmetry might lead to some of the results. That is, the finding that OFC activity is only weakly related to and unnecessary for choice in the task could reflect the weaker dependence of choice on MB information. And vice versa for the involvement in learning. If the task were more symmetrical – as one might speculate probe test behavior in devaluation is for example – could a stronger role for OFC in driving choices appear? Note this would not affect the significance of the current results for settings like economic choice, at least as experimentally tested; it would simply be a weaker distinction I think in the function. In any event, the authors could comment on this; if they disagree with the idea of an asymmetry at least in the analysis, then a careful explanation of that to me and perhaps in the paper would be useful. Or perhaps this is what the authors are thinking? I was confused a bit where they note several times that some other brain area is functioning as a controller for behavior – do they mean a MF area like striatum? Or another MB controller?</p></disp-quote><p>We thank the reviewer for bringing up this important question. The concern is that there may be an asymmetry between learning and choosing in terms of how “model-based” the rats are. In this situation, learning about outcome port values may be uniquely the purview of a model-based system, while deciding between choice ports might be influenced by other systems, including a model-free one that our analyses do not focus on.</p><p>We first note that while the reviewer is correct that most versions of the two-step task show an apparent influence of model-free RL on behavior, our rat version is unusual in that it apparently does not. Specifically, if we add a model-free agent to our mixture-of-agents behavioral model, we find that this does not meaningfully improve quality of fit. In our revision, we now make this point more robustly, comparing several different types of model-free agent (TD(1), TD(0), and TD(λ)), moving this analysis into a main-text figure (Figure 2c), and adding the following text to the Results section:</p><p>“Among these additional components we tested were several model-free reinforcement learning strategies, which have been reported to contribute to behavior on similar multi-step tasks (Daw et al., 2011; Hasz and David Redish, 2018; Dezfouli and Balleine, 2019; Groman et al., 2019; Akam et al., 2020; Miranda et al., 2020). That adding them to our model does not improve quality of fit suggests that model-free reinforcement learning does not contribute meaningfully to rat behavior in our task. This is important for our analysis because these strategies involve value representations of their own – that they are not part of our model means that model-based value representations are the only representations of expected reward that are present.”</p><p>This does not eliminate the possibility of an asymmetry though, because our rat behavior does show an apparent influence of other processes – perseveration, bias, and one we call “novelty preference”. These processes do not involve representations of value in the sense of expected future reward (they are entirely insensitive to reward). However they do influence choice, and their representations might therefore be considered “value” in the sense of revealed preference. In our revision, we have added a new analysis of neural recordings that uses analogs of “choice value difference” and “chosen value” that consider the combined influence of all of these components, instead of isolating the contribution of the model-based component, and added the following text to our Results section:</p><p>“These results were robust to replacing the choice-related value regressors with analogs that consider the full decision variable, including contributions from perseveration and novelty preference as well as expected value (Figure 4-S5).”</p><p>We have also added a new analysis of our optogenetics quantifying any effect of inactivation on perseverative, model-free, or novelty preference patterns of behavior, and added the following text to our Results section:</p><p>As in our previous study, inactivation of OFC did not significantly affect other regression-based behavioral indices (Figure 5-S2).</p><p>Together, we believe that these findings indicate that it is unlikely for our results to be driven by an asymmetry between learning vs. choosing in terms of how “model-based” vs “model-free” each is, because in this task, rat behavior appears to have no significant model-free component.</p><disp-quote content-type="editor-comment"><p>Another question I have is how the authors think the update might occur. Specifically, do the authors suggest that this is in agreement with the credit assignment hypothesis, indicating that these are incremental changes in values of the second stage states? Or could this be modeled as updates in state transitions if the final outcome is modeled as states with different probabilities of reward. These possibilities might not be distinguishable with the task, but perhaps they should be mentioned as they pertain to different interpretations of OFC function.</p></disp-quote><p>Our data are consistent with the idea that the OFC supports incremental learning, either of values of second stage states or of state transition probabilities. This distinction is closely related to the distinction between “common currency” vs “specific outcome” expectancy signals brought up by R3. We have added a paragraph to the discussion clarifying this:</p><p>“Our results are consistent with the view that the OFC carries expectancy signals (Schoenbaum and Roesch, 2005; Rudebeck and Murray, 2014) indicating which outcomes are expected to follow from the current state. This view is distinct from the “chosen value” view described earlier in that it claims that these signals indicate the particular identities of the expected outcomes (e.g. different types of foods) rather than the abstract “reward” or “common currency” value found in reinforcement learning or neuroeconomic theories. Our experiment does not speak to the difference between these views, because the only reward available in our task is a water droplet of a fixed size. Our “outcome value” correlates might therefore reflect expectations of this reward (the water droplet) in particular, and play a role in updates based on this expectation. In formal models, this might be described as updating estimates of a state transition probability (e.g. from an “at the outcome port” state to a “receive water droplet” state). Alternatively, “outcome value” might abstract over many different possible rewarding outcomes. In formal models, this might be described as updating estimates of the reward function (e.g. in an “at the outcome port” state).”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This manuscript has the feel of one that has been well reviewed before and so my suggestions are really just to make the paper more accessible:</p><p>The computational modeling is really under specified/described in the paper. I realize that the authors have published the model elsewhere but without more information in this manuscript a reader is left a little unsure how to interpret the effects reported. The authors need to fix this lack of specificity in two places: (1) in the behavioral analyses of the data and (2) the modeling of the effects of optogenetic inhibition. In both places the model is introduced rather suddenly. Figures 1 and 4 have schematics for how the model generally works but more of the text should be dedicated to explaining how the model is implemented. Similarly, in the section analyzing the effects of optogenetic inhibition on behavior the section/paragraph using modeling is quite opaque. It is simply introduced as an &quot;artificial planning agent&quot; but without specifying how this works a reader is unable to evaluate this approach. The authors should take the time to explain in the text how they simulated the choices of this agent and how that in turn was used to generate the different effects on behavior. This is essential to ensuring that the sophisticated approach to modeling behavior that was taken here can be fully appreciated.</p></disp-quote><p>We thank the reviewer for pointing this out. We agree that it is important for the manuscript to be clear in the Results section about the behavioral model and how it contributes to these analyses. We have addressed this by changing it in two places. The first is in the behavioral Results section, which we have expanded to include descriptions of the cognitive model and the trial-history regression analysis. We have combined the figure panels that go with this section into a new main text figure (Figure 2). The relevant paragraphs now read:</p><p>“As in our previous study (Miller, Botvinick and Brody, 2017), rat behavior was well-described by a cognitive model combining this model-based planning strategy with a mixture of three additional components. The first of these is “perseveration”, which reflects a tendency to repeat past choices, regardless of their outcomes (Akaishi et al., 2014; Miller, Shenhav and Ludvig, 2019). The second, which we term “novelty preference”, reflects a tendency to repeat (or to switch away from) choices that lead to an uncommon transition, regardless of whether or not they are rewarded. The third is a constant side bias, reflecting an overall tendency to prefer either the right or the left choice port. Each of these components is associated with a weighting parameter (<italic>β</italic>), reflecting the strength of its influence on the decision between the left and right choice port on each trial (Figure 2a, right: “Choose a choice port”). Fitting these parameters to the dataset for each rat, we find that the planning and perseverative components earn weights that are large and positive (Figure 2b), while the novelty preference and bias components earn weights that are generally smaller and differ in sign among rats (Figure 2b). The planning and perseveration components are each associated with a learning rate parameter (<italic>α</italic>), which reflects the relative influence of trials in the recent past vs the more distant past. Learning rates for the planning component were consistently larger than those for the perseverative component (Figure 2-S1).</p><p>We validate that our cognitive model provides a good description of rat behavior in two different ways. The first way is quantitative model comparison: we compute a quality-of-fit score for the model using cross-validated likelihood, and compare this score between our model and various alternatives (Figure 2c). Alternative models which are missing any of the four components perform substantially worse (Figure 2c, red points), while alternative models adding various additional components do not perform substantially better (Figure 2c, green points). Among these additional components we tested were several model-free reinforcement learning strategies, which have been reported to contribute to behavior on similar multi-step tasks (Daw et al., 2011; Hasz and David Redish, 2018; Dezfouli and Balleine, 2019; Groman et al., 2019; Akam et al., 2020; Miranda et al., 2020). That adding them to our model does not improve quality of fit suggests that model-free reinforcement learning does not contribute meaningfully to rat behavior in our task. This is important for our analysis because these strategies involve value representations of their own – that they are not part of our model means that model-based value representations are the only representations of expected reward that are present.</p><p>Our second way of validating the cognitive model makes use of a trial-history regression analysis (Lau and Glimcher, 2005), which provides a theory-neutral way of characterizing the patterns present in a behavioral dataset. This analysis fits separate weights for each of the four possible outcome types (common-rewarded, common-omission, uncommon-rewarded, uncommon-omission). These weights will be positive if the rat tends to repeat choices that lead to that outcome, and negative if the rat tends to switch away from such choices. Behavioral datasets produced by different strategies will have different patterns of weights (Miller, Brody and Botvinick, 2016). For example, a model-based planning strategy will show more positive weights for common-reward than for uncommon-reward (since a rewarding outcome port visited after a common transition is likely to be reached again by repeating the choice, while one reached after an uncommon transition is more likely to be reached by switching to the other choice port instead), as well as more negative weights for common-omission than for uncommon-omission (since a unrewarding outcome port visited after a common transition is likely to be avoided by switching the choice, while one reached after an uncommon transition is more likely to be avoided by repeating the choice). As in our previous study, rats trained for the present study universally show this qualitative pattern, and the quantitative patterns present in their weights are well-matched by fits of the cognitive model (example rat: Figure 2d; all rats: Figure 2-S2). To summarize the pattern and compare it to others in behavior, we define a “planning index” as the linear combination of weights consistent with the planning strategy, as well as a “stay” index, a “common-stay/uncommon-switch” index, and a “win-stay/lose-switch” index quantifying other patterns (Figure 2e).[…] All rats showed large values of the planning index and the stay index, and much smaller values of the common-stay/uncommon-switch and win-stay/lose-switch indices (Figure 2f).”</p><p>The second is in the optogenetics Results section. We have expanded our description of the modeling here, both in terms of how we generate the synthetic datasets and how we analyze them. We have also added a new equation to the main text. The relevant passage now reads:</p><p>“To help understand which aspect of the behavior was affected by silencing the OFC, we used our cognitive model (Figure 2a) to perform three different types of synthetic inactivation experiments. […] This is because value acts as a summarized memory of previous trials’ outcomes, and attenuating it affects the influence of all of these.”</p><disp-quote content-type="editor-comment"><p>The analyses of the neurophysiology data are solid but lack detail. The authors should provide: (1) breakdowns of the numbers of neurons/MUAs recorded per rat. (2) Given the variability in regression coefficients between animals, the authors should also ensure that their neurophysiology effects are not simply being driven by a single or subset of animals. This could be done by adding subject as a random effect into their models, subject dropping, data splitting or similar. Either way, the authors should provide evidence that the effects are not solely driven by a single/few animals. Related to the above, the authors should also prove that this effect of animal is consistent for both the CPD analyses as well as the number of neurons signaling the different factors as shown in figure 3 – supplement 3.</p></disp-quote><p>We have added two new supplemental figures (Figure 4-S4 and 4-S5). The first of these contains both a breakdown of the number of singles and multis recorded in each rat, as well as an analysis which uses a data splitting approach to address the concern that effects may be driven by only a subset of animals. Specifically, we compute the timecourse of CPDs (as in main text Figure 4b and c) separately for each rat. The key findings that outcome value earns a higher population-level CPD than either type of choice-related value is visible in each of the six animals. The second new figure computes the fraction of units which significantly encode each regressor in each time bin, separately for each rat (as in Figure 4-S1). The finding that outcome value is encoded by a larger fraction of units than either type of choice value information is visible in five of the six rats (the sixth has only a small number of recorded units, few of which encode any of the three expected value regressors).</p><disp-quote content-type="editor-comment"><p>While the analyses of behavior using the computational model is really elegant, only presenting the change in planning index as the primary measure of change in behavior after OFC inactivation feels very far removed from the data. The authors should provide more information in both text and figures on how inactivation of OFC alters subsequent choices or other metrics of behaviors such as reaction time. The authors could consider showing change in regression coefficients similar to supplemental Figure 1, win-stay/loose-shift behaviors etc. Further, analyses of reaction times/time to initiate choices after optogenetic inhibition could be another way to get insight into the effects of inactivating OFC. On trials after laser delivery, do rats slow down on the first or second stage of the two-step task? If they are slower to choose between left and right on the first step this might indicate a change in policy/decision-making whereas if they are slower to initiate/move based on the second stage it would indicate altered value expectations.</p></disp-quote><p>We have added new supplemental figures showing in more detail how inactivation affects behavioral strategy. The first of these (Figure 5-S2) shows the effect of inactivation on all four behavioral indices (which are now described in Figure 2). Only the planning index is significantly affected. Figure 5-S3 breaks this down further by lag, and figure 5-S4 by individual rat. The effects are not as clear when disaggregated in this way but we agree that these figures add completeness.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The central question in this study is whether value representations in the (rodent) OFC drive choice or whether they drive learning. The core question is an important one in neuroscience because the OFC is the focus of a great deal of long-standing debate. The authors of the study separate these possibilities using a well-known multi-step task. The task is powerful because it provides a computationally strong and elegant way to separate these things. Note that one innovative feature of the authors' research program is training rodents to perform this task – they have done so in previous work, and, here leverage that advance to begin to answer neuroscientific questions. Formally speaking, the authors use two complementary methods, unit recordings and optogenetic silencing. Results from both methods wind up pointing in the same direction, towards the learning account and away from the choice account.</p><p>I have a few thoughts that relate more to the presentation than to the data. I wonder how the frame structure of learning vs. choice relates to the existing larger literature on OFC. From my understanding, this frame skips cognitive mapping functions and the related expectancy signaling accounts, and also skips accounts in which OFC helps navigate complex of model-based choices. Im not saying that these accounts are all in conflict or must be addressed, but most readers will be familiar with them, and the authors should work through how they all relate. Are they saying these others are all part of a more general learning-based account? Additional explanation would help.</p></disp-quote><p>We thank the reviewer for pointing this out. We believe our data are consistent with either of these accounts, while focusing on aspects that have not yet been the main focus of studies in those accounts. We have expanded our Discussion section to clarify this. The relevant paragraphs now read:</p><p>“Our data are also broadly consistent with the idea that the OFC represents a “cognitive map” of task-relevant state information. […] A promising direction for future work would be to perform experiments analogous to ours in the regions of the rodent brain that are plausibly homologous to ventral parts of the medial network (VO and MO) as well as elsewhere in PFC.”</p><p>And</p><p>“Our results are consistent with the view that the OFC carries expectancy signals (Schoenbaum and Roesch 2005; Rudebeck and Murray 2014) indicating which outcomes are expected to follow from the current state. […] Experiments using this task are therefore blind to the difference between representations of the expected probability of this reward (the water droplet) in particular and representations of expected value that abstract over many possible rewarding outcomes.”</p><disp-quote content-type="editor-comment"><p>Relatedly, it would seem that the null hypothesis here is that OFC does both learning and choice – why not?? Is there any a priori reason it shouldn't do them? It seems reductive to insist a brain region can only have one function.</p></disp-quote><p>We agree very much that it is plausible that the OFC participates in both learning and choosing (indeed we had expected that our study would show that this was the case in our task). We have adjusted the wording in our abstract, introduction, and discussion to be more clear about this. Relevant sentences now read (emphasis added):</p><p>Abstract:</p><p>“Firstly, they drive choice: the expected values of available options are compared to one another, and the best option is selected. Secondly, they support learning: expected values are compared to rewards actually received, and future expectations are updated accordingly. Whether these different functions are mediated by different neural</p><p>representations remains an open question.”</p><p>Introduction:</p><p>“Recording studies in many species have revealed neural correlates of expected value in the OFC (Thorpe, Rolls and Maddison, 1983; Schoenbaum, Chiba and Gallagher, 1998; Gottfried, O’Doherty and Dolan, 2003; Padoa-Schioppa and Assad, 2006; Sul et al., 2010), but it has not been clear whether these neural correlates of expected value are selective for roles in learning and in choosing. This, along with limitations inherent in neural perturbation studies, has made it difficult to determine whether the OFC plays a role in learning, choosing, or both.”</p><p>“We asked whether OFC neuron firing rates were correlated with the expected values of the items being learned about, the values being chosen between, or both.”</p><p>“To causally probe whether OFC plays a role in learning, in choosing, or in both processes, we transiently silenced OFC activity, and found that the pattern of behavior induced by this silencing was reproduced in our computational model only when we disrupted only the role of expected value in learning. Disrupting the role of value in choice did not reproduce this effect.”</p><p>Results:</p><p>“We sought to determine whether our neural recording data contained correlates of choice-related value signals, outcome-related value signals, or both.”</p><p>Discussion:</p><p>“Value representations in the OFC have been reported in many tasks and species, but it is still unclear whether they drive one process, the other process, or both.”</p><disp-quote content-type="editor-comment"><p>So for example, the authors find that OFC is more active for outcomes than for offers. But it is still active for offers. One might therefore conclude most parsimoniously that OFC is involved more strongly in learning, but still involved in choice as well. Indeed, I would look at the significant CPDs for choice variables and say the authors have presented clear evidence for both. It's not even clear that bigger CPDs for outcome than offer implies that OFC is more involved in learning than choice – there are several reasons you could imagine why you would see this effect (e.g. learning is more difficult, requires more attention, is more narrowly focused on OFC). These other possibilities are all consistent with the &quot;both&quot; theory, which the authors do not seem to consider. Like, these sentences, for example, seem to contain within them the assumption that weak but significant correlations are otiose… which seems unlikely…. &quot;We find that neurons in the OFC correlate with the expected values of the items being learned about, but only weakly with the expected values of the items being chosen between. This indicates that, within a repeated-trials task, neural representations in the OFC carry value information that is selective for a role in learning, but do not seem to carry value information selective for a role in choice.&quot; Moreover, the analysis is conservative – if I understand correctly it attributes all variance to other confounding variables and counts only remaining variance. I see why the authors would do it, but this would seem to artificially deflate responses. That itself might not be bad, if we keep it in mind when interpreting the data. But the result here is that the effects are smaller than expected.</p><p>Even so, the effect size numerically is not as big as the authors' rhetoric implies – 1.1% is the good number and 0.52% is the bad number. Is it so obvious that we should interpret these as the authors want us to – that 1.1% means OFC is all about learning and 0.52% means it has nothing to do with choice? Stronger results are reported a few sentences later (the ratio is 3.8 or 4.0). However, again, I think it is a step too far to consider this to be evidence that OFC does one thing and not another. I am reminded, for example, of the fact that nodulation by attention in visual area V4 and strong driving by visual stimuli – in those areas, the modulation ratio is about 5:1. That doesn't mean, however, that V4 doesn't participate in attention.</p></disp-quote><p>We revised the text to be clear that the electrophysiology results do show significant, if weaker, correlates of choice-related value information, and that the electrophysiology data does not rule out the idea that these units may play some role in choice. Relevant passages now read:</p><p>Introduction</p><p>“This indicates that, within a repeated-trials task, neural representations in the OFC carry value information that is largely selective for a role in learning, but only weakly carry value information selective for a role in choice.”</p><p>Results</p><p>“Across all of these variants a clear pattern was present: neural activity in OFC encodes the expected value of the visited outcome port more strongly than it encodes either type of value information about the choice ports. In computational models (Figure 2a), this type of value information plays a role in supporting learning, but does not play a direct role in choice. Our neural recording results therefore suggest that the OFC may play a role in supporting learning, but cast doubt on the idea that they play a strong role in driving choice directly.”</p><p>Discussion</p><p>“we find weak (though significant) representation of values associated with the available choices (“choice values”), little or no effect of silencing OFC at the putative time of choice, and effects of silencing inconsistent with impairing choice values in a computational model. Instead, we find strong representation of values associated with immediately impending reward outcomes (“outcome values”), a strong behavioral effect of silencing OFC at the time of those representations, and effects of silencing that are consistent with specifically impairing the use of outcome values for learning.”</p><p>We have also modified the main text CPD timecourse analysis to subtract from each regressor’s CPD the mean of the values of that CPD found in the permuted datasets (Figure 4b). This correction makes these numbers easier to interpret, as the null hypothesis predicts a value of zero for each, rather than some small positive number as in the previous version of our analysis. The outcome value regressor reaches a peak value of 0.82, while the choice and chosen value regressors reach peak values of 0.21 and 0.29.</p><disp-quote content-type="editor-comment"><p>For the causal manipulation, I think a clearer case can be made about its novelty. The issue is a series of papers by Gardner and Schoenbaum using causal/optogenetic approaches to demonstrate the relevance of OFC to learning and learning-like processes. The authors here say that &quot;Our results build on this by pinpointing, out of all those possibilities, a specific computational role for OFC: it provides outcome value estimates to a learning system, elsewhere in the brain, that updates the estimates of these values.&quot; However, again, this claim is not as strong as it seems – there is no reason given to think that OFC only has one role or that its role in this task is the only role that it plays in cognition more generally. Reading this paper along with the Gardner papers, I would say that the authors' conclusion is wrong in that they do not reject alternative roles for OFC other than choice.</p></disp-quote><p>We agree that the Gardner and Schoenbaum papers demonstrate a role for the OFC in updating choice mechanisms. We believe that our data are consistent with this view, but add something more specific. We have revised our Discussion section to be more clear about this. The relevant passage now reads:</p><p>“[The Gardner and Schoenbaum] results indicate that the OFC activity is crucial specifically at times when a behavioral strategy is being updated. They leave open the question of what computational role this activity plays. They are consistent with several possible roles, including identifying when the strategy should be updated, driving the update of that strategy directly, or driving decisions between recently-updated alternatives. Our analysis of the coarse effects of inactivation in different time windows replicates this result. Our analysis of the fine-grained effects of inactivation builds on it, using a cognitive model to jointly interpret neural recordings and trial-by-trial behavioral effects of inactivation. They indicate that the detailed effects of inactivation are attributable in particular to degradation of the outcome value signal (and not, for example, the reward signal, which is stronger and carried by a larger fraction of clusters). This suggests that, at least in our task, the OFC plays a particular computational role in supporting update: it provides outcome value estimates to a learning system, elsewhere in the brain, that updates the behavioral policy.”</p></body></sub-article></article>