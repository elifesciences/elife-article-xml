<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">92595</article-id><article-id pub-id-type="doi">10.7554/eLife.92595</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92595.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Signatures of Bayesian inference emerge from energy-efficient synapses</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Malkin</surname><given-names>James</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-7473-1442</contrib-id><email>james.malkin@bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>O'Donnell</surname><given-names>Cian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2031-9177</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Houghton</surname><given-names>Conor J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5017-9473</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Aitchison</surname><given-names>Laurence</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>Faculty of Engineering, University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yp9g959</institution-id><institution>Intelligent Systems Research Centre, School of Computing, Engineering, and Intelligent Systems, Ulster University</institution></institution-wrap><addr-line><named-content content-type="city">Derry/Londonderry</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>08</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP92595</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-26"><day>26</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-06"><day>06</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2309.03194"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-12"><day>12</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92595.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-04"><day>04</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92595.2"/></event></pub-history><permissions><copyright-statement>© 2023, Malkin et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Malkin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-92595-v2.pdf"/><abstract><p>Biological synaptic transmission is unreliable, and this unreliability likely degrades neural circuit performance. While there are biophysical mechanisms that can increase reliability, for instance by increasing vesicle release probability, these mechanisms cost energy. We examined four such mechanisms along with the associated scaling of the energetic costs. We then embedded these energetic costs for reliability in artificial neural networks (ANNs) with trainable stochastic synapses, and trained these networks on standard image classification tasks. The resulting networks revealed a tradeoff between circuit performance and the energetic cost of synaptic reliability. Additionally, the optimised networks exhibited two testable predictions consistent with pre-existing experimental data. Specifically, synapses with lower variability tended to have (1) higher input firing rates and (2) lower learning rates. Surprisingly, these predictions also arise when synapse statistics are inferred through Bayesian inference. Indeed, we were able to find a formal, theoretical link between the performance-reliability cost tradeoff and Bayesian inference. This connection suggests two incompatible possibilities: evolution may have chanced upon a scheme for implementing Bayesian inference by optimising energy efficiency, or alternatively, energy-efficient synapses may display signatures of Bayesian inference without actually using Bayes to reason about uncertainty.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>Bayesian inference</kwd><kwd>energy efficiency</kwd><kwd>computational neuroscience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/T517872/1</award-id><principal-award-recipient><name><surname>Malkin</surname><given-names>James</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RPG-2019-229</award-id><principal-award-recipient><name><surname>O'Donnell</surname><given-names>Cian</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/W001845/1</award-id><principal-award-recipient><name><surname>O'Donnell</surname><given-names>Cian</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RF-2021-533</award-id><principal-award-recipient><name><surname>Houghton</surname><given-names>Conor J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The parallels between Bayesian synapses and energy-efficient synapses are highlighted.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The synapse is the major site of inter-cellular communication in the brain. The amplitude of synaptic postsynaptic potentials (PSPs) are usually highly variable or stochastic. This variability arises primarily presynaptically: the release of neurotransmitter from presynaptically housed vesicles into the synaptic cleft has variable release probabilities and variable quantal sizes (<xref ref-type="bibr" rid="bib54">Lisman and Harris, 1993</xref>; <xref ref-type="bibr" rid="bib12">Branco and Staras, 2009</xref>; <xref ref-type="bibr" rid="bib14">Brock et al., 2020</xref>). Unreliable synaptic transmission seems puzzling, especially in light of evidence for low-noise, almost failure-free transmission at some synapses (<xref ref-type="bibr" rid="bib63">Paulsen and Heggelund, 1994</xref>; <xref ref-type="bibr" rid="bib64">Paulsen and Heggelund, 1996</xref>; <xref ref-type="bibr" rid="bib6">Bellingham et al., 1998</xref>). Moreover, the degree to which a synapse is unreliable does not just vary from one synapse type to another, there is also an heterogeneity of precision amongst synapses of the same type (<xref ref-type="bibr" rid="bib61">Murthy et al., 1997</xref>; <xref ref-type="bibr" rid="bib18">Dobrunz and Stevens, 1997</xref>). Given that there is capacity for more precise transmission, why is this capacity not used in more synapses?</p><p>Unreliable transmission degrades accuracy but <xref ref-type="bibr" rid="bib50">Laughlin et al., 1998</xref>, showed that the synaptic connection from a photoreceptor to a retinal large monopolar cell could increase its precision by increasing the number of synapses, averaging the noise away, but this comes at the cost of extra energy per bit of information transmitted. Moreover, <xref ref-type="bibr" rid="bib52">Levy and Baxter, 2002</xref>, demonstrated that there is a value for the precision which optimises the energy cost of information transmission. In this paper, we explore this notion of a performance-energy tradeoff.</p><p>However, it is important to consider precision and energy cost in the context of neuronal computation; the brain does not simply transfer information from neuron to neuron, it performs computation through the interaction between neurons. However, models outlining a synaptic energy-performance tradeoff, (<xref ref-type="bibr" rid="bib50">Laughlin et al., 1998</xref>; <xref ref-type="bibr" rid="bib52">Levy and Baxter, 2002</xref>; <xref ref-type="bibr" rid="bib27">Goldman, 2004</xref>; <xref ref-type="bibr" rid="bib31">Harris et al., 2012</xref>; <xref ref-type="bibr" rid="bib32">Harris et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Karbowski, 2019</xref>), predominantly consider information transmission between just two neurons and the corresponding information-theoretic view treats the synapse as an isolated conduit of information (<xref ref-type="bibr" rid="bib74">Shannon, 1948</xref>). In contrast, in reality, a single synapse is just one unit of the computational machinery of the brain. As such, the performance of an individual synapse needs to be considered in the context of circuit performance. To perform computation in an energy-efficient way the circuit as a whole needs to allocate resources across different synapses to optimise the overall energy cost of computation (<xref ref-type="bibr" rid="bib83">Yu et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Schug et al., 2021</xref>).</p><p>Here, we consider the consequences of a tradeoff between network performance and energetic reliability costs that depend explicitly upon synapse precision. We estimate the energy costs associated with precision by considering the biological mechanisms underpinning synaptic transmission. By including these costs in a neural network designed to perform a classification task, we observe a heterogeneity in synaptic precision and find that this ‘allocation’ of precision is related to signatures of synapse ‘importance’, which can be understood formally on the grounds of Bayesian inference.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We proposed energetic costs for reliable synaptic transmission and then measured their consequences in an artificial neural network (ANN).</p><sec id="s2-1"><title>Biophysical costs</title><p>Here, we seek to understand the biophysical energetic costs of synaptic transmission, and how those costs relate to the reliability of transmission (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We start by considering the underlying mechanisms of synaptic transmission. In particular, synaptic transmission begins with the arrival of a spike at the axon terminal. This triggers a large influx of calcium ions into the axon terminal. The increase in calcium concentration causes the release of neurotransmitter-filled vesicles docked at axonal release sites. The neurotransmitter diffuses across the synaptic cleft to the postsynaptic dendritic membrane. There, the neurotransmitter binds with ligand-gated ion channels causing a change in voltage, i.e., a PSP. This process is often quantified using the <xref ref-type="bibr" rid="bib41">Katz and Miledi, 1965</xref>, quantal model of neurotransmitter release. Under this model, for each connection between two cells, there are <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> docked, readily releasable vesicles (see <xref ref-type="fig" rid="fig1">Figure 1a</xref> for an illustration of a single synaptic connection with multi-vesicular release).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Physiological reliability costs.</title><p>(<bold>a</bold>) Physiological processes that influence postsynaptic potential (PSP) precision. (<bold>b</bold>) A binomial model of vesicle release. For fixed PSP mean, increasing <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> decreases PSP variance. We have substituted <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi><mml:mo>∝</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> to reflect that vesicle volume scales quantal size (<xref ref-type="bibr" rid="bib40">Karunanithi et al., 2002</xref>). (<bold>c</bold>) Four different biophysical costs of reliable synaptic transmission. (I) Calcium pumps reverse the calcium influx that triggers vesicle release. A high probability of vesicle release requires a large influx of calcium, and extruding this calcium is costly. Note that <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> represents the vesicle radius. (II) An equivalent volume of neurotransmitter can be stored in few large vesicles or shared between many smaller vesicles. Sharing a fixed volume of neurotransmitter among many small vesicles reduces PSP variability but increases vesicle surface area, creating greater demand for phospholipid metabolism and hence greater energetic costs. (III) Actin filament supports the structure of vesicle clusters at the terminal. Many and large vesicles require more actin and higher rates of ATP-dependent actin turnover. (IV) There are biophysical costs that scale with the number of vesicles (<xref ref-type="bibr" rid="bib50">Laughlin et al., 1998</xref>; <xref ref-type="bibr" rid="bib4">Attwell and Laughlin, 2001</xref>), e.g., vesicle trafficking driven by myosin-V active transport along actin filaments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig1-v2.tif"/></fig><p>An alternative interpretation of this model might consider <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> the number of uni-vesicular connections between two neurons. When the presynaptic cell spikes, each docked vesicle releases with probability <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and each released vesicle causes a PSP of size <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula>. Thus, the mean, <italic>μ</italic>, and variance, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, of the PSP can be written (see <xref ref-type="fig" rid="fig1">Figure 1b</xref>),<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> is considered a scaling variable. An assertion in our model is that variability in PSP strength is the result of variable numbers of vesicle release, not variability in <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula>; here, during any PSP, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> is assumed constant across vesicles. While there is some suggestion that intra- and inter-site variability in <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> is a significant component of PSP variability (see <xref ref-type="bibr" rid="bib76">Silver, 2003</xref>), we ultimately expect quantal variability to be small relative to the variability attributed to vesicular release. This is supported by the classic observation that PSP amplitude histograms have a multi-peak structure (<xref ref-type="bibr" rid="bib11">Boyd and Martin, 1956</xref>; <xref ref-type="bibr" rid="bib35">Holler et al., 2021</xref>), and by more direct measurement and modelling of vesicle release (<xref ref-type="bibr" rid="bib24">Forti et al., 1997</xref>; <xref ref-type="bibr" rid="bib67">Raghavachari and Lisman, 2004</xref>).</p><p>We considered four biophysical costs associated with improving the reliability of synaptic transmission, while keeping the mean fixed, and derived the associated scaling of the energetic cost with PSP variance.</p><p><italic>Calcium efflux</italic>. Reliability is higher when the probability of vesicle release, <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, is higher. As vesicle release is triggered by an increase in intracellular calcium, greater calcium concentration implies higher release probability. However, increased calcium concentration implies higher energetic costs. In particular, calcium that enters the synaptic bouton will subsequently need to be pumped out. We take the cost of pumping out calcium ions to be proportional to the calcium concentration, and take the relationship between release probability and calcium concentration to be governed by a Hill Equation, following <xref ref-type="bibr" rid="bib72">Sakaba and Neher, 2001</xref>. The resulting relationship between energetic costs and reliability is <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>cost</mml:mtext><mml:mo>∝</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1cI</xref>; see Appendix 1, ‘Reliability costs’ for further details).</p><p><italic>Vesicle membrane surface area</italic>. There may also be energetic costs associated with producing and maintaining a large amount of vesicle membrane. <xref ref-type="bibr" rid="bib66">Purdon et al., 2002</xref>, argues that phospholipid metabolism may take a considerable proportion of the brain’s energy budget. Additionally, costs associated with membrane surface area may arise because of leakage of hydrogen ions across vesicles (<xref ref-type="bibr" rid="bib65">Pulido and Ryan, 2021</xref>). Importantly, a cost for vesicle surface area is implicitly a cost on reliability. In particular, we could obtain highly reliable synaptic release by releasing many small vesicles, such that stochasticity in individual vesicle release events averages out. However, the resulting many small vesicles have a far larger surface area than a single large vesicle, with the same mean PSP. Thus, a cost on surface area implies a relationship between energetic costs and reliability; in particular <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>cost</mml:mtext><mml:mo>∝</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1cII</xref>; see Appendix 1, ‘Reliability costs’ for further details).</p><p><italic>Actin</italic>. Another cost for small but numerous vesicles arises from a demand for structural organisation of the vesicles pool by filaments such as actin (<xref ref-type="bibr" rid="bib16">Cingolani and Goda, 2008</xref>; <xref ref-type="bibr" rid="bib26">Gentile et al., 2022</xref>). Critically, there are physical limits to the number of vesicles that can be attached to an actin filament of a given length. In particular, if vesicles are smaller we can attach more vesicles to a given length of actin, but at the same time, the total vesicle volume (and hence the total quantity of neurotransmitter) will be smaller (<xref ref-type="fig" rid="fig1">Figure 1cIII</xref>). A fixed cost per unit length of actin thus implies a relationship between energetic costs and reliability of, <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>cost</mml:mtext><mml:mo>∝</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>4</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (see Appendix 1, ‘Reliability costs’).</p><p><italic>Trafficking</italic>. A final class of costs is proportional to the number of vesicles (<xref ref-type="bibr" rid="bib50">Laughlin et al., 1998</xref>). One potential biophysical mechanism by which such a cost might emerge is from active transport of vesicles along actin filaments or microtubles to release sites (<xref ref-type="bibr" rid="bib15">Chenouard et al., 2020</xref>). In particular, vesicles are transported by ATP-dependent myosin-V motors (<xref ref-type="bibr" rid="bib13">Bridgman, 1999</xref>), so more vesicles require a greater energetic cost for trafficking. Any such cost proportional to the number of vesicles gives rise to a relationship between energetic cost and PSP variance of the form, <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>cost</mml:mtext><mml:mo>∝</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1cIV</xref>; see Appendix 1, ‘Reliability costs’).</p><p><italic>Costs related to PSP mean/magnitude</italic>. While costs on precision are the central focus of this paper, it is certainly the case that other costs relating to the mean PSP magnitude constitute a major cost of synaptic transmission. For example, high amplitude PSPs require a large quantity of neurotransmitter, high probability of vesicle release, and a large number of postsynaptic receptors (<xref ref-type="bibr" rid="bib4">Attwell and Laughlin, 2001</xref>). These can be formalised as costs on the PSP mean, <italic>μ</italic> , and can additionally be related to L1 weight decay in a machine learning context (<xref ref-type="bibr" rid="bib69">Rosset and Zhu, 2006</xref>; <xref ref-type="bibr" rid="bib70">Sacramento et al., 2015</xref>).</p></sec><sec id="s2-2"><title>Reliability costs in ANNs</title><p>Next, we sought to understand how these biophysical energetic costs of reliability might give rise to patterns of variability in a trained neural network. Specifically, we trained ANNs using an objective that embodied a tradeoff between performance and reliability costs,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mtext>performance</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mtext>magnitude</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mtext>reliability</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The ‘performance cost’ term measures the network’s performance on the task, for instance in our classification tasks we used the usual cross-entropy cost. The ‘magnitude cost’ term captures costs that depend on the PSP mean, while the ‘reliability cost’ term captures costs that depend on the PSP precision. In particular,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> indexes synapses, and recall that <italic>σ</italic><sub><italic>i</italic></sub> is the standard deviation of the <italic>i</italic>th synapse. The multiplier <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> in the reliability cost determines the strength of the reliability cost relative to the performance cost. Small values for <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> imply that the reliability cost term is less important, permitting precise transmission and higher performance. Large values for <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> give greater importance to the reliability cost encouraging energy efficiency by allowing higher levels of synaptic noise, causing detriment to performance (see <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Accuracy and postsynaptic potential (PSP) variance as we change the tradeoff between reliability and performance costs.</title><p>We changed the tradeoff by modifying <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, which multiplies the reliability cost. (<bold>a</bold>) As the reliability cost multiplier, <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, increases, the accuracy decreases considerably. The green lines show the heterogeneous noise setting where the noise level is optimised on a per-synapse basis, while the grey lines show the homogeneous noise setting, where the noise is optimised, but forced to be the same for all synapses. (<bold>b</bold>) When the reliability cost multiplier, <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, increases, the synaptic noise level (specifically, the average standard deviation, <italic>σ</italic>) increases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig2-v2.tif"/></fig><p>We trained fully connected, rate-based neural network to classify MNIST digits. Stochastic synaptic PSPs were sampled from a Normal distribution,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Normal</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where, recall, <italic>μ</italic><sub><italic>i</italic></sub> is the PSP mean and <italic>σ</italic><sub><italic>i</italic></sub><sup>2</sup> is the PSP variance for the <italic>i</italic>th synapse. The output firing rate was given by<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> can be understood as the somatic membrane potential, and <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> represents the relationship between somatic membrane potential and firing rate; we used ReLU (<xref ref-type="bibr" rid="bib25">Fukushima, 1975</xref>). We optimised network parameters <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> using Adam (<xref ref-type="bibr" rid="bib43">Kingma and Ba, 2014</xref>) (see Materials and methods for details on architecture and hyperparameters).</p></sec><sec id="s2-3"><title>The tradeoff between accuracy and reliability costs in trained networks</title><p>Next we sought to understand how the tradeoff between accuracy and reliability cost manifests in trained networks. Perhaps the critical parameter in the objective (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) was <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, which controlled the importance of the reliability cost relative to the performance cost. We trained networks with a variety of different values of <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, and with four values for <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> motivated by the biophysical costs (the different columns).</p><p>In practice all the reliability costs and others we may have overlooked should together constitute an overall energetic reliability cost. However, it is difficult to estimate the specific contributions of different costs, i.e., the individual values of <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>. While <xref ref-type="bibr" rid="bib4">Attwell and Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib20">Engl and Attwell, 2015</xref>, estimate the ATP demands for various synaptic processes, it is difficult to relate these to the relative scale of each cost at a synapse level. Therefore, for simplicity, we kept each cost separate, training neural networks with just one choice of reliability cost; emphasising results shared across all costs. It is possible that one cost dominates all the others, but if that is not the case it will be necessary to use a more complicated reliability cost. However, since we have considered four costs with very different power-law behaviours, it is likely the behaviour will not be significantly different to what we have observed.</p><p>As expected, we found that as <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> increased, performance fell (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) and the average synaptic standard deviation increased (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Importantly, we considered two different settings. First, we considered an homogeneous noise setting, where <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is optimised but kept the same across all synapses (grey lines). Second, we considered an heterogeneous noise setting, where <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is allowed to vary across synapses, and is optimised on a per-synapse basis. We found that heterogeneous noise (i.e. allowing the noise to vary on a per-synapse basis) improved accuracy considerably for a fixed value of <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, but only reduced the average noise slightly.</p><p>The findings in <xref ref-type="fig" rid="fig2">Figure 2</xref> imply a tradeoff between accuracy and average noise level, <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula>, as we change <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>. If we explicitly plot the accuracy against the noise level using the data from <xref ref-type="fig" rid="fig2">Figure 2</xref>, we see that as the synaptic noise level increases, the accuracy decreases (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Further, the synaptic noise level is associated with a reliability cost (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), and this relationship changes in the different columns as they use different values of <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> associated with different biological mechanisms that might give rise to the dominant biophysical reliability cost. Thus, there is also a relationship between accuracy and reliability costs (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), with accuracy increasing as we allow the system to invest more energy in becoming more reliable, which implies a higher reliability cost. Again, we plotted both the homogeneous (grey lines) and heterogeneous noise cases (green lines). We found that heterogeneous noise allowed for considerably improved accuracy at a given average noise standard deviation or a given reliability cost.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The performance-reliability cost tradeoff in artificial neural network (ANN) simulations.</title><p>(<bold>a</bold>) Accuracy decreases as the average postsynaptic potential (PSP) standard deviation, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula>, increases. The grey lines are for the homogeneous noise setting where the PSP variance is optimised but isotropic (i.e. the same across all synapses), while the green lines are for the heterogeneous noise setting, where the PSP variances are optimised individually on a per-synapse basis. (<bold>b</bold>) Increasing reliability by reducing <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> leads to greater reliability costs, and this relationship is different for different biophysical mechanisms and hence values for <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> (columns). (<bold>c</bold>) Higher accuracy therefore implies larger reliability cost.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig3-v2.tif"/></fig></sec><sec id="s2-4"><title>Energy-efficient patterns of synapse variability</title><p>We found that the heterogeneous noise setting, where we individually optimise synaptic noise on a per-synapse basis, performed considerably better than the homogeneous noise setting (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This raised an important question: how does the network achieve such large improvements by optimising the noise levels on a per-synapse basis? We hypothesised that the system invests a lot of energy in improving the reliability for ‘important’ synapses, i.e., synapses whose weights have a large impact on predictions and accuracy (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). Conversely, the system allows unimportant synapses to have high variability, which reduces reliability costs (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). To get further intuition, we compared both <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> on the same plot (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). Specifically, we put the important synapse, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from <xref ref-type="fig" rid="fig4">Figure 4a</xref>, on the horizontal axis, and the unimportant synapse, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from <xref ref-type="fig" rid="fig4">Figure 4b</xref>, on the vertical axis. In <xref ref-type="fig" rid="fig4">Figure 4c</xref>, the relative importance of the synapse is now depicted by how the cost increases as we move away from the optimal value of the weight. Specifically, the cost increases rapidly as we move away from the optimal value of <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, but increases much more slowly as we move away from the optimal value of <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Now, consider deviations in the synaptic weight driven by homogeneous synaptic variability (<xref ref-type="fig" rid="fig4">Figure 4c</xref> left, grey points). Many of these points have poor performance (i.e. a high performance cost), due to relatively high noise on the important synapse (i.e. <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>). Next, consider deviations in the synaptic weight driven by heterogeneous, optimised variability (<xref ref-type="fig" rid="fig4">Figure 4c</xref> left, green points). Critically, optimising synaptic noise reduces variability for the important synapse, and that reduces the average performance cost by eliminating large deviations on the important synapse. Thus, for the same overall reliability cost, heterogeneous, optimised variability can achieve much lower performance costs, and hence much lower overall costs than homogeneous variability (<xref ref-type="fig" rid="fig4">Figure 4d</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Schematic depiction of the impact of synaptic noise on synapses with different importance.</title><p>(<bold>a</bold>) First, we considered an important synapse for which small deviations in the weight, <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, e.g., driven by noise, imply a large increase in the performance cost. This can be understood as a high curvature of the performance cost as a function of <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. (<bold>b</bold>) Next we considered an unimportant synapse, for which deviations in the weights cause far less increase in performance cost. (<bold>c</bold>) A comparison of the impacts of homogeneous and optimised heterogeneous variability for synapses <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from (<bold>a and b</bold>). The performance cost is depicted using the purple contours, and realisations of the postsynaptic potentials (PSPs) driven by synaptic variability are depicted in the grey/green points. The grey points (left) depict homogeneous noise while the green points (right) depict optimised, heterogeneous noise. (<bold>d</bold>) The noise distributions in panel c are chosen to keep the same reliability cost (diagonally hatched area); but the homogeneous noise setting has far a higher performance cost, primarily driven by larger noise in the important synapse, <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig4-v2.tif"/></fig><p>To investigate experimental predictions arising from optimised, heterogeneous variability, we needed a way to formally assess the ‘importance’ of synapses. We used the ‘curvature’ of the performance cost: namely the degree to which small deviations in the weights from their optimal values will degrade performance. If the curvature is large (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), then small deviations in the weights, e.g., those caused by noise, can drastically reduce performance. In contrast, if the curvature is smaller (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), then small deviations in the weights cause a much smaller reduction in performance. As a formal measure of the curvature of the objective, we used the Hessian matrix, <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This describes the shape of the objective as a function of the synaptic weights, the <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>: specifically, it is the matrix of second derivatives of the objective, with respect to the weights, and measures the local curvature of objective. We were interested in the diagonal elements, <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>; the second derivatives of the objective with respect to <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p><p>We began by looking at how the optimised synaptic noise varied with synapse importance, as measured by the curvature or, more formally, the Hessian (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The Hessian values were estimated using the average-squared gradient, see Appendix 3, ‘Synapse importance and gradient magnitudes’. We found that as the importance of the synapse increased, the optimised noise level decreased. These patterns of synapse variability make sense because noise is more detrimental at important synapses and so it is worth investing energy to reduce the noise in those synapses.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The heterogeneous patterns of synapse variability in artificial neural networks (ANNs) optimised by the tradeoff.</title><p>We present data patterns on logarithmic axis between signatures of synapse importance and variability for 10,000 (100 neuron units, each with 100 synapses) synapses that connect two hidden layers in our ANN. (<bold>a</bold>) Synapses whose corresponding diagonal entry in the Hessian is large have smaller variance. (<bold>b</bold>) Synapses with high variance have faster learning rates. (<bold>c</bold>) As input firing rate increases, synapse variance decreases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig5-v2.tif"/></fig><p>However, this relationship (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) between the importance of a synapse and the synaptic variability is not experimentally testable, as we are not able to directly measure synapse importance. That said, we are able to obtain two testable predictions. First, the input rate in our simulations was negatively correlated with optimised synaptic variability (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). Second, the optimised synaptic variability was larger for synapses with larger learning rates (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). Critically, similar patterns have been observed in experimental data. In <xref ref-type="fig" rid="fig6">Figure 6a</xref> we present the negative correlation between learning rate and synaptic reliability presented by <xref ref-type="bibr" rid="bib73">Schug et al., 2021</xref>, from in vitro measurements of V1 (layer 5) pyramidal synapses before and after STDP-induced long-term plasticity (LTP) conducted by <xref ref-type="bibr" rid="bib77">Sjöström et al., 2001</xref>. Furthermore, a relationship between input firing rate and synaptic variability was observed by <xref ref-type="bibr" rid="bib3">Aitchison et al., 2021</xref>, using in vivo functional recordings from V1 (layer 2/3) (<xref ref-type="bibr" rid="bib46">Ko et al., 2013</xref>; <xref ref-type="fig" rid="fig6">Figure 6b</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Experimental signatures of Bayesian synapses.</title><p>The Bayesian synapse hypothesis predicts relationships between synapse reliability, learning rate, and input rate. (<bold>a</bold>) Synapses with higher probability of release, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, demonstrate smaller increases in synaptic mean following long-term plasticity (LTP) induction. This pattern was originally observed by <xref ref-type="bibr" rid="bib73">Schug et al., 2021</xref>. (<bold>b</bold>) As input firing rates are increased, normalised EPSP variability decreases with <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.62</mml:mn></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib3">Aitchison et al., 2021</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig6-v2.tif"/></fig><p>To understand why these patterns of variability emerge in our simulations and in data, we need to understand the connection between synapse importance, synaptic inputs (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, <xref ref-type="fig" rid="fig6">Figure 6b</xref>), and synaptic learning (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, <xref ref-type="fig" rid="fig6">Figure 6a</xref>). Perhaps the easiest connection is between the synapse importance and the input firing rate. If the input cell never fires, then the synaptic weight cannot affect the network output, and the synapse has zero importance (and also zero Hessian; see Appendix 2, ‘High input rates and high precision at important synapses’). This would suggest a tendency for synapses with higher input firing rates to be more important, and hence to have lower variability. This pattern is indeed borne out in our simulations (<xref ref-type="fig" rid="fig5">Figure 5b</xref>; also see <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1</xref>), though of course there is a considerable amount of noise: there are a few important synapses with low input rates, and vice versa.</p><p>Next, we consider the connection between learning rate and synapse importance. To understand this connection, we need to choose a specific scheme for modulating the learning rate as a function of the inputs. While the specific scheme for modulating the learning rate is ultimately an assumption, we believe modern deep learning offers strong guidance as to the optimal family of schemes for modulating the learning rate. In particular, modern, state-of-the-art, update rules for ANNs almost always use an adaptive learning rate. These adaptive learning rates, <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> (including the most common such as Adam and variants), almost always use a normalising learning rate which decreases in response to high incoming gradients,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>η</mml:mi><mml:mtext>base</mml:mtext></mml:msub><mml:msqrt><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Specifically, the local learning rate for the <italic>i</italic>th synapse, <italic>η</italic><sub><italic>i</italic></sub>, is usually a base learning rate, <italic>η</italic><sub>base</sub>, divided by the root-mean-squared gradient at this synapse <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msqrt><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:msqrt></mml:mstyle></mml:math></inline-formula>. Critically, the root-mean-squared gradient turns out to be strongly related to synapse importance. Intuitively, important synapses with greater impact on network predictions will have larger gradients (see Appendix 3, ‘Synapse importance and gradient magnitudes’).</p><p>In vivo performance requires selective formation, stabilisation, and elimination of LTP (<xref ref-type="bibr" rid="bib81">Yang et al., 2009</xref>), raising the questions as to which biological mechanisms are able to provide this selectivity. Reducing updates at historically important synapses is one potential approach to determining which synapses should have their strengths adjusted and which should be stabilised. Adjusting learning rates based on synapse importance enables fast, stable learning (<xref ref-type="bibr" rid="bib51">LeCun et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Kingma and Ba, 2014</xref>; <xref ref-type="bibr" rid="bib42">Khan et al., 2018</xref>; <xref ref-type="bibr" rid="bib2">Aitchison, 2020</xref>; <xref ref-type="bibr" rid="bib59">Martens, 2020</xref>; <xref ref-type="bibr" rid="bib37">Jegminat et al., 2022</xref>).</p><p>For our purposes, the crucial point is that when training using an adaptive learning rate such as <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, important synapses have higher root-mean-squared gradients, and hence lower learning rates. Here, we use a specific set of update rules which uses this adaptive learning rate (i.e. Adam: <xref ref-type="bibr" rid="bib43">Kingma and Ba, 2014</xref>; <xref ref-type="bibr" rid="bib82">Yang and Li, 2022</xref>). Thus, we can use learning rate as a proxy for importance, allowing us to obtain the predictions tested in <xref ref-type="fig" rid="fig5">Figure 5b</xref> which match <xref ref-type="fig" rid="fig5">Figure 5a/c</xref>.</p></sec><sec id="s2-5"><title>The connection to Bayesian inference</title><p>Surprisingly, our experimental predictions obtained for optimised, heterogeneous synaptic variability (<xref ref-type="fig" rid="fig5">Figure 5</xref>) match those arising from Bayesian synapses presented in <xref ref-type="fig" rid="fig6">Figure 6</xref> (i.e. synapses that use Bayes to infer their weights; <xref ref-type="bibr" rid="bib3">Aitchison et al., 2021</xref>). Our first prediction was that lower variability implies a lower learning rate. The same prediction also arises if we consider Bayesian synapses. In particular, if variability and hence uncertainty is low, then a Bayesian synapse is very certain that it is close to the optimal value. In that case, new information should have less impact on the synaptic weight, and the learning rate should be lower. Our second prediction was that higher presynaptic firing rates imply less variability. Again, this arises in Bayesian synapses: Bayesian synapses should become more certain and less variable if the presynaptic cell fires more frequently. Every time the presynaptic cell fires, the synapse gets a feedback signal which gives a small amount of information about the right value for that synaptic weight. So the more times the presynaptic cell fires, the more information the synapse receives, and the more certain it becomes.</p><p>This match between observations for our energy-efficient synapses and previous work on Bayesian synapses led us to investigate potential connections between energy efficiency and Bayesian inference. Intuitively, there turns out to be a strong connection between synapse importance and uncertainty. Specifically, if a synapse is very important, then the performance cost changes dramatically when there are errors in that synaptic weight. That synapse therefore receives large gradients, and hence strong information about the correct value, rapidly reducing uncertainty.</p><p>To assess the connection between Bayesian posteriors and energy-efficient variability in more depth, we estimated and plotted the posterior variance against the optimised synaptic variability (<xref ref-type="fig" rid="fig7">Figure 7a</xref>) (see Materials and methods). We considered our four different biophysical mechanisms (values for <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig7">Figure 7a</xref>, columns), and values for <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, rows). In all cases, there was a clear correlation between the posterior and the optimised variability: synapses with larger posterior variance also had large optimised variance. To further assess this connection, we used the relation between the Hessian and posterior variance given by <xref ref-type="disp-formula" rid="equ56">Equation 54c</xref> and the analytic result given in Appendix 5, ‘Analytic predictions for <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>’ to plot the relationships between <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and the posterior variability, <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>post</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, as a function of <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7b</xref>) and as a function of <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). Again, these plots show a clear correlation between synapse variance and posterior variance, though the relationship is far from perfect. For a perfect relationship, we would expect the lines in <xref ref-type="fig" rid="fig7">Figure 7b and c</xref> to all lie along the diagonal with slope equal to one. In contrast, these lines actually have a slope smaller than one, indicating that optimised variability is less heterogeneous than posterior variance (<xref ref-type="fig" rid="fig7">Figure 7b and c</xref>). Interestingly, the slope increases towards one as the associated <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> is decreased, this suggests that synapse variability best approximates the posterior when <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> is small.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>A comparison of optimised synaptic variability and posterior variance.</title><p>(<bold>a</bold>) Posterior variance (grey-dashed ellipses) plotted alongside optimised synaptic variability (green ellipses) for different values of <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> (columns) and <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> (rows) for an illustrative pair of synapses. Note that using fixed values of <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> for different <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>’s dramatically changed the scale of the ellipses. Instead, we chose <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> as a function of <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> to ensure that the scale of the optimised noise variance was roughly equal across different <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>. This allowed us to highlight the key pattern: that smaller values for <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> give optimised variance closer to the true posterior variances, while higher values for <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> tended to make the optimised synaptic variability more isotropic. (<bold>b</bold>) To understand this pattern more formally, we plotted the synaptic variability as a function of the posterior variance for different values of <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>. Note that we set <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2.0</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mi>ρ</mml:mi></mml:mfrac></mml:mstyle></mml:mstyle></mml:math></inline-formula> to avoid large additive offsets (see Connecting the entropy and the biological reliability cost – <xref ref-type="disp-formula" rid="equ48">Equation 48</xref> for details). (<bold>c</bold>) The synaptic variability as a function of the posterior variance for different values of <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mo>:</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>112</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.356</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (3 DP). As <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> increases (lighter blues) we penalise reliability more, and hence the optimised synaptic noise variability increases. (Here, we fixed <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> across different settings for <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>.)</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-fig7-v2.tif"/></fig><p>This strong, but not perfect, connection between the patterns of variability in Bayesian inference and energy-efficient networks motivated us to seek a formal connection between Bayesian and efficient synapses. As such, in the Appendix, we derive a theoretical connection between our overall performance cost and Bayesian inference (see Appendix 4, ‘Energy-efficient noise and variational Bayes for neural network weights’). Moreover, this connection is subsequently used to provide an explanation for why synapse variability aligns closer to posterior variance for small <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ51">Equation 51</xref>), specifically, variational inference, a well-known procedure for performing (approximate) Bayesian inference in NNs (<xref ref-type="bibr" rid="bib34">Hinton and van Camp, 1993</xref>; <xref ref-type="bibr" rid="bib29">Graves, 2011</xref>; <xref ref-type="bibr" rid="bib9">Blundell et al., 2015</xref>). Variational inference optimises the ‘evidence lower bound objective’ (ELBO) (<xref ref-type="bibr" rid="bib5">Barber and Bishop, 1998</xref>; <xref ref-type="bibr" rid="bib38">Jordan et al., 1999</xref>; <xref ref-type="bibr" rid="bib8">Blei et al., 2017</xref>), which surprisingly turns out to resemble our performance cost. Specifically, the ELBO includes a term which encourages the entropy of the approximating posterior distribution (which could be interpreted as our noise distribution) to be larger. This resembles a reliability cost, as our reliability costs also encourage the noise distribution to be larger. Critically, the biological power-law reliability cost has a different form from the ideal, entropic reliability cost. However, we are able to derive a formal relationship: the biological power-law reliability costs bound the ideal entropic reliability cost. Remarkably, this implies that our overall cost (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) bounds the ELBO, so reducing our cost (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) tightens the ELBO bound and gives an improved guarantee on the quality of Bayesian inference.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Comparing the brain’s computational roles with associated energetic costs provides a useful means for deducing properties of efficient neurophysiology. Here, we applied this approach to PSP variability. We began by looking at the biophysical mechanisms of synaptic transmission, and how the energy costs for transmission might vary with synaptic reliability. We modified a standard ANN to incorporate unreliable synapses and trained this on a classification task using an objective that combined classification accuracy and an energetic cost on reliability. This led to a performance-reliability cost tradeoff and heterogeneous patterns of synapse variability that correlated with input rate and learning rate. We noted that these patterns of variability have been previously observed in data (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). Remarkably, these are also the patterns of variability predicted by Bayesian synapses (<xref ref-type="bibr" rid="bib3">Aitchison et al., 2021</xref>) (i.e. when distributions over synaptic weights correspond with the Bayesian posterior). Finally, we showed empirical and formal connections between the synaptic variability implied by Bayesian synapses and our performance-reliability cost tradeoff.</p><p>The reliability cost in terms of the synaptic variability (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) is a critical component of the numerical experiments we present here. While the precise form of the cost is inevitably uncertain, we attempted to mitigate the uncertainty by considering a wide range of functional forms for the reliability cost. In particular, we considered four biophysical mechanisms, corresponding to four power-law exponents, (<inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>4</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>). Moreover, these different power-law costs already cover a reasonably wide-range of potential penalties and we would expect the results to hold for many other forms of reliability cost as the intuition behind the results ultimately relies merely on there being <italic>some</italic> penalty for increasing reliability.</p><p>The biophysical cost also includes a multiplicative factor, <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, which sets the magnitude of the reliability cost. In fact, the patterns of variability exhibited in <xref ref-type="fig" rid="fig5">Figure 5</xref> are preserved as <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> is changed: this was demonstrated for values of <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> which are 10 times larger and 10 times smaller, <xref ref-type="fig" rid="app6fig2">Appendix 6—figure 2</xref>. This multiplicative factor should be understood as being determined by the properties of the physics and chemistry underpinning synaptic dynamics, e.g., it could represent the quantity of ATP required by the metabolic costs of synaptic transmission (although this factor could vary, e.g. in different cell types).</p><p>Our ANNs used backpropagation to optimise the mean and variance of synaptic weights. While there are a number of schemes by which biological circuits might implement backpropagation (<xref ref-type="bibr" rid="bib80">Whittington and Bogacz, 2017</xref>; <xref ref-type="bibr" rid="bib71">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Richards and Lillicrap, 2019</xref>), it is not yet clear whether backpropagation is implemented by the brain (see <xref ref-type="bibr" rid="bib53">Lillicrap et al., 2020</xref>, for a review on the plausibility of propagation in the brain). Regardless, backpropagation is merely the route we used in our ANN setting to reach an energy-efficient configuration. The patterns we have observed are characteristic of an energy-efficient network and therefore should not depend on the learning rule that the brain uses to achieve energy efficiency.</p><p>Our results in ANNs used MNIST classification as an example of a task; this may appear somewhat artificial, but all brain areas ultimately do have a task: to maximise fitness (or reward as a proxy for fitness). Moreover, our results all ultimately arise from trading off biophysical reliability costs against the fact that if a synapse is important to performing a task, then variability in that synapse substantially impairs performance. Of course performance, in different brain areas, might mean reward, fitness, or some other measures. In contrast, if a synapse is unimportant, variability in that synapse impairs performance less. In all tasks there will be some synapses that are more, and some synapses that are less important, and our task, while relatively straightforward, captures this important property.</p><p>Our results have important implications for the understanding of Bayesian inference in synapses. In particular, we show that energy efficiency considerations give rise to two phenomena that are consistent with predictions outlined in previous work on Bayesian synapses (<xref ref-type="bibr" rid="bib3">Aitchison et al., 2021</xref>). First, that normalised variability decreases for synapses with higher presynaptic firing rates. Second, that synaptic plasticity is higher for synapses with higher variability.</p><p>Specifically, these findings suggest that synapses connect their uncertainty in the value of the optimal synaptic weight (see <xref ref-type="bibr" rid="bib3">Aitchison et al., 2021</xref>, for details) to variability. This is in essence a synaptic variant of the ‘sampling hypothesis’. Under the sampling hypothesis, neural activity is believed to represent a potential state of the world, and variability is believed to represent uncertainty (<xref ref-type="bibr" rid="bib36">Hoyer and Hyvärinen, 2002</xref>; <xref ref-type="bibr" rid="bib45">Knill and Pouget, 2004</xref>; <xref ref-type="bibr" rid="bib55">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib22">Fiser et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Berkes et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Orbán et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Aitchison and Lengyel, 2016</xref>; <xref ref-type="bibr" rid="bib30">Haefner et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Lange and Haefner, 2017</xref>; <xref ref-type="bibr" rid="bib75">Shivkumar et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Bondy et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Echeveste et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Festa et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Lange et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Lange and Haefner, 2022</xref>). This variability in neural activity, representing uncertainty in the state of the world, can then be read out by downstream circuits to inform behaviour. Here, we showed that a connection between synaptic uncertainty and variability can emerge simply as a consequence of maximising energy efficiency. This suggest that Bayesian synapses may emerge without any necessity for specific synaptic biophysical implementations of Bayesian inference.</p><p>Importantly though, while the brain might use synaptic noise for Bayesian computation, these results are also consistent with an alternative interpretation: that the brain is not Bayesian, it just looks Bayesian because it is energy efficient. To distinguish between these two interpretations, we ultimately need to know whether downstream brain areas exploit or ignore information about uncertainty that arises from synaptic variability.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>The ANN simulations were run in PyTorch with feedforward, fully connected neural networks with two hidden layers of width 100. The input dimension of 784 corresponded to the number of pixels in the greyscale MNIST images of handwritten digits, while the output dimension of 10 corresponded to the number of classes. We used the reparameterisation trick to backpropagate with respect to the mean and variance of the weights, in particular, we set <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>ξ</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ξ</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Normal</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib44">Kingma et al., 2015</xref>). MNIST classification was learned through optimisation of Gaussian parameters with respect to a cross-entropy loss in addition to reliability costs using minibatch gradient descent under Adam optimisation with a minibatch size of 20. To prevent negative values for the <italic>σs</italic>, they were reparameterised using a softplus function with argument <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>softplus</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The base learning rate in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> is <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>base</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. The <italic>μ</italic><sub><italic>i</italic></sub>s were initialised homogeneously across the network from <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>Uniform</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and the <italic>σ</italic><sub><italic>i</italic></sub>s were initialised homogeneously across the network at <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. Hyperparameters were chosen via grid search on the validation dataset to enable smooth learning, high performance, and rapid convergence. In the objective <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>BI</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> used to train our simulations, we also add an L1 regularisation term over synaptic weights, <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>μ</mml:mi><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.</p><p>Plots in <xref ref-type="fig" rid="fig2">Figure 2</xref> present mappings from hyperparameter, <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, to accuracy and <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula>. A different neural network was trained for each <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, after 50 training epochs the average <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> across synapses was computed, and accuracy was evaluated on the test dataset. Plots in <xref ref-type="fig" rid="fig3">Figure 3</xref> present mappings of this <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> against accuracy and reliability cost. The reliability cost was computed using fixed <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ48">Equation _48</xref>).</p><p>To compute the Hessian in <xref ref-type="fig" rid="fig5">Figure 5</xref> and elsewhere, we used the empirical Fisher information approximation (<xref ref-type="bibr" rid="bib23">Fisher, 1922</xref>), <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>H</mml:mi><mml:mo>≈</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. This was evaluated by taking the average <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>μ</mml:mi></mml:mstyle></mml:math></inline-formula> over 10 epochs after full training for 50 epochs. The average learning rate <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and the average input rate <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> were also evaluated over 10 epochs following training. The data presented illustrate these variables with regard to the weights of the second hidden layer. We set hyperparameter <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ47">Equation 47</xref>) in these simulations.</p><p>For geometric comparisons between the distribution over synapses and the Bayesian posterior presented in <xref ref-type="fig" rid="fig7">Figure 7</xref> we used the analytic results in Appendix 5, ‘Analytic predictions for <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>’. To estimate the posterior used in <xref ref-type="fig" rid="fig7">Figure 7</xref>, we optimised a factorised Gaussian approximation to the posterior over weights using variational inference and Bayes by backpropagation (<xref ref-type="bibr" rid="bib9">Blundell et al., 2015</xref>). We then took <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from two optimised weights. For the variance and slope comparisons between Bayesian and efficient synapses in <xref ref-type="fig" rid="fig7">Figure 7</xref>, we used the analytic results in Appendix 5, ‘Analytic predictions for <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>’.</p><p>Source code used in simulations is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/JamesMalkin/EfficientBayes">https://github.com/JamesMalkin/EfficientBayes</ext-link> copy archived at <xref ref-type="bibr" rid="bib58">Malkin, 2024</xref>.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Supervision, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Supervision, Visualization, Methodology, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-92595-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so any data generated is simulated data. The previously published dataset listed below and data from <xref ref-type="bibr" rid="bib46">Ko et al., 2013</xref> were used. All newly generated data is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/JamesMalkin/EfficientBayes">https://github.com/JamesMalkin/EfficientBayes</ext-link>, copy archived at <xref ref-type="bibr" rid="bib58">Malkin, 2024</xref>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>R</given-names></name><name><surname>Froemke</surname><given-names>R</given-names></name><name><surname>Sjöström</surname><given-names>P</given-names></name><name><surname>van Rossum</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Data from: Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.p286g</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Dr Stewart whose philanthropy supported GPU compute used in this project. JM was funded by the Engineering and Physical Sciences Research Council (EP/T517872/1). COD was funded by the Leverhulme Trust (RPG-2019-229) and Biotechnology and Biological Sciences Research Council (BB/W001845/1). CH is supported by the Leverhulme Trust (RF-2021-533).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The hamiltonian brain: efficient probabilistic inference with excitatory-inhibitory neural circuit dynamics</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005186</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005186</pub-id><pub-id pub-id-type="pmid">28027294</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>18173</fpage><lpage>18182</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Jegminat</surname><given-names>J</given-names></name><name><surname>Menendez</surname><given-names>JA</given-names></name><name><surname>Pfister</surname><given-names>J-P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Synaptic plasticity as Bayesian inference</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00809-5</pub-id><pub-id pub-id-type="pmid">33707754</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname><given-names>D</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>D</given-names></name><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Ensemble learning in Bayesian neural networks</article-title><source>Nato ASI Series F Computer and Systems Sciences</source><volume>168</volume><fpage>215</fpage><lpage>238</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellingham</surname><given-names>MC</given-names></name><name><surname>Lim</surname><given-names>R</given-names></name><name><surname>Walmsley</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Developmental changes in EPSC quantal size and quantal content at a central glutamatergic synapse in rat</article-title><source>The Journal of Physiology</source><volume>511 (Pt 3)</volume><fpage>861</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7793.1998.861bg.x</pub-id><pub-id pub-id-type="pmid">9714866</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title><source>Science</source><volume>331</volume><fpage>83</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.1195870</pub-id><pub-id pub-id-type="pmid">21212356</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Kucukelbir</surname><given-names>A</given-names></name><name><surname>McAuliffe</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variational inference: a review for statisticians</article-title><source>Journal of the American Statistical Association</source><volume>112</volume><fpage>859</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1080/01621459.2017.1285773</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Blundell</surname><given-names>D</given-names></name><name><surname>Cornebise</surname><given-names>J</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Weight uncertainty in neural network</article-title><conf-name>International conference on machine learning PMLR</conf-name><fpage>1613</fpage><lpage>1622</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bondy</surname><given-names>AG</given-names></name><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Cumming</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Feedback determines the structure of correlated variability in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>598</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0089-1</pub-id><pub-id pub-id-type="pmid">29483663</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>IA</given-names></name><name><surname>Martin</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>The end-plate potential in mammalian muscle</article-title><source>The Journal of Physiology</source><volume>132</volume><fpage>74</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1956.sp005503</pub-id><pub-id pub-id-type="pmid">13320373</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branco</surname><given-names>T</given-names></name><name><surname>Staras</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The probability of neurotransmitter release: variability and feedback control at single synapses</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>373</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1038/nrn2634</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bridgman</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Myosin Va movements in normal and dilute-lethal axons provide support for a dual filament motor complex</article-title><source>The Journal of Cell Biology</source><volume>146</volume><fpage>1045</fpage><lpage>1060</lpage><pub-id pub-id-type="doi">10.1083/jcb.146.5.1045</pub-id><pub-id pub-id-type="pmid">10477758</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brock</surname><given-names>JA</given-names></name><name><surname>Thomazeau</surname><given-names>A</given-names></name><name><surname>Watanabe</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>SSY</given-names></name><name><surname>Sjöström</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A practical guide to using CV analysis for determining the locus of synaptic plasticity</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>12</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2020.00011</pub-id><pub-id pub-id-type="pmid">32292337</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chenouard</surname><given-names>N</given-names></name><name><surname>Xuan</surname><given-names>F</given-names></name><name><surname>Tsien</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Synaptic vesicle traffic is supported by transient actin filaments and regulated by PKA and NO</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>5318</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-19120-1</pub-id><pub-id pub-id-type="pmid">33087709</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cingolani</surname><given-names>LA</given-names></name><name><surname>Goda</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Actin in action: the interplay between the actin cytoskeleton and synaptic efficacy</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>344</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1038/nrn2373</pub-id><pub-id pub-id-type="pmid">18425089</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>GW</given-names></name><name><surname>Müller</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Homeostatic control of presynaptic neurotransmitter release</article-title><source>Annual Review of Physiology</source><volume>77</volume><fpage>251</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1146/annurev-physiol-021014-071740</pub-id><pub-id pub-id-type="pmid">25386989</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobrunz</surname><given-names>LE</given-names></name><name><surname>Stevens</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Heterogeneity of release probability, facilitation, and depletion at central synapses</article-title><source>Neuron</source><volume>18</volume><fpage>995</fpage><lpage>1008</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80338-4</pub-id><pub-id pub-id-type="pmid">9208866</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Echeveste</surname><given-names>R</given-names></name><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1138</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0671-1</pub-id><pub-id pub-id-type="pmid">32778794</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engl</surname><given-names>E</given-names></name><name><surname>Attwell</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Non-signalling energy use in the brain</article-title><source>The Journal of Physiology</source><volume>593</volume><fpage>3417</fpage><lpage>3429</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2014.282517</pub-id><pub-id pub-id-type="pmid">25639777</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Aschner</surname><given-names>A</given-names></name><name><surname>Davila</surname><given-names>A</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Coen-Cagli</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuronal variability reflects probabilistic inference tuned to natural image statistics</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>3635</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23838-x</pub-id><pub-id pub-id-type="pmid">34131142</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.003</pub-id><pub-id pub-id-type="pmid">20153683</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1922">1922</year><article-title>On the mathematical foundations of theoretical statistics</article-title><source>Philosophical Transactions of the Royal Society of London</source><volume>222</volume><fpage>309</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1098/rsta.1922.0009</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forti</surname><given-names>L</given-names></name><name><surname>Bossi</surname><given-names>M</given-names></name><name><surname>Bergamaschi</surname><given-names>A</given-names></name><name><surname>Villa</surname><given-names>A</given-names></name><name><surname>Malgaroli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Loose-patch recordings of single quanta at individual hippocampal synapses</article-title><source>Nature</source><volume>388</volume><fpage>874</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1038/42251</pub-id><pub-id pub-id-type="pmid">9278048</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Cognitron: A self-organizing multilayered neural network</article-title><source>Biological Cybernetics</source><volume>20</volume><fpage>121</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1007/BF00342633</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentile</surname><given-names>JE</given-names></name><name><surname>Carrizales</surname><given-names>MG</given-names></name><name><surname>Koleske</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Control of synapse structure and function by actin and its regulators</article-title><source>Cells</source><volume>11</volume><elocation-id>603</elocation-id><pub-id pub-id-type="doi">10.3390/cells11040603</pub-id><pub-id pub-id-type="pmid">35203254</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Enhancement of information transmission efficiency by synaptic failures</article-title><source>Neural Computation</source><volume>16</volume><fpage>1137</fpage><lpage>1162</lpage><pub-id pub-id-type="doi">10.1162/089976604773717568</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramlich</surname><given-names>MW</given-names></name><name><surname>Klyachko</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Actin/Myosin-V- and activity-dependent inter-synaptic vesicle exchange in central neurons</article-title><source>Cell Reports</source><volume>18</volume><fpage>2096</fpage><lpage>2104</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.02.010</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Practical variational inference for neural networks</article-title><conf-name>Advances in Neural Information Processing Systems 24 (NIPS 2011)</conf-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual decision-making as probabilistic inference by neural sampling</article-title><source>Neuron</source><volume>90</volume><fpage>649</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.020</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JJ</given-names></name><name><surname>Jolivet</surname><given-names>R</given-names></name><name><surname>Attwell</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Synaptic energy use and supply</article-title><source>Neuron</source><volume>75</volume><fpage>762</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.019</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JJ</given-names></name><name><surname>Engl</surname><given-names>E</given-names></name><name><surname>Attwell</surname><given-names>D</given-names></name><name><surname>Jolivet</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Energy-efficient information transfer at thalamocortical synapses</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007226</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007226</pub-id><pub-id pub-id-type="pmid">31381555</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heidelberger</surname><given-names>R</given-names></name><name><surname>Heinemann</surname><given-names>C</given-names></name><name><surname>Neher</surname><given-names>E</given-names></name><name><surname>Matthews</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Calcium dependence of the rate of exocytosis in a synaptic terminal</article-title><source>Nature</source><volume>371</volume><fpage>513</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1038/371513a0</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>van Camp</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Keeping the neural networks simple by minimizing the description length of the weights</article-title><conf-name>Proceedings of COLT-93</conf-name><fpage>5</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1145/168304.168306</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holler</surname><given-names>S</given-names></name><name><surname>Köstinger</surname><given-names>G</given-names></name><name><surname>Martin</surname><given-names>KAC</given-names></name><name><surname>Schuhknecht</surname><given-names>GFP</given-names></name><name><surname>Stratford</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Structure and function of a neocortical synapse</article-title><source>Nature</source><volume>591</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03134-2</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hoyer</surname><given-names>P</given-names></name><name><surname>Hyvärinen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Interpreting neural response variability as Monte Carlo sampling of the posterior</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jegminat</surname><given-names>J</given-names></name><name><surname>Surace</surname><given-names>SC</given-names></name><name><surname>Pfister</surname><given-names>J-P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learning as filtering: Implications for spike-based plasticity</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009721</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009721</pub-id><pub-id pub-id-type="pmid">35196324</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Jaakkola</surname><given-names>TS</given-names></name><name><surname>Saul</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>An introduction to variational methods for graphical models</article-title><source>Machine Learning</source><volume>37</volume><fpage>183</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1023/A:1007665907178</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karbowski</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Metabolic constraints on synaptic learning and memory</article-title><source>Journal of Neurophysiology</source><volume>122</volume><fpage>1473</fpage><lpage>1490</lpage><pub-id pub-id-type="doi">10.1152/jn.00092.2019</pub-id><pub-id pub-id-type="pmid">31365284</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karunanithi</surname><given-names>S</given-names></name><name><surname>Marin</surname><given-names>L</given-names></name><name><surname>Wong</surname><given-names>K</given-names></name><name><surname>Atwood</surname><given-names>HL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Quantal size and variation determined by vesicle size in normal and mutant <italic>Drosophila</italic> glutamatergic synapses</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>10267</fpage><lpage>10276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-23-10267.2002</pub-id><pub-id pub-id-type="pmid">12451127</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>B</given-names></name><name><surname>Miledi</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>The measurement of synaptic delay, and the time course of acetylcholine release at the neuromuscular junction</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>161</volume><fpage>483</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1098/rspb.1965.0016</pub-id><pub-id pub-id-type="pmid">14278409</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>M</given-names></name><name><surname>Nielsen</surname><given-names>D</given-names></name><name><surname>Tangkaratt</surname><given-names>V</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Srivastava</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fast and scalable bayesian deep learning by weight-perturbation in adam</article-title><conf-name>International conference on machine learning PMLR</conf-name><fpage>2611</fpage><lpage>2620</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Salimans</surname><given-names>T</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variational dropout and the local reparameterization trick</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>TRENDS in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>H</given-names></name><name><surname>Cossell</surname><given-names>L</given-names></name><name><surname>Baragli</surname><given-names>C</given-names></name><name><surname>Antolik</surname><given-names>J</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The emergence of functional microcircuits in visual cortex</article-title><source>Nature</source><volume>496</volume><fpage>96</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/nature12015</pub-id><pub-id pub-id-type="pmid">23552948</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lange</surname><given-names>RD</given-names></name><name><surname>Haefner</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Characterizing and interpreting the influence of internal variables on sensory activity</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>84</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.07.006</pub-id><pub-id pub-id-type="pmid">28841439</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lange</surname><given-names>RD</given-names></name><name><surname>Chattoraj</surname><given-names>A</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Haefner</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A confirmation bias in perceptual decision-making due to hierarchical approximate inference</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009517</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009517</pub-id><pub-id pub-id-type="pmid">34843452</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lange</surname><given-names>RD</given-names></name><name><surname>Haefner</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Task-induced neural covariability as a signature of approximate Bayesian learning and inference</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009557</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009557</pub-id><pub-id pub-id-type="pmid">35259152</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Anderson</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The metabolic cost of neural information</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>36</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1038/236</pub-id><pub-id pub-id-type="pmid">10195106</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Orr</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Efficient backprop</chapter-title><person-group person-group-type="editor"><name><surname>LeCun</surname><given-names>Y</given-names></name></person-group><source>Neural Networks: Tricks of the Trade</source><publisher-name>Springer</publisher-name><fpage>9</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/3-540-49430-8_2</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>WB</given-names></name><name><surname>Baxter</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Energy-efficient neuronal computation via quantal synaptic failures</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>4746</fpage><lpage>4755</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-11-04746.2002</pub-id><pub-id pub-id-type="pmid">12040082</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Marris</surname><given-names>L</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Backpropagation and the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>335</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id><pub-id pub-id-type="pmid">32303713</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>JE</given-names></name><name><surname>Harris</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Quantal analysis and synaptic anatomy--integrating two views of hippocampal plasticity</article-title><source>Trends in Neurosciences</source><volume>16</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(93)90122-3</pub-id><pub-id pub-id-type="pmid">7682347</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><year iso-8601-date="1992">1992a</year><article-title>A practical bayesian framework for backpropagation networks</article-title><source>Neural Computation</source><volume>4</volume><fpage>448</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.3.448</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><year iso-8601-date="1992">1992b</year><article-title>The evidence framework applied to classification networks</article-title><source>Neural Computation</source><volume>4</volume><fpage>720</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.5.720</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Malkin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Efficient bayes</data-title><version designator="swh:1:rev:a341bff3e9241f958f351cb460b30047ec60d3a3">swh:1:rev:a341bff3e9241f958f351cb460b30047ec60d3a3</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:627379d202b9a70e656b8b3a6e7f75c5a4d2b55a;origin=https://github.com/JamesMalkin/EfficientBayes;visit=swh:1:snp:76bcd130a9aef445414073e0a5e8fc9e3f15e557;anchor=swh:1:rev:a341bff3e9241f958f351cb460b30047ec60d3a3">https://archive.softwareheritage.org/swh:1:dir:627379d202b9a70e656b8b3a6e7f75c5a4d2b55a;origin=https://github.com/JamesMalkin/EfficientBayes;visit=swh:1:snp:76bcd130a9aef445414073e0a5e8fc9e3f15e557;anchor=swh:1:rev:a341bff3e9241f958f351cb460b30047ec60d3a3</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martens</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>New insights and perspectives on the natural gradient method</article-title><source>The Journal of Machine Learning Research</source><volume>21</volume><fpage>5776</fpage><lpage>5851</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Machine Learning: A Probabilistic Perspective</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Stevens</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Heterogeneous release properties of visualized individual hippocampal synapses</article-title><source>Neuron</source><volume>18</volume><fpage>599</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80301-3</pub-id><pub-id pub-id-type="pmid">9136769</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural variability and sampling-based probabilistic representations in the visual cortex</article-title><source>Neuron</source><volume>92</volume><fpage>530</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.038</pub-id><pub-id pub-id-type="pmid">27764674</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulsen</surname><given-names>O</given-names></name><name><surname>Heggelund</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The quantal size at retinogeniculate synapses determined from spontaneous and evoked EPSCs in guinea‐pig thalamic slices</article-title><source>The Journal of Physiology</source><volume>480</volume><fpage>505</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1994.sp020379</pub-id><pub-id pub-id-type="pmid">7869264</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulsen</surname><given-names>O</given-names></name><name><surname>Heggelund</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Quantal properties of spontaneous EPSCs in neurones of the guinea‐pig dorsal lateral geniculate nucleus</article-title><source>The Journal of Physiology</source><volume>496</volume><fpage>759</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1996.sp021725</pub-id><pub-id pub-id-type="pmid">8930842</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulido</surname><given-names>C</given-names></name><name><surname>Ryan</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Synaptic vesicle pools are a major hidden resting metabolic burden of nerve terminals</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabi9027</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abi9027</pub-id><pub-id pub-id-type="pmid">34860552</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purdon</surname><given-names>AD</given-names></name><name><surname>Rosenberger</surname><given-names>TA</given-names></name><name><surname>Shetty</surname><given-names>HU</given-names></name><name><surname>Rapoport</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Energy consumption by phospholipid metabolism in mammalian brain</article-title><source>Neurochemical Research</source><volume>27</volume><fpage>1641</fpage><lpage>1647</lpage><pub-id pub-id-type="doi">10.1023/a:1021635027211</pub-id><pub-id pub-id-type="pmid">12515317</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raghavachari</surname><given-names>S</given-names></name><name><surname>Lisman</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Properties of quantal transmission at CA1 synapses</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>2456</fpage><lpage>2467</lpage><pub-id pub-id-type="doi">10.1152/jn.00258.2004</pub-id><pub-id pub-id-type="pmid">15115789</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dendritic solutions to the credit assignment problem</article-title><source>Current Opinion in Neurobiology</source><volume>54</volume><fpage>28</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.08.003</pub-id><pub-id pub-id-type="pmid">30205266</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosset</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Sparse, flexible and efficient modeling using L 1 regularization</article-title><source>Feature Extraction: Foundations and Applications</source><volume>01</volume><fpage>375</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-35488-8_17</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Wichert</surname><given-names>A</given-names></name><name><surname>van Rossum</surname><given-names>MCW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Energy efficient sparse connectivity from imbalanced synaptic plasticity rules</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004265</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004265</pub-id><pub-id pub-id-type="pmid">26046817</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Ponte Costa</surname><given-names>R</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakaba</surname><given-names>T</given-names></name><name><surname>Neher</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Quantitative relationship between transmitter release and calcium current at the calyx of held synapse</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>462</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-02-00462.2001</pub-id><pub-id pub-id-type="pmid">11160426</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schug</surname><given-names>S</given-names></name><name><surname>Benzing</surname><given-names>F</given-names></name><name><surname>Steger</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Presynaptic stochasticity improves energy efficiency and helps alleviate the stability-plasticity dilemma</article-title><source>eLife</source><volume>10</volume><elocation-id>e69884</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.69884</pub-id><pub-id pub-id-type="pmid">34661525</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A mathematical theory of communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shivkumar</surname><given-names>S</given-names></name><name><surname>Lange</surname><given-names>R</given-names></name><name><surname>Chattoraj</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A probabilistic population code based on neural samples</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Estimation of nonuniform quantal parameters with multiple-probability fluctuation analysis: theory, application and limitations</article-title><source>Journal of Neuroscience Methods</source><volume>130</volume><fpage>127</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.09.030</pub-id><pub-id pub-id-type="pmid">14667542</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title><source>Neuron</source><volume>32</volume><fpage>1149</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00542-6</pub-id><pub-id pub-id-type="pmid">11754844</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Leslie</surname><given-names>KR</given-names></name><name><surname>Desai</surname><given-names>NS</given-names></name><name><surname>Rutherford</surname><given-names>LC</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title><source>Nature</source><volume>391</volume><fpage>892</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1038/36103</pub-id><pub-id pub-id-type="pmid">9495341</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Homeostatic plasticity in the developing nervous system</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>97</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nrn1327</pub-id><pub-id pub-id-type="pmid">14735113</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity</article-title><source>Neural Computation</source><volume>29</volume><fpage>1229</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00949</pub-id><pub-id pub-id-type="pmid">28333583</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Pan</surname><given-names>F</given-names></name><name><surname>Gan</surname><given-names>W-B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stably maintained dendritic spines are associated with lifelong memories</article-title><source>Nature</source><volume>462</volume><fpage>920</fpage><lpage>924</lpage><pub-id pub-id-type="doi">10.1038/nature08577</pub-id><pub-id pub-id-type="pmid">19946265</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Synaptic Dynamics Realize First-Order Adaptive Learning and Weight Symmetry</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2212.09440">https://arxiv.org/abs/2212.09440</ext-link></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Energy-efficient population coding constrains network size of a neuronal array system</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>19369</elocation-id><pub-id pub-id-type="doi">10.1038/srep19369</pub-id><pub-id pub-id-type="pmid">26781354</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Reliability costs</title><p>The difficulty in determining reliability costs is that <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> depends on three variables: <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, the number of vesicles, <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> the probability of release, and <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula>, the quantal size, which measures the amount of neurotransmitter in each vesicle:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>However, these variables also determine the mean<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>so a straightforward optimisation of <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> under reliability costs will also change <italic>μ</italic> and one pitfall is to accidentally consider only those changes in reliability that derive from changes in mean. A solution to this problem is to eliminate one of the variables so that <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> is a function of <italic>μ</italic>, and the remaining variables. Eliminating <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> gives<disp-formula id="equ10">.<label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:msqrt></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The idea is to consider energetic costs associated with <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> and relate these to <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> while holding <italic>μ</italic> fixed. To simplify the biological motivation, we assume that during changes to the synapse aimed at manipulating the energetic cost, <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> will also change to compensate for any collateral changes in <italic>μ</italic> keeping <italic>μ</italic> constant. Hence, <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> a ‘compensatory variable’. Moreover, there is biological evidence that <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> is the mechanism used in real synapses to fix <italic>μ</italic> (<xref ref-type="bibr" rid="bib78">Turrigiano et al., 1998</xref>; <xref ref-type="bibr" rid="bib40">Karunanithi et al., 2002</xref>). Fixing <italic>μ</italic> through a compensatory variable is termed homeostatic plasticity. Fixing <italic>μ</italic> through <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> is described as ‘quantal scaling’. For reviews on homeostatic plasticity, see <xref ref-type="bibr" rid="bib79">Turrigiano and Nelson, 2004</xref>; <xref ref-type="bibr" rid="bib17">Davis and Müller, 2015</xref>.</p><p>In what follows four different energy costs are considered, the first depends on <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, the next two on <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, the situation for the final one is less clear, but in each case we derive a reliability cost in the form <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> for some value of <italic>ρ</italic>. Of course, since we are considering costs for fixed <italic>μ</italic> the coefficient of variation <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:mstyle></mml:math></inline-formula> is proportional to <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula>; it may be helpful to think of these calculations as finding the relationship between the cost and <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s9"><title>Calcium efflux – <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:math></inline-formula></title><p>Calcium influx into the synapse is an essential part of the mechanism for vesicle release. Presynaptic calcium pumps act to restore calcium concentrations in the synapse; this pumping is a significant portion of synaptic transmission costs (<xref ref-type="bibr" rid="bib4">Attwell and Laughlin, 2001</xref>). By rearranging the Hill equation defined by <xref ref-type="bibr" rid="bib72">Sakaba and Neher, 2001</xref>, it can be shown that vesicle release has an interaction coefficient of four, this means the <italic>odds</italic> of release per vesicle are related to intracellular calcium amplitude via the fourth power (<xref ref-type="bibr" rid="bib33">Heidelberger et al., 1994</xref>; <xref ref-type="bibr" rid="bib72">Sakaba and Neher, 2001</xref>):<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>∝</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>4</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To recover basal synaptic calcium concentration, the calcium influx is reversed by ATP-driven calcium pumps, where <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>∝</mml:mo><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mroot><mml:mfrac><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mn>4</mml:mn></mml:mroot><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since this physiological cost does not depend on <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> we assume it is fixed and so the odds of release <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is proportional to <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. Thus,<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>or <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s10"><title>Vesicle membrane – <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle></mml:math></inline-formula></title><p>There is a cost associated with the total area of vesicle membrane. Evidence in <xref ref-type="bibr" rid="bib65">Pulido and Ryan, 2021</xref>, suggest stored vesicles emit charged H<sup>+</sup> ions, with the number emitted proportional to the number of v-glut, glutamate transporters, on the surface of vesicles. v-ATPase pumps reverse this process maintaining the pH of the cell. It is suggested that this cost is 44% of the resting synaptic energy consumption. In addition, metabolism of the phospholipid bilayer that form the membrane of neurotransmitter-filled vesicles has been identified as a major energetic cost (<xref ref-type="bibr" rid="bib66">Purdon et al., 2002</xref>). Provided the total volume is the same, release of the same amount of neurotransmitter into the synaptic cleft can involve many smaller vesicles or fewer larger ones. However, while having many small vesicles will be more reliable, it requires a greater surface area of costly membrane. With fixed <italic>μ</italic> and <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ14">.<label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and using <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> this give<disp-formula id="equ15">.<label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since this reliability cost depends on <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, so <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> is regarded as constant, so <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> and hence <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s11"><title>Actin – <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle></mml:math></inline-formula></title><p>Actin polymers are an energy costly structural filament that support the structural organisation of vesicle reserve pools (<xref ref-type="bibr" rid="bib16">Cingolani and Goda, 2008</xref>), with the vesicles strung out along the filaments. We assume each vesicle to require a length of actin roughly proportional to its diameter, this means that the total length of actin is proportional to <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>. Hence,<disp-formula id="equ16">.<label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The calculation then proceeds much as for the membrane cost, but with <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> giving <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12"><title>Trafficking – <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula></title><p>ATP-fuelled myosin motors drive trains of actin filament along with associated cargo such as vesicles and actin-myosin trafficking moves vesicles from vesicle reserve pools to release sites sustaining the readily releasable pool following vesicle release (<xref ref-type="bibr" rid="bib13">Bridgman, 1999</xref>; <xref ref-type="bibr" rid="bib28">Gramlich and Klyachko, 2017</xref>). This gives a cost for vesicle recruitment proportional to <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, the number of vesicles released:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>so if <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> is regarded as the principal way this cost is changed, with <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> fixed then <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>∝</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and so <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>. This is the view point we are taking to motivate examining reliability costs with <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>. This is certainly useful in considering the range of model behaviours over a broad range of <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> values.</p><p>Nonetheless, it is sensible to ask whether the likely biological mechanism behind a varying trafficking cost is one which changes <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> itself. In this case, since a constant <italic>μ</italic> for varying <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> means <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi><mml:mo>∝</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, we have<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which, is, again, of the form cost <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∝</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> provided <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> is small. For larger <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, however, it depends on exactly how <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> changes as <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> changes.</p><p>Generally, throughout these calculation we have supposed that <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> is a compensatory variable and that either <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> changes in the process that changes the cost at a synapse. The benefit of this is that we are able to model costs directly in terms of reliability; in the future, though, it might be interesting to consider models which use <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi></mml:mstyle></mml:math></inline-formula> instead of <italic>μ</italic> and <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula>; this would certainly be convenient for the sort of comparisons we are making here and interesting, although two formulations seem equivalent, this does not mean that the learning dynamics will be the same.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s13"><title>High input rates and high precision at important synapses</title><p>Here, we show that under a linear model, important synapses, as measured by the Hessian, have high input rates. Additionally, we show that energy-efficient synapses imply that these important synapses have low optimized variability.</p><p>We consider a simplified linear model, with targets <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, inputs <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and weights <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We have<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We take the performance cost to be<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of datapoints. We take the network weights to be drawn from a multivariate Gaussian, with diagonal covariance, <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="italic">Q</mml:mi><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="italic" stretchy="false">)</mml:mo><mml:mo mathvariant="italic">=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="italic">;</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo mathvariant="italic">,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo mathvariant="italic" stretchy="false">)</mml:mo><mml:mo mathvariant="italic">.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>All our derivations rely on looking at the quadratic form for the performance cost. In particular,<disp-formula id="equ22">.<label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Q</mml:mi><mml:mo mathvariant="italic" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="italic" stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Expanding the brackets,<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and evaluating the expectations,<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the trace,<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>includes a sum over both the synapse index <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> and the time index <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>To measure synapse importance, we use the Hessian, i.e., the second derivative of the performance cost with respect to the mean. From <xref ref-type="disp-formula" rid="equ24">Equation 24</xref>, we can identify this as<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mtext>performance cost</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where, as before, <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is the time index.</p><p>Since the the only term in the performance cost that depends on the variance, <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, is the trace, we can identify <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> that optimises the performance and reliability cost. This allows us to observe how synapse variability relates to synapse importance in the tradeoff between performance and reliability costs:<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mtext>performance cost</mml:mtext><mml:mo>+</mml:mo><mml:mtext>reliability cost</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which means<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>or<disp-formula id="equ29">.<label>(29)</label><mml:math id="m29"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mroot><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mi>ρ</mml:mi></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mroot></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, more important synapses (as measured by the Hessian, <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) have lower variability if the synapse is energy efficient. Moreover, through <xref ref-type="disp-formula" rid="equ26">Equation 26</xref>, we expect synapses with higher input rates to be more important.</p></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s14"><title>Synapse importance and gradient magnitudes</title><p>In our ANN simulations we train synaptic weights using the most established adaptive optimisation scheme, Adam (<xref ref-type="bibr" rid="bib43">Kingma and Ba, 2014</xref>), which has recently been realised using biologically plausible mechanisms (<xref ref-type="bibr" rid="bib82">Yang and Li, 2022</xref>). Adam uses a synapse-specific learning rate, <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, which decreases in response to high gradients at that synapse,<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>η</mml:mi><mml:mtext>base</mml:mtext></mml:msub><mml:msqrt><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Specifically, the local learning rate for the <italic>i</italic>th synapse, <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, is usually a base learning rate, <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>base</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, divided by <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msqrt><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:msqrt></mml:mstyle></mml:math></inline-formula>, the root-mean-square of the gradient for each datapoint or minibatch.</p><p>The key intuition is that if the gradients for each datapoint/minibatch are large, that means that this synapse is important, as it has a big impact on the predictions for every datapoint. In fact, these mean-squared gradients can be related to our formal measure of synapse importance, the Hessian. Specifically, for data generated from the model, the Hessian (or Fisher information; <xref ref-type="bibr" rid="bib23">Fisher, 1922</xref>) is equivalent to the mean-squared gradient, where the gradient is taken over each datapoint separately,<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mtext>performance cost</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">g</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the expectation is evaluated over data generated by the model and <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined,<disp-formula id="equ32">.<label>(32)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Of course, in practice, the data is not drawn from the model, so the relationship between the squared gradients and the Hessian computed for real data is only approximate. But it is close enough to induce a relationship between synapse importance (measured as the diagonal of the Hessian) and learning rates (which are inversely proportional to the root-mean-squared gradients) in our simulations.</p></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s15"><title>Energy-efficient noise and variational Bayes for neural network weights</title><sec sec-type="appendix" id="s15-1"><title>Introduction to variational Bayes for neural network weights</title><p>One approach to performing Bayesian inference for the weights of a neural network is to use variational Bayes. In variational Bayes, we introduce a parametric approximate posterior, <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and fit the parameters of that approximate posterior using gradient descent on an objective, the ELBO (<xref ref-type="bibr" rid="bib9">Blundell et al., 2015</xref>). In particular,<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mtext>ELBO</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> is the entropy of the approximate posterior. Maximising the ELBO is particularly useful for selecting models as it forms a lower bound on the marginal likelihood, or evidence, <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib57">MacKay, 1992b</xref>; <xref ref-type="bibr" rid="bib5">Barber and Bishop, 1998</xref>). When using variational Bayes for neural network weights, we usually use Gaussian approximate posteriors (<xref ref-type="bibr" rid="bib9">Blundell et al., 2015</xref>),<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that optimising the ELBO with respect to the parameters of <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula> is difficult, because <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula> is the distribution over which the expectation is taken in <xref ref-type="disp-formula" rid="equ33">Equation 33</xref>. To circumvent this issue, we use the reparameterisation trick (<xref ref-type="bibr" rid="bib44">Kingma et al., 2015</xref>), which involves writing the weights in terms of IID standard Gaussian variables, <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϵ</mml:mi></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, we can write the ELBO as an expectation over <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϵ</mml:mi></mml:mstyle></mml:math></inline-formula>, which has a fixed IID standard Gaussian distribution,<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that we use <italic>w</italic>, <italic>μ,</italic> etc. without indices in this expression to indicate all the weights/mean weights.</p></sec><sec sec-type="appendix" id="s15-2"><title>Identifying the log-likelihood and log-prior</title><p>Following the usual practice in deep learning, we assume the likelihood is formed by a categorical distribution, with probabilities obtained by applying the softmax function to the output of the neural network. The log-likelihood for a categorical distribution with softmax probabilities is the negative cross-entropy (<xref ref-type="bibr" rid="bib60">Murphy, 2012</xref>),<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the output of the network with weights, <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula>, and inputs, <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>. We can additionally identify a Laplace prior with the magnitude cost. Specifically, if we take the prior to be Laplace, with scale <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mtext>const</mml:mtext><mml:mo>−</mml:mo><mml:mtext>magnitude cost</mml:mtext></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we identify the magnitude cost using <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>.</p></sec><sec sec-type="appendix" id="s15-3"><title>Connecting the entropy and the biological reliability cost</title><p>Now, we have identified the log-likelihood and log-prior terms in the ELBO (<xref ref-type="disp-formula" rid="equ33">Equation 33</xref>) with terms in our biological cost (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). We thus have two terms left: the entropy in the ELBO (<xref ref-type="disp-formula" rid="equ33">Equation 33</xref>) and the reliability cost in the biological cost (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Critically, the entropy term also acts as a reliability cost, in that it encourages more variability in the weights (as the entropy is positive in the ELBO [<xref ref-type="disp-formula" rid="equ33">Equation 33</xref>] and we are trying to maximise the ELBO, the entropy term gives a bonus for more variability). Intuitively, we can think of the entropy term in the ELBO (<xref ref-type="disp-formula" rid="equ33">Equation 33</xref>) as being an ‘entropic reliability cost’, as compared to the ‘biological reliability cost’ in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>.</p><p>This intuitive connection suggests that we might be able to find a more formal link. Specifically, we can define an entropic reliability cost as simply the negative entropy,<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mtext>VI;i</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Our goal is to write the biological reliability cost <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>BI;i</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, as a bound on <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>BI;i</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. By rearranging and introducing <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mtext>VI;i</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and noting that <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>a</mml:mi><mml:mo>≤</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>s</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>we can demonstrate that any reliability cost expressed as a generic power-law forms an upper bound on the entropic cost<disp-formula id="equ44">.<label>(44)</label><mml:math id="m44"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mtext>BI;i</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext> reliability cost</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mtext>const</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>BI;i</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the biological reliability cost for a synapse, which formally bounds the entropic reliability cost from VI,<disp-formula id="equ45">.<label>(45)</label><mml:math id="m45"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We can also sum these quantities across synapses,<disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The parameter <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> sets the importance of the reliability cost within the performance-reliability cost tradeoff (see <xref ref-type="fig" rid="fig2">Figure 2</xref>)<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mi>ρ</mml:mi></mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>ρ</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Note that while both <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>reliability cost</mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>BI;i</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represent the biological reliability costs, they are slightly different in that <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>BI;i</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> includes an additive constant. Importantly, this additive constant is independent of <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, so it does not affect learning and can be ignored.</p><p>Given that the biological reliability cost forms a bound on the ideal entropic reliability cost we can consider using the biological reliability cost in place of the entropic reliability cost,<disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mtext>ELBO</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mtext>ELBO</mml:mtext><mml:mo>≥</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>we find that our overall biological cost (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) forms a bound on the ELBO, which itself forms a bound on the evidence, <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. Thus, pushing down the overall biological cost (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) pushes up a bound on the model evidence.</p></sec><sec sec-type="appendix" id="s15-4"><title>Predictive probabilities arising from biological reliability costs</title><p>Given the connections between our overall cost and the ELBO, we expect that optimising our overall cost will give a similar result to variational Bayes. To check this connection, we plotted the distribution of predictions induced by noisy weights arising from variational Bayes (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1a</xref>) and our overall costs (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1b</xref>). Variational Bayes maximises the ELBO, therefore its predictive distribution is optimised to reflect the data distribution from which data is drawn (<xref ref-type="bibr" rid="bib56">MacKay, 1992a</xref>). We found comparable patterns for the predictive distributions learned through variational Bayes and our overall costs, albeit with some breakdown in predictive performance with higher values for <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Predictive distributions from variational Bayes and our overall costs are similar.</title><p>We trained a one hidden layer network with 20 hidden units on data <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (black dots). Network targets, <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, were drawn from a ‘true’ function, <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (grey line) with additive Gaussian noise of variance <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mn>0.05</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. Two standard deviations of the predictive distributions are depicted in the shaded areas. (<bold>a</bold>) The predictive distribution produced by variational Bayes have a larger density of predictions where there is a higher probability of data. Where there is an absence of data, the model has to extrapolate and the spread of the predictive distribution increases. (<bold>b</bold>) Optimising the overall cost with small <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> generates narrow response distributions. This is most noticeable in the upper-right panel, where the spread of predictive distribution is unrelated to the presence or absence of data. In contrast, while the predictive density for larger <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> do vary according to the presence or absence of data, these distributions poorly predict <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. This is most apparent in the lower-right panel where the network’s predictive distribution transects the inflections of <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-app4-fig1-v2.tif"/></fig><p><italic>Interpreting</italic> <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula><italic>,</italic> <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula><italic>, and</italic> <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>. As discussed in the main text, <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> are the fundamental parameters, and they are set by properties of the underlying biological system. It may nonetheless be interesting to consider the effects of <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> on the tightness of the bound of the biological reliability cost on the variational reliability cost. In particular, we consider settings of <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> for which the bound is looser or tighter (though again, there is no free choice in these parameters: they are set by properties of the biological system).</p><p>First, the biological reliability cost becomes equal to the ideal entropic reliability cost in the limit as <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>x</mml:mi></mml:mfrac></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mi>x</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>z</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, taking <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus,<disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mi>s</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This explains the apparent improvement in predictive performance (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>) and in matching the posteriors (<xref ref-type="fig" rid="fig7">Figure 7</xref>) with lower values of <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>Second, the biological reliability cost becomes equal to the ideal entropic reliability cost when <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>as the first term in <xref ref-type="disp-formula" rid="equ43">Equation 43</xref> cancels. However, <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> cannot be set individually across synapses, but is instead roughly constant, with a value set by underlying biological constraints. In particular, <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> can be written as a function of <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ47">Equation 47</xref>), and <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> are quantities that are roughly constant across synapses, with their values set by biological constraints. Thus, biological implications of a tightening bound as <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> tends to <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are not clear.</p></sec></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s16"><title>Analytic predictions for <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></title><p>At various points in the main text, we note a connection between the Hessian, synapse importance, and optimal variability. We start with <xref ref-type="disp-formula" rid="equ29">Equation 29</xref>, which relates the optimal, energy-efficient noise variance, <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, to the Hessian, <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> which gives <xref ref-type="disp-formula" rid="equ54">Equation 54a</xref>. Then, we combine this with the form for the Hessian (<xref ref-type="disp-formula" rid="equ26">Equation 26</xref>), which gives <xref ref-type="disp-formula" rid="equ55">Equation 54b</xref>. Finally, we note Hessian describes the log-likelihood (<xref ref-type="disp-formula" rid="equ26">Equation 26</xref> and <xref ref-type="disp-formula" rid="equ20">Equation 20</xref>). Thus, assuming the prior variance is large, we have <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>post</mml:mtext><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, which gives <xref ref-type="disp-formula" rid="equ56">Equation 54c</xref>,<disp-formula id="equ54"><label>(54a)</label><mml:math id="m54"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>∝</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>η</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle><mml:mspace width="2em"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mspace width="2em"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>4</mml:mn><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ55"><label>(54b)</label><mml:math id="m55"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mspace width="2em"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mspace width="2em"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ56"><label>(54c)</label><mml:math id="m56"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle><mml:mspace width="2em"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mspace width="2em"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To test these predictions, we performed a simpler simulation classifying MNIST in a network with no hidden layers. We found that the analytic results closely matched the simulations, and that the biological slopes tend to match the <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> better than the other values for <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>. However, while the direction of the slope was consistent in deeper networks, the exact value of the slope was not consistent (<xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1</xref>), so it is unclear whether we can draw any strong conclusions here.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Comparing analytic predictions for synapse variability with simulations and experimental data in a zero hidden layer network for MNIST classification.</title><p>The green dots show simulated synapses, and the grey line is fitted to these simulated points. The blue line is from our analytic predictions, while the red-dashed line is taken from experimental data (<xref ref-type="fig" rid="fig6">Figure 6b</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-app5-fig1-v2.tif"/></fig></sec></app><app id="appendix-6"><title>Appendix 6</title><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>Patterns of synapse variability for the remaining layers of the neural network used to provide our results.</title><p>In <xref ref-type="fig" rid="fig5">Figure 5</xref> we showed the heterogeneous patterns of synapse variability for the synapses connecting the two hidden layers of our artificial neural network (ANN). Here, we exhibit the equivalent plots for the other synapses, those between the input and the first hidden layer (layer 1) and from the final hidden layer to the output layer (layer 3). As in <xref ref-type="fig" rid="fig5">Figure 5</xref> we show the relationship between synaptic variance and (<bold>a</bold>) the Hessian; (<bold>b</bold>) learning rate; and (<bold>c</bold>) input rate. The patterns do not appear substantially different from layer to layer.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-app6-fig1-v2.tif"/></fig><fig id="app6fig2" position="float"><label>Appendix 6—figure 2.</label><caption><title>Patterns of synapse variability are robust to changes in the reliability.</title><p>We show that the patterns of variability of synapses connecting the two hidden layers presented in <xref ref-type="fig" rid="fig5">Figure 5</xref> are preserved over a wide range of <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>. (<bold>a, b</bold>) When the reliability cost multiplier, <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>, is either increased (<bold>a</bold>) or decreased (<bold>b</bold>) by a factor of 10, overall synapse variability increases or decreases accordingly, but the qualitative correlations seen in <xref ref-type="fig" rid="fig5">Figure 5</xref> are preserved.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92595-app6-fig2-v2.tif"/></fig></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92595.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study provides deep insight into a ubiquitous, but poorly understood, phenomenon: synaptic noise (primarily due to failures). Through a combination of theoretical analysis, simulations, and comparison to existing experimental data, this paper makes a <bold>compelling</bold> case that synapses are noisy because reducing noise is expensive. It touches on probably the most significant feature of living organisms -- their ability to learn -- and will be of broad interest to the neuroscience community.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92595.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Given the cost of producing action potentials and transmitting them along axons, it has always seemed a bit strange that there are synaptic failures: when a spike arrives at a synapse, about half the time nothing happens. This paper proposes a perfectly reasonable explanation: reducing failures (or, more generally, reducing noise) is costly. Four possible mechanisms are proposed, each associated with a different cost, with costs of the form 1/sigma_i^rho where sigma_i is the failure-induced variability at synapse i and rho is an exponent. The four different mechanisms produce four different values of rho.</p><p>What is interesting about the study is that the model makes experimental predictions about the relationship between learning rate, variability and presynaptic firing rate. Those predictions are consistent with experimental data, making it a strong candidate model. The fact that the predictions come from reasonable biological mechanisms make it a very strong candidate model and suggest several experiments to test it further.</p><p>Interestingly, the predictions made by this model are nearly indistinguishable from the predictions made by a normative model Synaptic plasticity as Bayesian inference. Aitchison it al., Nature Neurosci. 24:565-571 (2021). As pointed out by the authors, working out whether the brain is using Bayesian inference to tune learning rules, or it just looks like it's Bayesian inference but the root cause is cost minimization, will be an interesting avenue for future research.</p><p>Finally, the authors relate their cost of reliability to the cost used in variational Bayesian inference. Intriguingly, the biophysical cost provides an upper bound on the variational cost. This is intellectually satisfying, as it answers a &quot;why&quot; question: why would evolution evolve to produce the kind of costs seen in the brain?</p><p>Strengths:</p><p>This paper provides a strong mix of theoretical analysis, simulations and comparison to experiments. And the extended appendices, which are very easy to read, provide additional mathematical insight.</p><p>Weaknesses:</p><p>None.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92595.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>This manuscript argues about the similarity between two frameworks describing synaptic plasticity. In the Bayesian inference perspective, due to the noise and the limited available pre- and postsynaptic information, synapses can only have an estimate of what should be their weight. The belief about those weights is described by their mean and variance. In the energy efficient perspective, synaptic parameters (individual means and variances) are adapted such that the neural network achieves some task while penalizing large mean weights as well as small weight variances. Interestingly, the authors show both numerically and analytically the strong link between those two frameworks. In particular, both frameworks predict that (a) synaptic variances should decrease when the input firing rate increases and (b) that the learning rate should increase when the weight variances increase. Both predictions have some experimental support.</p><p>Strengths</p><p>(1) Overall, the paper is very well written and the arguments are clearly presented.</p><p>(2) The tight link between the Bayesian inference perspective and the energy efficiency perspective is elegant and well supported, both with numerical simulations as well as with analytical arguments.</p><p>(3) I also particularly appreciate the derivation of the reliability cost terms as a function of the different biophysical mechanisms (calcium efflux, vesicle membrane, actin and trafficking). Independently of the proposed mapping between the Bayesian inference perspective and the energy efficiency perspective, those reliability costs (expressed as power-law relationships) will be important for further studies on synaptic energetics.</p><p>Weaknesses</p><p>(1) As recognised by the authors, the correspondence between the entropy term in the variational inference description and the reliability cost in the energetic description is strong, but not perfect. Indeed, the entropy term scales as -log(sigma) while reliability cost scales as sigma^(-rho).</p><p>(2) Even though this is not the main point of the paper, I appreciate the effort made by the authors to look for experimental data that could in principle validate the Bayesian/energetic frameworks. A stronger validation will be an interesting avenue for future research.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92595.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Malkin</surname><given-names>James</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>O'Donnell</surname><given-names>Cian</given-names></name><role specific-use="author">Author</role><aff><institution>University of Ulster</institution><addr-line><named-content content-type="city">Derry</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Houghton</surname><given-names>Conor J</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Aitchison</surname><given-names>Laurence</given-names></name><role specific-use="author">Author</role><aff><institution>University of Bristol</institution><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p>Weaknesses</p><p>(1) The authors face a technical challenge (which they acknowledge): they use two numbers (mean and variance) to characterize synaptic variability, whereas in the brain there are three numbers (number of vesicles, release probability, and quantal size). Turning biological constraints into constraints on the variance, as is done in the paper, seems somewhat arbitrary. This by no means invalidates the results, but it means that future experimental tests of their model will be somewhat nuanced.</p></disp-quote><p>Agreed. There are two points to make here.</p><p>First, the mean and variance are far more experimentally accessible than <italic>n</italic>, <italic>p</italic> and <italic>q</italic>. The EPSP mean and variance is measured directly in paired-patch experiments, whereas getting <italic>n</italic>, <italic>p</italic> and <italic>q</italic> either requires far more extensive experimentation, or making strong assumptions. For instance, the data from Ko et al. (2013) gives the EPSP mean and variance, but not (directly) <italic>n</italic>, <italic>p</italic> and <italic>q</italic>. Thus, in some ways, predictions about means and variances are easier to test than predictions about <italic>n</italic>, <italic>p</italic> and <italic>q</italic>.</p><p>That said, we agree that in the absence of an extensive empirical accounting of the energetic costs at the synapse, there is inevitably some arbitrariness as we derive our energetic costs. That was why we considered four potential functional forms for the connection between the variance and energetic cost, which covered a wide range of sensible forms for this energetic cost. Our results were robust to this wide range functional forms, indicating that the patterns we describe are not specifically due to the particular functional form, but arise in many settings where there is an energetic cost for reliable synaptic transmission.</p><disp-quote content-type="editor-comment"><p>(2) The prediction that the learning rate should increase with variability relies on an optimization scheme in which the learning rate is scaled by the inverse of the magnitude of the gradients (Eq. 7). This seems like an extra assumption; the energy efficiency framework by itself does not predict that the learning rate should increase with variability. Further work will be needed to disentangle the assumption about the optimization scheme from the energy efficiency framework.</p></disp-quote><p>Agreed. The assumption that learning rates scale with synapse importance is separate. However, it is highly plausible as almost all modern state-of-the-art deep learning training runs use such an optimization scheme, as in practice it learns far faster than other older schemes. We have added a sentence to the main text (line 221), indicating that this is ultimately an assumption.</p><disp-quote content-type="editor-comment"><p>Major</p><p>(1) The correspondence between the entropy term in the variational inference description and the reliability cost in the energetic description is a bit loose. Indeed, the entropy term scales as −log(σ) while reliability cost scales as σ−ρ. While the authors do make the point that σ−ρ upper bounds −log(σ) (up to some constant), those two cost terms are different. This raises two important questions:</p><p>a. Is this difference important, i.e. are there scenarios for which the two frameworks would have different predictions due to their different cost functions?</p><p>b. Alternatively, is there a way to make the two frameworks identical (e.g. by choosing a proposal distribution Q(w) different from a Gaussian distribution (and tuneable by a free parameter that could be related to <italic>ρ</italic>) and therefore giving rise to an entropy term consistent with the reliability cost of the energy efficiency framework)?</p></disp-quote><p>To answer b first, there is no natural way to make the two frameworks identical (unless we assume the reliability cost is proportional to log_σsyn_, and we don’t think there’s a biophysical mechanism that would give rise to such a cost). Now, to answer a, in Fig. 7 we extensively assessed the differences between the energy efficient <italic>σsyn</italic> and the Bayesian <italic>σpost</italic>. In Fig.7bc, we find that <italic>σsyn</italic> and <italic>σpost</italic> are positively correlated in all models. This positive correlation indicates that the qualitative predictions made by the two frameworks (Bayesian inference and energy efficiency) are likely to be very similar. Importantly though, there are systematic differences highlighted by Fig. 7ab. Specifically, the energy efficient <italic>σsyn</italic> tends to vary less than the Bayesian <italic>σpost</italic>. This appears in Fig. 7b which shows the relationship between <italic>σsyn</italic> (on the y-axis) and <italic>σpost</italic> (on the x-axis). Specifically, this plot has a slope that is smaller than one for all our models of the biophysical cost. Further, the pattern also appears in the covariance ellipses in Fig. 7a, in that the Bayesian covariance ellipses tend to be long and thin, while the energy efficient covariance ellipsis are rounder. Critically though both covariance ellipses show the same pattern in that there is more noise along less important directions (as measured by the Hessian).</p><p>We have added a sentence (line 273) noting that the search for a theoretical link is motivated by our observations in Fig. 7 of a strong, but not perfect link between the pattern of variability predicted by Bayesian and energy-efficient synapses.</p><disp-quote content-type="editor-comment"><p>(2) Even though I appreciate the effort of the authors to look for experimental evidence, I still find that the experimental support (displayed in Fig. 6) is moderate for three reasons.</p><p>a. First, the experimental and simulation results are not displayed in a consistent way. Indeed, Fig 6a displays the relative weight change |<italic>Dw</italic>|<italic>/w</italic> as a function of the normalised variability <italic>σ2/</italic>|<italic>µ</italic>| in experiments whereas the simulation results in Fig 5c display the variance <italic>σ2 as a function of the learning rate. Also, Fig 6b displays the normalised variability σ2/|µ</italic>| as a function of the input rate whereas Fig 5b displays the variance _σ_2 as a function of the input rate. As a consequence the comparison between experimental and simulation results is difficult.</p><p>b. Secondly, the actual power-law exponents in the experiments (see Fig 6a resp. 6b) should be compared to the power-law exponents obtained in simulation (see Fig 5c resp. Fig 5b). The difficulty relies here on the fact that the power-law exponents obtained in the simulations directly depend on the (free) parameter <italic>ρ</italic>. So far the authors precisely avoided committing to a specific <italic>ρ</italic>, but rather argued that different biophysical mechanisms lead to different reliability exponents <italic>ρ</italic>. Therefore, since there are many possible exponents <italic>ρ</italic> (and consequently many possible power-law exponents in simulation results in Fig 5), it is likely that one of them will match the experimental data. For the argument to be stronger, one would need to argue which synaptic mechanism is dominating and therefore come up with a single prediction that can be falsified experimentally (see also point 4 below).</p><p>c, Finally, the experimental data presented in Fig6 are still “clouds of points&quot;. A coefficient of <italic>r</italic> = 0_.<italic>52 (in Fig 6a) is moderate evidence while the coefficient of r = −0</italic>._26 (in Fig 6b) is weak evidence.</p></disp-quote><p>The key thing to remember is that our paper is not about whether synapses are “really&quot; Bayesian or energy efficient (or both/neither). Instead, the key point of our paper, as expressed in the title, is to show that the experimental predictions of Bayesian synapses are very similar to the predictions from energy efficient synapses. And therefore energy efficient synapses are very difficult to distinguish experimentally from Bayesian synapses. In that context, the two plots in Fig. 6 are not really intended to present evidence in favour of the energy efficiency / Bayesian synapses. In fact, Fig. 6 isn’t meant to constitute a contribution of the paper at all, instead, Fig. 6 serves merely as illustrations of the kinds of experimental result that have (Aitchison et al. 2021) or might (Schug et al. 2021) be used to support Bayesian synapses. As such, Fig. 6 serves merely as a jumping-off point for discussing how very similar results might equally arise out of Bayesian and energy-efficiency viewpoints.</p><p>We have modified our description of Fig. 6 to further re-emphasise that the panels in Fig. 6 is not our contribution, but is taken directly from Schug et al. 2021 and Aitchison et al. 2021 (we have also modified Fig 6 to be precisely what was plotted in Schug et al. 2021, again to re-emphasise this point). Further, we have modified the presentation to emphasise that these plots serve merely as jumping off points to discuss the kinds of predictions that we might consider for Bayesian and energy efficient synapses.</p><p>This is important, because we would argue that the “strength of support&quot; should be assessed for our key claim, made in the title, that “Signatures of Bayesian inference emerge from energy efficient synapses&quot;.</p><p>a) To emphasise that these are previously published results, we have chosen axes to matchthose used in the original work (Aitchison et al. 2021) and (Schug et al. 2021).</p><p>b) We agree that a close match between power-law exponents would constitute strong evidencefor energy-efficiency / Bayesian inference, and might even allow us to distinguish them. We did consider such a comparison, but found it was difficult for two reasons. First, while the confidence intervals on the slopes exclude zero, they are pretty broad. Secondly, while the slopes in a one-layer network are consistent and match theory (Appendix 5) the slopes in deeper networks are far more inconsistent. This is likely to be due to a number of factors such as details of the optimization algorithm and initialization. Critically, if details of the optimization algorithm matter in simulation, they may also matter in the brain. Therefore, it is not clear to us that a comparison of the actual slopes is can be relied upon.</p><p>To reiterate, the point of our article is not to make judgements about the strength ofevidence in previously published work, but to argue that Bayesian and energy efficient synapses are difficult to distinguish experimentally as they produce similar predictions. That said, it is very difficult to make blanket statements about the strength of evidence for an effect based merely on a correlation coefficient. It is perfectly possible to have moderate correlation coefficients along with very strong evidence of an effect (and e.g. very strong p-values), e.g. if there is a lot of data. Likewise, it is possible to have a very large correlation coefficient along with weak evidence of an effect (e.g. if we only have three or four datapoints, which happen to lie in a straight line). A small correlation coefficient is much more closely related to the effect-size. Specifically, the effect-size, relative to the “noise&quot;, which usually arises from unmeasured factors of variation. Here, we know there are many, many unmeasured factors of variation, so even in the case that synapses are really Bayesian / energy-efficient, the best we can hope for is low correlation coefficients</p><disp-quote content-type="editor-comment"><p>As mentioned in the public review, a weakness in the paper is the derivation of the constraints on σi given the biophysical costs, for two reasons.</p><p>a.First, it seemed a bit arbitrary whether you hold n fixed or p fixed.</p><p>b.Second, at central synapses, n is usually small – possibly even usually 1: REF(Synaptic vesicles transiently dock to refill release sites, Nature Neuroscience 23:1329-1338, 2020); REF(The ubiquitous nature of multivesicular release Trends Neurosci. 38:428-438, 2015). Fixing n would radically change your cost function. Possibly you can get around this because when two neurons are connected there are multiple contacts (and so, effectively, reasonably large n). It seems like this is worth discussing.</p></disp-quote><p>a) Ultimately, we believe that the “real” biological cost function is very complex, and most likely cannot be written down in a simple functional form. Further, we certainly do not have the experimental evidence now, and are unlikely to have experimental evidence for a considerable period into the future to pin down this cost function precisely. In that context, we are forced to resort to two strategies. First, using simplifying assumptions to derive a functional form for the cost (such as holding <italic>n</italic> or <italic>p</italic> fixed). Second, considering a wide range of functional forms for the cost, and ensuring our argument works for all of them.</p><p>b) We appreciate the suggestion that the number of connections could be used as a surrogate where synapses have only a single release site. As you suggest we can propose an alternative model for this case where <italic>n</italic> represents the number of connections between neurons. We have added this alternative interpretation to our introduction of the quantal model under title “Biophysical costs&quot;. For a fixed PSP mean we could either have many connections with small vesicles or less connections with larger vesicles. Similarly for the actin cost we would certainly require more actin if the number of connections were increased.</p><disp-quote content-type="editor-comment"><p>Minor</p><p>(1) A few additional references could further strengthen some claims of the paper:</p><p>Davis, Graeme W., and Martin Muller. “Homeostatic Control of Presynaptic Neurotransmitter Release.&quot; Annual Review of Physiology 77, no. 1 (February 10, 2015): 251-70. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-physiol-021014-071740">https://doi.org/10.1146/annurev-physiol-021014-071740</ext-link>. This paper provides elegant experimental support for the claim (in line 538 now 583) that <italic>µ</italic> is kept constant and q acts as a compensatory variable.</p><p>Jegminat, Jannes, Simone Carlo Surace, and Jean-Pascal Pfister. “Learning as Filtering: Implications for Spike-Based Plasticity.&quot; Edited by Blake A Richards. PLOS Computational Biology 18, no. 2 (February 23, 2022): e1009721. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1009721">https://doi.org/10.1371/journal.pcbi.1009721</ext-link>.</p><p>This paper also showed that a lower uncertainty implies a lower learning rate (see e.g. in line 232), but in the context of spiking neurons.</p></disp-quote><p>Figure 1 of the the first suggested paper indeed shows that quantal size is a candidate for homeostatic scaling (fixing <italic>µ</italic>). This review also references lots of further evidence of quantal scaling and evidence for both presynaptic and postsynaptic scaling of <italic>q</italic> leaving space for speculation on whether vesicle radius or postsynaptic receptor number is the source of a compensatory <italic>q</italic>. On line 583 we have added a few lines pointing to the suggested review paper.</p><p>The second reference demonstrates Bayesian plasticity in the context of STDP, proposing learning rates tuned to the covariance in spike timing. We have added this as extra support for assuming an optimisation scheme that tunes learning rates to synapse importance and synapse variability (line 232).</p><disp-quote content-type="editor-comment"><p>In the numerical simulations, the reliability cost is implemented with a single power-law expression (<inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). However, in principle, all the reliability costs will play in conjunction, i.e. reliability cost <inline-formula><mml:math id="sa3m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. While I do recognise that it may be difficult to estimate the biophysical values of the various <italic>ci</italic>, it might be still relevant to comment on this.</p></disp-quote><p>Agreed. Limitations in the literature meant that we could only form a cursory review of the relative scale of each cost using estimates by Atwell, (2001), Engl, (2015). On line 135 we have added a paragraph explaining the rationale for considering each cost independently.</p><disp-quote content-type="editor-comment"><p>(3) In Eq. 8: σ<sup>2</sup> doesn’t depend on variability in <italic>q</italic>, which would add another term; barring algebra mistakes, it’s <inline-formula><mml:math id="sa3m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;=</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>q</mml:mi><mml:msup><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. It seems worth mentioning why you didn’t include it. Can you argue that it’s a small effect?</p></disp-quote><p>Agreed. Ultimately, we dropped this term because we expected it to be small relative to variability in vesicle release, and because it would be difficult to quantify In practice, the variability is believed to be contributed mostly by variability in vesicle release. The primary evidence for this is histograms of EPSP amplitudes which show classic multi-peak structure, corresponding to one, two three etc. EPSPs. Examples of these plots include:</p><p>- “The end-plate potential in mammalian muscle”, Boyd and Martin (1956); Fig. 8.</p><p>- “Structure and function of a neocortical synapse”, Holler-Rickauer et al. (2019); Extended Figure 5.</p><disp-quote content-type="editor-comment"><p>(3) On pg. 7 now pg. 8, when the Hessian is introduced, why not say what it is? Or at least the diagonal elements, for which you just sum up the squared activity. That will make it much less mysterious. Or are we relying too much on the linear model given in App 2? If so, you should tell us how the Hessian was calculated in general. Probably in an appendix.</p></disp-quote><p>With the intention of maintaining the interest of a wide audience we made the decision to avoid a mathematical definition of the Hessian, opting instead for a written definition i.e. line 192 - “<italic>Hii</italic>; the second derivatives of the objective with respect to <italic>wi</italic>.” and later on a schematic (Fig. 4) for how the second derivative can be understood as a measure of curvature and synapse importance. Nonetheless, this review point has made us aware that the estimated Hessian values plotted in Fig. 5a have been insufficiently explained so we have added a reference on line 197 to the appendix section where we show how we estimated the diagonal values of the Hessian.</p><disp-quote content-type="editor-comment"><p>(4) Fig. 5: assuming we understand things correctly, Hessian ∝ |<italic>x</italic>|2. Why also plot _σ_2 versus |<italic>x</italic>|? Or are we getting the Hessian wrong?</p></disp-quote><p>The Hessian is proportional to <inline-formula><mml:math id="sa3m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. If you assume that time steps are small and neurons spike, then <inline-formula><mml:math id="sa3m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="sa3m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. it is difficult to say what timestep is relevant in practice.</p><disp-quote content-type="editor-comment"><p>(5) To get Fig. 6a, did you start with Fig. Appendix 1-figure 4 from Schug et al, and then use <inline-formula><mml:math id="sa3m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, drop the <italic>q</italic>, and put 1 − <italic>p</italic> on the x-axis? Either way, you should provide details about where this came from. It could be in Methods.</p></disp-quote><p>We have modified Fig. 6 to use the same axes as in the original papers.</p><disp-quote content-type="editor-comment"><p>(6) Lines 190-3: “The relationship between input firing rate and synaptic variability was first observed by Aitchison et al. (2021) using data from Ko et al. (2013) (Fig. 6a). The relationship between learning rate and synaptic variability was first observed by Schug et al. (2021), using data from Sjostrom et al. (2003) as processed by Costa et al. (2017) (Fig. 6b).&quot; We believer 6a and 6b should be interchanged in that sentence.</p></disp-quote><p>Thank you. We have switched the text appropriately.</p><disp-quote content-type="editor-comment"><p>(7) What is posterior variance? This seems kind of important.</p></disp-quote><p>This refers to the “posterior variance&quot; obtained using a Bayesian interpretation of the problem of obtaining good synaptic weights (Aitchison et al. 2021). In our particular setting, we estimate posterior variances by setting up the problem as variational inference: see Appendix 4 and 5, which is now referred to in line 390.</p><disp-quote content-type="editor-comment"><p>(8) Lines 244-5: “we derived the relationships between the optimized noise, <italic>σi</italic> and the posterior variable, <italic>σpost</italic> as a function of <italic>ρ</italic> (Fig. 7b;) and as a function of c (Fig. 7c).&quot; You should tell the reader where you derived this. Which is Eq. 68c now 54c. Except you didn’t actually derive it; you just wrote it down. And since we don’t know what posterior variance is, we couldn’t figure it out.</p></disp-quote><p>If <italic>H</italic> is the Hessian of the log-likelihood, and if the prior is negligable relative to the the likelihood, then we get Eq. 69c. We have added a note on this point to the text.</p><disp-quote content-type="editor-comment"><p>(9) We believe Fig. 7a shows an example pair of synapses. Is this typical? And what about Figs. 7b and c. Also an example pair? Or averages? It would be helpful to make all this clear to the reader.</p></disp-quote><p>Fig. 7a shows an illustrative pair of synapses, chosen to best display the relative patterns of variability under energy efficient and Bayesian synapses. We have noted this point in the legend for Fig. 7. Fig. 7bc show analytic relationships between energy efficient and Bayesian synapses, so each line shows a whole continuum of synapses(we have deleted the misleading points at the ends of the lines in Fig. 7bc).</p><disp-quote content-type="editor-comment"><p>(10) The y-axis of Fig 6a refers to the synaptic weight as w while the x-axis refers to the mean synaptic weight as mu. Shouldn’t it be harmonised? It would be particularly nice if both were divided by <italic>µ</italic>, because then the link to Fig. 5c would be more clear.</p></disp-quote><p>We have changed the y-axis label of Fig. 6a from <italic>w</italic> to <italic>µ</italic>. Regarding the normalised variance, we did try this but our Gaussian posteriors allowed the mean to become small in our simulations, giving a very high normalised variance. To remedy this we would likely need to assume a log- posterior, but this was out of scope for the present work.</p><disp-quote content-type="editor-comment"><p>(11) Line 250 (now line 281): “Finally, in the Appendix&quot;. Please tell us which Appendix. Also, why not point out here that the bound is tightest at small <italic>ρ</italic>?</p></disp-quote><p>We have added the reference to the the section of the appendix with the derivation of the biological cost as a bound on the ELBO. We have also referenced the equation that gives the limit of the biological cost as ρ tends to zero.</p><disp-quote content-type="editor-comment"><p>(12) When symbols appear that previously appeared more than about two paragraphs ago, please tell us where they came from. For instance, we spent a lot of time hunting for <italic>ηi</italic>. And below we’ll complain about undefined symbols. Which might mean we just missed them; if you told us where they were, that problem would be eliminated.</p></disp-quote><p>We have added extra references for the symbols in the text following Eq. 69.</p><disp-quote content-type="editor-comment"><p>(13) Line 564, typo (we think): should be σ−2.</p></disp-quote><p>Good spot. This has been fixed.</p><disp-quote content-type="editor-comment"><p>(14) A bit out of order, but we don’t think you ever say explicitly that <italic>r</italic> is the radius of a vesicle. You do indicate it in Fig. 1, but you should say it in the main text as well.</p></disp-quote><p>We have added a note on this to the legend in Fig. 1.</p><disp-quote content-type="editor-comment"><p>(15) Eq. 14: presumably there’s a cost only if the vesicle is outside the synapse? Probably worth saying, since it’s not clear from the mechanism.</p></disp-quote><p>Looking at Pulido and Ryan (2021) carefully, it is clear that they are referring to a cost for vesicles inside the presynaptic side of the synapse. (Importantly, vesciles don’t really exist outside the synapse; during the release process, the vesicle membrane becomes part of the cell membrane, and the contents of the vesicle is ejected into the synaptic cleft).</p><disp-quote content-type="editor-comment"><p>(16) App. 2: why solve for mu, and why compute the trace of the Hessian? Not that it hurts, but things are sort of complicated, and the fewer side points the better.</p></disp-quote><p>Agreed, we have removed the solution for μ, and the trace, and generally rewritten Appendix 2 to clarify definitions, the Hessian etc.</p><disp-quote content-type="editor-comment"><p>(17) Eq. 35: we believe you need a minus sign on one side of the equation. And we don’t believe you defined p(d|w). Also, are you assuming g = partial log p(d|w)/partial w? This should be stated, along with its implications. And presumably, it’s not really true; people just postulate that <italic>p</italic>(<italic>d</italic>|<italic>w</italic>) ∝ exp(−log_loss_)?</p></disp-quote><p>We have replaced p(d|w) with p(y, x|w), and we replaced “overall cost” with log P(y|w, x). Yes, we are also postulating that p(y|w, x) ∝ exp(−log loss), though in our case that does make sense as it corresonds to a squared loss.</p><p>As regards the minus sign, in the orignal manuscript, we had the second derivative of the cost. There is no minus sign for the cost, as the Hessian of the cost at the mode is positive semi-definite. However, once we write the expression in terms of a log-likelihood, we do need a minus sign (as the Hessian of the log-likelihood at a mode is negative semi-definite).</p><disp-quote content-type="editor-comment"><p>(18) Eq. 47 now Eq. 44: first mention of <italic>CBi</italic>;<italic>i</italic>?</p></disp-quote><p>We have added a note describing CB around these equations.</p><disp-quote content-type="editor-comment"><p>(19) The “where&quot; doesn’t make sense for Eqs. 49 and 50; those are new definitions.</p></disp-quote><p>We have modified the introduction of these equations to avoid the problematic “where”.</p><disp-quote content-type="editor-comment"><p>(20) Eq. 57 and 58 are really one equation. More importantly: where does Eq. 58 come from? Is this the <italic>H</italic> that was defined previously? Either way, you should make that clear.</p></disp-quote><p>We have removed the problematic additional equation line number, and added a reference to where H comes from.</p><disp-quote content-type="editor-comment"><p>(21) In Eq. 59 now Eq. 60 aren’t you taking the trace of a scalar? Seems like you could skip this.</p></disp-quote><p>We have deleted this derivation, as it repeats material from the new Appendix 2.</p><disp-quote content-type="editor-comment"><p>(22) Eq. 66 is exactly the same as Eq. 32. Which is a bit disconcerting. Are they different derivations of the same quantity? You should comment on this.</p></disp-quote><p>We have deleted lots of the stuff in Appendix 5 as, we agree, it repeats material from Appendix 2 (which has been rewritten and considerably clarified).</p><disp-quote content-type="editor-comment"><p>(23) Eq. 68 now 54, left column: please derive. we got:</p><p>gai = gradient for weight i on trial <inline-formula><mml:math id="sa3m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>where the second equality came from Eq. 20. Thus<disp-formula id="sa3equ1"><mml:math id="sa3m9"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;=&lt;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>≈&lt;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;&lt;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>=&lt;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>i</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Is that correct? If so, it’s a lot to expect of the reader. Either way, a derivation would</p><p>be helpful.</p></disp-quote><p>We agree it was unnecessary and overly complex, so we have deleted it.</p><disp-quote content-type="editor-comment"><p>(24) App 5–Figure 2: presumably the data for panel b came from Fig. 6a, with the learning rate set to Δw/w? And the data for panel c from Fig. 6b? This (or the correct statement, if this is wrong) should be mentioned.</p></disp-quote><p>Yes, the data for panel c came from Fig. 6b. We have deleted the data in panel b, as there are some subtleties in interpretation of the learning rates in these settings.</p><disp-quote content-type="editor-comment"><p>(25) line 952 now 946: typo, “and the from&quot;.</p></disp-quote><p>Corrected to “and from&quot;.</p></body></sub-article></article>