<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75090</article-id><article-id pub-id-type="doi">10.7554/eLife.75090</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical adaptation to sound reverberation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-168111"><name><surname>Ivanov</surname><given-names>Aleksandar Z</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3139-8870</contrib-id><email>aleksandar.ivanov@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-14601"><name><surname>King</surname><given-names>Andrew J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5180-7179</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-38220"><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2969-7572</contrib-id><email>benjamin.willmore@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-81910"><name><surname>Walker</surname><given-names>Kerry MM</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1043-5302</contrib-id><email>kerry.walker@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-19854"><name><surname>Harper</surname><given-names>Nicol S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7851-4840</contrib-id><email>nicol.harper@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Physiology, Anatomy and Genetics, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75090</elocation-id><history><date date-type="received" iso-8601-date="2021-10-28"><day>28</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-25"><day>25</day><month>05</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-10-29"><day>29</day><month>10</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.10.28.466271"/></event></pub-history><permissions><copyright-statement>© 2022, Ivanov et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Ivanov et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75090-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75090-figures-v2.pdf"/><abstract><p>In almost every natural environment, sounds are reflected by nearby objects, producing many delayed and distorted copies of the original sound, known as reverberation. Our brains usually cope well with reverberation, allowing us to recognize sound sources regardless of their environments. In contrast, reverberation can cause severe difficulties for speech recognition algorithms and hearing-impaired people. The present study examines how the auditory system copes with reverberation. We trained a linear model to recover a rich set of natural, anechoic sounds from their simulated reverberant counterparts. The model neurons achieved this by extending the inhibitory component of their receptive filters for more reverberant spaces, and did so in a frequency-dependent manner. These predicted effects were observed in the responses of auditory cortical neurons of ferrets in the same simulated reverberant environments. Together, these results suggest that auditory cortical neurons adapt to reverberation by adjusting their filtering properties in a manner consistent with dereverberation.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ferret</kwd><kwd>reverberation</kwd><kwd>auditory cortex</kwd><kwd>normative model</kwd><kwd>neurophysiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>King</surname><given-names>Andrew J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/M010929/1</award-id><principal-award-recipient><name><surname>Walker</surname><given-names>Kerry MM</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000769</institution-id><institution>University of Oxford</institution></institution-wrap></funding-source><award-id>Christopher Welch Scholarship</award-id><principal-award-recipient><name><surname>Ivanov</surname><given-names>Aleksandar Z</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The auditory system adapts to the changing acoustics of reverberant environments by temporally shifting the inhibitory tuning of cortical neurons to reduce the effects of reverberation.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Reverberations accompany almost all natural sounds that we encounter and are the reflections of sound off objects in the environment, such as walls, furniture, trees, and the ground (<xref ref-type="bibr" rid="bib32">Huisman and Attenborough, 1991</xref>; <xref ref-type="bibr" rid="bib74">Sakai et al., 1998</xref>). Compared to the original sound, these reflections are attenuated and distorted due to frequency-selective absorption and delayed due to increased path length (<xref ref-type="bibr" rid="bib47">Kuttruff, 2017</xref>).</p><p>Reverberation can be useful, helping us judge room size, sound-source distance, and realism (<xref ref-type="bibr" rid="bib79">Shinn-Cunningham, 2000</xref>; <xref ref-type="bibr" rid="bib90">Trivedi et al., 2009</xref>; <xref ref-type="bibr" rid="bib44">Kolarik et al., 2021</xref>). However, strong reverberation can impair sound-source localization (<xref ref-type="bibr" rid="bib28">Hartmann, 1982</xref>; <xref ref-type="bibr" rid="bib81">Shinn-Cunningham and Kawakyu, 2003</xref>; <xref ref-type="bibr" rid="bib71">Rakerd and Hartmann, 2005</xref>; <xref ref-type="bibr" rid="bib82">Shinn-Cunningham et al., 2005</xref>) and segregation (<xref ref-type="bibr" rid="bib14">Culling et al., 1994</xref>; <xref ref-type="bibr" rid="bib16">Darwin and Hukin, 2000</xref>), pitch discrimination (<xref ref-type="bibr" rid="bib75">Sayles and Winter, 2008</xref>), and speech recognition (<xref ref-type="bibr" rid="bib42">Knudsen, 1929</xref>; <xref ref-type="bibr" rid="bib57">Nábĕlek et al., 1989</xref>; <xref ref-type="bibr" rid="bib25">Guediche et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Houtgast and Steeneken, 1985</xref>). Notably, reverberation can be detrimental for people with hearing impairments, increasing tone detection thresholds and reducing intelligibility of consonants (<xref ref-type="bibr" rid="bib33">Humes et al., 1986</xref>; <xref ref-type="bibr" rid="bib30">Helfer and Wilber, 1990</xref>). It can also impede the effectiveness of auditory prostheses such as hearing aids (<xref ref-type="bibr" rid="bib66">Qin and Oxenham, 2005</xref>; <xref ref-type="bibr" rid="bib64">Poissant et al., 2006</xref>) and substantially reduces the performance of automatic speech recognition devices (<xref ref-type="bibr" rid="bib97">Yoshioka et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Kinoshita et al., 2016</xref>).</p><p>The auditory system has mechanisms to help us cope with reverberation, to the extent that healthy listeners often only directly notice it when it is strong (in environments such as cathedrals). In the presence of mild-to-moderate reverberation, healthy listeners can continue to perform sound localization (<xref ref-type="bibr" rid="bib28">Hartmann, 1982</xref>; <xref ref-type="bibr" rid="bib71">Rakerd and Hartmann, 2005</xref>) and speech and auditory object recognition tasks (<xref ref-type="bibr" rid="bib31">Houtgast and Steeneken, 1985</xref>; <xref ref-type="bibr" rid="bib8">Bradley, 1986</xref>; <xref ref-type="bibr" rid="bib16">Darwin and Hukin, 2000</xref>; <xref ref-type="bibr" rid="bib15">Culling et al., 2003</xref>; <xref ref-type="bibr" rid="bib61">Nielsen and Dau, 2010</xref>). Because it is such a ubiquitous property of natural sounds, these findings highlight the importance, for both normal and impaired hearing, of understanding how the brain copes with reverberation (<xref ref-type="bibr" rid="bib95">Xia et al., 2018</xref>).</p><p>What are the neurophysiological mechanisms that support listening in reverberant environments? Previous studies have examined subcortical processes that facilitate localization of reverberant sounds (<xref ref-type="bibr" rid="bib96">Yin, 1994</xref>; <xref ref-type="bibr" rid="bib52">Litovsky and Yin, 1998</xref>; <xref ref-type="bibr" rid="bib23">Fitzpatrick et al., 1999</xref>; <xref ref-type="bibr" rid="bib85">Spitzer et al., 2004</xref>; <xref ref-type="bibr" rid="bib88">Tollin et al., 2004</xref>; <xref ref-type="bibr" rid="bib63">Pecka et al., 2007</xref>; <xref ref-type="bibr" rid="bib22">Devore et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Kuwada et al., 2012</xref>; <xref ref-type="bibr" rid="bib40">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib11">Brughera et al., 2021</xref>), and how subcortical processing of synthetic periodic sounds is disrupted by reverberation (<xref ref-type="bibr" rid="bib75">Sayles and Winter, 2008</xref>) and partially restored by compensatory mechanisms (<xref ref-type="bibr" rid="bib84">Slama and Delgutte, 2015</xref>). Much less is known about the neural processing of speech and other complex natural sounds in the presence of reverberation. However, converging evidence from electrophysiological recordings in animals (<xref ref-type="bibr" rid="bib68">Rabinowitz et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Moore et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref>) and from human EEG (<xref ref-type="bibr" rid="bib39">Khalighinejad et al., 2019</xref>) and fMRI (<xref ref-type="bibr" rid="bib38">Kell and McDermott, 2019</xref>) studies suggests that representations of sounds that are invariant to non-reverberant background noise emerge at the level of auditory cortex via neuronal adaptation to stimulus statistics (but see also <xref ref-type="bibr" rid="bib53">Lohse et al., 2020</xref>). Auditory cortex may play a similar role in adaptation to reverberation. Indeed, speech and vocalization stimuli reconstructed from auditory cortical responses in awake ferrets more closely resemble their anechoic versions than the reverberant ones, even if the sounds were presented in reverberant environments (<xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref>). Similar results have been found in humans using sound reconstructions from EEG measurements (<xref ref-type="bibr" rid="bib24">Fuglsang et al., 2017</xref>). It remains unclear, however, whether the observed cortical invariance to reverberation can occur in the absence of top-down attention, and through what neural mechanisms this is achieved.</p><p>Here, we addressed these questions by using a model to predict what neural tuning properties would be useful for effective attenuation of reverberation (a normative ‘dereverberation model’). We then test these predictions using neural recordings in the auditory cortex of anesthetized ferrets. More specifically, we made reverberant versions of natural sounds in simulated rooms of different sizes. Next, we trained a linear model to retrieve the clean anechoic sounds from their reverberant versions. Our trained model provided specific predictions for how the brain may achieve this task: with increased reverberation, neurons should adapt so that they are inhibited by sound energy further into the past, and this should occur in a sound frequency-dependent manner. We observed these predicted effects in the responses of auditory cortical neurons to natural sounds presented in simulated reverberant rooms, and show that they arise from an adaptive process. These results suggest that auditory cortical neurons may support hearing performance in reverberant spaces by temporally extending the inhibitory component of their spectrotemporal receptive fields.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Dereverberation model kernels show reverberation-dependent inhibitory fields</title><p>We trained a simple dereverberation model to estimate the spectrotemporal structure of anechoic sounds from reverberant versions of those sounds. The anechoic sounds comprised a rich 10-min-long set of anechoic recordings of natural sound sources, including speech, textures (e.g. running water) and other environmental sounds (e.g. footsteps) (see Sound stimuli and virtual acoustic space). Reverberation in small (3.0 × 0.3 × 0.3m) and large (15 × 1.5 × 1.5m) tunnel-shaped rooms was simulated using the virtual acoustic space simulator Roomsim (<xref ref-type="bibr" rid="bib12">Campbell et al., 2005</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The simulation also modelled the acoustic properties of the head and outer ear by using a ferret head-related transfer function (HRTF, <xref ref-type="bibr" rid="bib76">Schnupp et al., 2001</xref>). The dimensions of the smaller room made it less reverberant (reverberation time: <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 130ms, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 0.78s) than the larger room (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 430ms, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 2.6s).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Dereverberation model.</title><p>(<bold>A</bold>) Virtual acoustic space was used to simulate the sounds received by a ferret from a sound source in a reverberant room for diverse natural sounds. Schematic shows the simulated small room (length (L) = 3m, width (W) = 0.3m, height (H) = 0.3m) used in this study, and the position of the virtual ferret’s head and the sound source (1.5m from the ferret head) within the room. We also used a medium (x2.5 size) and large room (x5). The acoustic filtering by a ferret’s head and ears was simulated by a head-related transfer function (HRTF). (<bold>B</bold>) Schematic of the dereverberation model. The waveform (top left panel) shows a 4s clip of our anechoic recordings of natural sounds. For a given room, simulated room reverberation and ferret HRTF filtering were applied to the anechoic sound using Roomsim (<xref ref-type="bibr" rid="bib12">Campbell et al., 2005</xref>), and the resulting sound was then filtered using a model cochlea to produce a reverberant cochleagram (top right panel). A cochleagram of the anechoic sound was also produced (bottom left panel). For each room, a linear model was fitted to estimate the anechoic cochleagram from the reverberant cochleagram for diverse natural sounds. Each of the 30 kernels in the model was used to estimate one frequency band of the anechoic sound. One such model kernel is shown (middle right panel). Generating the estimated anechoic cochleagram (bottom right panel) involved convolving each model kernel with the reverberant cochleagram, and the mean squared error (MSE) between this estimate and the anechoic cochleagram was minimized with respect to the weights composing the kernels. (<bold>C</bold>) Sample cochleagrams of a 4s sound clip for the anechoic (left panel), small room (middle panel), and large room (right panel) reverberant conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig1-v2.tif"/></fig><p>After the reverberant sounds were generated, they were converted to cochleagrams (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We used a simple ‘log-pow’ cochlear model to produce the cochleagrams, as our recent work suggests that these cochleagrams enable better prediction of cortical responses than more detailed cochlear models (<xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref>). These spectrotemporal representations of the sound approximate the cochlear filtering and resulting representation of the sound at the auditory nerve (<xref ref-type="bibr" rid="bib9">Brown and Cooke, 1994</xref>; <xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref>). Cochleagrams of an example sound clip presented in the anechoic, small and large room conditions are shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</p><p>We trained a dereverberation model to recover the anechoic cochleagram, using either the small or large room cochleagrams as an input (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The dereverberation model was comprised of a set of ‘dereverberation’ kernels, one for each frequency in the anechoic cochleagram (see Model kernels). Each model kernel used the full reverberant cochleagram (up to 200ms in the past) to estimate the current power in the anechoic cochleagram within a single frequency band. This resulted in a set of positive and negative weights in each model kernel. Obtaining the estimated anechoic sounds involved convolution over time between the model kernels and the reverberant cochleagrams, and the model was trained to minimize the difference between this estimate and the original anechoic sound (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The model was trained separately to dereverberate the small and large room cochleagrams. For each room, on a held-out test set, the dereverberation model reduced the difference between the incoming reverberant cochleagram and the anechoic cochleagram (small room mean squared error reduction 26%; large room reduction 20%).</p><p>Three examples of model kernels are shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref> for the large room and the small room, with the anechoic frequency band they estimate indicated at the top. For each model kernel, the excitatory (red) and inhibitory (blue) weights represent spectrotemporal features in the reverberant cochleagrams that are associated with increased or decreased power in the specified frequency band of the anechoic cochleagram, respectively. The majority of the excitatory and inhibitory weights appear localized around a particular frequency, resembling the frequency tuning seen in auditory cortical neurons (<xref ref-type="bibr" rid="bib7">Bizley et al., 2005</xref>). This is expected in our dereverberation model since each kernel aims to estimate the power in a given frequency band of the anechoic cochleagram.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Similar reverberation effects were observed in the dereverberation model kernels and neuronal STRFs.</title><p>(<bold>A</bold>) Example model kernels resulting from the dereverberation model. Three example model kernels are shown, after training on the large (top row) or small (bottom row) room reverberation. The frequency channel which the model kernel is trained to estimate is indicated above each kernel. The color scale represents the weights for each frequency (y-axis) and time (x-axis). Red indicates positive weights (i.e. excitation), and blue indicates negative weights (i.e. inhibition; color bar right). (<bold>B</bold>) Each plot in the top row shows the temporal profile of the excitatory kernel weights for the corresponding example model kernels shown in A. Excitatory temporal profiles were calculated by positively rectifying the kernel and averaging over frequency (the y-axis), and were calculated separately for the small (pink) and large (red) rooms. The center of mass of the excitation, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, is indicated by the vertical arrows, which follow the same color scheme. The bottom row plots the inhibitory temporal profiles for the small (cyan) and large (blue) rooms. Inhibitory temporal profiles were calculated by negatively rectifying the kernel and averaging over frequency. The <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is indicated by the colored arrows. (<bold>C</bold>) Spectrotemporal receptive fields (STRFs) of three example units recorded in ferret auditory cortex, measured for responses to natural sounds in the large room (top row) or small room (bottom row), plotted as for model kernels in A. (<bold>D</bold>) Temporal profiles of the STRFs for the three example units shown in C, plotted as for the model kernels in B.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Model kernels and neuronal STRFs across frequency channels.</title><p>(<bold>A</bold>) Model kernels arranged by the anechoic frequency that they were trained to estimate. For each anechoic frequency, the top row shows the kernel for the large room condition, and the bottom row shows the kernel for the small room condition. In each plot, frequency is on the vertical axis and history on the horizontal. (<bold>B</bold>) Neuronal STRFs arranged by best frequency, the frequency in the STRF with the largest weight. The STRFs of all cortical units with the same best frequency were averaged to produce these plots. Plots are arranged as in A.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Model and neuronal temporal profiles across frequency channels.</title><p>(<bold>A</bold>) Temporal profiles of the excitatory (top rows) and inhibitory (bottom rows) weights of the model kernels shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>, plotted as in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. The estimated anechoic frequency channel is indicated above each pair of plots. The color code is as in <xref ref-type="fig" rid="fig2">Figure 2</xref>: pink = small room excitation; red = large room excitation; cyan = small room inhibition; blue = large room inhibition. The center of mass (<inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) values for the excitation and the inhibition in each room are indicated by the colored arrows. For each anechoic frequency, each temporal profile was normalized by dividing by the maximum value for the excitatory temporal profile of the same room. (<bold>B</bold>) Temporal profiles of the excitatory and inhibitory components of the averaged neuronal STRFs shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>, plotted and normalized as for the model kernels in A.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig2-figsupp2-v2.tif"/></fig></fig-group><p>The model kernels had temporally asymmetric structure, where strongest excitatory weights tended to occur first (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), followed soon after by a longer inhibitory field. These excitatory and inhibitory timings are readily apparent when we plot the frequency-averaged positive and negative kernel weights (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and are a common feature across all kernels in the model (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). This pattern has been commonly observed in the spectrotemporal receptive fields (STRFs) of auditory cortical neurons (<xref ref-type="bibr" rid="bib21">deCharms et al., 1998</xref>; <xref ref-type="bibr" rid="bib51">Linden et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Harper et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Rahman et al., 2019</xref>), so our model qualitatively reproduces the basic frequency tuning and temporal characteristics of auditory cortical neurons.</p><p>Importantly, we can compare the model kernels for the large room with those for the small room. The inhibitory components of the large-room kernels tended to be delayed and longer in duration, relative to the small-room kernels (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In contrast, the temporal profile of the excitatory components was similar for the small and large rooms. We predicted that a comparable shift in inhibitory filtering could play a role in reverberation adaptation in auditory cortical neurons.</p></sec><sec id="s2-2"><title>Auditory cortical neurons have reverberation-dependent inhibitory fields</title><p>To test the predictions of our dereverberation model in vivo, we presented to anesthetized ferrets an 80s subset of the natural sounds in the simulated small and large reverberant rooms (see Sound stimuli and virtual acoustic space). We did this while recording the spiking activity of neurons in the auditory cortex using Neuropixels high-density extracellular microelectrodes (<xref ref-type="bibr" rid="bib36">Jun et al., 2017</xref>; see Surgical procedure). Stimuli were presented as 40s blocks, in which all sounds were in the same reverberant room condition. This allowed neurons to adapt to the reverberation acoustics of the room. We recorded the responses of 2244 auditory cortical units. Of these, the 696 units (160 single units, 23%) that were responsive to the stimuli were used for further analysis (see Spike sorting).</p><p>We estimated the filtering properties of each unit by fitting a separate STRF to the neuronal responses for each reverberant condition. Neuronal STRFs are linear kernels mapping the cochleagram of the sound stimulus to the time-varying firing rate of the neuron (<xref ref-type="bibr" rid="bib87">Theunissen et al., 2001</xref>). The positive regions of an STRF represent sound features whose level is positively correlated with the neuron’s spike rate, providing the ‘excitatory’ part of the receptive field. Similarly, negative regions of the STRF indicate features whose level is negatively correlated with the cortical unit’s spike rate, providing the ‘inhibitory’ receptive field.</p><p>Examples of typical neuronal STRFs are shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, and these can be compared to the model kernel properties of our dereverberation model above (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). As mentioned above, the model kernels show some similarity to the STRFs typically reported for auditory cortical neurons (<xref ref-type="bibr" rid="bib21">deCharms et al., 1998</xref>; <xref ref-type="bibr" rid="bib51">Linden et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Harper et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Rahman et al., 2019</xref>). Likewise, the model kernels show similarity to the STRFs we present here, including having frequency tuning, early excitatory receptive fields and delayed inhibitory receptive fields (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). These consistencies between the general features of our model and neurophysiological responses validated our use of this normative approach to capture neural response properties. We next examined if the model could predict neural adaptation to different reverberant conditions.</p><p>The important prediction we observed in the model was that the inhibitory fields tended to be more delayed and of longer duration in the large-room kernels versus the small-room kernels, whereas the excitatory field remained unchanged. Strikingly, we observed the same pattern in the neuronal STRFs in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. This observation also held across different frequency channels in both the model and the data (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p></sec><sec id="s2-3"><title>Similar effects of reverberation on the inhibitory fields of model kernels and auditory cortical neurons</title><p>Since both the dereverberation model and the neuronal STRFs had structure which varied according to the reverberation condition, we sought to investigate these effects quantitatively. We used two metrics to estimate the temporal dynamics of the inhibitory (and excitatory) components of the model kernels and neuronal STRFs: Center of mass (<inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) and peak time (<inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) (see Quantification of the temporal effects in model kernels and neuronal STRFs). The <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> measured the average temporal delay of the inhibitory (<inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) or excitatory (<inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) components of the model kernels/neuronal STRFs (<xref ref-type="fig" rid="fig2">Figure 2B and D</xref>). The <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is the time at which the maximal inhibition (<inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) or excitation (<inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) occurred.</p><p>For each anechoic frequency channel in the dereverberation model, we calculated the difference between the <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for the kernels in the large room and small room conditions, providing 30 <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> differences (1 for each channel), and did the same for the <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. We plotted the distribution of these differences as histograms in <xref ref-type="fig" rid="fig3">Figure 3A</xref> (see also <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for supplementary statistics for this and other analyses). Similarly, a histogram of the <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> difference between the neuronal STRFs in the large and small room conditions is plotted for 696 cortical units in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. We found that the <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not differ significantly between the small and large rooms, either for model kernels (median <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 0.97ms, Wilcoxon signed-rank test, p = 0.066) or neuronal STRFs (median <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 0.32ms, p = 0.39). In contrast, the <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> showed clear dependence on room size. The inhibitory centers of mass were higher in the larger room for both the model kernels (median <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 7.9ms, p = 1.9 × 10<sup>-6</sup>), and neuronal STRFs (median <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 9.3ms, p = 1.5 × 10<sup>-66</sup>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Increased reverberation produces delayed inhibitory fields in dereverberation model kernels and neuronal STRFs.</title><p>(<bold>A</bold>) Histograms of the difference in center of mass of the temporal profiles (for the inhibitory field, <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, blue; excitatory field, <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, red) of dereverberation model kernels between the two different reverberant conditions (large - small room). The <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> were larger in the larger room, with a median difference = 7.9ms. <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not differ significantly between the rooms (median difference = 1.0ms). (<bold>B</bold>) Center of mass differences, plotted as in A, but for the auditory cortical units. The <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> increased in the larger room (median difference = 9.3ms), while <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> was not significantly different (median difference = 0.3ms). (<bold>C</bold>) Histograms of the large - small room difference in peak time for the temporal profiles of the model kernels (inhibitory, <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, blue; excitatory, <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, red). The <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were larger in the larger room (median difference = 5.3ms), whereas <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were not significantly different (median difference = 0.0ms). (<bold>D</bold>) Peak time differences for neuronal data, plotted as in C. The <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values increased in the larger room (median difference = 9.4ms), while <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not significantly differ between between the two rooms (median difference = 0.0ms). Asterisks indicate the significance of Wilcoxon signed-rank tests: <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Analyses using the Carney Bruce Erfani Zilany (CBEZ) cochlear model.</title><p>In order to explore the effects of using more biologically realistic cochlear models on our findings, we repeated the analyses from <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> using the ‘CBEZ’ cochlear model described in <xref ref-type="bibr" rid="bib10">Bruce et al., 2018</xref> and developed from earlier works by <xref ref-type="bibr" rid="bib99">Zilany et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Zilany et al., 2009</xref>; methods as in <xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref> (referred to as the BEZ model there). (<bold>A-D</bold>) As <xref ref-type="fig" rid="fig2">Figure 2A–D</xref>, but using the CBEZ cochlear model. (<bold>E-G</bold>) As <xref ref-type="fig" rid="fig3">Figure 3A–D</xref>, but using the CBEZ cochlear model. (<bold>E</bold>) Histograms of the difference in center of mass of the temporal profiles (for the inhibitory field, <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, blue; excitatory field, <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, red) of dereverberation model kernels between the two different reverberant conditions (large - small room). The <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> increased in the larger room with a median difference = 10.0ms; <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> decreased slightly in the larger room, median difference = –5.6ms. (<bold>F</bold>) Center of mass differences, plotted as in E, but for the auditory cortical units. The <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> increased in the larger room, median difference = 12.0ms; <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> increased slightly in the larger room, median difference = 2.7ms. (<bold>G</bold>) Histograms of the large - small room difference in peak time for the temporal profiles of the model kernels (inhibitory, <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, blue; excitatory, <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, red). The <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were larger in the larger room, median difference = 21.0ms, whereas <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were not significantly different, median difference = 0.0ms. (<bold>H</bold>) Peak time differences for neuronal data, plotted as in G. The <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values increased in the larger room, median difference = 12.0ms, and <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> also showed a significant change, but the median difference was only 0.3ms. Asterisks indicate the significance of Wilcoxon signed-rank tests: <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>A medium room condition shows intermediate center of mass and peak time values compared to the small and large room conditions.</title><p>(<bold>A</bold>) Violin plots for the center of mass (<inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) of the excitatory fields of the neuronal STRFs for the small, medium and large room conditions computed. (<bold>B</bold>) Same as A, but here the violin plots show the center of mass (<inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) of the inhibitory fields for the neuronal STRFs. (<bold>C</bold>) Violin plots for the peak time of the excitatory fields (<inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>). (<bold>D</bold>) The same data as (<bold>C</bold>) but here the violin plots show the peak time (<inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) of the inhibitory fields. In all violin plots, the white dot represents the median, the horizontal thick line the mean, the thick gray lines the interquartile range, the thin gray lines 1.5 x interquartile range, and the colored shaded area represents the distribution. The results of Kruskal–Wallis tests followed by multiple comparisons using Fisher’s least significant difference (LSD) procedure are indicated above the bars in A, B, and D: <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn><mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn><mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn><mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig3-figsupp2-v2.tif"/></fig></fig-group><p>The results of our analysis of <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> were largely consistent with our <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> findings (<xref ref-type="fig" rid="fig3">Figure 3C and D</xref>). The peak time of the excitatory component (<inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) of model kernels did not differ between the small and large room (median <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 0.0ms, p = 1.0), and neither did the <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> in the neural data (median <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 0.0ms, p = 0.38). The peak time of the inhibitory component, on the other hand, occurred later in the large room, in both the model kernels (median <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 5.3ms, p = 3.7 x 10<sup>-3</sup>) and neuronal STRFs (median <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 9.4ms, p = 4.0 x 10<sup>-44</sup>). We also observed these room-size-dependent delays in the <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> of inhibitory components when we used a more detailed cochlear model (<xref ref-type="bibr" rid="bib10">Bruce et al., 2018</xref>; <xref ref-type="bibr" rid="bib99">Zilany et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Zilany et al., 2009</xref>) to generate input cochleagrams (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>In general, there was more spread in the <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> in the neuronal data compared to the dereverberation model. This is likely because, unlike in the model, which was focused purely on dereverberation, the auditory cortex subserves multiple functions and a diversity of STRF spans is useful for other purposes (e.g. prediction, <xref ref-type="bibr" rid="bib83">Singer et al., 2018</xref>). Despite this, it is notable that the median <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> differences of the dereverberation model were of similar magnitude to those of the neuronal data.</p><p>As our stimulus set described above included only two reverberant rooms, it was not clear if the neurons treated these simulated rooms as two points along an ordered reverberation scale. To further examine whether the timing of the neuronal STRF inhibitory component scales with the amount of reverberation in our simulated room, we added a third ‘medium’ sized room with the same relative proportions and absorption properties as the small and large rooms. We measured auditory cortical responses to this extended stimulus set in 2 ferrets (266 cortical units).</p><p>The <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> measures of neuronal STRF dynamics were compared across the small, medium, and large room conditions (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). As expected, there was little effect of room size on the timing of the excitatory STRF components (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A,C</xref>). The <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> showed a weak but significant overall increase with room size (Kruskal-Wallis test; <inline-formula><mml:math id="inf74"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(2) = 6.4, p = 0.042), but there was no effect of room size on the peak time of excitation, <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf76"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(2) = 1.4, p = 0.50). In post-hoc pairwise comparisons, <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> only differed between the small and medium rooms (Fisher’s least significant differences; large-small: p = 0.21; large-medium: p = 0.21; medium-small: p = 0.012).</p><p>In contrast, and as predicted, we found that the delay of the inhibitory STRF components increased with greater room reverberation. The <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> was generally larger for larger rooms (Kruskal-Wallis test; <inline-formula><mml:math id="inf79"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(2) = 37, p = 7.6 x 10<sup>-9</sup>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). Post-hoc pairwise tests confirmed that <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> differed between all three reverberant conditions (Fisher’s least significant differences; large-small: p = 1.3 x 10<sup>-9</sup>; large-medium: p = 2.0 x 10<sup>-4</sup>; medium-small: p = 0.019). The peak time of STRF inhibition, <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, also increased with room size across all three rooms (<inline-formula><mml:math id="inf82"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(2) = 27, p = 1.6 x 10<sup>-6</sup>; large-small: p = 2.7 x 10<sup>-7</sup>; large-medium: p = 0.0024; medium-small: p = 0.036, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2D</xref>).</p><p>Thus, as room size, and hence reverberation time, was increased, we observed an increase in the delay of inhibition in the tuning properties of auditory cortical neurons. This increase is consistent with a normative model of dereverberation, suggesting that the tuning properties of auditory cortical neurons may adapt in order to dereverberate incoming sounds.</p></sec><sec id="s2-4"><title>Reverberation effects result from an adaptive neural process</title><p>In principle, there could be other reasons, unrelated to adaptation, why the temporal profile of the inhibitory field is delayed and broader in the more reverberant room. An important possibility is that differences in sound statistics between the reverberation conditions could result in different STRFs, even if the underlying neuronal tuning is unchanged. For example, the cochleagrams of more reverberant sounds are more temporally blurred (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This could lead to slower features in the neuronal STRFs for the larger room, purely due to systematic model fitting artefacts (<xref ref-type="bibr" rid="bib13">Christianson et al., 2008</xref>). In combination with changing sound statistics, a non-adaptive static non-linearity in the neural system could produce apparent differences in neuronal tuning between the reverberation conditions (<xref ref-type="bibr" rid="bib13">Christianson et al., 2008</xref>). We therefore performed several additional experiments and analyses to test whether the reverberation-dependent effects observed above are likely to result from a genuine adaptive process.</p><p>As a first test, for each recorded cortical unit, we fitted a simulated linear-nonlinear (LN) model neuron (<xref ref-type="bibr" rid="bib78">Schwartz et al., 2006</xref>), composed of a single STRF (fitted to the combined small and large room stimuli) feeding into a non-linear output function (see Simulated neurons). We assessed fit quality using normalized correlation coefficient, <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib77">Schoppe et al., 2016</xref>), on held-out test data, giving a <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> value of 0.64. Then a non-homogeneous Poisson process was appended to the LN model, to provide an LNP model. The noise in the recorded neuronal responses was close to Poisson (median Fano factor = 1.1). Since this non-linear model captured the spectrotemporal tuning of the cortical units but did not have an adaptive component, we used it to assess whether our reverberation-dependent results could arise from fitting artefacts in a non-adaptive neuron. To do this, we presented the same stimuli to the simulated non-adaptive neurons as we did to the real neural responses and performed the same analyses. Hence, we fitted STRFs to the simulated neural responses separately for the large and small room conditions. We then extracted <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> parameters from the excitatory and inhibitory temporal profiles of these STRFs, and compared them to those of the measured cortical units. The simulated results are shown alongside the neural results in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Simulated neurons suggest a role for adaptation in cortical dereverberation.</title><p>To confirm that STRF differences between rooms were genuinely a result of adaptation, we simulated the recorded neurons using a non-adaptive linear-nonlinear-Poisson model and compared STRF measures of the simulated responses with those of the real neuronal STRFs in the different room conditions. (<bold>A</bold>) The simulated neurons were made in the following way: (1) We fitted a single STRF for each neuron using the combined data from the small and large rooms; (2) We used this STRF along with a fitted non-linearity and a Poisson noise model to generate the simulated firing rate for the small and large rooms separately; (3) Using the small and large room cochleagrams and simulated firing rates, we fitted separate STRFs for the two conditions; (4) We computed the center of mass and peak time metrics as before. (<bold>B</bold>) Difference in center of mass between the large and small room conditions (large - small room) for the simulated neurons. The <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values (blue) were larger in the large room (median difference = 4.0ms, mean difference = 5.1ms), and the <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values (red) were slightly elevated too (median difference = 3.1ms, mean difference = 3.1ms). (<bold>C</bold>) Reproduction of <xref ref-type="fig" rid="fig3">Figure 3B</xref> showing the difference in center of mass of neuronal STRF components between the large and small room conditions (large - small room). The <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values increased in the larger room (median difference = 9.3ms, mean difference = 12.0ms), whereas <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not differ significantly (median difference = 0.32ms, mean difference = 0.59ms). (<bold>D</bold>) For each unit, the center of mass differences shown in B were subtracted from those in C and plotted as the resulting difference of <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> differences (real cortical unit - simulated neuron). The <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> differences between rooms were consistently larger in the neuronal data (median difference = 5.7ms, mean difference = 6.9ms), while the <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> effect was larger in the simulations (median difference = –2.0ms, mean difference = –2.5ms). (<bold>E</bold>) Difference in peak time between the large and small rooms (large - small) for the simulated neurons. The <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> median difference = 6.4ms (mean difference = 13ms) and the <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> median difference = –0.50ms (mean difference = –0.43ms). (<bold>F</bold>) Reproduction of <xref ref-type="fig" rid="fig3">Figure 3D</xref> showing the difference in peak time between the large and small rooms (large - small), calculated from neuronal STRFs. The <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were larger in the large room (median difference = 9.4ms, mean difference = 20.0ms). <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not differ significantly between the rooms (median difference = 0.0ms, mean difference = 3.0ms). (<bold>G</bold>) Histogram of the difference in peak time room differences between the cortical units and corresponding simulated neurons (cortical unit - simulated neuron), plotted as in D above. The <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> shifts were consistently larger in the neuronal data than in the simulated neurons (median difference = 1.1ms, mean difference = 7.4ms). <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, on the other hand, showed larger effects of room size in the simulated data (median difference = 0.95ms, mean difference = 3.5ms). Asterisks indicate the significance of Wilcoxon signed-rank tests: <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comparison of real neurons and non-adapting network receptive field-Poisson (NRFP) simulated neurons.</title><p>To confirm that STRF differences between rooms were genuinely a result of adaptation, we simulated the recorded neurons using a non-adapting NRFP model and compared STRF measures of the simulated responses with those of the real neuronal STRFs in the different room conditions. The simulated neurons were made using a similar process to that described in <xref ref-type="fig" rid="fig4">Figure 4</xref>, with the difference that the linear-nonlinear part (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) was substituted with the NRF model, which has more complex non-linearity. (<bold>A</bold>) Difference in center of mass between the large and small room conditions (large - small room) for the simulated neurons. The <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values (blue) were larger in the large room (mean difference = 5.5ms, median difference = 5.6ms), and the <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values (red) were slightly elevated too (median difference = 3.5ms, mean difference = 3.2ms). (<bold>B</bold>) The center of mass differences between the neuronal data and the simulated NRFP model data were subtracted for each unit and plotted as the resulting difference of differences (real cortical unit - simulated neuron). The <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> differences between rooms were consistently larger in the neuronal data (median difference = 4.7ms, mean difference = 6.3ms), while the <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>C</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>O</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> effects were modestly larger in the NRFP simulations (median difference = –1.9ms, mean difference = –2.8ms). (<bold>C</bold>) Difference in peak time between the large and small rooms (large - small) for the simulated neurons. The <inline-formula><mml:math id="inf105"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> increased from the small to the large room (median difference = 2.1ms, mean difference = 9.7ms) and the <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> showed a more subtle change (median difference = –0.4ms, mean difference = 1.3ms). (<bold>D</bold>) Histogram of the difference in peak time room differences between cortical units and corresponding simulated neurons (cortical unit - simulated neuron), plotted as in B. The <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> room effects were consistently larger in the neuronal data than in the simulated neurons (median difference = 1.7ms, mean difference = 10.0ms). <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>P</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo mathvariant="normal">+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> differences between rooms were small overall, but significantly larger in the simulations (median difference = 0.4ms, mean difference = 1.8ms). Asterisks indicate the significance of Wilcoxon signed-rank tests:<inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig4-figsupp1-v2.tif"/></fig></fig-group><p>We asked whether the shift in inhibition observed in the dereverberation model and neural data was also present in this adaptation-free simulation. In the simulation, the inhibitory <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> was larger for the more reverberant condition (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), but the effect size for the simulated neurons (median <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 4.0ms, p = 9.5 x 10<sup>-42</sup>, Wilcoxon signed-rank test) was less than half of that observed in the real neuronal data (median <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 9.3ms, <xref ref-type="fig" rid="fig4">Figure 4C</xref>). We directly compared the <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> room differences between cortical units and their simulated counterparts (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), and found that the reverberation effects on <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> were consistently larger in the neuronal data (median difference of differences = 5.7ms, mean difference of differences = 6.9ms, p = 7.2 x 10<sup>-29</sup>). An analysis of the peak time of inhibitory STRF components for neural and simulated units was in agreement with the center of mass results (<xref ref-type="fig" rid="fig4">Figure 4E–G</xref>), with the simulations showing inhibitory peak time shifts (median <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 6.4ms, p = 5.9 x 10<sup>-32</sup>) that were more modest than those we observed in the neural data (median difference = 9.4ms, p = 4.0 x 10<sup>-44</sup>). The simulations also showed a modest but significant effect of room size on the excitatory <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values (<xref ref-type="fig" rid="fig4">Figure 4B and E</xref>, median <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 3.1ms, p = 9.0 x 10<sup>-12</sup>; median <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = –0.5ms, p = 6.2 x 10<sup>-8</sup>), which was not observed in the neural data (median <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference between neural and simulated data = –2.0ms, p = 4.9 x 10<sup>-4</sup>, median <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> difference = 0.95ms, p = 6.0 x 10<sup>-8</sup>). When we directly compared the <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> room differences between cortical units and their simulated counterparts (<xref ref-type="fig" rid="fig4">Figure 4G</xref>), we found that the reverberation effects on <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> were consistently larger in the neuronal data (median difference of differences = 1.7ms, mean difference of differences = 10.0ms, p=2.5 x 10<sup>-7</sup>). In summary, these simulations suggest that differences in stimulus properties alone can account for a small shift in inhibitory receptive fields across rooms, but not the magnitude of delay that we observed in our neural data. Therefore, these effects are likely to arise, at least in part, from neural adaptation to room reverberation.</p><p>We also investigated the result of replacing the LN component of the LNP model with a model that has a stronger static non-linearity. We used the network-receptive field (NRF) model, which is essentially a single hidden layer neural network, with sigmoid non-linearities for its 10 hidden units and its single output unit (<xref ref-type="bibr" rid="bib27">Harper et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Rahman et al., 2019</xref>; <xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref>). We assessed fit quality using <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib77">Schoppe et al., 2016</xref>) on held-out test data, comparing this to the performance of the LN model. The NRF fits had a mean <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of 0.64 and showed statistically significant better performance than the LN fits (median <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> difference = 0.016, p = 0.0056, Wilcoxon signed-rank test). We repeated the spike rate simulation analyses with this NRF-Poisson (NRFP) model, keeping all other aspects of the analysis the same as described for the LNP model above. As with the LNP model, the NRFP model could not explain the magnitude of the shift in inhibitory center of mass or peak time seen in the real data (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This suggests that an increased non-linearity alone cannot account for the reverberation adaptation observed in auditory cortex.</p><p>To further confirm that the shift in inhibitory receptive fields arises from neuronal adaptation to reverberation and not to differences in stimulus statistics between the room conditions, we compared how all cortical units in our dataset respond to a probe stimulus (a non-reverberated noise burst) interspersed within the small and large room reverberation stimuli (see Noise burst analysis). If the neurons adapt to the current reverberation condition, we should expect them to respond differently to the noise probe when it occurs within the small room and large room stimuli, reflecting the different adaptation states of the neurons. The neuronal responses to the noise probe showed a similar initial onset excitation (5–25ms) in both conditions, but the return to baseline firing was slower in the large room condition (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). This is consistent with the previous STRF analysis, wherein the excitatory temporal profile was similar between the small and large rooms (<xref ref-type="fig" rid="fig3">Figure 3B and D</xref>), while the inhibitory components were delayed in time in the large room (<xref ref-type="fig" rid="fig3">Figure 3B and D</xref>). For each cortical unit, we compared the center of mass of the noise burst response between the small and large rooms (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> of the noise response increased slightly in the large room (median <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> difference = 1.0ms, p = 0.0063). Therefore, responses to an anechoic probe noise show further evidence for reverberation adaptation in auditory cortical neurons, and are consistent with the predicted delayed inhibition in the presence of increased reverberation.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Adaptation is confirmed by neural responses to a noise probe and to stimuli that switch between the small and large room.</title><p>(<bold>A</bold>) Average firing rate across all cortical units in response to an anechoic noise burst that was embedded within the reverberant stimuli. Responses to the noise within the small (light green) and large (dark green) rooms are plotted separately. Shaded areas show ± SEM across units. The vertical line indicates the noise onset. (<bold>B</bold>) Histogram of the difference in center of mass of the neuronal response to the noise probe (shown in A) between the two room conditions (large - small room). The center of mass shifted to a later time in the larger room (median difference = 1.0ms). Asterisks indicate significance of a Wilcoxon signed-rank test: <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>C</bold>) Schematic shows the structure of the ‘switching’ stimulus, which alternates between the large (dark green) and small room (light green) conditions. Letters indicate the reverberant condition in each stimulus block (S: small room, L: large room). Each 8s block within a given room condition was divided for analysis into an early (S1,L1) and late (S2,L2) period. STRFs were fitted to the data from each of the 4 periods independently (S1, S2, L1, L2). (<bold>D</bold>) Difference in center of mass of inhibitory (<inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, blue) and excitatory (<inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, red) STRF components between the late and early time period of the small room stimuli (S2 - S1, see A). The <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> decreased in S2 relative to S1 with a median difference = –0.9ms; <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not differ significantly, median difference = 0.52ms. (<bold>E</bold>) Center of mass difference plotted as in B, but for the large room stimuli (L2 - L1). The <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were larger in L2 relative to L1, median difference = 1.5ms, while the <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values were not significantly different, median difference = 0.8ms. Asterisks indicate the significance of Wilcoxon signed-rank tests: <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig5-v2.tif"/></fig><p>To further confirm and explore the adaptive basis of our results, we presented our reverberant sounds in blocks, which switched between the small and large room every 8s (see <xref ref-type="fig" rid="fig5">Figure 5C</xref> and Switching stimuli analysis). This switching stimulus was tested in 310 cortical units across 4 ferrets. If the room adaptation accumulates throughout the 8s following a room switch, we would expect the inhibitory component of neuronal STRFs to be decreasingly delayed throughout this period following a switch to the small room and increasingly delayed for a switch to the large room. To test this prediction, we fitted STRFs to neuronal responses separately from the first and last half of each 8s room block, for the small (S1 early and S2 late halves) and large room (L1 early and L2 late halves). The switching stimulus was designed to ensure that the stimulus set of L1 and L2 (or S1 and S2) was the same, but the order of stimuli was shuffled differently for these two time periods. Specifically, we predicted that the neuronal STRFs would have a larger <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> during the L2 than the L1 period, while <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> should remain unchanged. By the same reasoning, in a large-to-small room switch, we expected the <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> to be smaller in S2 than in S1, while <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> should remain similar.</p><p>We observed these predicted trends in our data, as show in <xref ref-type="fig" rid="fig5">Figure 5D and E</xref>. The <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> decreased from S1 to S2 (median difference = –0.9ms, Wilcoxon signed-rank test, p = 0.019), while <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not change across these two periods (median difference = 0.52ms, p = 0.85). In the switch to a large room, <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> increased from the first (L1) to second (L2) half of the block (median difference = 1.5ms, p = 0.0088), while <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> did not change (median difference = 0.8ms, p = 0.35). These results further suggest that auditory cortical receptive fields are genuinely adapting dynamically to the changing reverberant conditions.</p></sec><sec id="s2-5"><title>Neural adaptation helps to remove the effects of reverberation</title><p>The above results indicate that auditory cortical neurons show adaptation that is consistent with a model of room-dependent dereverberation. To further confirm that the neural adaptation we observed promotes reverberation invariance, we measured the similarity of cortical responses to the same natural sounds across different reverberation conditions. This was compared to the LNP model of the cortical units, which lacks adaptation but approximates each unit’s spectrotemporal tuning, output non-linearity, and response variability. We did this for 430 cortical units recorded from 5 ferrets, and included an anechoic room condition. We performed this analysis for three pairs of reverberant conditions: the small room and the anechoic room; the large room and the anechoic room; and the large room and the small room. In all three cases, the real neural responses showed significantly larger correlation coefficients between reverberation conditions than did the simulated neural responses (Wilcoxon signed-rank tests; p&lt;0.0001; <xref ref-type="fig" rid="fig6">Figure 6</xref>). A similar correlation analysis was used to demonstrate cochleagram dereverberation by our normative model (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). These results suggest that the adaptation we observed plays a role in dereverberation by producing neural representations of sounds that are similar across reverberant conditions.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Auditory cortical responses are more reverberation invariant than adaptation-free simulated neural responses.</title><p>Pearson’s correlation coefficient (<inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) was computed between the neural response-over-time (trial-averaged spike count in 10ms time bins) to natural sounds presented in two different reverberant conditions. The correlations for each cortical unit were then compared with the correlation coefficient for the unit’s corresponding LNP model. A positive difference between these correlations indicates that the real neuron is more invariant to reverberation than its LNP simulation, suggesting that adaptation may help in removing the effects of reverberation. (<bold>A-C</bold>) Each histogram plots the distribution over units of difference between the correlation coefficient for the recorded neural response-over-time (<inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and that for the corresponding simulated response-over-time (<inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; LNP simulations as described in <xref ref-type="fig" rid="fig4">Figure 4</xref>). (<bold>A</bold>) <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> difference between recorded and simulated cortical units for the small and anechoic rooms (median difference = 0.016; Z = 6.0; p = 1.5 x 10<sup>-9</sup>). (<bold>B</bold>) <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> difference for the large and anechoic rooms (median difference = 0.012; Z = 6.9; p = 7.2 x 10<sup>-2</sup>). (<bold>C</bold>) <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> difference for the large and small rooms (median difference = 0.036; Z = 13.0; p = 1.0 x 10<sup>-40</sup>). Asterisks indicate the significance of Wilcoxon signed-rank tests: <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>The estimated cochleagrams produced by the dereverberation model are more reverberation invariant than the original cochleagrams.</title><p>To assess reverberation invariance, we measured and compared the correlation coefficients (<inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>s) between corresponding rows of different cochleagrams. (<bold>A</bold>) We define <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:msub><mml:mi mathvariant="normal">e</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the correlation coefficient between row i (i.e. a single frequency channel) of the estimated cochleagram produced by the dereverberation model trained on small room data, and row i of the anechoic cochleagram of the original sound. This is a measure of the similarity of row i of these two cochleagrams. To provide a baseline for comparison, we measure <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the correlation coefficient between row i of the cochleagram of the small room sound, and row i of the anechoic cochleagram. We then plot a histogram of <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for all 30 values of i, corresponding to each of the 30 model kernels. We find that the resulting values are consistently above zero (median difference = 0.067; Z = 4.8; p = 1.7 × 10<sup>-9</sup>), indicating that <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is consistently larger than <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the similarity between the dereverberated cochleagram and the anechoic sound is greater than the similarity between the original echoic cochleagram and the anechoic sound. This suggests that the dereverberation model has successfully removed some effects of reverberation from the input cochleagrams, making its outputs more invariant to reverberation. (<bold>B</bold>) As for A, but for the large room. Median difference = 0.092; Z = 4.8; p = 1.7 × 10<sup>-9</sup>. (<bold>C</bold>) As for A-B, but comparing the <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of small and large room cochleagrams, <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to those of their corresponding model kernel estimates, <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:msub><mml:mi mathvariant="normal">e</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Positive values indicate that the dereverberation model makes the cochleagrams of the two rooms more similar and hence more invariant to reverberation. Median difference = 0.037; Z = 4.8; p = 1.7 × 10<sup>-9</sup>. Asterisks indicate the significance of Wilcoxon signed-rank tests: ****<italic>P</italic>&lt;0.0001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig6-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Frequency dependence of the temporal profile of adaptation</title><p>Reverberation is a frequency-dependent effect, as higher frequencies are usually attenuated by air and surfaces faster than lower ones in natural conditions (<xref ref-type="bibr" rid="bib89">Traer and McDermott, 2016</xref>; <xref ref-type="bibr" rid="bib47">Kuttruff, 2017</xref>). Therefore, we explored whether our dereverberation model and auditory cortical neurons also show frequency-dependent reverberation effects.</p><p><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> plot the reverberation model kernels and neuronal STRFs as a function of their frequency tuning. A visual inspection of these plots reveals that in both the model and the neuronal data, while the temporal spread of the excitatory components stays relatively constant across the preferred frequency, the inhibitory components tend to extend less far back in time as the preferred frequency increases. This temporal narrowing of the inhibitory fields is observed for both the large and the small reverberant rooms. Therefore, the frequency-dependent effects predicted by our dereverberation model are confirmed in our cortical recordings.</p><p>To further examine these frequency-tuning effects, we plotted the excitatory and inhibitory center of mass values (<inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) as a function of the anechoic frequency estimated by the model kernels (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) or the best frequency of the neuronal STRFs, i.e. the sound frequency of the highest weight (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The inhibitory components occurred systematically later in model kernels that were tuned to lower frequencies, in both the small (Pearson’s correlation: <italic>r</italic> = –0.57, p = 0.0037) and large room (<italic>r</italic> = –0.80, p = 2.6 × 10<sup>-6</sup>) simulations. The same correlation between best frequency and <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> was present in the neuronal STRFs (small room: <italic>r</italic> = –0.80, p = 3.0 x 10<sup>-6</sup>; large room: <italic>r</italic> = –0.85, p = 1.6 x 10<sup>-7</sup>). In contrast, the dereverberation model showed a smaller magnitude but significant increase of the excitatory <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> with best frequency (small room: <italic>r</italic> = 0.52, p = 0.0087; large room: <italic>r</italic> = 0.55, p = 0.0049), while there was no relationship between <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and best frequency in the neuronal data (small room: <italic>r</italic> = –0.34, p = 0.1; large room: <italic>r</italic> = –0.25, p = 0.24).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The inhibitory tuning latencies and reverberation times show similar frequency dependence.</title><p>(<bold>A</bold>) Center of mass values (<inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) are plotted against the anechoic frequency channel being estimated, for the excitatory and inhibitory fields of each model kernel for the large room and for the small room. These are color coded as follows: excitatory <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> (large room, <inline-formula><mml:math id="inf168"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, red; small room, <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, pink) and their inhibitory counterparts (<inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, blue; <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, cyan). The dashed lines show a linear regression fit for each room, and the Pearson’s r value for each fit is given at the top of each the plot. (<bold>B</bold>) <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> values are plotted against the best frequency for the neuronal data (sound frequency of highest STRF weight). Each cortical unit was assigned a best frequency and the <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> values measured. The solid lines represent the mean <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> value for each best frequency, the shaded areas show ± SEM; color scheme and other aspects as in A. (<bold>C</bold>) <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> values are plotted as a function of cochlear frequency bands, for the large (dark green) and small (light green) rooms. Linear regression fit (dotted line) was used as in A and B to calculate r. Significance of Pearson’s correlation: <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow/><mml:mrow><mml:mo>∗</mml:mo><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Binaural room impulse responses.</title><p>Spectrograms of the binaural room impulse responses are plotted. (<bold>A</bold>) The left panel shows the left ear impulse response of the small room, while the right panel shows that of the right ear. (<bold>B</bold>) Same as A, but the spectrogram of the left and right ear of the large room impulse responses are shown. In all panels in A and B, the gray scale represents the sound energy in decibels (dB). The spectrograms were created using 5ms windows with 2.5ms overlap.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig7-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig7">Figure 7A and B</xref> also show that the inhibitory components were later in time in the large room than in the small room across the entire best frequency range, for both the dereverberation model and neuronal data. The <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values, on the other hand, were largely overlapping between the two rooms across this frequency range. This is in agreement with our observations that the inhibitory components of the receptive fields shift reliably with room size, while the excitatory components do not.</p><p>The frequency dependence of the inhibitory shift may reflect a frequency dependence in the reverberation acoustics themselves. The decay rate of the power in the impulse response of a reverberant environment depends on sound frequency, and this dependence can change across different environments. However, many man-made and natural environments show a gradual increase in decay rate above about ∼0.5kHz (<xref ref-type="bibr" rid="bib89">Traer and McDermott, 2016</xref>). The decay rate can be measured as the reverberation time <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which is the time necessary for the sound level to decay by 60dB relative to an initial sound impulse (similarly, <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the time necessary for a decay by 10dB). The frequency-dependent <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> values for our small and large rooms are plotted in <xref ref-type="fig" rid="fig7">Figure 7C</xref>. The impulse responses of both rooms exhibited a decrease in <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> values as a function of frequency (Pearson’s correlation, small room: <italic>r</italic> = –0.82, p = 1.1 × 10<sup>-10</sup>, large room: <italic>r</italic> = –0.91, p = 8.0 × 10<sup>-10</sup>). This faster decay for higher frequencies can also be observed in the spectrograms of the impulse responses (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Therefore, the frequency-dependent delay in the inhibitory components of our dereverberation model and cortical STRFs paralleled the <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> frequency profile of the virtual rooms in which the sounds were presented.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we applied a normative modelling approach to ask the question: If a function of the auditory system is to remove reverberation from natural sounds, how might the filtering properties of neurons adapt to achieve this goal? To answer this question, we used a rich dataset of anechoic speech and natural environmental sounds, adding different amounts of reverberation to them. We then trained a linear dereverberation model to remove this reverberation. We constructed our model in such a way that the selectivity (kernels) of the model units after training can be compared to the filtering properties (STRFs) of real auditory cortex neurons in the ferret (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We confirmed the validity of our dereverberation model by showing that it recapitulated known properties of auditory cortical neurons, such as frequency tuning and temporally asymmetric STRFs with excitation followed by inhibition (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Interestingly, our dereverberation model also makes two novel predictions: (1) the inhibitory components of neuronal STRFs should be more delayed in more reverberant conditions (<xref ref-type="fig" rid="fig3">Figure 3</xref>); and (2) the inhibition should occur earlier for higher sound frequencies (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref> , <xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><p>We verified both of these predictions using electrophysiological recordings from ferret auditory cortex neurons, fitting STRFs to neuronal responses to sounds from the same rich dataset, and comparing them to the model kernels. Finally, we used three additional methods – non-adaptive simulated neurons, probe stimuli and switching stimuli – to confirm that the observed changes in the neuronal STRFs are consistent with a truly adaptive dynamic process (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>). Thus, our results suggest that the population of auditory cortex neurons adapt to reverberation by extending their inhibitory field in time in a frequency-dependent manner. This proposed auditory cortical adaptation is summarized in <xref ref-type="fig" rid="fig8">Figure 8</xref>. In the following, we explore these findings in the broader context of previous studies and possible mechanisms for adaptation to reverberation.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Schematic of dereverberation by auditory cortex.</title><p>Natural environments contain different levels of reverberation (illustrated by the left cochleagrams). Neurons in auditory cortex adjust their inhibitory receptive fields to ameliorate the effects of reverberation, with delayed inhibition for more reverberant environments (center). The consequence of this adaptive process is to arrive at a representation of the sound in which reverberation is reduced (right cochleagram).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75090-fig8-v2.tif"/></fig><sec id="s3-1"><title>Auditory cortical neurons adapt their responses to reverberation</title><p>Previous studies have shown that human hearing is remarkably robust to reverberation when listeners discriminate speech and naturalistic sounds (<xref ref-type="bibr" rid="bib31">Houtgast and Steeneken, 1985</xref>; <xref ref-type="bibr" rid="bib8">Bradley, 1986</xref>; <xref ref-type="bibr" rid="bib16">Darwin and Hukin, 2000</xref>; <xref ref-type="bibr" rid="bib15">Culling et al., 2003</xref>; <xref ref-type="bibr" rid="bib61">Nielsen and Dau, 2010</xref>). Our neurophysiological results in the ferret auditory cortex are consistent with such robust representation. We find that neurons recorded in the auditory cortex tend to adapt their responses in a way that is consistent with the computational goal of removing reverberation from natural sounds, even in anesthetized animals. Our results are also in good agreement with a previous study in awake passive listening ferrets, which showed that anechoic speech and vocalizations were more readily decodable from the responses of auditory cortex neurons to echoic sounds than the echoic sounds themselves (<xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref>). A similar study in humans using EEG corroborated these findings, showing speech envelopes reconstructed from neural responses to the reverberated stimuli resembled the original anechoic stimuli more than the echoic input, but only when listeners attended to the sound sources (<xref ref-type="bibr" rid="bib24">Fuglsang et al., 2017</xref>).</p><p>Interestingly, a human MEG study suggests that auditory cortex may contain both reverberant and dereverberated representations of speech in reverberant conditions (<xref ref-type="bibr" rid="bib65">Puvvada et al., 2017</xref>). In addition, <xref ref-type="bibr" rid="bib89">Traer and McDermott, 2016</xref> found that humans were able to discriminate different reverberant conditions well with both familiar and unfamiliar sounds. In line with this, a minority of neurons in our study did not change the timing of their inhibitory responses in different reverberant conditions or showed the opposite effect from our model prediction (i.e. their <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> decreased in the more reverberant room, <xref ref-type="fig" rid="fig3">Figure 3B and D</xref>). Thus, although most cortical neurons adapted to reverberation, it is possible that some of them might carry information about the reverberant environment or even represent it more explicitly. The larger variance in reverberation adaptation across neural units may also result from the fact that neural responses are inherently noisier than our model kernels.</p></sec><sec id="s3-2"><title>Temporal shifts in inhibition underlie adaptation to reverberation</title><p>Our findings build on and provide an explanation for those of <xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref>. These authors approximated a reverberant stimulus by convolving speech and vocalizations with exponentially decaying white noise. In contrast, we used a more diverse stimulus set, which included many environmental sounds that can have very different acoustical statistics (<xref ref-type="bibr" rid="bib5">Attias and Schreiner, 1996</xref>; <xref ref-type="bibr" rid="bib91">Turner, 2010</xref>), and a model of reverberation that included early reflections and their frequency dependence, which are known to have important perceptual effects (<xref ref-type="bibr" rid="bib89">Traer and McDermott, 2016</xref>). <xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref> proposed a combination of subtractive synaptic depression and multiplicative gain change as a potential mechanism for the observed adaptation in their study. However, they acknowledged that other functionally equivalent mechanisms might also be feasible. Notably, their study did not test different echoic conditions with varying amounts of reverberation. Therefore, the time constants of the synaptic depression and gain components in their model were fixed. <xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref> speculated that these time constants might have an important impact in conditions with different amounts of reverberation. This is indeed one of our main novel findings: more reverberant environments require more temporally delayed inhibitory responses within the STRFs of auditory cortical neurons.</p></sec><sec id="s3-3"><title>Adaptation to reverberation is frequency dependent</title><p>Another novel finding of the present study was that the temporal lag of the inhibition was frequency dependent in both the model kernels and neuronal STRFs (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>). For both the small and large rooms, the temporal lag of the inhibition, but not the excitation, approximately tracked the reverberant profile over sound frequency of the acoustic spaces (measured by the reverberation times <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig7">Figure 7</xref>). Natural and man-made environments exhibit certain regularities, and the decline in reverberation over this frequency range is one of them (<xref ref-type="bibr" rid="bib89">Traer and McDermott, 2016</xref>). Future studies could examine whether neurons adapt their responses accordingly to room impulse responses with more unusual reverberation time-frequency profiles.</p><p>The frequency dependence of the delay in inhibition likely relates to some degree to the time constants of mean-sound-level adaptation (<xref ref-type="bibr" rid="bib20">Dean et al., 2008</xref>), which also decrease with frequency in inferior colliculus neurons responding to non-reverberant noise stimuli (<xref ref-type="bibr" rid="bib20">Dean et al., 2008</xref>). A study by <xref ref-type="bibr" rid="bib94">Willmore et al., 2016</xref> found that this frequency dependence of mean-sound-level adaptation may impact cortical responses and is consistent with removing a running average from natural sounds. Hence, the frequency dependence we observe in the present study may to some extent reflect general mechanisms for removing both reverberation and the mean sound level, and may be at least partially inherited from subcortical areas.</p></sec><sec id="s3-4"><title>Possible biological implementations of the adaptation to reverberation</title><p>What might be the basis for the cortical adaptation to reverberation that we have observed? Some plausible mechanisms for altering the inhibitory field include synaptic depression (<xref ref-type="bibr" rid="bib17">David et al., 2009</xref>), intrinsic dynamics of membrane channels (<xref ref-type="bibr" rid="bib1">Abolafia et al., 2011</xref>), hyperpolarizing inputs from inhibitory neurons (<xref ref-type="bibr" rid="bib49">Li et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Natan et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Gwak and Kwag, 2020</xref>), or adaptation inherited from subcortical regions such as the inferior colliculus or auditory thalamus (medial geniculate body) (<xref ref-type="bibr" rid="bib20">Dean et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Devore et al., 2009</xref>; <xref ref-type="bibr" rid="bib94">Willmore et al., 2016</xref>; <xref ref-type="bibr" rid="bib53">Lohse et al., 2020</xref>). Further studies are required to discriminate among these mechanisms, and to determine if the observed reverberation adaptation is subcortical or cortical in origin.</p><p>Hence, it would be important to investigate whether the adaptive phenomenon we have found occurs at subcortical levels too, namely the inferior colliculus and the medial geniculate body. Previous research in the inferior colliculus of rabbits has shown that neural responses to amplitude-modulated noise partially compensate for background noise and, for some neurons, particularly when that noise comes from reverberation (<xref ref-type="bibr" rid="bib84">Slama and Delgutte, 2015</xref>). However, this study only examined one room size, so it did not investigate the temporal phenomenon we observed. <xref ref-type="bibr" rid="bib68">Rabinowitz et al., 2013</xref> found that neurons in the inferior colliculus in ferrets generally adapt less to the addition of non-reverberant background noise than those recorded in auditory cortex. This and other studies indicate that an increase in adaptation to sound statistics from auditory nerve to midbrain to cortex helps to construct noise-invariant sound representations in the higher auditory brain (<xref ref-type="bibr" rid="bib19">Dean et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">Dean et al., 2008</xref>; <xref ref-type="bibr" rid="bib92">Watkins and Barbour, 2008</xref>; <xref ref-type="bibr" rid="bib93">Wen et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Lohse et al., 2020</xref>). However, subcortical adaptation phenomena may be influenced by cortical activity through descending connections (<xref ref-type="bibr" rid="bib72">Robinson et al., 2016</xref>), making it challenging to dissect the neuroanatomical origin of these effects. Similarly, it is possible that reverberation adaptation also becomes more complete as we progress along the auditory pathway.</p></sec><sec id="s3-5"><title>Considerations and future work</title><p>We undertook our electrophysiological recordings in the present study under general anesthesia in order to control for the effects of attention on reverberation adaptation and to facilitate stable recording of neural responses during our large stimulus set. Cortical adaptation to reverberation has been previously observed in awake listeners (<xref ref-type="bibr" rid="bib55">Mesgarani et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Fuglsang et al., 2017</xref>), and we observed adaptive inhibitory plasticity in the anesthetized animal that is also consistent with dereverberation. This indicates that this form of adaptation is at least in part driven by stimulus statistics and can occur independently of activity and feedback from higher auditory areas (<xref ref-type="bibr" rid="bib46">Krom et al., 2020</xref>).</p><p>Previous work has shown no effect of anesthesia on another kind of adaptation, contrast gain control, in either the ferret auditory cortex (<xref ref-type="bibr" rid="bib67">Rabinowitz et al., 2011</xref>) or the mouse inferior colliculus (<xref ref-type="bibr" rid="bib53">Lohse et al., 2020</xref>). Furthermore, <xref ref-type="bibr" rid="bib39">Khalighinejad et al., 2019</xref> found that adaptation to background noise in human auditory cortical responses was similar whether subjects were actively performing speech-in-noise tasks or were distracted by a visual task. There is therefore no a priori reason to expect that cortical adaptation to reverberation should depend on brain state and be substantially different in awake and anesthetized ferrets. Nevertheless, the effects of attention and behavior on auditory cortical STRFs in the ferret are well documented (<xref ref-type="bibr" rid="bib18">David, 2018</xref>). These can manifest, for example, as gain changes and tuning shifts. Considering the importance of reverberation to perception, it would be interesting to explore the effects described here in behaving animals.</p><p>Another point for future research to consider is how our normative model could be further developed. For simplicity and interpretability, we used an elementary linear model. The frequency-dependent suppression observed in our normative model and neuronal receptive fields has relations to the linear frequency-domain approaches to dereverberation used in acoustical engineering (e.g. <xref ref-type="bibr" rid="bib43">Kodrasi et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Krishnamoorthy and Mahadeva Prasanna, 2009</xref>). However, the performance of such linear dereverberation solutions has limitations, such as when the impulse response changes due to the speaker moving through their environment (<xref ref-type="bibr" rid="bib45">Krishnamoorthy and Mahadeva Prasanna, 2009</xref>). There are more complex and powerful models for dereverberation in acoustical engineering, some of which may provide insight into the biology (<xref ref-type="bibr" rid="bib60">Naylor and Gaubitch, 2010</xref>), and these should be explored in future neurobiological studies. Also, in our modelling we were focused on assessing what characteristics of dereverberation model kernels might change under different conditions, not on how the brain learns to make these changes. Hence, we gave our dereverberation model access to the true anechoic sound, something the brain would not have access to. However, there are blind dereverberation models that aim to dereverberate sounds from just one or two microphones, without access to the original anechoic sounds or room impulse response (<xref ref-type="bibr" rid="bib50">Li et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Jeub et al., 2010</xref>). These blind dereverberation models will be particularly useful to compare to biology if we want to explore how the brain learns to perform dereverberation with just two ears. It is also worth considering that the auditory system will be performing other functions in addition to dereverberation and these may be useful to add into a model.</p></sec><sec id="s3-6"><title>Summary</title><p>We have observed in auditory cortical neurons a form of adaptation where the inhibitory component of the receptive fields is delayed as the reverberation time increases in a larger room. This is consistent with the cortex adapting to dereverberate its representation of incoming sounds in a given acoustic space. Dereverberated representations of sound sources would likely be more invariant under different acoustic conditions and thus easier to consistently identify and process, something valuable for any animal’s survival. Reverberation is a ubiquitous phenomenon in the natural world and provides a substantial challenge to the hearing impaired and speech recognition technologies. Understanding the adaptive phenomena of the brain that allow us to effortlessly filter out reverberation may help us to overcome these challenges.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>All animal procedures were approved by the local ethical review committee of the University of Oxford and performed under license from the UK Home Office. Three adult female and four adult male ferrets (<italic>Mustela putorius furo</italic>; Marshall BioResources, UK) were used in the electrophysiology experiments (mean age = 8.4 months; standard deviation = 4.2 months).</p></sec><sec id="s4-2"><title>Surgical procedure</title><p>Terminal electrophysiological recordings were performed on each ferret under general anesthesia. Anesthesia was induced with an intramuscular injection of ketamine (Vetalar; 5mg/kg) and medetomidine (Domitor; 0.02mg/kg), and was maintained with a continuous intravenous infusion of these two drugs in Hartmann’s solution with 3.5% glucose and dexamethasone (0.5mg/ml/hr). The animal was intubated and artificially ventilated with medical O<sub>2</sub>. Respiratory rate, end-tidal CO<sub>2</sub>, electrocardiogram and blood oxygenation were continuously monitored throughout the recording session. Eye ointment (Maxitrol; Alcon, UK) was applied throughout and body temperature was maintained at 36°C–38°C. Atropine (Atrocare; 0.06mg/kg i.m.) was administered every 6hr, or when bradycardia or arrhythmia was observed.</p><p>Once anesthetized, each ferret was placed in a custom-built stereotaxic frame and secured with ear bars and a mouthpiece. After shaving the scalp and injecting bupivacaine (Marcain,&lt;1mg/kg s.c.), the skin was incised and the left temporal muscle removed. A steel holding bar was secured to the skull using dental cement (SuperBond; C&amp;B, UK) and a stainless steel bone screw (Veterinary Instrumentation, UK). A circular craniotomy (10mm diameter) was drilled over the left auditory cortex, and the dura was removed in this region. The brain surface was covered with a solution of 1.25% agarose in 0.9% NaCl, and silicone oil was applied to the craniotomy regularly throughout recording.</p><p>With the ferret secured in the frame, the ear bars were removed, and the ferret and frame were placed in an electrically isolated anechoic chamber for recording. Recordings were then carried out in the left auditory cortex. An Ag/AgCl external reference wire was inserted between the dura and the skull on the edge of craniotomy. A Neuropixels Phase 3 microelectrode probe (<xref ref-type="bibr" rid="bib36">Jun et al., 2017</xref>) was inserted orthogonally to the brain surface through the entire depth of auditory cortex. The cortical area of each penetration was determined based on its anatomical location in the ferret ectosylvian gyrus, the local field potential response latency, and the frequency response area (FRA) shapes of recorded cortical units. Based on these criteria, 95% of the recorded units were either within or on the ventral border of the primary auditory areas (primary auditory cortex, A1, and anterior auditory field, AAF), while the remaining units were located in secondary fields on the posterior ectosylvian gyrus. Following each presentation of the complete stimulus set, the probe was moved to a new location within auditory cortex. Data were acquired at a 30kHz sampling rate using SpikeGLX software (<ext-link ext-link-type="uri" xlink:href="https://github.com/billkarsh/SpikeGLX">https://github.com/billkarsh/SpikeGLX</ext-link>; <xref ref-type="bibr" rid="bib37">Karsh, 2022</xref>) and custom Matlab scripts (Mathworks).</p></sec><sec id="s4-3"><title>Spike sorting</title><p>The recorded signal was processed offline by first digitally highpass filtering at 150Hz. Common average referencing was performed to remove noise across electrode channels (<xref ref-type="bibr" rid="bib54">Ludwig et al., 2009</xref>). Spiking activity was then automatically detected and clustered using Kilosort2 software (<xref ref-type="bibr" rid="bib62">Pachitariu et al., 2016</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/Kilosort2">https://github.com/MouseLand/Kilosort2</ext-link>; <xref ref-type="bibr" rid="bib86">Stringer et al., 2022</xref>). Responses from clusters were manually curated using Phy (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link>; <xref ref-type="bibr" rid="bib6">Bhagat et al., 2022</xref>), and a given cluster was labelled as a single unit if it had a stereotypical spike shape with low variance and its autocorrelation spike histogram showed a clear refractory period. Spikes from a given cluster were often measurable on 4–6 neighboring electrode channels, facilitating the isolation of single units. Only well isolated single units and multi-unit clusters that were responsive to the stimuli (noise ratio &lt;40, <xref ref-type="bibr" rid="bib73">Sahani and Linden, 2003</xref>; <xref ref-type="bibr" rid="bib67">Rabinowitz et al., 2011</xref>) were included in subsequent analyses.</p></sec><sec id="s4-4"><title>Sound presentation</title><p>Stimuli were presented binaurally via Panasonic RP-HV094E-K earphone drivers, coupled to otoscope speculae inserted into each ear canal. The speculae were sealed in place with Otoform (Dreve Otoplastik). The earphones were driven by a System 3 RP2.1 multiprocessor and headphone amplifier (Tucker-Davis Technologies). Sounds were presented at a sampling rate of 48,828Hz. The output response of the earphones was measured using a Brüel &amp; Kjær calibration system with a GRAS 40DP microphone coupled to the end of the otoscope speculae with a silicone tube. An inverse filter was applied to the speaker output to produce a flat spectral response (±3dB) over the stimulus frequency range (200Hz – 22kHz). Sound intensity was calibrated with an Iso-Tech TES-1356-G sound level calibrator.</p></sec><sec id="s4-5"><title>Sound stimuli and virtual acoustic space</title><p>There are two stimulus sets, the set used to train the dereverberation model, and the set played to the ferrets, which was prepared from a subset the sounds used to make the first set. The stimuli used to train the dereverberation model were constructed from a dataset consisting of clips of anechoic sounds containing human speech and other natural sounds, such as cracking branches, footsteps, and running water. Most of the sound clips were recorded in an anechoic chamber using a Zoom H2 or Zoom H4 sound recorder, apart from some that came from the RWCP Sound Scene Database in Real Acoustic Environments (<xref ref-type="bibr" rid="bib58">Nakamura et al., 1999</xref>). The clips varied in duration from 3s to 10s. A portion of the clips from the dataset was concatenated together to make a single stimulus of 600s duration. A 0.25s cosine ramp was applied to the onset and offset of each snippet to avoid clipping artifacts in concatenation. The 600s stimulus was then band-pass filtered from 200Hz – 20kHz using an 8th-order Butterworth filter. We also constructed a held-out test set of 100s duration in the same manner using different examples of the same types of sounds from the dataset.</p><p>Finally, this stimulus was played in a virtual acoustic space (VAS), providing it with reverberation and head-related filtering. We used the ‘Roomsim’ software (<xref ref-type="bibr" rid="bib12">Campbell et al., 2005</xref>) to generate the virtual acoustic space. This software creates a cuboidal room of arbitrary x, y and z dimensions and simulates its acoustic properties for a listener at a particular position and orientation in space, for a sound source at a particular position. The simulations are based on the room-image method (<xref ref-type="bibr" rid="bib3">Allen and Berkley, 1979</xref>; <xref ref-type="bibr" rid="bib29">Heinz, 1993</xref>; <xref ref-type="bibr" rid="bib80">Shinn-Cunningham et al., 2001</xref>). One difference between the standard room-image method and Roomsim is that the latter incorporates the absorption properties of different materials, which can be summarized by their frequency-dependent absorption coefficients. In principle, the amount of reverberation in a room will depend on its size, shape and the material from which the walls are made. For our room simulations, the walls, ceiling, and floor use the frequency-dependent absorption coefficients of stone (<xref ref-type="bibr" rid="bib4">Alvarez-Morales et al., 2014</xref>). We decided to vary the amount of reverberation by changing the room size whilst keeping the other parameters fixed. Four different corridor-shaped rooms were created:</p><list list-type="order"><list-item><p>Anechoic room</p></list-item><list-item><p>Small room (length x width x height, 3m × 0.3m × 0.3m, <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 0.78s)</p></list-item><list-item><p>Medium room (7.5m × 0.75m × 0.75m, <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 1.5s)</p></list-item><list-item><p>Large room (15m × 1.5m × 1.5m, <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> = 2.6s)</p></list-item></list><p>Thus, processing the 600s stimulus for each room provided four 600s stimuli. Note that the anechoic room does not have a clearly defined ‘shape’, having no reflecting walls, ceiling or floor, with the acoustic filtering determined only by the relative orientation and distances of the sound source and receiver. Roomsim simulates the orientation-specific acoustic properties of the receiver’s head and outer ear, represented by the head-related transfer function (HRTF). In all simulations, we used the same ferret HRTF provided from measurements previously made in the lab on a real ferret (from <xref ref-type="bibr" rid="bib76">Schnupp et al., 2001</xref>). The joint filtering properties of the ferret’s HRTF and the room were simulated together by Roomsim to produce a binaural room impulse response (BRIR). The ferret head position and orientation were simulated in the VAS, positioning it 0.15m from the floor, at the midpoint of the room’s width (0.15m for the small, 0.375m for the medium and 0.75m for the large) and 1/4 of the room’s length from one end (0.75m for the small, 1.875m for the medium and 3.75m for the large) and directly facing the opposite end. In all four room conditions, the sound source was positioned at the same height as the ferret’s head (0.15m) and at a distance of 1.5m straight ahead in the direction faced by the ferret (0° azimuth and 0° elevation relative to the ferret’s head). The reverberation time <inline-formula><mml:math id="inf192"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the time necessary for the sound level to decay by 60dB relative to an initial sound impulse, while <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the time for the sound level to decay by 10dB. We measured reverberation time using a cochlear model, as explained in the next section Cochlear model.</p><p>The stimuli presented to the ferrets were constructed from a representative subset of the anechoic natural stimuli used to train the dereverberation model. We cut 40 different snippets of natural sounds, each 2s in duration, from the clips in the datatset. These 2s snippets were concatenated together into two 40s long stimuli. A 0.25s cosine ramp was applied to the onset and offset of each snippet to avoid clipping artifacts in concatenation. The two 40s stimulus blocks were then processed in VAS in exactly the same way as with the modelling stimulus set, for the same small, medium large and anechoic rooms. This provided two 40s blocks for each reverberant condition (a small, medium, large or anechoic room). We played a stimulus set consisting of the anechoic, small, and large room conditions in five animals and a set consisting of the small, medium and large room conditions in two other animals. The 40s blocks were presented in pseudo random order, with ∼5s of silence between blocks. This presentation was repeated ten times, with a different order each time.</p></sec><sec id="s4-6"><title>Cochlear model</title><p>We used a power-spectrogram (‘log-pow’) based model of cochlear processing as described in <xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref>. Briefly, a spectrogram was produced from the sound waveform by taking the power spectrum through a short-time Fourier transform (STFT) using 20ms Hanning windows, with 10ms overlap between adjoining windows. Thus, the time bins were of 10ms duration. The power of adjacent frequency channels was summed using overlapping triangular windows (using code adapted from melbank.m, <ext-link ext-link-type="uri" xlink:href="http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html">http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html</ext-link>) to produce 30 log-spaced frequency channels ranging from 400Hz to 19kHz center frequencies. The resulting power in each channel at each time point was converted to log values and, to avoid log(0), any value below a low threshold (–94dB) was set to that threshold (&gt;99.9% of instances were above this threshold). All analyses involving cochleagrams used the right ear cochleagram, as we recorded from the left auditory cortex.</p><p>We used the cochleagram to measure the frequency-band-specific reverberation times (<inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) shown in <xref ref-type="fig" rid="fig7">Figure 7C</xref>. Our method is similar to that of <xref ref-type="bibr" rid="bib89">Traer and McDermott, 2016</xref>, but for consistency we used our cochlear model rather than theirs. First, we produced an impulse response, the sound induced at the right ear of the ferret in the virtual room by a simple click at the standard source position. Then, we put this impulse response through our cochlear model to generate a cochleagram. Next, for each frequency band in this cochleagram, we fitted a straight line to the plot of the decaying log power output (dB) of the cochleagram over time. Using the slope of this line of best fit, we found the amount of time it took for this output to decay by 60dB for the <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> or by 10dB for the <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. This provided the <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>10</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for each frequency band. We measured the overall <inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of each room by taking the median <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>60</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> over all 30 frequency bands.</p></sec><sec id="s4-7"><title>Model kernels</title><p>The dereverberation model consisted of a set of linear kernels, one for each of the 30 frequency channels in the anechoic cochleagram. The kernels were fitted separately for each reverberant condition, thus providing 30 kernels for each room. The dereverberation model is summarized by the following equation:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf202"><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>anech</mml:mi></mml:msubsup></mml:math></inline-formula> is the estimate of the anechoic cochleagram for frequency channel <inline-formula><mml:math id="inf203"><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> and time bin <inline-formula><mml:math id="inf204"><mml:mi>t</mml:mi></mml:math></inline-formula>. Obtaining <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> involved convolving the kernels <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with the reverberant cochleagram <inline-formula><mml:math id="inf207"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>reverb</mml:mi></mml:msubsup></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf208"><mml:mi>f</mml:mi></mml:math></inline-formula> is the frequency channel in the reverberant cochleagram and <inline-formula><mml:math id="inf209"><mml:mi>h</mml:mi></mml:math></inline-formula> indexes the time lag used in the convolutions. The model weights <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are composed of 30 kernels, one for each frequency channel <inline-formula><mml:math id="inf211"><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> in the anechoic cochleagram. Finally, the bias term for frequency channel <inline-formula><mml:math id="inf212"><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> is <inline-formula><mml:math id="inf213"><mml:msub><mml:mi>b</mml:mi><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:math></inline-formula>.</p><p>For each anechoic frequency channel <inline-formula><mml:math id="inf214"><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>, the associated model kernel was separately fitted to minimize the mean squared error between the kernel’s estimate of that frequency channel of the anechoic cochleagram <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and that actual channel of the anechoic cochleagram <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, subject to <italic>L</italic><sub>2</sub> regularization (‘ridge’ regression) on <inline-formula><mml:math id="inf217"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The weights were fitted using the glmnet package (GLM, J. Qian, T. Hastie, J. Friedman, R. Tibshirani, and N. Simon, Stanford University, Stanford, CA; <ext-link ext-link-type="uri" xlink:href="http://web.stanford.edu/~hastie/glmnet_matlab/index.html">http://web.stanford.edu/~hastie/glmnet_matlab/index.html</ext-link>). To select the regularization strength (the hyperparameter <inline-formula><mml:math id="inf218"><mml:mi>λ</mml:mi></mml:math></inline-formula>), we performed 10-fold cross-validation, using 90% of the data for the training set and 10% (an unbroken 60s segment) for the validation set. Our validation sets over folds were non-overlapping. We found the <inline-formula><mml:math id="inf219"><mml:mi>λ</mml:mi></mml:math></inline-formula> that gave the lowest mean-squared error averaged over the 10 folds. Using this <inline-formula><mml:math id="inf220"><mml:mi>λ</mml:mi></mml:math></inline-formula>, we then re-fitted the model kernels using the whole cross-validation set (training + validation set). These resulting kernels are the ones shown and used in all analyses. These kernels were also used to estimate the dereverberation capacity of the model on the held-out test set. Note that here onward we typically refer to individual model kernels by <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for brevity, dropping the <inline-formula><mml:math id="inf222"><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> index used for the full set of kernels <inline-formula><mml:math id="inf223"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Neuronal STRFs</title><p>For each cortical unit, for each reverberation condition, we separately estimated its spectro-temporal receptive field (STRF) using its response to the natural stimuli under that condition (<xref ref-type="bibr" rid="bib87">Theunissen et al., 2001</xref>). We used the STRF, a linear model, as this enabled comparison to our linear dereverberation model. The STRF of a cortical unit was the best linear fit from the cochleagram of the stimuli to the unit’s response-over-time <inline-formula><mml:math id="inf224"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. This response-over-time <inline-formula><mml:math id="inf225"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the spike counts of cortical unit <inline-formula><mml:math id="inf226"><mml:mi>n</mml:mi></mml:math></inline-formula> over time bins <inline-formula><mml:math id="inf227"><mml:mi>t</mml:mi></mml:math></inline-formula> and was made by counting the spikes in the same 10ms time bins used in the cochleagram and averaging over the 10 stimulus repeats. The STRF model can be summarized by the following equation:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf228"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the estimated spike counts of cortical unit <inline-formula><mml:math id="inf229"><mml:mi>n</mml:mi></mml:math></inline-formula> at time bin <inline-formula><mml:math id="inf230"><mml:mi>t</mml:mi></mml:math></inline-formula>. Also, <inline-formula><mml:math id="inf231"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>reverb</mml:mi></mml:msubsup></mml:math></inline-formula> is the reverberant cochleagram in frequency channel <inline-formula><mml:math id="inf232"><mml:mi>f</mml:mi></mml:math></inline-formula> and at time <inline-formula><mml:math id="inf233"><mml:mi>t</mml:mi></mml:math></inline-formula>. For each unit <inline-formula><mml:math id="inf234"><mml:mi>n</mml:mi></mml:math></inline-formula>, the weights in <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over frequency channel <inline-formula><mml:math id="inf236"><mml:mi>f</mml:mi></mml:math></inline-formula> and history (time lag) index <inline-formula><mml:math id="inf237"><mml:mi>h</mml:mi></mml:math></inline-formula> provide its STRF. Finally, <italic>b</italic><sub><italic>n</italic></sub> is the bias term of unit <inline-formula><mml:math id="inf238"><mml:mi>n</mml:mi></mml:math></inline-formula>.</p><p>Notice the similarity of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> of the dereverberation model. In both cases, we used the reverberant cochleagram as an input (from either the small, medium, or large room) and fitted the best linear mapping to the output. In the case of neuronal STRFs, the output is the neuronal spike count over time, whereas in the model kernel it is a frequency channel of the anechoic cochleagram. For each cortical unit and room, we separately fitted an STRF by minimizing the mean squared error between the estimated spike counts <inline-formula><mml:math id="inf239"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the observed spike counts <inline-formula><mml:math id="inf240"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. To do this, for a given room, we used the first 36s of neural response to the two 40s-stimuli associated with that room (as the last 4s contained a noise probe, see subsection Noise burst analysis). The weights were fitted using the glmnet package (GLM, J. Qian, T. Hastie, J. Friedman, R. Tibshirani, and N. Simon, Stanford University, Stanford, CA; <ext-link ext-link-type="uri" xlink:href="http://web.stanford.edu/~hastie/glmnet_matlab/index.html">http://web.stanford.edu/~hastie/glmnet_matlab/index.html</ext-link>). As for the model kernels (above), the fitting was subject to <italic>L</italic><sub>2</sub> regularization. To select the regularization strength (the hyperparameter <inline-formula><mml:math id="inf241"><mml:mi>λ</mml:mi></mml:math></inline-formula>), we performed 10-fold cross-validation, using 90% of the data for the training set and 10% (an unbroken 7.2s segment) for the validation set. Our validation sets over folds were non-overlapping. We found the <inline-formula><mml:math id="inf242"><mml:mi>λ</mml:mi></mml:math></inline-formula> that gave the lowest mean-squared error averaged over the 10 folds. Using this <inline-formula><mml:math id="inf243"><mml:mi>λ</mml:mi></mml:math></inline-formula>, we then re-fitted the STRFs using the whole cross-validation set (training + validation set). The resulting STRFs are the ones shown and used in all analyses. As with the model kernels, from here onwards we typically refer to an individual STRF for a given cortical unit by the form <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for brevity, dropping the unit index <inline-formula><mml:math id="inf245"><mml:mi>n</mml:mi></mml:math></inline-formula> used here in <inline-formula><mml:math id="inf246"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-9"><title>Quantification of the temporal effects in model kernels and neuronal STRFs</title><p>To quantify the temporal profiles of the model kernels and neuronal STRFs, we chose two different measures:</p><list list-type="order"><list-item><p>Center of mass (<inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>)</p></list-item><list-item><p>Peak time (<inline-formula><mml:math id="inf248"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>)</p></list-item></list><p>To compute them, we first obtained the averaged excitatory and inhibitory temporal profiles of the model kernels/neuronal STRFs as follows:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf249"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the neuronal STRF with <inline-formula><mml:math id="inf250"><mml:mi>f</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf251"><mml:mi>h</mml:mi></mml:math></inline-formula> subscripts denoting frequency channel and history, respectively. <xref ref-type="disp-formula" rid="equ3 equ4">Equation 3 and 4</xref> are the same for the dereverberation model kernels but with <inline-formula><mml:math id="inf252"><mml:mi>k</mml:mi></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf253"><mml:mi>w</mml:mi></mml:math></inline-formula>, as with all subsequent equations in this section. <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of frequencies (30) in the model kernel/neuronal STRF <inline-formula><mml:math id="inf255"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The notation <inline-formula><mml:math id="inf256"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf257"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula> stand for the element-wise operations max(<inline-formula><mml:math id="inf258"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>,0) and min(<inline-formula><mml:math id="inf259"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>,0), that is:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mtext> if </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mtext> otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mtext> if </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mtext> otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, <inline-formula><mml:math id="inf260"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf261"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msubsup></mml:math></inline-formula> are the frequency-averaged positive-only, <inline-formula><mml:math id="inf262"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, and negative-only, <inline-formula><mml:math id="inf263"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula>, parts of the kernel/STRF <inline-formula><mml:math id="inf264"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>From this, the <inline-formula><mml:math id="inf265"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> was defined as follows:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>The duration of a time bin is <inline-formula><mml:math id="inf266"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>ms, hence time lag in the history of the kernel/STRF ranges from <inline-formula><mml:math id="inf267"><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>ms to <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>195</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>ms. Thus, <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the temporal center of mass for the positive (excitatory) components of the kernel/STRF and <inline-formula><mml:math id="inf270"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> the temporal center of mass for the negative (inhibitory) components.</p><p>The peak time (<inline-formula><mml:math id="inf271"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) was defined as the time at which the excitation and inhibition in the frequency averaged neuronal STRFs/model kernels peaked. Due to the discrete nature of the peak time measure (it can only be a multiple of the time bin size), when measuring it we applied an interpolation (<xref ref-type="bibr" rid="bib2">Akima, 1978</xref>) with a factor of 100 to <inline-formula><mml:math id="inf272"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf273"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msubsup></mml:math></inline-formula> in order to obtain a smoother estimate of peak times. <inline-formula><mml:math id="inf274"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> was taken as the time in <inline-formula><mml:math id="inf275"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> at which the maximum value occurred, and likewise, <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> was taken as the time in <inline-formula><mml:math id="inf277"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msubsup></mml:math></inline-formula> at which the minimum value occurred.</p></sec><sec id="s4-10"><title>Simulated neurons</title><p>In order to explore whether the changes that we observed are truly adaptive, we used simulated neurons that lacked adaptive receptive fields to generate responses. We then applied the same analyses to these simulated neuronal responses as we did to the actual responses. For each real cortical unit <inline-formula><mml:math id="inf278"><mml:mi>n</mml:mi></mml:math></inline-formula>, we constructed a corresponding simulated neuron as a linear-nonlinear-Poisson (LNP) model in the following way. First, we fitted a single STRF as described in section Neuronal STRFs. However, in this case we used the full dataset from the “small” and “large” conditions together, rather than fitting separate STRFs to the two conditions as we did previously.</p><p>Next, we fitted a sigmoid output non-linearity by first generating a spike count prediction <inline-formula><mml:math id="inf279"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the full dataset according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> from section Neuronal STRFs, using this single STRF and then finding the sigmoid that best fits (minimizes mean-squared error) the actual spike count <inline-formula><mml:math id="inf280"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> according to the following equation:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>nonlin</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf281"><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>nonlin</mml:mi></mml:msubsup></mml:math></inline-formula> is the output of the point non-linearity at time bin <inline-formula><mml:math id="inf282"><mml:mi>t</mml:mi></mml:math></inline-formula>, providing a new estimate of the cortical unit’s spike count. As mentioned, <inline-formula><mml:math id="inf283"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the predicted spike count from the linear stage (see <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) at time bin <inline-formula><mml:math id="inf284"><mml:mi>t</mml:mi></mml:math></inline-formula>, when fitted to the small and large room responses together. It is the four parameters <inline-formula><mml:math id="inf285"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf286"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf287"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf288"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula> that are optimized in the fit.</p><p>We then used the fitted simulated model to produce an approximation of the real neuronal response-over-time to the reverberant stimulus sets for both the small and large conditions. In order to simulate realistic neuronal noise, we used the <inline-formula><mml:math id="inf289"><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>nonlin</mml:mi></mml:msubsup></mml:math></inline-formula> output, at each time bin <inline-formula><mml:math id="inf290"><mml:mi>t</mml:mi></mml:math></inline-formula>, as the mean of a Poisson distribution from which we generated 10 ‘virtual’ trials, over which we then averaged to provide the simulated response-over-time. Finally, we performed the same analyses on these simuluated neural responses as we did for the real data; we fitted STRFs for the two reverberation conditions separately using these simulated responses in place of the actual responses and then analyzed the resulting STRFs as outlined in the section above (Quantification of the temporal effects in model kernels and neuronal STRFs).</p><p>Additionally, we repeated the analysis with the simulated neurons, but replacing the LNP model for each simulated neuron with a network receptive field (NRF) model (<xref ref-type="bibr" rid="bib27">Harper et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Rahman et al., 2019</xref>; <xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref>) with an appended inhomogeneous Poisson process, to produce an NRFP model. Specifically, we used the NRF model from <xref ref-type="bibr" rid="bib70">Rahman et al., 2020</xref>, which is essentially a single hidden layer neural network with a sigmoid nonlinearity on the hidden units, the only difference from that model being that we used 10 hidden units. We fitted the NRF model to the neural data by minimizing mean squared error, subject to L1 regularization on the weights. We set the regularization strength using 10-fold cross-validation, as we did for the STRFs in the LN model. For each neuron, after fitting the NRF, it was used as input to an inhomogeneous Poisson process. This was used to simulate neural activity, which was then analyzed in exactly the same way as the LNP model simulated activity.</p></sec><sec id="s4-11"><title>Noise burst analysis</title><p>To further confirm the adaptive change in properties of neurons across the two reverberant conditions, we presented a 500ms long unreverberated broadband noise burst embedded at a random time in the last 4s of each 40s sound block (i.e. from 36 to 40s) for each condition (small and large). Seven out of the 10 repeats of any stimulus block contained a noise burst, with those seven randomly shuffled within the ten. The random timing distribution of the noise bursts was uniform and independent across repeats and conditions. For each cortical unit, its response to the noise burst was assessed using a peristimulus time histogram (PSTH) with 10ms time bins. For the majority of units, the firing rate had returned to baseline by 100ms, so we decided to use the 0–100ms time window for further analysis (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Different neurons had different response profiles, so in order to compare the adaptive properties in the two conditions we chose the center of mass (<inline-formula><mml:math id="inf291"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) of the firing rate profile within this window as a robust measure. This was defined similarly to the <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> measure in subsection Quantification of the temporal effects in model kernels and neuronal STRFs (see also <xref ref-type="disp-formula" rid="equ7 equ8">Equations 7 and 8)</xref>. The <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> for the noise bursts in the large and small conditions was calculated for each cortical unit individually and the difference between the two conditions computed (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p></sec><sec id="s4-12"><title>Switching stimuli analysis</title><p>In order to confirm and explore the adaptive nature of the neuronal responses to reverberant sounds, we presented “switching stimuli” (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). These stimuli switched back and forth every 8s between the large room and the small room and were created in the following way. First, we took our original reverberant stimuli for both the small room (80s duration) and large room (80s duration) conditions and divided them into consecutive 4s snippets, providing 20 snippets for each condition. We duplicated these two sets and shuffled each one independently, providing a total of four sets of 20 4s-long snippets. We then combined the snippets into eight 40s-long switching stimuli. These switching stimuli comprised 5 epochs of 8s duration each, with 4 ‘switches’ between the small and large epochs. Half of the stimuli started from the large room condition and the other half from the small room condition. Within each 8s epoch, we defined two periods (period 1: 0–4s and period 2: 4–8s). The large-room periods were denoted by L1 (0–4s) and L2 (4–8s), and the small-room periods by S1 (0–4s) and S2 (4–8s) (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). The snippets from the first small-room set of 20 snippets populated the 20 S1 periods in order, while those from the second small-room set populated the S2 periods in a different order, due to the shuffling. Likewise, snippets from the first large-room set of 20 snippets populated the 20 L1 periods, and those from the second large-room set populated the L2 periods. Thus, the same set of stimuli were included in S1 and S2, and in L1 and L2, with the only differences being their ordering, and between the small and large room stimuli the amount of reverberation. When the 4s periods and 8s epochs were spliced together, they were cross-faded into each other with a 10ms cosine ramp with 5ms overlap, such that the transition from one period to the next was smooth with no detectable clicks between them. We played the eight 40s stimuli in random order to the ferrets; this was repeated 10 times with the order different each time.</p><p>The cortical responses recorded with these stimuli were analyzed using the procedure outlined in subsection Neuronal STRFs. For each cortical unit, we fitted four separate STRFs using the neural responses to the S1, S2, L1, and L2 periods. We did not use the first 8s of each of the eight 40s stimuli, since there was no prior sound (silence) and thus they would not be directly comparable to the other 4 epochs. We also did not use the first 500ms of any of the periods, to avoid potential non-reverberation-related responses from the rapid transitions between them. From the resulting four STRFs, we extracted the <inline-formula><mml:math id="inf294"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> values for each and compared S1 to S2, and L1 to L2 (<xref ref-type="fig" rid="fig6">Figure 6D, E</xref>).</p></sec><sec id="s4-13"><title>Code availability</title><p>We have provided our Matlab scripts for generating our figures on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/PhantomSpike/DeReverb">https://github.com/PhantomSpike/DeReverb</ext-link>; <xref ref-type="bibr" rid="bib34">Ivanov, 2021</xref>. Our neural spiking data are available to download from Dryad: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1c59zw3xv">https://doi.org/10.5061/dryad.1c59zw3xv</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Senior editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Funding acquisition, Project administration, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Supervision, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Investigation, Methodology, Supervision, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Supervision, Visualization, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The animal procedures were approved by the University of Oxford Committee on Animal Care and Ethical Review and were carried out under license from the UK Home Office, in accordance with the Animals (Scientific Procedures) Act 1986 and in line with the 3Rs. Project licence PPL 30/3181 and PIL l23DD2122. All surgery was performed under general anesthesia (ketamine/medetomidine) and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Supplementary statistics tables, providing further details of all statistical tests described in this article.</title></caption><media xlink:href="elife-75090-supp1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-75090-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>We have provided our Matlab scripts for generating our model and figures on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/PhantomSpike/DeReverb">https://github.com/PhantomSpike/DeReverb</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:536396410cd39aa8a8113c6fef135de3c5404bfe;origin=https://github.com/PhantomSpike/DeReverb;visit=swh:1:snp:0454203068c52587758008b9504b8b94852ba7db;anchor=swh:1:rev:83237aa910b696cb82f4a08d50318eaca7e213e9">swh:1:rev:83237aa910b696cb82f4a08d50318eaca7e213e9</ext-link>).</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ivanov</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Willmore</surname><given-names>BD</given-names></name><name><surname>Walker</surname><given-names>KM</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Cortical adaptation to sound reverberation</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/PhantomSpike/DeReverb">PhantomSpike/DeReverb</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Ivanov</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Willmore</surname><given-names>BD</given-names></name><name><surname>Walker</surname><given-names>KM</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Cortical adaptation to sound reverberation</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.1c59zw3xv</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Dr Quentin Gaucher for assistance with these electrophysiological experiments. We thank Zlatina Dimitrova for her artwork in Figure 1. We are also grateful to Dr Monzilur Rahman for providing code and advice on analysis. This work was supported by a Wellcome Principal Research Fellowship to AJK (WT108369/Z/2015/Z), a BBSRC New Investigator Award (BB/M010929/1) to KMMW, and a Christopher Welch Scholarship (University of Oxford) to AZI.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abolafia</surname><given-names>JM</given-names></name><name><surname>Vergara</surname><given-names>R</given-names></name><name><surname>Arnold</surname><given-names>MM</given-names></name><name><surname>Reig</surname><given-names>R</given-names></name><name><surname>Sanchez-Vives</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical auditory adaptation in the awake rat and the role of potassium currents</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>977</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq163</pub-id><pub-id pub-id-type="pmid">20851851</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akima</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A Method of Bivariate Interpolation and Smooth Surface Fitting for Irregularly Distributed Data Points</article-title><source>ACM Transactions on Mathematical Software</source><volume>4</volume><fpage>148</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1145/355780.355786</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>JB</given-names></name><name><surname>Berkley</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Image method for efficiently simulating small‐room acoustics</article-title><source>The Journal of the Acoustical Society of America</source><volume>65</volume><fpage>943</fpage><lpage>950</lpage><pub-id pub-id-type="doi">10.1121/1.382599</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez-Morales</surname><given-names>L</given-names></name><name><surname>Zamarreño</surname><given-names>T</given-names></name><name><surname>Girón</surname><given-names>S</given-names></name><name><surname>Galindo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A methodology for the study of the acoustic environment of Catholic cathedrals: Application to the Cathedral of Malaga</article-title><source>Building and Environment</source><volume>72</volume><fpage>102</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.buildenv.2013.10.015</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Attias</surname><given-names>H</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Temporal Low-Order Statistics of Natural Sounds</source><publisher-name>In NIPS</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bhagat</surname><given-names>J</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Hunter</surname><given-names>M</given-names></name><name><surname>Gestes</surname><given-names>C</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Spacek</surname><given-names>M</given-names></name><name><surname>Wallace</surname><given-names>M</given-names></name><name><surname>Nolan</surname><given-names>C</given-names></name><name><surname>Buccino</surname><given-names>A</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Czuba</surname><given-names>T</given-names></name><name><surname>Shaheen</surname><given-names>L</given-names></name><name><surname>Minkowicz</surname><given-names>S</given-names></name><collab>The Cortical Processing Laboratory at UCL</collab><collab>ycanerol</collab><collab>The Gitter Badger</collab><collab>szapp</collab></person-group><year iso-8601-date="2022">2022</year><data-title>phy</data-title><version designator="8166fbe">8166fbe</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Nodal</surname><given-names>FR</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional organization of ferret auditory cortex</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>15</volume><fpage>1637</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi042</pub-id><pub-id pub-id-type="pmid">15703254</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Speech intelligibility studies in classrooms</article-title><source>The Journal of the Acoustical Society of America</source><volume>80</volume><fpage>846</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1121/1.393908</pub-id><pub-id pub-id-type="pmid">3760338</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>GJ</given-names></name><name><surname>Cooke</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Computational auditory scene analysis</article-title><source>Computer Speech &amp; Language</source><volume>8</volume><fpage>297</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1006/csla.1994.1016</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>IC</given-names></name><name><surname>Erfani</surname><given-names>Y</given-names></name><name><surname>Zilany</surname><given-names>MSA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A phenomenological model of the synapse between the inner hair cell and auditory nerve: Implications of limited neurotransmitter release sites</article-title><source>Hearing Research</source><volume>360</volume><fpage>40</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.12.016</pub-id><pub-id pub-id-type="pmid">29395616</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brughera</surname><given-names>A</given-names></name><name><surname>Mikiel-Hunter</surname><given-names>J</given-names></name><name><surname>Dietz</surname><given-names>M</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Auditory Brainstem Models: Adapting Cochlear Nuclei Improve Spatial Encoding by the Medial Superior Olive in Reverberation</article-title><source>Journal of the Association for Research in Otolaryngology: JARO</source><volume>22</volume><fpage>289</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1007/s10162-021-00797-0</pub-id><pub-id pub-id-type="pmid">33861395</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>DR</given-names></name><name><surname>Palomaki</surname><given-names>KJ</given-names></name><name><surname>Brown</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A MATLAB simulation of&quot; shoebox&quot; room acoustics for use in research and teaching</article-title><source>Computing and Information Systems Journal</source><volume>9</volume><fpage>48</fpage><lpage>51</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christianson</surname><given-names>GB</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The consequences of response nonlinearities for interpretation of spectrotemporal receptive fields</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>446</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1775-07.2007</pub-id><pub-id pub-id-type="pmid">18184787</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culling</surname><given-names>JF</given-names></name><name><surname>Summerfield</surname><given-names>Q</given-names></name><name><surname>Marshall</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Effects of simulated reverberation on the use of binaural cues and fundamental-frequency differences for separating concurrent vowels</article-title><source>Speech Communication</source><volume>14</volume><fpage>71</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/0167-6393(94)90058-2</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culling</surname><given-names>JF</given-names></name><name><surname>Hodder</surname><given-names>KI</given-names></name><name><surname>Toh</surname><given-names>CY</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Effects of reverberation on perceptual segregation of competing voices</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>2871</fpage><lpage>2876</lpage><pub-id pub-id-type="doi">10.1121/1.1616922</pub-id><pub-id pub-id-type="pmid">14650021</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>CJ</given-names></name><name><surname>Hukin</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Effects of reverberation on spatial, prosodic, and vocal-tract size cues to selective attention</article-title><source>The Journal of the Acoustical Society of America</source><volume>108</volume><fpage>335</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1121/1.429468</pub-id><pub-id pub-id-type="pmid">10923896</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>3374</fpage><lpage>3386</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5249-08.2009</pub-id><pub-id pub-id-type="pmid">19295144</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Incorporating behavioral and sensory context into spectro-temporal models of auditory encoding</article-title><source>Hearing Research</source><volume>360</volume><fpage>107</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.12.021</pub-id><pub-id pub-id-type="pmid">29331232</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>I</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural population coding of sound level adapts to stimulus statistics</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1684</fpage><lpage>1689</lpage><pub-id pub-id-type="doi">10.1038/nn1541</pub-id><pub-id pub-id-type="pmid">16286934</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>I</given-names></name><name><surname>Robinson</surname><given-names>BL</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Rapid neural adaptation to sound level statistics</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6430</fpage><lpage>6438</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0470-08.2008</pub-id><pub-id pub-id-type="pmid">18562614</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deCharms</surname><given-names>RC</given-names></name><name><surname>Blake</surname><given-names>DT</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Optimizing sound features for cortical neurons</article-title><source>Science (New York, N.Y.)</source><volume>280</volume><fpage>1439</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1126/science.280.5368.1439</pub-id><pub-id pub-id-type="pmid">9603734</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devore</surname><given-names>S</given-names></name><name><surname>Ihlefeld</surname><given-names>A</given-names></name><name><surname>Hancock</surname><given-names>K</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>B</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate sound localization in reverberant environments is mediated by robust encoding of spatial cues in the auditory midbrain</article-title><source>Neuron</source><volume>62</volume><fpage>123</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.018</pub-id><pub-id pub-id-type="pmid">19376072</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzpatrick</surname><given-names>DC</given-names></name><name><surname>Kuwada</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>DO</given-names></name><name><surname>Parham</surname><given-names>K</given-names></name><name><surname>Batra</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Responses of neurons to click-pairs as simulated echoes: auditory nerve to auditory cortex</article-title><source>The Journal of the Acoustical Society of America</source><volume>106</volume><fpage>3460</fpage><lpage>3472</lpage><pub-id pub-id-type="doi">10.1121/1.428199</pub-id><pub-id pub-id-type="pmid">10615686</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuglsang</surname><given-names>SA</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name><name><surname>Hjortkjær</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Noise-robust cortical tracking of attended speech in real-world acoustic scenes</article-title><source>NeuroImage</source><volume>156</volume><fpage>435</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.026</pub-id><pub-id pub-id-type="pmid">28412441</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guediche</surname><given-names>S</given-names></name><name><surname>Blumstein</surname><given-names>SE</given-names></name><name><surname>Fiez</surname><given-names>JA</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Speech perception under adverse conditions: insights from behavioral, computational, and neuroscience research</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><fpage>328</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.3389/fnsys.2013.00126</pub-id><pub-id pub-id-type="pmid">24427119</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwak</surname><given-names>J</given-names></name><name><surname>Kwag</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distinct subtypes of inhibitory interneurons differentially promote the propagation of rate and temporal codes in the feedforward neural network</article-title><source>Chaos (Woodbury, N.Y.)</source><volume>30</volume><elocation-id>053102</elocation-id><pub-id pub-id-type="doi">10.1063/1.5134765</pub-id><pub-id pub-id-type="pmid">32491918</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>Schoppe</surname><given-names>O</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Cui</surname><given-names>Z</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Network Receptive Field Modeling Reveals Extensive Integration and Multi-feature Selectivity in Auditory Cortical Neurons</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005113</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005113</pub-id><pub-id pub-id-type="pmid">27835647</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartmann</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Localization of sound in rooms</article-title><source>The Journal of the Acoustical Society of America</source><volume>72</volume><elocation-id>S93</elocation-id><pub-id pub-id-type="doi">10.1121/1.2020159</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Binaural room simulation based on an image source model with addition of statistical methods to include the diffuse sound scattering of walls and to predict the reverberant tail</article-title><source>Applied Acoustics</source><volume>38</volume><fpage>145</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/0003-682X(93)90048-B</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helfer</surname><given-names>KS</given-names></name><name><surname>Wilber</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Hearing loss, aging, and speech perception in reverberation and noise</article-title><source>Journal of Speech and Hearing Research</source><volume>33</volume><fpage>149</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1044/jshr.3301.149</pub-id><pub-id pub-id-type="pmid">2314073</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houtgast</surname><given-names>T</given-names></name><name><surname>Steeneken</surname><given-names>HJM</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>A review of the MTF concept in room acoustics and its use for estimating speech intelligibility in auditoria</article-title><source>The Journal of the Acoustical Society of America</source><volume>77</volume><fpage>1069</fpage><lpage>1077</lpage><pub-id pub-id-type="doi">10.1121/1.392224</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huisman</surname><given-names>WHT</given-names></name><name><surname>Attenborough</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Reverberation and attenuation in a pine forest</article-title><source>The Journal of the Acoustical Society of America</source><volume>90</volume><fpage>2664</fpage><lpage>2677</lpage><pub-id pub-id-type="doi">10.1121/1.401861</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humes</surname><given-names>LE</given-names></name><name><surname>Dirks</surname><given-names>DD</given-names></name><name><surname>Bell</surname><given-names>TS</given-names></name><name><surname>Ahlstrom</surname><given-names>C</given-names></name><name><surname>Kincaid</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Application of the Articulation Index and the Speech Transmission Index to the recognition of speech by normal-hearing and hearing-impaired listeners</article-title><source>Journal of Speech and Hearing Research</source><volume>29</volume><fpage>447</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1044/jshr.2904.447</pub-id><pub-id pub-id-type="pmid">3795887</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ivanov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>DeReverb</data-title><version designator="83237aa">83237aa</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/PhantomSpike/DeReverb">https://github.com/PhantomSpike/DeReverb</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jeub</surname><given-names>M</given-names></name><name><surname>Löllmann</surname><given-names>HW</given-names></name><name><surname>Vary</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Blind Dereverberation for Hearing Aids with Binaural Link</article-title><conf-name>In ITG Conference on Speech Communication</conf-name><conf-loc>Bochum, Germany</conf-loc></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Karsh</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>SpikeGLX</data-title><version designator="4ff6023">4ff6023</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/billkarsh/SpikeGLX">https://github.com/billkarsh/SpikeGLX</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Invariance to background noise as a signature of non-primary auditory cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3958</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11710-y</pub-id><pub-id pub-id-type="pmid">31477711</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalighinejad</surname><given-names>B</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptation of the human auditory cortex to changing background noise</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2509</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10611-4</pub-id><pub-id pub-id-type="pmid">31175304</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>DO</given-names></name><name><surname>Zahorik</surname><given-names>P</given-names></name><name><surname>Carney</surname><given-names>LH</given-names></name><name><surname>Bishop</surname><given-names>BB</given-names></name><name><surname>Kuwada</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Auditory distance coding in rabbit midbrain neurons and human perception: monaural amplitude modulation depth as a cue</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>5360</fpage><lpage>5372</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3798-14.2015</pub-id><pub-id pub-id-type="pmid">25834060</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinoshita</surname><given-names>K</given-names></name><name><surname>Delcroix</surname><given-names>M</given-names></name><name><surname>Gannot</surname><given-names>S</given-names></name><name><surname>P. Habets</surname><given-names>EA</given-names></name><name><surname>Haeb-Umbach</surname><given-names>R</given-names></name><name><surname>Kellermann</surname><given-names>W</given-names></name><name><surname>Leutnant</surname><given-names>V</given-names></name><name><surname>Maas</surname><given-names>R</given-names></name><name><surname>Nakatani</surname><given-names>T</given-names></name><name><surname>Raj</surname><given-names>B</given-names></name><name><surname>Sehr</surname><given-names>A</given-names></name><name><surname>Yoshioka</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A summary of the REVERB challenge: state-of-the-art and remaining challenges in reverberant speech processing research</article-title><source>EURASIP Journal on Advances in Signal Processing</source><volume>2016</volume><pub-id pub-id-type="doi">10.1186/s13634-016-0306-6</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>VO</given-names></name></person-group><year iso-8601-date="1929">1929</year><article-title>The Hearing of Speech in Auditoriums</article-title><source>The Journal of the Acoustical Society of America</source><volume>1</volume><fpage>56</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1121/1.1901470</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kodrasi</surname><given-names>I</given-names></name><name><surname>Gerkmann</surname><given-names>T</given-names></name><name><surname>Doclo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Frequency-domain single-channel inverse filtering for speech dereverberation: Theory and practice</article-title><conf-name>ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing</conf-name><conf-loc>Florence, Italy</conf-loc><pub-id pub-id-type="doi">10.1109/ICASSP.2014.6854590</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>AJ</given-names></name><name><surname>Moore</surname><given-names>BCJ</given-names></name><name><surname>Cirstea</surname><given-names>S</given-names></name><name><surname>Aggius-Vella</surname><given-names>E</given-names></name><name><surname>Gori</surname><given-names>M</given-names></name><name><surname>Campus</surname><given-names>C</given-names></name><name><surname>Pardhan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Factors Affecting Auditory Estimates of Virtual Room Size: Effects of Stimulus, Level, and Reverberation</article-title><source>Perception</source><volume>50</volume><fpage>646</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1177/03010066211020598</pub-id><pub-id pub-id-type="pmid">34053354</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnamoorthy</surname><given-names>P</given-names></name><name><surname>Mahadeva Prasanna</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal and Spectral Processing Methods for Processing of Degraded Speech: A Review</article-title><source>IETE Technical Review</source><volume>26</volume><elocation-id>e49103</elocation-id><pub-id pub-id-type="doi">10.4103/0256-4602.49103</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krom</surname><given-names>AJ</given-names></name><name><surname>Marmelshtein</surname><given-names>A</given-names></name><name><surname>Gelbard-Sagiv</surname><given-names>H</given-names></name><name><surname>Tankus</surname><given-names>A</given-names></name><name><surname>Hayat</surname><given-names>H</given-names></name><name><surname>Hayat</surname><given-names>D</given-names></name><name><surname>Matot</surname><given-names>I</given-names></name><name><surname>Strauss</surname><given-names>I</given-names></name><name><surname>Fahoum</surname><given-names>F</given-names></name><name><surname>Soehle</surname><given-names>M</given-names></name><name><surname>Boström</surname><given-names>J</given-names></name><name><surname>Mormann</surname><given-names>F</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Anesthesia-induced loss of consciousness disrupts auditory responses beyond primary cortex</article-title><source>PNAS</source><volume>117</volume><fpage>11770</fpage><lpage>11780</lpage><pub-id pub-id-type="doi">10.1073/pnas.1917251117</pub-id><pub-id pub-id-type="pmid">32398367</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuttruff</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Room Acoustics</source><publisher-name>CRC Press/Taylor &amp; Francis Group</publisher-name><pub-id pub-id-type="doi">10.1201/9781315372150</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuwada</surname><given-names>S</given-names></name><name><surname>Bishop</surname><given-names>B</given-names></name><name><surname>Kim</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Approaches to the study of neural coding of sound source location and sound envelope in real environments</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><elocation-id>42</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00042</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>LY</given-names></name><name><surname>Xiong</surname><given-names>XR</given-names></name><name><surname>Ibrahim</surname><given-names>LA</given-names></name><name><surname>Yuan</surname><given-names>W</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Differential Receptive Field Properties of Parvalbumin and Somatostatin Inhibitory Neurons in Mouse Auditory Cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1782</fpage><lpage>1791</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht417</pub-id><pub-id pub-id-type="pmid">24425250</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Single-channel Speech Dereverberation via Generative Adversarial Training</article-title><conf-name>Interspeech</conf-name><conf-loc>Hyderabad, India</conf-loc><pub-id pub-id-type="doi">10.21437/Interspeech.2018-1234</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linden</surname><given-names>JF</given-names></name><name><surname>Liu</surname><given-names>RC</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Spectrotemporal structure of receptive fields in areas AI and AAF of mouse auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>2660</fpage><lpage>2675</lpage><pub-id pub-id-type="doi">10.1152/jn.00751.2002</pub-id><pub-id pub-id-type="pmid">12815016</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litovsky</surname><given-names>RY</given-names></name><name><surname>Yin</surname><given-names>TCT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Physiological studies of the precedence effect in the inferior colliculus of the cat. I. Correlates of psychophysics</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>1285</fpage><lpage>1301</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.80.3.1285</pub-id><pub-id pub-id-type="pmid">9744939</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lohse</surname><given-names>M</given-names></name><name><surname>Bajo</surname><given-names>VM</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural circuits underlying auditory contrast gain control and their perceptual implications</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>324</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-14163-5</pub-id><pub-id pub-id-type="pmid">31949136</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludwig</surname><given-names>KA</given-names></name><name><surname>Miriani</surname><given-names>RM</given-names></name><name><surname>Langhals</surname><given-names>NB</given-names></name><name><surname>Joseph</surname><given-names>MD</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kipke</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Using a common average reference to improve cortical neuron recordings from microelectrode arrays</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>1679</fpage><lpage>1689</lpage><pub-id pub-id-type="doi">10.1152/jn.90989.2008</pub-id><pub-id pub-id-type="pmid">19109453</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mechanisms of noise robust representation of speech in primary auditory cortex</article-title><source>PNAS</source><volume>111</volume><fpage>6792</fpage><lpage>6797</lpage><pub-id pub-id-type="doi">10.1073/pnas.1318017111</pub-id><pub-id pub-id-type="pmid">24753585</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>RC</given-names></name><name><surname>Lee</surname><given-names>T</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Noise-invariant neurons in the avian auditory cortex: hearing the song in noise</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002942</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002942</pub-id><pub-id pub-id-type="pmid">23505354</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nábĕlek</surname><given-names>AK</given-names></name><name><surname>Letowski</surname><given-names>TR</given-names></name><name><surname>Tucker</surname><given-names>FM</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Reverberant overlap- and self-masking in consonant identification</article-title><source>The Journal of the Acoustical Society of America</source><volume>86</volume><fpage>1259</fpage><lpage>1265</lpage><pub-id pub-id-type="doi">10.1121/1.398740</pub-id><pub-id pub-id-type="pmid">2808901</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakamura</surname><given-names>S</given-names></name><name><surname>Hiyane</surname><given-names>K</given-names></name><name><surname>Asano</surname><given-names>F</given-names></name><name><surname>Endo</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Sound scene data collection in real acoustical environments</article-title><source>Journal of the Acoustical Society of Japan (E)</source><volume>20</volume><fpage>225</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1250/ast.20.225</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Natan</surname><given-names>RG</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Mwilambwe-Tshilobo</surname><given-names>L</given-names></name><name><surname>Jones</surname><given-names>SI</given-names></name><name><surname>Aizenberg</surname><given-names>M</given-names></name><name><surname>Goldberg</surname><given-names>EM</given-names></name><name><surname>Geffen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Complementary control of sensory adaptation by two types of cortical interneurons</article-title><source>eLife</source><volume>4</volume><elocation-id>e09868</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.09868</pub-id><pub-id pub-id-type="pmid">26460542</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Naylor</surname><given-names>PA</given-names></name><name><surname>Gaubitch</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Speech Dereverberation</source><publisher-loc>London</publisher-loc><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-84996-056-4</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nielsen</surname><given-names>JB</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Revisiting perceptual compensation for effects of reverberation in speech identification</article-title><source>The Journal of the Acoustical Society of America</source><volume>128</volume><fpage>3088</fpage><lpage>3094</lpage><pub-id pub-id-type="doi">10.1121/1.3494508</pub-id><pub-id pub-id-type="pmid">21110604</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Kenneth</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pecka</surname><given-names>M</given-names></name><name><surname>Zahn</surname><given-names>TP</given-names></name><name><surname>Saunier-Rebori</surname><given-names>B</given-names></name><name><surname>Siveke</surname><given-names>I</given-names></name><name><surname>Felmy</surname><given-names>F</given-names></name><name><surname>Wiegrebe</surname><given-names>L</given-names></name><name><surname>Klug</surname><given-names>A</given-names></name><name><surname>Pollak</surname><given-names>GD</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Inhibiting the inhibition: A neuronal network for sound localization in reverberant environments</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>1782</fpage><lpage>1790</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5335-06.2007</pub-id><pub-id pub-id-type="pmid">17301185</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poissant</surname><given-names>SF</given-names></name><name><surname>Whitmal</surname><given-names>NA</given-names></name><name><surname>Freyman</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Effects of reverberation and masking on speech intelligibility in cochlear implant simulations</article-title><source>The Journal of the Acoustical Society of America</source><volume>119</volume><fpage>1606</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1121/1.2168428</pub-id><pub-id pub-id-type="pmid">16583905</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Puvvada</surname><given-names>KC</given-names></name><name><surname>Villafañe-Delgado</surname><given-names>M</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural Coding of Noisy and Reverberant Speech in Human Auditory Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/229153</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>MK</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Effects of envelope-vocoder processing on F0 discrimination and concurrent-vowel identification</article-title><source>Ear and Hearing</source><volume>26</volume><fpage>451</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1097/01.aud.0000179689.79868.06</pub-id><pub-id pub-id-type="pmid">16230895</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contrast gain control in auditory cortex</article-title><source>Neuron</source><volume>70</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.030</pub-id><pub-id pub-id-type="pmid">21689603</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Constructing noise-invariant representations of sound in the auditory pathway</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001710</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001710</pub-id><pub-id pub-id-type="pmid">24265596</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A dynamic network model of temporal receptive fields in primary auditory cortex</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006618</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006618</pub-id><pub-id pub-id-type="pmid">31059503</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simple transformations capture auditory input to cortex</article-title><source>PNAS</source><volume>117</volume><fpage>28442</fpage><lpage>28451</lpage><pub-id pub-id-type="doi">10.1073/pnas.1922033117</pub-id><pub-id pub-id-type="pmid">33097665</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rakerd</surname><given-names>B</given-names></name><name><surname>Hartmann</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Localization of noise in a reverberant environment</chapter-title><person-group person-group-type="editor"><name><surname>Pressnitzer</surname><given-names>D</given-names></name><name><surname>Cheveigné</surname><given-names>A</given-names></name><name><surname>McAdams</surname><given-names>S</given-names></name><name><surname>Collet</surname><given-names>L</given-names></name></person-group><source>Auditory Signal Processing</source><publisher-loc>New York, US</publisher-loc><publisher-name>Springer</publisher-name><fpage>413</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1007/0-387-27045-0_51</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>BL</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Meta-adaptation in the auditory midbrain under cortical influence</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13442</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13442</pub-id><pub-id pub-id-type="pmid">27883088</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2003">2003</year><chapter-title>How Linear are Auditory Cortical Responses?</chapter-title><person-group person-group-type="editor"><name><surname>Becker</surname><given-names>S</given-names></name><name><surname>Thrun</surname><given-names>S</given-names></name><name><surname>Obermayer</surname><given-names>K</given-names></name></person-group><source>Advances in Neural Information Processing Systems 15</source><publisher-name>MIT Press</publisher-name><fpage>125</fpage><lpage>132</lpage></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakai</surname><given-names>H</given-names></name><name><surname>Sato</surname><given-names>S</given-names></name><name><surname>Ando</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Orthogonal acoustical factors of sound fields in a forest compared with those in a concert hall</article-title><source>The Journal of the Acoustical Society of America</source><volume>104</volume><fpage>1491</fpage><lpage>1497</lpage><pub-id pub-id-type="doi">10.1121/1.424360</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sayles</surname><given-names>M</given-names></name><name><surname>Winter</surname><given-names>IM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reverberation challenges the temporal representation of the pitch of complex sounds</article-title><source>Neuron</source><volume>58</volume><fpage>789</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.03.029</pub-id><pub-id pub-id-type="pmid">18549789</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Linear processing of spatial cues in primary auditory cortex</article-title><source>Nature</source><volume>414</volume><fpage>200</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1038/35102568</pub-id><pub-id pub-id-type="pmid">11700557</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoppe</surname><given-names>O</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Measuring the Performance of Neural Models</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2016.00010</pub-id><pub-id pub-id-type="pmid">26903851</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-triggered neural characterization</article-title><source>Journal of Vision</source><volume>6</volume><fpage>484</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1167/6.4.13</pub-id><pub-id pub-id-type="pmid">16889482</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Learning Reverberation: Considerations for Spatial Auditory Displays</article-title><conf-name>Proceedings of the 2000 International Conference on Auditory Display</conf-name><fpage>126</fpage><lpage>134</lpage></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname><given-names>B</given-names></name><name><surname>Desloge</surname><given-names>J</given-names></name><name><surname>Kopco</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Empirical and modeled acoustic transfer functions in a simple room: effects of distance and direction</article-title><conf-name>In Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No.01TH8575</conf-name><conf-loc>New Platz, NY, USA</conf-loc></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname><given-names>B</given-names></name><name><surname>Kawakyu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural representation of source direction in reverberant space</article-title><conf-name>In 2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No.03TH8684</conf-name><conf-loc>New Paltz, NY, USA</conf-loc></element-citation></ref><ref id="bib82"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Lin</surname><given-names>IF</given-names></name><name><surname>Streeter</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Trading Directional Accuracy for Realism in a Virtual Auditory Display</article-title><ext-link ext-link-type="uri" xlink:href="https://www.cmu.edu/dietrich/psychology/shinn/publications/pdfs/2005/2005vrinthcii_shinn.pdf">https://www.cmu.edu/dietrich/psychology/shinn/publications/pdfs/2005/2005vrinthcii_shinn.pdf</ext-link><date-in-citation iso-8601-date="2022-06-15">June 15, 2022</date-in-citation></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Teramoto</surname><given-names>Y</given-names></name><name><surname>Willmore</surname><given-names>BD</given-names></name><name><surname>Schnupp</surname><given-names>JW</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sensory cortex is optimized for prediction of future input</article-title><source>eLife</source><volume>7</volume><elocation-id>e31557</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31557</pub-id><pub-id pub-id-type="pmid">29911971</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slama</surname><given-names>MCC</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural coding of sound envelope in reverberant environments</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>4452</fpage><lpage>4468</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3615-14.2015</pub-id><pub-id pub-id-type="pmid">25762687</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitzer</surname><given-names>MW</given-names></name><name><surname>Bala</surname><given-names>ADS</given-names></name><name><surname>Takahashi</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A neuronal correlate of the precedence effect is associated with spatial selectivity in the barn owl’s auditory midbrain</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>2051</fpage><lpage>2070</lpage><pub-id pub-id-type="doi">10.1152/jn.01235.2003</pub-id><pub-id pub-id-type="pmid">15381741</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sosa</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Bondy</surname><given-names>AG</given-names></name><name><surname>Winter</surname><given-names>O</given-names></name><name><surname>Banga</surname><given-names>K</given-names></name><name><surname>Guzman</surname><given-names>J</given-names></name><name><surname>Janke</surname><given-names>A</given-names></name><name><surname>Czuba</surname><given-names>T</given-names></name><name><surname>Nakamura</surname><given-names>KC</given-names></name><name><surname>O’Shea</surname><given-names>D</given-names></name><name><surname>Botros</surname><given-names>P</given-names></name><name><surname>Saxena</surname><given-names>R</given-names></name><name><surname>Liddell</surname><given-names>A</given-names></name><name><surname>Dipper-Wawra</surname><given-names>M</given-names></name><name><surname>Pellman</surname><given-names>J</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Beau</surname><given-names>M</given-names></name><name><surname>Spacek</surname><given-names>M</given-names></name><name><surname>Karamanlis</surname><given-names>D</given-names></name><collab>Mouseland</collab><collab>geffenlab</collab><collab>bryzgalovdm</collab></person-group><year iso-8601-date="2022">2022</year><data-title>Kilosort3: spike sorting on GPUs with template matching, drift correction and a new clustering method</data-title><version designator="1a1fd3a">1a1fd3a</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</article-title><source>Network Computation in Neural Systems</source><volume>12</volume><fpage>289</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1088/0954-898X/12/3/304</pub-id><pub-id pub-id-type="pmid">11563531</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tollin</surname><given-names>DJ</given-names></name><name><surname>Populin</surname><given-names>LC</given-names></name><name><surname>Yin</surname><given-names>TCT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neural correlates of the precedence effect in the inferior colliculus of behaving cats</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>3286</fpage><lpage>3297</lpage><pub-id pub-id-type="doi">10.1152/jn.00606.2004</pub-id><pub-id pub-id-type="pmid">15295015</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Traer</surname><given-names>J</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Statistics of natural reverberation enable perceptual separation of sound and space</article-title><source>PNAS</source><volume>113</volume><fpage>E7856</fpage><lpage>E7865</lpage><pub-id pub-id-type="doi">10.1073/pnas.1612524113</pub-id><pub-id pub-id-type="pmid">27834730</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trivedi</surname><given-names>U</given-names></name><name><surname>Dieckman</surname><given-names>E</given-names></name><name><surname>Xiang</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reciprocal maximum‐length and related sequences in the generation of natural, spatial sounding reverberation</article-title><source>The Journal of the Acoustical Society of America</source><volume>125</volume><elocation-id>2735</elocation-id><pub-id pub-id-type="doi">10.1121/1.4784525</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistical Models for Natural Sounds PhD Thesis</article-title><publisher-name>Gatsby Computational Neuroscience Unit, UCL</publisher-name></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>PV</given-names></name><name><surname>Barbour</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Specialized neuronal adaptation for preserving input sensitivity</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1259</fpage><lpage>1261</lpage><pub-id pub-id-type="doi">10.1038/nn.2201</pub-id><pub-id pub-id-type="pmid">18820690</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>GI</given-names></name><name><surname>Dean</surname><given-names>I</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dynamic range adaptation to sound level statistics in the auditory nerve</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13797</fpage><lpage>13808</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5610-08.2009</pub-id><pub-id pub-id-type="pmid">19889991</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Schoppe</surname><given-names>O</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Incorporating Midbrain Adaptation to Mean Sound Level Improves Models of Auditory Cortical Processing</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>280</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2441-15.2016</pub-id><pub-id pub-id-type="pmid">26758822</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Pentony</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Swaminathan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effects of reverberation and noise on speech intelligibility in normal-hearing and aided hearing-impaired listeners</article-title><source>The Journal of the Acoustical Society of America</source><volume>143</volume><fpage>1523</fpage><lpage>1533</lpage><pub-id pub-id-type="doi">10.1121/1.5026788</pub-id><pub-id pub-id-type="pmid">29604671</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Physiological correlates of the precedence effect and summing localization in the inferior colliculus of the cat</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>5170</fpage><lpage>5186</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-09-05170.1994</pub-id><pub-id pub-id-type="pmid">8083729</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshioka</surname><given-names>T</given-names></name><name><surname>Sehr</surname><given-names>A</given-names></name><name><surname>Delcroix</surname><given-names>M</given-names></name><name><surname>Kinoshita</surname><given-names>K</given-names></name><name><surname>Maas</surname><given-names>R</given-names></name><name><surname>Nakatani</surname><given-names>T</given-names></name><name><surname>Kellermann</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Making Machines Understand Us in Reverberant Rooms: Robustness Against Reverberation for Automatic Speech Recognition</article-title><source>IEEE Signal Processing Magazine</source><volume>29</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1109/MSP.2012.2205029</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zilany</surname><given-names>MSA</given-names></name><name><surname>Bruce</surname><given-names>IC</given-names></name><name><surname>Nelson</surname><given-names>PC</given-names></name><name><surname>Carney</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A phenomenological model of the synapse between the inner hair cell and auditory nerve: long-term adaptation with power-law dynamics</article-title><source>The Journal of the Acoustical Society of America</source><volume>126</volume><fpage>2390</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1121/1.3238250</pub-id><pub-id pub-id-type="pmid">19894822</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zilany</surname><given-names>MSA</given-names></name><name><surname>Bruce</surname><given-names>IC</given-names></name><name><surname>Carney</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Updated parameters and expanded simulation options for a model of the auditory periphery</article-title><source>The Journal of the Acoustical Society of America</source><volume>135</volume><fpage>283</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1121/1.4837815</pub-id><pub-id pub-id-type="pmid">24437768</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75090.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.10.28.466271" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.28.466271"/></front-stub><body><p>This study identifies a mechanism based on context-dependent plasticity of inhibitory receptive fields that likely plays a role in suppression of reverberation signals in hearing. This new mechanism is a very interesting starting point to describe the biological circuit underpinnings of reverberation suppression, a complex signal processing ability of the auditory system.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75090.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Mesgarani</surname><given-names>Nima</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.28.466271">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.28.466271v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Cortical adaptation to sound reverberation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Brice Bathellier as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Nima Mesgarani (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1. The manuscript is mainly focused on the adaptation of inhibition parameters, and there is no assessment of the efficiency of the dereverberation by cortex. The authors should quantify the similarity of cortical responses for the same sounds with and without reverberation. This measure should be compared to the performance of the dereverberation model (see also comment 4. of reviewer #3).</p><p>2. The controls of figure 3 are crucial and should be put in a new main figure, not in the supplements. Especially figure 3 suppl. 2&amp;3.</p><p>3. The authors show that a linear receptive field model is not biased by reverberation statistics. However, many papers have shown that auditory processing is non-linear. Therefore, the authors should test this again in a non-linear model (e.g. filterbank followed by a quadratic non-linearity as in the Shamma lab) and improve the cochlear model for example by applying the auditory periphery model described in: Bruce, I. C., Erfani, Y., and Zilany, M. S. A. (2018). &quot;A phenomenological model of the synapse between the inner hair cell and auditory nerve: Implications of limited neurotransmitter release sites,&quot; Hearing Research 360:40-54. This and other alternatives are readily available as part of the Auditory Modeling Toolbox (https://www.amtoolbox.org).</p><p>Also, it is important to note that dereverberation requires highly nonlinear acoustic processing. The dereverberation algorithms that have been used in speech processing typically try to mask the spectrogram as a method to recover the &quot;direct path&quot; and remove the &quot;reflections.&quot; While the resulting &quot;high-pass&quot; filter found in the linear filtering attempts to approximate this nonlinear operation, it is not very effective when used in realistic conditions. While the linear modeling framework here allows the authors to perform a straightforward comparison with auditory receptive fields, this limitation should be noted so the readers are aware of the true difficulty of this task and hence, unexplained mechanisms that remain to be found.</p><p>4. The authors should show the energy time curves of the BRIRs at different frequencies and derive the expected adaptation mechanisms already from there. This would greatly simplify the overall concept.</p><p>5. A big issue is the differences between the stimuli used to find the receptive field in different reverberant scenarios. While the authors do a good job to show that the differences are not merely due to the statistics of the stimuli, particularly by showing a different response to the probe sound, they cannot claim that &quot;all&quot; of the observed changes are due to adaptation. It is likely reflecting a mix of both, some due to the change in the stimulus correlation and some due to adaptation. Currently, this inherent limitation is not acknowledged.</p><p>6. Regarding the discussion of the feedforward/feedback nature of the adaptation to changing background statistics, Khalighinejad et al. also showed that the suppression of background noise is the same when the subject is actively performing speech-in-noise perception and when the subject is distracted by a visual task (Figure 5). Perhaps this observation can strengthen the argument regarding the anesthetized/awake conditions and the nature of the adaptation (lines 447-451). Also, it should be made clear in the discussion that the adaptation phenomenon may not be appearing in cortex, but rather subcortically.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. It should be better discussed why the cortex has a much larger variability in adaptation time constants, than the dereverberation model. The authors suggest that it is because cortical neurons are performing other computations. But an alternative explanation could be that there is more noise in the data than in the model.</p><p>2. Related to 1, the manuscript is mainly focused on the adaptation of inhibition parameters, and there is no assessment of the efficiency of the dereverberation by cortex. The authors should quantify the similarity of cortical responses for the same sounds with and without reverberation. This measure should be compared to the performance of the dereverberation model.</p><p>3. The controls of figure 3 are crucial and should be put in a new main figure, not in the supplements. Especially figure 3 suppl. 2&amp;3.</p><p>4. The authors show that a linear receptive field model is not biased by reverberation statistics. However, many papers have shown that auditory processing is non-linear. Therefore, the authors should test this again in a non-linear model.</p><p>5. It should be made clear in the discussion that this phenomenon may not be appearing in cortex, but rather subcortically.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I recommend to show the energy time curves of the BRIRs at different frequencies and derive the expected adaptation mechanisms already from there. This would greatly simplify the overall concept.</p><p>As a replacement for the cochleagram, I can recommend to apply the auditory periphery model described in: Bruce, I. C., Erfani, Y., and Zilany, M. S. A. (2018). &quot;A phenomenological model of the synapse between the inner hair cell and auditory nerve: Implications of limited neurotransmitter release sites,&quot; Hearing Research 360:40-54. This and other alternatives are readily available as part of the Auditory Modeling Toolbox (https://www.amtoolbox.org).</p><p>l.610: How was the &quot;low threshold&quot; defined that was applied to limit the log power values?</p><p>l.620: This definition of RT10 is inconsistent with the nomenclature used in ISO standards. There, reverberation time is defined as the time it takes the sound energy to decay by 60 dB, denoted as RT60. To estimate this metric, one can also assume a linear decay and measure only the time it takes to decay, for instance, by 20 dB and then multiply by 3. Still, RT20 is then the result of this extrapolation, not the 20-dB-decay time itself. According to that definition, your reverberation times would need to be multiplied by six. Hence, your large room had a reverberation time of about 2.6 s, similar to a small cathedral.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This is a very interesting study that tests how the auditory system adapts to reverberant acoustic scenes. The paper is written well, and the results are overall compelling. I have a few comments that hopefully can strengthen the claims of the study.</p><p>1. It is important to note that dereverberation requires highly nonlinear acoustic processing. The dereverberation algorithms that have been used in speech processing typically try to mask the spectrogram as a method to recover the &quot;direct path&quot; and remove the &quot;reflections&quot;. While the resulting &quot;high-pass&quot; filter found in the linear filtering attempts to approximate this nonlinear operation, it is not very effective when used in realistic conditions. While the linear modeling framework here allows the authors to perform a straightforward comparison with auditory receptive fields, this limitation should be noted so the readers are aware of the true difficulty of this task and hence, unexplained mechanisms that remain to be found.</p><p>2. A big issue which the authors are well aware of is the differences between the stimuli used to find the receptive field in different reverberant scenarios. While the authors do a good job to show that the differences are not merely due to the statistics of the stimuli, particularly by showing a different response to the probe sound, they cannot claim that &quot;all&quot; of the observed changes are due to adaptation. It is likely reflecting a mix of both, some due to the change in the stimulus correlation and some due to adaptation. Currently, this inherent limitation is not acknowledged.</p><p>3. Related to 2, I think the control experiments that were done to show the changes in the response to the probe sound in different reverberant conditions are worthy of inclusion in the main figures. Perhaps a summary of that can be added to Figure 3, as this is a very important result, without which the observed changes are not very compelling.</p><p>4. On the same point, while it is an important observation that the responses to the probe stimulus (non-reverberant) are different in different reverberation contexts, a complementary observation that is missing is the similarity of the cortical responses to varying degrees of reverberation. In other words, if the STRFs change as suggested to produce a less variable response to reverberation, then the responses to the same sound with different reverb should stay constant and similar to the anechoic sound. While invariant cortical responses have been shown in other studies (including ferrets), it will strengthen this study if they can also confirm the presence of that effect which is the subject of this study.</p><p>5. Regarding the discussion of the feedforward/feedback nature of the adaptation to changing background statistics, Khalighinejad et al. also showed that the suppression of background noise is the same when the subject is actively performing speech-in-noise perception and when the subject is distracted by a visual task (Figure 5). Perhaps this observation can strengthen the argument regarding the anesthetized/awake conditions and the nature of the adaptation (lines 447-451).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75090.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The manuscript is mainly focused on the adaptation of inhibition parameters, and there is no assessment of the efficiency of the dereverberation by cortex. The authors should quantify the similarity of cortical responses for the same sounds with and without reverberation. This measure should be compared to the performance of the dereverberation model (see also comment 4. of reviewer #3).</p></disp-quote><p>Our understanding of this request is that the reviewers would like us to examine whether the cortex effectively removes reverberations from its representations of sound, as shown previously in the awake ferret auditory cortex using decoding approaches (Mesgarani et al., 2014). The reviewers suggest that we quantify the similarity of cortical responses to the same sounds across reverberant conditions. To achieve this, we could quantify the similarity of response for each cortical unit by calculating the correlation coefficient between the spiking responses (PSTHs) in the reverberant and anechoic conditions. However, to determine how effective the dereverberation has been, the resulting correlation must be compared to some baseline. The reviewers suggest that we use our normative dereverberation model as a baseline. However, the normative model maps from echoic cochleagrams to anechoic cochleagrams, and so does not produce a spike train with which we could make a direct comparison to neural spiking responses. We therefore need to find an alternative baseline.</p><p>Elsewhere in the paper, we have used a non-adapting LNP model as a baseline for comparison with the neural data (the simulated model neurons described in Figure 4). Since this model describes the responses of each cortical neuron in a way that does not include adaptation, it also provides a valid baseline for evaluation of the extent to which cortical adaptation has resulted in dereverberation. If the auditory cortex removes reverberations from its response to sounds, the correlation coefficients between cortical responses to sounds with and without reverberation will be higher than the same metrics calculated with the simulated responses. We would also expect the neuronal responses to be more similar than the model responses to sounds presented in rooms with different levels of reverberation, so we also examine the similarity of spiking responses between the small and large rooms.</p><p>The results of these analyses are plotted below. We find that the spiking responses of cortical units across the 3 different reverberant environments are more similar than those of their corresponding LNP simulated model responses. The measured cortical responses were more similar than simulated responses across the small and anechoic rooms (Wilcoxon signed-rank tests; Z = 6.0; p = 1.5 x 10<sup>-9</sup>), the large and anechoic rooms (Z = 6.9; p = 7.2 x 10<sup>-12</sup>), and the small and large rooms (Z = 13.0; p = 1.0 x 10<sup>-40</sup>). These analyses suggest that as a result of adaptive changes in the STRFs, the auditory cortex does effectively remove some of the effects of reverberation from its responses to natural sounds. We have added the figure of these results (below) to our manuscript as Figure 6, and describe it in the results text copied below.</p><p>Lines 323-338:</p><p>“Neural adaptation helps to remove the effects of reverberation</p><p>The above results indicate that auditory cortical neurons show adaptation that is consistent with a model of room-dependent dereverberation. To further confirm that the neural adaptation we observed promotes reverberation invariance, we measured the similarity of cortical responses to the same natural sounds across different reverberation conditions. This was compared to the LNP model of the cortical units, which lacks adaptation but approximates each unit’s spectrotemporal tuning, output nonlinearity, and response variability. We did this for 430 cortical units recorded from 5 ferrets, and included an anechoic room condition. We performed this analysis for three pairs of reverberant conditions: the small room and the anechoic room; the large room and the anechoic room; and the large room and the small room. In all three cases, the real neural responses showed significantly larger correlation coefficients between</p><p>reverberation conditions than did the simulated neural responses (Wilcoxon signed-rank tests; p &lt; 0.0001; Figure 6). A similar correlation analysis was used to demonstrate cochleagram dereverberation by our normative model (Wilcoxon signed-rank tests; p &lt; 0.0001; Figure 6 —figure supplement 1). These results suggest that the adaptation we observed plays a role in dereverberation by producing neural representations of sounds that are similar across reverberant conditions.”</p><p>We performed a similar analysis on the dereverberation model by measuring the correlation between cochleagram channel activities (rather than correlations between spike responses). As with the neural data, we found that the dereverberation model also leads to increased invariance across reverberant conditions, because the dereverberation process of the model renders the reverberant cochleagram more similar to the anechoic cochleagrams and to each other. We now include this analysis as a supplementary figure (Figure 6 —figure supplement 1).</p><disp-quote content-type="editor-comment"><p>2. The controls of figure 3 are crucial and should be put in a new main figure, not in the supplements. Especially figure 3 suppl. 2&amp;3.</p></disp-quote><p>We have implemented this suggestion to move the control figures into the main manuscript. The previous Supplementary Figure 3-2, which describes the results of our simulated neuron, is now Figure 4. Similarly, the noise burst control (previous Supp Figure 3-3) and room switching time course analysis (previous Supp Figure 3-4) are now combined in Figure 5 of our main manuscript. The references to figure numbers have been updated throughout.</p><disp-quote content-type="editor-comment"><p>3. The authors show that a linear receptive field model is not biased by reverberation statistics. However, many papers have shown that auditory processing is non-linear. Therefore, the authors should test this again in a non-linear model (e.g. filterbank followed by a quadratic non-linearity as in the Shamma lab)</p></disp-quote><p>In our original simulated neurons analysis, we showed that a static linear-nonlinear receptive field model in combination with reverberation statistics cannot explain the size of the temporal shifts in inhibition that we see. We have edited the results to further emphasize the non-linearity of this model, so that it is clearer to the reader.</p><p>Lines 241-242:</p><p>“Since this nonlinear model captured the spectrotemporal tuning of the cortical units but did not have an adaptive component,…”</p><p>However, we agree that it would be interesting to explore if the adaptive inhibitory shift we observed could be explained by a stronger form of static non-linearity, as we mentioned in our Discussion. We could not find a reference for the quadratic non-linearity model of the Shamma lab that the Reviewer mentioned, so we have opted instead to use the Network Receptive Field (NRF) model. This model is substantially more nonlinear than the LN model and consists of a single hidden layer neural network. It has been shown to improve predictions of auditory cortical responses over an LN model (Harper et al. 2016).</p><p>We replaced the LN model in our simulated neurons analysis with an NRF model, and otherwise repeated the simulation analysis in exactly the same way. As with the LN model, we found that the NRF model, which had nonlinearity both before and after the final integration, could not explain the size of the temporal shifts in inhibition that we see. This further argues that we see a genuine adaptive effect. We have added these NRF simulations to the manuscript in the following text.</p><p>Lines 273-284:</p><p>“We also investigated the result of replacing the LN component of the LNP model with a model that has a stronger static non-linearity. We used the network-receptive field (NRF) model, which is essentially a single hidden layer neural network, with sigmoid nonlinearities for its 10 hidden units and its single output unit (Harper et al. 2016; Rahman et al. 2019; 2020). We assessed fit quality using CC<sub>norm</sub> (Schoppe et al. 2016) on held-out test data, comparing this to the performance of the LN model. The NRF fits had a mean CC<sub>norm</sub> of 0.64 and showed statistically significant better performance than the LN fits (median CC<sub>norm</sub> difference = 0.016, p = 0.0056, Wilcoxon signed-rank test). We repeated the spike rate simulation analyses with this NRFPoisson (NRFP) model, keeping all other aspects of the analysis the same as described for the LNP model above. As with the LNP model, the NRFP model could not explain the magnitude of the shift in inhibitory center of mass or peak time seen in the real data (Figure 4—figure supplement 1). This suggests that an increased non-linearity alone cannot account for the reverberation adaptation observed in auditory cortex.”</p><p>Lines 784-794:</p><p>“Additionally, we repeated the analysis with the simulated neurons, but replacing the LNP model for the simulated neuron with a network receptive field (NRF) model (Harper et al. 2016; Rahman et al. 2019; 2020) with an appended inhomogeneous Poisson process, to produce an NRFP model. Specifically, we used the NRF model from Rahman et al. (2020), which is essentially a single hidden layer neural network with a sigmoid nonlinearity on the hidden units, the only difference being that we used 10 hidden units. We fitted the NRF model to the neural data by minimizing mean squared error, subject to L1 regularization on the weights. We set the regularization strength using 10-fold cross-validation, as we did for the STRFs in the LN model. For each neuron, after fitting the NRF, it was used as input to an inhomogeneous Poisson process. This was used to simulate neural activity, which was then analyzed in exactly the same way as the LNP model simulated activity.”</p><p>Relating to this point, we also added an estimate of the Fano factor of the neural responses to demonstrate that the neural noise was approximately Poisson, validating our assumption of Poisson noise in both our model simulations. We have also included a metric to quantify the quality of fit (CC<sub>norm</sub>) of the LN and NRF models, so that the reader can compare these two models in terms of their ability to predict our neural data:</p><p>Lines 237-243:</p><p>“We assessed fit quality using normalized correlation coefficient, CC<sub>norm</sub> (Schoppe et al., 2016), on held-out test data, giving a CC<sub>norm</sub> value of 0.64. Then a non-homogeneous Poisson process was appended to the LN model, to provide an LNP model. The noise in the recorded neuronal responses was close to Poisson (median Fano factor = 1.1). Since this nonlinear model did not have an adaptive component, we used it to assess whether our reverberation-dependent results could arise from fitting artefacts in a non-adaptive neuron.”</p><p>Related revision: investigating our LN model simulations for these analyses helped us to uncover a bug in our original code, in which the small room was used to simulate both responses in step 3 of Figure 4. We have now fixed the bug and re-run these simulations in the present version. Following this correction, the inhibitory shifts in our neural responses remained substantially larger than those observed with the simulated responses, as originally reported.</p><p>Related revision: Please note that when measuring peak time (PT) that we also now interpolate our frequency averaged neuronal STRFs/model kernels to provide a more resolved estimate of the peak time (PT). This is explained in the methods, copied below. Median PT values are updated throughout the manuscript, but these do not change our overall pattern of results or interpretations of them.</p><p>Lines 755-759: “Due to the discrete nature of the peak time measure (it can only be a multiple of the time bin size), when measuring it we applied an interpolation (Akima, 1978) with a factor of 100 to <italic>v<sub>H</sub><sup>+</sup></italic> and <italic>v<sub>h</sub><sup>-</sup></italic> in order to obtain a smoother estimate of peak times. PT<sup>+</sup> was taken as the time in <italic>v<sub>H</sub><sup>+</sup></italic> at which the maximum value occurred, and likewise, PT<sup>-</sup> was taken as the time in <italic>v<sub>h</sub><sup>-</sup></italic> at which the minimum value occurred.”</p><disp-quote content-type="editor-comment"><p>The author should improve the cochlear model for example by applying the auditory periphery model described in: Bruce, I. C., Erfani, Y., and Zilany, M. S. A. (2018). &quot;A phenomenological model of the synapse between the inner hair cell and auditory nerve: Implications of limited neurotransmitter release sites,&quot; Hearing Research 360:40-54. This and other alternatives are readily available as part of the Auditory Modeling Toolbox (https://www.amtoolbox.org).</p></disp-quote><p>We chose our cochlear model after careful consideration. In a recent study (Rahman et al., 2020), we examined which cochlear model provides the best preprocessing stage for predicting neural responses to natural sounds in the primary auditory cortex of ferrets on a held out dataset when the cochlear model is used as input to a linear or non-linear receptive field model (e.g. an LN model). We tested a wide range of cochlear models, including the one that we have used here (referred to as the “<italic>log-pow”</italic> cochlear model in Rahman et al. 2020) and the model of Bruce et al. (2018). The model which performed best was the log-pow model; this was the case across different stimulus types, for awake and anesthetized ferrets, and for linear and non-linear cortical receptive field models. This is why we chose to use the log-pow cochlear model here.</p><p>The Bruce et al. (2018) cochlear model, while useful for modeling cochlear responses themselves, did not perform as well as the log-pow model as a preprocessing stage for modeling auditory cortical responses to natural sounds. In Rahman et al. (2020), we speculate that this may be due to brainstem filtering removing many of the details of cochlear processing, rendering the simpler log-pow model more predictive of cortical responses. Thus, in the present paper we have kept the log-pow model as our primary cochlear model, and we hope the Editor and Reviewers will agree that this is the best choice given the results of Rahman et al. (2020). We now explain our choice of cochlear model in the Results:</p><p>Lines 90-94: “We used a simple “log-pow” cochlear model to produce the cochleagrams, as our recent work suggests that these cochleagrams enable better prediction of cortical responses than more detailed cochlear models (<italic>Rahman et al., 2020</italic>). These spectrotemporal representations of the sound estimate the cochlear filtering and resulting representation of the sound at the auditory nerve (Brown and Cook, 1994; Rahman et al., 2020).”</p><p>However, we agree that it is valuable to use more than one cochlear model to check the robustness of our results. To this end, we have re-calculated the main results of our study using the Bruce et al. (2018) cochlear model (also developed by Carney and colleagues in Zilany et al., 2014, 2009), as per the Reviewer’s suggestion. We refer to this model as the Carney Bruce Erfani Zilany (CBEZ) model. Our central findings regarding the inhibitory time shifts were replicated with this alternative cochlear model, and we have incorporated these analyses into the paper as a supplementary figure (Figure 3 —figure supplement 1, see below). Using the CBEZ model, we observed a shift in the center of mass of inhibitory components (COM<sup>-</sup>) to later time points in the larger room, in line with our original findings, for both the dereverberation model (median difference of 10.0ms for large – small room; Wilcoxon signed-rank test; p = 1.7 x 10<sup>-6</sup>) and neural units (median = 12.0ms; p = 4.2 x 10<sup>-78</sup>). Similarly, the peak time (PT<sup>-</sup>) of inhibitory components also showed a delay in the larger room, for both the dereverberation model (median = 21.0ms; p = 4.7 x 10<sup>-6</sup>) and neural units (median = 12ms; p = 4.6 x 10<sup>-58</sup>). It is worth noting that with the CBEZ model we observed relatively small shifts in the excitatory component COM (COM<sup>+</sup>) towards earlier values in the larger room for the dereverberation model (median difference = -5.6ms; large – small room; p = 1.6 x 10<sup>-5</sup>), and towards later COM<sup>+</sup> values in the larger room for the neural responses (median = 2.7ms; p = 1.2 x 10<sup>-7</sup>). However, the peak times of excitatory responses showed no difference between rooms in the dereverberation model (PT<sup>+</sup> median = 0.0ms; p = 1), and a small but statistically significant shift towards later peak times in the larger room in the neural data (PT<sup>+</sup> median = 0.3ms; p = 6.2 x 10<sup>-9</sup>). Therefore, excitatory shifts were small and inconsistent in both the log-pow and CBEZ models.</p><p>Overall, both cochlear models showed a robust delay in the negative components of model kernels and neuronal STRFs in the larger room, with modest or absent effects of room size on positive components. Given the results of Rahman et al. (2020), we have continued to use the simpler log-pow cochlear model in our main manuscript. We have included the CBEZ analysis as a supplementary figure, and refer to it in our main results text:</p><p>Lines 187-190: “We also observed these room-size-dependent delays in the COM and PT of inhibitory components when we used a more detailed cochlear model (<italic>Bruce et al., 2018; Zilany et al., 2014; 2009</italic>) to generate input cochleagrams (Figure 3—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>Also, it is important to note that dereverberation requires highly nonlinear acoustic processing. The dereverberation algorithms that have been used in speech processing typically try to mask the spectrogram as a method to recover the &quot;direct path&quot; and remove the &quot;reflections.&quot; While the resulting &quot;high-pass&quot; filter found in the linear filtering attempts to approximate this nonlinear operation, it is not very effective when used in realistic conditions. While the linear modeling framework here allows the authors to perform a straightforward comparison with auditory receptive fields, this limitation should be noted so the readers are aware of the true difficulty of this task and hence, unexplained mechanisms that remain to be found.</p></disp-quote><p>We agree that it is important to make the limitations of linear dereverberation clear to the reader. We have expanded the text that addresses this issue in the Discussion. Our new text is highlighted in yellow below.</p><p>Lines 509-518: “Another point for future research to consider is how our normative model could be further developed. For simplicity and interpretability, we used an elementary linear model. The frequency-dependent suppression observed in our normative model and neuronal receptive fields has relations to the linear frequency-domain approaches to dereverberation used in acoustical engineering (e.g., Kodrasi et al. (2014), Krishnamoorthy and Mahadeva Prasanna (2009)). However, the performance of such linear dereverberation solutions has limitations,</p><p>such as when the impulse response changes due to the speaker moving through their environment (Krishnamoorthy and Mahadeva Prasanna, 2009). There are more complex and powerful models for dereverberation in acoustical engineering, some of which may provide insight into the biology (Naylor and Gaubtich, 2010), and these should be explored in future neurobiological studies.”</p><p>We also now emphasize that our dereverberation model is a simple one, in the first line of our Results.</p><p>Lines 79-80: “We trained a simple dereverberation model to estimate the spectrotemporal structure of anechoic sounds from reverberant versions of those sounds.”</p><disp-quote content-type="editor-comment"><p>4. The authors should show the energy time curves of the BRIRs at different frequencies and derive the expected adaptation mechanisms already from there. This would greatly simplify the overall concept.</p></disp-quote><p>We agree it would be valuable to show the energy time curves of the BRIRs, and we now do this in Figure 7—figure supplement 1 (below). We reference this figure in the manuscript with the following text:</p><p>Lines 379-380: “This faster decay for higher frequencies can also be observed in the spectrograms of the impulse responses (Figures 7—figure supplement 1).”</p><p>As our analyses were based on monaural cochleagrams, we now specify that the right ear input was used:</p><p>Lines 667-668: “All analyses involving cochleagrams used the right ear cochleagram, as we recorded from the left auditory cortex.”</p><p>BRIR-based dereverberation has made important contributions to acoustical engineering. However, the goal of our manuscript is not to develop or test methods to achieve the best room dereverberation. Rather, our goal is to understand how neurons in the auditory cortex may achieve partial dereverberation through adaptive processes.</p><p>To this end, we built our normative model to have structures that resemble simplified versions of processing in the auditory system, in order to force brain-like constraints on the model and to enable comparison to neural data. This includes increased frequency bandwidth at higher frequencies, loss of phase information, substantial compression of intensity in the cochlear model, and an STRF-like linear process in the dereverberation model. Our simple model is far from a complete account of dereverberation in the auditory system, but it is sufficient to provide insights into the questions that we ask.</p><p>It is unclear to us how results that are easily comparable to neural data could be derived directly from the BRIR energy-time curves, and it seems to us easiest to instead fit a neural-processinglike model to the dereverberation task at hand as we have done, and then compare that to the neural data. The form of our normative model provides us with STRF-like kernels that can be compared directly with neural STRFs.</p><disp-quote content-type="editor-comment"><p>5. A big issue is the differences between the stimuli used to find the receptive field in different reverberant scenarios. While the authors do a good job to show that the differences are not merely due to the statistics of the stimuli, particularly by showing a different response to the probe sound, they cannot claim that &quot;all&quot; of the observed changes are due to adaptation. It is likely reflecting a mix of both, some due to the change in the stimulus correlation and some due to adaptation. Currently, this inherent limitation is not acknowledged.</p></disp-quote><p>We agree. This interpretation is supported by the results of our simulated neurons, which show some, albeit smaller, systematic shifts in their receptive fields in response to sounds presented in the two different rooms. In the case of our noise burst control, the effects are in fact all due to adaptation, as the noise burst stimulus is identical in both reverberation contexts (small and large room). Similarly for our room switching control, the effects must be due to adaptation, as the stimuli are identical in L1 and L2 and in S1 and S2. Therefore, as the Reviewers point out, while most of the effects in our main analysis are likely to reflect neural adaptation, stimulus statistics are likely to contribute as well.</p><p>We have reworded our interpretation of our simulation results to make this point clearer.</p><p>Lines 269-272: “In summary, these simulations suggest that differences in stimulus properties alone can account for a small shift in inhibitory receptive fields across rooms, but not the magnitude of delay that we observed in our neural data. Therefore, these effects are likely to arise, at least in part, from neural adaptation to room reverberation.”</p><disp-quote content-type="editor-comment"><p>6. Regarding the discussion of the feedforward/feedback nature of the adaptation to changing background statistics, Khalighinejad et al. also showed that the suppression of background noise is the same when the subject is actively performing speech-in-noise perception and when the subject is distracted by a visual task (Figure 5). Perhaps this observation can strengthen the argument regarding the anesthetized/awake conditions and the nature of the adaptation (lines 447-451).</p></disp-quote><p>We thank the Reviewers for bringing this relevant paper to our attention, and have amended our discussion to include it.</p><p>Lines 499-505: “Previous work has shown no effect of anesthesia on another kind of adaptation, contrast gain control, in either the ferret auditory cortex (<italic>Rabinowitz et al., 2011</italic>) or the mouse inferior colliculus (<italic>Lohse et al., 2020</italic>). Furthermore, <italic>Khalighinejad et al. (2019)</italic> found that adaptation to background noise in human auditory cortical responses was similar whether subjects were actively performing speech-in-noise tasks or were distracted by a visual task. There is therefore no a priori reason to expect that cortical adaptation to reverberation should depend on brain state and be substantially different in awake and anesthetized ferrets.”</p><disp-quote content-type="editor-comment"><p>Also, it should be made clear in the discussion that the adaptation phenomenon may not be appearing in cortex, but rather subcortically.</p></disp-quote><p>The Reviewers are correct in pointing out that the effects we have observed in auditory cortex could be subcortical in origin. We make this point in two parts of our Discussion.</p><p>First, in the context of our frequency dependence effect, where we have now strengthened the wording:</p><p>Lines 463-465: “Hence, the frequency dependence we observe in the present study may to some extent reflect general mechanisms for removing both reverberation and the mean sound level, and may be at least partially inherited from subcortical areas.”</p><p>Second, we raise this point in our discussion of potential mechanisms. We have also edited this text to make the point about potential subcortical contributions clearer:</p><p>Lines 467-474: “What might be the basis for the cortical adaptation to reverberation that we have observed? Some plausible mechanisms for altering the inhibitory field include synaptic depression (David et al., 2009), intrinsic dynamics of membrane channels (Abolafiaetal., 2011), hyperpolarizing inputs from inhibitory neurons (Li et al., 2015; Natan et al., 2015; Gwak and Kwag, 2020), or adaptation inherited from subcortical regions such as the inferior colliculus or auditory thalamus (medial geniculate body) (Dean et al., 2008; Devore et al., 2009; Willmore et al., 2016; Lohse et al., 2020). Further studies are required to discriminate among these mechanisms, and to determine if the observed reverberation adaptation is subcortical or cortical in origin.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. It should be better discussed why the cortex has a much larger variability in adaptation time constants, than the dereverberation model. The authors suggest that it is because cortical neurons are performing other computations. But an alternative explanation could be that there is more noise in the data than in the model.</p></disp-quote><p>We have now acknowledged this point in the text.</p><p>Lines 429-431: “The larger variance in reverberation adaptation across neural units may also result from the fact that neural responses are inherently noisier than our model kernels.”</p><disp-quote content-type="editor-comment"><p>2. Related to 1, the manuscript is mainly focused on the adaptation of inhibition parameters, and there is no assessment of the efficiency of the dereverberation by cortex. The authors should quantify the similarity of cortical responses for the same sounds with and without reverberation. This measure should be compared to the performance of the dereverberation model.</p></disp-quote><p>See response to Essential revision 1 above.</p><disp-quote content-type="editor-comment"><p>3. The controls of figure 3 are crucial and should be put in a new main figure, not in the supplements. Especially figure 3 suppl. 2&amp;3.</p></disp-quote><p>See response to Essential Revision 2 above.</p><disp-quote content-type="editor-comment"><p>4. The authors show that a linear receptive field model is not biased by reverberation statistics. However, many papers have shown that auditory processing is non-linear. Therefore, the authors should test this again in a non-linear model.</p></disp-quote><p>See response to Essential revision 3 above.</p><disp-quote content-type="editor-comment"><p>5. It should be made clear in the discussion that this phenomenon may not be appearing in cortex, but rather subcortically.</p></disp-quote><p>See response to Essential revision 6 above.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I recommend to show the energy time curves of the BRIRs at different frequencies and derive the expected adaptation mechanisms already from there. This would greatly simplify the overall concept.</p></disp-quote><p>See response to Essential revision 4 above.</p><disp-quote content-type="editor-comment"><p>As a replacement for the cochleagram, I can recommend to apply the auditory periphery model described in: Bruce, I. C., Erfani, Y., and Zilany, M. S. A. (2018). &quot;A phenomenological model of the synapse between the inner hair cell and auditory nerve: Implications of limited neurotransmitter release sites,&quot; Hearing Research 360:40-54. This and other alternatives are readily available as part of the Auditory Modeling Toolbox (https://www.amtoolbox.org).</p></disp-quote><p>See response to Essential revision 3 above.</p><disp-quote content-type="editor-comment"><p>l.610: How was the &quot;low threshold&quot; defined that was applied to limit the log power values?</p></disp-quote><p>The threshold was used to avoid the problem that log(0) = -infinity. The threshold was set to 94dB. This value ensures that almost all the output of the log function is above threshold. We have now added this threshold value to our methods section. In practice, over 99.9% of our cochleagram values were above the threshold of -94dB.</p><p>Lines 664-667: “The resulting power in each channel at each time point was converted to log values and, to avoid log(0), any value below a low threshold (-94dB) was set to that threshold (&gt;99.9% of instances were above this threshold).”</p><disp-quote content-type="editor-comment"><p>l.620: This definition of RT10 is inconsistent with the nomenclature used in ISO standards. There, reverberation time is defined as the time it takes the sound energy to decay by 60 dB, denoted as RT60. To estimate this metric, one can also assume a linear decay and measure only the time it takes to decay, for instance, by 20 dB and then multiply by 3. Still, RT20 is then the result of this extrapolation, not the 20-dB-decay time itself. According to that definition, your reverberation times would need to be multiplied by six. Hence, your large room had a reverberation time of about 2.6 s, similar to a small cathedral.</p></disp-quote><p>We reported the RT<sub>10</sub> values in our original manuscript because the time frame of RT<sub>10</sub> (&lt;600ms) is similar to that of the reverberation adaptation we have observed (over a timeframe of 200ms). However, the Reviewer makes a fair point about RT<sub>60</sub> values being more comparable to other literature. We have edited the plot in question to now report both the RT<sub>60</sub> (left y-axis) and RT<sub>10</sub> (right y-axis) for the rooms we tested. We also now discuss our results primarily in terms of RT<sub>60</sub> rather than RT<sub>10</sub>. As these values are just scaled versions of one another, the correlations remain unchanged.</p><p>Lines 373-382: “The decay rate can be measured as the reverberation time RT<sub>60</sub>, which is the time necessary for the sound level to decay by 60dB relative to an initial sound impulse (similarly, RT<sub>10</sub> is the time necessary for a decay by 10dB). The frequency-dependent RT<sub>60</sub> and RT<sub>10</sub> values for our small and large rooms are plotted in Figure 7C. The impulse responses of both rooms exhibited a decrease in RT<sub>60</sub> values as a function of frequency (Pearson's correlation, small room: r = -0.82, p = 1.1 x 10<sup>-10</sup>, large room: r = -0.91, p = 8.0 x 10<sup>-10</sup>). This faster decay for higher frequencies can also be observed in the spectrograms of the impulse responses (Figure 7—figure supplement 1). Therefore, the frequency-dependent delay in the inhibitory components of our dereverberation model and cortical STRFs paralleled the RT<sub>60</sub> frequency profile of the virtual rooms in which the sounds were presented.”</p><p>We also added the RT<sub>60</sub> values to our descriptions of the room sizes at the beginning of our Results section.</p><p>Lines 86-88: “The dimensions of the smaller room made it less reverberant (reverberation time: RT<sub>10</sub> = 130ms, RT<sub>60</sub> = 0.78s) than the larger room RT<sub>10</sub> = 430ms, RT<sub>60</sub> = 2.6s.”</p><p>And throughout our methods section. For example:</p><p>Lines 642-643: “The reverberation time RT<sub>60</sub> is the time necessary for the sound level to decay by 60dB relative to an initial sound impulse, while RT<sub>10</sub> is the time for the sound level to decay by 10dB.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This is a very interesting study that tests how the auditory system adapts to reverberant acoustic scenes. The paper is written well, and the results are overall compelling. I have a few comments that hopefully can strengthen the claims of the study.</p><p>1. It is important to note that dereverberation requires highly nonlinear acoustic processing. The dereverberation algorithms that have been used in speech processing typically try to mask the spectrogram as a method to recover the &quot;direct path&quot; and remove the &quot;reflections&quot;. While the resulting &quot;high-pass&quot; filter found in the linear filtering attempts to approximate this nonlinear operation, it is not very effective when used in realistic conditions. While the linear modeling framework here allows the authors to perform a straightforward comparison with auditory receptive fields, this limitation should be noted so the readers are aware of the true difficulty of this task and hence, unexplained mechanisms that remain to be found.</p></disp-quote><p>See response to Essential revision 3 above.</p><disp-quote content-type="editor-comment"><p>2. A big issue which the authors are well aware of is the differences between the stimuli used to find the receptive field in different reverberant scenarios. While the authors do a good job to show that the differences are not merely due to the statistics of the stimuli, particularly by showing a different response to the probe sound, they cannot claim that &quot;all&quot; of the observed changes are due to adaptation. It is likely reflecting a mix of both, some due to the change in the stimulus correlation and some due to adaptation. Currently, this inherent limitation is not acknowledged.</p></disp-quote><p>See response to Essential revision 5 above.</p><disp-quote content-type="editor-comment"><p>3. Related to 2, I think the control experiments that were done to show the changes in the response to the probe sound in different reverberant conditions are worthy of inclusion in the main figures. Perhaps a summary of that can be added to Figure 3, as this is a very important result, without which the observed changes are not very compelling.</p></disp-quote><p>We have moved these into the main manuscript figures. See response to Essential Revision 2 above.</p><disp-quote content-type="editor-comment"><p>4. On the same point, while it is an important observation that the responses to the probe stimulus (non-reverberant) are different in different reverberation contexts, a complementary observation that is missing is the similarity of the cortical responses to varying degrees of reverberation. In other words, if the STRFs change as suggested to produce a less variable response to reverberation, then the responses to the same sound with different reverb should stay constant and similar to the anechoic sound. While invariant cortical responses have been shown in other studies (including ferrets), it will strengthen this study if they can also confirm the presence of that effect which is the subject of this study.</p></disp-quote><p>See response to Essential revision 1 above.</p><disp-quote content-type="editor-comment"><p>5. Regarding the discussion of the feedforward/feedback nature of the adaptation to changing background statistics, Khalighinejad et al. also showed that the suppression of background noise is the same when the subject is actively performing speech-in-noise perception and when the subject is distracted by a visual task (Figure 5). Perhaps this observation can strengthen the argument regarding the anesthetized/awake conditions and the nature of the adaptation (lines 447-451).</p></disp-quote><p>See response to Essential revision 6 above.</p></body></sub-article></article>