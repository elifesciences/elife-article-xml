<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83708</article-id><article-id pub-id-type="doi">10.7554/eLife.83708</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Involvement of superior colliculus in complex figure detection of mice</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-293682"><name><surname>Cazemier</surname><given-names>J Leonie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2875-6283</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-340143"><name><surname>Haak</surname><given-names>Robin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-293683"><name><surname>Tran</surname><given-names>TK Loan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-293684"><name><surname>Hsu</surname><given-names>Ann TY</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-293685"><name><surname>Husic</surname><given-names>Medina</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-340144"><name><surname>Peri</surname><given-names>Brandon D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-293686"><name><surname>Kirchberger</surname><given-names>Lisa</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-50550"><name><surname>Self</surname><given-names>Matthew W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5731-579X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-33135"><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1625-0034</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-153567"><name><surname>Heimel</surname><given-names>J Alexander</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5291-4184</contrib-id><email>a.heimel@nin.knaw.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043c0p156</institution-id><institution>Department of Circuits, Structure &amp; Function, The Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and Sciences (KNAW)</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043c0p156</institution-id><institution>Department of Vision and Cognition, The Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and Sciences (KNAW)</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Department of Integrative Neurophysiology, VU University</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03t4gr691</institution-id><institution>Department of Psychiatry, Academic Medical Centre</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000zhpw23</institution-id><institution>Laboratory of Visual Brain Therapy, Sorbonne Université, Institut National de la Santé et de la Recherche Médicale, Centre National de la Recherche Scientifique, Institut de la Vision</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wolff</surname><given-names>Mathieu</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057qpr032</institution-id><institution>CNRS, University of Bordeaux</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>25</day><month>01</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>13</volume><elocation-id>e83708</elocation-id><history><date date-type="received" iso-8601-date="2022-09-28"><day>28</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-01-08"><day>08</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-09-26"><day>26</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.25.509365"/></event></pub-history><permissions><copyright-statement>© 2024, Cazemier et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Cazemier et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83708-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83708-figures-v1.pdf"/><abstract><p>Object detection is an essential function of the visual system. Although the visual cortex plays an important role in object detection, the superior colliculus can support detection when the visual cortex is ablated or silenced. Moreover, it has been shown that superficial layers of mouse SC (sSC) encode visual features of complex objects, and that this code is not inherited from the primary visual cortex. This suggests that mouse sSC may provide a significant contribution to complex object vision. Here, we use optogenetics to show that mouse sSC is involved in figure detection based on differences in figure contrast, orientation, and phase. Additionally, our neural recordings show that in mouse sSC, image elements that belong to a figure elicit stronger activity than those same elements when they are part of the background. The discriminability of this neural code is higher for correct trials than for incorrect trials. Our results provide new insight into the behavioral relevance of the visual processing that takes place in sSC.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>superior colliculus</kwd><kwd>vision</kwd><kwd>electrophysiology</kwd><kwd>object detection</kwd><kwd>optogenetics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>FLAG-ERA CHAMPmouse</award-id><principal-award-recipient><name><surname>Heimel</surname><given-names>J Alexander</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Mouse superior colliculus is involved in figure detection based on differences in contrast and texture orientation and phase.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When using vision to survey the environment, the brain segregates objects from each other and from the background. This object detection is an essential function of the visual system, since behavior typically needs to be performed in relation to the objects surrounding the organism. The primary visual cortex (V1) is involved in object detection and segregation (e.g. <xref ref-type="bibr" rid="bib47">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib71">Ress et al., 2000</xref>; <xref ref-type="bibr" rid="bib49">Li et al., 2006</xref>). Silencing or ablating V1, however, does not completely abolish the detection of simple visual stimuli in mice (<xref ref-type="bibr" rid="bib70">Prusky and Douglas, 2004</xref>; <xref ref-type="bibr" rid="bib30">Glickfeld et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Resulaj et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). Furthermore, humans with bilateral lesions of V1 remain capable of detecting some visual stimuli, even in the absence of conscious vision; a phenomenon termed ‘blindsight’ (<xref ref-type="bibr" rid="bib5">Ajina and Bridge, 2017</xref>). This capacity is likely to be mediated by the superior colliculus (SC), a sensorimotor hub in the midbrain (<xref ref-type="bibr" rid="bib78">Tamietto et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Kato et al., 2011</xref>; <xref ref-type="bibr" rid="bib37">Ito and Feldheim, 2018</xref>; <xref ref-type="bibr" rid="bib44">Kinoshita et al., 2019</xref>).</p><p>The SC receives direct input from the retina, as well as from V1 and other sensory areas (<xref ref-type="bibr" rid="bib10">Basso and May, 2017</xref>), and mediates orienting responses to salient stimuli in primates and rodents (<xref ref-type="bibr" rid="bib86">White and Munoz, 2012</xref>; <xref ref-type="bibr" rid="bib7">Allen et al., 2021</xref>). In mice, detection of change in isolated visual stimuli is impaired in a space- and time-specific manner when the SC is locally and transiently inhibited (<xref ref-type="bibr" rid="bib84">Wang et al., 2020</xref>). The mouse SC is also involved in hunting (<xref ref-type="bibr" rid="bib33">Hoy et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Shang et al., 2019</xref>) as well as defensive responses (<xref ref-type="bibr" rid="bib23">Evans et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Shang et al., 2018</xref>) to visual stimuli that are clearly isolated from the background. In recent years, a growing number of experiments point towards a role for the SC in more complex processes that are usually associated with the cerebral cortex (<xref ref-type="bibr" rid="bib46">Krauzlis et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Basso and May, 2017</xref>; <xref ref-type="bibr" rid="bib11">Basso et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Jun et al., 2021</xref>; <xref ref-type="bibr" rid="bib91">Zhang et al., 2021</xref>). It is, however, not yet clear whether SC is also involved in detection of stimuli on a complex background.</p><p>When V1 is transiently silenced, mice are still able to detect stimuli that are defined by contrast on a homogeneous background, but they cannot detect texture-defined figures that only differ from the surrounding textured background by orientation (<xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). Models for detection of texture-defined figure stimuli focus on the visual cortex and presume that interactions within V1 enhance the neural response to the figure edges, and that higher cortical visual areas feed back to ‘fill in’ the neuronal representation of the figure in V1 (<xref ref-type="bibr" rid="bib73">Roelfsema et al., 2002</xref>; <xref ref-type="bibr" rid="bib68">Poort et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">Liang et al., 2017</xref>). Indeed, neurons in V1 of mice and primates respond more vigorously to image elements in their receptive fields that differ from the surrounding texture (<xref ref-type="bibr" rid="bib47">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib68">Poort et al., 2012</xref>; <xref ref-type="bibr" rid="bib75">Self et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">Li et al., 2018</xref>; <xref ref-type="bibr" rid="bib74">Schnabel et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). This figure-ground modulation (FGM) depends on feedback from higher visual cortical areas, as was predicted by the models (<xref ref-type="bibr" rid="bib73">Roelfsema et al., 2002</xref>; <xref ref-type="bibr" rid="bib42">Keller et al., 2020</xref>; <xref ref-type="bibr" rid="bib66">Pak et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>).</p><p>However, the ability to encode visual contextual effects is not exclusive to the visual cortex. The superficial layers of the rodent SC (sSC) have been shown to display orientation-tuned surround suppression (<xref ref-type="bibr" rid="bib29">Girman and Lund, 2007</xref>; <xref ref-type="bibr" rid="bib2">Ahmadlou et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">De Franceschi and Solomon, 2020</xref>): the responses of orientation-tuned neurons to an optimally oriented grating stimulus are attenuated when the surround contains a grating of the same orientation, whereas the responses are less suppressed or facilitated when the surround contains an orthogonal grating (<xref ref-type="bibr" rid="bib8">Allman et al., 1985</xref>). This property is thought to play an important role in object segregation (<xref ref-type="bibr" rid="bib47">Lamme, 1995</xref>). Interestingly, <xref ref-type="bibr" rid="bib2">Ahmadlou et al., 2017</xref> showed that the orientation-tuned surround suppression in sSC is computed independently of V1. The presence of this contextual modulation in the SC, combined with research showing the involvement of SC in visual detection (<xref ref-type="bibr" rid="bib84">Wang et al., 2020</xref>) leads us to hypothesize that SC in the mouse might also be involved in detecting and segregating stimuli from a complex background. Here, we show that inhibiting the sSC reduces performance on a variety of figure detection tasks – indicating a role for the sSC in this behavior. Furthermore, we use extracellular recordings in mice performing figure detection to show that mouse SC indeed contains a neural code for figure detection.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Superior colliculus is involved in figure detection</title><p>To test the involvement of superior colliculus in figure detection based on different features, we trained mice on three different versions of a figure detection task: a task based on figure contrast, a task based on figure orientation, and a task based on figure phase (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). On each trial, the mice had to indicate the position of the figure (left vs. right) by licking the corresponding side of a Y-shaped lick spout (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To test the involvement of the sSC in object detection, we injected a viral vector with Cre-dependent ChR2, an excitatory opsin, in sSC of GAD2-Cre mice. We subsequently implanted optic fibers to target blue light onto the SC (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Laser light activated the inhibitory neurons in sSC and reduced the overall activity in the superior colliculus (<xref ref-type="bibr" rid="bib3">Ahmadlou et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Hu et al., 2019</xref>). In order to test not only <italic>if</italic>, but also <italic>when</italic> superior colliculus is involved in figure detection, we inhibited the sSC at different latencies (0–200 ms) after stimulus onset. The mice were allowed to respond from 200 ms after stimulus onset (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). A total of n=8 mice were used for these experiments. The mice typically performed 100–250 trials per session (one session per day, five sessions per week, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>), and the recording period lasted for 2–5 months (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Superior colliculus is involved in figure detection.</title><p>(<bold>A</bold>) The stimulus types that were used for the figure detection task. The stimulus consisted of a static grating that differed from the background in either contrast (top left), orientation (top right), or phase (bottom). (<bold>B</bold>) Two example stimuli (both orientation task). Licking on the side corresponding to the figure constituted a hit, a lick on the other side an error. (<bold>C</bold>) Top: Schematic illustration of viral injections and optic fiber implantation. Bottom: histological verification of viral expression. Red: ChR2-mCherry. Blue: DAPI. Scale bar is 600 μm.(<bold>D</bold>) Timing of the task. We optogenetically inhibited activity in superficial layers of the SC (sSC) by activating sSC GABAergic neurons in both hemispheres at different delays after stimulus appearance. The mice reported the figure location after 200 ms by licking on the same side as the figure. (<bold>E</bold>) Inhibition of sSC significantly decreased task performance for each figure detection task. Accuracy is defined as hits/(hits + errors). The accuracy on unperturbed trials without the laser condition is indicated by ‘no.’ Colored dots represent means ± SEM of accuracies across mice. Arrow and error bar indicate mean ± SD of bootstrapped fitted inflection points. Dashed line indicates chance level performance. *p&lt;0.05, **p&lt;0.01. Detailed statistics can be found in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>It contains details of statistical tests for <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplements 2</xref>–<xref ref-type="fig" rid="fig1s5">5</xref>.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-83708-fig1-data1-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Example behavioral session and learning over sessions.</title><p>(<bold>A</bold>) Example behavioral session. Optogenetic interference took place in trials indicated by blue tag on the right. Session performance was 74% on all trials, and 68% on trials with laser on. (<bold>B</bold>) Performance over all training/recording sessions for one mouse. Training started with the contrast task (gray background; green sessions), after which background complexity was increased by a change in luminance (yellow sessions), a background grating increasing in contrast (orange), and finally a full contrast grating as background (red). The session of figure A is indicated by the blue arrowhead. The first three sessions did not assess the performance of the mouse as the mouse was learning to lick the spout.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Reduction of visually-evoked neural activity in superficial layers of the SC (sSC) of awake mice by optogenetic activation of GABAergic neurons.</title><p>(<bold>A</bold>) Extracellular recording in sSC of awake GAD2-Cre mice transfected with channelrhodopsin. (<bold>B</bold>) Example spike raster plot of two putative GAD2-positive neurons during a 50 ms laser pulse. (<bold>C</bold>) Rate of multi- and single-units in the sSC during the first 10 ms of a 50 ms laser pulse and the last 10 ms before laser onset. Putative GAD2-positive neurons that show an increased rate (p&lt;0.05, zeta-test) indicated in orange. Example units of (<bold>B</bold>) are indicated by cyan (top example) and magenta (bottom example). (<bold>D</bold>) Average number of spikes evoked in a 10 ms window after laser onset by putative GAD2-positive neurons. (<bold>E</bold>) First spike latency and (<bold>F</bold>) Jitter of the first spike of putative GAD2-positive neurons evoked by laser stimulation. (<bold>D–F</bold>) red lines indicate the mean. (<bold>G</bold>) Mean (± SEM) population responses of visually responsive neurons in sSC of awake mice to a static grating in laser off (black) and laser on (blue) conditions. Laser was turned on approx. 30 ms before stimulus onset Inset, Average firing rate 0–200 ms after grating onset in laser off and laser on conditions. Red lines indicate the mean. ***p&lt;0.001. (<bold>H</bold>) Scatter plot of mean rates during presentation of a static grating for laser on and off conditions of putative GAD2-positive neurons (orange), visually responsive (closed circles), and non-visually responsive cells (open circles). Dashed line indicates parity. (<bold>I</bold>) Percentage change in average firing rate 0–200 ms after grating onset in laser on relative to laser off trials. (<bold>J</bold>) The reduction of the rate during optogenetic interference in GAD2-Cre mice is present across sessions. (<bold>K</bold>) The reduction of the rate during optogenetic interference in GAD2-Cre mice is present across the entire depth of sSC. Depth is relative to the dorsal SC surface. (<bold>J–K</bold>) Red and black lines indicate the mean and median, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>No visual response to the laser light in wild-type mice.</title><p>(<bold>A</bold>) Extracellular recording in sSC of awake wild-type mice. (<bold>B</bold>) Population rates (mean ± SEM) show no response to a 50 ms laser pulse around the time of response to retinal visual input, indicating that the laser does not cause a visual response. Laser artifact is removed ± 1 ms around laser onset and offset. (<bold>C</bold>) Firing rates before and for the 200 ms following the start of the 50 ms laser pulse for single- and multi-units in sSC of 2 wild-type (WT) mice (49 visually responsive units, 35 non-visual units) show no units responding to the laser light. (<bold>D</bold>) Mean (± SEM) population responses of visually responsive neurons in sSC of awake mice to a static grating in laser off (black) and laser on (blue) trials. Laser was turned on approx. 30 ms before stimulus onset (109 visually responsive units, 111 non-visual units). (<bold>E</bold>) Mean rates per unit during visual stimulation for laser on versus laser off trials. Red lines indicate the mean. (<bold>E</bold>) Laser illumination causes a small, but significant, reduction in visual response (p&lt;0.001, LME).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Superior colliculus is involved in figure detection: data for individual mice.</title><p>(<bold>A</bold>) Supplementary data to <xref ref-type="fig" rid="fig1">Figure 1E</xref>, showing accuracies for individual mice. Inhibition of superficial layers of the SC (sSC) significantly decreased task performance for each figure detection task. The accuracy on unperturbed trials without the laser condition is indicated by ‘no.’ Colored dots represent means ± SEM of accuracies across mice. Gray symbols indicate accuracies of individual mice for each laser onset condition. Arrow and error bars indicate mean ± SD of bootstrapped fitted inflection points. Dashed line indicates chance level performance. *p&lt;0.05, **p&lt;0.01. (<bold>B</bold>) Average proportions of hit, error, and miss responses separated by laser onset and trial type. The without-laser condition is indicated by ‘no.’ Colored dots represent means ± SEM of average proportions across mice. Curves indicate logistic fits.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Optogenetic inhibition by activating inhibitory superficial layers of the SC (sSC) neurons does not affect lick rates and reaction times.</title><p>(<bold>A</bold>) Example of licking behavior in four different types of trials. Upward ticks indicate licks on the correct side, downward ticks indicate licks on the incorrect side. Top row shows Hit trials, bottom row shows Error trials. Left column shows laser OFF trials, right column shows laser ON trials. (<bold>B</bold>) Mean (± SEM) lick rates for each of the trial types, across mice. The mice typically only licked one side on each trial. Mice continued to lick on Hit trials, as they received the water reward through the lick spout. (<bold>C</bold>) Maximum lick rates (on Hit trials) were not significantly affected by the latency of optogenetic inhibition. Dots represent means ± SEM of reaction times across mice. The reaction time on unperturbed trials without the laser condition is indicated by ‘no.’ (<bold>D</bold>) Reaction times of the mice were not significantly affected by the latency of optogenetic inhibition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig1-figsupp5-v1.tif"/></fig></fig-group><p>Control experiments with recordings in the sSC during activation of GAD2-positive neurons (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A–F</xref>) showed that the rates during visual stimulation in sSC were significantly reduced by 76% on average (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2G</xref>). The net reduction of the evoked rates was also present in putative GAD2-positive neurons and was consistent across different recording sessions and depths (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2H–K</xref>). In control experiments without channelrhodopsin, the laser light did not cause responses in the SC that could come from stimulation of the photoreceptors through the brain (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Direct laser light on the brain also did not affect accuracy in this task (<xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). However, because it was impossible to exclude the possibility of some laser light reaching the retina, we also placed a blue LED above the head of the mouse to provide ambient blue light that flashed at random intervals, which the animal learned to ignore.</p><p>For each of the three-figure detection tasks, the onset of the sSC inhibition significantly affected the accuracy of the mice (<xref ref-type="fig" rid="fig1">Figure 1E</xref> and <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4A</xref>)<bold>,</bold> p=0.005, p=0.027, and p=0.003 for the contrast, orientation, and phase task, respectively. For details on all statistics, see the source data attached to the figures. Inclusion of the trials without responses (misses) as error trials gives similar results (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4B</xref> p=0.00011, p=0.055, and p=0.00050 for a dependence of the proportion of hits of the total number of trials on the onset of the optogenetic manipulation for the contrast, orientation and phase task, respectively). The optogenetic interference did not significantly influence the mice’s licking rates (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5A–C</xref>) or reaction times (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5D</xref>). These experiments, therefore, suggest that sSC is involved in figure detection, be it based on grating contrast, orientation, or phase.</p><p>Although the accuracies of the mice were decreased by the optogenetic manipulation of the sSC, they typically were still above the chance level. This might be related to the incomplete silencing of the sSC, although it is also possible that the thalamocortical pathway is involved in figure detection in parallel to the sSC. The accuracy of the mice recovered when we postponed the optogenetic interference. For the contrast task, the accuracy reached the half-maximal value when we postponed the laser onset to 99 ms (±8 ms). In the orientation and phase task, the mice reached their half-maximum performance when we postponed the laser onset to 156 ms (±35 ms) and 134 ms (±30 ms), respectively. The data suggest that short-lasting activity in the sSC suffices for the contrast detection task, whereas orientation-and phase-defined object detection necessitate a longer phase of sSC activity. Based on comparisons between the response accuracy and processing time of the mice in their experiments, <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref> concluded that it is not likely that these differences depend on task difficulty for the mouse. Rather, they probably reflect differences in the encoding strategy of the brain for the different tasks.</p></sec><sec id="s2-2"><title>sSC shows figure-ground modulation for contrast- and orientation-defined figures</title><p>Having confirmed that the sSC is involved when performing figure detection tasks, we next set out to investigate if neurons in the superior colliculus encode the visual information needed for the task, or encode decision-related information. Recent experiments have demonstrated that orientation- and phase-defined figures elicit more activity in the visual cortex of the mouse than the background image does; a phenomenon called figure-ground modulation (FGM) (<xref ref-type="bibr" rid="bib47">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). To test whether this also occurs in sSC, we recorded neural activity in SC using 32-channel laminar electrodes while the mice performed the figure detection tasks (<xref ref-type="fig" rid="fig2">Figure 2A–B</xref>). In this experiment, we kept the lick spout away from the mouse until 500 ms after stimulus onset to prevent electrical noise caused by premature licks from interfering with the spike detection. After this delay, the lick spout automatically moved within licking distance (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The mice could perform the tasks reliably above chance level during these recording sessions (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). During each recording session, we first mapped the receptive fields (RFs) of the recorded sites. We then set up the visual stimuli of the figure detection task such that in each trial, the figure stimulus was placed either over the RF, or 50–60 degrees lateral from the RF in the other visual hemifield (<xref ref-type="fig" rid="fig2">Figure 2E–F</xref>). This way, as the mouse was reporting the side of the figure stimulus in each trial, the recorded neurons would variably respond to the figure or to the background (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). We recorded neural activity from five mice.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Superior colliculus activity elicited by contrast and figure-ground stimuli.</title><p>(<bold>A</bold>) Schematic illustration of setup. (<bold>B</bold>) Histological verification of electrode track. Blue: DAPI. Red: diI. Scale bar is 600 μm. (<bold>C</bold>) Timing of the task. The mice could report the figure location after 500 ms. (<bold>D</bold>) Accuracy of the mice in each task, mean ± SEM. (<bold>E</bold>) Estimated receptive fields of neurons with a receptive field (RF) entirely inside the figure. (<bold>F–H</bold>) Example neuron. (<bold>F</bold>) Receptive field of an example neuron. Black circles indicate the position of the figure (on top of RF) and ground (outside of RF) stimulus in the visual field. Red circle indicates estimated RF. (<bold>G</bold>) Raster plot, sorted by task and trial type. Each dot indicates a spike. Green, red and blue colors indicate contrast, orientation, and phase task trials, respectively. Brighter colors indicate figure trials, and darker colors indicate ground trials. (<bold>H</bold>) Mean (± SEM) activity of the example neuron for each task. (<bold>I</bold>) Mean (± SEM) population responses for each task. Gray patches indicate time clusters where the difference between figure and ground is significant (p&lt;0.05). (<bold>J</bold>) Difference between figure and ground responses in each task and estimated onset of the response difference. Colored lines indicate data, black lines indicate fit of the response. Arrows indicate the onset latency of the response difference. Detailed statistics can be found in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>It contains details of statistical tests for <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-83708-fig2-data1-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Putative multisensory neurons show task-related responses.</title><p>(<bold>A</bold>) Mean responses of all recorded neurons, sorted by the time of their peak response. A small group of neurons (indicated by green sidebars) is time-locked to the stimulus (p&lt;0.05, Zeta-test) and most active around 700 ms: just after the lick spout moves towards the mouse, but before the mouse licks. (<bold>B</bold>) Responses of example putative multisensory neuron. Top: Raster plot where each dot indicates a spike. Red and blue indicate orientation and phase task trials, respectively. Brighter colors indicate figure trials, darker colors indicate ground trials. Bottom: Mean response of example neuron for figure vs. ground trials (orientation and phase trials combined). Shading indicates SEM. (<bold>C</bold>) Population figure-ground responses for hits and errors across all putative multisensory cells. Dashed gray line indicates the mean reaction time of the mice. Note that the response difference between figure and ground trials is larger for hits than for errors. (<bold>D</bold>) Neuronal d-primes for hit and error trials, p=0.044 for the main effect of response (Hit vs. Error), post-hoc analysis showed p&gt;0.05 for the individual tasks. (<bold>E</bold>) The variances of estimated histological depth of visual vs. putative multisensory cells are different. *p=0.046. (<bold>F</bold>) Neural activity for example putative multisensory neurons plotted together with mean eye movement speed for different (stimulus/response) types of trials. Shading indicates SEM. (<bold>G</bold>) No significant correlation between eye movement speed and neural activity of putative multisensory neurons (p=0.49). Dots indicate individual neurons. Blue dot indicates the example neuron. (<bold>H</bold>) Population responses for hits vs. errors for all putative multisensory cells, centered on the time of the first lick in the response window. Note that the distinctive peak from (<bold>C</bold>) is not present here, suggesting that the peak response is locked to the stimulus onset, and not to the first lick.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Figure-ground modulation of individual neurons for different stimuli.</title><p>(<bold>A</bold>) Normalized figure vs. ground response for contrast-defined figures during the period of significant difference in <xref ref-type="fig" rid="fig2">Figure 2I</xref>. (<bold>B</bold>) Normalized figure versus ground response for orientation-defined figures during the period of significant difference in <xref ref-type="fig" rid="fig2">Figure 2I</xref>. (<bold>C</bold>) Normalized figure versus ground response for phase-defined figures during the period of significant difference for the orientation-defined figures in <xref ref-type="fig" rid="fig2">Figure 2I</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Eye position and pupil dilation during object detection tasks.</title><p>(<bold>A</bold>) Normalized eye tracking data and average firing rate of the recorded neurons during one example trial. (<bold>B</bold>) Mean (± SEM) eye X/Y position and pupil dilation across mice. Trials with early (0–450 ms) eye movements were excluded from analysis. In the included trials, the mice show no eye movements or pupil dilation changes during the peak of the visual response (0–250 ms).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig2-figsupp3-v1.tif"/></fig></fig-group><p>Most neurons showed a short latency visual response (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>), although not all neurons in our data set were exclusively visually responsive and some appeared to respond to the movement of the lick spout (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for an extensive analysis of these). For analyses in the main figures, we only included neurons with a visual response and an estimated RF completely inside of the figure stimulus (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), except where mentioned otherwise. In these visually responsive neurons, the contrast-defined figure elicited a strong response compared to the gray background, with an estimated onset of 67 ms (<xref ref-type="fig" rid="fig2">Figure 2H–J</xref>, left; p&lt;0.05 from 68 to 99 ms in <xref ref-type="fig" rid="fig2">Figure 2I</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). For the orientation-defined figures, we also found significant FGM, with an estimated onset of 75 ms (<xref ref-type="fig" rid="fig2">Figure 2I–J</xref>, middle; p&lt;0.05 from 84 to 129 ms in <xref ref-type="fig" rid="fig2">Figure 2I</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>). When the figure was defined by a phase difference, FGM was not significant (<xref ref-type="fig" rid="fig2">Figure 2I–J</xref>, right; p&gt;0.05 for all time bins in <xref ref-type="fig" rid="fig2">Figure 2I</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref>). We conclude that the activity of sSC neurons elicited by contrast- and orientation-based figures is stronger than that elicited by a background. As we had excluded trials with eye movements from the analysis (see methods section), the neural modulation in the contrast and orientation tasks could not be explained by eye movements or changes in pupil dilation (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p></sec><sec id="s2-3"><title>Superior colliculus represents the location of figures</title><p>Our data indicates that phase-defined figures on average elicit a similar visual response as the background. However, the task performance of the mice did decrease when inhibiting the sSC during the phase task. We therefore examined the neuronal responses in more detail. Whereas the results in <xref ref-type="fig" rid="fig2">Figure 2</xref> represented neurons with RFs confined to the figure interior, we also recorded neurons with RFs on the edge of the figure stimulus (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For these neurons, we found a significantly higher response for figure vs. ground in the orientation task, but not in the phase task (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; p&lt;0.05 from 81 to 102 ms in the orientation task, p&gt;0.05 for all time bins in the phase task, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). We did not record enough edge-RF data to perform this analysis for the contrast task so it is excluded here.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Decoding the stimulus identity from population responses in superior colliculus.</title><p>(<bold>A</bold>) Estimated receptive fields of the neurons with a receptive field (RF) on the figure edge. (<bold>B</bold>) Mean (± SEM) population responses of the RF edge neurons for each task. Gray patches indicate time clusters where the difference between figure and ground is significant (p&lt;0.05). (<bold>C</bold>) Schematic illustration of bootstrapping and decoding process. (<bold>D</bold>) Decoding performance of a linear support vector machine (SVM) classifier for each task with neurons with RF inside the figure and on the figure edge. Performance was computed using a sliding window of 50 ms in steps of 10 ms. Gray regions indicate decoding performance significantly different from chance (p&lt;0.05). (<bold>E</bold>) Mean (± SEM) of relative model weights for each task and RF type. *p&lt;0.05 (<bold>F</bold>) Neuronal d-primes during the window with the best decoding performance. Black bars indicate the mean (±95% confidence interval) population d-primes of bootstraps with shuffled trial identities. Colored dots indicate the real mean of d-primes in the population. *p&lt;0.05, ***p&lt;0.001. Detailed statistics can be found in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>It contains details of statistical tests for <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-83708-fig3-data1-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Figure-ground modulation for neurons with receptive field (RF) on figure edge.</title><p>(<bold>A</bold>) Normalized figure versus ground response of neurons with an RF on the figure edge for orientation-defined figures during the period of significant difference in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. (<bold>B</bold>) Normalized figure versus ground response of neurons with an RF on the figure edge for phase-defined figures during the period of significant difference for orientation-defined stimulus in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig3-figsupp1-v1.tif"/></fig></fig-group><p>We conclude that the phase-defined figure-ground stimulus did not cause a significantly enhanced population response in the sSC. It is, however, conceivable that the figure could be represented by some neurons that enhance their response and others that decrease their response, without an overall influence on the firing rate at the population level. To examine this possibility, we used a linear support vector machine (SVM) model to decode the stimulus identity (figure vs. ground) from the recorded population. As the data set was recorded during mouse behavior, it was not balanced with regard to trial numbers per trial type. In order to correct this, we used a bootstrapping strategy (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). In brief, we trained the model many times, each time on a different balanced pseudo-randomized sub-selection of the trials, and used leave-one-out cross-validation to test the model performance. The decoding results are shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. For the orientation task, the decoder could detect the stimulus identity at a performance of around 75% (lowest p&lt;0.001 at a time window 80–130 ms). Interestingly, the decoder also detected the phase stimulus identity above chance level, more specifically in later time windows, after the peak of the visual response (lowest p&lt;0.001 at time window 180–230 ms). These results are in line with the onset and relative strength of the figure-ground modulation for each task, and show that visual information from both the orientation and the phase task is represented in the sSC.</p><p>To understand which information was used by the SVM model, we first analyzed the model weights of the individual neurons using a linear mixed effects (LME) model (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). The relative normalized weights were significantly higher for the orientation task compared to the phase task. This indicates that, whereas sSC neurons encode the orientation-based figures by increasing their firing rate, some neurons may indeed encode the phase-based figures by decreasing their firing rate, leading to negative relative weights. We also computed the d-prime of the recorded neurons; a measure of the reliability of the difference between the figure and ground response on single trials. For both tasks, the d-primes were significantly higher than the chance level in the time window with the best SVM decoding performance (<xref ref-type="fig" rid="fig3">Figure 3F</xref>; p&lt;0.001; and p&lt;0.05 for orientation and phase, respectively). We conclude that although the average difference between figure vs. ground responses was small in the phase task, the variability of the neuronal responses was low enough for reliable decoding.</p></sec><sec id="s2-4"><title>Different discriminability in sSC preceding hits vs. errors</title><p>Because superior colliculus is a sensorimotor hub, we wanted to further investigate whether the colliculus might not only encode the visual stimulus but also the decision of the mouse. To this end, we split up the data from the visually responsive neurons between hit (correct) and error (incorrect) trials (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), and analyzed the firing rates between stimulus onset and the response of the mouse (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We did not have enough data to perform this analysis for the contrast task, and we, therefore, focused on the orientation and phase task. To statistically compare the response difference between hit and error trials, we computed d-primes and created a linear mixed effects model (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Since we recorded all versions of the task during single recording sessions, we had recorded neural responses for both tasks from the neurons, and we were able to pool the data from the two tasks. The discriminability (d-prime) was significantly higher for hit trials than error trials (p=0.001), showing that activity in the sSC is reflected in the behavioral performance. Post-hoc analysis showed that the difference in discriminability was mainly driven by the difference between hits and errors in the orientation task (<xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>; p=0.001; and p=0.303 for orientation and phase, respectively).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Different discriminability in superficial layers of the SC (sSC) for hits vs.errors.</title><p>(<bold>A</bold>) Two example stimuli (both orientation task). Licking on the side corresponding to the figure constituted a hit, and vice versa. RF: receptive field. Note that the information inside the receptive field is the same between the two stimuli. (<bold>B</bold>) Population responses for hits vs. errors. Dashed gray line indicates the mean reaction time of the mice. Note that the difference between figure and ground is larger for hits than errors. (<bold>C</bold>) Neuronal d-primes were higher for hit trials than for error trials. **p&lt;0.01. Detailed statistics can be found in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>It contains details of statistical tests for <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-83708-fig4-data1-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Figure-ground modulation for neurons split in hit and error trials.</title><p>(<bold>A</bold>) Normalized figure versus ground response of neurons for orientation-defined figures during the period between stimulus onset and first lick, split in hit (left) and error trials (right). (B) Normalized figure versus ground response of neurons with an RF on the figure edge for phase-defined figures during the period between stimulus onset and first lick, split in hit (left) and error trials (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83708-fig4-figsupp1-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our experiments show that the superficial layers of superior colliculus are involved in detecting objects on a non-homogeneous background and detecting objects based on figure contrast, orientation, and phase. Indeed, neurons in sSC show an increased response to figure stimuli compared to ground stimuli in both the contrast and orientation task. We did not find a significantly increased population response for phase-defined figures, but a linear SVM decoder indicated that the response difference was consistent enough to decode the phase stimulus above chance level. The discriminability between figure- and ground responses was higher for hit trials than error trials, suggesting that sSC activity may contribute to the decision of the mouse.</p><p>This study was aimed at the superficial part of SC (i.e. the stratum zonale, stratum griseum superficiale, and the stratum opticum), although we cannot rule out the possibility that in some animals deeper parts of SC were recorded or were affected by optogenetics. Interestingly, to our knowledge, this is one of only few studies that specifically inhibited the superficial part of SC bilaterally. Previous studies have inhibited or ablated superior colliculus, but often this was done in a unilateral fashion and/or targeting the deeper layers or the entirety of the colliculi (<xref ref-type="bibr" rid="bib80">Tunkl and Berkley, 1977</xref>; <xref ref-type="bibr" rid="bib58">Mohler and Wurtz, 1977</xref>; <xref ref-type="bibr" rid="bib79">Tan et al., 2011</xref>; <xref ref-type="bibr" rid="bib89">Wolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Ahmadlou et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Hu et al., 2019</xref>; <xref ref-type="bibr" rid="bib84">Wang et al., 2020</xref>; but see also <xref ref-type="bibr" rid="bib15">Casagrande and Diamond, 1974</xref>). Therefore, our study provides new insight into the behavioral relevance of the visual processing that takes place specifically in sSC.</p><p>Optogenetic inhibition was done by activating GABAergic neurons in the sSC. This is a commonly used strategy to inhibit areas in the neocortex (<xref ref-type="bibr" rid="bib53">Lien and Scanziani, 2018</xref>; <xref ref-type="bibr" rid="bib81">Vangeneugden et al., 2019</xref>), where GABAergic neurons only project locally. There, it allows relatively long and repeated silencing without unwanted side-effects. Most of the GABAergic neurons in the sSC are also locally projecting, but there is a fraction of GABAergic neurons projecting out of the superior colliculus to the parabigeminal nucleus and the lateral geniculate nucleus (LGN); the vLGN in particular (<xref ref-type="bibr" rid="bib26">Gale and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib88">Whyland et al., 2020</xref>; <xref ref-type="bibr" rid="bib51">Li et al., 2022</xref>). Although during the visual stimulus, the activity of the putative GAD2-neurons was reduced by our optogenetic stimulation, we cannot exclude the possibility that briefly activating the extracollicular GABAergic projections also has a direct effect on behavior. Recently, unilateral activation of GABAergic neurons below the sSC led to paradoxical effects on behavior that were best explained by an inhibiting effect of the GABAergic neurons projecting to the contralateral superior colliculus, rather than inhibiting the ipsilateral superior colliculus (<xref ref-type="bibr" rid="bib22">Essig et al., 2021</xref>). Our transfections may have included some of these GABAergic contralaterally projecting neurons, but because we optogenetically stimulated simultaneously in both hemispheres the net result of these connections is to silence the superior colliculus bilaterally.</p><p>Our results suggest that superior colliculus is necessary not just for simple but also for relatively complex object detection, when the figure does not stand out from the background by contrast. Previous experiments have already shown that the SC is causally involved in detecting orientation change (<xref ref-type="bibr" rid="bib84">Wang et al., 2020</xref>), looming stimuli (<xref ref-type="bibr" rid="bib23">Evans et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Shang et al., 2018</xref>), and detecting moving objects during hunting (<xref ref-type="bibr" rid="bib33">Hoy et al., 2019</xref>). Here, we show that SC is also involved in the detection of more complex, static objects. This behavior is often called figure-ground segregation, but we have to point out an important difference between previous figure-ground segregation research (e.g. <xref ref-type="bibr" rid="bib47">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib68">Poort et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Jones et al., 2015</xref>) and our study. Unlike commonly used stimuli for macaques, our stimuli showed a clear figure edge, due to the adaptation of the stimulus to mouse acuity. In addition to that, the tasks did not involve any eye fixation. Therefore, it would have been a viable task strategy for the mouse to simply inspect the figure edge – a strategy that, in macaques, is normally prevented by using stimuli with high and varied spatial frequency. In line with this, it has been shown that a linear decoder fed with simple cell-like inputs is able to perform the orientation task (<xref ref-type="bibr" rid="bib54">Luongo et al., 2023</xref>). The same network failed to learn the phase task, but even the image of a phase-defined figure contains features that are not present in the background image, and could be solved by learning only local features. Even the texture-defined figures used in <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref> and in earlier monkey studies (<xref ref-type="bibr" rid="bib47">Lamme, 1995</xref>), which do not contain any sharp stimulus edges, can be detected without integrating the local edges into objects. So although we can be sure that the mice perform object <italic>detection</italic>, we cannot be entirely sure that they perform object <italic>segregation</italic> in the purest sense of the word. Because our task used a limited number of grating orientations and positions, a potential task strategy would be for the mice to learn the correct responses to the complete image of the figure and background. Earlier experiments with the same stimuli in freely walking mice, however, suggested that after training on a restricted training set, mice generalize over size, location, and orientation of the figure gratings, and therefore, perform the task as if they detect the presence of an object (<xref ref-type="bibr" rid="bib74">Schnabel et al., 2018</xref>). Mice and rats have difficulty generalizing from luminance-defined objects to texture-defined objects (<xref ref-type="bibr" rid="bib20">De Keyser et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Khastkhodaei et al., 2016</xref>), but once they are acquainted with one set of texture-defined figures, they immediately generalize to other texture-orientations (<xref ref-type="bibr" rid="bib20">De Keyser et al., 2015</xref>; <xref ref-type="bibr" rid="bib54">Luongo et al., 2023</xref>). This suggests that at least some generalization for feature detection to object detection occurs in this task. In any case, our results add to the growing number of experiments (<xref ref-type="bibr" rid="bib11">Basso et al., 2021</xref>; <xref ref-type="bibr" rid="bib14">Bogadhi and Hafed, 2022</xref>) that nuance and expand upon the view of the superior colliculus as a saliency map that depends on the visual cortex for complex visual processing (<xref ref-type="bibr" rid="bib48">Lamme et al., 1998</xref>; <xref ref-type="bibr" rid="bib25">Fecteau and Munoz, 2006</xref>; <xref ref-type="bibr" rid="bib28">Gilbert and Li, 2013</xref>; <xref ref-type="bibr" rid="bib92">Zhaoping, 2016</xref>; <xref ref-type="bibr" rid="bib87">White et al., 2017</xref>).</p><p>Interestingly, the accuracy of the mice did not decrease to chance level when inhibiting the sSC. This might be partially due to the incomplete silencing of sSC (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), but also suggests that other brain areas contribute in parallel to the performance of this visual task. The obvious candidate area for this is the visual cortex. Many studies have shown the involvement of mouse V1 in object detection (<xref ref-type="bibr" rid="bib30">Glickfeld et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Katzner et al., 2019</xref>). Evidence that SC works in parallel to the visual cortex comes from the finding that mice can perform the contrast detection task above chance level when V1 is silenced (<xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). The detection of orientation-defined and phase-defined figures, however, was abolished when V1 was silenced (<xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). This suggests that the parallel processing stream through the superior colliculus does not suffice for detection of these more complex figures.</p><p>To increase our understanding of the role division between sSC and V1 in object detection, we can compare our sSC results to the V1 results from <xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>. The experiments where we inhibited sSC during object detection yielded half-maximum performance times of 99 ms, 156 ms, and 134 ms for detection based on contrast, orientation, and phase, respectively. In V1, the corresponding half-maximum performance times were 62 ms, 101 ms, and 141 ms. The parsimonious interpretation of these findings is that V1 performs a role in object detection at an earlier point in time than sSC (for object detection based on contrast and orientation, but not phase). From there, we could hypothesize that sSC inherits some of its task-related code from V1 and that sSC operates downstream of V1 in the object detection process. Indeed, superior colliculus is involved in sensorimotor transformations (e.g. <xref ref-type="bibr" rid="bib27">Gandhi and Katnani, 2011</xref>; <xref ref-type="bibr" rid="bib21">Duan et al., 2021</xref>). However, the onset times of the neural responses paint a slightly different picture: in V1, the onset times of the FGM for contrast, orientation, and phase were estimated at 43 ms, 75 ms, and 91 ms, respectively. The onset times we recorded in sSC were 67 and 75ms for contrast and orientation stimuli, and a non-significant result for the phase stimuli. Although our estimates of the onset time of FGM are not precise enough to fully rule out the possibility that modulation of V1 is transferred to sSC through the direct projection, the onset time of the modulation in sSC suggests that it is computed independently of V1. This independency of V1 has also been shown for orientation-dependent surround suppression in sSC (<xref ref-type="bibr" rid="bib29">Girman and Lund, 2007</xref>), which even increases if V1 is inhibited (<xref ref-type="bibr" rid="bib2">Ahmadlou et al., 2017</xref>). This suggests a role for the sSC upstream of V1 or in parallel to V1, perhaps in strengthening the representation of pop-out stimuli in V1 through pathways that include the lateral posterior nucleus of the thalamus (<xref ref-type="bibr" rid="bib34">Hu et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Fang et al., 2020</xref>) or LGN (<xref ref-type="bibr" rid="bib38">Jones et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Ahmadlou et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Poltoratski et al., 2019</xref>). In addition to the two parallel visual pathways from the retina via V1 and sSC to decision areas, there is a pathway from the dLGN to (primarily medial) higher visual areas (<xref ref-type="bibr" rid="bib12">Bienkowski et al., 2019</xref>). <xref ref-type="bibr" rid="bib31">Goldbach et al., 2021</xref> showed that these medial visual areas could be silenced without a drop in performance in a simple visual detection task. Therefore, it does not seem likely that these geniculate projections would be of major importance in the figure detection task.</p><p>Our study provides evidence for a neural code in mouse sSC that is necessary for normal visual detection of complex static objects. These results fit in with the growing number of studies that show mouse sSC provides a significant contribution to the processing of complex visual stimuli (<xref ref-type="bibr" rid="bib2">Ahmadlou et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Hu et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Fang et al., 2020</xref>). The superior colliculus (or the optic tectum in non-mammalian species) is a brain area conserved across vertebrate evolution (<xref ref-type="bibr" rid="bib36">Isa et al., 2021</xref>). Even though clear differences exist between mouse and primate sSC, for example in their received retinal inputs (<xref ref-type="bibr" rid="bib37">Ito and Feldheim, 2018</xref>), representation of visual features (<xref ref-type="bibr" rid="bib82">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib1">Ahmadlou and Heimel, 2015</xref>; <xref ref-type="bibr" rid="bib17">Chen and Hafed, 2018</xref>), and more generally the animals’ strategies for visual segmentation (<xref ref-type="bibr" rid="bib54">Luongo et al., 2023</xref>), many functions of sSC are shared between rodents and primates. Some of these include saliency mapping (<xref ref-type="bibr" rid="bib87">White et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Barchini et al., 2018</xref>), spatial attention (<xref ref-type="bibr" rid="bib46">Krauzlis et al., 2013</xref>; <xref ref-type="bibr" rid="bib83">Wang and Krauzlis, 2018</xref>; <xref ref-type="bibr" rid="bib35">Hu and Dan, 2022</xref>; <xref ref-type="bibr" rid="bib85">Wang et al., 2022</xref>), and orienting behavior (<xref ref-type="bibr" rid="bib13">Boehnke and Munoz, 2008</xref>; <xref ref-type="bibr" rid="bib57">Masullo et al., 2019</xref>; <xref ref-type="bibr" rid="bib90">Zahler et al., 2021</xref>). This, together with recent work showing object coding in the primate (<xref ref-type="bibr" rid="bib32">Griggs et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Bogadhi and Hafed, 2022</xref>), suggests that also primate sSC contributes to visual processing in more various and complex ways than anticipated based on previous work.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All offline analysis was performed using MATLAB (R2019a, R2022b; MathWorks).</p><sec id="s4-1"><title>Experimental animals</title><p>For the experiments, we used a total of 16 mice. For awake-behaving electrophysiology, we used five C57BL/6 J mice (Charles River, all male). For behavior combined with optogenetics, we used eight GAD2-Cre mice (Stock #028867, Jackson; six males, two females). For awake electrophysiology without behavior, we used four C57BL/6 J mice (Janvier) and three GAD2-Cre mice. Mice were 2–5 months old at the start of experiments. The mice were housed in a reversed light/dark cycle (12 hr/12 hr) with ad libitum access to laboratory food pellets. All experiments took place during the dark cycle of the animals. Mice were either housed solitarily or in pairs. All experimental protocols were approved by the institutional animal care and use committee of the Royal Netherlands Academy of Sciences (KNAW) and were in accordance with the Dutch Law on Animal Experimentation under project licenses AVD80100 2016 631, AVD80100 2016 728, and AVD80100 2022 15877.</p></sec><sec id="s4-2"><title>Surgeries</title><sec id="s4-2-1"><title>General surgical preparation and anesthesia</title><p>Anesthesia was induced using 3–5% isoflurane in an induction box and was maintained using 1.2–2% isoflurane in an oxygen-enriched air mixture (50% air and 50% O<sub>2</sub>, 0.5  L per min flow rate). After induction, the mice were positioned in a Kopf stereotactic frame. The temperature of the animal was monitored and kept between 36.5° and 37.5° using a heating pad coupled to a rectal thermometer. We subcutaneously injected 2.5 mg/kg meloxicam as a general analgesic, and the eyes were covered with Bepanthen ointment to prevent dehydration and to prevent microscope light from entering the eye. The depth of anesthesia was monitored by frequently checking paw reflexes and breathing rate. We added a thin layer of xylocaine cream to the ear bars for analgesia and stabilized the mouse’s head in the frame using the ear bars. Then the area of the incision was trimmed or shaved, cleaned with betadine, and lidocaine spray was applied to the skin as a local analgesic. We made an incision in the skin, and then again applied lidocaine, this time on the periosteum. Further methods for specific surgeries are described below. At the end of each surgery, we injected 2.5 mg/kg meloxicam for post-surgical analgesia and kept the mice warm until they had woken up. We monitored their appearance and weight daily for at least 2 days post-surgery.</p></sec><sec id="s4-2-2"><title>Head bar implantation</title><p>After induction of anesthesia as described above, we cleaned the skull, thereby removing the periosteum, and slightly etched the skull using a micro curette. We then applied a light-cured dental primer (Kerr Optibond) to improve the bonding of cement to the skull. After applying the primer, we created a base layer of cement on top of the primer using Heraeus Charisma light-cured dental cement. The head bar was placed on top of this base layer and fixed in place using Vivadent Tetric evoflow light-cured dental cement. Lastly, we sutured the skin around the implant.</p></sec><sec id="s4-2-3"><title>Viral injections</title><p>We diluted ssAAV-9/2-hEF1α-dlox-hChR2(H134R)_mCherry(rev)-dlox-WPRE-hGHp(A) (titer 5.4 × 10<sup>12</sup> vg/ml, VVF ETH Zurich) 1:1 in sterile saline and loaded it into a Nanoject II or Nanoject III injector (Drummond Scientific). After induction of anesthesia as described above, we drilled two small craniotomies (0.5 mm in diameter) bilaterally above superior colliculus (0.3 mm anterior and 0.5 mm lateral to the lambda cranial landmark). Next, we inserted the pipette and slowly injected the viral vector solution at two different depths (55 nl each at 1.4 and 1.2 mm depth). After each depth, we waited 2 min before moving the pipette up. We left the pipette in place for at least 10 min before fully retracting it to avoid efflux. We repeated this for the second hemisphere. After the injections, we cleaned the scalp with sterile saline and sutured the skin.</p></sec><sec id="s4-2-4"><title>Fiber implant surgery</title><p>Optic fiber implants were custom-made using grooved ferrules (Kientec Systems Inc, ID 230 um, L=6.45 mm, OD = 1.249 mm), multimode optic fiber (Thorlabs FP200URT, NA = 0.5), and 2-component epoxy glue. Fiber implant surgery was performed at least one week after viral injection. After induction of anesthesia as described above, we cleaned and etched the skull using sterile saline and a micro curette. We then applied a light-cured dental primer (Kerr Optibond) to improve the bonding of cement to the skull. Then, we drilled two small craniotomies (0.5 mm in diameter) above bilateral superior colliculus (0.5 mm anterior and 0.8 mm lateral from the lambda cranial landmark). We put an optic fiber in a custom holder at a 14° angle in the mediolateral plane – the angle prevented the fibers from blocking each other’s connection sleeve – and inserted it 0.9 mm deep. We added Vivadent Tetric evoflow light-cured dental cement to stabilize the implant and then removed the fiber from the holder. This was repeated for the second hemisphere. Once the optic fibers were thoroughly stabilized with cement, we placed a head bar anterior to the optic fibers, as described above. After this, we sutured the skin around the implant.</p></sec><sec id="s4-2-5"><title>Surgery for awake electrophysiology (control experiments)</title><p>Viral injections and the head bar attachment were performed during the same surgery session. Anesthesia was induced as described above. Rimadyl (carprofen) was injected subcutaneously (5 mg/kg) at the start of the surgery. We applied a lidocaine spray to the scalp as a local analgesic. After making the incision, the lidocaine spray was also applied to the periosteum. We then cleaned the skull, thereby removing the periosteum, and slightly etched the skull using a micro curette. We then applied a light-cured dental primer (Kerr Optibond) to improve the bonding of cement to the skull. After applying the primer, we created a base layer of cement on top of the primer using Vivadent Tetric evoflow light-cured dental cement. The head bar was placed on top of this base layer and fixed in place using more cement. Viral injection methods were as described above. Post-surgery, we administered carprofen through drinking water in the home cage (0.06 mg/ml) for ~72 hr, starting the day after the surgery. Mice were habituated to being head-fixed in a setup for up to two weeks prior to recording. When the mice were habituated, we performed a craniotomy surgery. Here, in addition to carprofen, buprenorphine was administered subcutaneously (0.05 mg/kg) at the start of the surgery. Further methods for craniotomy surgery were described above.</p></sec><sec id="s4-2-6"><title>Surgery for electrophysiology during task performance</title><p>After the mice had learned the task, we performed a surgery in which we made a craniotomy and placed a reference screw to enable awake-behaving electrophysiology. After induction of anesthesia and before opening the skin, we injected 3 mg/kg dexamethasone s.c. to prevent cortical edema. We made an incision in the skin over the midline, posterior to the head bar implant. With a small razor, we cut the tendons of the neck muscle on the occipital bone to create space on the bone. After this, we cleaned and dried the skull and applied light-cured dental primer (Kerr Optibond) for adhesion. We marked the location of the center of the craniotomy for the electrode insertion (0.5 mm anterior and 0.5 mm lateral from the lambda cranial landmark). We then first drilled a 0.6 mm craniotomy for the reference screw in the occipital or parietal bone, contralateral of the craniotomy, and inserted the screw. For some mice, we repeated this for a second screw to separate the electrical reference and ground. We then used Vivadent Tetric evoflow light-cured dental cement to stabilize the screws on the skull and to create a well around the marked location for the craniotomy. This well could hold a bit of sterile saline during the recordings to prevent desiccation of the brain tissue. We then continued to drill a 1.5–2 mm craniotomy above SC. We thoroughly cleaned the craniotomy with sterile saline and used sterile silicone (Kwik-Cast, World Precision Instruments) to seal the well. Finally, we sutured the skin around the implant.</p></sec></sec><sec id="s4-3"><title>Behavioral task</title><sec id="s4-3-1"><title>Habituation and water restriction</title><p>The mice were handled for 5–10 min per day for at least 5 days before training them in the setup. For the behavior sessions, the mice were head-restricted in a tube. We habituated the mice to the head restriction by putting them in the setup each day for at least 5 days, ramping up the time in the setup from several minutes to ca. 30 min. Once they were fully habituated, they were put on a fluid restriction protocol with a minimal intake of 0.025 ml/g per day (in line with national guidelines, i.e. <ext-link ext-link-type="uri" xlink:href="https://www.ncadierproevenbeleid.nl/adviezen-ncad">https://www.ncadierproevenbeleid.nl/adviezen-ncad</ext-link>), while their health was carefully monitored. The minimal intake was guaranteed by monitoring the water intake during the task performance. If the intake after behavioral experiments was below the daily minimum, mice were given HydroGel (Clear H<sub>2</sub>O) to reach the minimum.</p></sec><sec id="s4-3-2"><title>Visual stimulation</title><p>For all the experiments including behavior, we created the visual stimuli with the Cogent toolbox (developed by J. Romaya at the LON (Laboratory of Neurobiology) at the Wellcome Department of Imaging Neuroscience) and linearized the luminance profile of the monitor/projector. For the optogenetic experiments, we used a 23-inch LCD monitor (1920 × 1080 pixels, Iiyama ProLite SB2380HS), placed 12 cm in front of the eyes. For the behavioral electrophysiological experiments, the stimuli were presented on a 21-inch LCD monitor (1280 × 720 pixels, Dell 059DJP) placed 15 cm in front of the mouse. Both screens had a refresh rate of 60 Hz. We applied a previously described correction (<xref ref-type="bibr" rid="bib56">Marshel et al., 2011</xref>) for the larger distance between the screen and the mouse at higher eccentricities. This method defines stimuli on a sphere and calculates the projection onto a flat surface. The figure and background were composed of 100% contrast sinusoidal gratings with a spatial frequency of 0.08–0.1 cycles/deg and a mean luminance of 20 cd/m<sup>2</sup>. The diameter of the figure was 35° (optogenetics) or 40° (electrophysiology). For the contrast-defined stimuli, we presented the figure gratings on a gray background (20 cd/m<sup>2</sup>). For the orientation-defined figures, the grating orientation in the background was either horizontal or vertical (0° or 90°), and the orientation of the figure was orthogonal. For the phase-defined figures, the phase of the figure grating was shifted by 180° relative to that of the background.</p></sec><sec id="s4-3-3"><title>Behavioral task</title><p>The animals were trained to indicate the side on which a figure appeared by licking the corresponding side of a custom-made y-shaped lick spout (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We registered licks by measuring a change in either capacitance (for optogenetics experiments) or current (for electrophysiology experiments) with an Arduino and custom-written software. Water rewards were provided through two tubes that were connected to the lick spout, one on each side. The water flow was controlled using two valves which were, like the lick detection, Arduino-controlled.</p><p>The exact figure location varied slightly depending on the RF positions, but the figure center was generally close to an azimuth of 30° (left or right of the mouse) and an elevation of 15°. The trials were distributed in sequences of 4, with each sequence containing one trial of each orientation (0/90°) and figure location (left/right). The four trials were always randomized within the sequences. A trial started when the stimulus with a figure on the left or right appeared on the screen. The stimulus was displayed for 1.5 s and the mice could respond from 0.2 up until 2 s after stimulus onset. The first lick of the mouse counted as their response. The reaction time was also based on the timing of the first lick. Because some mice made early random licks, especially during training, we disregarded licks from 0 to 200ms. A response was considered correct if the first lick between 0.2 and 2 s after stimulus onset was on the side of the figure. A response was considered an error if this first lick was on the other side. A response was considered a miss if the mouse did not lick between 0.2 and 2 s after stimulus onset. Correct responses were rewarded with a drop of water by briefly opening the valve that controlled the water flow to the correct side of the spout. Stimulus presentation was followed by an intertrial interval (ITI). In this period, first, a gray background was shown for 3.5 s. If the animal made an error, a 5 s timeout was added to this period. Next, we presented a background texture (the full-screen grating without a figure of the trial background orientation) for 1.5 s, followed by a gray background for a variable duration of 3.0–5.0 s, before presentation of the task stimulus. We did not give a reward if the mice licked during this period, so that they learned and would be reminded to ignore the background. In some sessions, we included correction trials, which were repeats of the same trial type after an error. We only included non-correction trials for our analysis of the accuracy of the mice. We define task accuracy as hits/(hits + errors).</p><p>During the electrophysiology experiments, the lick spout was placed slightly below the mouse, out of its reach. We then used an Arduino-controlled servo motor that moved the lick spout towards the mouth of the mouse 500 ms after the presentation of the stimulus, thereby ensuring that the first 500 ms of the visual response could be recorded without electrical artifacts from the lick detector.</p></sec><sec id="s4-3-4"><title>Behavioral training</title><p>The training for the task involved various steps of increasing difficulty. First, the mice were trained on the contrast task, detecting the position of a circular grating figure on a gray (20 cd/m<sup>2</sup>) background. After learning this, the mice were introduced to different backgrounds. This was done by starting out with a figure grating on a black (0 cd/m<sup>2</sup>) or white background (40 cd/m<sup>2</sup>). Essentially this meant that the contrast of the grating in the background was 0%. When the mouse performed above 70% accuracy, the background grating would gradually change from 0% contrast to 100% contrast. Likewise, when the mouse performed below 60% accuracy, the background grating would decrease in contrast. When they reached 100% contrast, the mice had learned the orientation task. We then introduced phase-defined stimuli in 50% of the trials so they could generalize to these stimuli.</p></sec></sec><sec id="s4-4"><title>Optogenetic inhibition during behavior</title><sec id="s4-4-1"><title>Recording</title><p>For bilateral optogenetic inhibition of SC through activation of GABAergic neurons, we used either a 473 nm BL473T8-100FC laser or a 462 nm BLM462TA-100F laser (Shanghai Laser &amp; Optics Century Co.). The laser was connected to a two-way split patch cord (Doric Lenses), with a power of 3–5 mW at each fiber tip. We placed a blue distractor LED light, driven by an Arduino, at a height of 38 cm above the mouse. The light flickered on for 0–1.2 s and off for 0–2.8 s to habituate the mouse to any flickering stray blue light. The contrast task was recorded in sessions that were independent from the sessions with the orientation- and phase-defined tasks. In the orientation/phase sessions, orientation-defined and phase-defined stimuli were pseudorandomly shuffled in a 50/50% ratio. The mouse was placed in the setup, and the patch cord was connected to the bilateral implant. The implant was shielded using Creall super soft modeling clay to prevent light scatter out of the implant. When the animals performed the task consistently with an accuracy larger than 65%, we initiated the inhibition of SC using laser light in a random 25% of the trials. The onset of stimulation was shifted relative to the onset of the visual stimulus in steps of 16.7 ms, conforming to the frame rate of the screen. The optogenetic stimulation lasted for 2 s.</p></sec><sec id="s4-4-2"><title>Analysis</title><p>For the analysis, we included only trials from the periods in recording sessions where optogenetic manipulation was used, i.e., periods in which the mice had an accuracy higher than 65%. Data from one mouse for a particular task was included when the mouse had performed at least 100 trials with optogenetic manipulation, aggregated across latencies (i.e. minimally ~17 trials per latency, but for most mice we recorded closer to 40 trials per latency). In our analysis of the influence of optogenetic silencing, we computed the accuracy for each laser onset latency for each mouse. We fit a logistic function to the mean accuracies using the Palamedes toolbox in MATLAB (<xref ref-type="bibr" rid="bib69">Prins and Kingdom, 2018</xref>). In order to get a good estimate of the time at which the accuracy reached its half maximum (i.e. the inflection point of the fitted curve), we used bootstrapping (1000 times) by sampling trials from each mouse with replacement. For each bootstrap, we fit a logistic function to the results, resulting in a distribution of estimated inflection points. To test the significance of the effect of the optogenetic manipulation on the accuracy, maximum lick rate, and reaction time of the mice, we used one-way repeated measures analysis of variance (ANOVA) with Bonferroni correction.</p></sec></sec><sec id="s4-5"><title>Awake electrophysiology for assessing strength of response reduction by optogenetics</title><sec id="s4-5-1"><title>Recording</title><p>Starting the day after the craniotomy surgery, we performed recordings using Neuropixels silicon probes over the course of 1–4 days. These recordings were performed using a National Instruments I/O PXIe-6341 module and SpikeGLX. Prior to electrode insertion, we inserted a 200 um diameter optic fiber at a~15° to a depth of ca. 750 um. The probe was then inserted ca. 400 um posteromedial into the fiber at a near-zero-degree angle. Probes were coated with a fluorescent dye (dii, did, or dio) for post-hoc reconstruction of the recording location. Brain areas were assigned using the UniversalProbeFinder pipeline (<ext-link ext-link-type="uri" xlink:href="https://github.com/JorrritMontijn/UniversalProbeFinder">https://github.com/JorrritMontijn/UniversalProbeFinder</ext-link>; <xref ref-type="bibr" rid="bib60">Montijn, 2022</xref>; <xref ref-type="bibr" rid="bib61">Montijn and Heimel, 2022</xref>). We used the Acquipix toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/JorritMontijn/Acquipix">https://github.com/JorritMontijn/Acquipix</ext-link>, copy archived at <xref ref-type="bibr" rid="bib62">Montijn, 2024</xref>) for visual stimulation and synchronized the stimulation with high accuracy using photodiode signals that recorded visual stimulus onsets. Stimuli were displayed at 60 Hz on a 51 × 29 cm screen (Dell) at a 23 cm distance from the animal’s left eye. We only included clusters from the superficial SC (i.e. the stratum zonale, stratum griseum superficiale, and the stratum opticum) for further analysis.</p><p>To assess the approximate receptive field locations of units along the probe, we presented repetitions of square 9° drifting grating patches (0.11 cycles/deg, drifting at 3 Hz) in random positions on a gray background. To test the effect of the optogenetic manipulation, we presented static sinusoidal gratings (100% contrast, 1 s trial duration) in two orientations (horizontal and vertical) with a spatial frequency of 0.1 cycles/deg. ITI duration was 1.5 s. Trials with and without optogenetic stimulation, starting about 30 ms before the onset of the visual stimulus, were randomized. For optogenetic stimulation during stimulus presentation, we used a 462 nm BLM462TA-100F laser (Shanghai Laser &amp; Optics Century Co.), with a power of 4 mW at the fiber tip.</p></sec><sec id="s4-5-2"><title>Analysis</title><p>Spikes were isolated using Kilosort3 (<xref ref-type="bibr" rid="bib65">Pachitariu et al., 2023</xref>). Both single- and multi-unit clusters were included for analysis, if they were stable throughout the stimulation period (non-stationarity &lt;0.25). Units were considered putative GAD2-positive if they were responsive during the first 10 ms of a 50 ms laser pulse (p&lt;0.05, zeta-test) and showed an increased firing rate during this period. The visual response was the mean rate during 0–0.2 s after stimulus onset. Responses to both grating orientations were combined. The minimum evoked response (visual response minus the spontaneous rate during –1 s to 0.1 s before stimulus onset) for a unit to be included as ‘visual’ was 2 spikes/s. We used linear mixed effects (LME) models (MATLAB <italic>fitlme</italic>) to assess the significance of the difference between the laser on and off conditions.</p></sec></sec><sec id="s4-6"><title>Awake behaving electrophysiology</title><sec id="s4-6-1"><title>Recording</title><p>After the craniotomy surgery and at least 2 days of recovery, the mice were recorded daily for up to two weeks. First, the mouse was placed in the setup. The left eye of the mouse was tracked using an ISCAN camera and software. We used matte black aluminum foil (Thorlabs) to shield its eyes from light during electrode insertion, and also to shield the craniotomy from electrical noise. During the last recording session of each mouse, we coated the electrode tip with diI for histological verification. While looking through a microscope (Zeiss Stemi 508), we removed the Kwik-Cast from the well and cleaned the recording chamber with sterile saline. We connected the ground/reference screws to the recording system, and slowly inserted a Neuronexus probe (A1x32-5 mm-25-177; 32-channel probe with 25 um spacing) into the brain, until the electrode would span the depths of ca. 800–1600 µm from the dura – thereby covering superficial SC. We waited about 15 min for the electrode to stabilize inside the brain before we started recording. The electrical signal from the electrodes was amplified and sampled at 24.4 kHz using a Tucker-Davis Technologies recording system.</p><p>First, we probed visual responses using a checkerboard stimulus consisting of black and white checkers of 20 visual degrees, that was displayed for 250 ms, then reversed for 250 ms, and was followed by a gray screen during the 1 s ITI. We then measured the RF of the recording sites using a sparse noise stimulus consisting of either 4 or 12 squares (50% black, 50% white) of five visual degrees at random locations on a gray background, that were displayed for 0.5 s followed by a 0.5 s ITI. This stimulus was shown for a total of 5–10 min. Using the receptive field data, we could ensure that the figure stimuli during the task were placed either inside or outside of the receptive field of the recorded sites. For the ‘figure’ stimulus, the figure was placed over the RF; for the ‘ground’ stimulus, the figure was placed 50–60 visual degrees lateral of the receptive field, in the hemifield contralateral to the RF (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). We proceeded to let the mouse perform the task while recording neuronal responses. After recording, we first disconnected the grounding and reference pins and shielding material close to the probe. We then removed the electrode from the brain and once again cleaned the craniotomy with sterile saline, and then sealed the craniotomy with Kwik-cast.</p></sec><sec id="s4-6-2"><title>Analysis: In- and exclusion of trials</title><p>For our analysis of the electrophysiology data, we only included behavior sessions with good performance. Therefore, we tested whether the accuracy of the mouse on each variation of the task (i.e. contrast, orientation, phase) was significantly above chance level using a binomial test. If the session was shorter than 40 trials (the threshold for reaching statistical significance with 65% performance), we included the session if task accuracy was at least 65% and task accuracy on each side (i.e. figure stimulus on the left or right) was at least 50%.</p><p>To ensure the image was stable on the retina during the task, we excluded trials with eye movements. Given that the mice generally did not make many eye movements during the task, we excluded trials where the eye speed in the period between 0 and 450 ms after stimulus onset was higher than the mean speed + 2.5*SD.</p><p>For the identification of artifacts, we used an estimate of the envelop multi-unit activity (eMUA). The raw data was band-pass filtered between 500–5000 Hz, half-wave rectified (negative becomes positive), and then low-pass filtered at 200 Hz. The resulting signal constitutes the envelope of high-frequency activity. Each channel’s envelope signal was first z-scored across all trials <italic>j</italic> and time-points <italic>I</italic> and the absolute value was taken to produce <italic>zmua<sub>ij</sub></italic>. To identify time-points at which the majority of recording-channels showed large excursions from the mean we took the geometric mean of <italic>zmua<sub>ij</sub></italic> across all recording channels to produce Z<sub>ij</sub>:<disp-formula id="equ1"><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></disp-formula></p><p>where <italic>c</italic> is the identity of the recording channel and <italic>n</italic> is the total number of recording channels. Z<sub>ij</sub> will be a positive number reflecting the consistency and extremeness of excursions from the mean across recording channels. As the geometric mean was used, Z<sub>ij</sub> can only reach extreme values if the majority of recording channels show large excursions from the mean. We identified samples at which Z<sub>ij</sub> was greater than three and removed these samples from all channels as well as removing the preceding and following three samples. We then recalculated Z<sub>ij</sub> after the removal of the extreme samples. To identify trials with extreme mean values (likely due to muscle artefacts) we took the mean value of Z<sub>ij</sub> for each trial <italic>j</italic> and squared it to produce <italic>χ<sub>i</sub></italic>:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>χ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>k</italic> was the total number of trials. The distribution of <italic>χ<sub>j</sub></italic> across trials was approximately normal and we fit the resulting distribution with a Gaussian function using non-linear least-squares fitting (using <italic>fminsearch.m</italic> in MATLAB) with mean μ and standard deviation σ. Extreme trials were identified as trials with <italic>χ<sub>j</sub></italic> values more than 3σ from μ and were removed.</p></sec><sec id="s4-6-3"><title>Analysis: In- and exclusion of neurons</title><p>After applying the inclusion and exclusion criteria for the trials described above, we analyzed the single-unit responses during the included trials. First, we subtracted the common average across channels from the raw ephys data to reduce noise. The data was then further preprocessed and spike sorted using Kilosort2 (<ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort</ext-link>, copy archived at <xref ref-type="bibr" rid="bib63">MouseLand, 2024</xref>; <xref ref-type="bibr" rid="bib64">Pachitariu et al., 2016</xref>), with a spike detection threshold of –2 SD. The spike sorting results were manually curated using Phy (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link>, copy archived at <xref ref-type="bibr" rid="bib18">cortex-lab, 2024</xref>). The manual curation was done in two phases, the first one being clean-up of the automatically generated clusters. Some clustered contained large-amplitude noise artifacts, due to muscle contractions of the mouse or electrical currents from lick detection. In the second phase, we labeled the clusters as being single- or multi-unit, based on Kilosort quality scores and spread of spike detection across the laminar probe. Some multi-unit clusters seemed to include small single-unit clusters, these we separated from their multi-unit cluster. Only single-unit clusters that were stable across the recording session were included in the analysis for this paper. We convolved the detected spikes of each unit with a Gaussian with an SD of 10 ms to derive a continuous estimate of the spike rate. This preprocessing left us with a total of 241 neurons, 95 of which were excluded because they were not stably present throughout their respective recording sessions.</p><p>To estimate the receptive field of each neuron, we averaged the spikes that were evoked by each RF map checker in a time window between 40–300 ms after checker onset. Given the variety of PSTH shapes, each neuron was assigned its own time window where the neuron showed increased or decreased spiking. We then fit a two-dimensional (2D)–Gaussian to estimate the width and center of both the ON and OFF RF. The quality of the fit was assessed using r<sup>2</sup> and a bootstrapped variability index (BVI), which estimated the reliability of the RF center estimate (<xref ref-type="bibr" rid="bib45">Kirchberger et al., 2021</xref>). We resampled an equal number of trials as in the experimental dataset (with replacement) and regenerated the Gaussian fit. The BVI is defined as the ratio of the SD of the RF center position and the SD of the fitted Gaussian. We used the most reliable fit of the RF (ON or OFF) as our RF estimate. Out of the 146 stable neurons we recorded, 75 neurons had a reliable RF either on the center or edge of the figure.</p><p>To investigate visually responsive neurons (<xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig4">4</xref>), we included cells with an evoked response of at least three spikes/s in the period from 50 to 200 ms after stimulus onset. Out of the neurons with reliable RFs, 64 fulfilled this criterion. These 64 neurons are the neurons that are used for the analysis in <xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig4">4</xref>.</p><p>For investigating putative multisensory neurons (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), we included cells that had their peak firing rate between 650–900 ms after stimulus onset and were time-locked to the stimulus (p&lt;0.05 Zeta-test for all cells; <xref ref-type="bibr" rid="bib59">Montijn et al., 2021</xref>).</p></sec><sec id="s4-6-4"><title>Analysis: Statistical tests</title><p>Responses of each individual neuron were normalized to the mean response of that neuron across all trials where a grating was displayed inside the RF<italic>: R<sub>normalized</sub> = (R – R<sub>baseline</sub>)/(R<sub>max</sub> – R<sub>baseline</sub>),</italic> where R is the rate during the sliding window, R<sub>baseline</sub> is the average rate in the 0.15 s before the stimulus onset, and R<sub>max</sub> is the maximum average rate between 0.05–0.20 s after the stimulus onset.</p><p>We tested the difference between figure and ground (<xref ref-type="fig" rid="fig2">Figures 2I</xref> and <xref ref-type="fig" rid="fig3">3B</xref>) based on an approach by <xref ref-type="bibr" rid="bib55">Maris and Oostenveld, 2007</xref> using a permutation test. In brief, surrogate data-sets are made with randomly swapped condition labels. For each surrogate, we cluster together time points with significant p-values from a linear mixed effects (LME) model (FitMethod REML; StartMethod random) and then take the maximum summed F-statistic across all clusters as a statistic. This builds up a null distribution of maximum cluster F-statistics. We then compare the cluster F-statistics from the unshuffled data to identify significant clusters. We estimated the latency of the figure-ground modulation by fitting a function (<xref ref-type="bibr" rid="bib68">Poort et al., 2012</xref>) to the figure minus background response in a time window from 0 to 300ms after stimulus onset. Briefly, the function is the sum of an exponentially modulated Gaussian and a cumulative Gaussian, capturing the Gaussian onset of neural modulation across trials/neurons and the dissipation of modulation over time. The latency was defined as the (arbitrary) point in time at which the fitted function reached 33% of its maximum value.</p><p>For the plots in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1F</xref>, we averaged the z-scored data across trials for one example neuron. For the analysis of the correlation in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1G</xref>, we concatenated the data across all the trials (i.e. creating one long time series) and computed the correlation coefficient between the eye movement and the neuronal responses. To test whether the resulting coefficients were significantly different from zero, we used a one-sample t-test (p=0.488).</p></sec><sec id="s4-6-5"><title>Analysis: Decoding</title><p>To further investigate the neural code in SC, we tried to decode the trial identities from the neural responses we recorded. Generally, decoder algorithms need balanced data sets as input, to ensure non-biased training. However, our neural data was recorded during different behavioral sessions. Therefore, we did not have balanced trial numbers across conditions for each neuron. Hence, we created a surrogate data set from our data to decode the stimulus type (figure vs. ground, <xref ref-type="fig" rid="fig3">Figure 3C</xref>). For the surrogate data set of each task, we included only neurons for which we recorded at least five trials for each of the stimulus types (figure/ground). The neuronal responses were normalized as described above (section <italic>Statistical tests</italic>). For each of the neurons, we excluded one random trial (either a figure trial of all neurons or a ground trial of all neurons). These trials together comprised the test set. We then pseudorandomly drew, with replacement, 10 trials of each stimulus type from each neuron’s remaining data. These comprised the training set. We then generated a linear support vector machine (SVM) model that predicts the stimulus type based on the training data, and subsequently used that model to decode the test set. The model was built using a script derived from MATLAB’s Classification learner app, with the ‘<italic>fitcsvm'</italic> function at its core (KernelFunction linear; PolynomialOrder None; KernelScale auto; BoxConstraint 1; Standardize true). We repeated the training and testing 2000 times, with balanced test sets, for each time window; the performance was computed using a sliding window of 50 ms in steps of 10 ms. The resulting mean performances are reported. We tested whether the decoding performances were significantly different from chance using binomial tests with Bonferroni-Holm correction. We also extracted the average weight of each neuron from the SVM model and compared the relative weights using an LME model (MATLAB <italic>fitlme</italic>, FitMethod REML).</p><p>The d-prime was used to quantify the discriminability between figure and ground responses; it is a measure for the reliability of the signal on individual trials:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where μ<sub>G</sub> and μ<sub>G</sub> are the means, and σ<sub>F</sub><sup>2</sup> and σ<sub>G</sub><sup>2</sup> are the variances of the figure and ground response across trials, respectively. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, we analyze d-prime values for the time window with the best decoding performance of each task. The shuffled data was generated by shuffling the trial identities of the real data 1000 times. In <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, we analyze d-prime values (and firing rates) for the time window between the stimulus onset and the lick. To estimate the significance of the d-prime difference between hits and errors, we fit the data with a linear mixed-effects model. We defined the best model – balancing model fit and complexity - using the Akaike information criterion (AIC) <xref ref-type="bibr" rid="bib6">Akaike, 1974</xref>; <xref ref-type="bibr" rid="bib4">Aho et al., 2014</xref>. To investigate whether the neural responses were related to eye movements, we computed the average Z-scored eye position and pupil dilation during the different trial types for each mouse and then plotted the mean (± SEM) data across mice.</p></sec></sec><sec id="s4-7"><title>Histology</title><p>We deeply anesthetized the mice with Nembutal and transcardially perfused them with phosphate-buffered saline (PBS), followed by 4% paraformaldehyde (PFA) in PBS. We extracted the brain and post-fixated it overnight in 4% PFA before moving it to a PBS solution. We cut the brains into 75-um-thick coronal slices and mounted them on glass slides in Vectashield DAPI solution (Vector Laboratories). We imaged the slices on either a Zeiss Axioplan 2 microscope (10×objective, Zeiss Plan-Apochromat, 0.16 NA) using custom-written Image-Pro Plus software or a Zeiss Axioscan.Z1 using ZEN software. The resulting histology images were used to confirm the location of fiber implants, the electrode trace, and/or virus expression.</p><p>For estimating the histological depth of the neurons that we recorded during the electrophysiology experiments, we used a combination of electrophysiological and histological data. After outlier removal (see above) and common average subtraction of the raw data, we low-pass filtered the data to get the local field potential (LFP). 50 Hz artifacts were removed by digital notch filtering. We computed the current source densities (CSD) from the LFP as described in <xref ref-type="bibr" rid="bib75">Self et al., 2014</xref>. The CSD of the sSC typically showed one strong sink during the peak of the visual response. We, therefore, took the channel that had recorded the lowest value of the CSD as a reference for the relative position of the recording electrode in the sSC. To get an estimate of the absolute depth from the sSC surface, we measured the depths of the electrode tracks in the histological slices using ImageJ. From this, we estimated that the channel with the strongest CSD sink was located ca. 119±48 µm from the SC surface. This value was combined with the information from the CSD to compute our estimate of the absolute depths of the recorded neurons. We tested the difference between the two groups using a one-sample F test (for the difference between variances) and a Mann-Whitney U-test (for the difference between means).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Project administration</p></fn><fn fn-type="con" id="con4"><p>Investigation, Project administration</p></fn><fn fn-type="con" id="con5"><p>Investigation, Project administration</p></fn><fn fn-type="con" id="con6"><p>Investigation</p></fn><fn fn-type="con" id="con7"><p>Software, Visualization, Methodology</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Software, Formal analysis, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental protocols were approved by the institutional animal care and use committee of the Royal Netherlands Academy of Sciences (KNAW) and were in accordance with the Dutch Law on Animal Experimentation under project licenses AVD80100 2016 631, AVD80100 2016 728 and AVD80100 2022 15877.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83708-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Electrophysiology data is available on Open Science Framework with <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/X8D6T">https://doi.org/10.17605/OSF.IO/X8D6T</ext-link>. Analysis code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/leoniecazemier/SC-figure-detection">https://github.com/leoniecazemier/SC-figure-detection</ext-link> (copy archived at <xref ref-type="bibr" rid="bib16">Cazemier, 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Heimel</surname><given-names>JA</given-names></name><name><surname>Cazemier</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Involvement of superior colliculus in complex figure detection of mice</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/X8D6T</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Christiaan Levelt for sharing experimental facilities and Emma Ruimschotel for genotyping. We further thank Enny van Beest, Mehran Ahmadlou, Chris van der Togt, and Ulf Schnabel for sharing their expertise. J.L.C and J.A.H. were funded by FLAG-ERA grant CHAMPmouse through de Nederlandse Organisatie voor Wetenschappelijk Onderzoek (NWO).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadlou</surname><given-names>M</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Preference for concentric orientations in the mouse superior colliculus</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6773</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7773</pub-id><pub-id pub-id-type="pmid">25832803</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadlou</surname><given-names>M</given-names></name><name><surname>Tafreshiha</surname><given-names>A</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual cortex limits pop-out in the superior colliculus of awake mice</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>5772</fpage><lpage>5783</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx254</pub-id><pub-id pub-id-type="pmid">29029071</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadlou</surname><given-names>M</given-names></name><name><surname>Zweifel</surname><given-names>LS</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional modulation of primary visual cortex by the superior colliculus in the mouse</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3895</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06389-6</pub-id><pub-id pub-id-type="pmid">30254324</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aho</surname><given-names>K</given-names></name><name><surname>Derryberry</surname><given-names>D</given-names></name><name><surname>Peterson</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Model selection for ecologists: the worldviews of AIC and BIC</article-title><source>Ecology</source><volume>95</volume><fpage>631</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1890/13-1452.1</pub-id><pub-id pub-id-type="pmid">24804445</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ajina</surname><given-names>S</given-names></name><name><surname>Bridge</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Blindsight and unconscious vision: what they teach us about the human visual system</article-title><source>The Neuroscientist</source><volume>23</volume><fpage>529</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1177/1073858416673817</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>KM</given-names></name><name><surname>Lawlor</surname><given-names>J</given-names></name><name><surname>Salles</surname><given-names>A</given-names></name><name><surname>Moss</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Orienting our view of the superior colliculus: specializations and general functions</article-title><source>Current Opinion in Neurobiology</source><volume>71</volume><fpage>119</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.10.005</pub-id><pub-id pub-id-type="pmid">34826675</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allman</surname><given-names>J</given-names></name><name><surname>Miezin</surname><given-names>F</given-names></name><name><surname>McGuinness</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Stimulus specific responses from beyond the classical receptive field: neurophysiological mechanisms for local-global comparisons in visual neurons</article-title><source>Annual Review of Neuroscience</source><volume>8</volume><fpage>407</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.08.030185.002203</pub-id><pub-id pub-id-type="pmid">3885829</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barchini</surname><given-names>J</given-names></name><name><surname>Shi</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bidirectional encoding of motion contrast in the mouse superior colliculus</article-title><source>eLife</source><volume>7</volume><elocation-id>e35261</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.35261</pub-id><pub-id pub-id-type="pmid">29963987</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basso</surname><given-names>MA</given-names></name><name><surname>May</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Circuits for action and cognition: a view from the superior colliculus</article-title><source>Annual Review of Vision Science</source><volume>3</volume><fpage>197</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061234</pub-id><pub-id pub-id-type="pmid">28617660</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basso</surname><given-names>MA</given-names></name><name><surname>Bickford</surname><given-names>ME</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unraveling circuits of visual perception and cognition through the superior colliculus</article-title><source>Neuron</source><volume>109</volume><fpage>918</fpage><lpage>937</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.01.013</pub-id><pub-id pub-id-type="pmid">33548173</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienkowski</surname><given-names>MS</given-names></name><name><surname>Benavidez</surname><given-names>NL</given-names></name><name><surname>Wu</surname><given-names>K</given-names></name><name><surname>Gou</surname><given-names>L</given-names></name><name><surname>Becerra</surname><given-names>M</given-names></name><name><surname>Dong</surname><given-names>HW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Extrastriate connectivity of the mouse dorsal lateral geniculate thalamic nucleus</article-title><source>The Journal of Comparative Neurology</source><volume>527</volume><fpage>1419</fpage><lpage>1442</lpage><pub-id pub-id-type="doi">10.1002/cne.24627</pub-id><pub-id pub-id-type="pmid">30620046</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehnke</surname><given-names>SE</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>On the importance of the transient visual response in the superior colliculus</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>544</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.11.004</pub-id><pub-id pub-id-type="pmid">19059772</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bogadhi</surname><given-names>AR</given-names></name><name><surname>Hafed</surname><given-names>ZM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Express detection and discrimination of visual objects by primate superior colliculus neurons</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.02.08.479583</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casagrande</surname><given-names>VA</given-names></name><name><surname>Diamond</surname><given-names>IT</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Ablation study of the superior colliculus in the tree shrew (<italic>Tupaia glis</italic>)</article-title><source>The Journal of Comparative Neurology</source><volume>156</volume><fpage>207</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1002/cne.901560206</pub-id><pub-id pub-id-type="pmid">4424699</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Cazemier</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>SC-figure-detection</data-title><version designator="swh:1:rev:88fe069ca3361f9050bf8544dec1fa74db6aa2a1">swh:1:rev:88fe069ca3361f9050bf8544dec1fa74db6aa2a1</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:215b4a075552058fa354dd91a7aad45363f85096;origin=https://github.com/leoniecazemier/SC-figure-detection;visit=swh:1:snp:81eb7d12350040efd62c3ca04073dd83f21e0f12;anchor=swh:1:rev:88fe069ca3361f9050bf8544dec1fa74db6aa2a1">https://archive.softwareheritage.org/swh:1:dir:215b4a075552058fa354dd91a7aad45363f85096;origin=https://github.com/leoniecazemier/SC-figure-detection;visit=swh:1:snp:81eb7d12350040efd62c3ca04073dd83f21e0f12;anchor=swh:1:rev:88fe069ca3361f9050bf8544dec1fa74db6aa2a1</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CY</given-names></name><name><surname>Hafed</surname><given-names>ZM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Orientation and contrast tuning properties and temporal flicker fusion Characteristics of primate SUPERIOR COLLICULUS NEurons</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fncir.2018.00058</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><collab>cortex-lab</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Phy</data-title><version designator="swh:1:rev:8bf8cf751fcbd44210b3c5bbdba8c34a7b2cb1ce">swh:1:rev:8bf8cf751fcbd44210b3c5bbdba8c34a7b2cb1ce</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ca8acbda00d97da0dcb0a9005def02c7c466add6;origin=https://github.com/cortex-lab/phy;visit=swh:1:snp:fb86b64ad0dba5e38435549d7947a26d91e2e764;anchor=swh:1:rev:8bf8cf751fcbd44210b3c5bbdba8c34a7b2cb1ce">https://archive.softwareheritage.org/swh:1:dir:ca8acbda00d97da0dcb0a9005def02c7c466add6;origin=https://github.com/cortex-lab/phy;visit=swh:1:snp:fb86b64ad0dba5e38435549d7947a26d91e2e764;anchor=swh:1:rev:8bf8cf751fcbd44210b3c5bbdba8c34a7b2cb1ce</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamic contextual modulation in superior colliculus of awake mouse</article-title><source>eNeuro</source><volume>7</volume><elocation-id>ENEURO.0131-20.2020</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0131-20.2020</pub-id><pub-id pub-id-type="pmid">32868308</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Keyser</surname><given-names>R</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cue-invariant shape recognition in rats as tested with second-order contours</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.15.14</pub-id><pub-id pub-id-type="pmid">26605843</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Piet</surname><given-names>AT</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Riordan</surname><given-names>AJ</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Collicular circuits for flexible sensorimotor routing</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1110</fpage><lpage>1120</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00865-x</pub-id><pub-id pub-id-type="pmid">34083787</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Essig</surname><given-names>J</given-names></name><name><surname>Hunt</surname><given-names>JB</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inhibitory neurons in the superior colliculus mediate selection of spatially-directed movements</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>719</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02248-1</pub-id><pub-id pub-id-type="pmid">34117346</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>DA</given-names></name><name><surname>Stempel</surname><given-names>AV</given-names></name><name><surname>Vale</surname><given-names>R</given-names></name><name><surname>Ruehle</surname><given-names>S</given-names></name><name><surname>Lefler</surname><given-names>Y</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A synaptic threshold mechanism for computing escape decisions</article-title><source>Nature</source><volume>558</volume><fpage>590</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0244-6</pub-id><pub-id pub-id-type="pmid">29925954</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>Q</given-names></name><name><surname>Chou</surname><given-names>XL</given-names></name><name><surname>Peng</surname><given-names>B</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A differential circuit via retino-colliculo-pulvinar pathway enhances feature selectivity in visual cortex through surround suppression</article-title><source>Neuron</source><volume>105</volume><fpage>355</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.027</pub-id><pub-id pub-id-type="pmid">31812514</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fecteau</surname><given-names>JH</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Salience, relevance, and firing: a priority map for target selection</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>382</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.06.011</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gale</surname><given-names>SD</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distinct representation and distribution of visual information by specific cell types in mouse superficial superior colliculus</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>13458</fpage><lpage>13471</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2768-14.2014</pub-id><pub-id pub-id-type="pmid">25274823</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gandhi</surname><given-names>NJ</given-names></name><name><surname>Katnani</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Motor functions of the superior colliculus</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>205</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-061010-113728</pub-id><pub-id pub-id-type="pmid">21456962</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Top-down influences on visual processing</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>350</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id><pub-id pub-id-type="pmid">23595013</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girman</surname><given-names>SV</given-names></name><name><surname>Lund</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Most superficial sublamina of rat superior colliculus: neuronal response properties and correlates with perceptual figure-ground segregation</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>161</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1152/jn.00059.2007</pub-id><pub-id pub-id-type="pmid">17475720</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Histed</surname><given-names>MH</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mouse primary visual cortex is used to detect both orientation and contrast changes</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19416</fpage><lpage>19422</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3560-13.2013</pub-id><pub-id pub-id-type="pmid">24336708</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldbach</surname><given-names>HC</given-names></name><name><surname>Akitake</surname><given-names>B</given-names></name><name><surname>Leedy</surname><given-names>CE</given-names></name><name><surname>Histed</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Performance in even a simple perceptual task depends on mouse secondary visual areas</article-title><source>eLife</source><volume>10</volume><elocation-id>e62156</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.62156</pub-id><pub-id pub-id-type="pmid">33522482</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griggs</surname><given-names>WS</given-names></name><name><surname>Amita</surname><given-names>H</given-names></name><name><surname>Gopal</surname><given-names>A</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual neurons in the superior colliculus discriminate many objects by their historical values</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>396</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00396</pub-id><pub-id pub-id-type="pmid">29942248</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Bishop</surname><given-names>HI</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Defined cell types in superior colliculus make distinct contributions to prey capture behavior in the mouse</article-title><source>Current Biology</source><volume>29</volume><fpage>4130</fpage><lpage>4138</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.017</pub-id><pub-id pub-id-type="pmid">31761701</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Kamigaki</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Dan</surname><given-names>U</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prefrontal corticotectal neurons enhance visual processing through the superior colliculus and pulvinar thalamus</article-title><source>Neuron</source><volume>104</volume><fpage>1141</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.019</pub-id><pub-id pub-id-type="pmid">31668485</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>An inferior-superior colliculus circuit controls auditory cue-directed visual spatial attention</article-title><source>Neuron</source><volume>110</volume><fpage>109</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.10.004</pub-id><pub-id pub-id-type="pmid">34699777</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isa</surname><given-names>T</given-names></name><name><surname>Marquez-Legorreta</surname><given-names>E</given-names></name><name><surname>Grillner</surname><given-names>S</given-names></name><name><surname>Scott</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The tectum/superior colliculus as the vertebrate solution for spatial sensory integration and action</article-title><source>Current Biology</source><volume>31</volume><fpage>R741</fpage><lpage>R762</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.04.001</pub-id><pub-id pub-id-type="pmid">34102128</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>S</given-names></name><name><surname>Feldheim</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The mouse superior colliculus: an emerging model for studying circuit formation and function</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00010</pub-id><pub-id pub-id-type="pmid">29487505</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>HE</given-names></name><name><surname>Andolina</surname><given-names>IM</given-names></name><name><surname>Shipp</surname><given-names>SD</given-names></name><name><surname>Adams</surname><given-names>DL</given-names></name><name><surname>Cudeiro</surname><given-names>J</given-names></name><name><surname>Salt</surname><given-names>TE</given-names></name><name><surname>Sillito</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Figure-ground modulation in awake primate thalamus</article-title><source>PNAS</source><volume>112</volume><fpage>7085</fpage><lpage>7090</lpage><pub-id pub-id-type="doi">10.1073/pnas.1405162112</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>EJ</given-names></name><name><surname>Bautista</surname><given-names>AR</given-names></name><name><surname>Nunez</surname><given-names>MD</given-names></name><name><surname>Allen</surname><given-names>DC</given-names></name><name><surname>Tak</surname><given-names>JH</given-names></name><name><surname>Alvarez</surname><given-names>E</given-names></name><name><surname>Basso</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Causal role for the primate superior colliculus in the computation of evidence for perceptual decisions</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1121</fpage><lpage>1131</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00878-6</pub-id><pub-id pub-id-type="pmid">34183869</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname><given-names>R</given-names></name><name><surname>Takaura</surname><given-names>K</given-names></name><name><surname>Ikeda</surname><given-names>T</given-names></name><name><surname>Yoshida</surname><given-names>M</given-names></name><name><surname>Isa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contribution of the retino‐tectal pathway to visually guided saccades after lesion of the primary visual cortex in monkeys</article-title><source>European Journal of Neuroscience</source><volume>33</volume><fpage>1952</fpage><lpage>1960</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07729.x</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Born</surname><given-names>G</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>V1 microcircuits underlying mouse visual behavior</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>191</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.006</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>AJ</given-names></name><name><surname>Roth</surname><given-names>MM</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Feedback generates a second receptive field in neurons of the visual cortex</article-title><source>Nature</source><volume>582</volume><fpage>545</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2319-4</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khastkhodaei</surname><given-names>Z</given-names></name><name><surname>Jurjut</surname><given-names>O</given-names></name><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mice can use second-order, contrast-modulated stimuli to guide visual perception</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>4457</fpage><lpage>4469</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4595-15.2016</pub-id><pub-id pub-id-type="pmid">27098690</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinoshita</surname><given-names>M</given-names></name><name><surname>Kato</surname><given-names>R</given-names></name><name><surname>Isa</surname><given-names>K</given-names></name><name><surname>Kobayashi</surname><given-names>K</given-names></name><name><surname>Kobayashi</surname><given-names>K</given-names></name><name><surname>Onoe</surname><given-names>H</given-names></name><name><surname>Isa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissecting the circuit for blindsight to reveal the critical role of pulvinar and superior colliculus</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>135</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08058-0</pub-id><pub-id pub-id-type="pmid">30635570</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirchberger</surname><given-names>L</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name><name><surname>Schnabel</surname><given-names>UH</given-names></name><name><surname>van Beest</surname><given-names>EH</given-names></name><name><surname>Barsegyan</surname><given-names>A</given-names></name><name><surname>Levelt</surname><given-names>CN</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>van der Togt</surname><given-names>C</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The essential role of recurrent processing for figure-ground perception in mice</article-title><source>Science Advances</source><volume>7</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1126/sciadv.abe1833</pub-id><pub-id pub-id-type="pmid">34193411</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauzlis</surname><given-names>RJ</given-names></name><name><surname>Lovejoy</surname><given-names>LP</given-names></name><name><surname>Zénon</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Superior colliculus and visual spatial attention</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>165</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170249</pub-id><pub-id pub-id-type="pmid">23682659</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VAF</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The neurophysiology of figure-ground segregation in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>1605</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-02-01605.1995</pub-id><pub-id pub-id-type="pmid">7869121</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80042-1</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Piëch</surname><given-names>V</given-names></name><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contour saliency in primary visual cortex</article-title><source>Neuron</source><volume>50</volume><fpage>951</fpage><lpage>962</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.04.035</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Jiang</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>TY</given-names></name><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Yao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Phase-specific surround suppression in mouse primary visual cortex correlates with figure detection behavior based on phase discontinuity</article-title><source>Neuroscience</source><volume>379</volume><fpage>359</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.03.039</pub-id><pub-id pub-id-type="pmid">29608945</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Kühn</surname><given-names>NK</given-names></name><name><surname>Alkislar</surname><given-names>I</given-names></name><name><surname>Dublanc</surname><given-names>AS</given-names></name><name><surname>Zemmouri</surname><given-names>F</given-names></name><name><surname>Paesmans</surname><given-names>S</given-names></name><name><surname>Reinhard</surname><given-names>K</given-names></name><name><surname>Farrow</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Pathway-specific inputs to the superior colliculus support flexible triggering of innate behaviors</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.07.08.499294</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Gong</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interactions between feedback and lateral connections in the primary visual cortex</article-title><source>PNAS</source><volume>114</volume><fpage>8637</fpage><lpage>8642</lpage><pub-id pub-id-type="doi">10.1073/pnas.1706183114</pub-id><pub-id pub-id-type="pmid">28739915</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lien</surname><given-names>AD</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical direction selectivity emerges at convergence of thalamic synapses</article-title><source>Nature</source><volume>558</volume><fpage>80</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0148-5</pub-id><pub-id pub-id-type="pmid">29795349</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luongo</surname><given-names>FJ</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Ho</surname><given-names>CLA</given-names></name><name><surname>Hesse</surname><given-names>JK</given-names></name><name><surname>Wekselblatt</surname><given-names>JB</given-names></name><name><surname>Lanfranchi</surname><given-names>FF</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mice and primates use distinct strategies for visual segmentation</article-title><source>eLife</source><volume>12</volume><elocation-id>e74394</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.74394</pub-id><pub-id pub-id-type="pmid">36790170</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization of seven mouse visual cortical areas</article-title><source>Neuron</source><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masullo</surname><given-names>L</given-names></name><name><surname>Mariotti</surname><given-names>L</given-names></name><name><surname>Alexandre</surname><given-names>N</given-names></name><name><surname>Freire-Pritchett</surname><given-names>P</given-names></name><name><surname>Boulanger</surname><given-names>J</given-names></name><name><surname>Tripodi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Genetically defined functional modules for spatial orienting in the mouse superior colliculus</article-title><source>Current Biology</source><volume>29</volume><fpage>2892</fpage><lpage>2904</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.07.083</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohler</surname><given-names>CW</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Role of striate cortex and superior colliculus in visual guidance of saccadic eye movements in monkeys</article-title><source>Journal of Neurophysiology</source><volume>40</volume><fpage>74</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1152/jn.1977.40.1.74</pub-id><pub-id pub-id-type="pmid">401874</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montijn</surname><given-names>JS</given-names></name><name><surname>Seignette</surname><given-names>K</given-names></name><name><surname>Howlett</surname><given-names>MH</given-names></name><name><surname>Cazemier</surname><given-names>JL</given-names></name><name><surname>Kamermans</surname><given-names>M</given-names></name><name><surname>Levelt</surname><given-names>CN</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A parameter-free statistical test for neuronal responsiveness</article-title><source>eLife</source><volume>10</volume><elocation-id>e71969</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71969</pub-id><pub-id pub-id-type="pmid">34570697</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Montijn</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Universalprobefinder</data-title><version designator="1.0.3">1.0.3</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/JorritMontijn/UniversalProbeFinder">https://github.com/JorritMontijn/UniversalProbeFinder</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Montijn</surname><given-names>JS</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Universal pipeline for the alignment of electrode tracks with slice histology and electrophysiological data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.06.20.496782</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Montijn</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Acquipix</data-title><version designator="swh:1:rev:6ed92aa29c85eafd8d151bb78e5e50bfd9cb0ee8">swh:1:rev:6ed92aa29c85eafd8d151bb78e5e50bfd9cb0ee8</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:624091f7aa0af7e9412a4ec0c549e7c1630a1742;origin=https://github.com/JorritMontijn/Acquipix;visit=swh:1:snp:126b9be46d63e0bbcd9272bc9d9d38ce2e65c2fb;anchor=swh:1:rev:6ed92aa29c85eafd8d151bb78e5e50bfd9cb0ee8">https://archive.softwareheritage.org/swh:1:dir:624091f7aa0af7e9412a4ec0c549e7c1630a1742;origin=https://github.com/JorritMontijn/Acquipix;visit=swh:1:snp:126b9be46d63e0bbcd9272bc9d9d38ce2e65c2fb;anchor=swh:1:rev:6ed92aa29c85eafd8d151bb78e5e50bfd9cb0ee8</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><collab>MouseLand</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Kilosort</data-title><version designator="swh:1:rev:d55179f4bed45d4f17e5481283bc3f260212c1c7">swh:1:rev:d55179f4bed45d4f17e5481283bc3f260212c1c7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a6ed5c487c98492f124a2d064f40f035be09dc45;origin=https://github.com/MouseLand/Kilosort;visit=swh:1:snp:7e172e70c103f75aeebc6af7fd33987b219f1bde;anchor=swh:1:rev:d55179f4bed45d4f17e5481283bc3f260212c1c7">https://archive.softwareheritage.org/swh:1:dir:a6ed5c487c98492f124a2d064f40f035be09dc45;origin=https://github.com/MouseLand/Kilosort;visit=swh:1:snp:7e172e70c103f75aeebc6af7fd33987b219f1bde;anchor=swh:1:rev:d55179f4bed45d4f17e5481283bc3f260212c1c7</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Fast and accurate spike sorting of high-channel count probes with Kilosort</chapter-title><person-group person-group-type="editor"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>NIPS</publisher-name><fpage>4455</fpage><lpage>4463</lpage></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Sridhar</surname><given-names>S</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Solving the spike sorting problem with kilosort</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.07.523036</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pak</surname><given-names>A</given-names></name><name><surname>Ryu</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Chubykin</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Top-down feedback controls the cortical representation of illusory contours in mouse primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>648</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1998-19.2019</pub-id><pub-id pub-id-type="pmid">31792152</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poltoratski</surname><given-names>S</given-names></name><name><surname>Maier</surname><given-names>A</given-names></name><name><surname>Newton</surname><given-names>AT</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Figure-Ground modulation in the human lateral geniculate nucleus is distinguishable from top-down attention</article-title><source>Current Biology</source><volume>29</volume><fpage>2051</fpage><lpage>2057</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.068</pub-id><pub-id pub-id-type="pmid">31178323</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Raudies</surname><given-names>F</given-names></name><name><surname>Wannig</surname><given-names>A</given-names></name><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Neumann</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of attention in figure-ground segregation in areas V1 and V4 of the visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.032</pub-id><pub-id pub-id-type="pmid">22794268</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prins</surname><given-names>N</given-names></name><name><surname>Kingdom</surname><given-names>FAA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Applying the model-comparison approach to test specific research hypotheses in psychophysical research using the palamedes toolbox</article-title><source>Frontiers in Psychology</source><volume>9</volume><elocation-id>1250</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01250</pub-id><pub-id pub-id-type="pmid">30083122</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prusky</surname><given-names>GT</given-names></name><name><surname>Douglas</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Characterization of mouse cortical spatial vision</article-title><source>Vision Research</source><volume>44</volume><fpage>3411</fpage><lpage>3418</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.09.001</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ress</surname><given-names>D</given-names></name><name><surname>Backus</surname><given-names>BT</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Activity in primary visual cortex predicts performance in a visual detection task</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>940</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1038/78856</pub-id><pub-id pub-id-type="pmid">10966626</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resulaj</surname><given-names>A</given-names></name><name><surname>Ruediger</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>First spikes in visual cortex enable perceptual discrimination</article-title><source>eLife</source><volume>7</volume><elocation-id>e34044</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34044</pub-id><pub-id pub-id-type="pmid">29659352</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name><name><surname>Bosch</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Figure-ground segregation in a recurrent network architecture</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>525</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1162/08989290260045756</pub-id><pub-id pub-id-type="pmid">12126495</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnabel</surname><given-names>UH</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Figure-ground perception in the awake mouse and neuronal activity elicited by figure-ground stimuli in primary visual cortex</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>17800</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-36087-8</pub-id><pub-id pub-id-type="pmid">30542060</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>Vangeneugden</surname><given-names>J</given-names></name><name><surname>van Beest</surname><given-names>EH</given-names></name><name><surname>Grigore</surname><given-names>ME</given-names></name><name><surname>Levelt</surname><given-names>CN</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orientation-tuned surround suppression in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>9290</fpage><lpage>9304</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5051-13.2014</pub-id><pub-id pub-id-type="pmid">25009262</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Qu</surname><given-names>B</given-names></name><name><surname>Yan</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Cao</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Divergent midbrain circuits orchestrate escape and freezing responses to looming stimuli in mice</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>1232</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-03580-7</pub-id><pub-id pub-id-type="pmid">29581428</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Huang</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>WL</given-names></name><name><surname>Cao</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A subcortical excitatory circuit for sensory-triggered predatory hunting in mice</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>909</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0405-4</pub-id><pub-id pub-id-type="pmid">31127260</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamietto</surname><given-names>M</given-names></name><name><surname>Cauda</surname><given-names>F</given-names></name><name><surname>Corazzini</surname><given-names>LL</given-names></name><name><surname>Savazzi</surname><given-names>S</given-names></name><name><surname>Marzi</surname><given-names>CA</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Weiskrantz</surname><given-names>L</given-names></name><name><surname>de Gelder</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Collicular vision guides nonconscious behavior</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>888</fpage><lpage>902</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21225</pub-id><pub-id pub-id-type="pmid">19320547</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>AYY</given-names></name><name><surname>Brown</surname><given-names>BD</given-names></name><name><surname>Scholl</surname><given-names>B</given-names></name><name><surname>Mohanty</surname><given-names>D</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Orientation selectivity of synaptic input to neurons in mouse and cat primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12339</fpage><lpage>12350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2039-11.2011</pub-id><pub-id pub-id-type="pmid">21865476</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tunkl</surname><given-names>JE</given-names></name><name><surname>Berkley</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>The role of superior colliculus in vision: visual form discrimination in cats with superior colliculus ablations</article-title><source>The Journal of Comparative Neurology</source><volume>176</volume><fpage>575</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1002/cne.901760408</pub-id><pub-id pub-id-type="pmid">925202</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vangeneugden</surname><given-names>J</given-names></name><name><surname>van Beest</surname><given-names>EH</given-names></name><name><surname>Cohen</surname><given-names>MX</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name><name><surname>Kirchberger</surname><given-names>L</given-names></name><name><surname>Montijn</surname><given-names>JS</given-names></name><name><surname>Thamizharasu</surname><given-names>P</given-names></name><name><surname>Camillo</surname><given-names>D</given-names></name><name><surname>Levelt</surname><given-names>CN</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Activity in lateral visual areas contributes to surround suppression in awake mouse V1</article-title><source>Current Biology</source><volume>29</volume><fpage>4268</fpage><lpage>4275</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.037</pub-id><pub-id pub-id-type="pmid">31786063</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Sarnaik</surname><given-names>R</given-names></name><name><surname>Rangarajan</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual receptive field properties of neurons in the superficial superior colliculus of the mouse</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>16573</fpage><lpage>16584</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3305-10.2010</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Krauzlis</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual selective attention in mice</article-title><source>Current Biology</source><volume>28</volume><fpage>676</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.038</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>McAlonan</surname><given-names>K</given-names></name><name><surname>Goldstein</surname><given-names>S</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Krauzlis</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A causal role for mouse superior colliculus in visual perceptual decision-making</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3768</fpage><lpage>3782</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2642-19.2020</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Herman</surname><given-names>JP</given-names></name><name><surname>Krauzlis</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neuronal modulation in the mouse superior colliculus during covert visual selective attention</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>2482</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-06410-5</pub-id><pub-id pub-id-type="pmid">35169189</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>White</surname><given-names>BJ</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>The superior colliculus</chapter-title><person-group person-group-type="editor"><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><source>The Oxford Handbook of Eye Movements</source><publisher-name>Oxford Academic</publisher-name><fpage>195</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199539789.013.0011</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>BJ</given-names></name><name><surname>Berg</surname><given-names>DJ</given-names></name><name><surname>Kan</surname><given-names>JY</given-names></name><name><surname>Marino</surname><given-names>RA</given-names></name><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Superior colliculus neurons encode a visual saliency map during free viewing of natural dynamic video</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14263</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14263</pub-id><pub-id pub-id-type="pmid">28117340</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whyland</surname><given-names>KL</given-names></name><name><surname>Slusarczyk</surname><given-names>AS</given-names></name><name><surname>Bickford</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>GABAergic cell types in the superficial layers of the mouse superior colliculus</article-title><source>The Journal of Comparative Neurology</source><volume>528</volume><fpage>308</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1002/cne.24754</pub-id><pub-id pub-id-type="pmid">31396959</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>AB</given-names></name><name><surname>Lintz</surname><given-names>MJ</given-names></name><name><surname>Costabile</surname><given-names>JD</given-names></name><name><surname>Thompson</surname><given-names>JA</given-names></name><name><surname>Stubblefield</surname><given-names>EA</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An integrative role for the superior colliculus in selecting targets for movements</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>2118</fpage><lpage>2131</lpage><pub-id pub-id-type="doi">10.1152/jn.00262.2015</pub-id><pub-id pub-id-type="pmid">26203103</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zahler</surname><given-names>SH</given-names></name><name><surname>Taylor</surname><given-names>DE</given-names></name><name><surname>Wong</surname><given-names>JY</given-names></name><name><surname>Adams</surname><given-names>JM</given-names></name><name><surname>Feinberg</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Superior colliculus drives stimulus-evoked directionally biased saccades and attempted head movements in head-fixed mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e73081</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.73081</pub-id><pub-id pub-id-type="pmid">34970968</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Kan</surname><given-names>JYY</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Tu</surname><given-names>J</given-names></name><name><surname>Dorris</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Transforming absolute value to categorical choice in primate superior colliculus during value-based decision making</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>3410</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23747-z</pub-id><pub-id pub-id-type="pmid">34099726</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhaoping</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>From the optic tectum to the primary visual cortex: migration through evolution of the saliency map for exogenous attentional guidance</article-title><source>Current Opinion in Neurobiology</source><volume>40</volume><fpage>94</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.06.017</pub-id><pub-id pub-id-type="pmid">27420378</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83708.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wolff</surname><given-names>Mathieu</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057qpr032</institution-id><institution>CNRS, University of Bordeaux</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.09.25.509365" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.25.509365"/></front-stub><body><p>The authors present important work showing that the superficial, retinorecipient layers of the mouse superior colliculus (SC) contribute to figure-ground segregation and object recognition. Solid optogenetic approaches and analyses support these novel findings, which provide new insights into the circuits responsible for visual perception.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83708.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wolff</surname><given-names>Mathieu</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057qpr032</institution-id><institution>CNRS, University of Bordeaux</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.25.509365">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.09.25.509365v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Involvement of superior colliculus in complex figure detection of mice&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The optogenetic approach is undermined by possible methodological flaws including light excitation, the efficacy of the photostimulation and its specificity with possible additional circuit-level effect through GABA. More appropriate controls are needed to address these issues as suggested in the individual reviews (all reviewers).</p><p>2) The behavioral analysis and the statistical approach need to be strengthened: please consider other behavioral variables such as pupil linked arousal or licking (which possibly means new experimental work), some statistical analyses are currently not compelling and need a more rigorous take (e.g. take more units or sessions into account) (see reviewers 2 and 3 in particular).</p><p>In addition you are encouraged to address the specific points raised by reviewers in their individual reviews.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Crucial details about the used methodology are missing.</p><p>1) Which stimuli were presented and when?</p><p>What is &quot;background&quot; stimulus, which was presented during interstimulus intervals? What was the order of figure ground combinations, e.g. were they randomly presented or in blocks?</p><p>2) How is a correct response determine? Is it the side of the first lick after 200 ms after stimulus onset?</p><p>3) When is a trial counted as miss?</p><p>4) What data were used to decode stimulus (Figure 3C)? Was neural activity normalized? If so, how? This is important for interpreting the weights of the model. How many neurons were used in each decoder?</p><p>5) The method to exclude electrophysiology trials is not clear (line 627-648). What are samples i? What is multiplied in line 634 as c does not appear in the expression within the multiplication?</p><p>6) How many neurons were recorded in total and how were they selected for each analysis?</p><p>7) Showing how optogenetic inhibition influences reaction times could corroborate claims about the functional significance of sSC inhibition.</p><p>8) It was not immediately clear what positive and negative going tick marks mean, e.g., in Figure 1D. I guess these are left and right licks.</p><p>9) Use blue bars for phase in Figure 3D for a consistent colour code.</p><p>10) Explain abbreviation in one-way &quot;rm&quot; ANOVA.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>It is an important study and it should be published. However, amount of sessions and units used in analysis is low and not necessarily convincing at this stage. Also, I suggest some experiments to clarify the interpretation of results.</p><p>1) Keep similar size of individual panels in figures, for example Figure 2, panels I and J are much larger than other panels.</p><p>2) Show an example of single neurons (spikes/s), and then population response.</p><p>3) It was not a complete silencing of SC, rather a reduction of activity, as the authors mentioned. Can inactivation be done more efficient, for example using inhibitory opsin?</p><p>4) Figure 5A – not sure why shaded plot? Could authors show cross-validation? Are these are neurons included in analysis? Then this panel should earlier (when ephys recordings are introduced).</p><p>5) Add more sessions of contrast task, so this can be also included to main figures. Figures 3-5 are only about orientation and phase.</p><p>6) Show single behavioral sessions.</p><p>7) Could authors consider performing an experiment with inactivation of V1 and ephys recordings in SCs during the task? or simultaneous ephys recordings of the activity in V1 and SCs during the task? This could clarify the role of two visual pathways during the figure detection task and gives ne insight into interaction between cortex and the midbrain in perceptual task. Unfortunately, this study only speculates about different possibilities and there is lack of a clear result.</p><p>I think hypothesis here is that SCs computes visual information from the retina in this task so the FGM for contrast, orientation and phase occur much faster than in V1. However, comparison of results in SCs with previous study, showed that the onset times of the FGM for contrast, orientation and phase occur faster in V1 than SCs. Does it mean that these responses are inherited from V1, and SCs not necessary compute visual information that receives from the retina in this task? These results are confusing to me, this is why I suggest to perform additional experiment.</p><p>8) Overall more units should be included in analysis (Figure 2-5).</p><p>9) Figure 5 shows results from 8 units, and there is no clear measurement to clarify what these responses are. Overall these neurons seem to have a very weak responses (Suppl. Figure 3). SC is rich of multisensory (visual, auditory) and behavioral responses (lick, movements), hence 8 units is too low number. Could authors consider adding more data and performing analysis of behavioral correlates and neural activity? For example, DLC (Mathis et al. 2018) or facemap (Syeda et al. 2022, Stringer, Pachitariu et al. 2019) analysis could be used to extract behavioral variables from videos.</p><p>10) Decoding activity from 12 units/14 units from 3 mice in Figure 3 is underestimation. Could authors add more sessions and therefore units?</p><p>11) Citation formatting eg. line 31 (Prusky and Douglas, 2004; Glickfeld et al., 2013; Resulaj et al., 2018; Kirchberger et al., 2021). (Glickfeld et al., 2013; Kirchberger et al., 2021; Prusky and Douglas, 2004)</p><p>12) Could authors include more details about the task structure? When does animal receive a reward, after the first lick? How authors control animal lick direction and does the trial ends if the first lick is wrong? Are licks before the visual stimulus somehow punished? Could authors show example of the progress of learning the task? This was not clear, do authors use all grating orientations?</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Involvement of superior colliculus in complex figure detection of mice&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Joshua Gold (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been considerably improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Both reviewers have appreciated your effort in addressing their comments and improving the manuscript but a main issue still needs to be tackled in a more comprehensive manner.</p><p>Essential remaining revisions:</p><p>1/ As the approach taken to inactivate the SC leaves open the possibility that extra-SC projections may contribute to some extent at behavioral level, some of the claims need to be tempered throughout the whole manuscript, not only a specific portion of the discussion. Reviewer 1 provides some suggestions to do this and points out the parts that are still problematic.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I would like to thank the authors for their work and the improvement of their manuscript. Most of my concerns have been addressed well. However, I am not convinced by their arguments regarding the activation of inhibitory neurons in the sSC and I don't think that the additional statements in their discussion address the problem adequately.</p><p>The discussion clearly states that direct inhibition of other brain areas by the activation of inhibitory neurons in the sSC cannot be excluded, which is correct. The argument that a reduced firing rate of inhibitory neurons shortly after the onset of optogenetic stimulation would lead to a negligible effect in areas outside the SC is not convincing. If the optogenetic stimulation of inhibitory neurons was irrelevant for other brain areas, how could it be relevant for excitatory neurons in the SC causing reduced activity? However, the fact that direct inhibition of other areas, and thus their causal involvement in figure detection, cannot be excluded does not falsify the results of this paper. But the interpretation of the results needs to be changed. It cannot be concluded that the SC is &quot;causally involved in figure detection&quot; (Impact statement, lines 20, 73, 83-84, 106, 121, 204, 967, 1053). The statement that &quot;sSC was optogenetically inhibited&quot; (line 79) is also confusing. Instead, the method should be described correctly (inhibitory neurons in sSC were optogenetically activated with the goal to inactivate sSC, but which could also lead to inactivation of other brain areas). And while it is incorrect to say that SC is &quot;causally involved&quot;, I think it is fair to say that SC is involved in figure detection (as the paper title states), possibly by gating other areas that are causally involved. As the authors point out in their response, many studies have used the activation of SC inhibitory neurons with the goal to inactivate the SC. I think this trend is unfortunate as the interpretation of the results is very difficult and cannot be attributed to SC inactivation alone. The same mistake should not be repeated.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The authors have addressed the majority of previous comments by creating supplementary figures and discussing various points. Nevertheless, to improve the clarity of this manuscript and methods, it would be important for the authors to consider addressing the following points:</p><p>1. I disagree with the selection criteria of putative GAD2 neurons (Figure 1 – Supplementary Figure 2C).</p><p>The long duration of laser stimulation could impact postsynaptic excitation of the local network or through long-range pathways. Hence, authors should modify their criteria for identifying GABA-ergic neurons (Supplementary Figure 2C). This could be done by using a shorter time, e.g., 10ms, to measure a significant increase in photostimulation compared to baseline (10ms before). In the current version, 50ms is rather long and may indirectly impact other neurons in the network. Therefore, not all selected neurons are &quot;putative&quot; GAD2 neurons. Also, authors can build a stronger argument by presenting some features of selected GAD2 neurons, for example, plotting the number of selected GAD2 neurons and the number of spikes evoked during the 10ms window, the number of neurons and their latencies, as well as spike durations (see, for example, Thomas et al. 2023 Supplementary Figure 8E). Also, authors should report for selected putative GAD2 neurons a mean spike evoked, spike latency, and spike duration.</p><p>2. Fraction of units being impacted by photostimulation (Figure 1 – Supplementary Figure 2F).</p><p>The effect of inactivation is convincing, but the authors did not fully address my comment to measure a fraction of neurons being suppressed, activated, and those that did not change baseline firing rate during the activation of GAD2 neurons. The scatter plot is difficult to interpret; I suggest creating an additional bar plot representing the fraction of neurons being impacted by photostimulation.</p><p>3. References to panels in supplementary figures are unclear in the Results section. The authors refer to the entire Supplementary Figure, which contains different panels and points to different results. Additionally, it is confusing that the authors used &quot;Figure1-Supplementary Figure 1,&quot; &quot;Figure1-Supplementary Figure 2,&quot; etc., rather than &quot;Suppl. Figure 1,&quot; &quot;Suppl. Figure 2.&quot; This should be simplified.</p><p>4. For decoding SVM and LME models, authors should provide more details in the method section. The authors refer to SVM and LME models at multiple points. It would be beneficial for readers less familiar with decoding models to know which functions, parameters/features, or Matlab version are used. Additionally, the authors should mention the full name of models and not only use acronyms.</p><p>5. On line 618, the spike sorting results were manually curated using Phy. Authors should write their criteria for selecting MUA, &quot;putative – good&quot; units.</p><p>6. On line 542, spikes were isolated using Kilosort3. Add a citation. Is this the same version of Kilosort as mentioned in lines 616-617?</p><p>7. Figure 1 – Supplementary Figure 1H, it is not clear what is the reference, please clarify in the figure captions. The reduction of the evoked rate during optogenetic interference in GAD2-Cre mice is present across the entire depth of sSC. What is the reference in this plot (0 indicates the surface of sSC)?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83708.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #2 (Recommendations for the authors):</p><p>Crucial details about the used methodology are missing.</p><p>1) Which stimuli were presented and when?</p><p>What is &quot;background&quot; stimulus, which was presented during interstimulus intervals? What was the order of figure ground combinations, e.g. were they randomly presented or in blocks?</p></disp-quote><p>This information is now given in lines 445-456 and line 458. In short, each trial consisted of a grating stimulus with a figure (either on the left or right side), which was displayed for 1.5 seconds. Before each trial, during the ITI, we presented a full-screen grating without a figure. The mice could not earn rewards during the display of this grating, it served to teach/remind them that they can only earn reward by detecting a figure. The trials were distributed in blocks of 4, with each block containing one trial of each orientation (0/90 deg) and figure location (left/right). The 4 trials were always randomized within the blocks.</p><disp-quote content-type="editor-comment"><p>2) How is a correct response determine? Is it the side of the first lick after 200 ms after stimulus onset?</p></disp-quote><p>Indeed, the response of the mouse is the side of the first lick after 200 ms after stimulus onset. We have edited the Methods section (lines 449-456) to clarify this.</p><disp-quote content-type="editor-comment"><p>3) When is a trial counted as miss?</p></disp-quote><p>The response of the mouse is counted as a miss when the mouse does not lick in the 200 – 2000 ms window after stimulus onset. We have added this to the Methods (lines 449-456).</p><disp-quote content-type="editor-comment"><p>4) What data were used to decode stimulus (Figure 3C)? Was neural activity normalized? If so, how? This is important for interpreting the weights of the model. How many neurons were used in each decoder?</p></disp-quote><p>Yes, the neural activity was normalized before using it for decoding. Responses of each individual neuron were normalized to the mean response of that neuron across all trials where a grating was displayed inside the RF: Rnormalized = (R – Rbaseline)/(Rmax – Rbaseline), where R is the rate (see methods section, line 645-637). Each decoder used all recorded neurons that had been presented in the FGM figures, so 49+12 (RF inside + RF edge) units for orientation, 32+14 units for phase.</p><disp-quote content-type="editor-comment"><p>5) The method to exclude electrophysiology trials is not clear (line 627-648). What are samples i? What is multiplied in line 634 as c does not appear in the expression within the multiplication?</p></disp-quote><p>Samples i represent time-points. We’ve edited the wording in the paper for clarity. And indeed, the sub-script ‘c’ should also have appeared inside the geometric sum term as we take product of Zij over all channels. So the full subscript should have been Zijc. We have also updated this in the paper.</p><disp-quote content-type="editor-comment"><p>6) How many neurons were recorded in total and how were they selected for each analysis?</p></disp-quote><p>We have updated the methods section to include this information. The spike sorting preprocessing left us with a total of 241 neurons, 95 of which were excluded because they were not stably present throughout their respective recording sessions. Out of the 146 stable neurons we had recorded, 75 neurons had a reliable RF either on the center or edge of the figure. Out of the neurons with reliable RFs, 64 fulfilled the visually evoked response criterion. These 64 neurons are the neurons that are used for the analysis in Figures 2-4.</p><disp-quote content-type="editor-comment"><p>7) Showing how optogenetic inhibition influences reaction times could corroborate claims about the functional significance of sSC inhibition.</p></disp-quote><p>We’ve added a new Figure 1—figure supplement 5, which shows that optogenetic inhibition did not significantly affect lick rate and reaction times.</p><disp-quote content-type="editor-comment"><p>8) It was not immediately clear what positive and negative going tick marks mean, e.g., in Figure 1D. I guess these are left and right licks.</p></disp-quote><p>We have now added L/R indicators to the figure.</p><disp-quote content-type="editor-comment"><p>9) Use blue bars for phase in Figure 3D for a consistent colour code.</p></disp-quote><p>Fixed (in what is now Figure 3E)</p><disp-quote content-type="editor-comment"><p>10) Explain abbreviation in one-way &quot;rm&quot; ANOVA.</p></disp-quote><p>This means one-way repeated measures ANOVA. We have now written the description in full.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>It is an important study and it should be published. However, amount of sessions and units used in analysis is low and not necessarily convincing at this stage. Also, I suggest some experiments to clarify the interpretation of results.</p><p>1) Keep similar size of individual panels in figures, for example Figure 2, panels I and J are much larger than other panels.</p></disp-quote><p>We have updated Figures 2 and 3 to have more similar panel sizes across subfigures.</p><disp-quote content-type="editor-comment"><p>2) Show an example of single neurons (spikes/s), and then population response.</p></disp-quote><p>We have updated Figure 2H to show spikes/s of the example single neuron response<italic>.</italic></p><disp-quote content-type="editor-comment"><p>3) It was not a complete silencing of SC, rather a reduction of activity, as the authors mentioned. Can inactivation be done more efficient, for example using inhibitory opsin?</p></disp-quote><p>We have conducted new control experiments for the impact of laser stimulation on neural activity, now in awake animals (see Figure 1—figure supplement 2). It appears that the impact of the laser stimulation is much stronger in awake mice than anesthetized mice; we see an average spike rate reduction of 90% when the laser is on. We view this as sufficient reduction to draw some conclusions on the role of sSC in the behavioral tasks.</p><p>When we started with the original experiments, we had doubts whether the inhibitory opsins with which we had experience (halorhodopsin and archaerhodopsin) would sufficiently silence the sSC during the entire duration of the task. Also unwanted side effects of activating halorhodopsin and archaerhodopsin had been reported. We therefore chose the strategy to activate inhibitory neurons. Given the confounding factor of directly inhibiting some other areas, as discussed in response to remarks from other reviewers, and the arrival of new, possibly better, inhibitory opsins such as stGtACR2, we think that using an inhibitory opsin would now be a better choice.</p><disp-quote content-type="editor-comment"><p>4) Figure 5A – not sure why shaded plot? Could authors show cross-validation? Are these are neurons included in analysis? Then this panel should earlier (when ephys recordings are introduced).</p></disp-quote><p>The shading was to highlight the putative multisensory neurons. We have removed the shading and highlighted these cells in a different way. We don’t explicitly show the cross-validation but instead we now do a statistical test on whether these responses are time-locked (Zeta-test, Montijn et al., 2021). We have changed the figure from late in the manuscript to become Figure 2-figure supplement 2 and refer to it when the ephys recordings are introduced.</p><disp-quote content-type="editor-comment"><p>5) Add more sessions of contrast task, so this can be also included to main figures. Figures 3-5 are only about orientation and phase.</p></disp-quote><p>For each mouse, we could only record a limited number of sessions, because we reinserted the electrodes during every session. We were therefore limited in the number of trials give per animal and had to make a choice about which tasks to focus on. As earlier work had already shown involvement of the sSC in visually-evoked behaviours based on objects that are clearly isolated from the background, the main focus in this work was to show involvement of sSC in complex object detection, where the visual contrast and luminance is the same across object and background.</p><disp-quote content-type="editor-comment"><p>6) Show single behavioral sessions.</p></disp-quote><p>We have added a new Figure 1—figure supplement 1 with an example behavioral session and an example of the learning trajectory over the behavioral sessions. We have also added summarized data for individual mice in Figure 1—figure supplement 4.</p><disp-quote content-type="editor-comment"><p>7) Could authors consider performing an experiment with inactivation of V1 and ephys recordings in SCs during the task? or simultaneous ephys recordings of the activity in V1 and SCs during the task? This could clarify the role of two visual pathways during the figure detection task and gives ne insight into interaction between cortex and the midbrain in perceptual task. Unfortunately, this study only speculates about different possibilities and there is lack of a clear result.</p></disp-quote><p>Thank you for the suggestion. We agree that this would be an interesting experiment, and that it could confirm the suggestion from the latency analysis and the previous literature that the figure modulation that we see in orientation-defined figures is computed independently in the sSC. However, given the very considerable effort that would go into to training a new set of mice on this task, and that we will have to train a new researcher to train mice on this task as the first author has gotten a new position outside of academia, we believe that the limited resources of the lab are better spend on other investigations.</p><disp-quote content-type="editor-comment"><p>I think hypothesis here is that SCs computes visual information from the retina in this task so the FGM for contrast, orientation and phase occur much faster than in V1. However, comparison of results in SCs with previous study, showed that the onset times of the FGM for contrast, orientation and phase occur faster in V1 than SCs. Does it mean that these responses are inherited from V1, and SCs not necessary compute visual information that receives from the retina in this task? These results are confusing to me, this is why I suggest to perform additional experiment.</p></disp-quote><p>We believe that the modulation is computed in sSC independently from V1, because the onset latencies for the FGM in the orientation-defined figures in sSC and V1 are equal. The onset for the contrast-defined figure is longer in the sSC than in V1, but inspection of Figure 2I suggests that the long latency until the difference between figure and gray background reached the threshold value was mostly due to spurious spontaneous activity in the gray background trials occurring even before the onset of the responses in the figure trials. We therefore do not give much weight to the latency difference for this task, where we recorded a much smaller number of units than for the orientation-defined figure that was the main aim of our study. From previous experiments (Ahmadlou et al. 2017) we already know that the responses to contrast-defined figures in the sSC are not delayed when visual cortex is silenced.</p><disp-quote content-type="editor-comment"><p>8) Overall more units should be included in analysis (Figure 2-5).</p></disp-quote><p>We have updated figure 3 such that it is hopefully clearer that we used data from both figure 2 and 3 for our decoding analysis. Unfortunately, the first author has taken up another position, and training a new researcher to train a new set of animals to record from requires a very large time investments. We agree that more data would be nice, but we think that we have sufficient data to draw the conclusions that we draw, and that there are better purposes for this time investment and animals.</p><disp-quote content-type="editor-comment"><p>9) Figure 5 shows results from 8 units, and there is no clear measurement to clarify what these responses are. Overall these neurons seem to have a very weak responses (Suppl. Figure 3). SC is rich of multisensory (visual, auditory) and behavioral responses (lick, movements), hence 8 units is too low number. Could authors consider adding more data and performing analysis of behavioral correlates and neural activity? For example, DLC (Mathis et al. 2018) or facemap (Syeda et al. 2022, Stringer, Pachitariu et al. 2019) analysis could be used to extract behavioral variables from videos.</p></disp-quote><p>Thank you for the suggestion of extraction behavioral variables from the videos. Unfortunately, the pupil information was directly on-line analyzed from the eye cameras, and the movies were not recorded. We had a camera installed to monitor the behavior and state of the mouse during the experiment, but we did not store these movies. Given the low number of neurons and the lack of specific tests to better understand to what these putative multisensory neurons responded, we have decided to move this figure and the accompanying text to a figure supplement (Figure 2—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>10) Decoding activity from 12 units/14 units from 3 mice in Figure 3 is underestimation. Could authors add more sessions and therefore units?</p></disp-quote><p>We were unclear in how we did this decoding. Decoding activity was not done using only RF edge neurons (12/14) but was also including the RF center neurons (49/32 neurons). So the total included neurons for orientation/phase task decoding was 61/46. We’ve added a new subfigure (3C) that summarizes the methods of the decoding and hopefully also clarifies this.</p><disp-quote content-type="editor-comment"><p>11) Citation formatting eg. line 31 (Prusky andDouglas, 2004; Glickfeld et al., 2013; Resulaj et al., 2018; Kirchberger et al., 2021). (Glickfeld et al., 2013; Kirchberger et al., 2021; Prusky and Douglas, 2004)</p></disp-quote><p>Thank you for noticing. We have corrected it.</p><disp-quote content-type="editor-comment"><p>12) Could authors include more details about the task structure? When does animal receive a reward, after the first lick? How authors control animal lick direction and does the trial ends if the first lick is wrong? Are licks before the visual stimulus somehow punished?</p></disp-quote><p>We have added additional information in the section on the behavioral task in the methods section to cover these questions (lines 442-456). The animal indeed receives the reward after the first lick, if the lick was on the correct side. If the first lick is on the incorrect side, the stimulus timing does not change, the mouse simply does not receive a reward. During initial training we broke off trials after an early lick, so that the mouse could not earn a reward on that trial. The mice generally learned to not lick early, so during the experiments represented in this paper, early licks were not punished.</p><disp-quote content-type="editor-comment"><p>Could authors show example of the progress of learning the task? This was not clear, do authors use all grating orientations?</p></disp-quote><p>We have added a new figure 1—figure supplement 1B with an example of the task performance over sessions. We used grating orientations of 0 and 90 degrees. We have added this more explicitly to line 447.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Essential remaining revisions:</p><p>Reviewer #2 (Recommendations for the authors):</p><p>I would like to thank the authors for their work and the improvement of their manuscript. Most of my concerns have been addressed well. However, I am not convinced by their arguments regarding the activation of inhibitory neurons in the sSC and I don't think that the additional statements in their discussion address the problem adequately.</p><p>The discussion clearly states that direct inhibition of other brain areas by the activation of inhibitory neurons in the sSC cannot be excluded, which is correct. The argument that a reduced firing rate of inhibitory neurons shortly after the onset of optogenetic stimulation would lead to a negligible effect in areas outside the SC is not convincing. If the optogenetic stimulation of inhibitory neurons was irrelevant for other brain areas, how could it be relevant for excitatory neurons in the SC causing reduced activity? However, the fact that direct inhibition of other areas, and thus their causal involvement in figure detection, cannot be excluded does not falsify the results of this paper. But the interpretation of the results needs to be changed. It cannot be concluded that the SC is &quot;causally involved in figure detection&quot; (Impact statement, lines 20, 73, 83-84, 106, 121, 204, 967, 1053). The statement that &quot;sSC was optogenetically inhibited&quot; (line 79) is also confusing. Instead, the method should be described correctly (inhibitory neurons in sSC were optogenetically activated with the goal to inactivate sSC, but which could also lead to inactivation of other brain areas). And while it is incorrect to say that SC is &quot;causally involved&quot;, I think it is fair to say that SC is involved in figure detection (as the paper title states), possibly by gating other areas that are causally involved. As the authors point out in their response, many studies have used the activation of SC inhibitory neurons with the goal to inactivate the SC. I think this trend is unfortunate as the interpretation of the results is very difficult and cannot be attributed to SC inactivation alone. The same mistake should not be repeated.</p></disp-quote><p>We have made the suggested changes. In particular, we have removed the word ‘causally’ from the impact statement, lines 20, 73, 83-84, 106, 121, 204, 967, 1053. We removed statement in line 79 that the sSC was optogenetically inhibited. The text around that point is now “To test the involvement of the sSC in object detection, we injected a viral vector with Cre-dependent ChR2, an excitatory opsin, in sSC of GAD2-Cre mice. We subsequently implanted optic fibers to target blue light onto the SC (Figure 1C). Laser light activated the inhibitory neurons in sSC and reduced the overall activity in superior colliculus”. For clarity, we have also removed similar references to “optogenetic inhibition” where this was not directly accompanied by an explanation that we activated the GABAergic neurons, in lines 109, 502, 504, 511, 515, 532, 973, 1069. The point that activation GABAergic neurons may also directly impact other regions is discussed in lines 218-231. We think that the discussion should provide ample warning about possible side-effects.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The authors have addressed the majority of previous comments by creating supplementary figures and discussing various points. Nevertheless, to improve the clarity of this manuscript and methods, it would be important for the authors to consider addressing the following points:</p><p>1. I disagree with the selection criteria of putative GAD2 neurons (Figure 1 – Supplementary Figure 2C).</p><p>The long duration of laser stimulation could impact postsynaptic excitation of the local network or through long-range pathways. Hence, authors should modify their criteria for identifying GABA-ergic neurons (Supplementary Figure 2C). This could be done by using a shorter time, e.g., 10ms, to measure a significant increase in photostimulation compared to baseline (10ms before). In the current version, 50ms is rather long and may indirectly impact other neurons in the network. Therefore, not all selected neurons are &quot;putative&quot; GAD2 neurons. Also, authors can build a stronger argument by presenting some features of selected GAD2 neurons, for example, plotting the number of selected GAD2 neurons and the number of spikes evoked during the 10ms window, the number of neurons and their latencies, as well as spike durations (see, for example, Thomas et al. 2023 Supplementary Figure 8E). Also, authors should report for selected putative GAD2 neurons a mean spike evoked, spike latency, and spike duration.</p></disp-quote><p>We have revised the selection criteria for putative GAD2+ units. Units are now considered putative GAD2+ if, during the 10ms following laser onset, their firing rate is higher than the 10 ms before onset, and the zeta test indicates significant (p &lt; 0.05) time-locking to the laser onset of spiking activity in that same period. Additionally, we exclude units that never spike during the visual stimulus presentation and are potentially artefactual. To better characterize the putative GAD2+ units, we have added plots of the number of spikes evoked during the first 10 ms after laser onset, the first spike latency, and first spike jitter (new Figure 1—figure supplement 2D-F). The reason for including the putative GAD2+ category was to illustrate that even for units that initially display increased spiking activity in response to optogenetic stimulation (and may thus express ChR2), visually evoked firing rates in laser on trials are reduced relative to laser off trials. Since we do not conclude anything specific about GAD2+ neurons, further characterization of the putative GAD2+ units is beyond the scope of this paper.</p><disp-quote content-type="editor-comment"><p>2. Fraction of units being impacted by photostimulation (Figure 1 – Supplementary Figure 2F).</p><p>The effect of inactivation is convincing, but the authors did not fully address my comment to measure a fraction of neurons being suppressed, activated, and those that did not change baseline firing rate during the activation of GAD2 neurons. The scatter plot is difficult to interpret; I suggest creating an additional bar plot representing the fraction of neurons being impacted by photostimulation.</p></disp-quote><p>We have added a histogram of the percentage change in firing rate in laser on relative to laser off trials, (new Figure 1—figure supplement 2I) for the different categories of units, showing a strong reduction of visually evoked spiking activity for almost all units (only 3/170 units displayed an increase).</p><disp-quote content-type="editor-comment"><p>3. References to panels in supplementary figures are unclear in the Results section. The authors refer to the entire Supplementary Figure, which contains different panels and points to different results. Additionally, it is confusing that the authors used &quot;Figure1-Supplementary Figure 1,&quot; &quot;Figure1-Supplementary Figure 2,&quot; etc., rather than &quot;Suppl. Figure 1,&quot; &quot;Suppl. Figure 2.&quot; This should be simplified.</p></disp-quote><p>We have made changes in lines 75, 84-85, 86-88, 90, 102-104 to more specifically refer to individual panels of the supplementary figures, where appropriate. We agree that the use of ‘Figure X—supplementary figure Y’ is, if not confusing, at least a bit cumbersome, but this is now the new journal policy to which we have been asked to adhere.</p><disp-quote content-type="editor-comment"><p>4. For decoding SVM and LME models, authors should provide more details in the method section. The authors refer to SVM and LME models at multiple points. It would be beneficial for readers less familiar with decoding models to know which functions, parameters/features, or Matlab version are used. Additionally, the authors should mention the full name of models and not only use acronyms.</p></disp-quote><p>We have now written out the acronyms SVM (support vector machine) and LME (linear mixed effect). We have added that we used the Matlab functions “fitcsvm” for the linear SVM (line 689) and “fitlme” for the LME (line 552). We have now included the options that we used for the fitting in the Methods. For fitcsvm, these were KernelFunction linear; PolynomialOrder None; KernelScale auto; BoxConstraint 1; Standardize true. For fitlme, this was FitMethod REML. The exact LME model definitions that we used, e.g. <italic>Normalized rate ~ stimulus + (1|mouse) + (1|session) + (1|unit),</italic> for each figure panel are given in the source data attached to the figures. The Matlab versions that we used are R2019a and R2022b. We have added this information to the Methods section.</p><disp-quote content-type="editor-comment"><p>5. On line 618, the spike sorting results were manually curated using Phy. Authors should write their criteria for selecting MUA, &quot;putative – good&quot; units.</p></disp-quote><p>We have elaborated on the manual curation in line 624-630. Manual curation was only done for the analysis of behaving electrophysiology data, which were analyzed using Kilosort2. The manual curation was done in two phases, the first one being clean-up of the automatically generated clusters. Some clusters contained large-amplitude noise artifacts, due to muscle contractions of the mouse or electrical currents from lick detection. In the second phase we labeled the clusters as being single- or multi-unit, based on Kilosort quality scores and spread of spike detection across the laminar probe. Some multi-unit clusters seemed to include small single-unit clusters, these we separated from their multi-unit cluster. Only single-unit clusters that were stable across the recording session were included in the analysis for this paper.</p><disp-quote content-type="editor-comment"><p>6. On line 542, spikes were isolated using Kilosort3. Add a citation. Is this the same version of Kilosort as mentioned in lines 616-617?</p></disp-quote><p>This was the newer version of Kilosort, as these control experiments were redone later in response to reviewers’ comments. We believe that the details of the sorting do not affect the conclusions. We have added a reference for the new version (Pachitariu et al. 2023) in the manuscript.</p><disp-quote content-type="editor-comment"><p>7. Figure 1 – Supplementary Figure 1H, it is not clear what is the reference, please clarify in the figure captions. The reduction of the evoked rate during optogenetic interference in GAD2-Cre mice is present across the entire depth of sSC. What is the reference in this plot (0 indicates the surface of sSC)?</p></disp-quote><p>The reviewer was right in assuming that 0 indicates the dorsal surface of the sSC. We have changed the axis label accordingly and added a longer explanation to the figure caption.</p></body></sub-article></article>