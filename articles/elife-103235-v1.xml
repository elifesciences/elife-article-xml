<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103235</article-id><article-id pub-id-type="doi">10.7554/eLife.103235</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103235.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Selective attention and sensitivity to auditory disturbances in a virtually real classroom</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Levy</surname><given-names>Orel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0008-9075-2818</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Libman Hackmon</surname><given-names>Shirley</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zvilichovsky</surname><given-names>Yair</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Korisky</surname><given-names>Adi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Bidet-Caulet</surname><given-names>Aurelie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0002-8135-0725</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Schweitzer</surname><given-names>Julie B</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zion Golumbic</surname><given-names>Elana</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8831-3188</contrib-id><email>elana.zion-golumbic@biu.ac.il</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03kgsv495</institution-id><institution>The Gonda Brain Research Center, Bar Ilan University</institution></institution-wrap><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>Aix Marseille Univ, Inserm, INS, Inst Neurosci Syst</institution></institution-wrap><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>Department of Psychiatry and Behavioral Sciences, University of California, Davis</institution></institution-wrap><addr-line><named-content content-type="city">Sacramento</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>MIND Institute, University of California, Davis</institution></institution-wrap><addr-line><named-content content-type="city">Sacramento</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ding</surname><given-names>Nai</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Zhejiang University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>05</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP103235</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-09-16"><day>16</day><month>09</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-09-08"><day>08</day><month>09</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.17.590012"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-04"><day>04</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103235.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-04-08"><day>08</day><month>04</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103235.2"/></event></pub-history><permissions><copyright-statement>Â© 2024, Levy et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Levy et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103235-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-103235-figures-v1.pdf"/><abstract><p>Many people, and particularly individuals with attention deficit (hyperactivity) disorder (AD(H)D), find it difficult to maintain attention during classroom learning. However, traditional paradigms used to evaluate attention do not capture the complexity and dynamic nature of real-life classrooms. Using a novel virtual reality platform, coupled with measurement of neural activity, eye-gaze, and skin conductance, here we studied the neurophysiological manifestations of attention and distractibility, under realistic learning conditions. Individuals with AD(H)D exhibited higher neural responses to irrelevant sounds and reduced speech tracking of the teacher, relative to controls. Additional neurophysiological measures, such the power of alpha-oscillations and frequency of gaze-shifts away from the teacher, contributed to explaining variance in self-reported AD(H)D symptoms across the sample. These ecologically valid findings provide critical insight into the neurophysiological mechanisms underlying individual differences in the capacity for sustained attention and the proneness to distraction and mind-wandering, experienced in real-life situations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>attention</kwd><kwd>EEG</kwd><kwd>virtual reality</kwd><kwd>eye tracking</kwd><kwd>speech processing</kwd><kwd>ADHD</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006221</institution-id><institution>United States - Israel Binational Science Foundation</institution></institution-wrap></funding-source><award-id>2022024</award-id><principal-award-recipient><name><surname>Schweitzer</surname><given-names>Julie B</given-names></name><name><surname>Zion Golumbic</surname><given-names>Elana</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>2339/20</award-id><principal-award-recipient><name><surname>Zion Golumbic</surname><given-names>Elana</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>MH135266-02</award-id><principal-award-recipient><name><surname>Schweitzer</surname><given-names>Julie B</given-names></name><name><surname>Zion Golumbic</surname><given-names>Elana</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R33MH110043</award-id><principal-award-recipient><name><surname>Schweitzer</surname><given-names>Julie B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural and gaze-based metrics provide novel insights into individual differences in attention and distractability in a simulated classroom.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Many people find it difficult to maintain attention to a frontal classroom lecture. Doing so requires listening, processing, and comprehending the teacherâs speech over a prolonged period of time, while avoiding distraction from both irrelevant background sounds and internal mind-wandering (<xref ref-type="bibr" rid="bib104">Smallwood et al., 2007</xref>; <xref ref-type="bibr" rid="bib107">Szpunar et al., 2013</xref>; <xref ref-type="bibr" rid="bib109">Thomson et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Esterman and Rothlein, 2019</xref>). This task is supposedly even more difficult for individuals diagnosed with attention-deficit disorder or attention-deficit and hyperactivity disorder (referred to jointly as AD(H)D), which is often characterized by increase proneness to distraction and mind-wandering and poorer sustained attention (<xref ref-type="bibr" rid="bib6">Avisar and Shalev, 2011</xref>; <xref ref-type="bibr" rid="bib13">Berger and Cassuto, 2014</xref>; <xref ref-type="bibr" rid="bib72">Merrill et al., 2022</xref>). Unsurprisingly perhaps, individuals are most likely to seek clinical help as a result of attentional difficulties experienced in school contexts (<xref ref-type="bibr" rid="bib61">Litner, 2003</xref>), and there is a tight link between attention performance and learning outcomes (<xref ref-type="bibr" rid="bib62">Loe and Feldman, 2007</xref>; <xref ref-type="bibr" rid="bib39">Gray et al., 2017</xref>).</p><p>However, current clinical and scientific tools used for evaluating and quantifying the constructs of âdistractibilityâ and âinattentionâ are greatly removed from the real-life experience in organic classrooms and other ecological contexts. Commonly used approaches either rely on subjective self-report (e.g., rating scales), or are based on highly artificial computerized tests, such as Continuous Performance Tests (CPT). Unfortunately, in recent years, there is a growing understanding that these tools lack sufficient sensitivity and specificity to reliably capture the type and severity of real-life attention challenges that people face outside the lab or the clinic (<xref ref-type="bibr" rid="bib91">Rabiner et al., 2010</xref>; <xref ref-type="bibr" rid="bib80">Narad et al., 2015</xref>; <xref ref-type="bibr" rid="bib42">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="bib78">Murray et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Mulraney et al., 2022</xref>; <xref ref-type="bibr" rid="bib48">Hobbiss and Lavie, 2024</xref>).</p><p>In attempt to create laboratory-based tasks that are more ecologically valid, and better capture the type of attentional challenges faced in real-life, recent years have seen increased use of virtual reality (VR) in cognitive research (<xref ref-type="bibr" rid="bib85">Parsons, 2015</xref>; <xref ref-type="bibr" rid="bib101">SeesjÃ¤rvi et al., 2022</xref>). In particular, VR classrooms have been used to test different aspects of cognitive performance and distraction by ecologically valid stimuli. For example, it has been suggested that CPT tasks administered in VR classroom environments show greater sensitivity for AD(H)D classification relative to standard evaluations and can provide more refined measures of attention and distractibility (<xref ref-type="bibr" rid="bib95">Rizzo et al., 2006</xref>; <xref ref-type="bibr" rid="bib81">NeguÈ et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Coleman et al., 2019</xref>; <xref ref-type="bibr" rid="bib86">Parsons et al., 2019</xref>; <xref ref-type="bibr" rid="bib106">Stokes et al., 2022</xref>). However, to date, most VR classroom studies still use relatively non-ecological tasks, and rely primarily on indirect behavioral outcomes to assess aspects of attentional performance. To improve the utility of VR classrooms for understanding attention and distraction in real-life classroom learning, these platforms need to use genuine academic tasks and stimuli, to more faithfully simulate the classroom experience. Moreover, there is need to incorporate more objective metrics related to attention, such as measurements of neural activity and physiological responses, in order to obtain a more comprehensive picture of how listeners actually respond to stimuli and process the content of a lesson in a VR classroom. Here, we address this challenge by introducing a highly realistic multimodal VR classroom platform, that simulates participating in a frontal lesson by an avatar-teacher with occasional disturbances from different types of ecological sound-events (non-verbal human sounds such as coughs and throat clearings; and artificial sounds such as phone ringtones) (see also <xref ref-type="bibr" rid="bib59">Levy et al., 2025</xref>). Our novel platform also integrates measurements of neurophysiological responses, including neural activity (using electroencephalography; EEG), eye-gaze, and arousal levels, as reflected in changes in skin conductance (SC). These multi-level recordings allow us to investigate which neurophysiological metrics distinguish between students with and without a diagnosis of AD(H)D when they engage in realistic classroom learning, and which metrics reliably predict the severity of AD(H)D symptoms.</p><p>We focus on several specific neural and physiological metrics, that have been associated with aspects of attention, distraction, and information processing in more traditional attention research, and test their generalizability to this ecological context. These include: (1) Neural speech tracking of the teacherâs lesson, which captures aspects of its sensory and linguistic processing, and is known to be reduced under conditions of inattention or distraction (<xref ref-type="bibr" rid="bib26">Ding and Simon, 2012a</xref>; <xref ref-type="bibr" rid="bib120">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Huang and Elhilali, 2020</xref>; <xref ref-type="bibr" rid="bib49">Holtze et al., 2021</xref>; <xref ref-type="bibr" rid="bib55">Kaufman and ZionâGolumbic, 2023</xref>). Accordingly, this metric might be expected to be reduced in individuals with AD(H)D and/or in conditions that contain external disturbances; (2) Neural event-related potentials (ERPs), transient changes in SC, and overt gaze-shifts following unexpected sound-events in the background of the classroom. These metrics are thought to reflect exogenous capture of attention, increases in arousal and potentially distraction by salient irrelevant stimuli (<xref ref-type="bibr" rid="bib90">Posner, 1980</xref>; <xref ref-type="bibr" rid="bib15">Bidet-Caulet et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Masson and Bidet-Caulet, 2019</xref>); (3) The frequency of gaze-shifts away from the teacher, and time spent looking around the classroom, metrics associated with attention-shifts and distractibility (<xref ref-type="bibr" rid="bib40">Grosbras et al., 2005</xref>; <xref ref-type="bibr" rid="bib97">Schomaker et al., 2017</xref>), and are often heightened in individuals with AD(H)D (<xref ref-type="bibr" rid="bib71">Mauriello et al., 2022</xref>; <xref ref-type="bibr" rid="bib106">Stokes et al., 2022</xref>; <xref ref-type="bibr" rid="bib102">Selaskowski et al., 2023</xref>); (4) The power of alpha- and beta-oscillations, which are often associated with increased mind-wandering or boredom (<xref ref-type="bibr" rid="bib22">Clarke et al., 2001</xref>; <xref ref-type="bibr" rid="bib16">Boudewyn and Carter, 2018</xref>; <xref ref-type="bibr" rid="bib52">Jin et al., 2019</xref>), and some have suggested may be altered in individuals with AD(H)D (although use of spectral signatures as biomarkers for AD(H)D is highly controversial; <xref ref-type="bibr" rid="bib38">Gloss et al., 2016</xref>); (5) Continuous levels of arousal, as measured by SC, which some propose are either heightened or reduced in individuals with AD(H)D relative to their control peers (<xref ref-type="bibr" rid="bib103">Sergeant, 2000</xref>; <xref ref-type="bibr" rid="bib11">Bellato et al., 2020</xref>).</p><p>This novel ecological approach and the rich repertoire of neurophysiological metrics measured here, afford unique insights into the mechanistic underpinnings of paying attention in class and the disruptive effects of background sounds, in both the neurotypical and AD(H)D population.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The current dataset is extremely rich, consisting of many different behavioral, neural, and physiological responses. In reporting these results, we have separated between metrics that are associated with <bold>paying attention to the teacher</bold> (behavioral performance, neural tracking of the teacherâs speech, and looking at the teacher), those capturing <bold>responses to the irrelevant sound-events</bold> (ERPs and event-related changes in SC and gaze); as well as more global neurophysiological measures that may be associated with the listenersâ overall <bold>âstateâ of attention or arousal</bold> (alpha- and beta-power and tonic SC).</p><sec id="s2-1"><title>Paying attention to the teacher</title><p>First, we tested whether there are differences between individuals with and without AD(H)D in metrics associated with focusing onesâ attention on the teacher â accuracy on answering comprehension questions, neural speech tracking of the teacherâs speech, and the gaze-patterns of focusing overt attention toward the teacher.</p><p>Due to the difference in number of trials statistical testing of between-group differences were performed separately for the Events and Quiet conditions (when applicable), and comparisons between conditions are evaluated only qualitatively.</p><sec id="s2-1-1"><title>Behavioral accuracy</title><p>Participants demonstrated overall good performance on the comprehension task, achieving an average accuracy of 87.87% (Â±6.77% SEM). This serves as verification that participants followed the instructions, listened to teacherâs speech and understood the content of the mini-lessons. Performance levels were similarly good in both the Events and Quiet conditions, indicating that the presence of occasional sound-events did not disrupt overall understanding of the mini-lessons. No significant differences in performance were found between groups, in either condition [Events: <italic>t</italic>(47) = 1.052, p = 0.297, Cohenâs <italic>d</italic> = 0.30, Bayes Factor (BF<sub>10</sub>) = 0.448 (weak support of H0); Quiet: <italic>t</italic>(47) = 1.004, p = 0.320, Cohenâs <italic>d</italic> = 0.28, BF<sub>10</sub> = 0.430 (weak support for H0); <xref ref-type="fig" rid="fig1">Figure 1</xref>].</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Accuracy on comprehension questions.</title><p>Shown separately for the attention deficit (hyperactivity) disorder (AD(H)D) and Control groups, in the Quiet and Events conditions. Bar graphs represent average accuracy levels across participants and error bars represent the SEM. No significant differences were found between groups in either condition (n.s).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig1-v1.tif"/></fig></sec><sec id="s2-1-2"><title>Speech tracking of teacherâs speech</title><p>We conducted speech-tracking analysis of the neural response to the teacherâs speech, using both an encoding and decoding approach. As noted in the methods section, speech-tracking analysis could only be performed reliably in the Events condition, and we used a multivariate approach to account for variance in the neural activity due to the presence of event-sounds (see <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref> for comparison with univariate analysis).</p><p>The temporal response functions (TRFs) estimated in response to the teachersâ speech were similar across both groups showing the typical negative peak at ~100 ms (N1), followed by a positive peak ~200 ms (P2), and the modelâs predictive power following the traditional mid-central topography associated with auditory responses (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Speech tracking of the teacher in the presence of sound-events.</title><p>(<bold>A</bold>) Topographical distribution of the predictive power values, estimated using the multivariate encoding model, in the ADHD and Control groups. No significant differences were observed between groups. (<bold>B</bold>) Temporal response functions (TRFs) estimated for each group in the Events condition, based on the average of electrodes FC1, FC2, and FCz, which exhibited the strongest activity for the two main components. Shaded areas represent the SEM. Below the TRFs, topographies of the main component are presented. (<bold>C</bold>) Bar graphs showing the average reconstruction correlation of the decoding model (Pearsonâs <italic>r</italic>) in each group. The ADHD group showed slightly lower reconstruction correlations for the teacherâs speech, though the effect did not reach statistical significance (p = 0.057). Error bars represent the SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Comparison between univariate and multivariate temporal response function (TRF) model.</title><p>In the main manuscript, we report the TRF estimated to capture the response to the teacherâs speech using a multivariate model, that takes into account both the acoustic envelope of the speech as well as the sound-events. Here, we compared TRF estimation using this multivariate encoding model versus a univariate encoding model comprised only the speech-envelope as a single regressor. (<bold>A</bold>) Grand-average TRFs (across both groups) reflecting the neural tracking response of the teacherâs speech, estimated for the Events condition using both a univariate and multivariate model, as well as for the Quiet condition (univariate model). Shading represent the standard error of the mean (SEM). The estimated TRFs to the speech were qualitatively similar using both approaches, and were also similar to those estimated in the Quiet condition, which did not contain additional event-sound, lending additional credibility to the approach. (<bold>B</bold>) Grand-average TRF estimated for the regressor representing the audio of the event-sounds, in the multivariate model of the Events condition. Shaded areas represent the SEM. This analysis is akin to calculating the event-related potential (ERP) to event-sounds (given the linear nature of the model), and the resulting TRF is qualitatively similar to the ERPs reported in <xref ref-type="fig" rid="fig6">Figure 6</xref>. (<bold>C</bold>) Topographical distribution (top) and bar graph (bottom) depicting the predictive power values (Pearsonâs <italic>r</italic>) in the Events condition for the univariate and multivariate encoding models. When comparing the predictive power of the univariate and multivariate models, both models yielded significant predictive power relative to null-permutations, however the multivariate model was able to explain a larger degree of the variance in the neural response [<italic>t</italic>(48) = 10.354, p &lt; 0.0001, Cohenâs <italic>d</italic> = 1.268, Bayes Factor (BF<sub>10</sub>) = +100 (Extreme evidence for H1)]. These results indicate that including a regressor that captures the time-course of the sound-events in addition to a regressor capturing the speech stimulus allowed the model to more faithfully capture the neural representation of the entire virtual reality (VR) classroom soundscape (<xref ref-type="bibr" rid="bib25">Crosse et al., 2021</xref>). Therefore, this approach is preferable when analyzing neural response to stimuli embedded in realistically noisy contexts. That said, the spatio-temporal TRF itself can be reliably extracted for the speech stimulus using both a univariate and a multivariate model.***p &lt; 0.0001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig2-figsupp1-v1.tif"/></fig></fig-group><p>No significant between-group differences were found in the predictive power of the encoding model (averaged across electrodes), and Bayes analysis indicates moderate support for the null hypothesis [<italic>t</italic>(47) = 0.23, p = 0.814, Cohenâs <italic>d</italic> = 0.067, BF<sub>10</sub> = 0.15]. However, when using a decoding model, which considers all electrodes in one model, we did find slightly lower reconstruction correlations for the teacherâs speech in the AD(H)D group relative to controls [<italic>t</italic>(47) = â1.948, p = 0.057, Cohenâs <italic>d</italic> = â0.557, BF<sub>10</sub> = 1.308 (weak/moderate support for H1); <xref ref-type="fig" rid="fig2">Figure 2C</xref>].</p></sec><sec id="s2-1-3"><title>Looking at the teacher</title><p>Analysis of gaze-patterns showed that, as expected, participants spent most of the time looking at the teacher (60.2 Â± 20.9% SEM of each trial). However, we also note the large variability across participants, with some focusing almost exclusively on the teacher (near 100%) and others spending less than 40% of the trial looking at the teacher. When not looking at the teacher, the next most popular places to look at were the blackboards behind the teacher, to the right and to the left (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We tested whether the amount of time spent looking at the teacher and the number of gaze-shifts away from the teacher were modulated by Group (AD(H)D vs. Control) separately in each Condition (Quiet and Events). No significant differences were found, however BF analysis indicated that the null hypotheses was not strongly supported [<italic>Percent gaze-time at teacher:</italic> Events Condition: <italic>t</italic>(47) = â0.899, p = 0.372, Cohenâs <italic>d</italic> = â0.257, BF<sub>10</sub> = 0.397 (weak support for H0); Quiet Condition: <italic>t</italic>(47) = â0.597, p = 0.553, Cohenâs <italic>d</italic> = â0.170, BF<sub>10</sub> = 0.33 (weak support for H0); <italic>Number of gaze-shifts away from teacher</italic>: Events Condition: <italic>t</italic>(47) = 1.265, p = 0.211, Cohenâs <italic>d</italic> = 0.361, BF<sub>10</sub> = 0.547 (weak support for H0); Quiet Condition: <italic>t</italic>(47) = 0.644, p = 0.522, Cohenâs <italic>d</italic> = 0.184, BF<sub>10</sub> = 0.338 (weak support for H0); <xref ref-type="fig" rid="fig3">Figure 3C</xref>].</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Eye-gaze results.</title><p>(<bold>A</bold>) Pie chart representing the average amount of time that participants spent looking at different areas within the virtual reality (VR) classroom. (<bold>B</bold>) Distribution of the proportion of gaze-time toward the teacher (top) and number of gaze-shifts performed away from the teacher (bottom), for all participants in the attention deficit (hyperactivity) disorder (AD(H)D) and Control groups. (<bold>C</bold>) Bar graph represents the average number of gaze-shifts performed away from the teacher, shown separately for the AD(H)D and Control groups, and for the Quiet and Events conditions. No significant differences were found in any comparison (n.s).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig3-v1.tif"/></fig></sec></sec><sec id="s2-2"><title>Responses to irrelevant sound-events</title><p>Next, we tested whether neurophysiological responses to the event-sounds themselves differed between groups. The three event-related metrics tested here were: neural ERPs, transient changes in SC, and gaze-shifts following event-sounds. In addition, we compared the response to the two different event-types presented here (Artificial vs. Non-verbal human sounds).</p><sec id="s2-2-1"><title>Neural ERPs</title><p>Visual inspection of the neural ERPs showed three prominent centro-parietal components â a negative peak around 100 ms, which corresponds to the early sensory N1; followed by two positive peaks around 240 and 350 ms, which likely corresponds to the P2 and P3 responses, respectively (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Event-related potentials (ERPs) in response to sound-events.</title><p>(<bold>A</bold>) Grand-average ERP to sound-events, shown separately for the attention deficit (hyperactivity) disorder (AD(H)D) versus Control groups. ERPs are averaged across the cluster of electrodes where significant differences were found (see panel B). The black horizontal line indicates the time-window where significant differences between groups were found (75â155 ms). Shaded areas around the waveforms represent the SEM. (<bold>B</bold>) Scalp topographies of the N1 and P2 responses in the AD(H)D and Control groups, and the difference between them. Electrodes where significant differences between groups were found are marked in white (p &lt; 0.05, cluster corrected). (<bold>C</bold>) Grand-average ERP to Artificial and Non-verbal human event-sound. ERPs are averaged across the cluster of electrodes where significant differences were found (see panel D). The black horizontal lines indicate the time-windows where significant differences between ERP to the two types of sound-events were found (67â178 and 187â291 ms). Shaded areas around waveforms represent the SEM. (<bold>D</bold>) Scalp topographies of the N1 and P2 responses to Artificial and Non-verbal human sounds and the difference between them. Electrode where significant differences were found are marked in white (p &lt; 0.05, cluster corrected). (<bold>E, F, G</bold>) Box plots depicting the average N1, P2 and P3 responses, separately for each group (AD(H)D vs. Control) and Event-type (Artificial vs. Non-verbal human). **p &lt; 0.001; *p &lt; 0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig4-v1.tif"/></fig><p>Statistical analysis comparing ERPs in the AD(H)D vs. Control groups (collapsed across both Event-types) showed that the early N1 response was significantly larger in the AD(H)D group (p &lt; 0.05, cluster correction; <xref ref-type="fig" rid="fig4">Figure 4A, B</xref>). However, no significant differences between groups were found for the later P2 response. When comparing ERPs to Artificial vs. Non-verbal human sounds we found that both the N1 and the P2 were significantly modulated by Event-type, with a larger N1 for the Artificial sounds and a larger P2 for the non-verbal human sounds (both p &lt; 0.002, cluster correction; <xref ref-type="fig" rid="fig4">Figure 4C, D</xref>). A mixed ANOVA performed on the amplitudes of each component confirmed these main effects, but did not reveal any significant interaction between Group and Event-type [N1: <italic>Group</italic> [<italic>F</italic>(1, 47) = 6.15, p = 0.044, Î·<sup>2</sup> = 0.11], <italic>Event-type</italic> [<italic>F</italic>(1, 47) = 63.82, p &lt; 10<sup>â9</sup>, <italic>Î·</italic><sup>2</sup> = 0.57], <italic>interaction</italic> [<italic>F</italic>(1, 47) = 0.389, p = 0.24, Î·<sup>2</sup> = 0.008]; P2: <italic>Group</italic> [<italic>F</italic>(1, 47) = 0.84, p = 0.36, <italic>Î·</italic><sup>2</sup> = 0.017], <italic>Event-type</italic> [<italic>F</italic>(1, 47) = 71.75, p &lt; 10<sup>â9</sup>, <italic>Î·</italic><sup>2</sup> = 0.6], <italic>interaction</italic> [<italic>F</italic>(1, 47) = 0.07, p = 0.78, <italic>Î·</italic><sup>2</sup> = 0.001]; P3: <italic>Group</italic> [<italic>F</italic>(1, 47) = 0.028, p = 0.876, <italic>Î·</italic><sup>2</sup> = 0.0005], <italic>Event-type</italic> [<italic>F</italic>(1, 47) = 0.184, p = 0.669, <italic>Î·</italic><sup>2</sup> = 0.003], <italic>interaction</italic> [<italic>F</italic>(1, 47) = 0.166, p = 0.684, <italic>Î·</italic><sup>2</sup> = 0.003]; <xref ref-type="fig" rid="fig4">Figure 4</xref>].</p></sec><sec id="s2-2-2"><title>Event-related increase in SC</title><p>Sound-events elicited clear event-related changes in the phasic SC response, which peaked 2â3 s after the sounds, and took another ~1 s to return to baseline (<xref ref-type="fig" rid="fig5">Figure 5A, B</xref>). This response is consistent with the well-documented orienting reflex, elicited following salient events.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Event-related changes in skin conductance (SC).</title><p>Time-course of event-related changes in phasic SC following the sound-events, shown separately for each group (<bold>A</bold>) and for the two sound-events types (<bold>B</bold>). Shaded areas around waveforms represent the SEM. Horizontal line represents the time-windows where significant differences were found. (<bold>C</bold>) Average levels of event-related changes in phasic SC (peak between 2 and 3 s) shown for each group and for Artificial vs. Non-verbal human sound-events. Error bars represent the SEM. **p &lt; 0.01.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig5-v1.tif"/></fig><p>A mixed ANOVA showed a significant main effects of Event-type, with a higher response following artificial sounds relative to non-verbal human sounds [<italic>F</italic>(1, 47) = 7.88, p = 0.007, <italic>Î·</italic><sup>2</sup> = 0.143], however there was no significant main effect of Group [<italic>F</italic>(1, 47) = 0.4, p = 0.53, <italic>Î·</italic><sup>2</sup> = 0.008] or interaction between them [<italic>F</italic>(1, 47) = 0.02, p = 0.87, <italic>Î·</italic><sup>2</sup> = 0.0005; <xref ref-type="fig" rid="fig5">Figure 5B, C</xref>].</p></sec><sec id="s2-2-3"><title>Event-related gaze-shifts</title><p>We tested whether the sound-events triggered overt gaze-shifts away from the teacher, by quantifying the percentage of event-sound that were followed by at least one gaze-shift, and comparing this to control epochs taken from the Quiet condition. However, no significant differences were found, suggesting that the sound-events did not elicit more frequent overt gaze-shifts away from the teacher, relative to what might be observed when no sound-events are present [<italic>t</italic>(48) = 1.34, p = 0.18; <xref ref-type="fig" rid="fig6">Figure 6A</xref>]. We also tested whether the likelihood of performing event-related gaze-shifts was different for the two Event-types (Artificial vs. Non-verbal human sounds) or in the two Groups (AD(H)D vs. Control) but did not find any significant main effects or interactions [<italic>Group</italic>: <italic>F</italic>(47) = 0.18, p = 0.67, <italic>Î·</italic><sup>2</sup> = 0.003; <italic>Condition</italic>: <italic>F</italic>(47) = 2.25, p = 0.14, <italic>Î·</italic><sup>2</sup> = 0.045; <italic>Interaction</italic>: <italic>F</italic>(47) = 0.51, p = 0.47, <italic>Î·</italic><sup>2</sup> = 0.01; <xref ref-type="fig" rid="fig6">Figure 6B</xref>].</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Event-related gaze-shifts.</title><p>Attention deficit (hyperactivity) disorder (AD(H)D). (<bold>A</bold>) Bar graph showing the number of gaze-shifts performed in 2-s epochs following event-sounds versus control epochs, averaged across groups and sound types. No significant differences were found, suggesting that event-sounds were not more likely to elicit overt gaze-shifts. (<bold>B</bold>) Bar graph showing the number of gaze-shifts performed in 2-s epochs following each type of sound-event, separately for each group. No significant differences were found in any comparison. Error bars represent the SEM in all bar graphs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig6-v1.tif"/></fig></sec></sec><sec id="s2-3"><title>Neurophysiological metrics associated with overall attention and arousal</title><p>Besides analyzing neurophysiological metrics that can be directly associated with processing the teacherâs speech or with response to the sound-events, several additional neurophysiological measures have been associated more broadly with the listenersâ state of attention or arousal. These including spectral properties of the EEG (and specifically power in the alpha and beta range), the overall level of tonic and phasic SC, and spontaneous gaze-patterns.</p><p>We tested whether these metrics differed between the two groups, separately in the Quiet and Events conditions.</p><sec id="s2-3-1"><title>Spectral EEG features</title><p>Spectral analysis of the EEG focused on the two frequency bands for which clear peaks were detected in the periodic power-spectrum, after applying the FOOOF algorithm: alpha- (8â12 Hz) and beta-power (18â25 Hz; <xref ref-type="fig" rid="fig7">Figure 7A</xref>). Power in each frequency band was assessed for each participant at their personal peak within a pre-selected cluster of electrodes where each response was maximal (<xref ref-type="fig" rid="fig7">Figure 7</xref>). We tested whether power in either frequency differed significantly between Groups, using unpaired <italic>t</italic>-tests, separately for each Condition (Quiet and Event trials). However, none of the comparisons revealed any significant differences between groups, in either frequency band (<xref ref-type="fig" rid="fig7">Figure 7C</xref>) [<italic>Alpha-power</italic>: Events Condition: <italic>t</italic>(47) = â0.656, p = 0.514, Cohenâs <italic>d</italic> = â0.18, BF<sub>10</sub>=0.34 (weak evidence for H0); Quiet Condition: <italic>t</italic>(47) = â0.394, p = 0.695, Cohenâs <italic>d</italic> = â0.11, BF<sub>10</sub> = 0.30 (moderate evidence for H0); <italic>Beta-power:</italic> Events Condition: <italic>t</italic>(47) = â1.01, p = 0.315, Cohenâs <italic>d</italic> = â0.29, BF<sub>10</sub> = 0.43 (weak evidence for H0); Quiet Condition: <italic>t</italic>(47) = â0.484, p = 0.630, Cohenâs <italic>d</italic> = â0.14, BF<sub>10</sub> = 0.314 (moderate evidence for H0)].</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Spectral electroencephalography (EEG) and skin conductance analysis.</title><p>(<bold>A</bold>) Grand-average power spectral density (PSD) of the periodic portion of the EEG signal (after applying the FOOOF algorithm), shows two clear peaks corresponding to the alpha (8â12 Hz) and beta (15â25 Hz) bands. Shaded areas around waveforms represent the SEM. (<bold>B</bold>) Topographical distribution of the average alpha- and beta-power peaks, with the clusters of electrodes used to detect personal peaks in each frequency band marked by white circles. (<bold>C</bold>) Average alpha- and beta-power in Group (attention deficit (hyperactivity) disorder [AD(H)D] vs. Control) and condition (Quiet and Events). (<bold>D</bold>) Phasic and (<bold>E</bold>) tonic skin conductance levels in the same groups and conditions. No significant between-group difference was found in any comparison (n.s.). Bar graphs represent the mean values, and error bars represent the SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig7-v1.tif"/></fig></sec><sec id="s2-3-2"><title>Global SC levels</title><p>No significant differences between the two Groups were observed for the global tonic or phasic SC metrics, in either the Events or Quiet condition [<bold><italic>Phasic SC</italic></bold> (<xref ref-type="fig" rid="fig7">Figure 7D, E</xref>): Events Condition: <italic>t</italic>(47) = â0.003, p = 0.99, Cohenâs <italic>d</italic> = â0.0009, BF<sub>10</sub> = 0.285 (moderate evidence for H0); Quiet Condition: <italic>t</italic>(47) = â0.51, p = 0.61, Cohenâs <italic>d</italic> = â0.146, BF<sub>10</sub> = 0.317 (moderate evidence for H0); <bold><italic>Tonic SC</italic></bold>: Events Condition: <italic>t</italic>(47) = â0.85, p = 0.398, Cohenâs <italic>d</italic> = â0.244, BF<sub>10</sub> = 0.383 (weak evidence for H0); Quiet Condition: <italic>t</italic>(47) = â1.65, p = 0.104, Cohenâs <italic>d</italic> = â0.476, BF<sub>10</sub> = 0.865 (weak evidence for H0)].</p></sec></sec><sec id="s2-4"><title>Multivariate analyses</title><p>The univariate analyses testing for between-group differences separately for each metric were complemented with a multivariate regression analysis, aimed at understanding the relative contribution of each neurophysiological metric to explaining between-group effects. For brevity, we focused on 10 key measures as described in the Methods section.</p><p>The pairwise Spearmanâs correlations between all measures are shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>. None of the correlations were significant after correcting for multiple comparisons (fdr-correction), however three pairs did reach significance before correction (Speech-Decoding vs. ERP-P2: <italic>r</italic> = â0.33, p = 0.021; TRF-N1 vs. event-related SC: <italic>r</italic> = â0.3, p = 0.033; ERP-N1 vs. event-related SC: <italic>r</italic> = 0.29, p = 0.04). Given these generally weak correlations, we consider these measures to be independent for the purpose of multiple regression analysis.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Correlation matrix.</title><p>Heatmap of the pairwise Spearmanâs correlation coefficient between the different neurophysiological measures included in the multivariate analysis. Red-shading indicates negative correlation values and blue-shading indicate positive values. Asterisks* indicate correlation values that pass a non-corrected threshold for statistical significance, however none survived fdr-correction for multiple comparisons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig8-v1.tif"/></fig><p>To test the contribution of each measure for predicting whether an individual was in the ADHD or control group we performed a logistic multiple regression analysis. An âomnibusâ model containing all 10 measures achieved <italic>Ï</italic><sup>2</sup> = 15.6 with an AUC of 0.807 for predicting group allocation. We assessed the relative contribution (dominance) of each measure by comparing the AUC of the omnibus model to 10 models with a single measure held-out. <xref ref-type="table" rid="table1">Table 1</xref> shows ÎAUC contributed by each measure, indicating that the two most dominant measures were the ERP-N1 (positive contribution) and Speech Decoding (negative contribution), which were also the only two measures whose addition to the model reduced the AIC. This result is consistent with our univariate results, and indicates that these two measures contribute independently to between-group differences. Together, these two measures contributed an AUC of 0.15 to the omnibus model, which corresponds to half of the modelâs fit (Î<italic>Ï</italic><sup>2</sup> = 11.8, p = 0.003 relative to a model with both measures held-out).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Result of the dominance analysis of a multivariate logistic regression, describing the contribution of each measure for predicting whether an individual was in the ADHD or control group.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Factor</th><th align="left" valign="bottom">Dominance (ÎAUC)</th><th align="left" valign="bottom">ÎAIC</th></tr></thead><tbody><tr><td align="left" valign="bottom">ERP-N1<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="left" valign="bottom">0.054</td><td align="left" valign="bottom">â5.13</td></tr><tr><td align="left" valign="bottom">Speech Decoding<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="left" valign="bottom">0.05</td><td align="left" valign="bottom">â3.29</td></tr><tr><td align="left" valign="bottom">TRF-P2</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">+0.18</td></tr><tr><td align="left" valign="bottom">Beta-power</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">+1.57</td></tr><tr><td align="left" valign="bottom">ERP-P2</td><td align="left" valign="bottom">0.005</td><td align="left" valign="bottom">+0.48</td></tr><tr><td align="left" valign="bottom">Event-related SC</td><td align="left" valign="bottom">â0.005</td><td align="left" valign="bottom">+1.69</td></tr><tr><td align="left" valign="bottom">TRF-N1</td><td align="left" valign="bottom">0.009</td><td align="left" valign="bottom">+1.42</td></tr><tr><td align="left" valign="bottom">Gaze-shifts</td><td align="left" valign="bottom">0.007</td><td align="left" valign="bottom">+1.64</td></tr><tr><td align="left" valign="bottom">Alpha-power</td><td align="left" valign="bottom">0.007</td><td align="left" valign="bottom">+1.6</td></tr><tr><td align="left" valign="bottom">Behavior</td><td align="left" valign="bottom">0.007</td><td align="left" valign="bottom">+1.6</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>indicate the factors that significantly reduced the modelâs AIC (p&lt;0.05).</p></fn></table-wrap-foot></table-wrap><p>Since ADHD is arguably not a binary category but symptoms of attentional difficulties vary on a continuum, we complemented the logistic regression analysis with a multivariate linear regression analysis, using ASRS scores as a continuous dependent measure, reflecting the severity of ADHD symptoms. An âomnibusâ model containing all 10 achieved <italic>F</italic>(10, 38) = 1.48 (p = 0.18) with an <italic>R</italic><sup>2</sup> of 0.28 for predicting ASRS scores. We assessed the relative contribution (dominance) of each measure by comparing the <italic>R</italic><sup>2</sup> in the omnibus model (containing all 10 measures) to 10 models with a single measure held-out. <xref ref-type="table" rid="table2">Table 2</xref> shows Î<italic>R</italic><sup>2</sup> and ÎAIC contributed by each measure (smaller values are better). Interestingly, this analysis identified slightly different factors than the logistic regression analysis where individuals were grouped based on having a prior diagnosis of AD(H)D (<xref ref-type="table" rid="table2">Table 2</xref>). The most dominant measures, who contributed at least Î<italic>R</italic><sup>2</sup> &gt; 0.3 the model, were the ERP-N1 (<italic>Î²</italic> = 0.3), Gaze-shifts (<italic>Î²</italic> = 0.288), Alpha-power (<italic>Î²</italic> = 0.2) and ERP-P2 (<italic>Î²</italic> = â0.182). Although the omnibus model was not significant (see above), a multivariate regression model that contained only the four most dominant measures did significantly predicted ASRS scores (<italic>R</italic><sup>2</sup> = 0.227, p = 0.021), and collectively contributed Î<italic>R</italic><sup>2</sup> = 0.225 (p = 0.032) to the omnibus model.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Result of the dominance analysis of a multivariate linear regression, describing the contribution of each measure for predicting individual ASRS scores.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Factor</th><th align="left" valign="bottom">Î<italic>R</italic><sup>2</sup></th><th align="left" valign="bottom">ÎAIC</th></tr></thead><tbody><tr><td align="left" valign="bottom">ERP-N1</td><td align="left" valign="bottom">0.087</td><td align="left" valign="bottom">â3.58</td></tr><tr><td align="left" valign="bottom">Gaze-shifts</td><td align="left" valign="bottom">0.065</td><td align="left" valign="bottom">â2.23</td></tr><tr><td align="left" valign="bottom">Alpha-power</td><td align="left" valign="bottom">0.038</td><td align="left" valign="bottom">â0.49</td></tr><tr><td align="left" valign="bottom">ERP-P2</td><td align="left" valign="bottom">0.033</td><td align="left" valign="bottom">â0.22</td></tr><tr><td align="left" valign="bottom">Behavior</td><td align="left" valign="bottom">0.028</td><td align="left" valign="bottom">â0.14</td></tr><tr><td align="left" valign="bottom">Speech Decoding</td><td align="left" valign="bottom">0.021</td><td align="left" valign="bottom">+0.58</td></tr><tr><td align="left" valign="bottom">TRF-N1</td><td align="left" valign="bottom">0.006</td><td align="left" valign="bottom">+1.58</td></tr><tr><td align="left" valign="bottom">Event-related SC</td><td align="left" valign="bottom">0.001</td><td align="left" valign="bottom">+1.91</td></tr><tr><td align="left" valign="bottom">TRF-P2</td><td align="left" valign="bottom">0.008</td><td align="left" valign="bottom">+1.18</td></tr><tr><td align="left" valign="bottom">Beta-power</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">+1.97</td></tr></tbody></table></table-wrap></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Recent years have seen a reckoning in the field of AD(H)D research, acknowledging that most current tools used for clinical assessments and research do not adequately capture the attentional challenges faced in real life (<xref ref-type="bibr" rid="bib42">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Barkley, 2019</xref>; <xref ref-type="bibr" rid="bib77">Mulraney et al., 2022</xref>; <xref ref-type="bibr" rid="bib5">Arrondo et al., 2024</xref>; <xref ref-type="bibr" rid="bib100">Schweitzer and Zion Golumbic, 2024</xref>). We offer a new approach for advancing attention research, by utilizing VR to simulate real-life experiences together with comprehensive measurement of different neurophysiological responses. This combination affords a more objective and well-rounded description of how individuals deal with the plethora of stimuli and task demands of their environments. Here, we focused on VR classroom learning, since this is the context where many individuals, and particularly those with AD(H)D, experience difficulties in sustaining attention and avoiding distraction. Specifically, we evaluated neurophysiological measures related to focusing attention on the teacher and responses to irrelevant background sound-events alongside metrics associated more generally with levels of attention/engagement. We found that individuals diagnosed with AD(H)D exhibited significantly larger N1 response to background sound-events and somewhat reduced neural tracking of the teacherâs speech. These were also the two most dominant factors contributing to a multivariate classifier that could significantly predict whether an individual was diagnosed with AD(H)D. When considering the severity of AD(H)D symptoms as a continuous variable, variance across individuals was best explained by the combined contribution of the N1 and P2 response to sound-events, amplitude of alpha-power oscillations and the frequency of gaze-shifts away from the teacher. Together, these findings emphasize the variety of attentional challenges that individuals with AD(H)D may experience in realistic scenarios and provide novel insights into the underlying neurophysiological mechanisms.</p><sec id="s3-1"><title>Neurophysiological responses in the VR classroom associated with symptoms of AD(H)D</title><p>The finding with the largest effect size in this study was larger N1 response to background sound-events in individuals with AD(H)D. This response showed a significant univariate between-group effect, and was the most dominant contributor to predicting group-classification (AD(H)D vs. control) as well as the severity of AD(H)D symptoms. Enhanced N1 responses are often associated with heightened sensory sensitivity (<xref ref-type="bibr" rid="bib45">Heil, 1997</xref>; <xref ref-type="bibr" rid="bib64">LÃ¼tkenhÃ¶ner and Klein, 2007</xref>), perceptual predictability (<xref ref-type="bibr" rid="bib21">Chennu et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Lange, 2013</xref>; <xref ref-type="bibr" rid="bib98">Schwartze and Kotz, 2015</xref>), and may reflect a transient detector mechanism of surprising events that can potentially (though not necessarily) trigger an attention shift (<xref ref-type="bibr" rid="bib79">NÃ¤Ã¤tÃ¤nen and Picton, 1987</xref>; <xref ref-type="bibr" rid="bib31">Escera et al., 1998</xref>; <xref ref-type="bibr" rid="bib32">Escera et al., 2003</xref>; <xref ref-type="bibr" rid="bib14">Berti, 2013</xref>). Interestingly, most studies comparing auditory ERPs in individuals with and without AD(H)D, using more traditional paradigms (e.g., oddball), typically <italic>have not reported</italic> differences in early-sensory components such as the N1, but rather sometimes find group differences for later ERP responses (<xref ref-type="bibr" rid="bib9">Barry et al., 2003</xref>; <xref ref-type="bibr" rid="bib41">Gumenyuk et al., 2005</xref>; <xref ref-type="bibr" rid="bib54">Kaiser et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Peisch et al., 2021</xref>). As discussed below, we attribute these discrepancies to the non-ecological nature of most traditional paradigm. The current result is in line with increased sensory sensitivity to background events, which might be especially pertinent under ecological condition. In addition, we found that neural tracking of the teacherâs speech was reduced in the AD(H)D group, and this metric also emerged as a dominant predictor of AD(H)D diagnosis in multivariate between-group classification. Neural speech tracking of continuous speech is known to be modulated by attention, and is reduced when speech is not attended (<xref ref-type="bibr" rid="bib27">Ding and Simon, 2012b</xref>; <xref ref-type="bibr" rid="bib73">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="bib120">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">Petersen et al., 2017</xref>; <xref ref-type="bibr" rid="bib114">Vanthornhout et al., 2019</xref>; <xref ref-type="bibr" rid="bib55">Kaufman and ZionâGolumbic, 2023</xref>). We are not aware of previous studies that directly investigated neural speech tracking in AD(H)D, however these results are consistent with reduced levels of sustained attention toward the teacher in this group.</p><p>Reduced sustained attention and higher proneness to distraction are, of course, considered key characteristics of AD(H)D, however are rarely demonstrated under realistic conditions, using ecological tasks and materials. The current results demonstrate how, by coupling the ecological validity and flexibility of VR with recordings of neural activity, we can substantially advance our mechanistic understanding of attention and distractibility and obtain objective metrics of stimulus processing in realistic scenarios. Interestingly, since the magnitude of the N1 to sound-events and speech tracking of the teacher were not correlated with each other, these results do not support the notion of an inherent âtradeoffâ between paying attention to the teacher and responding to irrelevant stimuli. Rather, they may represent two independent ways in which individuals with AD(H)D differ from their peers in the manner they respond to and process stimuli in dynamic environments.</p><p>However, adopting a binary perspective of AD(H)D can be misleading, as it does not capture the vast variability within groups, as emphasized by the RDoC framework (<xref ref-type="bibr" rid="bib76">Morris et al., 2022</xref>). This is evident from inspecting the distribution of ASRS scores in the current sample, which shows that although the two groups are clearly separable from each other, they are far from uniform in the severity of symptoms experienced. For this reason, we conducted a continuous multiple regression analysis, to identify which neurophysiological measured explained variance in self-reported AD(H)D symptoms. This analysis confirmed the link between the magnitude of the N1 response to sound-events to AD(H)D symptoms, but also identified several additional factors that contribute to explaining variance in ASRS scores across individuals, primarily: the level of alpha-power and the frequency of gaze-shifts away from the teacher. This finding is consistent with the hypothesized role of these metrics in attention. Higher alpha-power is associated with reduced levels of attention/arousal, and is consistently found to increase in conditions of boredom, tiredness or prolonged time on tasks (<xref ref-type="bibr" rid="bib22">Clarke et al., 2001</xref>; <xref ref-type="bibr" rid="bib28">Dockree et al., 2007</xref>; <xref ref-type="bibr" rid="bib83">Palva and Palva, 2007</xref>; <xref ref-type="bibr" rid="bib117">WÃ¶stmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Boudewyn and Carter, 2018</xref>; <xref ref-type="bibr" rid="bib52">Jin et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Haro et al., 2022</xref>). Several studies have also found higher baseline alpha-power in individuals with AD(H)D versus controls (<xref ref-type="bibr" rid="bib9">Barry et al., 2003</xref>; <xref ref-type="bibr" rid="bib53">Johnstone et al., 2013</xref>; <xref ref-type="bibr" rid="bib17">Bozhilova et al., 2022</xref>; <xref ref-type="bibr" rid="bib75">Michelini et al., 2022</xref>), although results are not always consistent (<xref ref-type="bibr" rid="bib63">Loo and Makeig, 2012</xref>; <xref ref-type="bibr" rid="bib53">Johnstone et al., 2013</xref>). Similarly, gaze-shifts are often used as a proxy for distraction, particularly under naturalistic conditions (<xref ref-type="bibr" rid="bib69">Marius ât Hart et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">Foulsham et al., 2011</xref>; <xref ref-type="bibr" rid="bib93">Risko and Kingstone, 2011</xref>; <xref ref-type="bibr" rid="bib94">Risko et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Hoppe et al., 2018</xref>), and recent VR and real-life eye-tracking studies have found increased frequency and prolonged duration of gaze-shifts toward irrelevant locations in individuals with AD(H)D (<xref ref-type="bibr" rid="bib18">Braga et al., 2016</xref>; <xref ref-type="bibr" rid="bib111">TÃ¼rkan et al., 2016</xref>; <xref ref-type="bibr" rid="bib113">Vakil et al., 2019</xref>; <xref ref-type="bibr" rid="bib71">Mauriello et al., 2022</xref>; <xref ref-type="bibr" rid="bib106">Stokes et al., 2022</xref>). The current results extend these findings to more ecological contexts and indicate that individuals who exhibited higher alpha-power, performed more frequent gaze-shifts and had larger N1 response to background sound-events in our VR classroom were more likely to report experiencing attentional difficulties in real life.</p><p>It is important to note that although the current results had significant explanatory power of variance in AD(H)D symptoms, they are far from exhaustive with substantial variance still unexplained by these neurophysiological measures. In this regard it is important to bear in mind that the measures of AD(H)D themselves (diagnosis and ASRS scores) are also not entirely reliable, given the diversity diagnostic practice (<xref ref-type="bibr" rid="bib42">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Barkley, 2019</xref>; <xref ref-type="bibr" rid="bib77">Mulraney et al., 2022</xref>; <xref ref-type="bibr" rid="bib5">Arrondo et al., 2024</xref>; <xref ref-type="bibr" rid="bib100">Schweitzer and Zion Golumbic, 2024</xref>) and the subjective nature of self-reports which are prone to bias (e.g., recency, salience, and generalization effects; <xref ref-type="bibr" rid="bib91">Rabiner et al., 2010</xref>; <xref ref-type="bibr" rid="bib110">Toplak et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Brevik et al., 2020</xref>). What is clear, though, is that no single neurophysiological measure alone is sufficient for explaining differences between the individuals â whether through the lens of clinical diagnosis or through report of symptoms. These data emphasize the complex nature of attentional functioning, especially under realistic conditions, and that fact that âattentional challengesâ may manifest in a variety of different ways, indexed though different neurophysiological responses. Moving forward, we believe that by measuring multiple aspects of neural and physiological responses, and studying these in diverse and ecologically valid conditions, will ultimately lead to distilling specific neurophysiological patterns that can reliably generalize across contexts.</p></sec><sec id="s3-2"><title>What about the P300?</title><p>Notably, the current results diverge from previous studies of distractibility in AD(H)D in several ways. In traditional ERP studies, distractibility by irrelevant stimuli is often associated with a family of late-latency positive neural responses (the P300 family), and specifically the P3a ERP component, which is typically observed at a latency of ~250â400 ms after hearing novel or surprising sounds (<xref ref-type="bibr" rid="bib96">SanMiguel et al., 2010</xref>; <xref ref-type="bibr" rid="bib115">Wetzel et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Masson and Bidet-Caulet, 2019</xref>; <xref ref-type="bibr" rid="bib10">Barry et al., 2020</xref>). Given this vast literature, we had assumed that background sound-events in our VR classroom would also elicit this response. Moreover, past studies have shown reduced P3a responses in individuals with AD(H)D (<xref ref-type="bibr" rid="bib9">Barry et al., 2003</xref>; <xref ref-type="bibr" rid="bib41">Gumenyuk et al., 2005</xref>; <xref ref-type="bibr" rid="bib54">Kaiser et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Peisch et al., 2021</xref>; <xref ref-type="bibr" rid="bib57">Kwasa et al., 2023</xref>), leading us to expect a similar pattern here. However, here the early N1 response to sound-events was larger in the ADHD group but no differences were found for any of later responses. Moreover, visual inspection of the ERPs to Artificial and Non-verbal human sound-events illustrates that not all ecological sounds elicit the same time-course of responses in mid- to late time-windows.</p><p>When comparing the current results from the VR classroom with more traditional ERP studies of distractibility, we must consider the vast differences in context and task requirements and the âfunctional statusâ of background sound-events in these studies. The P3a is typically observed in tasks that require high levels of perceptual vigilance to discriminate between specific stimulus features, such as the pitch, duration or intensity of sounds presented in an oddball paradigm. In these tasks, so-called ânovelâ or âdistractorâ sounds/stimuli are presented as part of the sequence of stimuli that need to be discriminated, and these stimuli elicit a P3a (whereas target sounds elicit a somewhat similar P3b response; <xref ref-type="bibr" rid="bib10">Barry et al., 2020</xref>). Accordingly, although these âdistractorâ stimuli do not require an explicit response, they are in fact within the focus of participantsâ endogenous attention, rather than truly âbackgroundâ stimuli (<xref ref-type="bibr" rid="bib66">Makov et al., 2023</xref>). This differs substantially from the role of the novel sounds-events in the VR classroom task used here, where sound-events are clearly in the backgroundâ both spatially and semantically â and occur in parallel to an ongoing speech stimulus (the lecture) that participants are primarily paying attention to. In addition, the perceptual and cognitive demands of the VR Classroom â where participants are instructed to listen and understand the speech as they would in a real-life classroom â differ substantially from the highly vigilant demands of speeded-response tasks. These important differences in task and stimulus characteristic ultimately lead to a different treatment of novel sound-events by the brain, emphasizing the importance of considering context when comparing results across experimental designs (<xref ref-type="bibr" rid="bib66">Makov et al., 2023</xref>; <xref ref-type="bibr" rid="bib67">Mandal et al., 2024</xref>).</p></sec><sec id="s3-3"><title>Null results â insufficient sensitivity or true lack of differences?</title><p>Besides the lack of a reliable P300-like effect here, several additional measures that were evaluated here did not show any group-wise effects and did not contribute to explaining ASRS scores, despite previous literature suggesting that they might. Although null results are difficult to interpret fully, we find it worthwhile to note these here and briefly discuss potential reasons for the lack of group-related effects. Perhaps the most surprising null effect was the lack of differences in behavioral performance. Participants in both groups achieved similar behavioral performance in the Quiet and Events condition, suggesting that the presence of occasional sound-events ultimately did not ultimately impair their ability to understand the content of the mini-lectures (at least at the level probed by our comprehension questions). These results should be interpreted bearing in mind the details of the current task design â we deliberately chose a task that would mimic the level of information processing required in real-life learning contexts as opposed to speeded-response tasks used in traditional attention research that encourage failures of performance by creating perceptually âextremeâ conditions (<xref ref-type="bibr" rid="bib42">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Barkley, 2019</xref>; <xref ref-type="bibr" rid="bib5">Arrondo et al., 2024</xref>). Moreover, the presence of occasional sound-events â while noticeable, was not design to be extremely adverse. Under these conditions, individuals with and without AD(H)D were able to maintain good performance despite the occasional acoustic disturbances (<xref ref-type="bibr" rid="bib67">Mandal et al., 2024</xref>), indicating that neurophysiological indication of somewhat reduced attention do not necessarily translate to poorer behavioral outcomes. It is of course possible that larger behavioral effects would have emerged if task demands or the severity of acoustic disturbances were more challenging.</p><p>Besides neural evoked responses, background sound-events also elicited a transient increase in SC that is associated with an âorienting reflexâ, reflecting transient changes in arousal/alertness (<xref ref-type="bibr" rid="bib36">Frith and Allen, 1983</xref>; <xref ref-type="bibr" rid="bib119">Zimmer and Richter, 2023</xref>). This response is considered obligatory and automatic, and does not require allocation of attention as it can be observed during sleep and other unconscious states (albeit the magnitude of these responses can vary as a function of attention). Although we had hypothesized that this response might be heightened in individuals with AD(H)D, as has been reported in some studies (<xref ref-type="bibr" rid="bib103">Sergeant, 2000</xref>; <xref ref-type="bibr" rid="bib11">Bellato et al., 2020</xref>) (similar to the effect on the N1 response), the current results do not support this. Indeed, there has been much debate regarding the relationship between the SC orienting reflex and early neural responses, with conflicting results reported across studies (<xref ref-type="bibr" rid="bib60">Lim et al., 1996</xref>; <xref ref-type="bibr" rid="bib65">MacDonald and Barry, 2020</xref>). Additional research is likely required to better characterize the relationship between neural and physiological responses to surprising events in the environment.</p><p>Last, another interesting null result was the lack of overt-gaze-shifts following sound-events. Gaze-shifts are often considered a proxy for distractibility and capture of attention (<xref ref-type="bibr" rid="bib84">Parkhurst et al., 2002</xref>; <xref ref-type="bibr" rid="bib37">Geisler and Cormack, 2011</xref>; <xref ref-type="bibr" rid="bib82">Nissens et al., 2017</xref>), particularly in individuals with AD(H)D. Accordingly, we had expected to find that sound-event elicited overt shifts toward the spatial location from which they were emitted. However, this was not the case, and individuals (in both groups) did not perform more gaze-shifts following sound-events relative to their baseline tendencies. A similar result was recently reported by our group in a VR cafÃ© scenario, where that hearing semantically salient words in background speech (e.g., your name or semantic violations) did elicited neural and SC responses, but was not accompanied by gaze-shifts (<xref ref-type="bibr" rid="bib20">Brown et al., 2023</xref>). These results might indicate that peripheral auditory stimuli are, generally, less effective than visual stimuli at capturing overt attention (<xref ref-type="bibr" rid="bib112">Turoman and Vergauwe, 2024</xref>; <xref ref-type="bibr" rid="bib67">Mandal et al., 2024</xref>), or that in ecological audiovisual settings individuals are less prone to move their eyes to search for the source of auditory disturbances, despite noticing them. These possibilities will be further investigated in follow-up VR studies, comparing the effects of irrelevant auditory and visual events.</p></sec><sec id="s3-4"><title>General considerations</title><p>Before concluding, we note several important caveats, pertaining to the specific sample tested here. First, as noted, classification of the AD(H)D group was based on their prior clinical diagnosis, which likely varied across participants in terms of diagnosis approach and age of diagnosis. Although the two groups were distinguishable in their self-reported ASRS scores, a more reliable assessment of the severity of current AD(H)D symptoms might have been achieved if we had conducted a full clinical assessment of all participants, as per the acceptable clinical guidelines (<xref ref-type="bibr" rid="bib3">American Psychiatric Association, 2022</xref>). Second, this study was conducted on adults, whereas the majority of AD(H)D research â and particularly all VR classroom studies â have focused on children (<xref ref-type="bibr" rid="bib89">Pollak et al., 2009</xref>; <xref ref-type="bibr" rid="bib81">NeguÈ et al., 2017</xref>; <xref ref-type="bibr" rid="bib99">Schweitzer and Rizzo, 2022</xref>; <xref ref-type="bibr" rid="bib101">SeesjÃ¤rvi et al., 2022</xref>; <xref ref-type="bibr" rid="bib106">Stokes et al., 2022</xref>). AD(H)D is primarily defined as a childhood condition and although it often persists into adulthood, with profound implications on professional and academics outcomes, how the nature of the deficit changes with age is still unknown (<xref ref-type="bibr" rid="bib7">Barkley et al., 2008</xref>). Moreover, the adult cohort tested here included primarily university students, who do not represent all adults but rather are a subgroup with relatively high cognitive and learning abilities, who are accustomed to sitting in lectures for long periods of time (<xref ref-type="bibr" rid="bib46">Henrich et al., 2010</xref>). Hence, it is possible that the lack of group differences in some metrics in the current dataset (and particularly in their behavioral outcomes) is because these individuals have learned to deal with distraction and have developed appropriate strategies, which allow them to thrive in academic settings. This caveat highlights the importance of considering sampling techniques in group-comparison studies, which sometimes limits their interpretation to more circumscribed sub-samples (<xref ref-type="bibr" rid="bib92">Rad et al., 2018</xref>). At the same time, even if the current findings pertain only to this highly functioning academic subgroup of individuals with AD(H)D, they imply an impressive ability to adapt and cope with the naturally occurring distractions of realistic classrooms. Specifically, they suggest that withstanding distraction in real-life contexts may be improved through training and good habit-forming, even in individuals with AD(H)D. We hope to follow up on this research in the future, to deepen our understanding of the diversity within the AD(H)D and typical population and to better characterize attention and distractibility across different types of real-life scenarios.</p></sec><sec id="s3-5"><title>Conclusions</title><p>Paying attention in class requires both investing cognitive resources in processing the lesson itself (e.g., the teacherâs speech), and trying to minimize intrusions from background events. Here, we found that both of these operations are less effective in individuals who are diagnosed with AD(H)D or who report more severe symptoms of difficulties in everyday attention. Specifically, these individuals exhibit heightened sensory response to irrelevant sounds, reduced speech tracking of the teacher, a tendency toward more frequent gaze-shifts away from the teacher, and somewhat increased alpha-power. Interestingly, since these metrics were not correlated with each other, they likely represent different ways through which inattention or distraction may manifest, rather than a general âbiomarkerâ of inattention. Counter to some prevalent notions, these data suggest that AD(H)D may not be characterized by a specific âneural markerâ indicating attentional deficits, particularly when tested under ecological circumstances (<xref ref-type="bibr" rid="bib8">Barkley, 2019</xref>; <xref ref-type="bibr" rid="bib34">Faraone et al., 2021</xref>; <xref ref-type="bibr" rid="bib105">Sonuga-Barke et al., 2023</xref>). Rather, they resonate with recent calls to revisit the parameters used for defining âattention-disordersâ and the need to adopt more nuanced and dimensional approach, as well as designing more ecologically valid means of testing, in order to better characterize and understand the complex nature of real-life attentional challenges (<xref ref-type="bibr" rid="bib68">Marcus and Barry, 2011</xref>; <xref ref-type="bibr" rid="bib44">Heidbreder, 2015</xref>; <xref ref-type="bibr" rid="bib76">Morris et al., 2022</xref>; <xref ref-type="bibr" rid="bib30">Elahi et al., 2024</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Fifty-four participants initially participated in the study; however, the data from five participants were excluded due to technical issues or their voluntary request to discontinue participation. The final sample consisted of 49 participants (21 male, 28 female; 46 right-handed, 3 left-handed). Of these participants, 24 had a prior clinical diagnosis of attention deficit hyperactivity disorder (AD(H)D group), and 25 participants did not (control group). The two groups were matched for age, which ranged between 20 and 38 years (mean age of all participants: 26.9 Â± 3.0; AD(H)D participants: 26.7 Â± 3.7; control participants: 25.7 Â± 4.4). The majority of participants were university students. Participants in the AD(H)D group were questioned about their medication intake regimen, and the experiment was scheduled at least 12 hr after their last dose of medication. None of the participants (in either group) reported taking any other medications on a regular basis. The experimental protocol was approved by the ethics committee at Bar-Ilan University (protocol # ISU202112002), and all participants provided written informed consent prior to their involvement in the study and data collection procedures. Participants received financial compensation or course credit in exchange for their participation.</p></sec><sec id="s4-2"><title>ASRS questionnaire</title><p>All participants completed a self-report questionnaire assessing AD(H)D symptoms (ASRS-v1.1; <xref ref-type="bibr" rid="bib1">Adler et al., 2006</xref>, Hebrew version: <xref ref-type="bibr" rid="bib121">Zohar and Konfortes, 2010</xref>), which has been shown to have good psychometric properties high sensitivity for AD(H)D diagnosis (<xref ref-type="bibr" rid="bib56">Kessler et al., 2005</xref>). Participants in the AD(H)D group were instructed to complete the questionnaire based on their experiences at a time when they are <underline>not</underline> taking medication. The questionnaire consists of 18 items, which are rated on a 5-point Likert scale ranging from 0 (never) to 4 (very often). ASRS scores for each participant were calculated as the total sum of marks across all items, as this has been shown to yield the highest sensitivity to AD(H)D diagnosis (<xref ref-type="bibr" rid="bib121">Zohar and Konfortes, 2010</xref>). We did not try to categorize AD(H)D subtypes (e.g., inattention, hyperactivity, and combined) since these subtypes are less reliable in adulthood. As expected, the group diagnosed with AD(H)D exhibited significantly higher ASRS scores compared to the control group [<italic>t</italic>(47) = 8.49, p &lt; 0.001; see <xref ref-type="fig" rid="fig9">Figure 9</xref>]. This outcome validates the group allocation and demonstrates the consistency between AD(H)D diagnosis and self-reported AD(H)D symptoms.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Histogram (<bold>A</bold>) and bar graph (<bold>B</bold>) showing the distribution of ASRS scores in the attention deficit (hyperactivity) disorder (AD(H)D) and Control groups.</title><p>Error bars represent standard error of the mean (SEM). This confirms group allocation, within the tested sample.***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig9-v1.tif"/></fig></sec><sec id="s4-3"><title>The VR classroom: design and stimuli</title><p>The VR classroom platform was developed using the Unity game engine (<ext-link ext-link-type="uri" xlink:href="https://unity.com/">unity.com</ext-link>; JavaScript and C# programming). Development and programming of the VR classroom were done primarily in-house, using assets (avatars and environment) were sourced from pre-existing databases. The classroom environment was adapted from assets provided by Tirgames on TurboSquid (<ext-link ext-link-type="uri" xlink:href="https://www.turbosquid.com/Search/Artists/Tirgames">https://www.turbosquid.com/Search/Artists/Tirgames</ext-link>) and modified to meet the experimental needs. The avatars and their basic animations were sourced from the Mixamo library, which at the time of development supported legacy avatars with facial blendshapes (this functionality is no longer available in current versions of Mixamo). A brief video example of the VR classroom is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/svjqg">https://osf.io/svjqg</ext-link>.</p><p>Participants experienced the virtual environment through an HTC Vive ProEye VR headset that is equipped with an embedded eye-tracker (120 Hz binocular sampling rate). Audio was delivered through in-ear headphones (Etymotic ER1-14 with disposable foam eartips).</p><p>In the VR classroom (<xref ref-type="fig" rid="fig10">Figure 10A</xref>), participants experienced sitting at a desk in the second row of a classroom, facing a male avatar-teacher, who was standing in front of a blackboard and a wall adorned with maps. Ten additional avatar-students occupied the remaining desks in the classroom. Although these student avatars did not engage in verbal communication during the experiment, their bodies were animated to simulate natural sitting movements. The avatar-teacher delivered a series of 30 mini-lessons on a range of scientific and historical topics (mean duration 42.7 s Â±â5.1). The audio for these mini-lessons was taken from existing informational podcast, recorded by a male speaker. Speech audio was presented using 3D spatial audio (implemented in Unity) to be perceive as emitting from the teacherâs lips. To achieve realistic lip-speech synchronization, the teacherâs lip movements were controlled by the temporal envelope of the speech, adjusting both timing and mouth size dynamically. His body motions were animated using natural talking gestures.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Virtual reality classroom setup.</title><p>(<bold>A</bold>) Illustration of the virtual classroom used in the study.Inset: a participant wearing the virtual reality (VR) headset over the electroencephalography (EEG) cap. (<bold>B</bold>) Example of the sequence of sound-events presented in Event-trials, which were emitted from a spatial location to the right or left of the participant. Event-trials contained five events, of both types (Artificial and Human non-verbal sounds), randomized across trials and separated by 3â7 s. (<bold>C</bold>) Example of a multiple-choice comprehension question presented after each trial (English translation).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-fig10-v1.tif"/></fig><p>Besides the teacherâs lesson, short sounds (sound-events) were presented occasionally in most trials, from a spatial location to the right or left of the participant (randomized), as if originating from one of the students sitting beside them in class (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). A total of 110 different sounds were used, taken from existing sound databases (<ext-link ext-link-type="uri" xlink:href="https://freesfx.co.uk/">freesfx.co.uk</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://findsounds.com/">findsounds.com</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://freesound.org/">freesound.org)</ext-link>, and the IADS-E sound stimuli database (<xref ref-type="bibr" rid="bib118">Yang et al., 2018</xref>), from two broad categories: Non-verbal human sounds (mainly coughs and throat clearings), and Artificial sounds (phone ringtones and digitally generated âboingâ sounds). All sound-events were normalized to the same loudness level using the Normalize function in the audio-editing software Audacity (<ext-link ext-link-type="uri" xlink:href="http://theaudacityteam.org/">theaudacityteam.org</ext-link>, ver 3.4), with the peak amplitude parameter set to â5 dB, and trimmed to a duration of 300 ms.</p><p>In the VR classroom, the loudness of the teacherâs speech was set to a comfortable level for each participant, as determined in a training trial. The precise intensity level cannot be measured reliably, since it is affected by the specific positioning of the foam earphone inserts in the participantsâ ear, but was roughly between 60 and 70 dB SPL. The sound-events were played at a loudness level of 50% relative to the teacherâs voice (â3 dB SPL).</p></sec><sec id="s4-4"><title>Experimental procedure</title><p>The experiment was conducted in a sound attenuated room. After comfortably fitting the VR headset on the participant, the eye-tracker was calibrated and validated using a standard 9-point calibration procedure. Participants were then familiarized with the VR classroom scene, and performed two training trials â one without events-sounds and one with â for training and calibration purposes.</p><p>The experiment consisted of 30 trials. In each trial, the avatar-teacher delivered a different mini-lesson and participants were instructed to pay attention to its content. They were not given any specific instructions regarding the other sounds or stimuli in the classroom. After each mini-lesson, participants answered four multiple-choice questions about its content (<xref ref-type="fig" rid="fig10">Figure 10C</xref>). Most trials (22) contained occasional background event-sound (Events condition; with 5 of both types sound-events per trial, separated by 3â7 s; <xref ref-type="fig" rid="fig10">Figure 10B</xref>), whereas 8 trials did not, serving as a Quiet baseline condition. In total, 104 sound-events were presented throughout the experiment (52 of each type). The order of mini-lecture presentation and their allocation to the Events or Quiet condition was randomized across participants.</p></sec><sec id="s4-5"><title>EEG, GSR, and eye-gaze recordings</title><p>Neural activity was recorded using 64 active ag-agCl EEG electrodes (BioSemi) and was sampled at 1024 Hz. Electrooculographic (EOG) signals were also included in this recording, measured by three additional electrodes located above the right eye and on the external side of both eyes.</p><p>The SC response (<xref ref-type="bibr" rid="bib2">Akash et al., 2018</xref>) was recorded using two passive Ni hon Kohden electrodes placed on the fingertips of the index and middle fingers of participantsâ non-dominant hand. These signals were also sampled by the BioSemi system, and thus synchronized to EEG data.</p><p>Eye-gaze data were recorded using the eye tracker embedded in the VR headset, which provides information about the <italic>xâyâz</italic> coordinate of the gaze in 3D space at each time-point, as well as automatic blink-detection. We defined several a priori regions of interest (ROIs) in the 3D classroom space, which included: âTeacherâ, âLeft Boardâ, âRight Boardâ, âCeilingâ, âFloorâ, âMiddle Windowâ, âLeft Studentâ, âRight Studentâ. Note that the sound-events were emitted from the ROI labeled âRight Studentâ. Using a combined calculation of the participantsâ head position and direction of the gaze vector, gaze data could be automatically classified to describe which ROI the participant looked at, at each point in time.</p></sec><sec id="s4-6"><title>Behavioral data analysis</title><p>To evaluate participantsâ comprehension and memory retention, we calculated their accuracy on four multiple-choice questions following each trial (average correct responses). Between-group differences were assessed using unpaired <italic>t</italic>-tests, separately for the Quiet and Events conditions. These were evaluated separately (here and in all other analyses), given the imbalanced number of trials in each condition. The BF<sub>10</sub> was also calculated for each pairwise comparison, to test the level of confidence for embracing or rejecting the null hypothesis (H0; BF<sub>10</sub> thresholds used throughout: BF<sub>10</sub> &lt;1 indicates weak support of H0 and BF<sub>10</sub> &lt;0.25 indicates moderate/strong support of H0; BF<sub>10</sub> &gt;1 indicates weak support for rejecting H0 and BF<sub>10</sub> &gt;2.5 indicates moderate/strong support for rejecting H0).</p></sec><sec id="s4-7"><title>EEG data analysis</title><p>The EEG data were preprocessed using MATLAB with the FieldTrip toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org">https://www.fieldtriptoolbox.org</ext-link>, version 20220729). The raw EEG data was referenced to linked right and left mastoids, then band-pass filtered between 0.5 and 40 Hz (fourth-order zero-phase Butterworth IIR filter), detrended and demeaned. Visual inspection was performed to identify and remove gross artifacts (excluding eye movements). Independent component analysis was then used to further remove components associated with horizontal or vertical eye movements and heartbeats, which were selected manually based on their characteristic time-course patterns and topographical distributions, consistent with known artifact signatures. Remaining noisy electrodes, containing extensive high-frequency activity or DC drifts, were interpolated using adjacent electrodes, either on the entire dataset or on a per-trial basis, as needed.</p><p>Three types of analyses were applied to the clean EEG data: (1) Speech tracking analysis of the neural response to teacherâs speech, (2) ERPs analysis capturing the time-locked neural response to the Sound-events, and (3) Spectral analysis, looking specifically at ongoing power in two canonical frequency bands previously associated with attention: the alpha (6â15 Hz) and beta (13â30 Hz) ranges.</p><sec id="s4-7-1"><title>Speech tracking response (TRF)</title><p>The clean EEG data were segmented into trials from the onset the teacherâs speech until the end of the trial. For this analysis, EEG data were further bandpass filtered between 0.8 and 20 Hz (fourth-order zero-phase Butterworth IIR filter), and downsampled to 100 Hz for computational efficiency. To ensure comparability across participants and conditions, the EEG data were normalized using <italic>z</italic>-score transformation. To estimate the neural response to the speech we performed speech-tracking analysis which estimates a linear TRF that best describes the relationship between features of the stimulus (S) and the neural response (R) in a given trial. Speech tracking analysis was conducted using the mTRF MATLAB toolbox (<xref ref-type="bibr" rid="bib24">Crosse et al., 2016</xref>), using both an encoding and decoding approach. Speech-tracking analysis was limited only to the Events condition, since the Quiet condition contained substantially less data (eight trials; ~5 min of data in total), which was insufficient for obtaining a reliable speech-tracking response (predictive power non-significant vs. permutations of shuffled data, p = 0.43). For speech tracking in the Events condition we used a multivariate approach, and included the temporal envelopes of both the teacherâs speech and the sound-events as separate regressors (S) when modeling the neural response, so as to represent the entire soundscape they heard. For both regressors, we derived a broadband temporal envelope by filtering the audio through a narrow-band cochlear filterbank, extracting the narrowband envelopes using a Hilbert transform, and averaging across bands. The regressors were then downsampled to 100 Hz to match the EEG data and normalized using <italic>z</italic>-score.</p><p>In the <bold>encoding</bold> approach, TRFs are estimated for each EEG channel, with lags between the S and the R ranging from â150 ms (baseline) to 450 ms. The mTRF toolbox uses a ridge-regression approach for L2 regularization of the model to ensure better generalization to new data. We tested a range of ridge parameter values (<italic>Î»</italic>âs; between 10<sup>â2</sup> and 10<sup>4</sup>) and used a leave-one-out cross-validation procedure to assess the modelâs predictive power, whereby in each iteration, all but one trials are used to train the model, and it is then applied to the left-out trial. The predictive power of the model (for each <italic>Î»</italic>) is estimated as the Pearsonâs correlation between the predicted neural responses and the actual neural responses, separately for each electrode, averages across all iterations. We report results of the model with the <italic>Î»</italic> the yielded the highest predictive power at the group-level (rather than selecting a different <italic>Î»</italic> for each participant which can lead to incomparable TRF models across participants; see discussion in <xref ref-type="bibr" rid="bib55">Kaufman and ZionâGolumbic, 2023</xref>).</p><p>The statistical reliability of the modelâs predictive power at each electrode is evaluated using a permutation test where the same procedure is repeated 100 times on shuffled data where the S from each trial is randomly paired with the R from a different trial. From each permutation, we extract the highest predictive power across all electrodes, yielding a null distribution of the maximal predictive power that could be obtained by chance. The predictive power in the real data is compared to this distribution, and electrodes with values falling in the top 5%<sup>tile</sup> of the null distribution are considered to have a significant speech tracking response. Note that by choosing the maximal value across electrodes in each permutation, this procedure also corrects for multiple comparisons.</p><p>A complementary way to examine the neural representation of continuous speech is the <bold>decoding</bold> approach. In the decoding model, a multidimensional transfer function is estimated using the neural data (R) from all electrodes as input in attempt to reconstruct the stimulus feature (S).</p><p>For the decoding analysis we used time lags between S and R ranging from â400 to 0 ms (negative lags imply that the S precedes the R; here including a baseline would be detrimental to decoding). Here too, we used a ridge-regression regularization approach and used a leave-one-out cross-validation procedure to assess the quality of the reconstruction. We tested a range of ridge parameter values (<italic>Î»</italic>âs; between 10<sup>â2</sup> and 10<sup>4</sup>) and used a leave-one-out cross-validation procedure to assess the modelâs predictive power, whereby in each iteration, all but one trials are used to train the model, and it is then applied to the left-out trial. The quality of the model (for each <italic>Î»</italic>) is estimated as the Pearsonâs correlation between the reconstructed S of a left-out-trial vs, the actual S (reconstruction correlation). We report results of the model with the <italic>Î»</italic> the yielded the highest predictive power at the group-level. The statistical significance of the reconstruction correlation was assessed using a permutation test in which the same procedure is repeated 100 times on shuffled data where the S from each trial is randomly paired with the R from a different trial. This yields a null distribution of predictive power values that could be obtained by chance, and the real model is considered reliable if its predictive power falls within the top 5%<sup>tile</sup> of the null distribution.</p><p>Speech tracking analysis â using both encoding and decoding approaches â was performed separately for each Group (AD(H)D and Control). To test for between-group differences, we compared the modelsâ predictive power, using unpaired <italic>t</italic>-tests. We additionally calculated the BF<sub>10</sub> for each comparison, to test the level of confidence for embracing or rejecting the null hypothesis.</p></sec><sec id="s4-7-2"><title>Event-related response</title><p>To quantify neural responses to the sound-events, we estimated the time-locked ERPs. For this analysis, the clean EEG data were segmented into epochs ranging from â100 to 500 ms around each event. The data were further low-passed filtered at 12 Hz, as is common in ERP analysis (fourth-order zero-phase Butterworth IIR filter), and baseline corrected to the pre-stimulus period (â100 to 0 ms). ERPs were derived separately for each group (AD(H)D vs. Control) and Event-type (Artificial vs. Non-verbal human sounds). Visual inspection of the ERPs showed that they were dominated by two early components: the N1-component (75â200 ms) and later P2-component (210â260 ms), which primarily reflect auditory sensory responses, but are sometimes modulated by attention (<xref ref-type="bibr" rid="bib47">Hillyard et al., 1973</xref>; <xref ref-type="bibr" rid="bib116">Woldorff and Hillyard, 1991</xref>).</p><p>Statistical analysis of the ERPs focused on the two simple effects, using a Monte-Carlo spatio-temporal cluster-based permutation test, across all electrodes and time-points. To test for difference in the ERPs between Groups, we used unpaired <italic>t</italic>-tests on the ERPs, averaged across Event-types. For this analysis, we had to set the cluster alpha-level to a very low level of p = 10<sup>â5</sup>, in order to obtain separate clusters for the N1 and P2 responses. To test for difference in the ERPs generated for the two Event-types, we use paired <italic>t</italic>-tests on the ERPs of all participant, irrespective of their group. For this analysis, the cluster alpha-level was set to p = 0.05. Last, to verify the main effects and in order to test for potential interactions between Group and Event-type, we extracted the peak amplitudes of the N1 and P2 component from each participant, and performed a mixed ANOVA with repeated measures, with the factors Group (AD(H)D vs. Control; between) and Event-Type (Artificial vs. Human non-verbal; within).</p></sec><sec id="s4-7-3"><title>Spectral analysis</title><p>The third analysis performed on the EEG data was a spectral analysis. This analysis was performed on the clean EEG data, segmented into full trials (same segmentation used for the speech-tracking analysis). We calculated the EEG spectral power density (PSD) of individual trials using multitaper fast-fourier transform (FFT) with Hanning tapers (method âmtmfftâ in the fieldtrip toolbox). The PSDs were averaged across trials for each participant, separately for each electrode and condition (Quiet and Events). We used the FOOOF algorithm (<xref ref-type="bibr" rid="bib29">Donoghue et al., 2020</xref>) to decompose the PSD into periodic (oscillatory) and aperiodic components. The periodic portion of the PSD showed clear peaks in the alpha (8â12 Hz) and beta (15â25 Hz) ranges, suggesting reliable oscillatory activity (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Since the peaks of alpha and beta activity can vary substantially across individuals, for each participant, we identified the frequency with the largest amplitude within each range, focusing on a cluster of occipitalâparietal electrodes (for alpha) or frontalâcentral electrodes (for beta), where these responses were largest. These were used for statistical analyses of between-group differences. assessed using unpaired <italic>t</italic>-tests, separately for the Quiet and Events conditions, in each frequency band, and the BF<sub>10</sub> was calculated for each pairwise comparison to test the level of confidence for embracing or rejecting the null hypothesis.</p></sec><sec id="s4-7-4"><title>SC data analysis</title><p>The SC signal was analyzed using the Ledalab MATLAB toolbox (version 3.4.9; <xref ref-type="bibr" rid="bib12">Benedek and Kaernbach, 2010</xref>; <ext-link ext-link-type="uri" xlink:href="http://www.ledalab.de/">http://www.ledalab.de/</ext-link>). The raw data were downsampled to 16 Hz using FieldTripâs ft_resampledata function, which applies a built-in anti-aliasing low-pass filter to prevent aliasing artifacts. Data were inspected manually for any noticeable artifacts (large âjumpsâ), and if present were corrected using linear interpolation in Ledalab. A continuous decomposition analysis (CDA) was employed to separate the tonic and phasic SC responses for each participant. The CDA was conducted using the âsdecoâ mode (signal decomposition), which iteratively optimizes the separation of tonic and phasic components using the default regularization settings. We tested for between-group differences in <italic>global SC levels</italic> (tonic and phasic responses separately, averaged across full trials) using unpaired <italic>t</italic>-tests, separately for the Quiet and Events conditions and the BF<sub>10</sub> was calculated for each pairwise comparison.</p><p>In addition, we analyzed <italic>event-related changes in SC</italic> following Sound-events. This analysis focused on a time-window between 0 and 5 s after the onset of Sound-events. Since changes in SC are relatively slow and peak between 2 and 3 s after a stimulus, the mean response between 0 and 1 s was used as a baseline and was subtracted from the signal. Statistical analysis focused the time-window surrounding the mean response between 2 and 3 s, extracted for each participant. A mixed ANOVA with repeated measures was used to test for main effects of Group (AD(H)D vs. Control; between factor), and Event-Type (Artificial vs. Non-verbal human; within factor) and the interaction between them.</p></sec></sec><sec id="s4-8"><title>Eye tracking data analysis</title><p>Eye-tracking data were used to study participants' spontaneous gaze-dynamics in the VR classroom throughout the entire trial and also to examine whether background Sound-events elicited overt gaze-shifts. Gaze-data preprocessing involved removing measurements around blinks (from â100 to 200 ms after) and any other data points marked as âmissing dataâ by the eye-tracker. Gaze-data were analyzed using Python, closely following recommended procedures for analysis of gaze-data in VR (<xref ref-type="bibr" rid="bib4">Anderson et al., 2023</xref>). To assess gaze-patterns and detect gaze-shift, we calculated the Euclidean distance between adjacent data points, using the <italic>x</italic>, <italic>y</italic>, <italic>z</italic> coordinates in 3D space provided by the eye-tracker. âFixationsâ were defined as clusters of adjacent time-points within a radius of less than 0.01 distance from each other, lasting at least 80 ms. Each fixation was associated with a specific ROI in the VR classroom, indicating the object in the virtual space that the participant looked at. âGaze-shiftsâ were defined as transitions between two stable fixations.</p><p>To analyze spontaneous gaze-dynamics throughout an entire trial, we quantified the percent of each trial that participants spent looking at the teacher (target) versus all other ROIs, as well as the number of gaze-shifts away from the teacher per trial. Between-group differences in these measures of spontaneous gaze-patterns were assessed using unpaired <italic>t</italic>-tests, separately for the Quiet and Events conditions. The BF<sub>10</sub> was calculated for each pairwise comparison to test the level of confidence for embracing or rejecting the null hypothesis.</p><p>To determine whether background Sound-events elicited overt gaze-shifts, we segmented the gaze-data into 2-s long epochs around each Event-sound. As a control, we randomly chose a similar number of 2-s long epochs from the Quiet-condition. We then counted how many epochs included at least one gaze-shift away from the teacher, and used a paired <italic>t</italic>-test to determine whether the frequency of gaze-shifts was higher following Sound-events relative to the Control epochs (regardless of Group). We also used a mixed ANOVA to test for potential differences in the likelihood of gaze-shifts following the different event-types (Artificial vs. Human non-verbal), and for the two Groups (AD(H)D vs. Control). For this analysis, the proportion of gaze-shifts was normalized for each participant by subtracting the number of gaze-shifts following Sound-events relative to the control epochs.</p></sec><sec id="s4-9"><title>Multivariate analyses</title><p>Besides testing for effects of Group in each metric individually, we also performed multivariate analyses to examine the relationship between the various metrics collected here and their ability to predict ADHD diagnosis (binary) and/or the severity of AD(H)D symptoms.</p><p>This analysis focused on 10 key measures, based on the results of the univariate analyses (averaged across conditions, when relevant): 5 measures related to the response to the teacherâs speech: behavioral performance, neural speech tracking decoding (reconstruction correlation), the N1- and P2 peaks of the speech tracking response (TRF-N1; TRF-P2), and the average number of gaze-shifts away from the teacher; 3 measures related to responses to the sound-events: amplitude of the N1- and P2-responses to sound-events (ERP-N1; ERP-P2), phasic SC response to sound-events (event-related SC); and 2 global measures associated with the listenersâ âstateâ: alpha- and beta-power.</p><p>First, we calculated Spearmanâs correlation between all pairs of measures (fdr correction for multiple comparisons was applied). Next, we entered all factors into a multiple regression model and performed a dominance analysis to determine the relative importance of each measure for predicting whether an individual was in the ADHD or control group (logistic regression) or the severity of ADHD symptoms, as reflected by their ASRS scores (linear regression). Dominance analysis was performed as follows: For each of the 10 measures, we computed the difference in a model-fit index (AUC for logistic regression; <italic>R</italic><sup>2</sup> for linear regression) between a full model containing all 10 predictors, and a model where the measure under consideration was held-out, as a means for assessing the unique contribution of each measure to overall model performance (dominance). We then sorted the measures based on their dominance and report the multiple regression model that explains the most amount of variance using the least number of variables. These analyses were conducted using the statistical software JASP (<xref ref-type="bibr" rid="bib108">Team, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Methodology</p></fn><fn fn-type="con" id="con4"><p>Supervision, Methodology, Project administration</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Writing â review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Writing â review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing â original draft, Project administration, Writing â review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The experimental protocol was approved by the ethics committee at Bar-Ilan University (protocol # ISU202112002), and all participants provided written informed consent prior to their involvement in the study and data collection procedures.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103235-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Video examples of the VR classroom and the entire dataset are publicly available on OSF: <ext-link ext-link-type="uri" xlink:href="https://osf.io/svjqg">https://osf.io/svjqg</ext-link>. Please contact authors for additional information on data structure.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>O</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Selective attention and sensitivity to auditory disturbances in a virtually real Classroom</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/svjqg">svjqg</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Israel Science Foundation (ISF grant # 2339/20 to EZG), the National Institute of Mental Health (NIMH grant R33MH110043 to JS), and the Binational Science Foundation (BSF grant # 2022024 to EZG and JS). We would also like to thank the Minerva Center for Human Intelligence in Immersive, Augmented and Mixed Realities at Tel Aviv University, for supporting for this research.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adler</surname><given-names>LA</given-names></name><name><surname>Spencer</surname><given-names>T</given-names></name><name><surname>Faraone</surname><given-names>SV</given-names></name><name><surname>Kessler</surname><given-names>RC</given-names></name><name><surname>Howes</surname><given-names>MJ</given-names></name><name><surname>Biederman</surname><given-names>J</given-names></name><name><surname>Secnik</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Validity of pilot adult ADHD self- report scale (ASRS) to rate adult ADHD symptoms</article-title><source>Annals of Clinical Psychiatry</source><volume>18</volume><fpage>145</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1080/10401230600801077</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akash</surname><given-names>K</given-names></name><name><surname>Hu</surname><given-names>W-L</given-names></name><name><surname>Jain</surname><given-names>N</given-names></name><name><surname>Reid</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A classification model for sensing human trust in machines using EEG and GSR</article-title><source>ACM Transactions on Interactive Intelligent Systems</source><volume>8</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1145/3132743</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><collab>American Psychiatric Association</collab></person-group><year iso-8601-date="2022">2022</year><source>Diagnostic and Statistical Manual of Mental Disorders</source><publisher-name>American Psychological Association</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>NC</given-names></name><name><surname>Bischof</surname><given-names>WF</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Eye tracking in virtual reality</article-title><source>Current Topics in Behavioral Neurosciences</source><volume>65</volume><fpage>73</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1007/7854_2022_409</pub-id><pub-id pub-id-type="pmid">36710302</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arrondo</surname><given-names>G</given-names></name><name><surname>Mulraney</surname><given-names>M</given-names></name><name><surname>Iturmendi-Sabater</surname><given-names>I</given-names></name><name><surname>Musullulu</surname><given-names>H</given-names></name><name><surname>Gambra</surname><given-names>L</given-names></name><name><surname>Niculcea</surname><given-names>T</given-names></name><name><surname>Banaschewski</surname><given-names>T</given-names></name><name><surname>Simonoff</surname><given-names>E</given-names></name><name><surname>DÃ¶pfner</surname><given-names>M</given-names></name><name><surname>Hinshaw</surname><given-names>SP</given-names></name><name><surname>Coghill</surname><given-names>D</given-names></name><name><surname>Cortese</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Systematic review and meta-analysis: clinical utility of continuous performance tests for the identification of attention-deficit/hyperactivity disorder</article-title><source>Journal of the American Academy of Child and Adolescent Psychiatry</source><volume>63</volume><fpage>154</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.jaac.2023.03.011</pub-id><pub-id pub-id-type="pmid">37004919</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avisar</surname><given-names>A</given-names></name><name><surname>Shalev</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sustained attention and behavioral characteristics associated with ADHD in adults</article-title><source>Applied Neuropsychology</source><volume>18</volume><fpage>107</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1080/09084282.2010.547777</pub-id><pub-id pub-id-type="pmid">21660762</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barkley</surname><given-names>RA</given-names></name><name><surname>Murphy</surname><given-names>KR</given-names></name><name><surname>Fischer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>ADHD in Adults: What the Science Says</source><publisher-name>Guilford Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barkley</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neuropsychological testing is not useful in the diagnosis of ADHD: stop it (or prove It)!</article-title><source>The ADHD Report</source><volume>27</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1521/adhd.2019.27.2.1</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname><given-names>RJ</given-names></name><name><surname>Johnstone</surname><given-names>SJ</given-names></name><name><surname>Clarke</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A review of electrophysiology in attention-deficit/hyperactivity disorder: II. Event-related potentials</article-title><source>Clinical Neurophysiology</source><volume>114</volume><fpage>184</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(02)00363-2</pub-id><pub-id pub-id-type="pmid">12559225</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname><given-names>RJ</given-names></name><name><surname>Steiner</surname><given-names>GZ</given-names></name><name><surname>De Blasio</surname><given-names>FM</given-names></name><name><surname>Fogarty</surname><given-names>JS</given-names></name><name><surname>Karamacoska</surname><given-names>D</given-names></name><name><surname>MacDonald</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Components in the P300: Donât forget the novelty P3!</article-title><source>Psychophysiology</source><volume>57</volume><elocation-id>e13371</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13371</pub-id><pub-id pub-id-type="pmid">30920012</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellato</surname><given-names>A</given-names></name><name><surname>Arora</surname><given-names>I</given-names></name><name><surname>Hollis</surname><given-names>C</given-names></name><name><surname>Groom</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Is autonomic nervous system function atypical in attention deficit hyperactivity disorder (ADHD)? A systematic review of the evidence</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>108</volume><fpage>182</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.11.001</pub-id><pub-id pub-id-type="pmid">31722229</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benedek</surname><given-names>M</given-names></name><name><surname>Kaernbach</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A continuous measure of phasic electrodermal activity</article-title><source>Journal of Neuroscience Methods</source><volume>190</volume><fpage>80</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.04.028</pub-id><pub-id pub-id-type="pmid">20451556</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>I</given-names></name><name><surname>Cassuto</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The effect of environmental distractors incorporation into a CPT on sustained attention and ADHD diagnosis among adolescents</article-title><source>Journal of Neuroscience Methods</source><volume>222</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.10.012</pub-id><pub-id pub-id-type="pmid">24211249</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berti</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The role of auditory transient and deviance processing in distraction of task performance: a combined behavioral and event-related brain potential study</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>352</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00352</pub-id><pub-id pub-id-type="pmid">23874278</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidet-Caulet</surname><given-names>A</given-names></name><name><surname>Bottemanne</surname><given-names>L</given-names></name><name><surname>Fonteneau</surname><given-names>C</given-names></name><name><surname>Giard</surname><given-names>MH</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Brain dynamics of distractibility: interaction between top-down and bottom-up mechanisms of auditory attention</article-title><source>Brain Topography</source><volume>28</volume><fpage>423</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1007/s10548-014-0354-x</pub-id><pub-id pub-id-type="pmid">24531985</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boudewyn</surname><given-names>MA</given-names></name><name><surname>Carter</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>I must have missed that: Alpha-band oscillations track attention to spoken language</article-title><source>Neuropsychologia</source><volume>117</volume><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.05.024</pub-id><pub-id pub-id-type="pmid">29842859</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bozhilova</surname><given-names>N</given-names></name><name><surname>Kuntsi</surname><given-names>J</given-names></name><name><surname>Rubia</surname><given-names>K</given-names></name><name><surname>Asherson</surname><given-names>P</given-names></name><name><surname>Michelini</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Event-related brain dynamics during mind wandering in attention-deficit/hyperactivity disorder: An experience-sampling approach</article-title><source>NeuroImage. Clinical</source><volume>35</volume><elocation-id>103068</elocation-id><pub-id pub-id-type="doi">10.1016/j.nicl.2022.103068</pub-id><pub-id pub-id-type="pmid">35696811</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braga</surname><given-names>RM</given-names></name><name><surname>Fu</surname><given-names>RZ</given-names></name><name><surname>Seemungal</surname><given-names>BM</given-names></name><name><surname>Wise</surname><given-names>RJS</given-names></name><name><surname>Leech</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Eye movements during auditory attention predict individual differences in dorsal attention network activity</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>164</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00164</pub-id><pub-id pub-id-type="pmid">27242465</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brevik</surname><given-names>EJ</given-names></name><name><surname>Lundervold</surname><given-names>AJ</given-names></name><name><surname>Haavik</surname><given-names>J</given-names></name><name><surname>Posserud</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Validity and accuracy of the Adult Attention-Deficit/Hyperactivity Disorder (ADHD) Self-Report Scale (ASRS) and the Wender Utah Rating Scale (WURS) symptom checklists in discriminating between adults with and without ADHD</article-title><source>Brain and Behavior</source><volume>10</volume><elocation-id>e01605</elocation-id><pub-id pub-id-type="doi">10.1002/brb3.1605</pub-id><pub-id pub-id-type="pmid">32285644</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>A</given-names></name><name><surname>Pinto</surname><given-names>D</given-names></name><name><surname>Burgart</surname><given-names>K</given-names></name><name><surname>Zvilichovsky</surname><given-names>Y</given-names></name><name><surname>Zion-Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neurophysiological evidence for semantic processing of irrelevant speech and own-name detection in a virtual cafÃ©</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>5045</fpage><lpage>5056</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1731-22.2023</pub-id><pub-id pub-id-type="pmid">37336758</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chennu</surname><given-names>S</given-names></name><name><surname>Noreika</surname><given-names>V</given-names></name><name><surname>Gueorguiev</surname><given-names>D</given-names></name><name><surname>Blenkmann</surname><given-names>A</given-names></name><name><surname>Kochen</surname><given-names>S</given-names></name><name><surname>IbÃ¡Ã±ez</surname><given-names>A</given-names></name><name><surname>Owen</surname><given-names>AM</given-names></name><name><surname>Bekinschtein</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Expectation and attention in hierarchical auditory prediction</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>11194</fpage><lpage>11205</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0114-13.2013</pub-id><pub-id pub-id-type="pmid">23825422</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>AR</given-names></name><name><surname>Barry</surname><given-names>RJ</given-names></name><name><surname>McCarthy</surname><given-names>R</given-names></name><name><surname>Selikowitz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Excess beta activity in children with attention-deficit/hyperactivity disorder: an atypical electrophysiological group</article-title><source>Psychiatry Research</source><volume>103</volume><fpage>205</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/s0165-1781(01)00277-3</pub-id><pub-id pub-id-type="pmid">11549408</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coleman</surname><given-names>B</given-names></name><name><surname>Marion</surname><given-names>S</given-names></name><name><surname>Rizzo</surname><given-names>A</given-names></name><name><surname>Turnbull</surname><given-names>J</given-names></name><name><surname>Nolty</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Virtual reality assessment of classroom - related attention: an ecologically relevant approach to evaluating the effectiveness of working memory training</article-title><source>Frontiers in Psychology</source><volume>10</volume><elocation-id>1851</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2019.01851</pub-id><pub-id pub-id-type="pmid">31481911</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The multivariate temporal response function (mTRF) Toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linear modeling of neurophysiological responses to speech and other continuous stimuli: methodological considerations for applied research</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>705621</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.705621</pub-id><pub-id pub-id-type="pmid">34880719</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>78</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1152/jn.00297.2011</pub-id><pub-id pub-id-type="pmid">21975452</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dockree</surname><given-names>PM</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Robertson</surname><given-names>IH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Optimal sustained attention is linked to the spectral content of background EEG activity: greater ongoing tonic alpha (approximately 10 Hz) power supports successful phasic goal activation</article-title><source>The European Journal of Neuroscience</source><volume>25</volume><fpage>900</fpage><lpage>907</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2007.05324.x</pub-id><pub-id pub-id-type="pmid">17328783</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoghue</surname><given-names>T</given-names></name><name><surname>Haller</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>EJ</given-names></name><name><surname>Varma</surname><given-names>P</given-names></name><name><surname>Sebastian</surname><given-names>P</given-names></name><name><surname>Gao</surname><given-names>R</given-names></name><name><surname>Noto</surname><given-names>T</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Shestyuk</surname><given-names>A</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parameterizing neural power spectra into periodic and aperiodic components</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1655</fpage><lpage>1665</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00744-x</pub-id><pub-id pub-id-type="pmid">33230329</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elahi</surname><given-names>H</given-names></name><name><surname>Iosif</surname><given-names>AM</given-names></name><name><surname>Mukherjee</surname><given-names>P</given-names></name><name><surname>Hinshaw</surname><given-names>SP</given-names></name><name><surname>Schweitzer</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Using hot and cool measures to phenotype and predict functional outcomes across dimensions of ADHD and typical development in adolescents</article-title><source>Research on Child and Adolescent Psychopathology</source><volume>52</volume><fpage>579</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1007/s10802-023-01149-7</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escera</surname><given-names>C</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Winkler</surname><given-names>I</given-names></name><name><surname>NÃ¤Ã¤tÃ¤nen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural mechanisms of involuntary attention to acoustic novelty and change</article-title><source>Journal of Cognitive Neuroscience</source><volume>10</volume><fpage>590</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1162/089892998562997</pub-id><pub-id pub-id-type="pmid">9802992</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escera</surname><given-names>C</given-names></name><name><surname>Yago</surname><given-names>E</given-names></name><name><surname>Corral</surname><given-names>MJ</given-names></name><name><surname>Corbera</surname><given-names>S</given-names></name><name><surname>NuÃ±ez</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Attention capture by auditory significant stimuli: semantic analysis follows attention switching</article-title><source>The European Journal of Neuroscience</source><volume>18</volume><fpage>2408</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02937.x</pub-id><pub-id pub-id-type="pmid">14622204</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esterman</surname><given-names>M</given-names></name><name><surname>Rothlein</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Models of sustained attention</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>174</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.005</pub-id><pub-id pub-id-type="pmid">30986621</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faraone</surname><given-names>SV</given-names></name><name><surname>Banaschewski</surname><given-names>T</given-names></name><name><surname>Coghill</surname><given-names>D</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Biederman</surname><given-names>J</given-names></name><name><surname>Bellgrove</surname><given-names>MA</given-names></name><name><surname>Newcorn</surname><given-names>JH</given-names></name><name><surname>Gignac</surname><given-names>M</given-names></name><name><surname>Al Saud</surname><given-names>NM</given-names></name><name><surname>Manor</surname><given-names>I</given-names></name><name><surname>Rohde</surname><given-names>LA</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>Cortese</surname><given-names>S</given-names></name><name><surname>Almagor</surname><given-names>D</given-names></name><name><surname>Stein</surname><given-names>MA</given-names></name><name><surname>Albatti</surname><given-names>TH</given-names></name><name><surname>Aljoudi</surname><given-names>HF</given-names></name><name><surname>Alqahtani</surname><given-names>MMJ</given-names></name><name><surname>Asherson</surname><given-names>P</given-names></name><name><surname>Atwoli</surname><given-names>L</given-names></name><name><surname>BÃ¶lte</surname><given-names>S</given-names></name><name><surname>Buitelaar</surname><given-names>JK</given-names></name><name><surname>Crunelle</surname><given-names>CL</given-names></name><name><surname>Daley</surname><given-names>D</given-names></name><name><surname>Dalsgaard</surname><given-names>S</given-names></name><name><surname>DÃ¶pfner</surname><given-names>M</given-names></name><name><surname>Espinet</surname><given-names>S</given-names></name><name><surname>Fitzgerald</surname><given-names>M</given-names></name><name><surname>Franke</surname><given-names>B</given-names></name><name><surname>Gerlach</surname><given-names>M</given-names></name><name><surname>Haavik</surname><given-names>J</given-names></name><name><surname>Hartman</surname><given-names>CA</given-names></name><name><surname>Hartung</surname><given-names>CM</given-names></name><name><surname>Hinshaw</surname><given-names>SP</given-names></name><name><surname>Hoekstra</surname><given-names>PJ</given-names></name><name><surname>Hollis</surname><given-names>C</given-names></name><name><surname>Kollins</surname><given-names>SH</given-names></name><name><surname>Sandra Kooij</surname><given-names>JJ</given-names></name><name><surname>Kuntsi</surname><given-names>J</given-names></name><name><surname>Larsson</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Merzon</surname><given-names>E</given-names></name><name><surname>Mattingly</surname><given-names>G</given-names></name><name><surname>Mattos</surname><given-names>P</given-names></name><name><surname>McCarthy</surname><given-names>S</given-names></name><name><surname>Mikami</surname><given-names>AY</given-names></name><name><surname>Molina</surname><given-names>BSG</given-names></name><name><surname>Nigg</surname><given-names>JT</given-names></name><name><surname>Purper-Ouakil</surname><given-names>D</given-names></name><name><surname>Omigbodun</surname><given-names>OO</given-names></name><name><surname>Polanczyk</surname><given-names>GV</given-names></name><name><surname>Pollak</surname><given-names>Y</given-names></name><name><surname>Poulton</surname><given-names>AS</given-names></name><name><surname>Rajkumar</surname><given-names>RP</given-names></name><name><surname>Reding</surname><given-names>A</given-names></name><name><surname>Reif</surname><given-names>A</given-names></name><name><surname>Rubia</surname><given-names>K</given-names></name><name><surname>Rucklidge</surname><given-names>J</given-names></name><name><surname>Romanos</surname><given-names>M</given-names></name><name><surname>Ramos-Quiroga</surname><given-names>JA</given-names></name><name><surname>Schellekens</surname><given-names>A</given-names></name><name><surname>Scheres</surname><given-names>A</given-names></name><name><surname>Schoeman</surname><given-names>R</given-names></name><name><surname>Schweitzer</surname><given-names>JB</given-names></name><name><surname>Shah</surname><given-names>H</given-names></name><name><surname>Solanto</surname><given-names>MV</given-names></name><name><surname>Sonuga-Barke</surname><given-names>E</given-names></name><name><surname>Soutullo</surname><given-names>C</given-names></name><name><surname>Steinhausen</surname><given-names>H-C</given-names></name><name><surname>Swanson</surname><given-names>JM</given-names></name><name><surname>Thapar</surname><given-names>A</given-names></name><name><surname>Tripp</surname><given-names>G</given-names></name><name><surname>van de Glind</surname><given-names>G</given-names></name><name><surname>van den Brink</surname><given-names>W</given-names></name><name><surname>Van der Oord</surname><given-names>S</given-names></name><name><surname>Venter</surname><given-names>A</given-names></name><name><surname>Vitiello</surname><given-names>B</given-names></name><name><surname>Walitza</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The world federation of ADHD international consensus statement: 208 evidence-based conclusions about the disorder</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>128</volume><fpage>789</fpage><lpage>818</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2021.01.022</pub-id><pub-id pub-id-type="pmid">33549739</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foulsham</surname><given-names>T</given-names></name><name><surname>Walker</surname><given-names>E</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The where, what and when of gaze allocation in the lab and the natural environment</article-title><source>Vision Research</source><volume>51</volume><fpage>1920</fpage><lpage>1931</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.07.002</pub-id><pub-id pub-id-type="pmid">21784095</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Allen</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The skin conductance orienting response as an index of attention</article-title><source>Biological Psychology</source><volume>17</volume><fpage>27</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/0301-0511(83)90064-9</pub-id><pub-id pub-id-type="pmid">6626635</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Cormack</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Models of overt attention</chapter-title><person-group person-group-type="editor"><name><surname>Liversedge</surname><given-names>SP</given-names></name><name><surname>Gilchrist</surname><given-names>I</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name></person-group><source>The Oxford Handbook of Eye Movements Get Access Arrow</source><publisher-name>Oxford University Press</publisher-name><fpage>440</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199539789.013.0024</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gloss</surname><given-names>D</given-names></name><name><surname>Varma</surname><given-names>JK</given-names></name><name><surname>Pringsheim</surname><given-names>T</given-names></name><name><surname>Nuwer</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Practice advisory: The utility of EEG theta/beta power ratio in ADHD diagnosis</article-title><source>Neurology</source><volume>87</volume><fpage>2375</fpage><lpage>2379</lpage><pub-id pub-id-type="doi">10.1212/WNL.0000000000003265</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>SA</given-names></name><name><surname>Dueck</surname><given-names>K</given-names></name><name><surname>Rogers</surname><given-names>M</given-names></name><name><surname>Tannock</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Qualitative review synthesis: the relationship between inattention and academic achievement</article-title><source>Educational Research</source><volume>59</volume><fpage>17</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1080/00131881.2016.1274235</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosbras</surname><given-names>MH</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Paus</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cortical regions involved in eye movements, shifts of attention, and gaze perception</article-title><source>Human Brain Mapping</source><volume>25</volume><fpage>140</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1002/hbm.20145</pub-id><pub-id pub-id-type="pmid">15846814</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gumenyuk</surname><given-names>V</given-names></name><name><surname>Korzyukov</surname><given-names>O</given-names></name><name><surname>Escera</surname><given-names>C</given-names></name><name><surname>HÃ¤mÃ¤lÃ¤inen</surname><given-names>M</given-names></name><name><surname>Huotilainen</surname><given-names>M</given-names></name><name><surname>HÃ¤yrinen</surname><given-names>T</given-names></name><name><surname>Oksanen</surname><given-names>H</given-names></name><name><surname>NÃ¤Ã¤tÃ¤nen</surname><given-names>R</given-names></name><name><surname>von Wendt</surname><given-names>L</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Electrophysiological evidence of enhanced distractibility in ADHD children</article-title><source>Neuroscience Letters</source><volume>374</volume><fpage>212</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2004.10.081</pub-id><pub-id pub-id-type="pmid">15663965</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>CL</given-names></name><name><surname>Valentine</surname><given-names>AZ</given-names></name><name><surname>Groom</surname><given-names>MJ</given-names></name><name><surname>Walker</surname><given-names>GM</given-names></name><name><surname>Sayal</surname><given-names>K</given-names></name><name><surname>Daley</surname><given-names>D</given-names></name><name><surname>Hollis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The clinical utility of the continuous performance test and objective measures of activity for diagnosing and monitoring ADHD in children: a systematic review</article-title><source>European Child &amp; Adolescent Psychiatry</source><volume>25</volume><fpage>677</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1007/s00787-015-0798-x</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haro</surname><given-names>S</given-names></name><name><surname>Rao</surname><given-names>HM</given-names></name><name><surname>Quatieri</surname><given-names>TF</given-names></name><name><surname>Smalt</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>EEG alpha and pupil diameter reflect endogenous auditory attention switching and listening effort</article-title><source>The European Journal of Neuroscience</source><volume>55</volume><fpage>1262</fpage><lpage>1277</lpage><pub-id pub-id-type="doi">10.1111/ejn.15616</pub-id><pub-id pub-id-type="pmid">35098604</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heidbreder</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ADHD symptomatology is best conceptualized as a spectrum: a dimensional versus unitary approach to diagnosis</article-title><source>ADHD Attention Deficit and Hyperactivity Disorders</source><volume>7</volume><fpage>249</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1007/s12402-015-0171-4</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heil</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Auditory cortical onset responses revisited</article-title><source>Ii. Response Strength. J Neurophysiol</source><volume>77</volume><fpage>2642</fpage><lpage>2660</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.5.2642</pub-id><pub-id pub-id-type="pmid">9163381</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henrich</surname><given-names>J</given-names></name><name><surname>Heine</surname><given-names>SJ</given-names></name><name><surname>Norenzayan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The weirdest people in the world?</article-title><source>The Behavioral and Brain Sciences</source><volume>33</volume><fpage>61</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1017/S0140525X0999152X</pub-id><pub-id pub-id-type="pmid">20550733</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Hink</surname><given-names>RF</given-names></name><name><surname>Schwent</surname><given-names>VL</given-names></name><name><surname>Picton</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Electrical signs of selective attention in the human brain</article-title><source>Science</source><volume>182</volume><fpage>177</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1126/science.182.4108.177</pub-id><pub-id pub-id-type="pmid">4730062</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobbiss</surname><given-names>MH</given-names></name><name><surname>Lavie</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Sustained selective attention in adolescence: Cognitive development and predictors of distractibility at school</article-title><source>Journal of Experimental Child Psychology</source><volume>238</volume><elocation-id>105784</elocation-id><pub-id pub-id-type="doi">10.1016/j.jecp.2023.105784</pub-id><pub-id pub-id-type="pmid">37862789</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holtze</surname><given-names>B</given-names></name><name><surname>Jaeger</surname><given-names>M</given-names></name><name><surname>Debener</surname><given-names>S</given-names></name><name><surname>AdiloÄlu</surname><given-names>K</given-names></name><name><surname>Mirkovic</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Are they calling my name? attention capture is reflected in the neural tracking of attended and ignored speech</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>643705</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.643705</pub-id><pub-id pub-id-type="pmid">33828451</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoppe</surname><given-names>S</given-names></name><name><surname>Loetscher</surname><given-names>T</given-names></name><name><surname>Morey</surname><given-names>SA</given-names></name><name><surname>Bulling</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye movements during everyday behavior predict personality traits</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><elocation-id>105</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00105</pub-id><pub-id pub-id-type="pmid">29713270</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>N</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Push-pull competition between bottom-up and top-down auditory attention to natural soundscapes</article-title><source>eLife</source><volume>9</volume><elocation-id>e52984</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.52984</pub-id><pub-id pub-id-type="pmid">32196457</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>CY</given-names></name><name><surname>Borst</surname><given-names>JP</given-names></name><name><surname>van Vugt</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predicting task-general mind-wandering with EEG</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><volume>19</volume><fpage>1059</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.3758/s13415-019-00707-1</pub-id><pub-id pub-id-type="pmid">30850931</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnstone</surname><given-names>SJ</given-names></name><name><surname>Barry</surname><given-names>RJ</given-names></name><name><surname>Clarke</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ten years on: a follow-up review of ERP research in attention-deficit/hyperactivity disorder</article-title><source>Clinical Neurophysiology</source><volume>124</volume><fpage>644</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2012.09.006</pub-id><pub-id pub-id-type="pmid">23063669</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>A</given-names></name><name><surname>Aggensteiner</surname><given-names>PM</given-names></name><name><surname>Baumeister</surname><given-names>S</given-names></name><name><surname>Holz</surname><given-names>NE</given-names></name><name><surname>Banaschewski</surname><given-names>T</given-names></name><name><surname>Brandeis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Earlier versus later cognitive event-related potentials (ERPs) in attention-deficit/hyperactivity disorder (ADHD): A meta-analysis</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>112</volume><fpage>117</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.01.019</pub-id><pub-id pub-id-type="pmid">31991190</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>M</given-names></name><name><surname>ZionâGolumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><source>Listening to Two Speakers: Capacity and Tradeoffs in Neural Speech Tracking during Selective and Distributed Attention</source><publisher-loc>Neuroimage</publisher-loc><publisher-name>Elsevier</publisher-name><pub-id pub-id-type="doi">10.1101/2022.02.08.479628</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kessler</surname><given-names>RC</given-names></name><name><surname>Adler</surname><given-names>L</given-names></name><name><surname>Ames</surname><given-names>M</given-names></name><name><surname>Demler</surname><given-names>O</given-names></name><name><surname>Faraone</surname><given-names>S</given-names></name><name><surname>Hiripi</surname><given-names>E</given-names></name><name><surname>Howes</surname><given-names>MJ</given-names></name><name><surname>Jin</surname><given-names>R</given-names></name><name><surname>Secnik</surname><given-names>K</given-names></name><name><surname>Spencer</surname><given-names>T</given-names></name><name><surname>Ustun</surname><given-names>TB</given-names></name><name><surname>Walters</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The World Health Organization Adult ADHD Self-Report Scale (ASRS): a short screening scale for use in the general population</article-title><source>Psychological Medicine</source><volume>35</volume><fpage>245</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1017/s0033291704002892</pub-id><pub-id pub-id-type="pmid">15841682</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwasa</surname><given-names>JA</given-names></name><name><surname>Noyce</surname><given-names>AL</given-names></name><name><surname>Torres</surname><given-names>LM</given-names></name><name><surname>Richardson</surname><given-names>BN</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Top-down auditory attention modulates neural responses more strongly in neurotypical than ADHD young adults</article-title><source>Brain Research</source><volume>1798</volume><elocation-id>148144</elocation-id><pub-id pub-id-type="doi">10.1016/j.brainres.2022.148144</pub-id><pub-id pub-id-type="pmid">36328068</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lange</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The ups and downs of temporal orienting: a review of auditory temporal orienting studies and a model associating the heterogeneous findings on the auditory N1 with opposite effects of attention and prediction</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>263</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00263</pub-id><pub-id pub-id-type="pmid">23781186</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>O</given-names></name><name><surname>Korisky</surname><given-names>A</given-names></name><name><surname>Zvilichovsky</surname><given-names>Y</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>The neurophysiological costs of learning in a noisy classroom: an ecological virtual reality study</article-title><source>Journal of Cognitive Neuroscience</source><volume>37</volume><fpage>300</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_02249</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>CL</given-names></name><name><surname>Barry</surname><given-names>RJ</given-names></name><name><surname>Gordon</surname><given-names>E</given-names></name><name><surname>Sawant</surname><given-names>A</given-names></name><name><surname>Rennie</surname><given-names>C</given-names></name><name><surname>Yiannikas</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The relationship between quantified EEG and skin conductance level</article-title><source>International Journal of Psychophysiology</source><volume>21</volume><fpage>151</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/0167-8760(95)00049-6</pub-id><pub-id pub-id-type="pmid">8792203</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Teens with adhd: the challenge of high school</article-title><source>Child &amp; Youth Care Forum</source><volume>32</volume><fpage>137</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1023/A:1023350308485</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loe</surname><given-names>IM</given-names></name><name><surname>Feldman</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Academic and educational outcomes of children with ADHD</article-title><source>Journal of Pediatric Psychology</source><volume>32</volume><fpage>643</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1093/jpepsy/jsl054</pub-id><pub-id pub-id-type="pmid">17569716</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loo</surname><given-names>SK</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Clinical utility of EEG in attention-deficit/hyperactivity disorder: a research update</article-title><source>Neurotherapeutics</source><volume>9</volume><fpage>569</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1007/s13311-012-0131-z</pub-id><pub-id pub-id-type="pmid">22814935</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LÃ¼tkenhÃ¶ner</surname><given-names>B</given-names></name><name><surname>Klein</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Auditory evoked field at threshold</article-title><source>Hearing Research</source><volume>228</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.02.011</pub-id><pub-id pub-id-type="pmid">17434696</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDonald</surname><given-names>B</given-names></name><name><surname>Barry</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Integration of three investigations of Novelty, Intensity, and Significance in dishabituation paradigms: A study of the phasic Orienting Reflex</article-title><source>International Journal of Psychophysiology</source><volume>147</volume><fpage>113</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2019.11.009</pub-id><pub-id pub-id-type="pmid">31778726</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makov</surname><given-names>S</given-names></name><name><surname>Pinto</surname><given-names>D</given-names></name><name><surname>Har-Shai Yahav</surname><given-names>P</given-names></name><name><surname>Miller</surname><given-names>LM</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>âUnattended, distracting or irrelevantâ: Theoretical implications of terminological choices in auditory selective attention research</article-title><source>Cognition</source><volume>231</volume><elocation-id>105313</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105313</pub-id><pub-id pub-id-type="pmid">36344304</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandal</surname><given-names>A</given-names></name><name><surname>Liesefeld</surname><given-names>AM</given-names></name><name><surname>Liesefeld</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>The surprising robustness of visual search against concurrent auditory distraction</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>50</volume><fpage>99</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1037/xhp0001168</pub-id><pub-id pub-id-type="pmid">38236258</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>DK</given-names></name><name><surname>Barry</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Does attention-deficit/hyperactivity disorder have A dimensional latent structure? A taxometric analysis</article-title><source>Journal of Abnormal Psychology</source><volume>120</volume><fpage>427</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1037/a0021405</pub-id><pub-id pub-id-type="pmid">20973595</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marius ât Hart</surname><given-names>B</given-names></name><name><surname>Vockeroth</surname><given-names>J</given-names></name><name><surname>Schumann</surname><given-names>F</given-names></name><name><surname>Bartl</surname><given-names>K</given-names></name><name><surname>Schneider</surname><given-names>E</given-names></name><name><surname>KÃ¶nig</surname><given-names>P</given-names></name><name><surname>EinhÃ¤user</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaze allocation in natural stimuli: Comparing free exploration to head-fixed viewing conditions</article-title><source>Visual Cognition</source><volume>17</volume><fpage>1132</fpage><lpage>1158</lpage><pub-id pub-id-type="doi">10.1080/13506280902812304</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masson</surname><given-names>R</given-names></name><name><surname>Bidet-Caulet</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fronto-central P3a to distracting sounds: An index of their arousing properties</article-title><source>NeuroImage</source><volume>185</volume><fpage>164</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.10.041</pub-id><pub-id pub-id-type="pmid">30336252</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauriello</surname><given-names>C</given-names></name><name><surname>Pham</surname><given-names>E</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Piguet</surname><given-names>C</given-names></name><name><surname>Deiber</surname><given-names>MP</given-names></name><name><surname>Aubry</surname><given-names>JM</given-names></name><name><surname>Dayer</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>CM</given-names></name><name><surname>Perroud</surname><given-names>N</given-names></name><name><surname>Berchio</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Dysfunctional temporal stages of eye-gaze perception in adults with ADHD: A high-density EEG study</article-title><source>Biological Psychology</source><volume>171</volume><elocation-id>108351</elocation-id><pub-id pub-id-type="doi">10.1016/j.biopsycho.2022.108351</pub-id><pub-id pub-id-type="pmid">35568095</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merrill</surname><given-names>BM</given-names></name><name><surname>Raiker</surname><given-names>JS</given-names></name><name><surname>Mattfeld</surname><given-names>AT</given-names></name><name><surname>Macphee</surname><given-names>FL</given-names></name><name><surname>Ramos</surname><given-names>MC</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Altszuler</surname><given-names>AR</given-names></name><name><surname>Schooler</surname><given-names>JW</given-names></name><name><surname>Coxe</surname><given-names>S</given-names></name><name><surname>Gnagy</surname><given-names>EM</given-names></name><name><surname>Greiner</surname><given-names>AR</given-names></name><name><surname>Coles</surname><given-names>EK</given-names></name><name><surname>Pelham</surname><given-names>WE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mind-Wandering and Childhood ADHD: experimental manipulations across laboratory and naturalistic settings</article-title><source>Research on Child and Adolescent Psychopathology</source><volume>50</volume><fpage>1139</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1007/s10802-022-00912-6</pub-id><pub-id pub-id-type="pmid">35247108</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title><source>Nature</source><volume>485</volume><fpage>233</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature11020</pub-id><pub-id pub-id-type="pmid">22522927</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesik</surname><given-names>J</given-names></name><name><surname>Wojtczak</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The effects of data quantity on performance of temporal response function analyses of natural speech processing</article-title><source>Frontiers in Neuroscience</source><volume>16</volume><elocation-id>963629</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2022.963629</pub-id><pub-id pub-id-type="pmid">36711133</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michelini</surname><given-names>G</given-names></name><name><surname>Salmastyan</surname><given-names>G</given-names></name><name><surname>Vera</surname><given-names>JD</given-names></name><name><surname>Lenartowicz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Event-related brain oscillations in attention-deficit/hyperactivity disorder (ADHD): A systematic review and meta-analysis</article-title><source>International Journal of Psychophysiology</source><volume>174</volume><fpage>29</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2022.01.014</pub-id><pub-id pub-id-type="pmid">35124111</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>SE</given-names></name><name><surname>Sanislow</surname><given-names>CA</given-names></name><name><surname>Pacheco</surname><given-names>J</given-names></name><name><surname>Vaidyanathan</surname><given-names>U</given-names></name><name><surname>Gordon</surname><given-names>JA</given-names></name><name><surname>Cuthbert</surname><given-names>BN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Revisiting the seven pillars of RDoC</article-title><source>BMC Medicine</source><volume>20</volume><elocation-id>220</elocation-id><pub-id pub-id-type="doi">10.1186/s12916-022-02414-0</pub-id><pub-id pub-id-type="pmid">35768815</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulraney</surname><given-names>M</given-names></name><name><surname>Arrondo</surname><given-names>G</given-names></name><name><surname>Musullulu</surname><given-names>H</given-names></name><name><surname>Iturmendi-Sabater</surname><given-names>I</given-names></name><name><surname>Cortese</surname><given-names>S</given-names></name><name><surname>Westwood</surname><given-names>SJ</given-names></name><name><surname>Donno</surname><given-names>F</given-names></name><name><surname>Banaschewski</surname><given-names>T</given-names></name><name><surname>Simonoff</surname><given-names>E</given-names></name><name><surname>Zuddas</surname><given-names>A</given-names></name><name><surname>DÃ¶pfner</surname><given-names>M</given-names></name><name><surname>Hinshaw</surname><given-names>SP</given-names></name><name><surname>Coghill</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Systematic review and meta-analysis: screening tools for attention-deficit/hyperactivity disorder in children and adolescents</article-title><source>Journal of the American Academy of Child &amp; Adolescent Psychiatry</source><volume>61</volume><fpage>982</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.1016/j.jaac.2021.11.031</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>AL</given-names></name><name><surname>Booth</surname><given-names>T</given-names></name><name><surname>Ribeaud</surname><given-names>D</given-names></name><name><surname>Eisner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Disagreeing about development: An analysis of parent-teacher agreement in ADHD symptom trajectories across the elementary school years</article-title><source>International Journal of Methods in Psychiatric Research</source><volume>27</volume><elocation-id>e1723</elocation-id><pub-id pub-id-type="doi">10.1002/mpr.1723</pub-id><pub-id pub-id-type="pmid">29845677</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NÃ¤Ã¤tÃ¤nen</surname><given-names>R</given-names></name><name><surname>Picton</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The N1 wave of the human electric and magnetic response to sound: A review and an analysis of the component structure</article-title><source>Psychophysiology</source><volume>24</volume><fpage>375</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1987.tb00311.x</pub-id><pub-id pub-id-type="pmid">3615753</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narad</surname><given-names>ME</given-names></name><name><surname>Garner</surname><given-names>AA</given-names></name><name><surname>Peugh</surname><given-names>JL</given-names></name><name><surname>Tamm</surname><given-names>L</given-names></name><name><surname>Antonini</surname><given-names>TN</given-names></name><name><surname>Kingery</surname><given-names>KM</given-names></name><name><surname>Simon</surname><given-names>JO</given-names></name><name><surname>Epstein</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Parent-teacher agreement on ADHD symptoms across development</article-title><source>Psychological Assessment</source><volume>27</volume><fpage>239</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1037/a0037864</pub-id><pub-id pub-id-type="pmid">25222436</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NeguÈ</surname><given-names>A</given-names></name><name><surname>Jurma</surname><given-names>AM</given-names></name><name><surname>David</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual-reality-based attention assessment of ADHD: ClinicaVR: Classroom-CPT versus a traditional continuous performance test</article-title><source>Child Neuropsychology</source><volume>23</volume><fpage>692</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1080/09297049.2016.1186617</pub-id><pub-id pub-id-type="pmid">27258210</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nissens</surname><given-names>T</given-names></name><name><surname>Failing</surname><given-names>M</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>People look at the object they fear: oculomotor capture by stimuli that signal threat</article-title><source>Cognition &amp; Emotion</source><volume>31</volume><fpage>1707</fpage><lpage>1714</lpage><pub-id pub-id-type="doi">10.1080/02699931.2016.1248905</pub-id><pub-id pub-id-type="pmid">27797292</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palva</surname><given-names>S</given-names></name><name><surname>Palva</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>New vistas for alpha-frequency band oscillations</article-title><source>Trends in Neurosciences</source><volume>30</volume><fpage>150</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2007.02.001</pub-id><pub-id pub-id-type="pmid">17307258</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhurst</surname><given-names>D</given-names></name><name><surname>Law</surname><given-names>K</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Modeling the role of salience in the allocation of overt visual attention</article-title><source>Vision Research</source><volume>42</volume><fpage>107</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00250-4</pub-id><pub-id pub-id-type="pmid">11804636</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parsons</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Virtual reality for enhanced ecological validity and experimental control in the clinical, affective and social neurosciences</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>660</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00660</pub-id><pub-id pub-id-type="pmid">26696869</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parsons</surname><given-names>TD</given-names></name><name><surname>Duffield</surname><given-names>T</given-names></name><name><surname>Asbee</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A comparison of virtual reality classroom continuous performance tests to traditional continuous performance tests in delineating ADHD: A meta-analysis</article-title><source>Neuropsychology Review</source><volume>29</volume><fpage>338</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1007/s11065-019-09407-6</pub-id><pub-id pub-id-type="pmid">31161465</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peisch</surname><given-names>V</given-names></name><name><surname>Rutter</surname><given-names>T</given-names></name><name><surname>Wilkinson</surname><given-names>CL</given-names></name><name><surname>Arnett</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensory processing and P300 event-related potential correlates of stimulant response in children with attention-deficit/hyperactivity disorder: A critical review</article-title><source>Clinical Neurophysiology</source><volume>132</volume><fpage>953</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2021.01.015</pub-id><pub-id pub-id-type="pmid">33677205</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>EB</given-names></name><name><surname>WÃ¶stmann</surname><given-names>M</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Lunner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural tracking of attended versus ignored speech is differentially affected by hearing loss</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>18</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1152/jn.00527.2016</pub-id><pub-id pub-id-type="pmid">27707813</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollak</surname><given-names>Y</given-names></name><name><surname>Weiss</surname><given-names>PL</given-names></name><name><surname>Rizzo</surname><given-names>AA</given-names></name><name><surname>Weizer</surname><given-names>M</given-names></name><name><surname>Shriki</surname><given-names>L</given-names></name><name><surname>Shalev</surname><given-names>RS</given-names></name><name><surname>Gross-Tsur</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The utility of a continuous performance test embedded in virtual reality in measuring ADHD-related deficits</article-title><source>Journal of Developmental and Behavioral Pediatrics</source><volume>30</volume><fpage>2</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1097/DBP.0b013e3181969b22</pub-id><pub-id pub-id-type="pmid">19194324</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Orienting of attention</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>32</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/00335558008248231</pub-id><pub-id pub-id-type="pmid">7367577</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>DL</given-names></name><name><surname>Murray</surname><given-names>DW</given-names></name><name><surname>Rosen</surname><given-names>L</given-names></name><name><surname>Hardy</surname><given-names>K</given-names></name><name><surname>Skinner</surname><given-names>A</given-names></name><name><surname>Underwood</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Instability in teacher ratings of childrenâs inattentive symptoms</article-title><source>Journal of Developmental &amp; Behavioral Pediatrics</source><volume>31</volume><fpage>175</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1097/DBP.0b013e3181d5a2d8</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rad</surname><given-names>MS</given-names></name><name><surname>Martingano</surname><given-names>AJ</given-names></name><name><surname>Ginges</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Toward a psychology of <italic>Homo sapiens</italic>: Making psychological science more representative of the human population</article-title><source>PNAS</source><volume>115</volume><fpage>11401</fpage><lpage>11405</lpage><pub-id pub-id-type="doi">10.1073/pnas.1721165115</pub-id><pub-id pub-id-type="pmid">30397114</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Risko</surname><given-names>EF</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Eyes wide shut: implied social presence, eye tracking and attention</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>73</volume><fpage>291</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.3758/s13414-010-0042-1</pub-id><pub-id pub-id-type="pmid">21264723</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Risko</surname><given-names>EF</given-names></name><name><surname>Anderson</surname><given-names>N</given-names></name><name><surname>Sarwal</surname><given-names>A</given-names></name><name><surname>Engelhardt</surname><given-names>M</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Everyday attention: variation in mind wandering and memory in a Lecture</article-title><source>Applied Cognitive Psychology</source><volume>26</volume><fpage>234</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1002/acp.1814</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzo</surname><given-names>ASA</given-names></name><name><surname>Bowerly</surname><given-names>T</given-names></name><name><surname>Buckwalter</surname><given-names>JG</given-names></name><name><surname>Klimchuk</surname><given-names>D</given-names></name><name><surname>Mitura</surname><given-names>R</given-names></name><name><surname>Parsons</surname><given-names>TD</given-names></name><name><surname>Bowerly</surname><given-names>T</given-names></name><name><surname>Buckwalter</surname><given-names>JG</given-names></name><name><surname>Rizzo</surname><given-names>ASA</given-names></name><name><surname>Adams</surname><given-names>R</given-names></name><name><surname>Finn</surname><given-names>P</given-names></name><name><surname>Moes</surname><given-names>E</given-names></name><name><surname>Flannery</surname><given-names>K</given-names></name><name><surname>Rizzo</surname><given-names>ASA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A Virtual reality scenario for all seasons: <italic>The virtual classroom</italic></article-title><source>CNS Spectrums</source><volume>11</volume><fpage>35</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1017/S1092852900024196</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SanMiguel</surname><given-names>I</given-names></name><name><surname>Morgan</surname><given-names>HM</given-names></name><name><surname>Klein</surname><given-names>C</given-names></name><name><surname>Linden</surname><given-names>D</given-names></name><name><surname>Escera</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>On the functional significance of Novelty-P3: facilitation by unexpected novel sounds</article-title><source>Biological Psychology</source><volume>83</volume><fpage>143</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2009.11.012</pub-id><pub-id pub-id-type="pmid">19963034</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schomaker</surname><given-names>J</given-names></name><name><surname>Walper</surname><given-names>D</given-names></name><name><surname>Wittmann</surname><given-names>BC</given-names></name><name><surname>EinhÃ¤user</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention in natural scenes: Affective-motivational factors guide gaze independently of visual salience</article-title><source>Vision Research</source><volume>133</volume><fpage>161</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2017.02.003</pub-id><pub-id pub-id-type="pmid">28279712</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartze</surname><given-names>M</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The timing of regular sequences: production, perception, and covariation</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>1697</fpage><lpage>1707</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00805</pub-id><pub-id pub-id-type="pmid">25803600</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweitzer</surname><given-names>JB</given-names></name><name><surname>Rizzo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Virtual reality and ADHD: clinical assessment and treatment in the metaverse</article-title><source>The ADHD Report</source><volume>30</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1521/adhd.2022.30.3.1</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweitzer</surname><given-names>JB</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Editorial: the use of continuous performance tasks in attention-deficit/hyperactivity disorder diagnosis: a cautionary note</article-title><source>Journal of the American Academy of Child and Adolescent Psychiatry</source><volume>63</volume><fpage>114</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.jaac.2023.06.012</pub-id><pub-id pub-id-type="pmid">37402465</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SeesjÃ¤rvi</surname><given-names>E</given-names></name><name><surname>Puhakka</surname><given-names>J</given-names></name><name><surname>Aronen</surname><given-names>ET</given-names></name><name><surname>Lipsanen</surname><given-names>J</given-names></name><name><surname>Mannerkoski</surname><given-names>M</given-names></name><name><surname>Hering</surname><given-names>A</given-names></name><name><surname>Zuber</surname><given-names>S</given-names></name><name><surname>Kliegel</surname><given-names>M</given-names></name><name><surname>Laine</surname><given-names>M</given-names></name><name><surname>Salmi</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Quantifying ADHD symptoms in open-ended everyday life contexts with a new virtual reality task</article-title><source>Journal of Attention Disorders</source><volume>26</volume><fpage>1394</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1177/10870547211044214</pub-id><pub-id pub-id-type="pmid">34865551</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selaskowski</surname><given-names>B</given-names></name><name><surname>AschÃ©</surname><given-names>LM</given-names></name><name><surname>Wiebe</surname><given-names>A</given-names></name><name><surname>Kannen</surname><given-names>K</given-names></name><name><surname>Aslan</surname><given-names>B</given-names></name><name><surname>Gerding</surname><given-names>TM</given-names></name><name><surname>Sanchez</surname><given-names>D</given-names></name><name><surname>Ettinger</surname><given-names>U</given-names></name><name><surname>KÃ¶lle</surname><given-names>M</given-names></name><name><surname>Lux</surname><given-names>S</given-names></name><name><surname>Philipsen</surname><given-names>A</given-names></name><name><surname>Braun</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Gaze-based attention refocusing training in virtual reality for adult attention-deficit/hyperactivity disorder</article-title><source>BMC Psychiatry</source><volume>23</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.1186/s12888-023-04551-z</pub-id><pub-id pub-id-type="pmid">36703134</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sergeant</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The cognitive-energetic model: an empirical approach to attention-deficit hyperactivity disorder</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>24</volume><fpage>7</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/s0149-7634(99)00060-3</pub-id><pub-id pub-id-type="pmid">10654654</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Fishman</surname><given-names>DJ</given-names></name><name><surname>Schooler</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Counting the cost of an absent mind: mind wandering as an underrecognized influence on educational performance</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>14</volume><fpage>230</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.3758/bf03194057</pub-id><pub-id pub-id-type="pmid">17694906</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonuga-Barke</surname><given-names>EJS</given-names></name><name><surname>Becker</surname><given-names>SP</given-names></name><name><surname>BÃ¶lte</surname><given-names>S</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name><name><surname>Franke</surname><given-names>B</given-names></name><name><surname>Newcorn</surname><given-names>JH</given-names></name><name><surname>Nigg</surname><given-names>JT</given-names></name><name><surname>Rohde</surname><given-names>LA</given-names></name><name><surname>Simonoff</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Annual Research Review: Perspectives on progress in ADHD science - from characterization to cause</article-title><source>Journal of Child Psychology and Psychiatry, and Allied Disciplines</source><volume>64</volume><fpage>506</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1111/jcpp.13696</pub-id><pub-id pub-id-type="pmid">36220605</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>JD</given-names></name><name><surname>Rizzo</surname><given-names>A</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name><name><surname>Schweitzer</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Measuring attentional distraction in children With ADHD using virtual reality technology with eye-tracking</article-title><source>Frontiers in Virtual Reality</source><volume>3</volume><elocation-id>55895</elocation-id><pub-id pub-id-type="doi">10.3389/frvir.2022.855895</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szpunar</surname><given-names>KK</given-names></name><name><surname>Moulton</surname><given-names>ST</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mind wandering and education: from the classroom to online learning</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>495</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00495</pub-id><pub-id pub-id-type="pmid">23914183</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Team</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>JASP</data-title><version designator="version 0.19.0">version 0.19.0</version><source>Jasp-Stats</source><ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/download/">https://jasp-stats.org/download/</ext-link></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomson</surname><given-names>DR</given-names></name><name><surname>Besner</surname><given-names>D</given-names></name><name><surname>Smilek</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A resource-control account of sustained attention</article-title><source>Perspectives on Psychological Science</source><volume>10</volume><fpage>82</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1177/1745691614556681</pub-id><pub-id pub-id-type="pmid">25910383</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toplak</surname><given-names>ME</given-names></name><name><surname>West</surname><given-names>RF</given-names></name><name><surname>Stanovich</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Practitioner review: do performance-based measures and ratings of executive function assess the same construct?</article-title><source>Journal of Child Psychology and Psychiatry, and Allied Disciplines</source><volume>54</volume><fpage>131</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1111/jcpp.12001</pub-id><pub-id pub-id-type="pmid">23057693</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>TÃ¼rkan</surname><given-names>BN</given-names></name><name><surname>Amado</surname><given-names>S</given-names></name><name><surname>Ercan</surname><given-names>ES</given-names></name><name><surname>PerÃ§inel</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of change detection performance and visual search patterns among children with/without ADHD: Evidence from eye movements</article-title><source>Research in Developmental Disabilities</source><volume>49â50</volume><fpage>205</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.ridd.2015.12.002</pub-id><pub-id pub-id-type="pmid">26707929</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turoman</surname><given-names>N</given-names></name><name><surname>Vergauwe</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>The effect of multisensory distraction on working memory: A role for task relevance?</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>50</volume><fpage>1220</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1037/xlm0001323</pub-id><pub-id pub-id-type="pmid">38127491</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vakil</surname><given-names>E</given-names></name><name><surname>Mass</surname><given-names>M</given-names></name><name><surname>Schiff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Eye movement performance on the stroop test in adults with ADHD</article-title><source>Journal of Attention Disorders</source><volume>23</volume><fpage>1160</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1177/1087054716642904</pub-id><pub-id pub-id-type="pmid">27060499</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Decruy</surname><given-names>L</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effect of task and attention on neural tracking of speech</article-title><source>Frontiers in Neuroscience</source><volume>13</volume><elocation-id>977</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2019.00977</pub-id><pub-id pub-id-type="pmid">31607841</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wetzel</surname><given-names>N</given-names></name><name><surname>SchrÃ¶ger</surname><given-names>E</given-names></name><name><surname>Widmann</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The dissociation between the P3a event-related potential and behavioral distraction</article-title><source>Psychophysiology</source><volume>50</volume><fpage>920</fpage><lpage>930</lpage><pub-id pub-id-type="doi">10.1111/psyp.12072</pub-id><pub-id pub-id-type="pmid">23763292</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woldorff</surname><given-names>MG</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Modulation of early auditory processing during selective listening to rapidly presented tones</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>79</volume><fpage>170</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(91)90136-r</pub-id><pub-id pub-id-type="pmid">1714809</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>WÃ¶stmann</surname><given-names>M</given-names></name><name><surname>Lim</surname><given-names>SJ</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The human neural alpha response to speech is a proxy of attentional control</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>3307</fpage><lpage>3317</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx074</pub-id><pub-id pub-id-type="pmid">28334352</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Makita</surname><given-names>K</given-names></name><name><surname>Nakao</surname><given-names>T</given-names></name><name><surname>Kanayama</surname><given-names>N</given-names></name><name><surname>Machizawa</surname><given-names>MG</given-names></name><name><surname>Sasaoka</surname><given-names>T</given-names></name><name><surname>Sugata</surname><given-names>A</given-names></name><name><surname>Kobayashi</surname><given-names>R</given-names></name><name><surname>Hiramoto</surname><given-names>R</given-names></name><name><surname>Yamawaki</surname><given-names>S</given-names></name><name><surname>Iwanaga</surname><given-names>M</given-names></name><name><surname>Miyatani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Affective auditory stimulus database: An expanded version of the International Affective Digitized Sounds (IADS-E)</article-title><source>Behavior Research Methods</source><volume>50</volume><fpage>1415</fpage><lpage>1429</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1027-6</pub-id><pub-id pub-id-type="pmid">29520632</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmer</surname><given-names>H</given-names></name><name><surname>Richter</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Novelty detection and orienting: effects on skin conductance and heart rate</article-title><source>Psychological Research</source><volume>87</volume><fpage>1101</fpage><lpage>1113</lpage><pub-id pub-id-type="doi">10.1007/s00426-022-01735-2</pub-id><pub-id pub-id-type="pmid">36107250</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>EM</given-names></name><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Goodman</surname><given-names>RR</given-names></name><name><surname>Emerson</surname><given-names>R</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a âcocktail partyâ</article-title><source>Neuron</source><volume>77</volume><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id><pub-id pub-id-type="pmid">23473326</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohar</surname><given-names>AH</given-names></name><name><surname>Konfortes</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Diagnosing ADHD in Israeli adults: the psychometric properties of the adult ADHD Self Report Scale (ASRS) in Hebrew</article-title><source>The Israel Journal of Psychiatry and Related Sciences</source><volume>47</volume><fpage>308</fpage><lpage>315</lpage><pub-id pub-id-type="pmid">21270505</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103235.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ding</surname><given-names>Nai</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Zhejiang University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study investigates how AD(H)D affects attention using neural and physiological measures in a Virtual Reality (VR) environment. <bold>Solid</bold> evidence is provided that individuals diagnosed with AD(H)D differ from control participants in both the encoding of the target sound and the encoding of acoustic interference. The VR paradigm here can potentially bridge lab experiments and real-life experiments. The study is of potential interests to researchers who are interested in auditory cognition, education, and ADHD.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103235.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This is an interesting study on AD(H)D. The authors combine a variety of neural and physiological metrics to study attention in a VR classroom setting. The manuscript is well written and the results are interesting, ranging from an effect of group (AD(H)D vs. control) on metrics such as envelope tracking, to multivariate regression analyses considering alpha-power, gaze, TRF, ERPs, and behaviour simultaneously. I find the first part of the results clear and strong. The multivariate analyses in Tables 1 and 2 are good ideas, but I think they would benefit from additional clarifications. Overall, I think that the methodological approach is useful in itself. The rest is interesting in that it informs us on which metrics are sensitive to group-effects and correlated with each other. I think this might be one interesting way forward. Indeed, much more work is needed to clarify how these results change with different stimuli and tasks. So, I see this as an interesting first step into more naturalistic measurement of speech attention.</p><p>Strengths:</p><p>I praise the authors for this interesting attempt to tackle a challenging topic with naturalistic experiment and metrics. I think the results broadly make sense and they contribute to a complex literature that is far from being linear and cohesive.</p><p>Weaknesses:</p><p>The authors have successfully addressed most of my concerns during the review process. Some weaknesses remain in this resubmission, but they do not make the results invalid. For example:</p><p>- The EEG data was filtered twice, which is not recommended as that can introduce additional filtering artifacts. So, while I definitely do not recommend doing that, I do not expect that issue to have an impact on this specific result.</p><p>- The authors did not check whether participants were somewhat familiar with the topics in the speech material. The authors agreed that this point might be beneficial for future research.</p><p>- The hyperparameter tuning is consistent with previous work from the authors, and it involves selecting the optimal lambda value of the regularized regression based on the group average, thus choosing a single lambda value for all participants. In my opinion, that is not the optimal way to run those models, and I do not generally recommend using this approach. The reason is that the lambda can change depending on the magnitude of the signals and the SNR, leading to different optimal lambdas for distinct participants. On the other hand, finding those optimal lambda values for individual participants can be difficult depending on the amount of data and amount of noise, so it is sometimes necessary to apply strategies that ensure an appropriate choice of lambda. Using the group average as a metric for hyperparameter tuning produces a more stable metric and lambda value selection, which might be preferrable (even though this choice should not be taken lightly). In this specific case, I think the authors had a good reason to do so.</p><p>Comments on revisions:</p><p>The authors have done a great job at addressing my comments. I don't have any further concerns. Congratulations!</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103235.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>While selective attention is a crucial ability of human beings, previous studies on selective attention are primarily conducted in a strictly controlled context, leaving a notable gap in underlying the complexity and dynamic nature of selective attention in a naturalistic context. This issue is particularly important for classroom learning in individuals with ADHD, as selecting the target and ignoring the distractions are pretty difficult for them but are the pre-requirement of effective learning. The authors of this study have addressed this challenge using a well-motivated study. I believe the findings of this study will be a nice addition to the fields both cognitive neuroscience and educational neuroscience.</p><p>Strengths:</p><p>To achieve the purpose of setting up a naturalistic context, the authors have based their study on a novel Virtual Reality platform. This is clever as it is usually difficult to perform such a study in the real classroom. Moreover, various techniques such as brain imaging, eye-tracking and physiological measurement are combined to collect multi-level data. They found that, different from the controls, individuals with ADHD had higher neural responses to the irrelevant rather than the target sounds, reduced speech tracking of the teacher. Additionally, the power of alpha-oscillations and frequency of gaze-shifts away from the teacher are found to be associated with the ADHD symptoms. These results provide new insights into the mechanism of selective attention among ADHD populations.</p><p>Weaknesses:</p><p>It is worth noting that nowadays there has been some studies trying to do so in the real classroom, and thus the authors should acknowledge the difference between the virtual and real classroom context and foresee the potential future changes.</p><p>The approach of combining multi-level data owns advantage to obtain reliable results, but also raises significant difficult for the readers to understand the main results.</p><p>- An appraisal of whether the authors achieved their aims, and whether the results support their conclusions.</p><p>As expected, individuals with ADHD showed anomalous pattern of neural responses, and eye-tracking pattern, compared to the controls. But there are also some similarities between groups such as amount of time paying attention to teachers, etc. In general, their conclusions are supported.</p><p>- A discussion of the likely impact of the work on the field, and the utility of the methods and data to the community.</p><p>The findings are an extension of previous efforts in understanding selective attention in the naturalistic context. The findings of this study are particularly helpful in inspiring teacher's practice and advancing the research of educational neuroscience. This study demonstrates, again, that it is important to understand the complexity of cognitive process in the naturalistic context.</p><p>Comments on revisions:</p><p>The authors have appropriately responded to my concerns. I do not have other comments. I do hope to see more data and results from the authors in future.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103235.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors conducted a well-designed experiment, incorporating VR classroom scenes and background sound events, with both control and ADHD participants. They employed multiple neurophysiological measures, such as EEG, eye movements, and skin conductance, to investigate the mechanistic underpinnings of paying attention in class and the disruptive effects of background noise.</p><p>The results revealed that individuals with ADHD exhibited heightened sensory responses to irrelevant sounds and reduced tracking of the teacher's speech. Overall, this manuscript presented an ecologically valid paradigm for assessing neurophysiological responses in both control and ADHD groups. The analyses were comprehensive and clear, making the study potentially valuable for the application of detecting attentional deficits.</p><p>Strengths:</p><p>â¢ The VR learning paradigm is well-designed and ecologically valid.</p><p>â¢ The neurophysiological metrics and analyses are comprehensive, and two physiological markers are identified capable of diagnosing ADHD.</p><p>â¢ The data shared could serve as a benchmark for future studies on attention deficits in ecologically valid scenarios.</p><p>Weaknesses:</p><p>â¢ Several results are null results, i.e., no significant differences were found between ADHD and control populations.</p><p>Comments on revisions:</p><p>The authors have addressed all of my concerns with the original manuscript.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103235.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Levy</surname><given-names>Orel</given-names></name><role specific-use="author">Author</role><aff><institution>Bar-Ilan University</institution><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Libman Hackmon</surname><given-names>Shirley</given-names></name><role specific-use="author">Author</role><aff><institution>Bar-Ilan University</institution><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Zvilichovsky</surname><given-names>Yair</given-names></name><role specific-use="author">Author</role><aff><institution>Bar-Ilan University</institution><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Korisky</surname><given-names>Adi</given-names></name><role specific-use="author">Author</role><aff><institution>Bar-Ilan University</institution><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Bidet-Caulet</surname><given-names>Aurelie</given-names></name><role specific-use="author">Author</role><aff><institution>Aix Marseille Univ, Inserm</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Schweitzer</surname><given-names>Julie B</given-names></name><role specific-use="author">Author</role><aff><institution>University of California Davis</institution><addr-line><named-content content-type="city">Sacramento</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Zion Golumbic</surname><given-names>Elana</given-names></name><role specific-use="author">Author</role><aff><institution>Bar-Ilan University</institution><addr-line><named-content content-type="city">Ramat Gan</named-content></addr-line><country>Israel</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>(1) Line numbers are missing.</p></disp-quote><p>Added</p><disp-quote content-type="editor-comment"><p>(2) VR classroom. Was this a completely custom design based on Unity, or was this developed on top of some pre-existing code? Many aspects of the VR classroom scenario are only introduced (e.g., how was the lip-speech synchronisation done exactly?). Additional detail is required. Also, is or will the experiment code be shared publicly with appropriate documentation? It would also be useful to share brief example video-clips.</p></disp-quote><p>We have added details about the VR classroom programming to the methods section (p. 6-7), and we have now included a video-example as supplementary material.</p><p>âDevelopment and programming of the VR classroom were done primarily in-house, using assets (avatars and environment) were sourced from pre-existing databases. The classroom environment was adapted from assets provided by Tirgames on TurboSquid (<ext-link ext-link-type="uri" xlink:href="https://www.turbosquid.com/Search/Artists/Tirgames">https://www.turbosquid.com/Search/Artists/Tirgames</ext-link>) and modified to meet the experimental needs. The avatars and their basic animations were sourced from the Mixamo library, which at the time of development supported legacy avatars with facial blendshapes (this functionality is no longer available in current versions of Mixamo). A brief video example of the VR classroom is available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/rf6t8">https://osf.io/rf6t8</ext-link>.</p><p>âTo achieve realistic lip-speech synchronization, the teacherâs lip movements were controlled by the temporal envelope of the speech, adjusting both timing and mouth size dynamically. His body motions were animated using natural talking gestures.â</p><p>While we do intent to make the dataset publicly available for other researchers, at this point we are not making the code for the VR classroom public. However, we are happy to share it on an individual-basis with other researchers who might find it useful for their own research in the future.</p><disp-quote content-type="editor-comment"><p>(3) &quot;normalized to the same loudness level using the software Audacity&quot;. Please specify the Audacity function and parameters.</p></disp-quote><p>We have added these details (p.7)</p><p>âAll sound-events were normalized to the same loudness level using the Normalize function in the audio-editing software Audacity (theaudacityteam.org, ver 3.4), with the peak amplitude parameter set to -5 dB, and trimmed to a duration of 300 milliseconds.â</p><disp-quote content-type="editor-comment"><p>(4) Did the authors check if the participants were already familiar with some of the content in the mini-lectures?</p></disp-quote><p>This is a good point. Since the mini-lectures spanned many different topics, we did not pre-screen participants for familiarity with the topics, and it is possible that some of the participants had some pre-existing knowledge.</p><p>In hindsight, it would have been good to have added some reflective questions regarding participants prior knowledge as well as other questions such as level of interest in the topic and/or how well they understood the content. These are elements that we hope to include in future versions of the VR classroom.</p><disp-quote content-type="editor-comment"><p>(5) &quot;Independent Component Analysis (ICA) was then used to further remove components associated with horizontal or vertical eye movements and heartbeats&quot;. Please specify how this selection was carried out.</p></disp-quote><p>Selection of ICA components was done manually based on visual inspection of their time-course patterns and topographical distributions, to identify components characteristic of blinks, horizontal eye-movements and heartbeats. Examples of these distinct components are provided in Author response image 1 below. These is now specified in the methods section.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-sa4-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(6) &quot;EEG data was further bandpass filtered between 0.8 and 20 Hz&quot;. If I understand correctly, the data was filtered a second time. If that's the case, please do not do that, as that will introduce additional and unnecessary filtering artifacts. Instead, the authors should replace the original filter with this one (so, filtering the data only once). Please see de Cheveigne and Nelkn, Neuron, 2019 for an explanation. Also, please provide an explanation of the rationale for further restricting the cut-off bands in the methods section. Finally, further details on the filters should be included (filter type and order, for example).</p></disp-quote><p>Yes, the data was indeed filtered twice. The first filter is done as part of the preprocessing procedure, in order to remove extremely high- and low- frequency noise but retain most activity within the range of âneuralâ activity. This broad range is mostly important for the ICA procedure, so as to adequately separate between ocular and neural contribution to the recorded signal.</p><p>However, since both the speech tracking responses and ERPs are typically less broadband and are comprised mostly of lower frequencies (e.g., those that make up the speech-envelope), a second narrower filter was applied to improve TRF model-fit and make ERPs more interpretable.</p><p>In both cases we used a fourth order zero-phase Butterworth IIR filter with 1-seconds of padding, as implemented in the Fieldtrip toolbox. We have added these details to the manuscript.</p><disp-quote content-type="editor-comment"><p>(7) &quot;(~ 5 minutes of data in total), which is insufficient for deriving reliable TRFs&quot;. That is a bit pessimistic and vague. What does &quot;reliable&quot; mean? I would tend to agree when talking about individual subject TRFs, which 5 min per participant can be enough at the group level. Also, this depends on the specific speech material. If the features are univariate or multivariate. Etc. Please narrow down and clarify this statement.</p></disp-quote><p>We determined that the data in the Quiet condition (~5 min) was insufficient for performing reliable TRF analysis, by assessing whether its predictive-power was significantly better than chance. As shown in Author response image 2 below, the predictive power achieved using this data was not higher than values obtained in permuted data (p = 0.43). Therefore, we did not feel that it was appropriate to include TRF analysis of the Quiet condition in this manuscript. We have now clarified this in the manuscript (p. 10)</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-sa4-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(8) &quot;Based on previous research in by our group (Kaufman &amp; Zion Golumbic 2023), we chose to use a constant regularization ridge parameter (Î» = 100) for all participants and conditions&quot;. This is an insufficient explanation. I understand that there is a previous paper involved. However, such an unconventional choice that goes against the original definition and typical use of these methods should be clearly reported in this manuscript.</p></disp-quote><p>We apologize for not clarifying this point sufficiently, and have added an explanation of this methodological choice (p.11):</p><p>âThe mTRF toolbox uses a ridge-regression approach for L2 regularization of the model to ensure better generalization to new data. We tested a range of ridge parameter values (Î»'s) and used a leave-one-out cross-validation procedure to assess the modelâs predictive power, whereby in each iteration, all but one trials are used to train the model, and it is then applied to the left-out trial. The predictive power of the model (for each Î») is estimated as the Pearsonâs correlation between the predicted neural responses and the actual neural responses, separately for each electrode, averages across all iterations. We report results of the model with the Î» the yielded the highest predictive power at the group-level (rather than selecting a different Î» for each participant which can lead to incomparable TRF models across participants; see discussion in Kaufman &amp; Zion Golumbic 2023).â</p><disp-quote content-type="editor-comment"><p>Assuming that the explanation will be sufficiently convincing, which is not a trivial case to make, the next issue that I will bring up is that the lambda value depends on the magnitude of input and output vectors. While the input features are normalised, I don't see that described for the EEG signals. So I assume they are not normalized. In that case, the lambda would have at least to be adapted between subjects to account for their different magnitude.</p></disp-quote><p>We apologize for omitting this detail â yes, the EEG signals were normalized prior to conducting the TRF analysis. We have updated the methods section to explicitly state this pre-processing step (p.10).</p><disp-quote content-type="editor-comment"><p>Another clarification, is that value (i.e., 100) would not be comparable either across subjects or across studies. But maybe the authors have a simple explanation for that choice? (note that this point is very important as this could lead others to use TRF methods in an inappropriate way - but I understand that the authors might have specific reasons to do so here). Note that, if the issue is finding a reliable lambda per subject, a more reasonable choice would be to use a fixed lambda selected on a generic (i.e., group-level) model. However selecting an arbitrary lambda could be problematic (e.g., would the results replicate with another lambda; and similarly, what if a different EEG system was used, with different overall magnitude, hence the different impact of the regularisation).</p></disp-quote><p>We fully agree that selecting an arbitrary lambda is problematic (esp across studies). As clarified above, the group-level lambda chosen here for the encoding more was data-driven, optimized based on group-level predictive power.</p><disp-quote content-type="editor-comment"><p>(9) &quot;L2 regularization of the model, to reduce its complexity&quot;. Could the authors explain what &quot;reduce its complexity&quot; refers to?</p></disp-quote><p>Our intension here was to state that the L2 regularization constrains the modelâs weights so that it can better generalize between to left-out data. However, for clarity we have now removed this statement.</p><disp-quote content-type="editor-comment"><p>(10) The same lambda value was used for the decoding model. From personal experience, that is very unlikely to be the optimal selection. Decoding models typically require a different (usually larger) lambda than forward models, which can be due to different reasons (different SNR of &quot;input&quot; of the model and, crucially, very different dimensionality).</p></disp-quote><p>We agree with the reviewer that treatment of regularization parameters might not be identical for encoding and decoding models. Our initial search of lambda parameters was limited to Î» = 0.01 - 100, with Î» = 100 showing the best reconstruction correlations. However, following the reviewerâs suggestion we have now broadened the range and found that, in fact reconstruction correlations are further improved and the best lambda is Î» = 1000 (see Author response image 3 below, left panel). Importantly, the difference in decoding reconstruction correlations between the groups is maintained regardless of the choice of lambda (although the effect-size varies; see Author response image 3, right panel). We have now updated the text to reflect results of the model with Î» = 1000.</p><fig id="sa4fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-sa4-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(11) Skin conductance analysis. Additional details are required. For example, how was the linear interpolation done exactly? The raw data was downsampled, sure. But was an anti-aliasing filter applied? What filter exactly? What implementation for the CDA was run exactly?</p></disp-quote><p>We have added the following details to the methods section (p. 14):</p><p>âThe Skin Conductance (SC) signal was analyzed using the Ledalab MATLAB toolbox (version 3.4.9; Benedek and Kaernbach, 2010; <ext-link ext-link-type="uri" xlink:href="http://www.ledalab.de/">http://www.ledalab.de/</ext-link>) and custom-written scripts. The raw data was downsampled to 16Hz using FieldTrip's ft_resampledata function, which applies a built-in anti-aliasing low-pass filter to prevent aliasing artifacts. Data were inspected manually for any noticeable artifacts (large âjumpsâ), and if present were corrected using linear interpolation in Ledalab. A continuous decomposition analysis (CDA) was employed to separate the tonic and phasic SC responses for each participant. The CDA was conducted using the 'sdeco' mode (signal decomposition), which iteratively optimizes the separation of tonic and phasic components using the default regularization settings.â</p><disp-quote content-type="editor-comment"><p>(12) &quot;N1- and P2 peaks of the speech tracking response&quot;. Have the authors considered using the N1-P2 complex rather than the two peaks separately? Just a thought.</p></disp-quote><p>This is an interesting suggestion, and we know that this has been used sometimes in more traditional ERP literature. In this case, since neither peak was modulated across groups, we did not think this would yield different results. However, it is a good point to keep in mind for future work.</p><disp-quote content-type="editor-comment"><p>(13) Figure 4B. The ticks are missing. From what I can see (but it's hard without the ticks), the N1 seems later than in other speech-EEG tracking experiments (where is closer to ~80ms). Could the authors comment on that? Or maybe this looks similar to some of the authors' previous work?</p></disp-quote><p>We apologize for this and have added ticks to the figure.</p><p>In terms of time-course, a N1 peak at around 100ms is compatible with many of our previous studies, as well as those from other groups.</p><disp-quote content-type="editor-comment"><p>(14) Figure 4C. Strange thin vertical grey bar to remove.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>(15) Figure 4B: What about the topographies for the TRF weights? Could the authors show that for the main components?</p></disp-quote><p>Yes. The topographies of the main TRF components are similar to those of the predictive power and are compatible with auditory responses. We have added them to Figure 4B.</p><disp-quote content-type="editor-comment"><p>(16) Figure 4B: I just noticed that this is a grand average TRF. That is ok (but not ideal) only because the referencing is to the mastoids. The more appropriate way of doing this is to look at the GFP, instead, which estimates the presence of dipoles. And then look at topographies of the components. Averaging across channels makes the plotted TRF weaker and noisier. I suggest adding the GFP to the plot. Also, the colour scale in Figure 4A is deceiving, as blue is usually used for +/- in plots of the weights. While that is a heatmap, where using a single colour or even yellow to red would be less deceiving at first look. Only cosmetics, indeed. The result is interesting nonetheless!</p></disp-quote><p>We apologize for this, and agree with the reviewer that it is better not to average across EEG channels. In the revised Figure, we now show the TRFs based on the average of electrodes FC1, FC2, and FCz, which exhibited the strongest activity for the two main components.</p><p>Following the previous comment, we have also included the topographical representation of the TRF main components, to give readers a whole-head perspective of the TRF.</p><p>We have also fixed the color-scales.</p><p>We are glad that the reviewer finds this result interesting!</p><disp-quote content-type="editor-comment"><p>(17) Figure 4C. This looks like a missed opportunity. That metric shows a significant difference overall. But is that underpinned but a generally lower envelope reconstruction correlation, or by a larger deviation in those correlations (so, that metric is as for the control in some moments, but it drops more frequently due to distractibility)?</p></disp-quote><p>We understand the reviewerâs point here, and ideally would like to be able to address this in a more fine-grained analysis, for example on a trial-by-trial basis. However, the design of the current experiment was not optimized for this, in terms of (for example) number of trials, the distribution of sound-events and behavioral outcomes. We hope to be able to address this issue in our future research.</p><disp-quote content-type="editor-comment"><p>(18) I am not a fan of the term &quot;accuracy&quot; for indicating envelope reconstruction correlations. Accuracy is a term typically associated with classification. Regression models are typically measured through errors, loss, and sometimes correlations. 'Accuracy' is inaccurate (no joke intended).</p></disp-quote><p>We accept this comment and now used the term âreconstruction correlationâ.</p><disp-quote content-type="editor-comment"><p>(19) Discussion. &quot;The most robust finding in&quot;. I suggest using more precise terminology. For example, &quot;largest effect-size&quot;.</p></disp-quote><p>We agree and have changed the terminology (p. 31).</p><disp-quote content-type="editor-comment"><p>(20) &quot;individuals who exhibited higher alpha-power [...]&quot;. I probably missed this. But could the authors clarify this result? From what I can see, alpha did not show an effect on the group. Is this referring to Table 2? Could the authors elaborate on that? How does that reconcile with the non-significant effect of the group? In that same sentence, do you mean &quot;and were more likely&quot;? If that's the case, and they were more likely to report attentional difficulties, how is it that there is no group-effect when studying alpha?</p></disp-quote><p>Yes, this sentence refers to the linear regression models described in Figure 10 and in Table 2. As the reviewer correctly points out, this is one place where there is a discrepancy between the results of the between-group analysis (ADHD diagnosis yes/no) and the regression analysis, which treats ADHD symptoms as a continuum, across both groups. The same is true for the gaze-shift data, which also did not show a significance between-group effect but was identified in the regression analysis as contributing to explaining the variance in ADHD symptoms.</p><p>We discuss this point on pages 30-31, noting that âalthough the two groups are clearly separable from each other, they are far from uniform in the severity of symptoms experiencedâ, which motivated the inclusion of both analyses in this paper.</p><p>At the bottom of p. 31 we specifically address the similarities and differences between the between-group and regression-based results. In our opinion, this pattern emphasizes that while neither approach is âconclusiveâ, looking at the data through both lenses contributes to an overall better understanding of the contributing factors, as well as highlighting that âno single neurophysiological measure alone is sufficient for explaining differences between the individuals â whether through the lens of clinical diagnosis or through report of symptomsâ.</p><disp-quote content-type="editor-comment"><p>(21) &quot;why in the latter case the neural speech-decoding accuracy did not contribute to explaining ASRS scores [...]&quot;. My previous point 1 on separating overall envelope decoding from its deviation could help there. The envelope decoding correlation might go up and down due to SNR, while you might be more interested in the dynamics over time (i.e., looking at the reconstructions over time).</p></disp-quote><p>Again, we appreciate this comment, but believe that this additional analysis is outside the scope of what would be reliably-feasible with the current dataset. However, since the data will be made publicly available, perhaps other researchers will have better ideas as to how to do this.</p><disp-quote content-type="editor-comment"><p>(22) Data and code sharing should be discussed. Also, specific links/names and version numbers should be included for the various libraries used.</p></disp-quote><p>We are currently working on organizing the data to make it publicly available on the Open Science Project.</p><p>We have updated links and version numbers for the various toolboxes/software used, throughout the manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>(1) While it is highly appreciated to study selective attention in a naturalistic context, the readers would expect to see whether there are any potential similarities or differences in the cognitive and neural mechanisms between contexts. Whether the classic findings about selective attention would be challenged, rebutted, or confirmed? Whether we should expect any novel findings in such a novel context? Moreover, there are some studies on selective attention in the naturalistic context though not in the classroom, it would be better to formulate specific hypotheses based on previous findings both in the strictly controlled and naturalistic contexts.</p></disp-quote><p>Yes, we fully agree that comparing results across different contexts would be extremely beneficial and important.</p><p>The current paper serves as an important proof-first-concept demonstrating the plausibility and scientific potential of using combined EEG-VR-eyetracking to study neurophysiological aspects of attention and distractibility, but is also the basis for formulating specific hypothesis that will be tested in follow-up studies.</p><p>If fact, a follow up study is already ongoing in our lab, where we are looking into this point, by testing users in different VR scenarios (e.g., classroom, cafÃ©, office etc.), and assessing whether similar neurophysiological patterns are observed across contexts and to what degree they are replicable within and across individuals. We hope to share these data with the community in the near future.</p><disp-quote content-type="editor-comment"><p>(2) Previous studies suggest handedness and hemispheric dominance might impact the processing of information in each hemisphere. Whether these issues have been taken into consideration and appropriately addressed?</p></disp-quote><p>This is an interesting point. In this study we did not specifically control for handedness/hemispheric dominance, since most of the neurophysiological measured used here are sensory/auditory in their nature, and therefore potentially invariant to handedness. Moreover, the EEG signal is typically not very sensitive to hemispheric dominance, at least for the measures used here. However, this might be something to consider more explicitly in future studies. Nonetheless, we have added handedness information to the Methods section (p. 5): â46 right-handed, 3 left-handedâ</p><disp-quote content-type="editor-comment"><p>(3) It would be interesting to know how students felt about the Virtual Classroom context, whether it is indeed close to the real classroom or to some extent different.</p></disp-quote><p>Yes, we agree. Obviously, the VR classroom differs in many ways from a real classroom, in terms of the perceptual experience, social aspects and interactive possibilities. We did ask participants about their VR experience after the experiment, and most reported feeling highly immersed in the VR environment and engaged in the task, with a strong sense of presence in the virtual-classroom.</p><p>We note that, in parallel to the VR studies in our lab, we are also conducting experiments in real classrooms, and we hope that the cross-study comparison will be able to shed more light on these similarities/differences.</p><disp-quote content-type="editor-comment"><p>(4) One intriguing issue is whether neural tracking of the teacher's speech can index students' attention, as the tracking of speech may be relevant to various factors such as sound processing without semantic access.</p></disp-quote><p>Another excellent point. While separating the âacousticâ and âsemanticâ contributions to the speech tracking response is non-trivial, we are currently working on methodological approaches to do this (again, in future studies) following, for example, the hierarchical TRF approach used by Brodbeck et al. and others.</p><disp-quote content-type="editor-comment"><p>(5) There are many results associated with various metrics, and many results did not show a significant difference between the ADHD group and the control group. It is difficult to find the crucial information that supports the conclusion. I suggest the authors reorganize the results section and report the significant results first, and to which comparison(s) the readers should pay attention.</p></disp-quote><p>We apologize if the organization of the results section was difficult to follow. This is indeed a challenge when collecting so many different neurophysiological metrics.</p><p>To facilitate this, we have now added a paragraph at the beginning of the result section, clarifying its structure (p.16):</p><p>The current dataset is extremely rich, consisting of many different behavioral, neural and physiological responses. In reporting these results, we have separated between metrics that are associated with paying attention to the teacher (behavioral performance, neural tracking of the teacherâs speech, and looking at the teacher), those capturing responses to the irrelevant sound-events (ERPs and event-related changes in SC and gaze); as well as more global neurophysiological measures that may be associated with the listenersâ overall âstateâ of attention or arousal (alpha- and beta-power and tonic SC).</p><p>Moreover, within each section we have ordered the analysis such that the ones with significant effects are first. We hope that this contributes to the clarity of the results section.</p><disp-quote content-type="editor-comment"><p>(6) The difference between artificial and non-verbal humans should be introduced earlier in the introduction and let the readers know what should be expected and why.</p></disp-quote><p>We have added this to the Introduction (p. 4)</p><disp-quote content-type="editor-comment"><p>(7) It would be better to discuss the results against a theoretical background rather than majorly focusing on technical aspects.</p></disp-quote><p>We appreciate this comment. In our opinion, the discussion does contain a substantial theoretical component, both regarding theories of attention and attention-deficits, and also regarding their potential neural correlates. However, we agree that there is always room for more in depth discussion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Major:</p><p>(1) While the study introduced a well-designed experiment with comprehensive physiological measures and thorough analyses, the key insights derived from the experiment are unclear. For example, does the high ecological validity provide a more sensitive biomarker or a new physiological measure of attention deficit compared to previous studies? Or does the study shed light on new mechanisms of attention deficit, such as the simultaneous presence of inattention and distraction (as mentioned in the Conclusion)? The authors should clearly articulate their contributions.</p></disp-quote><p>Thanks for this comment.</p><p>We would not say that this paper is able to provide a âmore sensitive biomarkerâ or a ânew physiological measure of attentionâ â in order to make those type of grand statements we would need to have much more converging evidence from multiple studies and using both replication and generalization approaches.</p><p>Rather, from our perspective, the key contribution of this work is in broadening the scope of research regarding the neurophysiological mechanisms involved in attention and distraction.</p><p>Specifically, this work:</p><p>(1) Offers a significant methodological advancement of the field â demonstrating the plausibility and scientific potential of using combined EEG-VR-eyetracking to study neurophysiological aspects of attention and distractibility in contexts that âmimicâ real-life situations (rather than highly controlled computerized tasks).</p><p>(2) Provides a solid basis formulating specific mechanistic hypothesis regarding the neurophysiological metrics associated with attention and distraction, the interplay between them, and their potential relation to ADHD-symptoms. Rather than being an end-point, we see these results as a start-point for future studies that emphasize ecological validity and generalizability across contexts, that will hopefully lead to improved mechanisms understanding and potential biomarkers of real-life attentional capabilities (see also response to Rev #2 comment #1 above).</p><p>(3) Highlights differences and similarities between the current results and those obtained in traditional âhighly controlledâ studies of attention (e.g., in the way ERPs to sound-events differ between ADHD and controls; variability in gaze and alpha-power; and more broadly about whether ADHD symptoms do or donât map onto specific neurophysiological metrics). Again, we do not claim to give a definitive âanswerâ to these issues, but rather to provide a new type of data that can expands the conversation and address the ecological validity gap in attention research.</p><disp-quote content-type="editor-comment"><p>(2) Based on the multivariate analyses, ASRS scores correlate better with the physiological measures rather than the binary deficit category. It may be worthwhile to report the correlation between physiological measures and ASRS scores for the univariate analyses. Additionally, the correlation between physiological measures and behavioral accuracy might also be interesting.</p></disp-quote><p>Thanks for this. The beta-values reported for the regression analysis reflect the correlations between the different physiological measures and the ASRS scores (p. 30). From a statistical perspective, it is better to report these values rather than the univariate correlation-coefficients, since these represent the âuniqueâ relationship with each factor, after controlling for all the others.</p><p>The univariate correlations between the physiological measures themselves, as well as with behavioral accuracy, are reported in Figure 10</p><disp-quote content-type="editor-comment"><p>(3) For the TRF and decoding analysis, the authors used a constant regularization parameter per a previous study. However, the optimal regularization parameter is data-dependent and may differ between encoding and decoding analyses. Furthermore, the authors did not conduct TRF analysis for the quiet condition due to the limited ~5 minutes of data. However, such a data duration is generally sufficient to derive a stable TRF with significant predictive power (<xref ref-type="bibr" rid="bib74">Mesik and Wojtczak, 2022</xref>).</p></disp-quote><p>The reviewer raises two important points, also raised by Rev #1 (see above).</p><p>Regarding the choice of regularization parameters, we have now clarified that although we used a common lambda value for all participants, it was selected in a data-driven manner, so as to achieve an optimal predictive power at the group-level.</p><p>See revised methods section:</p><p>âThe mTRF toolbox uses a ridge-regression approach for L2 regularization of the model to ensure better generalization to new data. We tested a range of ridge parameter values (Î»'s) and used a leave-one-out cross-validation procedure to assess the modelâs predictive power, whereby in each iteration, all but one trials are used to train the model, and it is then applied to the left-out trial. The predictive power of the model (for each Î») is estimated as the Pearsonâs correlation between the predicted neural responses and the actual neural responses, separately for each electrode, averages across all iterations. We report results of the model with the Î» the yielded the highest predictive power at the group-level (rather than selecting a different Î» for each participant which can lead to incomparable TRF models across participants; see discussion in Kaufman &amp; Zion Golumbic 2023).â</p><p>Regarding whether data was sufficient in the Quiet condition for performing TRF analysis â we are aware of the important work by Mesik &amp; Wojtczak, and had initially used this estimate when designing our study. However, when assessing the predictive-power of the TRF model trained on data from the Quiet condition, we found that it was not significantly better than chance (see Author response image 2, ârealâ predictive power vs. permuted data). Therefore, we ultimately did not feel that it was appropriate to include TRF analysis of the Quiet condition in this manuscript. We have now clarified this in the manuscript (p. 10)</p><disp-quote content-type="editor-comment"><p>(4) As shown in Figure 4, for ADHD participants, decoding accuracy appears to be lower than the predictive power of TRF. This result is surprising because more data (i.e., data from all electrodes) is used in the decoding analysis.</p></disp-quote><p>This is an interesting point â however, in our experience it is not necessarily the case that decoding accuracy (i.e., reconstruction correlation with the stimulus) is higher than encoding predictive-power. While both metrics use Pearsonâsâ correlations, they quantify the similarity between two different types of signals (the EEG and the speech-envelope). Although the decoding procedure does use data from all electrodes, many of them donât actually contain meaningful information regarding the stimulus, and thus could just as well hinder the overall performance of the decoding.</p><disp-quote content-type="editor-comment"><p>(5) Beyond the current analyses, the authors may consider analyzing inter-subject correlation, especially for the gaze signal analysis. Given that the area of interest during the lesson changes dynamically, the teacher might not always be the focal point. Therefore, the correlation of gaze locations between subjects might be better than the percentage of gaze duration on the teacher.</p></disp-quote><p>Thanks for this suggestion. We have tried to look into this, however working with eye-gaze in a 3-D space is extremely complex and we are not able to calculate reliable correlations between participants.</p><disp-quote content-type="editor-comment"><p>(6) Some preprocessing steps relied on visual and subjective inspection. For instance, &quot; Visual inspection was performed to identify and remove gross artifacts (excluding eye movements) &quot; (P9); &quot; The raw data was downsampled to 16Hz and inspected for any noticeable artifacts &quot; (P13). Please consider using objective processes or provide standards for subjective inspections.</p></disp-quote><p>We are aware of the possible differences between objective methods of artifact rejection vs. use of manual visual inspection, however we still prefer the manual (subjective) approach. As noted, in this case only very large artifacts were removed, exceeding ~ 4 SD of the amplitude variability, so as to preserve as many full-length trials as possible.</p><disp-quote content-type="editor-comment"><p>(7) Numerous significance testing methods were employed in the manuscript. While I appreciate the detailed information provided, describing these methods in a separate section within the Methods would be more general and clearer. Additionally, the authors may consider using a linear mixed-effects model, which is more widely adopted in current neuroscience studies and can account for random subject effects.</p></disp-quote><p>Indeed, there are many statistical tests in the paper, given the diverse types of neurophysiological data collected here. We actually thought that describing the statistics per method rather than in a separate âgeneralâ section would be easier to follow, but we understand that readers might diverge in their preferences.</p><p>Regarding the use of mixed-effect models â this is indeed a great approach. However, it requires deriving reliable metrics on a per-trial basis, and while this might be plausible for some of our metrics, the EEG and GSR metrics are less reliable at this level. This is why we ultimately chose to aggregate across trials and use a regular regression model rather than mixed-effects.</p><disp-quote content-type="editor-comment"><p>(8) Some participant information is missing, such as their academic majors. Given that only two lesson topics were used, the participants' majors may be a relevant factor.</p></disp-quote><p>To clarify â the mini-lectures presented here actually covered a large variety of topics, broadly falling within the domains of history, science and social-science and technology. Regarding participantsâ academic majors, these were relatively diverse, as can be seen in Author response table 1 and Author response image 4.</p><table-wrap id="sa4table1" position="float"><label>Author response table 1.</label><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Academic major</th><th valign="bottom">Count</th></tr></thead><tbody><tr><td align="left" valign="bottom">Not a student</td><td align="left" valign="bottom">14</td></tr><tr><td align="left" valign="bottom">Neuroscience</td><td align="left" valign="bottom">7</td></tr><tr><td align="left" valign="bottom">Middle Esstern Studies</td><td align="left" valign="bottom">2</td></tr><tr><td align="left" valign="bottom">Criminology</td><td align="left" valign="bottom">2</td></tr><tr><td align="left" valign="bottom">Psychology and Economics</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Business Administration</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Veterinary Medicine</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Computer Engineering</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Arabic and Middle Eastern Studies</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Computer Science</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Psychology and Special Education</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Agriculture</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Communication and Political Science</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Materials Engineering</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Medicine</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Criminology and Special Education</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Music</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Arabic</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Nursing</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Education</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Psychology</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Electrical Engineering</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Psychology and History</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Information Systerns</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Sociology and Anthropology</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Jewish Art</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Law</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Life Sciences</td><td align="left" valign="bottom">1</td></tr></tbody></table></table-wrap><fig id="sa4fig4" position="float"><label>Author response image 4.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103235-sa4-fig4-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(9) Did the multiple regression model include cross-validation? Please provide details regarding this.</p></disp-quote><p>Yes, we used a leave-one-out cross validation procedure. We have now clarified this in the methods section which now reads:</p><p>âThe mTRF toolbox uses a ridge-regression approach for L2 regularization of the model to ensure better generalization to new data. We tested a range of ridge parameter values (Î»'s) and used a leave-one-out cross-validation procedure to assess the modelâs predictive power, whereby in each iteration, all but one trials are used to train the model, and it is then applied to the left-out trial. The predictive power of the model (for each Î») is estimated as the Pearsonâs correlation between the predicted neural responses and the actual neural responses, separately for each electrode, averages across all iterations. We report results of the model with the Î» the yielded the highest predictive power at the group-level (rather than selecting a different Î» for each participant which can lead to incomparable TRF models across participants; see discussion in Kaufman &amp; Zion Golumbic 2023).â</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>(10) Typographical errors: P5, &quot;forty-nine 49 participants&quot;; P21, &quot;$ref&quot;; P26, &quot;Table X&quot;; P4, please provide the full name for &quot;SC&quot; when first mentioned.</p></disp-quote><p>Thanks! corrected</p></body></sub-article></article>