<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98033</article-id><article-id pub-id-type="doi">10.7554/eLife.98033</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98033.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Semantical and geometrical protein encoding toward enhanced bioactivity and thermostability</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Tan</surname><given-names>Yang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0004-7261-1705</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Zhou</surname><given-names>Bingxin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3897-9766</contrib-id><email>bingxin.zhou@sjtu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zheng</surname><given-names>Lirong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6803-5048</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Fan</surname><given-names>Guisheng</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Hong</surname><given-names>Liang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0107-336X</contrib-id><email>hongl3liang@sjtu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Shanghai-Chongqing Institute of Artificial Intelligence, Shanghai Jiao Tong University</institution></institution-wrap><addr-line><named-content content-type="city">Chongqing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01vyrm377</institution-id><institution>School of Information Science and Engineering, East China University of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Zhangjiang Institute for Advanced Study, Shanghai Jiao Tong University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03wkvpx79</institution-id><institution>Shanghai Artificial Intelligence Laboratory</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Shanghai Jiao Tong University, Institute of Natural Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Shanghai National Center for Applied Mathematics (SJTU Center), Shanghai Jiao Tong University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Koo</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qz8b764</institution-id><institution>Cold Spring Harbor Laboratory</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Cui</surname><given-names>Qiang</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Boston University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>05</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP98033</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-03-28"><day>28</day><month>03</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-03-19"><day>19</day><month>03</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.01.569522"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-25"><day>25</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98033.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-18"><day>18</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98033.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-02-26"><day>26</day><month>02</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98033.3"/></event></pub-history><permissions><copyright-statement>© 2024, Tan, Zhou et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Tan, Zhou et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98033-v1.pdf"/><abstract><p>Protein engineering is a pivotal aspect of synthetic biology, involving the modification of amino acids within existing protein sequences to achieve novel or enhanced functionalities and physical properties. Accurate prediction of protein variant effects requires a thorough understanding of protein sequence, structure, and function. Deep learning methods have demonstrated remarkable performance in guiding protein modification for improved functionality. However, existing approaches predominantly rely on protein sequences, which face challenges in efficiently encoding the geometric aspects of amino acids’ local environment and often fall short in capturing crucial details related to protein folding stability, internal molecular interactions, and bio-functions. Furthermore, there lacks a fundamental evaluation for developed methods in predicting protein thermostability, although it is a key physical property that is frequently investigated in practice. To address these challenges, this article introduces a novel pre-training framework that integrates sequential and geometric encoders for protein primary and tertiary structures. This framework guides mutation directions toward desired traits by simulating natural selection on wild-type proteins and evaluates variant effects based on their fitness to perform specific functions. We assess the proposed approach using three benchmarks comprising over 300 deep mutational scanning assays. The prediction results showcase exceptional performance across extensive experiments compared to other zero-shot learning methods, all while maintaining a minimal cost in terms of trainable parameters. This study not only proposes an effective framework for more accurate and comprehensive predictions to facilitate efficient protein engineering, but also enhances the in silico assessment system for future deep learning models to better align with empirical requirements. The PyTorch implementation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ai4protein/ProtSSN">https://github.com/ai4protein/ProtSSN</ext-link>.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>deep learning</kwd><kwd>protein language model</kwd><kwd>mutation effect prediction</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Science and Technology Innovation Key R&amp;D Program of Chongqing</institution></institution-wrap></funding-source><award-id>CSTB2022TIAD-STX0017</award-id><principal-award-recipient><name><surname>Hong</surname><given-names>Liang</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>62302291</award-id><principal-award-recipient><name><surname>Zhou</surname><given-names>Bingxin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>12104295</award-id><principal-award-recipient><name><surname>Hong</surname><given-names>Liang</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>The Computational Biology Key Program of Shanghai Science and Technology Commission</institution></institution-wrap></funding-source><award-id>23JS1400600</award-id><principal-award-recipient><name><surname>Hong</surname><given-names>Liang</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Shanghai Jiao Tong University Scientific and Technological Innovation Funds</institution></institution-wrap></funding-source><award-id>21X010200843</award-id><principal-award-recipient><name><surname>Hong</surname><given-names>Liang</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The novel pre-training framework integrating sequential and geometric encoders significantly enhances protein engineering by accurately predicting variant effects and thermostability, outperforming existing zero-shot learning methods with minimal computational cost.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The diverse roles proteins play in nature necessitate their involvement in varied bio-functions, such as catalysis, binding, scaffolding, and transport. Advances in science and technology have positioned proteins, particularly those with desired catalytic and binding properties, to pivotal positions in medicine, biology, and scientific research (<xref ref-type="bibr" rid="bib81">Zhou et al., 2024a</xref>). While wild-type proteins exhibit optimal bio-functionality in their native environments, industrial applications often demand adaptation to conditions such as high temperature, high pressure, strong acidity, and strong alkalinity. The constrained efficacy of proteins in meeting the stringent requirements of industrial functioning environments hinders their widespread applications (<xref ref-type="bibr" rid="bib70">Woodley, 2013</xref>; <xref ref-type="bibr" rid="bib15">Ismail et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Jiang et al., 2023</xref>). Consequently, there arises a need to engineer these natural proteins to enhance their functionalities, aligning them with the demands of both industrial and scientific applications.</p><p>Analyzing the relationship between protein sequence and function yields valuable insights for engineering proteins with new or enhanced functions in synthetic biology. The intrinsic goal of protein engineering is to unveil highly functional sequences by navigating the intricate, high-dimensional surface of the fitness landscape (<xref ref-type="bibr" rid="bib73">Yang et al., 2019</xref>), which delineates the relationship between amino acid (AA) sequences and the desired functional state. Predicting the effects of AA substitutions, insertions, and deletions (<xref ref-type="bibr" rid="bib47">Riesselman et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Gray et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>) in proteins, that is, mutation, thus allows researchers to dissect how changes in the AA sequence can impact the protein’s catalytic efficiency, stability, and binding affinity (<xref ref-type="bibr" rid="bib55">Shin et al., 2021</xref>). However, the extensive, non-contiguous search space of AA combinations poses challenges for conventional methods, such as rational design (<xref ref-type="bibr" rid="bib1">Aprile et al., 2020</xref>) or directed evolution (<xref ref-type="bibr" rid="bib69">Wang et al., 2021</xref>), in efficiently and effectively identifying protein variants with desired properties. In response, deep learning has emerged as a promising solution that proposes favorable protein variants with high fitness.</p><p>Deep learning approaches have been instrumental in advancing scientific insights into proteins, predominantly categorized into sequence-based and structure-based methods. Autoregressive protein language models (<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>; <xref ref-type="bibr" rid="bib30">Madani et al., 2023</xref>) interpret AA sequences as raw text to generate with self-attention mechanisms (<xref ref-type="bibr" rid="bib65">Vaswani et al., 2017</xref>). Alternatively, masked language modeling objectives develop attention patterns that correspond to the residue–residue contact map of the protein (<xref ref-type="bibr" rid="bib31">Meier et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Rao et al., 2021b</xref>; <xref ref-type="bibr" rid="bib48">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Vig et al., 2021</xref>; <xref ref-type="bibr" rid="bib2">Brandes et al., 2022</xref>; <xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>). Other methods start from a multiple sequence alignment, summarizing the evolutionary patterns in target proteins (<xref ref-type="bibr" rid="bib47">Riesselman et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Frazer et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Rao et al., 2021a</xref>). These methods result in a strong capacity for discovering the hidden protein space. However, the many-to-one relationship of both sequence-to-structure and structure-to-function projections requires excessive training input or substantial learning resources, which raises concerns regarding the efficiency of these pathways when navigating the complexities of the vast sequence space associated with a target function. Moreover, the overlooked local environment of proteins hinders the model’s ability to capture structure-sensitive properties that impact protein’s thermostability, interaction with substrates, and catalytic process (<xref ref-type="bibr" rid="bib78">Zhao et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Koehler Leman et al., 2023</xref>). Alternatively, structure-based modeling methods serve as an effective enhancement to complement sequence-oriented inferences for proteins with their local environment (<xref ref-type="bibr" rid="bib63">Tan et al., 2024e</xref>; <xref ref-type="bibr" rid="bib83">Zhou et al., 2024c</xref>; <xref ref-type="bibr" rid="bib76">Yi et al., 2024</xref>; <xref ref-type="bibr" rid="bib82">Zhou et al., 2024b</xref>; <xref ref-type="bibr" rid="bib61">Tan et al., 2024c</xref>). Given that core mutations often induce functional defects through subtle disruptions to structure or dynamics (<xref ref-type="bibr" rid="bib51">Roscoe et al., 2013</xref>), incorporating protein geometry into the learning process can offer valuable insights into stabilizing protein functioning. Recent efforts have been made to encode geometric information of proteins for topology-sensitive tasks such as molecule binding (<xref ref-type="bibr" rid="bib17">Jin et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Myung et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">Kong et al., 2023</xref>) and protein properties prediction (<xref ref-type="bibr" rid="bib77">Zhang et al., 2022</xref>; <xref ref-type="bibr" rid="bib62">Tan et al., 2024d</xref>; <xref ref-type="bibr" rid="bib59">Tan et al., 2024a</xref>). Nevertheless, structure encoders fall short in capturing non-local connections for AAs beyond their contact region and overlook correlations that do not conform to the ‘structure–function determination’ heuristic.</p><p>There is a pressing need to develop a novel framework that overcomes the limitations inherent in individual implementations of sequence or structure-based investigations. To this end, we introduce ProtSSN to assimilate the semantics and topology of <bold>Prot</bold>eins from their <bold>S</bold>equence and <bold>S</bold>tructure with deep neural <bold>N</bold>etworks. ProtSSN incorporates the intermediate state of protein structures and facilitates the discovery of an efficient and effective trajectory for mapping protein sequences to functionalities. The developed model extends the generalization and robustness of self-supervised protein language models while maintaining low computational costs, thereby facilitating self-supervised training and task-specific customization. A funnel-shaped learning pipeline, as depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>, is designed due to the limited availability of protein crystal structures compared to observed protein sequences. Initially, the linguistic embedding establishes the semantic and grammatical rules in AA chains by inspecting millions of protein sequences. The topological embedding then enhances the interactions among locally connected AAs. Since a geometry can be placed or observed by different angles and positions in space, we represent proteins’ topology by graphs and enhance the model’s robustness and efficiency with a rotation and translation equivariant graph representation learning scheme.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>An illustration of ProtSSN that extracts the semantical and geometrical characteristics of a protein from its sequentially ordered global construction and spatially gathered local contacts with protein language models and equivariant graph neural networks.</title><p>The encoded hidden representation can be used for downstream tasks such as variants effect prediction that recognizes the impact of mutating a few sites of a protein on its functionality.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98033-fig1-v1.tif"/></fig><p>The pre-trained ProtSSN demonstrates its feasibility across a broad range of mutation effect prediction benchmarks covering catalysis, interaction, and thermostability. Mutation effects prediction serves as a common in silico assessment for evaluating the capability of an established deep learning framework in identifying favorable protein variants. With the continuous evolution of deep mutation scanning techniques and other high-throughput technologies, benchmark datasets have been curated to document fitness changes in mutants across diverse proteins with varying degrees of combinatorial and random modifications (<xref ref-type="bibr" rid="bib34">Moal and Fernández-Recio, 2012</xref>; <xref ref-type="bibr" rid="bib38">Nikam et al., 2021</xref>; <xref ref-type="bibr" rid="bib66">Velecký et al., 2022</xref>). <bold>ProteinGym v1</bold> (<xref ref-type="bibr" rid="bib42">Notin et al., 2023</xref>) comprehends fitness scoring of mutants to catalytic activity and binding affinity across over 200 well-studied proteins of varying deep mutational scanning (DMS) assays and taxa. Each protein includes tens of thousands of mutants documented from high-throughput screening techniques. While <bold>ProteinGym v1</bold> initiates the most comprehensive benchmark dataset for mutants toward different properties enhancement, these fitness scores are normalized and simplified into several classes without further specifications. For instance, a good portion of protein assays is scored by their stability, which, in practice, is further categorized into thermostability, photostability, pH-stability, etc. Moreover, these stability properties should be discussed under certain environmental conditions, such as pH and temperature. Unfortunately, these detailed attributes are excluded in <bold>ProteinGym v1</bold>. Since a trade-off relationship emerges between activity and thermostability (<xref ref-type="bibr" rid="bib79">Zheng et al., 2022</xref>), protein engineering in practice extends beyond enhancing catalytic activities to maintaining or increasing thermostability for prolonged functional lifetimes and applications under elevated temperatures and chemical conditions (<xref ref-type="bibr" rid="bib29">Liu et al., 2019</xref>). Consequently, there is a need to enrich mutation effect prediction benchmarks that evaluate the model’s efficacy in capturing variant effects concerning thermostability under distinct experimental conditions. In this study, we address the mutation effect prediction tasks on thermostability with <bold>DTm</bold> and <bold>DDG</bold>, two new single-site mutation benchmarks that measure thermostability using ΔTm and ΔΔG values, respectively. Both benchmarks group experimental assays based on the protein–condition combinations. These two datasets supplement the publicly available DMS assay benchmarks and facilitate ready-to-use assessment for future deep learning methods toward protein thermostability enhancement.</p><p>This work fulfills practical necessities in the development of deep learning methods for protein engineering from three distinct perspectives. Firstly, the developed ProtSSN employs self-supervised learning during training to obviate the necessity for additional supervision in downstream tasks. This zero-shot scenario is desirable due to the scarcity of experimental results, as well as the ‘cold-start’ situation common in many wet lab experiments. Secondly, the trained model furnishes robust and meaningful approximations to the joint distribution of the entire AA chain, thereby augmenting the <italic>epistatic effect</italic> (<xref ref-type="bibr" rid="bib52">Sarkisyan et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Khersonsky et al., 2018</xref>) in deep mutations. This augmentation stems from the model’s consideration of the nonlinear combinatorial effects of AA sites. Additionally, we complement the existing dataset of DMS assay with additional benchmark assays related to protein–environment interactions, specifically focusing on thermostability—an integral objective in numerous protein engineering projects.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Variant effect prediction</title><p>We commence our investigation by assessing the predictive performance of ProtSSN on four benchmark datasets using state-of-the-art (SOTA) protein learning models. We deploy different versions of ProtSSN that learns <italic>k</italic>NN graphs with <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>h</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>512</mml:mn><mml:mo>,</mml:mo><mml:mn>768</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>280</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> hidden neurons in each of the six <italic>Equivariant Graph Neural Networks</italic> EGNN layers. For all baselines, their performance on <bold>DTm</bold> and <bold>DDG</bold> is reproduced with the official implementation listed in Table 5, and the scores on <bold>ProteinGym v1</bold> are retrieved from <ext-link ext-link-type="uri" xlink:href="https://proteingym.org/benchmarks">https://proteingym.org/benchmarks</ext-link> by <xref ref-type="bibr" rid="bib42">Notin et al., 2023</xref>. As visualized in <xref ref-type="fig" rid="fig2">Figure 2</xref> and reported in <xref ref-type="table" rid="table1">Table 1</xref>, ProtSSN demonstrated exceptional predictive performance with a significantly smaller number of trainable parameters when predicting the function and thermostability of mutants.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Number of trainable parameters and Spearman’s <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> correlation on <bold>DTm</bold>, <bold>DDG</bold>, and <bold>ProteinGym v1</bold>, with the medium value located by the dashed lines.</title><p>Dot, cross, and diamond markers represent sequence-based, structure-based, and sequence-structure models, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98033-fig2-v1.tif"/></fig><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Spearman’s <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> correlation of variant effect prediction by with zero-shot methods on <bold>DTm</bold>, <bold>DDG</bold>, and <bold>ProteinGym v1</bold>.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model</th><th align="left" valign="bottom" rowspan="2">Version</th><th align="left" valign="bottom"># Params</th><th align="left" valign="bottom" rowspan="2">DTm</th><th align="left" valign="bottom" rowspan="2">DDG</th><th align="left" valign="bottom" colspan="6">ProteinGym v1</th></tr><tr><th align="left" valign="bottom">(million)</th><th align="left" valign="bottom">Activity</th><th align="left" valign="bottom">Binding</th><th align="left" valign="bottom">Expression</th><th align="left" valign="bottom">Organismal</th><th align="left" valign="bottom">Stability</th><th align="left" valign="bottom">Overall fitness</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="11">Sequence encoder</td></tr><tr><td align="left" valign="bottom" rowspan="4">RITA</td><td align="left" valign="bottom">Small</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">0.122</td><td align="left" valign="bottom">0.143</td><td align="left" valign="bottom">0.294</td><td align="left" valign="bottom">0.275</td><td align="left" valign="bottom">0.337</td><td align="left" valign="bottom">0.327</td><td align="left" valign="bottom">0.289</td><td align="left" valign="bottom">0.304</td></tr><tr><td align="left" valign="bottom">Medium</td><td align="left" valign="bottom">300</td><td align="left" valign="bottom">0.131</td><td align="left" valign="bottom">0.188</td><td align="left" valign="bottom">0.352</td><td align="left" valign="bottom">0.274</td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.371</td><td align="left" valign="bottom">0.348</td><td align="left" valign="bottom">0.350</td></tr><tr><td align="left" valign="bottom">Large</td><td align="left" valign="bottom">680</td><td align="left" valign="bottom">0.213</td><td align="left" valign="bottom">0.236</td><td align="left" valign="bottom">0.359</td><td align="left" valign="bottom">0.291</td><td align="left" valign="bottom">0.422</td><td align="left" valign="bottom">0.374</td><td align="left" valign="bottom">0.383</td><td align="left" valign="bottom">0.366</td></tr><tr><td align="left" valign="bottom">xlarge</td><td align="left" valign="bottom">1,200</td><td align="left" valign="bottom">0.221</td><td align="left" valign="bottom">0.264</td><td align="left" valign="bottom">0.402</td><td align="left" valign="bottom">0.302</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">0.387</td><td align="left" valign="bottom">0.445</td><td align="left" valign="bottom">0.373</td></tr><tr><td align="left" valign="bottom" rowspan="5">ProGen2</td><td align="left" valign="bottom">Small</td><td align="left" valign="bottom">151</td><td align="left" valign="bottom">0.135</td><td align="left" valign="bottom">0.194</td><td align="left" valign="bottom">0.333</td><td align="left" valign="bottom">0.275</td><td align="left" valign="bottom">0.384</td><td align="left" valign="bottom">0.337</td><td align="left" valign="bottom">0.349</td><td align="left" valign="bottom">0.336</td></tr><tr><td align="left" valign="bottom">Medium</td><td align="left" valign="bottom">764</td><td align="left" valign="bottom">0.226</td><td align="left" valign="bottom">0.214</td><td align="left" valign="bottom">0.393</td><td align="left" valign="bottom">0.296</td><td align="left" valign="bottom">0.436</td><td align="left" valign="bottom">0.381</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.380</td></tr><tr><td align="left" valign="bottom">Base</td><td align="left" valign="bottom">764</td><td align="left" valign="bottom">0.197</td><td align="left" valign="bottom">0.253</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.294</td><td align="left" valign="bottom">0.444</td><td align="left" valign="bottom">0.379</td><td align="left" valign="bottom">0.383</td><td align="left" valign="bottom">0.379</td></tr><tr><td align="left" valign="bottom">Large</td><td align="left" valign="bottom">2700</td><td align="left" valign="bottom">0.181</td><td align="left" valign="bottom">0.226</td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.294</td><td align="left" valign="bottom">0.429</td><td align="left" valign="bottom">0.379</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.381</td></tr><tr><td align="left" valign="bottom">xlarge</td><td align="left" valign="bottom">6400</td><td align="left" valign="bottom">0.232</td><td align="left" valign="bottom">0.270</td><td align="left" valign="bottom">0.402</td><td align="left" valign="bottom">0.302</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">0.387</td><td align="left" valign="bottom">0.445</td><td align="left" valign="bottom">0.392</td></tr><tr><td align="left" valign="bottom" rowspan="4">ProtTrans</td><td align="left" valign="bottom">bert</td><td align="left" valign="bottom">420</td><td align="left" valign="bottom">0.268</td><td align="left" valign="bottom">0.313</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">bert_bfd</td><td align="left" valign="bottom">420</td><td align="left" valign="bottom">0.217</td><td align="left" valign="bottom">0.293</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">t5_xl_uniref50</td><td align="left" valign="bottom">3000</td><td align="left" valign="bottom">0.310</td><td align="left" valign="bottom">0.365</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">t5_xl_bfd</td><td align="left" valign="bottom">3000</td><td align="left" valign="bottom">0.239</td><td align="left" valign="bottom">0.334</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom" rowspan="3">Tranception</td><td align="left" valign="bottom">Small</td><td align="left" valign="bottom">85</td><td align="left" valign="bottom">0.119</td><td align="left" valign="bottom">0.169</td><td align="left" valign="bottom">0.287</td><td align="left" valign="bottom">0.349</td><td align="left" valign="bottom">0.319</td><td align="left" valign="bottom">0.270</td><td align="left" valign="bottom">0.258</td><td align="left" valign="bottom">0.288</td></tr><tr><td align="left" valign="bottom">Medium</td><td align="left" valign="bottom">300</td><td align="left" valign="bottom">0.189</td><td align="left" valign="bottom">0.256</td><td align="left" valign="bottom">0.349</td><td align="left" valign="bottom">0.285</td><td align="left" valign="bottom">0.409</td><td align="left" valign="bottom">0.362</td><td align="left" valign="bottom">0.342</td><td align="left" valign="bottom">0.349</td></tr><tr><td align="left" valign="bottom">Large</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom">0.197</td><td align="left" valign="bottom">0.284</td><td align="left" valign="bottom">0.401</td><td align="left" valign="bottom">0.289</td><td align="left" valign="bottom">0.415</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.389</styled-content></td><td align="left" valign="bottom">0.381</td><td align="left" valign="bottom">0.375</td></tr><tr><td align="left" valign="bottom">ESM-1v</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">650</td><td align="left" valign="bottom">0.279</td><td align="left" valign="bottom">0.266</td><td align="left" valign="bottom">0.390</td><td align="left" valign="bottom">0.268</td><td align="left" valign="bottom">0.431</td><td align="left" valign="bottom">0.362</td><td align="left" valign="bottom">0.476</td><td align="left" valign="bottom">0.385</td></tr><tr><td align="left" valign="bottom">ESM-1b</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">650</td><td align="left" valign="bottom">0.271</td><td align="left" valign="bottom">0.343</td><td align="left" valign="bottom">0.428</td><td align="left" valign="bottom">0.289</td><td align="left" valign="bottom">0.427</td><td align="left" valign="bottom">0.351</td><td align="left" valign="bottom">0.500</td><td align="left" valign="bottom">0.399</td></tr><tr><td align="left" valign="bottom" rowspan="5">ESM2</td><td align="left" valign="bottom">t12</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">0.214</td><td align="left" valign="bottom">0.216</td><td align="left" valign="bottom">0.314</td><td align="left" valign="bottom">0.292</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom">0.218</td><td align="left" valign="bottom">0.439</td><td align="left" valign="bottom">0.325</td></tr><tr><td align="left" valign="bottom">t30</td><td align="left" valign="bottom">150</td><td align="left" valign="bottom">0.288</td><td align="left" valign="bottom">0.317</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.328</td><td align="left" valign="bottom">0.425</td><td align="left" valign="bottom">0.305</td><td align="left" valign="bottom">0.510</td><td align="left" valign="bottom">0.392</td></tr><tr><td align="left" valign="bottom">t33</td><td align="left" valign="bottom">650</td><td align="left" valign="bottom">0.330</td><td align="left" valign="bottom">0.392</td><td align="left" valign="bottom">0.425</td><td align="left" valign="bottom">0.339</td><td align="left" valign="bottom">0.415</td><td align="left" valign="bottom">0.338</td><td align="left" valign="bottom">0.523</td><td align="left" valign="bottom">0.419</td></tr><tr><td align="left" valign="bottom">t36</td><td align="left" valign="bottom">3000</td><td align="left" valign="bottom">0.327</td><td align="left" valign="bottom">0.351</td><td align="left" valign="bottom">0.417</td><td align="left" valign="bottom">0.322</td><td align="left" valign="bottom">0.425</td><td align="left" valign="bottom">0.379</td><td align="left" valign="bottom">0.509</td><td align="left" valign="bottom">0.410</td></tr><tr><td align="left" valign="bottom">t48</td><td align="left" valign="bottom">15,000</td><td align="left" valign="bottom">0.311</td><td align="left" valign="bottom">0.252</td><td align="left" valign="bottom">0.405</td><td align="left" valign="bottom">0.318</td><td align="left" valign="bottom">0.425</td><td align="left" valign="bottom">0.388</td><td align="left" valign="bottom">0.488</td><td align="left" valign="bottom">0.405</td></tr><tr><td align="left" valign="bottom">CARP</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">640</td><td align="left" valign="bottom">0.288</td><td align="left" valign="bottom">0.333</td><td align="left" valign="bottom">0.395</td><td align="left" valign="bottom">0.274</td><td align="left" valign="bottom">0.419</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom">0.414</td><td align="left" valign="bottom">0.373</td></tr><tr><td align="left" valign="bottom" colspan="11">Structure encoder</td></tr><tr><td align="left" valign="bottom">ESM-if1</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">142</td><td align="left" valign="bottom">0.395</td><td align="left" valign="bottom"><bold>0.409</bold></td><td align="left" valign="bottom">0.368</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.392</styled-content></td><td align="left" valign="bottom">0.403</td><td align="left" valign="bottom">0.324</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.624</styled-content></td><td align="left" valign="bottom">0.422</td></tr><tr><td align="left" valign="bottom" colspan="11">Sequence + structure encoder</td></tr><tr><td align="left" valign="bottom">MIF-ST</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">643</td><td align="left" valign="bottom"><bold>0.400</bold></td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.390</td><td align="left" valign="bottom">0.323</td><td align="left" valign="bottom">0.432</td><td align="left" valign="bottom">0.373</td><td align="left" valign="bottom">0.486</td><td align="left" valign="bottom">0.401</td></tr><tr><td align="left" valign="bottom" rowspan="2">SaProt</td><td align="left" valign="bottom">Masked</td><td align="left" valign="bottom">650</td><td align="left" valign="bottom">0.382</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.459</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.382</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.485</styled-content></td><td align="left" valign="bottom">0.371</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.583</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.456</styled-content></td></tr><tr><td align="left" valign="bottom">Unmasked</td><td align="left" valign="bottom">650</td><td align="left" valign="bottom">0.376</td><td align="left" valign="bottom">0.359</td><td align="left" valign="bottom">0.450</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.376</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.460</styled-content></td><td align="left" valign="bottom">0.372</td><td align="left" valign="bottom">0.577</td><td align="left" valign="bottom">0.447</td></tr><tr><td align="left" valign="bottom" rowspan="2">ProtSSN (ours)</td><td align="left" valign="bottom">k20_h512</td><td align="left" valign="bottom">148</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.419</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.442</styled-content></td><td align="left" valign="bottom">0.458</td><td align="left" valign="bottom">0.371</td><td align="left" valign="bottom">0.436</td><td align="left" valign="bottom">0.387</td><td align="left" valign="bottom">0.566</td><td align="left" valign="bottom">0.444</td></tr><tr><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">1467</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.425</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.440</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.466</styled-content></td><td align="left" valign="bottom">0.371</td><td align="left" valign="bottom">0.451</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.398</styled-content></td><td align="left" valign="bottom">0.568</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.451</styled-content></td></tr></tbody></table><table-wrap-foot><fn><p>The top three are highlighted by first, second, and <bold>third</bold>.</p></fn></table-wrap-foot></table-wrap><p>Compared to protein language models (<xref ref-type="fig" rid="fig2">Figure 2</xref>, colored circles), ProtSSN benefits from abundant structure information to more accurately capture the overall protein characteristics, resulting in enhanced thermostability and thus achieving higher correlation scores on <bold>DTm</bold> and <bold>DDG</bold>. This is attributed to the compactness of the overall architecture as well as the presence of stable local structures such as alpha helices and beta sheets, both of which are crucial to protein thermostability by providing a framework that resists unfolding at elevated temperatures (<xref ref-type="bibr" rid="bib50">Robinson-Rechavi et al., 2006</xref>). Consequently, the other two structures involved models, that is, ESM-if1 and MIF-ST, also exhibit higher performance on the two thermostability benchmarks.</p><p>On the other hand, although protein language models can typically increase their scale, that is, the number of trainable parameters, to capture more information from sequences, they cannot fully replace the role of the structure-based analysis. This observation aligns with the mechanism of protein functionality, where local structures (such as binding pockets and catalytic pockets) and the overall structure (such as conformational changes and allosteric effect) are both crucial for binding and catalysis (<xref ref-type="bibr" rid="bib54">Sheng et al., 2014</xref>). Unlike the thermostability benchmarks, the discrepancies between structure-involved models and protein language models are mitigated in <bold>ProteinGym v1</bold> by increasing the model scale. In summary, structure-based and sequence-based methods are suitable for different types of assays, but a comprehensive framework (such as ProtSSN) demonstrates superior overall performance compared to large-scale language models. Moreover, ProtSSN demonstrates consistency in providing high-quality fitness predictions of thermostability. We randomly bootstrap 50% of the samples from <bold>DTm</bold> and <bold>DDG</bold> for 10 independent runs, the results are reported In Table 3 for both the average performance and the standard deviation. ProtSSN achieves top performance with minimal variance.</p></sec><sec id="s2-2"><title>Ablation study</title><p>The effectiveness of ProtSSN with different modular designs is examined by its performance on the early released version <bold>ProteinGym v0</bold> (<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>) with 86 DMS assays. For the ablation study, the results are summarized in <xref ref-type="fig" rid="fig3">Figure 3a–d</xref>, where each record is subject to a combination of key arguments under investigation. For instance, in the top orange box of <xref ref-type="fig" rid="fig3">Figure 3a</xref>, we report all ablation results that utilize six EGNN layers for graph convolution, regardless of the different scales of ESM2 or the definitions of node attributes. For all modules investigated in this section, we separately discuss their influence on predicting mutation effects when modifying a single site or an arbitrary number of sites. The two cases are marked on the y-axis by ‘single’ and ‘all’, respectively.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Ablation study on <bold>ProteinGym v0</bold> with different modular settings of ProtSSN.</title><p>Each record represents the average Spearman’s correlation of all assays. (<bold>a</bold>) Performance using different structure encoders: Equivariant Graph Neural Networks (EGNN) (orange) versus GCN/GAT (purple). (<bold>b</bold>) Performance using different node attributes: ESM2-embedded hidden representation (orange) versus one-hot encoding (purple). (<bold>c</bold>) Performance with varying numbers of EGNN layers. (<bold>d</bold>) Performance with different versions of ESM2 for sequence encoding. (<bold>e</bold>) Performance using different amino acid perturbation strategies during pre-training.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98033-fig3-v1.tif"/></fig><sec id="s2-2-1"><title>Inclusion of roto-translation equivariance</title><p>We assess the effect of incorporating rotation and translation equivariance for protein geometric and topological encoding. Three types of graph convolutions are compared, including GCN (<xref ref-type="bibr" rid="bib23">Kipf and Welling, 2017</xref>), GAT (<xref ref-type="bibr" rid="bib67">Veličković et al., 2018</xref>), and EGNN (<xref ref-type="bibr" rid="bib53">Satorras et al., 2021</xref>). The first two are classic non-equivariant graph convolutional methods, and the last one, which we apply in the main algorithm, preserves roto-translation equivariance. We fix the number of EGNN layers to 6 and examine the performance of the other two methods with either 4 or 6 layers. We find that integrating equivariance when embedding protein geometry would significantly improve prediction performance.</p></sec><sec id="s2-2-2"><title>Sequence encoding</title><p>We next investigate the benefits of defining data-driven node attributes for protein representation learning. We compare the performance of models trained on two sets of graph inputs: the first set defines its AA node attributes through trained ESM2 (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>), while the second set uses one-hot encoded AAs for each node. A clear advantage of using hidden representations by prefix models over hardcoded attributes is evident from the results presented in <xref ref-type="fig" rid="fig3">Figure 3b</xref>.</p></sec><sec id="s2-2-3"><title>Depth of EGNN</title><p>Although graph neural networks can extract topological information from geometric inputs, it is vital to select an appropriate number of layers for the module to deliver the most expressive node representation without encountering the oversmoothing problem. We investigate a wide range of choices for the number of EGNN layers among <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mn>18</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. As reported in <xref ref-type="fig" rid="fig3">Figure 3c</xref>, embedding graph topology with deeper networks does not lead to performance improvements. A moderate choice of six EGNN layers is sufficient for our learning task.</p></sec><sec id="s2-2-4"><title>Scale of ESM</title><p>We also evaluate ProtSSN on different choices of language embedding dimensions to study the trade-off between computational cost and input richness. Various scales of prefix models, including <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>150</mml:mn><mml:mo>,</mml:mo><mml:mn>650</mml:mn><mml:mo>,</mml:mo><mml:mn>3000</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> millions of parameters, have been applied to produce different sequential embeddings with <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>320</mml:mn><mml:mo>,</mml:mo><mml:mn>640</mml:mn><mml:mo>,</mml:mo><mml:mn>1280</mml:mn><mml:mo>,</mml:mo><mml:mn>2560</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> dimensions, respectively. <xref ref-type="fig" rid="fig3">Figure 3d</xref> reveals a clear preference for ESM2-t33, which employs 650 million parameters to achieve optimal model performance with the best stability. Notably, a higher dimension and richer semantic expression do not always yield better performance. A performance degradation is observed at ESM2 with 3 billion parameters.</p></sec></sec><sec id="s2-3"><title>AA perturbation strategies</title><p>ProtSSN utilizes noise sampling at each epoch on the node attributes to emulate random mutations in nature. This introduction of noise directly affects the node attribute input to the graphs. Alongside the ‘mutate-then-recode’ method we implemented in the main algorithm, we examined four additional strategies to perturb the input data during training. The construction of these additional strategies is detailed below, and their corresponding Spearman’s <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> on <bold>ProteinGym v0</bold> is given in <xref ref-type="fig" rid="fig3">Figure 3e</xref>.</p><sec id="s2-3-1"><title>Global average</title><p>Suppose the encoded sequential representation of a node is predominantly determined by its residue. In essence, the protein sequence encoder will return similar embeddings for nodes of the same residue, albeit at different positions within a protein. With this premise, the first strategy replaces the node embedding for perturbed nodes with the average representations of the same residues. For example, when perturbing an AA to glycine, an overall representation of glycine is assigned by extracting and average-pooling all glycine representations from the input sequence.</p></sec><sec id="s2-3-2"><title>Sliding window</title><p>The presumption in the previous method neither aligns with the algorithmic design nor biological heuristics. Self-attention discerns the interaction of the central token with its neighbors across the entire document (protein). The representation of a residue is influenced by both its neighbors’ residues and locations. Thus, averaging embeddings of residues from varying positions is likely to forfeit positional information of the modified residue. For this purpose, we consider a sliding window of size 3 along the protein sequence.</p></sec><sec id="s2-3-3"><title>Gaussian noise</title><p>The third option regards node embeddings of AA as hidden vectors and imposes white noise on the vector values. We define the mean = 0 and variance = 0.5 on the noise, making the revised node representation <inline-formula><mml:math id="inf9"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s2-3-4"><title>Mask</title><p>Finally, we employ the masking technique prevalent in masked language modeling and substitute the perturbed residue token with a special <monospace>&lt;mask&gt;</monospace> token. The prefix language model will interpret it as a novel token and employ self-attention mechanisms to assign a new representation to it. We utilize the same hyperparameter settings as that of BERT (<xref ref-type="bibr" rid="bib4">Devlin et al., 2018</xref>) and choose 15% of tokens per iteration as potentially influenced subjects. Specifically, 80% of these subjects are replaced with <monospace>&lt;mask&gt;</monospace>, 10% of them are replaced randomly (to other residues), and the remaining 10% stay unchanged.</p></sec></sec><sec id="s2-4"><title>Folding methods</title><p>The analysis of protein structure through topological embeddings poses challenges due to the inherent difficulty in obtaining accurate structural observations through experimental techniques such as NMR (<xref ref-type="bibr" rid="bib71">Wüthrich, 1990</xref>) and cryo-EM (<xref ref-type="bibr" rid="bib5">Elmlund et al., 2017</xref>). As a substitution, we use in silico folding models such as AlphaFold2 (<xref ref-type="bibr" rid="bib19">Jumper et al., 2021</xref>) and ESMFold (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>). Although these folding methods may introduce additional errors due to the inherent uncertainty or inaccuracy in structural predictions, we investigate the robustness of our model in the face of potential inaccuracies introduced by these folding strategies.</p><p><xref ref-type="table" rid="table2">Table 2</xref> examines the impact of different folding strategies on the performance of DMS predictions for ESM-if1, MIF-ST, and ProtSSN. Although the performance based on ESMFold-derived protein structures generally lags behind structures folded by AlphaFold2, the minimal difference between the two strategies across all four benchmarks validates the robustness of our ProtSSN in response to variations in input protein structures. We deliberately divide the assays in <bold>ProteinGym v0</bold> (<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>) into two groups of interaction (e.g., binding and stability) and catalysis (e.g., activity), as the former is believed more sensitive to the structure of the protein (<xref ref-type="bibr" rid="bib49">Robertson and Murphy, 1997</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Influence of folding strategies (AlphaFold2 and ESMFold) on prediction performance for structure-involved models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="3">DTm</th><th align="left" valign="bottom" colspan="3">DDG</th><th align="left" valign="bottom" colspan="3">ProteinGym-interaction</th><th align="left" valign="bottom" colspan="3">ProteinGym-catalysis</th></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"><bold>AlphaFold2</bold></td><td align="left" valign="bottom"><bold>ESMFold</bold></td><td align="left" valign="bottom"><bold>diff ↓</bold></td><td align="left" valign="bottom"><bold>AlphaFold2</bold></td><td align="left" valign="bottom"><bold>ESMFold</bold></td><td align="left" valign="bottom"><bold>diff</bold><bold>↓</bold></td><td align="left" valign="bottom"><bold>AlphaFold2</bold></td><td align="left" valign="bottom"><bold>ESMFold</bold></td><td align="left" valign="bottom"><bold>diff</bold><bold>↓</bold></td><td align="left" valign="bottom"><bold>AlphaFold2</bold></td><td align="left" valign="bottom"><bold>ESMFold</bold></td><td align="left" valign="bottom"><bold>diff</bold><bold>↓</bold></td></tr><tr><td align="left" valign="bottom">Avg. pLDDT</td><td align="left" valign="bottom">90.86</td><td align="left" valign="bottom">83.22</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">95.19</td><td align="left" valign="bottom">86.03</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">82.86</td><td align="left" valign="bottom">65.69</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">85.80</td><td align="left" valign="bottom">73.45</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">ESM-if1</td><td align="left" valign="bottom">0.395</td><td align="left" valign="bottom">0.371</td><td align="left" valign="bottom">0.024</td><td align="left" valign="bottom">0.457</td><td align="left" valign="bottom">0.488</td><td align="left" valign="bottom">–0.031</td><td align="left" valign="bottom">0.351</td><td align="left" valign="bottom">0.259</td><td align="left" valign="bottom">0.092</td><td align="left" valign="bottom">0.386</td><td align="left" valign="bottom">0.368</td><td align="left" valign="bottom">0.018</td></tr><tr><td align="left" valign="bottom">MIF-ST</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.378</td><td align="left" valign="bottom">0.022</td><td align="left" valign="bottom">0.438</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">–0.015</td><td align="left" valign="bottom">0.390</td><td align="left" valign="bottom">0.327</td><td align="left" valign="bottom">0.063</td><td align="left" valign="bottom">0.408</td><td align="left" valign="bottom">0.388</td><td align="left" valign="bottom">0.020</td></tr><tr><td align="left" valign="bottom">k30_h1280</td><td align="left" valign="bottom">0.384</td><td align="left" valign="bottom">0.370</td><td align="left" valign="bottom">0.014</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.390</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.006</styled-content></td><td align="left" valign="bottom">0.398</td><td align="left" valign="bottom">0.373</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.025</styled-content></td><td align="left" valign="bottom">0.443</td><td align="left" valign="bottom">0.439</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.004</styled-content></td></tr><tr><td align="left" valign="bottom">k30_h768</td><td align="left" valign="bottom">0.359</td><td align="left" valign="bottom">0.356</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.003</styled-content></td><td align="left" valign="bottom">0.378</td><td align="left" valign="bottom">0.366</td><td align="left" valign="bottom">0.012</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.374</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.026</styled-content></td><td align="left" valign="bottom">0.442</td><td align="left" valign="bottom">0.436</td><td align="left" valign="bottom"><bold>0.006</bold></td></tr><tr><td align="left" valign="bottom">k30_h512</td><td align="left" valign="bottom">0.413</td><td align="left" valign="bottom">0.399</td><td align="left" valign="bottom">0.014</td><td align="left" valign="bottom">0.408</td><td align="left" valign="bottom">0.394</td><td align="left" valign="bottom">0.014</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.372</td><td align="left" valign="bottom"><bold>0.028</bold></td><td align="left" valign="bottom">0.447</td><td align="left" valign="bottom">0.442</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.005</styled-content></td></tr><tr><td align="left" valign="bottom">k20_h1280</td><td align="left" valign="bottom">0.415</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.024</td><td align="left" valign="bottom">0.429</td><td align="left" valign="bottom">0.410</td><td align="left" valign="bottom">0.019</td><td align="left" valign="bottom">0.399</td><td align="left" valign="bottom">0.365</td><td align="left" valign="bottom">0.034</td><td align="left" valign="bottom">0.446</td><td align="left" valign="bottom">0.441</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.005</styled-content></td></tr><tr><td align="left" valign="bottom">k20_h768</td><td align="left" valign="bottom">0.415</td><td align="left" valign="bottom">0.403</td><td align="left" valign="bottom"><bold>0.012</bold></td><td align="left" valign="bottom">0.419</td><td align="left" valign="bottom">0.397</td><td align="left" valign="bottom">0.022</td><td align="left" valign="bottom">0.401</td><td align="left" valign="bottom">0.370</td><td align="left" valign="bottom">0.031</td><td align="left" valign="bottom">0.449</td><td align="left" valign="bottom">0.442</td><td align="left" valign="bottom">0.007</td></tr><tr><td align="left" valign="bottom">k20_h512</td><td align="left" valign="bottom">0.419</td><td align="left" valign="bottom">0.395</td><td align="left" valign="bottom">0.024</td><td align="left" valign="bottom">0.441</td><td align="left" valign="bottom">0.432</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.009</styled-content></td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.371</td><td align="left" valign="bottom">0.035</td><td align="left" valign="bottom">0.449</td><td align="left" valign="bottom">0.439</td><td align="left" valign="bottom">0.010</td></tr><tr><td align="left" valign="bottom">k10_h1280</td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.015</td><td align="left" valign="bottom">0.426</td><td align="left" valign="bottom">0.411</td><td align="left" valign="bottom">0.015</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.365</td><td align="left" valign="bottom">0.031</td><td align="left" valign="bottom">0.440</td><td align="left" valign="bottom">0.434</td><td align="left" valign="bottom"><bold>0.006</bold></td></tr><tr><td align="left" valign="bottom">k10_h768</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.009</styled-content></td><td align="left" valign="bottom">0.414</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.014</td><td align="left" valign="bottom">0.379</td><td align="left" valign="bottom">0.349</td><td align="left" valign="bottom">0.030</td><td align="left" valign="bottom">0.431</td><td align="left" valign="bottom">0.421</td><td align="left" valign="bottom">0.010</td></tr><tr><td align="left" valign="bottom">k10_h512</td><td align="left" valign="bottom">0.383</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom">0.019</td><td align="left" valign="bottom">0.424</td><td align="left" valign="bottom">0.414</td><td align="left" valign="bottom"><bold>0.010</bold></td><td align="left" valign="bottom">0.389</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.025</styled-content></td><td align="left" valign="bottom">0.440</td><td align="left" valign="bottom">0.432</td><td align="left" valign="bottom">0.008</td></tr></tbody></table><table-wrap-foot><fn><p>The top three are highlighted by first, second, and <bold>third</bold>.</p></fn></table-wrap-foot></table-wrap><p>ProtSSN emerges as the most robust method, maintaining consistent performance across different folding strategies, which underscores the importance of our model’s resilience to variations in input protein structures. The reported performance difference for both ESM-if1 and MIF-ST is 3–5 times higher than that of ProtSSN. The inconsistency between the optimal results for DDG and the scores reported in <xref ref-type="table" rid="table1">Table 1</xref> comes from the utilization of crystal structures in the main results. Another noteworthy observation pertains to the performance of ESM-if1 and MIF-ST on DDG. In this case, predictions based on ESMFold surpass those based on AlphaFold2, even outperforming predictions derived from crystal structures. However, the superior performance of these methods with ESMFold hinges on inaccurate protein structures, rendering them less reliable.</p></sec><sec id="s2-5"><title>Performance enhancement with MSA and ensemble</title><p>We extend the evaluation on <bold>ProteinGym v0</bold> to include a comparison of our individual-sequence-level, zero-shot ProtSSN with methods that include MSAs (which we consider here, ‘few-shot’ methods [<xref ref-type="bibr" rid="bib31">Meier et al., 2021</xref>]). The details are reported in <xref ref-type="table" rid="table3">Table 3</xref>, where the performance of baseline methods is retrieved from <ext-link ext-link-type="uri" xlink:href="https://github.com/OATML-Markslab/ProteinGym">https://github.com/OATML-Markslab/ProteinGym</ext-link>, copy archived at <xref ref-type="bibr" rid="bib44">ProteinGym, 2024</xref>. For ProtSSN, we employ an ensemble of nine models with varying sizes of <italic>k</italic>NN graphs (<inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>) and EGNN hidden neurons (<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>512</mml:mn><mml:mo>,</mml:mo><mml:mn>768</mml:mn><mml:mo>,</mml:mo><mml:mn>1280</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>), as discussed previously. The MSA depth reflects the number of MSA sequences that can be found for the protein, which influences the quality of MSA-based methods.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Variant effect prediction on <bold>ProteinGym v0</bold> with both zero-shot and few-shot methods.</title><p>Results are retrieved from <xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model</th><th align="left" valign="bottom" rowspan="2">Type</th><th align="left" valign="bottom"># Params</th><th align="left" valign="bottom" colspan="3"><inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>(by mutation depth)<inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">↑</mml:mo></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom" colspan="3"><inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>(by MSA depth)<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">↑</mml:mo></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom" colspan="4"><inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula>(by taxon)<inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">↑</mml:mo></mml:mstyle></mml:math></inline-formula></th></tr><tr><th align="left" valign="bottom">(million)</th><th align="left" valign="bottom">Single</th><th align="left" valign="bottom">Double</th><th align="left" valign="bottom">All</th><th align="left" valign="bottom">Low</th><th align="left" valign="bottom">Medium</th><th align="left" valign="bottom">High</th><th align="left" valign="bottom">Prokaryote</th><th align="left" valign="bottom">Human</th><th align="left" valign="bottom">Eukaryote</th><th align="left" valign="bottom">Virus</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="13"><italic>Few-shot methods</italic></td></tr><tr><td align="left" valign="bottom">SiteIndep</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.378</td><td align="left" valign="bottom">0.322</td><td align="left" valign="bottom">0.378</td><td align="left" valign="bottom">0.431</td><td align="left" valign="bottom">0.375</td><td align="left" valign="bottom">0.342</td><td align="left" valign="bottom">0.343</td><td align="left" valign="bottom">0.375</td><td align="left" valign="bottom">0.401</td><td align="left" valign="bottom">0.406</td></tr><tr><td align="left" valign="bottom">EVmutation</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">0.401</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.403</td><td align="left" valign="bottom">0.484</td><td align="left" valign="bottom">0.499</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.429</td><td align="left" valign="bottom">0.381</td></tr><tr><td align="left" valign="bottom">Wavenet</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.399</td><td align="left" valign="bottom">0.344</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.286</td><td align="left" valign="bottom">0.404</td><td align="left" valign="bottom">0.489</td><td align="left" valign="bottom">0.492</td><td align="left" valign="bottom">0.373</td><td align="left" valign="bottom">0.442</td><td align="left" valign="bottom">0.321</td></tr><tr><td align="left" valign="bottom">GEMME</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.460</styled-content></td><td align="left" valign="bottom">0.397</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.463</styled-content></td><td align="left" valign="bottom"><bold>0.444</bold></td><td align="left" valign="bottom">0.446</td><td align="left" valign="bottom">0.520</td><td align="left" valign="bottom">0.505</td><td align="left" valign="bottom">0.436</td><td align="left" valign="bottom">0.479</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.451</styled-content></td></tr><tr><td align="left" valign="bottom" rowspan="2">DeepSequence</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.411</td><td align="left" valign="bottom">0.358</td><td align="left" valign="bottom">0.416</td><td align="left" valign="bottom">0.386</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.505</td><td align="left" valign="bottom">0.497</td><td align="left" valign="bottom">0.396</td><td align="left" valign="bottom">0.461</td><td align="left" valign="bottom">0.332</td></tr><tr><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.433</td><td align="left" valign="bottom">0.394</td><td align="left" valign="bottom">0.435</td><td align="left" valign="bottom">0.386</td><td align="left" valign="bottom">0.411</td><td align="left" valign="bottom">0.534</td><td align="left" valign="bottom">0.522</td><td align="left" valign="bottom">0.405</td><td align="left" valign="bottom">0.480</td><td align="left" valign="bottom">0.361</td></tr><tr><td align="left" valign="bottom" rowspan="2">EVE</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">0.451</td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.452</td><td align="left" valign="bottom">0.417</td><td align="left" valign="bottom">0.434</td><td align="left" valign="bottom">0.525</td><td align="left" valign="bottom">0.518</td><td align="left" valign="bottom">0.411</td><td align="left" valign="bottom">0.469</td><td align="left" valign="bottom">0.436</td></tr><tr><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom"><bold>0.459</bold></td><td align="left" valign="bottom"><bold>0.409</bold></td><td align="left" valign="bottom"><bold>0.459</bold></td><td align="left" valign="bottom">0.424</td><td align="left" valign="bottom">0.441</td><td align="left" valign="bottom">0.532</td><td align="left" valign="bottom">0.526</td><td align="left" valign="bottom">0.419</td><td align="left" valign="bottom">0.481</td><td align="left" valign="bottom"><bold>0.437</bold></td></tr><tr><td align="left" valign="bottom" rowspan="2">MSA-Transfomer</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">0.405</td><td align="left" valign="bottom">0.358</td><td align="left" valign="bottom">0.426</td><td align="left" valign="bottom">0.372</td><td align="left" valign="bottom">0.415</td><td align="left" valign="bottom">0.500</td><td align="left" valign="bottom">0.506</td><td align="left" valign="bottom">0.387</td><td align="left" valign="bottom">0.468</td><td align="left" valign="bottom">0.379</td></tr><tr><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">500</td><td align="left" valign="bottom">0.440</td><td align="left" valign="bottom">0.374</td><td align="left" valign="bottom">0.440</td><td align="left" valign="bottom">0.387</td><td align="left" valign="bottom">0.428</td><td align="left" valign="bottom">0.513</td><td align="left" valign="bottom">0.517</td><td align="left" valign="bottom">0.398</td><td align="left" valign="bottom">0.467</td><td align="left" valign="bottom">0.406</td></tr><tr><td align="left" valign="bottom">Tranception-R</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom">0.450</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.427</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.453</styled-content></td><td align="left" valign="bottom">0.442</td><td align="left" valign="bottom"><bold>0.438</bold></td><td align="left" valign="bottom">0.500</td><td align="left" valign="bottom">0.495</td><td align="left" valign="bottom">0.424</td><td align="left" valign="bottom"><bold>0.485</bold></td><td align="left" valign="bottom">0.434</td></tr><tr><td align="left" valign="bottom">TranceptEVE</td><td align="left" valign="bottom">Single</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.481</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.445</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.481</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.454</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.465</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.542</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.539</styled-content></td><td align="left" valign="bottom"><bold>0.447</bold></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.498</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.461</styled-content></td></tr><tr><td align="left" valign="bottom" colspan="13"><italic>Zero-shot methods</italic></td></tr><tr><td align="left" valign="bottom">RITA</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">2,210</td><td align="left" valign="bottom">0.393</td><td align="left" valign="bottom">0.236</td><td align="left" valign="bottom">0.399</td><td align="left" valign="bottom">0.350</td><td align="left" valign="bottom">0.414</td><td align="left" valign="bottom">0.407</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.405</td><td align="left" valign="bottom">0.417</td></tr><tr><td align="left" valign="bottom">ESM-1v</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">3,250</td><td align="left" valign="bottom">0.416</td><td align="left" valign="bottom">0.309</td><td align="left" valign="bottom">0.417</td><td align="left" valign="bottom">0.390</td><td align="left" valign="bottom">0.378</td><td align="left" valign="bottom">0.536</td><td align="left" valign="bottom">0.521</td><td align="left" valign="bottom">0.439</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">0.268</td></tr><tr><td align="left" valign="bottom">ProGen2</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">10,779</td><td align="left" valign="bottom">0.421</td><td align="left" valign="bottom">0.312</td><td align="left" valign="bottom">0.423</td><td align="left" valign="bottom">0.384</td><td align="left" valign="bottom">0.421</td><td align="left" valign="bottom">0.467</td><td align="left" valign="bottom">0.497</td><td align="left" valign="bottom">0.412</td><td align="left" valign="bottom">0.459</td><td align="left" valign="bottom">0.373</td></tr><tr><td align="left" valign="bottom">ProtTrans</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">6,840</td><td align="left" valign="bottom">0.417</td><td align="left" valign="bottom">0.360</td><td align="left" valign="bottom">0.413</td><td align="left" valign="bottom">0.372</td><td align="left" valign="bottom">0.395</td><td align="left" valign="bottom">0.492</td><td align="left" valign="bottom">0.498</td><td align="left" valign="bottom">0.419</td><td align="left" valign="bottom">0.400</td><td align="left" valign="bottom">0.322</td></tr><tr><td align="left" valign="bottom">ESM2</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">18,843</td><td align="left" valign="bottom">0.415</td><td align="left" valign="bottom">0.316</td><td align="left" valign="bottom">0.413</td><td align="left" valign="bottom">0.391</td><td align="left" valign="bottom">0.381</td><td align="left" valign="bottom">0.509</td><td align="left" valign="bottom">0.508</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.456</styled-content></td><td align="left" valign="bottom">0.461</td><td align="left" valign="bottom">0.213</td></tr><tr><td align="left" valign="bottom">SaProt</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">1,285</td><td align="left" valign="bottom">0.447</td><td align="left" valign="bottom">0.368</td><td align="left" valign="bottom">0.450</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.456</styled-content></td><td align="left" valign="bottom">0.410</td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.544</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.534</styled-content></td><td align="left" valign="bottom"><styled-content style="color: #D50000;">0.464</styled-content></td><td align="left" valign="bottom">0.460</td><td align="left" valign="bottom">0.334</td></tr><tr><td align="left" valign="bottom">ProtSSN</td><td align="left" valign="bottom">Ensemble</td><td align="left" valign="bottom">1,476</td><td align="left" valign="bottom">0.433</td><td align="left" valign="bottom">0.381</td><td align="left" valign="bottom">0.433</td><td align="left" valign="bottom">0.406</td><td align="left" valign="bottom">0.402</td><td align="left" valign="bottom">0.532</td><td align="left" valign="bottom"><bold>0.530</bold></td><td align="left" valign="bottom">0.436</td><td align="left" valign="bottom"><styled-content style="color: #9C27B0;">0.491</styled-content></td><td align="left" valign="bottom">0.290</td></tr></tbody></table><table-wrap-foot><fn><p>The top three are highlighted by first, second, and <bold>third</bold>.</p></fn></table-wrap-foot></table-wrap><p>ProtSSN demonstrates superior performance among non-MSA zero-shot methods in predicting both single-site and multi-site mutants, underscoring its pivotal role in guiding protein engineering toward deep mutation. Furthermore, ProtSSN outperforms MSA-based methods across taxa, except for viruses, where precise identification of conserved sites is crucial for positive mutations, especially in spike proteins (<xref ref-type="bibr" rid="bib20">Katoh and Standley, 2021</xref>; <xref ref-type="bibr" rid="bib32">Mercurio et al., 2021</xref>). In essence, ProtSSN provides an efficient and effective solution for variant effects prediction when compared to both non-MSA and MSA-based methods. Note that although MSA-based methods generally consume fewer trainable parameters than non-MSA methods, they incur significantly higher costs in terms of search time and memory usage. Moreover, the inference performance of MSA-based methods relies heavily on the quality of the input MSA, where the additionally involved variance makes impacts the stability of the model performance.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The development of modern computational methodologies for protein engineering is a crucial facet of in silico protein design. Effectively assessing the fitness of protein mutants not only supports cost-efficient experimental validations but also guides the modification of proteins toward enhanced or novel functions. Traditionally, computational methods rely on analyzing small sets of labeled data for specific protein assays, such as FLIP (<xref ref-type="bibr" rid="bib3">Dallago et al., 2021</xref>), PEER (<xref ref-type="bibr" rid="bib72">Xu et al., 2022</xref>), and PETA (<xref ref-type="bibr" rid="bib60">Tan et al., 2024b</xref>). More recent work has also focused on examining the relationship between models and supervised fitness prediction (<xref ref-type="bibr" rid="bib27">Li et al., 2024</xref>). However, obtaining experimental data is often infeasible in real-world scenarios, particularly for positive mutants, due to the cost and complexity of protein engineering. This limitation renders supervised learning methods impractical for many applications. As a result, it is crucial to develop solutions that can efficiently suggest site mutations for wet experiments, even when starting from scratch without prior knowledge. Recent deep learning solutions employ a common strategy that involves establishing a hidden protein representation and masking potential mutation sites to recommend plausible AAs. Previous research has primarily focused on extracting protein representations from either their sequential or structural modalities, with many treating the prediction of mutational effects merely as a secondary task following inverse folding or de novo protein design. These approaches often overlook the importance of comprehensive investigation on both global and local perspectives of AA interaction that are critical for featuring protein functions. Furthermore, these methods hardly tailored model designs for suggesting mutations, despite the significance of this type of task. In this work, we introduce ProtSSN, a denoising framework that effectively cascades protein sequence and structure embedding for predicting mutational effects. Both the protein language model and equivariant graph neural network are employed to encode the global interaction of AAs with geometry-aware local enhancements.</p><p>On the other hand, existing benchmarks for evaluating computational solutions mostly focus on assessing model generalization on large-scale datasets (e.g., over 2 million mutants in <bold>ProteinGym v1</bold>). However, such high-throughput datasets often lack accurate experimental measurements (<xref ref-type="bibr" rid="bib9">Frazer et al., 2021</xref>), leading to significant noise in the labels. Furthermore, the larger data volumes typically do not include environmental labels for mutants (e.g., temperature, pH), despite the critical importance of these conditions for biologists. In response, we propose <bold>DTm</bold> and <bold>DDG</bold>. These low-throughput datasets, which we have collected, emphasize the consistency of experimental conditions and data accuracy. They are traceable and serve as valuable complements to ProteinGym, providing a more detailed and reliable evaluation of computational models. We have extensively validated the efficacy of ProtSSN across various protein function assays and taxa, including two thermostability datasets prepared by ourselves. Our approach consistently demonstrates substantial promise for protein engineering applications, such as facilitating the design of mutation sequences with enhanced interaction and enzymatic activity (<xref ref-type="table" rid="table4">Table 4</xref>).</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Average Spearman’s <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> correlation of variant effect prediction on <bold>DTm</bold> and <bold>DDG</bold> for zero-shot methods with model ensemble.</title><p>The values within () indicate the standard deviation of bootstrapping.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom"># Params (million)</th><th align="left" valign="bottom">DTm</th><th align="left" valign="bottom">DDG</th></tr></thead><tbody><tr><td align="left" valign="bottom">RITA</td><td align="left" valign="bottom">2210</td><td align="left" valign="bottom">0,195<sub>(0.045)</sub></td><td align="left" valign="bottom">0.255<sub>(0.061)</sub></td></tr><tr><td align="left" valign="bottom">ESM-1v</td><td align="left" valign="bottom">3250</td><td align="left" valign="bottom">0.300<sub>(0.036)</sub></td><td align="left" valign="bottom">0.310<sub>(0.054)</sub></td></tr><tr><td align="left" valign="bottom">Tranception</td><td align="left" valign="bottom">1085</td><td align="left" valign="bottom">0.202<sub>(0.039)</sub></td><td align="left" valign="bottom">0.277<sub>(0.062)</sub></td></tr><tr><td align="left" valign="bottom">ProGen2</td><td align="left" valign="bottom">10,779</td><td align="left" valign="bottom">0.293<sub>(0.042)</sub></td><td align="left" valign="bottom">0.282<sub>(0.063)</sub></td></tr><tr><td align="left" valign="bottom">ProtTrans</td><td align="left" valign="bottom">6840</td><td align="left" valign="bottom">0.323<sub>(0.039)</sub></td><td align="left" valign="bottom">0.389<sub>(0.059)</sub></td></tr><tr><td align="left" valign="bottom">ESM2</td><td align="left" valign="bottom">18,843</td><td align="left" valign="bottom">0.346<sub>(0.035)</sub></td><td align="left" valign="bottom">0.383<sub>(0.55)</sub></td></tr><tr><td align="left" valign="bottom">SaProt</td><td align="left" valign="bottom">1285</td><td align="left" valign="bottom">0.392<sub>(0.040)</sub></td><td align="left" valign="bottom">0.415<sub>(0.061)</sub></td></tr><tr><td align="left" valign="bottom">ProtSSN</td><td align="left" valign="bottom">1476</td><td align="left" valign="bottom"><bold>0.425<sub>(0.033)</sub></bold></td><td align="left" valign="bottom"><bold>0.440<sub>(0.057)</sub></bold></td></tr></tbody></table></table-wrap></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>This section starts by introducing the three mutational scanning benchmark datasets used in this study, including an open benchmark dataset <bold>ProteinGym v1</bold> and two new datasets on protein thermostability, that is, <bold>DTm</bold> and <bold>DDG</bold>. We establish the proposed ProtSSN for protein sequences and structures encoding in the section ‘Proposed method’. The overall pipeline of model training and inference is presented in the section ‘Model pipeline’. The experimental setup for model evaluation is provided in the section ‘Experimantal protocol’.</p><sec id="s4-1"><title>Dataset</title><p>To evaluate model performance in predicting mutation effects, we compare ProtSSN with SOTA baselines on <bold>ProteinGym v1</bold>, the largest open benchmark for DMS assays. We also introduce two novel benchmarks, <bold>DTm</bold> and <bold>DDG</bold>, for assessing protein thermostability under consistent control environments. Details of the new datasets are provided in <xref ref-type="table" rid="table5">Table 5</xref>. Considering the significant influence of experimental conditions on protein temperature, we explicitly note the pH environment in each assay. Refer to the following paragraphs for more details.</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Statistical summary of <bold>DTm</bold> and <bold>DDG</bold>.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">pH range</th><th align="left" valign="bottom" colspan="3">DTm</th><th align="left" valign="bottom" colspan="3">DDG</th></tr><tr><th align="left" valign="bottom"># Assays</th><th align="left" valign="bottom">Avg. # mut</th><th align="left" valign="bottom">Avg. AA</th><th align="left" valign="bottom"># Assays</th><th align="left" valign="bottom">Avg. # mut</th><th align="left" valign="bottom">Avg.AA</th></tr></thead><tbody><tr><td align="left" valign="bottom">Acid (pH &lt; 7)</td><td align="left" valign="bottom">29</td><td align="left" valign="bottom">29.1</td><td align="left" valign="bottom">272.8</td><td align="left" valign="bottom">21</td><td align="left" valign="bottom">22.6</td><td align="left" valign="bottom">125.3</td></tr><tr><td align="left" valign="bottom">Neutral (pH = 7)</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">37.4</td><td align="left" valign="bottom">221.2</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">23.1</td><td align="left" valign="bottom">78.3</td></tr><tr><td align="left" valign="bottom">Alkaline (pH &gt; 7)</td><td align="left" valign="bottom">23</td><td align="left" valign="bottom">50.1</td><td align="left" valign="bottom">233.3</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">24.6</td><td align="left" valign="bottom">101.4</td></tr><tr><td align="left" valign="bottom">Sum</td><td align="left" valign="bottom">66</td><td align="left" valign="bottom">38.2</td><td align="left" valign="bottom">221.2</td><td align="left" valign="bottom">36</td><td align="left" valign="bottom">23.0</td><td align="left" valign="bottom">108.8</td></tr></tbody></table></table-wrap></sec><sec id="s4-2"><title>ProteinGym</title><p>The assessment of ProtSSN for deep mutation effects prediction is conducted on <bold>ProteinGym v1</bold> (<xref ref-type="bibr" rid="bib42">Notin et al., 2023</xref>). It is the most extensive protein substitution benchmark comprising 217 assays and more than 2.5<italic>M</italic> mutants. These DMS assays cover a broad spectrum of functional properties (e.g., ligand binding, aggregation, viral replication, and drug resistance) and span various protein families (e.g., kinases, ion channel proteins, transcription factors, and tumor suppressors) across different taxa (e.g., humans, other eukaryotes, prokaryotes, and viruses) (<xref ref-type="table" rid="table6">Table 6</xref>).</p><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Details of zero-shot baseline models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model</th><th align="left" valign="bottom" colspan="2">Type</th><th align="left" valign="bottom" rowspan="2">Description</th><th align="left" valign="bottom" rowspan="2">Source code</th></tr><tr><th align="left" valign="bottom">Seq</th><th align="left" valign="bottom">Struct</th></tr></thead><tbody><tr><td align="left" valign="bottom">RITA (<xref ref-type="bibr" rid="bib11">Hesslow et al., 2022</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/><td align="left" valign="bottom">A generative protein language model with billion-level parameters</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/lightonai/RITA">https://github.com/lightonai/RITA</ext-link> (<xref ref-type="bibr" rid="bib12">Hesslow and Poli, 2023</xref>)</td></tr><tr><td align="left" valign="bottom">ProGen2 (<xref ref-type="bibr" rid="bib37">Nijkamp et al., 2023</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/><td align="left" valign="bottom">A generative protein language model with billion-level parameters</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/salesforce/progen">https://github.com/salesforce/progen</ext-link> (<xref ref-type="bibr" rid="bib36">Nijkamp, 2022</xref>)</td></tr><tr><td align="left" valign="bottom">ProtTrans (<xref ref-type="bibr" rid="bib6">Elnaggar et al., 2021</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Transformer-based models trained on large protein sequence corpus</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/agemagician/ProtTrans">https://github.com/agemagician/ProtTrans</ext-link> (<xref ref-type="bibr" rid="bib7">Elnaggar and Heinzinger, 2025</xref>)</td></tr><tr><td align="left" valign="bottom">Tranception (<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/><td align="left" valign="bottom">An autoregressive model for variant effect prediction with retrieve machine</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/OATML-Markslab/Tranception">https://github.com/OATML-Markslab/Tranception</ext-link> (<xref ref-type="bibr" rid="bib41">Notin, 2023</xref>)</td></tr><tr><td align="left" valign="bottom">CARP (<xref ref-type="bibr" rid="bib75">Yang et al., 2024</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Pretrained CNN protein sequence masked language models of various sizes</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/protein-sequence-models">https://github.com/microsoft/protein-sequence-models</ext-link> (<xref ref-type="bibr" rid="bib33">microsoft, 2024</xref>)</td></tr><tr><td align="left" valign="bottom">ESM-1b (<xref ref-type="bibr" rid="bib48">Rives et al., 2021</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/><td align="left" valign="bottom" rowspan="3">A masked language model-based pre-train method with various pre-training<break/>dataset and positional embedding strategies</td><td align="left" valign="middle" rowspan="4"><ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/esm">https://github.com/facebookresearch/esm</ext-link> (<xref ref-type="bibr" rid="bib8">facebookresearch, 2023</xref>)</td></tr><tr><td align="left" valign="bottom">ESM-1v (<xref ref-type="bibr" rid="bib31">Meier et al., 2021</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">ESM2 (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">ESM-if1 (<xref ref-type="bibr" rid="bib14">Hsu et al., 2022</xref>)</td><td align="left" valign="bottom"/><td align="left" valign="bottom">✓</td><td align="left" valign="bottom">An inverse folding method with mask language modeling and geometric<break/>vector perception (<xref ref-type="bibr" rid="bib18">Jing et al., 2020</xref>)</td></tr><tr><td align="left" valign="bottom">MIF-ST (<xref ref-type="bibr" rid="bib74">Yang et al., 2023</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom">Pretrained masked inverse folding models with sequence pretraining transfer</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/protein-sequence-models">https://github.com/microsoft/protein-sequence-models</ext-link></td></tr><tr><td align="left" valign="bottom">SaProt (<xref ref-type="bibr" rid="bib56">Su et al., 2023</xref>)</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom">✓</td><td align="left" valign="bottom">Structure-aware vocabulary for protein language modeling with FoldSeek</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/westlake-repl/SaProt">https://github.com/westlake-repl/SaProt</ext-link> (<xref ref-type="bibr" rid="bib57">Su and fajieyuan, 2025</xref>)</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>DTm</title><p>The assays in the first novel thermostability benchmark are sourced from single-site mutations in <bold>ProThermDB</bold> (<xref ref-type="bibr" rid="bib38">Nikam et al., 2021</xref>). Each assay is named in the format ‘<italic>UniProt_ID-pH’</italic>. For instance, ‘<italic>O00095-8.0’</italic> signifies mutations conducted and evaluated under pH 8.0 for protein O00095. The attributes include ‘mutant’, ‘score’, and ‘UniProt_ID’, with at least 10 mutants to ensure a meaningful evaluation. To concentrate on single-site mutations compared to wild-type proteins, we exclude records with continuous mutations. To avoid dealing with partial or misaligned protein structures resulting from incomplete wet experimental procedures, we employ UniProt ID to pair protein sequences with folded structures predicted by AlphaFold2 (<xref ref-type="bibr" rid="bib19">Jumper et al., 2021</xref>) from <ext-link ext-link-type="uri" xlink:href="https://alphafold.ebi.ac.uk">https://alphafold.ebi.ac.uk</ext-link>. In total, <bold>DTm</bold> consists of 66 such protein–environment pairs and 2520 mutations.</p></sec><sec id="s4-4"><title>DDG</title><p>The second novel thermostability benchmark is sourced from <bold>DDMut</bold> (<xref ref-type="bibr" rid="bib80">Zhou et al., 2023</xref>), where mutants are paired with their PDB documents. We thus employ crystal structures for proteins in DDG and conduct additional preprocessing steps for environmental consistency. In particular, we group mutants of the same protein under identical experimental conditions and examine the alignment between mutation sites and sequences extracted from crystal structures. We discard mutant records that cannot be aligned with the sequence loaded from the associated PDB document. After removing data points with fewer than 10 mutations, <bold>DDG</bold> comprises 36 data points and 829 mutations. As before, assays are named in the format ‘<italic>PDB_ID-pH-temperature’</italic> to indicate the chemical condition (pH) and temperature (in Celsius) of the experiment on the protein. An example assay content is provided in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>An example source record of the mutation assay.</title><p>The record is derived from DDG for the A chain of protein 1A7V, experimented at pH = 6.5 and degree at 25°C.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98033-fig4-v1.tif"/></fig></sec><sec id="s4-5"><title>Proposed method</title><p>Labeled data are usually scarce in biomolecule research, which demands designing a general self-supervised model for predicting variant effects on unknown proteins and protein functions. We propose ProtSSN with a pre-training scheme that recovers residues from a given protein backbone structure with noisy local environment observations.</p></sec><sec id="s4-6"><title>Multilevel protein representation</title><sec id="s4-6-1"><title>Protein primary structure (Noisy)</title><p>Denote a given observation with residue <inline-formula><mml:math id="inf19"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula>. We assume this arbitrary observation is undergoing random mutation toward a stable state. The training objective is to learn a revised state <inline-formula><mml:math id="inf20"><mml:mi mathvariant="bold-italic">𝒗</mml:mi></mml:math></inline-formula> that is less likely to be eliminated by natural selection due to unfavorable properties such as instability or inability to fold. The perturbed observation is defined by a multinomial distribution.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi mathvariant="bold-italic">π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∣</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo/><mml:mo>,</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where an AA in a protein chain has a chance of <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> to mutate to one of 20 AAs (including itself) following a <italic>replacement distribution</italic> <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> of remaining unchanged. We consider <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> as a tunable parameter and define <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by the frequency of residues observed in wild-type proteins.</p></sec><sec id="s4-6-2"><title>Protein tertiary structure</title><p>The geometry of a protein is described by <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒢</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒱</mml:mi><mml:mo>,</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝑿</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is a residue graph based on the <italic>k</italic>-nearest neighbor algorithm (<italic>k</italic>NN). Each node <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">V</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents an AA in the protein connected to up to k closest nodes within 30 Å. Node attributes <inline-formula><mml:math id="inf28"><mml:msub><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> are hidden semantic embeddings of residues extracted by the semantic encoder (see the section ‘Semantic encoding of global AA contacts), and edge attributes <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>93</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> feature relationships of connected nodes based on inter-atomic distances, local N-C positions, and sequential position encoding (<xref ref-type="bibr" rid="bib82">Zhou et al., 2024b</xref>). Additionally, <inline-formula><mml:math id="inf30"><mml:msub><mml:mi mathvariant="bold-italic">𝑿</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> records 3D coordinates of AAs in the Euclidean space, which plays a crucial role in the subsequent geometric embedding stage to preserve roto-translation equivariance.</p></sec></sec><sec id="s4-7"><title>Semantic encoding of global AA contacts</title><p>Although it is generally believed by the community that a protein’s sequence determines its biological function via the folded structure, following strictly to this singular pathway risks overlooking other unobserved yet potentially influential inter-atomic communications impacting protein fitness. Our proposed ProtSSN thus includes pairwise relationships for residues through an analysis of proteins’ primary structure from <inline-formula><mml:math id="inf31"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝑽</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> and embeds them into hidden representations <inline-formula><mml:math id="inf32"><mml:msub><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> for residues. At each iteration, the input sequences are modified randomly by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and then embedded via an <italic>Evolutionary Scale Modeling</italic> method, ESM2 (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>), which employs a BERT-style masked language modeling objective. This objective predicts the identity of randomly selected AAs in a protein sequence by observing their context within the remainder of the sequence. Alternative semantic encoding strategies are discussed in the section ‘AA perturbation strategies’.</p></sec><sec id="s4-8"><title>Geometrical encoding of local AA contacts</title><p>Proteins are structured in 3D space, which requires the geometric encoder to possess roto-translation equivariance to node positions as well as permutation invariant to node attributes. This design is vital to avoid the implementation of costly data augmentation strategies. We practice EGNN (<xref ref-type="bibr" rid="bib53">Satorras et al., 2021</xref>) to acquire the hidden node representation <inline-formula><mml:math id="inf33"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal"/><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and node coordinates <inline-formula><mml:math id="inf34"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">𝑿</mml:mi><mml:mtext>pos</mml:mtext><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal"/><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the <italic>l</italic>+1th layer<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In these equations, <inline-formula><mml:math id="inf35"><mml:msub><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msub></mml:math></inline-formula> represents the input edge attribute on <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">V</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, which is not updated by the network. The propagation rules <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are defined by differentiable functions, for example, .multilayer perceptrons (MLPs). The final hidden representation on nodes <inline-formula><mml:math id="inf39"><mml:msubsup><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula> embeds the microenvironment and local topology of AAs, and it will be carried on by readout layers for label predictions.</p></sec><sec id="s4-9"><title>Model pipeline</title><p>ProtSSN is designed for protein engineering with a self-supervised learning scheme. The model is capable of conducting zero-shot variant effect prediction on an unknown protein, and it can generate the joint distribution for the residue of all AAs along the protein sequence of interest. This process accounts for the epistatic effect and concurrently returns all AA sites in a sequence. Below, we detail the workflow for training the zero-shot model and scoring the effect of a specific mutant.</p><sec id="s4-9-1"><title>Training</title><p>The fundamental model architecture cascades a frozen sequence encoding module and a trainable tertiary structure encoder. The protein is represented by its sequence and structure characteristics, where the former is treated as plain text and the latter is formalized as a <italic>k</italic>NN graph with model-encoded node attributes, handcrafted edge attributes, and 3D positions of the corresponding AAs. Each input sequence <inline-formula><mml:math id="inf40"><mml:mi mathvariant="bold-italic">𝑽</mml:mi></mml:math></inline-formula> are first one-hot encoded by 33 tokens, comprising both AA residues and special tokens (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>). The ground truth sequences, during training, will be permuted into<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf41"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">𝑽</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> is the perturbed AA to be encoded by a protein language model, which analyses pairwise hidden relationships among AAs from the input protein sequence and produces a vector representation <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝑾</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for each AA, that is,<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>𝑾</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mtext>LM</mml:mtext><mml:mtext>frozen</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>𝑽</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The language model <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>LM</mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>frozen</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, ESM2 (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>) for instance, has been pre-trained on a massive protein sequence database (e.g., UniRef50 [<xref ref-type="bibr" rid="bib58">Suzek et al., 2015</xref>]) to understand the semantic and grammatical rules of wild-type proteins with high-dimensional AA-level long-short-range interactions. The independent perturbation on AA residues follows (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) operates in every epoch, which requires constant updates on the sequence embedding. To further enhance the interaction of locally connected AAs in the node representation, a stack of LEGNN layers is implemented on the input graph <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">G</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to yield<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>𝑾</mml:mi><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>EGNN</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒢</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>During the pre-training phase for protein sequence recovery, the output layer <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> provides the probability of tokens in one of 33 types, that is,<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>𝒀</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>𝑾</mml:mi><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo/><mml:mn>33</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>for a protein of <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> AAs. The model’s learnable parameters are refined by minimizing the cross-entropy of the recovered AAs with respect to the ground-truth AAs in wild-type proteins:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mtext>loss</mml:mtext><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>33</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtext>softmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> is the length of the sequence, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> represents the position index within the sequence, and each <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> corresponds to a potential type of token.</p></sec><sec id="s4-9-2"><title>Inferencing</title><p>For a given wild-type protein and a set of mutants, the fitness scores of the mutants are derived from the joint distribution of the altered residues relative to the wild-type template. First, the wild-type protein sequence and structure are encoded into hidden representations following (<xref ref-type="disp-formula" rid="equ4 equ5">Equations 4 and 5</xref>). Unlike the pre-training process, here the residue in the wild-type protein is considered as a reference state and is compared with the predicted probability of AAs at the mutated site. Next, for a mutant with mutated sites <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">T</mml:mi></mml:mrow></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>), we define its fitness score <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> using the corresponding <italic>log-odds ratio</italic>, that is,<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒯</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒗</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf53"><mml:msub><mml:mi mathvariant="bold-italic">𝒚</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf54"><mml:msub><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> denote the mutated and wild-type residue of the <italic>t</italic>th AA.</p></sec></sec><sec id="s4-10"><title>Experimental protocol</title><p>We validate the efficacy of ProtSSN on zero-shot mutation effect prediction tasks. The performance is compared with other SOTA models of varying numbers of trainable parameters. The implementations are programmed with <monospace>PyTorch-Geometric</monospace> (ver 2.2.0) and <monospace>PyTorch</monospace> (ver 1.12.1) and executed on an NVIDIA Tesla A100 GPU with 6912 CUDA cores and 80GB HBM2 installed on an HPC cluster.</p></sec><sec id="s4-11"><title>Training setup</title><p>ProtSSN is pre-trained on a non-redundant subset of <bold>CATH v4.3.0</bold> (<xref ref-type="bibr" rid="bib43">Orengo et al., 1997</xref>) domains, which contains 30,948 experimental protein structures with less than 40% sequence identity. We remove proteins that exceed 2000 AAs in length for efficiency. For each <italic>k</italic>NN protein graph, node features are extracted and updated by a frozen ESM2-t33 prefix model (<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>). Protein topology is inferred by a six-layer EGNN (<xref ref-type="bibr" rid="bib53">Satorras et al., 2021</xref>) with the hidden dimension tuned from <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>512</mml:mn><mml:mo>,</mml:mo><mml:mn>768</mml:mn><mml:mo>,</mml:mo><mml:mn>1280</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. Adam (<xref ref-type="bibr" rid="bib22">Kingma and Ba, 2015</xref>) is used for backpropagation with the learning rate set to 0.0001. To avoid training instability or CUDA out-of-memory, we limit the maximum input to 8192 AA tokens per batch, constituting approximately 32 graphs.</p></sec><sec id="s4-12"><title>Baseline methods</title><p>We undertake an extensive comparison with baseline methods of self-supervised sequence or structure-based models on the fitness of mutation effects prediction (<xref ref-type="table" rid="table6">Table 5</xref>). Sequence models employ position embedding strategies such as autoregression (RITA [<xref ref-type="bibr" rid="bib11">Hesslow et al., 2022</xref>], Tranception [<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>], TraceptEVE [<xref ref-type="bibr" rid="bib40">Notin et al., 2022b</xref>], and ProGen2 [<xref ref-type="bibr" rid="bib37">Nijkamp et al., 2023</xref>]), masked language modeling (ESM-1b [<xref ref-type="bibr" rid="bib48">Rives et al., 2021</xref>], ESM-1v [<xref ref-type="bibr" rid="bib31">Meier et al., 2021</xref>], and ESM2 [<xref ref-type="bibr" rid="bib28">Lin et al., 2023</xref>]), a combination of both (ProtTrans [<xref ref-type="bibr" rid="bib6">Elnaggar et al., 2021</xref>]), and convolutional modeling (CARP [<xref ref-type="bibr" rid="bib75">Yang et al., 2024</xref>]). Additionally, we also compare with ESM-if1 (<xref ref-type="bibr" rid="bib14">Hsu et al., 2022</xref>) which incorporates masked language modeling objectives with GVP (<xref ref-type="bibr" rid="bib18">Jing et al., 2020</xref>), as well as MIF-ST (<xref ref-type="bibr" rid="bib74">Yang et al., 2023</xref>) and SaProt (<xref ref-type="bibr" rid="bib56">Su et al., 2023</xref>) with both sequence and structure encoding.</p><p>The majority of the performance comparison was conducted on zero-shot deep learning methods. However, for completeness, we also report a comparison with popular MSA-based methods, such as GEMME (<xref ref-type="bibr" rid="bib26">Laine et al., 2019</xref>), Site-Independent (<xref ref-type="bibr" rid="bib13">Hopf et al., 2017</xref>), EVmutation (<xref ref-type="bibr" rid="bib13">Hopf et al., 2017</xref>), Wavenet (<xref ref-type="bibr" rid="bib55">Shin et al., 2021</xref>), DeepSequence (<xref ref-type="bibr" rid="bib47">Riesselman et al., 2018</xref>), and EVE (<xref ref-type="bibr" rid="bib9">Frazer et al., 2021</xref>). Other deep learning methods use MSA for training or retrieval, including MSA-Transformer (<xref ref-type="bibr" rid="bib46">Rao et al., 2021b</xref>), Tranception (<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>), and TranceptEVE (<xref ref-type="bibr" rid="bib40">Notin et al., 2022b</xref>).</p></sec><sec id="s4-13"><title>Scoring function</title><p>Following the convention, the fitness scores of the zero-shot models are calculated by the log-odds ratio in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> for encoder methods, such as the proposed ProtSSN and the ProtTrans series models. For autoregressive and inverse folding models (e.g., Tranception and ProGen2) that predict the next token <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> based on the context of previous <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> tokens, the fitness score <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of a mutated sequence <inline-formula><mml:math id="inf59"><mml:mi mathvariant="bold-italic">𝒚</mml:mi></mml:math></inline-formula> is computed via the log-likelihood ratio with the wild-type sequence <inline-formula><mml:math id="inf60"><mml:mi mathvariant="bold-italic">𝒗</mml:mi></mml:math></inline-formula>, that is,<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-14"><title>Benchmark datasets</title><p>We conduct a comprehensive comparison of deep learning predictions on hundreds of DMS assays concerning different protein types and biochemical assays. We prioritize experimentally measured properties that possess a monotonic relationship with protein fitness, such as catalytic activity, binding affinity, expression, and thermostability. In particular, <bold>ProteinGym v1</bold> (<xref ref-type="bibr" rid="bib42">Notin et al., 2023</xref>) groups five sets of protein properties from 217 assays. Two new datasets named <bold>DTm</bold> and <bold>DDG</bold> examine 102 environment-specific assays with thermostability scores. In total, 319 assays with around 2.5 million mutants are scored. To better understand the novelty of the proposed ProtSSN, we designed additional investigations on <bold>ProteinGym v0</bold> (<xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>), an early-release version of <bold>ProteinGym</bold> with 1.5<italic>M</italic> missense variants across 86 assays (excluding one assay that failed to be folded by both AlphaFold2 and ESMFold, that is, <named-content content-type="sequence"><monospace>A0A140D2T1_ZIKV_Sourisseau_growth_2019</monospace></named-content>).</p></sec><sec id="s4-15"><title>Evaluation metric</title><p>We evaluate the performance of pre-trained models on a diverse set of proteins and protein functions using Spearman’s <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> correlation that measures the strength and direction of the monotonic relationship between two ranked sequences, that is, experimentally evaluated mutants and model-inferred mutants (<xref ref-type="bibr" rid="bib31">Meier et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Notin et al., 2022a</xref>; <xref ref-type="bibr" rid="bib9">Frazer et al., 2021</xref>; <xref ref-type="bibr" rid="bib82">Zhou et al., 2024b</xref>). This non-parametric rank measure is robust to outliers and asymmetry in mutation scores, and it does not assume any specific distribution of mutation fitness scores. The scale of <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> ranges from -1 to 1, indicating the negative or positive correlation of the predicted sequence to the ground truth. The prediction is preferred if its <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi></mml:mstyle></mml:math></inline-formula> to the experimentally examined ground truth is close to 1.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Methodology</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing, Formal analysis</p></fn><fn fn-type="con" id="con3"><p>Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98033-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>The source code .ZIP file contains the complete implementation of model training and evaluation, the associated processed datasets, and the .README document which provides a general instruction.</title></caption><media xlink:href="elife-98033-code1-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The source code and datasets can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/ai4protein/ProtSSN">https://github.com/ai4protein/ProtSSN</ext-link> (copy archived at <xref ref-type="bibr" rid="bib64">Tan, 2025</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Science and Technology Innovation Key R&amp;D Program of Chongqing (CSTB2022TIAD-STX0017), the National Science Foundation of China (Grant Number 62302291, 12104295), the Computational Biology Key Program of Shanghai Science and Technology Commission (23JS1400600), Shanghai Jiao Tong University Scientific and Technological Innovation Funds (21X010200843), the Student Innovation Center at Shanghai Jiao Tong University, and Shanghai Artificial Intelligence Laboratory.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aprile</surname><given-names>FA</given-names></name><name><surname>Sormanni</surname><given-names>P</given-names></name><name><surname>Podpolny</surname><given-names>M</given-names></name><name><surname>Chhangur</surname><given-names>S</given-names></name><name><surname>Needham</surname><given-names>LM</given-names></name><name><surname>Ruggeri</surname><given-names>FS</given-names></name><name><surname>Perni</surname><given-names>M</given-names></name><name><surname>Limbocker</surname><given-names>R</given-names></name><name><surname>Heller</surname><given-names>GT</given-names></name><name><surname>Sneideris</surname><given-names>T</given-names></name><name><surname>Scheidt</surname><given-names>T</given-names></name><name><surname>Mannini</surname><given-names>B</given-names></name><name><surname>Habchi</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>SF</given-names></name><name><surname>Salinas</surname><given-names>PC</given-names></name><name><surname>Knowles</surname><given-names>TPJ</given-names></name><name><surname>Dobson</surname><given-names>CM</given-names></name><name><surname>Vendruscolo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rational design of A conformation-specific antibody for the quantification of Aβ oligomers</article-title><source>PNAS</source><volume>117</volume><fpage>13509</fpage><lpage>13518</lpage><pub-id pub-id-type="doi">10.1073/pnas.1919464117</pub-id><pub-id pub-id-type="pmid">32493749</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Peleg</surname><given-names>Y</given-names></name><name><surname>Rappoport</surname><given-names>N</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>ProteinBERT: a universal deep-learning model of protein sequence and function</article-title><source>Bioinformatics</source><volume>38</volume><fpage>2102</fpage><lpage>2110</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btac020</pub-id><pub-id pub-id-type="pmid">35020807</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Mou</surname><given-names>J</given-names></name><name><surname>Johnston</surname><given-names>KE</given-names></name><name><surname>Wittmann</surname><given-names>BJ</given-names></name><name><surname>Bhattacharya</surname><given-names>N</given-names></name><name><surname>Goldman</surname><given-names>S</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>FLIP: Benchmark Tasks in Fitness Landscape Inference for Proteins</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.09.467890</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>BERT: pre-training of deep bidirectional transformers for language understanding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://aclanthology.org/N19-1423/">https://aclanthology.org/N19-1423/</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elmlund</surname><given-names>D</given-names></name><name><surname>Le</surname><given-names>SN</given-names></name><name><surname>Elmlund</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-resolution cryo-EM: the nuts and bolts</article-title><source>Current Opinion in Structural Biology</source><volume>46</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2017.03.003</pub-id><pub-id pub-id-type="pmid">28342396</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Rehawi</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gibbs</surname><given-names>T</given-names></name><name><surname>Feher</surname><given-names>T</given-names></name><name><surname>Angerer</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Bhowmik</surname><given-names>D</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>ProtTrans: towards cracking the language of life’s code through self-supervised learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.12.199554</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>ProtTrans</data-title><version designator="54f46a1">54f46a1</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/agemagician/ProtTrans">https://github.com/agemagician/ProtTrans</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="software"><person-group person-group-type="author"><collab>facebookresearch</collab></person-group><year iso-8601-date="2023">2023</year><data-title>Esm</data-title><version designator="2b36991">2b36991</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/esm">https://github.com/facebookresearch/esm</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazer</surname><given-names>J</given-names></name><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Dias</surname><given-names>M</given-names></name><name><surname>Gomez</surname><given-names>A</given-names></name><name><surname>Min</surname><given-names>JK</given-names></name><name><surname>Brock</surname><given-names>K</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Disease variant prediction with deep generative models of evolutionary data</article-title><source>Nature</source><volume>599</volume><fpage>91</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04043-8</pub-id><pub-id pub-id-type="pmid">34707284</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>VE</given-names></name><name><surname>Hause</surname><given-names>RJ</given-names></name><name><surname>Luebeck</surname><given-names>J</given-names></name><name><surname>Shendure</surname><given-names>J</given-names></name><name><surname>Fowler</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Quantitative missense variant effect prediction using large-scale mutagenesis data</article-title><source>Cell Systems</source><volume>6</volume><fpage>116</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2017.11.003</pub-id><pub-id pub-id-type="pmid">29226803</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hesslow</surname><given-names>D</given-names></name><name><surname>Zanichelli</surname><given-names>N</given-names></name><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Poli</surname><given-names>I</given-names></name><name><surname>Marks</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>RITA: A Study on Scaling up Generative Protein Sequence Models</source><publisher-name>ICML Workshop on Computational Biology</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hesslow</surname><given-names>D</given-names></name><name><surname>Poli</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>RITA</data-title><version designator="20b2b55">20b2b55</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/lightonai/RITA">https://github.com/lightonai/RITA</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>TA</given-names></name><name><surname>Ingraham</surname><given-names>JB</given-names></name><name><surname>Poelwijk</surname><given-names>FJ</given-names></name><name><surname>Schärfe</surname><given-names>CPI</given-names></name><name><surname>Springer</surname><given-names>M</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mutation effects predicted from sequence co-variation</article-title><source>Nature Biotechnology</source><volume>35</volume><fpage>128</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1038/nbt.3769</pub-id><pub-id pub-id-type="pmid">28092658</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>C</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Hie</surname><given-names>B</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learning inverse folding from millions of predicted structures</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.04.10.487779</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ismail</surname><given-names>AR</given-names></name><name><surname>Kashtoh</surname><given-names>H</given-names></name><name><surname>Baek</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Temperature-resistant and solvent-tolerant lipases as industrial biocatalysts: Biotechnological approaches and applications</article-title><source>International Journal of Biological Macromolecules</source><volume>187</volume><fpage>127</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.ijbiomac.2021.07.101</pub-id><pub-id pub-id-type="pmid">34298046</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>F</given-names></name><name><surname>Bian</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Bai</surname><given-names>X</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Jin</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>GY</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Creatinase: using increased entropy to improve the activity and thermostability</article-title><source>The Journal of Physical Chemistry. B</source><volume>127</volume><fpage>2671</fpage><lpage>2682</lpage><pub-id pub-id-type="doi">10.1021/acs.jpcb.2c08062</pub-id><pub-id pub-id-type="pmid">36926920</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Wohlwend</surname><given-names>J</given-names></name><name><surname>Barzilay</surname><given-names>R</given-names></name><name><surname>Jaakkola</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>Iterative refinement graph neural network for antibody sequence-structure co-design</source><publisher-name>ICLR</publisher-name></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jing</surname><given-names>B</given-names></name><name><surname>Eismann</surname><given-names>S</given-names></name><name><surname>Suriana</surname><given-names>P</given-names></name><name><surname>Townshend</surname><given-names>RJL</given-names></name><name><surname>Dror</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Learning from Protein Structure with Geometric Vector Perceptrons</source><publisher-name>ICLR</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Ballard</surname><given-names>AJ</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Nikolov</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>J</given-names></name><name><surname>Back</surname><given-names>T</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Reiman</surname><given-names>D</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Pacholska</surname><given-names>M</given-names></name><name><surname>Berghammer</surname><given-names>T</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Senior</surname><given-names>AW</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><pub-id pub-id-type="pmid">34265844</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katoh</surname><given-names>K</given-names></name><name><surname>Standley</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Emerging SARS-CoV-2 variants follow a historical pattern recorded in outgroups infecting non-human hosts</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>1134</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02663-4</pub-id><pub-id pub-id-type="pmid">34552191</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khersonsky</surname><given-names>O</given-names></name><name><surname>Lipsh</surname><given-names>R</given-names></name><name><surname>Avizemer</surname><given-names>Z</given-names></name><name><surname>Ashani</surname><given-names>Y</given-names></name><name><surname>Goldsmith</surname><given-names>M</given-names></name><name><surname>Leader</surname><given-names>H</given-names></name><name><surname>Dym</surname><given-names>O</given-names></name><name><surname>Rogotner</surname><given-names>S</given-names></name><name><surname>Trudeau</surname><given-names>DL</given-names></name><name><surname>Prilusky</surname><given-names>J</given-names></name><name><surname>Amengual-Rigo</surname><given-names>P</given-names></name><name><surname>Guallar</surname><given-names>V</given-names></name><name><surname>Tawfik</surname><given-names>DS</given-names></name><name><surname>Fleishman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Automated design of efficient and functionally diverse enzyme repertoires</article-title><source>Molecular Cell</source><volume>72</volume><fpage>178</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.molcel.2018.08.033</pub-id><pub-id pub-id-type="pmid">30270109</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ADAM: A method for stochastic optimization</article-title><conf-name>International Conference on Learning Representation</conf-name></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kipf</surname><given-names>TN</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Semi-Supervised Classification with Graph Convolutional Networks</source><publisher-name>ICLR</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koehler Leman</surname><given-names>J</given-names></name><name><surname>Szczerbiak</surname><given-names>P</given-names></name><name><surname>Renfrew</surname><given-names>PD</given-names></name><name><surname>Gligorijevic</surname><given-names>V</given-names></name><name><surname>Berenberg</surname><given-names>D</given-names></name><name><surname>Vatanen</surname><given-names>T</given-names></name><name><surname>Taylor</surname><given-names>BC</given-names></name><name><surname>Chandler</surname><given-names>C</given-names></name><name><surname>Janssen</surname><given-names>S</given-names></name><name><surname>Pataki</surname><given-names>A</given-names></name><name><surname>Carriero</surname><given-names>N</given-names></name><name><surname>Fisk</surname><given-names>I</given-names></name><name><surname>Xavier</surname><given-names>RJ</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Bonneau</surname><given-names>R</given-names></name><name><surname>Kosciolek</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Sequence-structure-function relationships in the microbial protein universe</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>2351</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-37896-w</pub-id><pub-id pub-id-type="pmid">37100781</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><source>Conditional Antibody Design as 3D Equivariant Graph Translation</source><publisher-name>ICLR</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laine</surname><given-names>E</given-names></name><name><surname>Karami</surname><given-names>Y</given-names></name><name><surname>Carbone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>GEMME: a simple and fast global epistatic model predicting mutational effects</article-title><source>Molecular Biology and Evolution</source><volume>36</volume><fpage>2604</fpage><lpage>2619</lpage><pub-id pub-id-type="doi">10.1093/molbev/msz179</pub-id><pub-id pub-id-type="pmid">31406981</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>FZ</given-names></name><name><surname>Amini</surname><given-names>AP</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Lu</surname><given-names>AX</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Feature reuse and scaling: understanding transfer learning with protein language models</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.02.05.578959</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Akin</surname><given-names>H</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Hie</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Lu</surname><given-names>W</given-names></name><name><surname>Smetanin</surname><given-names>N</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Kabeli</surname><given-names>O</given-names></name><name><surname>Shmueli</surname><given-names>Y</given-names></name><name><surname>Dos Santos Costa</surname><given-names>A</given-names></name><name><surname>Fazel-Zarandi</surname><given-names>M</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Candido</surname><given-names>S</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Evolutionary-scale prediction of atomic-level protein structure with a language model</article-title><source>Science</source><volume>379</volume><fpage>1123</fpage><lpage>1130</lpage><pub-id pub-id-type="doi">10.1126/science.ade2574</pub-id><pub-id pub-id-type="pmid">36927031</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Xun</surname><given-names>G</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The state-of-the-art strategies of protein engineering for enzyme stabilization</article-title><source>Biotechnology Advances</source><volume>37</volume><fpage>530</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1016/j.biotechadv.2018.10.011</pub-id><pub-id pub-id-type="pmid">31138425</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Krause</surname><given-names>B</given-names></name><name><surname>Greene</surname><given-names>ER</given-names></name><name><surname>Subramanian</surname><given-names>S</given-names></name><name><surname>Mohr</surname><given-names>BP</given-names></name><name><surname>Holton</surname><given-names>JM</given-names></name><name><surname>Olmos</surname><given-names>JL</given-names></name><name><surname>Xiong</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>ZZ</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Fraser</surname><given-names>JS</given-names></name><name><surname>Naik</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Large language models generate functional protein sequences across diverse families</article-title><source>Nature Biotechnology</source><volume>41</volume><fpage>1099</fpage><lpage>1106</lpage><pub-id pub-id-type="doi">10.1038/s41587-022-01618-2</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.09.450648</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercurio</surname><given-names>I</given-names></name><name><surname>Tragni</surname><given-names>V</given-names></name><name><surname>Busto</surname><given-names>F</given-names></name><name><surname>De Grassi</surname><given-names>A</given-names></name><name><surname>Pierri</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Protein structure analysis of the interactions between SARS-CoV-2 spike protein and the human ACE2 receptor: from conformational changes to novel neutralizing antibodies</article-title><source>Cellular and Molecular Life Sciences</source><volume>78</volume><fpage>1501</fpage><lpage>1522</lpage><pub-id pub-id-type="doi">10.1007/s00018-020-03580-1</pub-id><pub-id pub-id-type="pmid">32623480</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><collab>microsoft</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Protein-sequence-models</data-title><version designator="af69577">af69577</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/protein-sequence-models">https://github.com/microsoft/protein-sequence-models</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moal</surname><given-names>IH</given-names></name><name><surname>Fernández-Recio</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>SKEMPI: a structural kinetic and energetic database of mutant protein interactions and its use in empirical models</article-title><source>Bioinformatics</source><volume>28</volume><fpage>2600</fpage><lpage>2607</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts489</pub-id><pub-id pub-id-type="pmid">22859501</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myung</surname><given-names>Y</given-names></name><name><surname>Pires</surname><given-names>DEV</given-names></name><name><surname>Ascher</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>CSM-AB: graph-based antibody–antigen binding affinity prediction and docking scoring function</article-title><source>Bioinformatics</source><volume>38</volume><fpage>1141</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab762</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nijkamp</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Progen</data-title><version designator="485b2ea">485b2ea</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/salesforce/progen">https://github.com/salesforce/progen</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nijkamp</surname><given-names>E</given-names></name><name><surname>Ruffolo</surname><given-names>JA</given-names></name><name><surname>Weinstein</surname><given-names>EN</given-names></name><name><surname>Naik</surname><given-names>N</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>ProGen2: Exploring the boundaries of protein language models</article-title><source>Cell Systems</source><volume>14</volume><fpage>968</fpage><lpage>978</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2023.10.002</pub-id><pub-id pub-id-type="pmid">37909046</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikam</surname><given-names>R</given-names></name><name><surname>Kulandaisamy</surname><given-names>A</given-names></name><name><surname>Harini</surname><given-names>K</given-names></name><name><surname>Sharma</surname><given-names>D</given-names></name><name><surname>Gromiha</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>ProThermDB: thermodynamic database for proteins and mutants revisited after 15 years</article-title><source>Nucleic Acids Research</source><volume>49</volume><fpage>D420</fpage><lpage>D424</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa1035</pub-id><pub-id pub-id-type="pmid">33196841</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Dias</surname><given-names>M</given-names></name><name><surname>Frazer</surname><given-names>J</given-names></name><name><surname>Hurtado</surname><given-names>JM</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Marks</surname><given-names>D</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022a</year><source>Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-Time Retrieval</source><publisher-name>ICML</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Van Niekerk</surname><given-names>L</given-names></name><name><surname>Kollasch</surname><given-names>AW</given-names></name><name><surname>Ritter</surname><given-names>D</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>TranceptEVE: Combining Family-Specific and Family-Agnostic Models of Protein Sequences for Improved Fitness Prediction</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.12.07.519495</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Notin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Tranception</data-title><version designator="2ddf40e">2ddf40e</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/OATML-Markslab/Tranception">https://github.com/OATML-Markslab/Tranception</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Kollasch</surname><given-names>AW</given-names></name><name><surname>Ritter</surname><given-names>D</given-names></name><name><surname>van Niekerk</surname><given-names>L</given-names></name><name><surname>Paul</surname><given-names>S</given-names></name><name><surname>Spinner</surname><given-names>H</given-names></name><name><surname>Rollins</surname><given-names>N</given-names></name><name><surname>Shaw</surname><given-names>A</given-names></name><name><surname>Weitzman</surname><given-names>R</given-names></name><name><surname>Frazer</surname><given-names>J</given-names></name><name><surname>Dias</surname><given-names>M</given-names></name><name><surname>Franceschi</surname><given-names>D</given-names></name><name><surname>Orenbuch</surname><given-names>R</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>ProteinGym: large-scale benchmarks for protein design and fitness prediction</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.12.07.570727</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orengo</surname><given-names>CA</given-names></name><name><surname>Michie</surname><given-names>AD</given-names></name><name><surname>Jones</surname><given-names>S</given-names></name><name><surname>Jones</surname><given-names>DT</given-names></name><name><surname>Swindells</surname><given-names>MB</given-names></name><name><surname>Thornton</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>CATH--a hierarchic classification of protein domain structures</article-title><source>Structure</source><volume>5</volume><fpage>1093</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1016/s0969-2126(97)00260-8</pub-id><pub-id pub-id-type="pmid">9309224</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="software"><person-group person-group-type="author"><collab>ProteinGym</collab></person-group><year iso-8601-date="2024">2024</year><data-title>OATML-markslab</data-title><version designator="swh:1:rev:72c8e5456174fdf9073138ccde335e2f88f3ca54">swh:1:rev:72c8e5456174fdf9073138ccde335e2f88f3ca54</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:710ae038ce6e0abf9a369f90a1b06d647a2164c8;origin=https://github.com/OATML-Markslab/ProteinGym;visit=swh:1:snp:8b6da53abeff3959b15df80952dd70acc3ecd591;anchor=swh:1:rev:72c8e5456174fdf9073138ccde335e2f88f3ca54">https://archive.softwareheritage.org/swh:1:dir:710ae038ce6e0abf9a369f90a1b06d647a2164c8;origin=https://github.com/OATML-Markslab/ProteinGym;visit=swh:1:snp:8b6da53abeff3959b15df80952dd70acc3ecd591;anchor=swh:1:rev:72c8e5456174fdf9073138ccde335e2f88f3ca54</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RM</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Canny</surname><given-names>J</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021a</year><source>MSA Transformer</source><publisher-name>ICML</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021b</year><source>Transformer Protein Language Models Are Unsupervised Structure Learners</source><publisher-name>Synthetic Biology</publisher-name><pub-id pub-id-type="doi">10.1101/2020.12.15.422761</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesselman</surname><given-names>AJ</given-names></name><name><surname>Ingraham</surname><given-names>JB</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep generative models of genetic variation capture the effects of mutations</article-title><source>Nature Methods</source><volume>15</volume><fpage>816</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0138-4</pub-id><pub-id pub-id-type="pmid">30250057</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>PNAS</source><volume>118</volume><elocation-id>2016239118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>AD</given-names></name><name><surname>Murphy</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Protein structure and the energetics of protein stability</article-title><source>Chemical Reviews</source><volume>97</volume><fpage>1251</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1021/cr960383c</pub-id><pub-id pub-id-type="pmid">11851450</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson-Rechavi</surname><given-names>M</given-names></name><name><surname>Alibés</surname><given-names>A</given-names></name><name><surname>Godzik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contribution of electrostatic interactions, compactness and quaternary structure to protein thermostability: lessons from structural genomics of Thermotoga maritima</article-title><source>Journal of Molecular Biology</source><volume>356</volume><fpage>547</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.jmb.2005.11.065</pub-id><pub-id pub-id-type="pmid">16375925</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roscoe</surname><given-names>BP</given-names></name><name><surname>Thayer</surname><given-names>KM</given-names></name><name><surname>Zeldovich</surname><given-names>KB</given-names></name><name><surname>Fushman</surname><given-names>D</given-names></name><name><surname>Bolon</surname><given-names>DNA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Analyses of the effects of all ubiquitin point mutants on yeast growth rate</article-title><source>Journal of Molecular Biology</source><volume>425</volume><fpage>1363</fpage><lpage>1377</lpage><pub-id pub-id-type="doi">10.1016/j.jmb.2013.01.032</pub-id><pub-id pub-id-type="pmid">23376099</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarkisyan</surname><given-names>KS</given-names></name><name><surname>Bolotin</surname><given-names>DA</given-names></name><name><surname>Meer</surname><given-names>MV</given-names></name><name><surname>Usmanova</surname><given-names>DR</given-names></name><name><surname>Mishin</surname><given-names>AS</given-names></name><name><surname>Sharonov</surname><given-names>GV</given-names></name><name><surname>Ivankov</surname><given-names>DN</given-names></name><name><surname>Bozhanova</surname><given-names>NG</given-names></name><name><surname>Baranov</surname><given-names>MS</given-names></name><name><surname>Soylemez</surname><given-names>O</given-names></name><name><surname>Bogatyreva</surname><given-names>NS</given-names></name><name><surname>Vlasov</surname><given-names>PK</given-names></name><name><surname>Egorov</surname><given-names>ES</given-names></name><name><surname>Logacheva</surname><given-names>MD</given-names></name><name><surname>Kondrashov</surname><given-names>AS</given-names></name><name><surname>Chudakov</surname><given-names>DM</given-names></name><name><surname>Putintseva</surname><given-names>EV</given-names></name><name><surname>Mamedov</surname><given-names>IZ</given-names></name><name><surname>Tawfik</surname><given-names>DS</given-names></name><name><surname>Lukyanov</surname><given-names>KA</given-names></name><name><surname>Kondrashov</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Local fitness landscape of the green fluorescent protein</article-title><source>Nature</source><volume>533</volume><fpage>397</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1038/nature17995</pub-id><pub-id pub-id-type="pmid">27193686</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Satorras</surname><given-names>VG</given-names></name><name><surname>Hoogeboom</surname><given-names>E</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>E(n) Equivariant Graph Neural Networks</source><publisher-name>ICML</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheng</surname><given-names>G</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>Y</given-names></name><name><surname>Tian</surname><given-names>W</given-names></name><name><surname>Swarts</surname><given-names>DC</given-names></name><name><surname>van der Oost</surname><given-names>J</given-names></name><name><surname>Patel</surname><given-names>DJ</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Structure-based cleavage mechanism of <italic>Thermus thermophilus</italic> Argonaute DNA guide strand-mediated DNA target cleavage</article-title><source>PNAS</source><volume>111</volume><fpage>652</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1073/pnas.1321032111</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>JE</given-names></name><name><surname>Riesselman</surname><given-names>AJ</given-names></name><name><surname>Kollasch</surname><given-names>AW</given-names></name><name><surname>McMahon</surname><given-names>C</given-names></name><name><surname>Simon</surname><given-names>E</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Manglik</surname><given-names>A</given-names></name><name><surname>Kruse</surname><given-names>AC</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Protein design and variant prediction using autoregressive generative models</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2403</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22732-w</pub-id><pub-id pub-id-type="pmid">33893299</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Su</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>C</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Shan</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Yuan</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>SaProt: Protein Language Modeling with Structure-Aware Vocabulary</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.10.01.560349</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Su</surname><given-names>J</given-names></name><collab>fajieyuan</collab></person-group><year iso-8601-date="2025">2025</year><data-title>SaProt</data-title><version designator="50ec846">50ec846</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/westlake-repl/SaProt">https://github.com/westlake-repl/SaProt</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzek</surname><given-names>BE</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>McGarvey</surname><given-names>PB</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name><name><surname>Consortium</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title><source>Bioinformatics</source><volume>31</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Tan</surname><given-names>P</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Fan</surname><given-names>G</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024a</year><article-title>PETA: evaluating the impact of protein transfer learning with sub-word tokenization on downstream applications</article-title><source>Journal of Cheminformatics</source><volume>16</volume><elocation-id>92</elocation-id><pub-id pub-id-type="doi">10.1186/s13321-024-00884-3</pub-id><pub-id pub-id-type="pmid">39095917</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Zhong</surname><given-names>B</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Tan</surname><given-names>P</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Fan</surname><given-names>G</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024b</year><article-title>Simple, efficient, and scalable structure-aware adapter boosts protein language models</article-title><source>Journal of Chemical Information and Modeling</source><volume>64</volume><fpage>6338</fpage><lpage>6349</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.4c00689</pub-id><pub-id pub-id-type="pmid">39110130</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024c</year><article-title>Retrieval-enhanced mutation mastery</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/html/2410.21127v1">https://arxiv.org/html/2410.21127v1</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024d</year><article-title>PROTSOLM: Protein Solubility Prediction with Multi-modal Features</article-title><conf-name>2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</conf-name><pub-id pub-id-type="doi">10.1109/BIBM62325.2024.10822310</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Zhong</surname><given-names>B</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024e</year><article-title>Retrieval-enhanced mutation mastery: augmenting zero-shot prediction of protein language model</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2406.19755">https://arxiv.org/abs/2406.19755</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>ProtSSN</data-title><version designator="swh:1:rev:61e5aa932fed18fb2759ffcf1a6c5ce2d420bce1">swh:1:rev:61e5aa932fed18fb2759ffcf1a6c5ce2d420bce1</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:635b7d4c28b0205cc0dd254c3daf44db3d24fabb;origin=https://github.com/ai4protein/ProtSSN;visit=swh:1:snp:fe086637a7be11904adee9432aa7a6a553abfb1a;anchor=swh:1:rev:61e5aa932fed18fb2759ffcf1a6c5ce2d420bce1">https://archive.softwareheritage.org/swh:1:dir:635b7d4c28b0205cc0dd254c3daf44db3d24fabb;origin=https://github.com/ai4protein/ProtSSN;visit=swh:1:snp:fe086637a7be11904adee9432aa7a6a553abfb1a;anchor=swh:1:rev:61e5aa932fed18fb2759ffcf1a6c5ce2d420bce1</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Attention Is All You Need</source><publisher-name>NeurIPS</publisher-name></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velecký</surname><given-names>J</given-names></name><name><surname>Hamsikova</surname><given-names>M</given-names></name><name><surname>Stourac</surname><given-names>J</given-names></name><name><surname>Musil</surname><given-names>M</given-names></name><name><surname>Damborsky</surname><given-names>J</given-names></name><name><surname>Bednar</surname><given-names>D</given-names></name><name><surname>Mazurenko</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>SoluProtMut<sup>DB</sup>: A manually curated database of protein solubility changes upon mutations</article-title><source>Computational and Structural Biotechnology Journal</source><volume>20</volume><fpage>6339</fpage><lpage>6347</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2022.11.009</pub-id><pub-id pub-id-type="pmid">36420168</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Veličković</surname><given-names>P</given-names></name><name><surname>Cucurull</surname><given-names>G</given-names></name><name><surname>Casanova</surname><given-names>A</given-names></name><name><surname>Romero</surname><given-names>A</given-names></name><name><surname>Lio</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Graph Attention Networks</source><publisher-name>ICLR</publisher-name></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Varshney</surname><given-names>LR</given-names></name><name><surname>Xiong</surname><given-names>C</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Rajani</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>BERTology meets biology: interpreting attention in protein language models</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.26.174417</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xue</surname><given-names>P</given-names></name><name><surname>Cao</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><name><surname>Lane</surname><given-names>ST</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Directed evolution: methodologies and applications</article-title><source>Chemical Reviews</source><volume>121</volume><fpage>12384</fpage><lpage>12444</lpage><pub-id pub-id-type="doi">10.1021/acs.chemrev.1c00260</pub-id><pub-id pub-id-type="pmid">34297541</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodley</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Protein engineering of enzymes for process applications</article-title><source>Current Opinion in Chemical Biology</source><volume>17</volume><fpage>310</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/j.cbpa.2013.03.017</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wüthrich</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Protein structure determination in solution by NMR spectroscopy</article-title><source>The Journal of Biological Chemistry</source><volume>265</volume><fpage>22059</fpage><lpage>22062</lpage><pub-id pub-id-type="pmid">2266107</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Chang</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Tang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Peer: A comprehensive and multi-task benchmark for protein sequence understanding</article-title><source>NeurIPS</source><volume>35</volume><fpage>35156</fpage><lpage>35173</lpage></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Arnold</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine-learning-guided directed evolution for protein engineering</article-title><source>Nature Methods</source><volume>16</volume><fpage>687</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0496-6</pub-id><pub-id pub-id-type="pmid">31308553</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Zanichelli</surname><given-names>N</given-names></name><name><surname>Yeh</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Masked inverse folding with sequence transfer for protein representation learning</article-title><source>Protein Engineering, Design &amp; Selection</source><volume>36</volume><elocation-id>gzad015</elocation-id><pub-id pub-id-type="doi">10.1093/protein/gzad015</pub-id><pub-id pub-id-type="pmid">37883472</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Fusi</surname><given-names>N</given-names></name><name><surname>Lu</surname><given-names>AX</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Convolutions are competitive with transformers for protein sequence pretraining</article-title><source>Cell Systems</source><volume>15</volume><fpage>286</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2024.01.008</pub-id><pub-id pub-id-type="pmid">38428432</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Shen</surname><given-names>Y</given-names></name><name><surname>Liò</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Graph denoising diffusion for inverse protein folding</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>5535</fpage><lpage>5548</lpage></element-citation></ref><ref id="bib77"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>N</given-names></name><name><surname>Bi</surname><given-names>Z</given-names></name><name><surname>Liang</surname><given-names>X</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Deng</surname><given-names>S</given-names></name><name><surname>Lian</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Ontoprotein: Protein Pretraining with Gene Ontology Embedding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2201.11147">https://arxiv.org/abs/2201.11147</ext-link></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>W</given-names></name><name><surname>Zhong</surname><given-names>B</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Tan</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Leng</surname><given-names>H</given-names></name><name><surname>de Souza</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name><name><surname>Xiao</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Proteome-wide 3D structure prediction provides insights into the ancestral metabolism of ancient archaea and bacteria</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>7861</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-35523-8</pub-id><pub-id pub-id-type="pmid">36543797</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Zan</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Jiang</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Loosely-packed dynamical structures with partially-melted surface being the key for thermophilic argonaute proteins achieving high DNA-cleavage activity</article-title><source>Nucleic Acids Research</source><volume>50</volume><fpage>7529</fpage><lpage>7544</lpage><pub-id pub-id-type="doi">10.1093/nar/gkac565</pub-id><pub-id pub-id-type="pmid">35766425</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Pan</surname><given-names>Q</given-names></name><name><surname>Pires</surname><given-names>DEV</given-names></name><name><surname>Rodrigues</surname><given-names>CHM</given-names></name><name><surname>Ascher</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>DDMut: predicting effects of mutations on protein stability using deep learning</article-title><source>Nucleic Acids Research</source><volume>51</volume><fpage>W122</fpage><lpage>W128</lpage><pub-id pub-id-type="doi">10.1093/nar/gkad472</pub-id><pub-id pub-id-type="pmid">37283042</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Zhong</surname><given-names>B</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024a</year><article-title>Protein engineering in the deep learning era</article-title><source>mLife</source><volume>3</volume><fpage>477</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1002/mlf2.12157</pub-id><pub-id pub-id-type="pmid">39744096</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Lv</surname><given-names>O</given-names></name><name><surname>Yi</surname><given-names>K</given-names></name><name><surname>Fan</surname><given-names>G</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024b</year><article-title>Protein engineering with lightweight graph denoising neural networks</article-title><source>Journal of Chemical Information and Modeling</source><volume>64</volume><fpage>3650</fpage><lpage>3661</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.4c00036</pub-id><pub-id pub-id-type="pmid">38630581</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Yi</surname><given-names>K</given-names></name><name><surname>Zhong</surname><given-names>B</given-names></name><name><surname>Tan</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Liò</surname><given-names>P</given-names></name><name><surname>Hong</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024c</year><article-title>A conditional protein diffusion model generates artificial programmable endonuclease sequences with enhanced activity</article-title><source>Cell Discovery</source><volume>10</volume><elocation-id>95</elocation-id><pub-id pub-id-type="doi">10.1038/s41421-024-00728-2</pub-id><pub-id pub-id-type="pmid">39251570</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98033.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Koo</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Cold Spring Harbor Laboratory</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>ProtSSN is a <bold>valuable</bold> approach that generates protein embeddings by integrating sequence and structural information, demonstrating improved prediction of mutation effects on thermostability compared to competing models. The evidence supporting the authors' claims is <bold>compelling</bold>, with well-executed comparisons. This work will be of particular interest to researchers in bioinformatics and structural biology, especially those focused on protein function and stability.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98033.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors introduce a denoising-style model that incorporates both structure and primary-sequence embeddings to generate richer embeddings of peptides. My understanding is that the authors use ESM for the primary sequence embeddings, take resolved structures (or use structural predictions from AlphaFold when they're not available), then develop an architecture to combine these two with a loss that seems reminiscent of diffusion models or masked language model approaches. The embeddings can be viewed as ensemble-style embedding of the two levels of sequence information, or with AlphaFold, an ensemble of two methods (ESM+AlphaFold). The authors also gather external datasets to evaluate their approach and compare it to previous approaches. The approach seems promising and appears to out-compete previous methods at several tasks. Nonetheless, I have strong concerns about a lack of verbosity as well as exclusion of relevant methods and references.</p><p>Advances:</p><p>I appreciate the breadth of the analysis and comparisons to other methods. The authors separate tasks, models, and sizes of models in an intuitive, easy-to-read fashion that I find valuable for selecting a method for embedding peptides. Moreover, the authors gather two datasets for evaluating embeddings' utility for predicting thermostability. Overall, the work should be helpful for the field as more groups choose methods/pretraining strategies amenable to their goals, and can do so in an evidence-guided manner.</p><p>Considerations:</p><p>Primarily, a majority of the results and conclusions (e.g., Table 3) are reached using data and methods from ProteinGym, yet the best-performing methods on ProteinGym are excluded from the paper (e.g., EVE-based models and GEMME). In the ProteinGym database, these methods outperform ProtSSN models. Moreover, these models were published over a year---or even 4 years in the case of GEMME---before ProtSSN, and I do not see justification for their exclusion in the text.</p><p>Secondly, related to comparison of other models, there is no section in the methods about how other models were used, or how their scores were computed. When comparing these models, I think it's crucial that there are explicit derivations or explanations for the exact task used for scoring each method. In other words, if the pre-training is indeed the important advance of the paper, the paper needs to show this more explicitly by explaining exactly which components of the model (and previous models) are used for evaluation. Are the authors extracting the final hidden layer representations of the model, treating these as features, then using these features in a regression task to predict fitness/thermostability/DDG etc.? How are the model embeddings of other methods being used, since, for example, many of these methods output a k-dimensional embedding of a given sequence, rather than one single score that can be correlated with some fitness/functional metric. Summarily, I think the text is lacking an explicit mention of how these embeddings are being summarized or used, as well as how this compares to the model presented.</p><p>I think the above issues can mainly be addressed by considering and incorporating points from Li et al. 2024[1] and potentially Tang &amp; Koo 2024[2]. Li et al.[1] make extremely explicit the use of pretraining for downstream prediction tasks. Moreover, they benchmark pretraining strategies explicitly on thermostability (one of the main considerations in the submitted manuscript), yet there is no mention of this work nor the dataset used (FLIP (Dallago et al., 2021)) in this current work. I think a reference and discussion of [1] is critical, and I would also like to see comparisons in line with [1], as [1] is very clear about what features from pretraining are used, and how. If the comparisons with previous methods were done in this fashion, this level of detail needs to be included in the text.</p><p>To conclude, I think the manuscript would benefit substantially from a more thorough comparison of previous methods. Maybe one way of doing this is following [1] or [2], and using the final embeddings of each method for a variety of regression tasks---to really make clear where these methods are performing relative to one another. I think a more thorough methods section detailing how previous methods did their scoring is also important. Lastly, TranceptEVE (or a model comparable to it) and GEMME should also be mentioned in these results, or at the bare minimum, be given justification for their absence.</p><p>[1] Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models, Francesca-Zhoufan Li, Ava P. Amini, Yisong Yue, Kevin K. Yang, Alex X. Lu bioRxiv 2024.02.05.578959; doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2024.02.05.578959">https://doi.org/10.1101/2024.02.05.578959</ext-link></p><p>[2] Evaluating the representational power of pre-trained DNA language models for regulatory genomics, Ziqi Tang, Peter K Koo bioRxiv 2024.02.29.582810; doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2024.02.29.582810">https://doi.org/10.1101/2024.02.29.582810</ext-link></p><p>Comments on revisions:</p><p>My concerns have been addressed. What seems to remain are some semantical disagreements and I'm not sure that these will be answered here. Do MSAs and other embedding methods lead to some notable type of data leakage? Does this leakage qualify as &quot;x-shot&quot; learning under current definitions?</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98033.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>To design proteins and predict disease, we want to predict the effects of mutations on the function of a protein. To make these predictions, biologists have long turned to statistical models that learn patterns that are conserved across evolution. There is potential to improve our predictions however by incorporating structure. In this paper the authors build a denoising auto-encoder model that incorporates sequence and structure to predict mutation effects. The model is trained to predict the sequence of a protein given its perturbed sequence and structure. The authors demonstrate that this model is able to predict the effects of mutations better than sequence-only models.</p><p>As well, the authors curate a set of assays measuring the effect of mutations on thermostability. They demonstrate their model also predicts the effects of these mutations better than previous models and make this benchmark available for the community.</p><p>Strengths:</p><p>The authors describe a method that makes accurate mutation effect predictions by informing its predictions with structure.</p><p>The authors curate a new dataset of assays measuring thermostability. These can be used to validate and interpret mutation effect prediction methods in the future.</p><p>Weaknesses:</p><p>In the review period, the authors included a previous method, SaProt, that similarly uses protein structure to predict the effects of mutations, in their evaluations. They see that SaProt performs similarly to their method.</p><p>ProteinGym is largely made of deep mutational scans, which measure the effect of every mutation on a protein. These new benchmarks contain on average measurements of less than a percent of all possible point mutations of their respective proteins. It is unclear what sorts of protein regions these mutations are more likely to lie in; therefore it is challenging to make conclusions about what a model has necessarily learned based on its score on this benchmark. For example, several assays in this new benchmark seem to be similar to each other, such as four assays on ubiquitin performed in pH 2.25 to pH 3.0.</p><p>Comments on revisions:</p><p>I think the rounds of review have improved the paper and I've raised my score.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98033.4.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tan</surname><given-names>Yang</given-names></name><role specific-use="author">Author</role><aff><institution>East China University of Science and Technology</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Bingxin</given-names></name><role specific-use="author">Author</role><aff><institution>Shanghai Jiao Tong University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zheng</surname><given-names>Lirong</given-names></name><role specific-use="author">Author</role><aff><institution>Shanghai Jiao Tong University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Fan</surname><given-names>Guisheng</given-names></name><role specific-use="author">Author</role><aff><institution>East China University of Science and Technology</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Hong</surname><given-names>Liang</given-names></name><role specific-use="author">Author</role><aff><institution>Shanghai Jiao Tong University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><p><bold>Response to Reviewer 1</bold></p><p>Thank you for your recognition of our revised work.</p><p><bold>Response to Reviewer 2</bold></p><disp-quote content-type="editor-comment"><p>It would be useful to have a demonstration of where this model outperforms SaProt systematically, and a discussion about what the success of this model teaches us given there is a similar, previously successful model, SaProt.</p></disp-quote><p>As two concurrent works, ProtSSN and SaProt employ different methods to incorporate the structure information of proteins. Generally speaking, for two deep learning models that are developed during a close period, it is challenging to conclude that one model is systematically superior to another. Nonetheless, on DTm and DDG (the two low-throughput datasets that we constructed), ProtSSN demonstrates better empirical performance than SaProt.</p><p>Moreover, ProtSSN is more efficient in both training and inference compared to SaProt. In terms of training cost, SaProt uses 40 million protein structures for pretraining (requiring 64 A100 GPUs for three months), whereas ProtSSN requires only about 30,000 crystal structures from the CATH database (trained on a single 3090 GPU for two days). Despite SaProt’s significantly higher training cost, its pretrained version does not exhibit superior performance on low-throughput datasets such as DTm, DDG, and Clinvar. Furthermore, the high training cost limits many users from retraining or fine-tuning the model for specific needs or datasets.</p><p>Regarding the inference cost, ProtSSN requires only one embedding computation for a wild-type protein, regardless of the number of mutants (n). In contrast, SaProt computes a separate embedding and score for each mutant. For instance, when evaluating the scoring performance on ProteinGym, ProtSSN only needs 217 inferences, while SaProt needs more than 2M inferences. This inference speed is important in practice, such as high-throughput design and screening.</p><disp-quote content-type="editor-comment"><p>Please remove the reference to previous methods as &quot;few shot&quot;. This typically refers to their being trained on experimental data, not their using MSAs. A &quot;few shot&quot; model would be ProteinNPT.</p></disp-quote><p>The definition of &quot;few-shot&quot; we used here is following ESM1v [1]. This concept originates from providing a certain number of examples as input to GPT-3 [2]. In the context of protein deep learning models, MSA serves as the wild-type protein examples.</p><p>Also, Reviewer 1 uses the concept in the same way.</p><p>“Readers should note that methods labelled as &quot;few-shot&quot; in comparisons do not make use of experimental labels, but rather use sequences inferred as homologous; these sequences are also often available even if the protein has never been experimentally tested.”</p><p>In the main text, we also included this definition as well as the reference of ESM-1v in lines 457-458.</p><p>“We extend the evaluation on ProteinGym v0 to include a comparison of our zero-shot ProtSSN with few-shot learning methods that leverage MSA information of proteins (Meier et al., 2021).”</p><p>(1) Meier J, Rao R, Verkuil R, et al. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems, 2021.</p><p>(2) Brown T, Mann B, Ryder N, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020.</p><disp-quote content-type="editor-comment"><p>Furthermore, I don't think it is fair to state that your method is not comparable to these models -- one can run an MSA just as one can predict a structure. A fairer comparison would be to highlight particular assays for which getting an MSA could be challenging -- Transcription did this by showing that they outperform EVE when MSAs are shallow.</p></disp-quote><p>We recognize that there are often differences in the definitions and classifications of various methodologies. Here, we follow the definitions provided by ProteinGym. As the most comprehensive and large scale open benchmark in the community, we believe this classification scheme should be widely accepted. All classifications are available on the official website of ProteinGym (<ext-link ext-link-type="uri" xlink:href="https://proteingym.org/benchmarks">https://proteingym.org/benchmarks</ext-link>), which categorizes methods into PLMs, Structure-based models, and Alignment-based models. For example, GEMME is classified as an alignment-based model, and MSA Transformer is considered a hybrid model combining alignment and PLM features.</p><p>We believe that methodologies with different inputs and architectures can lead to inherent unfairness. Also, it is generally believed that models including evolutionary relationships tend to outperform end-to-end models due to the extra information and efforts involved during the training phase. Some empirical evidence and discussions are in the ablation studies of retrieval factors in Tranception [3]. Moreover, the choice of MSA search parameters can introduce uncertainty, which could have positive or negative impacts.</p><p>We showcase the impact of MSA depth on model performance with an additional analysis below. Author response image 1 visualizes the Spearman’s correlation between the scores of each model and the number of MSAs on 217 ProteinGym assays, where each point represents one of 217 assays. The summary correlation of each model with respect to all assays are reported in Author response table 1. These results demonstrate no clear correlation between MSA depth and model performance even for MSA-based models.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Scatter plots of the number of MSA sequences and spearman’s correlation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98033-sa3-fig1-v1.tif"/></fig><table-wrap id="sa3table1" position="float"><label>Author response table 1.</label><caption><title>Spearmar’s score of the number of MSA sequences and the model’s performance.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Model Name</th><th valign="bottom">Model Type</th><th valign="bottom">Spearmanr's score</th></tr></thead><tbody><tr><td align="left" valign="bottom">EVE (ensemble)</td><td align="left" valign="bottom">Alignment-based model</td><td align="char" char="." valign="bottom">0.239</td></tr><tr><td align="left" valign="bottom">GEMME</td><td align="left" valign="bottom">Alignment-based model</td><td align="char" char="." valign="bottom">0.207</td></tr><tr><td align="left" valign="bottom">TranceptEVE L</td><td align="left" valign="bottom">Hybrid - Alignment &amp; PLM</td><td align="char" char="." valign="bottom">0.237</td></tr><tr><td align="left" valign="bottom">MSA Transformer (ensemble)</td><td align="left" valign="bottom">Hybrid - Alignment &amp; PLM</td><td align="char" char="." valign="bottom">0.262</td></tr><tr><td align="left" valign="bottom">ESM-IF1</td><td align="left" valign="bottom">Inverse folding model</td><td align="char" char="." valign="bottom">0.346</td></tr><tr><td align="left" valign="bottom">ESM2 (650M)</td><td align="left" valign="bottom">Protein language model</td><td align="char" char="." valign="bottom">0.297</td></tr><tr><td align="left" valign="bottom">ESM-1v (ensemble)</td><td align="left" valign="bottom">Protein language model</td><td align="char" char="." valign="bottom">0.372</td></tr><tr><td align="left" valign="bottom">SaProt (650M)</td><td align="left" valign="bottom">Hybrid - Structure &amp; PLM</td><td align="char" char="." valign="bottom">0.260</td></tr><tr><td align="left" valign="bottom">ProtSSN (ensemble)</td><td align="left" valign="bottom">Hybrid - Structure &amp; PLM</td><td align="char" char="." valign="bottom">0.217</td></tr></tbody></table></table-wrap><p>(3) Notin P, Dias M, Frazer J, et al. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. International Conference on Machine Learning, 2022.</p><disp-quote content-type="editor-comment"><p>The authors state that DTm and DDG are conceptually appealing because they come from low-throughput assays with lower experimental noise and are also mutations that are particularly chosen to represent the most interesting regions of the protein. I agree with the conceptual appeal but I don't think these claims have been demonstrated in practice. The cited comparison with Frazer as a particularly noisy source of data I think is particularly unconvincing: ClinVar labels are not only rigorously determined from multiple sources of evidence, Frazer et al demonstrates that these labels are actually more reliable than experiment in some cases. They also state that ProteinGym data doesn't come with environmental conditions, but these can be retrieved from the papers the assays came from. The paper would be strengthened by a demonstration of the conceptual benefit of these new datasets, say a comparison of mutations and signal for a protein that may be in one of these datasets vs ProteinGym.</p></disp-quote><p>In the work by Frazer et al. [4], they mentioned that</p><p>&quot;However, these technologies do not easily scale to thousands of proteins, especially not to combinations of variants, and depend critically on the availability of assays that are relevant to or at least associated with human disease phenotypes.&quot;</p><p>It points out that the results of high-throughput experiments are usually based on the design of specific genes (such as BRCA1 and TP53.) and cannot be easily extended to thousands of other genes. At the same time, due to the complexity of the experiment, there may be problems with reproducibility or deviations from clinical relevance.</p><p>This statement aligns with our perspective that high-throughput experiments inherently involve a significant amount of noise and error. It is important to clarify that the noise we discuss here arises from the limitations of high-throughput experiments themselves, instead of from the reliability of the data sources, such as systematic errors in experimental measurements. This latter issue is a complex problem common to all wetlab experiments and falls outside the scope of our study.</p><p>Under this premise, low-throughput datasets like DTm and DDG can be considered to have less noise than high-throughput datasets, as they have undergone manual curation. As for your suggestion, while valuable, unfortunately, we were unable to identify datasets in DTM and DDG that align with those in ProteinGym after a careful search. Thus, we are unable to conduct this comparative experiment at this stage.</p><p>(4) Frazer J, Notin P, Dias M, et al. Disease variant prediction with deep generative models of evolutionary data. Nature, 2021.</p></body></sub-article></article>