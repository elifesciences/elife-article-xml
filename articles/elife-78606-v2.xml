<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78606</article-id><article-id pub-id-type="doi">10.7554/eLife.78606</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Population codes enable learning from few examples by shaping inductive bias</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-275182"><name><surname>Bordelon</surname><given-names>Blake</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0455-9445</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-149984"><name><surname>Pehlevan</surname><given-names>Cengiz</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9767-6063</contrib-id><email>cpehlevan@seas.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>John A Paulson School of Engineering and Applied Sciences, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Center for Brain Science, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e78606</elocation-id><history><date date-type="received" iso-8601-date="2022-03-14"><day>14</day><month>03</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-12-15"><day>15</day><month>12</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-03-31"><day>31</day><month>03</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.03.30.437743"/></event></pub-history><permissions><copyright-statement>© 2022, Bordelon and Pehlevan</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Bordelon and Pehlevan</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78606-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-78606-figures-v2.pdf"/><abstract><p>Learning from a limited number of experiences requires suitable inductive biases. To identify how inductive biases are implemented in and shaped by neural codes, we analyze sample-efficient learning of arbitrary stimulus-response maps from arbitrary neural codes with biologically-plausible readouts. We develop an analytical theory that predicts the generalization error of the readout as a function of the number of observed examples. Our theory illustrates in a mathematically precise way how the structure of population codes shapes inductive bias, and how a match between the code and the task is crucial for sample-efficient learning. It elucidates a bias to explain observed data with simple stimulus-response maps. Using recordings from the mouse primary visual cortex, we demonstrate the existence of an efficiency bias towards low-frequency orientation discrimination tasks for grating stimuli and low spatial frequency reconstruction tasks for natural images. We reproduce the discrimination bias in a simple model of primary visual cortex, and further show how invariances in the code to certain stimulus variations alter learning performance. We extend our methods to time-dependent neural codes and predict the sample efficiency of readouts from recurrent networks. We observe that many different codes can support the same inductive bias. By analyzing recordings from the mouse primary visual cortex, we demonstrate that biological codes have lower total activity than other codes with identical bias. Finally, we discuss implications of our theory in the context of recent developments in neuroscience and artificial intelligence. Overall, our study provides a concrete method for elucidating inductive biases of the brain and promotes sample-efficient learning as a general normative coding principle.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>population codes</kwd><kwd>inductive bias</kwd><kwd>learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DMS-2134157</award-id><principal-award-recipient><name><surname>Bordelon</surname><given-names>Blake</given-names></name><name><surname>Pehlevan</surname><given-names>Cengiz</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural sensory representations impose an inductive bias over the space of learning tasks, allowing some tasks to be learned by a downstream neuron more sample-efficiently than others.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ability to learn quickly is crucial for survival in a complex and an everchanging environment, and the brain effectively supports this capability. Often, only a few experiences are sufficient to learn a task, whether acquiring a new word (<xref ref-type="bibr" rid="bib21">Carey and Bartlett, 1978</xref>) or recognizing a new face (<xref ref-type="bibr" rid="bib87">Peterson et al., 2009</xref>). Despite the importance and ubiquity of sample efficient learning, our understanding of the brain’s information encoding strategies that support this faculty remains poor (<xref ref-type="bibr" rid="bib118">Tenenbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Lake et al., 2017</xref>; <xref ref-type="bibr" rid="bib108">Sinz et al., 2019</xref>).</p><p>In particular, when learning and generalizing from past experiences, and especially from few experiences, the brain relies on implicit assumptions it carries about the world, or its inductive biases (<xref ref-type="bibr" rid="bib127">Wolpert, 1996</xref>; <xref ref-type="bibr" rid="bib108">Sinz et al., 2019</xref>). Reliance on inductive bias is not a choice: inferring a general rule from finite observations is an ill-posed problem which requires prior assumptions since many hypotheses can explain the same observed experiences (<xref ref-type="bibr" rid="bib51">Hume, 1998</xref>). Consider learning a rule that maps photoreceptor responses to a prediction of whether an observed object is a threat or is neutral. Given a limited number of visual experiences of objects and their threat status, many threat-detection rules are consistent with these experiences. By choosing one of these threat-detection rules, the nervous system reveals an inductive bias. Without the right biases that suit the task at hand, successful generalization is impossible (<xref ref-type="bibr" rid="bib127">Wolpert, 1996</xref>; <xref ref-type="bibr" rid="bib108">Sinz et al., 2019</xref>). In order to understand why we can quickly learn to perform certain tasks accurately but not others, we must understand the brain’s inductive biases (<xref ref-type="bibr" rid="bib118">Tenenbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Lake et al., 2017</xref>; <xref ref-type="bibr" rid="bib108">Sinz et al., 2019</xref>).</p><p>In this paper, we study sample efficient learning and inductive biases in a general neural circuit model which comprises of a population of sensory neurons and a readout neuron learning a stimulus-response map with a biologically-plausible learning rule (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). For this circuit and learning rule, inductive bias arises from the nature of the neural code for sensory stimuli, specifically its similarity structure. While different population codes can encode the same stimulus variables and allow learning of the same output with perfect performance given infinitely many samples, learning performance can depend dramatically on the code when restricted to a small number of samples, where the reliance on and the effect of inductive bias are strong (<xref ref-type="fig" rid="fig1">Figure 1B, C and D</xref>). Given the same sensory examples and their associated response values, the readout neuron may make drastically different predictions depending on the inductive bias set by the nature of the code, leading to successful or failing generalizations (<xref ref-type="fig" rid="fig1">Figure 1C and D</xref>). We say that a code and a learning rule, together, have a good inductive bias for a task if the task can be learned from a small number of examples.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Learning tasks through linear readouts exploit representations of the population code to approximate a target response.</title><p>(<bold>A</bold>) The readout weights from the population to a downstream neuron, shown in blue, are updated to fit target values <inline-formula><mml:math id="inf1"><mml:mi>y</mml:mi></mml:math></inline-formula>, using the local, biologically plausible delta rule. (<bold>B</bold>) Examples of tuning curves for two different population codes: Smooth tuning curves (Code 1) and rapidly varying tuning curves (Code 2). (<bold>C</bold>) (Left) A target function with low frequency content is approximated through the learning rule shown in A using these two codes. The readout from Code 1 (turquoise) fits the target function (black) almost perfectly with only <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula> training examples, while readout from Code 2 (purple) does not accurately approximate the target function. (Right) However, when the number of training examples is sufficiently large (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula>), the target function is estimated perfectly by both codes, indicating that both codes are equally expressive. (<bold>D</bold>) The same experiment is performed on a task with higher frequency content. (Left) Code 1 fails to perform well with <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula> samples indicating mismatch between inductive bias and the task can prevent sample efficient learning while Code 2 accurately fits the target. (Right) Again, provided enough data <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula>, both models can accurately estimate the target function. Details of these simulations are given in Methods Generating example codes (Figure 1).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig1-v2.tif"/></fig><p>In order to understand how population codes shape inductive bias and allow fast learning of certain tasks over others with a biologically plausible learning rule, we develop an analytical theory of the readout neuron’s learning performance as a function of the number of sampled examples, or sample size. We find that the readout’s performance is completely determined by the code’s kernel, a function which takes in pairs of population response vectors and outputs a representational similarity defined by the inner product of these vectors. We demonstrate that the spectral properties of the kernel introduce an inductive bias toward explaining sampled data with simple stimulus-response maps and determine compatibility of the population code with the learning task, and hence the sample-efficiency of learning. We apply this theory to data from the mouse primary visual cortex (V1) (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>; <xref ref-type="bibr" rid="bib113">Stringer et al., 2018a</xref>; <xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>), and show that mouse V1 responses support sample-efficient learning of low frequency orientation discrimination and low spatial frequency reconstruction tasks over high frequency ones. We demonstrate the discrimination bias in a simple model of V1 and show how response nonlinearity, sparsity, and relative proportion of simple and complex cells influence the code’s bias and performance on learning tasks, including ones that involve invariances. We extend our theory to temporal population codes, including codes generated by recurrent neural networks learning a delayed response task. We observe that many codes could support the same kernel function, however, by analyzing data from mouse primary visual cortex (V1) (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>; <xref ref-type="bibr" rid="bib113">Stringer et al., 2018a</xref>; <xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>), we find that the biological code is metabolically more efficient than others.</p><p>Overall, our results demonstrate that for a fixed learning rule, the neural sensory representation imposes an inductive bias over the space of learning tasks, allowing some tasks to be learned by a downstream neuron more sample-efficiently than others. Our work provides a concrete method for elucidating inductive biases of populations of neurons and suggest sample-efficient learning as a novel functional role for population codes.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Problem setup</title><p>We denote vectors with bold lower-case symbols <inline-formula><mml:math id="inf6"><mml:mi mathvariant="bold">r</mml:mi></mml:math></inline-formula> and matrices <inline-formula><mml:math id="inf7"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> with bold upper-case symbols. We denote an average of a function <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over random variable <inline-formula><mml:math id="inf9"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Euclidean inner products between vectors are denoted either as <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf12"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula> and real Euclidean <inline-formula><mml:math id="inf13"><mml:mi>n</mml:mi></mml:math></inline-formula>-space is denoted <inline-formula><mml:math id="inf14"><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math></inline-formula>. Sets of variables are represented with <inline-formula><mml:math id="inf15"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>We consider a population of <inline-formula><mml:math id="inf16"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons whose responses, <inline-formula><mml:math id="inf17"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, vary with the input stimuli, which is parameterized by a vector variable <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, such as the orientation and the phase of a grating (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). These responses define the population code. Throughout this work, we will mostly assume that this population code is deterministic: that identical stimuli generate identical neural responses.</p><p>From the population responses, a readout neuron learns its weights <inline-formula><mml:math id="inf19"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula> to approximate a stimulus-response map, or a target function <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, such as one that classifies stimuli as apetitive (<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) or aversive (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), or a more smooth one that attaches intermediate values of valence. We emphasize that in our model only the readout neuron performs learning, and the population code is assumed to be static through learning. Our theory is general in its assumptions about the structure of the population code and the stimulus-response map considered (Methods Theory of generalization), and can apply to many scenarios.</p><p>The readout neuron learns from <inline-formula><mml:math id="inf23"><mml:mi>P</mml:mi></mml:math></inline-formula> stimulus-response examples with the goal of generalizing to previously unseen ones. Example stimuli <inline-formula><mml:math id="inf24"><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) are sampled from a probability distribution describing stimulus statistics <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This distribution can be natural or artificially created, for example, for a laboratory experiment (Appendix Discrete stimulus spaces: finding eigenfunctions with matrix eigendecomposition). From the set of learning examples, <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the readout weights are learned with the local, biologically-plausible delta-rule, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>,where <inline-formula><mml:math id="inf29"><mml:mi>η</mml:mi></mml:math></inline-formula> is a learning rate (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Learning with weight decay, which privileges readouts with smaller norm, can also be accommodated in our theory as we discuss in (Appendix Weight decay and ridge regression). With or without weight decay, the learning rule converges to a unique set of weights <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (Appendix Convergence of the delta-rule without weight decay). Generalization error with these weights is given by<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which quantifies the expected error of the trained readout over the entire stimulus distribution <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This quantity will depend on the population code <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the target function <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the set of training examples <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Our theoretical analysis of this model provides insights into how populations of neurons encode information and allow sample-efficient learning.</p></sec><sec id="s2-2"><title>Kernel structure of population codes controls learning performance</title><p>First, we note that the generalization performance of the learned readout on a given task depends entirely on the inner product kernel, defined by <inline-formula><mml:math id="inf35"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which quantifies the similarity of population responses to two different stimuli <inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The kernel, or similarity matrix, encodes the geometry of the neural responses. Concretely, distances (in neural space) between population vectors for stimuli <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> can be computed from the kernel <inline-formula><mml:math id="inf39"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib31">Edelman, 1998</xref>; <xref ref-type="bibr" rid="bib57">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Laakso and Cottrell, 2000</xref>; <xref ref-type="bibr" rid="bib56">Kornblith et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib85">Pehlevan et al., 2018</xref>). The fact that the solution to the learning problem only depends on the kernel is due to the convergence of the learning rule to a unique solution <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for the training set <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib77">Neal, 1994</xref>; <xref ref-type="bibr" rid="bib41">Girosi et al., 1995</xref>). The dataset-dependent fixed point <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of the learning rule is a linear combination of the population vectors on the dataset <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the learned function computed by the readout neuron is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the coefficient vector satisfies <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (Appendix Convergence of the delta-rule without weight decay), and the matrix <inline-formula><mml:math id="inf45"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> has entries <inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The matrix K<sup>+</sup> is the pseudo-inverse of <inline-formula><mml:math id="inf48"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>. In these expressions the population code only appears through the kernel <inline-formula><mml:math id="inf49"><mml:mi>K</mml:mi></mml:math></inline-formula>, showing that the kernel alone controls the learned response pattern. This result applies also to nonlinear readouts (Appendix Convergence of Delta-rule for nonlinear readouts), showing that the kernel can control the learned solution in a variety of cases.</p><p>Since predictions only depend on the kernel, a large set of codes achieve identical desired performance on learning tasks. This is because the kernel is invariant with respect to rotation of the population code. An orthogonal transformation <inline-formula><mml:math id="inf50"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> applied to a population code <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> generates a new code <inline-formula><mml:math id="inf52"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Qr</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with an identical kernel (Appendix Alternative neural codes with same kernel) since <inline-formula><mml:math id="inf53"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">Qr</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Codes <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will have identical readout performance on all possible learning tasks. We illustrate this degeneracy in <xref ref-type="fig" rid="fig2">Figure 2</xref> using a publicly available dataset which consists of activity recorded from ∼20,000 neurons from the primary visual cortex of a mouse while shown static gratings (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>). An original code <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is rotated to generate <inline-formula><mml:math id="inf57"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) which have the same kernels (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and the same performance on a learning task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The inner product kernel controls the generalization performance of readouts.</title><p>(<bold>A</bold>) Tuning curves <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for three example recorded Mouse V1 neurons to varying static grating stimuli oriented at angle <inline-formula><mml:math id="inf59"><mml:mi>θ</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>) (Left) are compared with a randomly rotated version (Middle) <inline-formula><mml:math id="inf60"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the same population code. (Right) These two codes, original (Ori.) and rotated (Rot.) can be visualized as parametric trajectories in neural space. (<bold>B</bold>) The inner product kernel matrix has elements <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The original V1 code and its rotated counterpart have identical kernels. (<bold>C</bold>) In a learning task involving uniformly sampled angles, readouts from the two codes perform identically, resulting in identical approximations of the target function (shown on the left as blue and red curves) and consequently identical generalization performance as a function of training set size <inline-formula><mml:math id="inf62"><mml:mi>P</mml:mi></mml:math></inline-formula> (shown on right with blue and red points). The theory curve will be described in the main text.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig2-v2.tif"/></fig></sec><sec id="s2-3"><title>Code-task alignment governs generalization</title><p>We next examine how the population code affects generalization performance of the readout. We calculated analytical expressions of the average generalization error in a task defined by the target response <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> after observing <inline-formula><mml:math id="inf64"><mml:mi>P</mml:mi></mml:math></inline-formula> stimuli using methods from statistical physics (Methods Theory of generalization). Because the relevant quantity in learning performance is the kernel, we leveraged results from our previous work studying generalization in kernel regression (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>), and approximated the generalization error averaged over all possible realizations of the training dataset composed of <inline-formula><mml:math id="inf65"><mml:mi>P</mml:mi></mml:math></inline-formula> stimuli, <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. As <inline-formula><mml:math id="inf67"><mml:mi>P</mml:mi></mml:math></inline-formula> increases, the variance in <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> due to the composition of the dataset decreases, and our expressions become descriptive of the typical case. Our final analytical result is given in <xref ref-type="disp-formula" rid="equ11">Equation (11)</xref> in Methods Theory of generalization. We provide details of our calculations in Methods Theory of generalization and Appendix Theory of generalization, and focus on their implications here.</p><p>One of our main observations is that given a population code <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the singular value decomposition of the code gives the appropriate basis to analyze the inductive biases of the readouts (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The tuning curves for individual neurons <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> form an <inline-formula><mml:math id="inf71"><mml:mi>N</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf72"><mml:mi>M</mml:mi></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf73"><mml:mi mathvariant="bold">R</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf74"><mml:mi>M</mml:mi></mml:math></inline-formula>, possibly infinite, is the number of all possible stimuli. We discuss the SVD for continuous stimulus spaces in Appendix Singular value decomposition of continuous population responses. The left-singular vectors (or principal axes) and singular values of this matrix have been used in neuroscience for describing lower dimensional structure in the neural activity and estimating its dimensionality, see e.g. (<xref ref-type="bibr" rid="bib112">Stopfer et al., 2003</xref>; <xref ref-type="bibr" rid="bib55">Kato et al., 2015</xref>; <xref ref-type="bibr" rid="bib12">Bathellier et al., 2008</xref>; <xref ref-type="bibr" rid="bib35">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib98">Sadtler et al., 2014</xref>; <xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>, <xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib66">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Gao et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Gao and Ganguli, 2015</xref>). We found that the function approximation properties of the code are controlled by the singular values, or rather their squares <inline-formula><mml:math id="inf75"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> which give variances along principal axes, indexed in decreasing order, and the corresponding right singular vectors <inline-formula><mml:math id="inf76"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, which are also the kernel eigenfunctions (Methods Theory of generalization and Appendix Singular value decomposition of continuous population responses). This follows from the fact that learned response (<xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>) is only a function of the kernel <inline-formula><mml:math id="inf77"><mml:mi>K</mml:mi></mml:math></inline-formula>, and the eigenvalues <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and orthonormal (uncorrelated) eigenfunctions <inline-formula><mml:math id="inf79"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> collectively define the code’s inner-product kernel <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through an eigendecomposition <inline-formula><mml:math id="inf81"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib73">Mercer, 1909</xref>) (Methods Theory of generalization and Appendix Theory of generalization).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The singular value decomposition (SVD) of the population code reveals the structure and inductive bias of the code.</title><p>(<bold>A</bold>) SVD of the response matrix <inline-formula><mml:math id="inf82"><mml:mi mathvariant="bold">R</mml:mi></mml:math></inline-formula> gives left singular vectors <inline-formula><mml:math id="inf83"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> (principal axes), kernel eigenvalues <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, and kernel eigenfunctions <inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The ordering of eigenvalues provides an ordering of which modes <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> can be learned by the code from few training examples. The eigenfunctions were offset by 0.5 for visibility. (<bold>B</bold>) (Left) Two different learning tasks <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, a low frequency (blue) and high frequency (red) function, are shown. (Middle) The cumulative power distribution rises more rapidly for the low frequency task than the high frequency, indicating better alignment with top kernel eigenfunctions and consequently more sample-efficient learning as shown in the learning curves (right). Dashed lines show theoretical generalization error while dots and solid vertical lines are experimental average and standard deviation over 30 repeats. (<bold>C</bold>) The feature space representations of the low (left) and high (middle and right) frequency tasks. Each point represents the embedding of a stimulus response vector along the <inline-formula><mml:math id="inf88"><mml:mi>k</mml:mi></mml:math></inline-formula>-th principal axis <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The binary target value <inline-formula><mml:math id="inf90"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> is indicated with the color of the point. The easy (left), low frequency task is well separated along the top two dimensions, while the hard, high frequency task is not linearly separable in two (middle) or even with four feature dimensions (right). (<bold>D</bold>) On an image discrimination task (recognizing birds vs mice), V1 has an entangled representation which does not allow good performance of linear readouts. This is evidenced by the top principal components (middle) and the slowly rising <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> curve (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Kernels, spectra and eigenfunctions are well preserved across a variety of bin counts.</title><p>(<bold>A</bold>) The kernels for the V1 code where trial averaging is performed with 50, 100, 200 bins. (<bold>B</bold>) Despite the different bin values, the top eigenvalues are all very close, however, effects at the tail are visible. This is likely due to the reduction of trials per bin as the number of bins is increased. (<bold>C</bold>) Further, the top eigenfunctions are very similar for all bin counts. Since a higher number of bins means fewer trials to average over, the eigenfunctions for large bin count experiments are noisier.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig3-figsupp1-v2.tif"/></fig></fig-group><p>Our analysis shows the existence of a bias in the readout towards learning certain target responses faster than others. The target response <inline-formula><mml:math id="inf92"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the learned readout response <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> can be expressed in terms of these eigenfunctions <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. Our theory shows that the readout’s generalization is better if the target function <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is aligned with the top eigenfunctions <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, equivalent to <inline-formula><mml:math id="inf97"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> decaying rapidly with <inline-formula><mml:math id="inf98"><mml:mi>k</mml:mi></mml:math></inline-formula> (Appendix Spectral bias and code-task alignment). We formalize this notion by the following metric. Mathematically, generalization error <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> can be decomposed into normalized estimation errors <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> for the coefficients of these eigenfunctions <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We found that the ordering of the eigenvalues <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> controls the rates at which these mode errors <inline-formula><mml:math id="inf105"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> decrease as <inline-formula><mml:math id="inf106"><mml:mi>P</mml:mi></mml:math></inline-formula> increases (Methods Theory of generalization, Appendix Spectral bias and code-task alignment), (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>): <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo stretchy="false">⟹</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Hence, larger eigenvalues mean lower generalization error for those normalized mode errors <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. We term this phenomenon the <italic>spectral bias</italic> of the readout. Based on this observation, we propose <italic>code-task alignment</italic> as a principle for good generalization. To quantify code-task alignment, we use a metric which was introduced in <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref> to measure the compatibility of a kernel with a learning task. This is the cumulative power distribution <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which measures the total power of the target function in the top <inline-formula><mml:math id="inf110"><mml:mi>k</mml:mi></mml:math></inline-formula> eigenmodes, normalized by the total power (<xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msubsup><mml:mi>v</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Stimulus-response maps that have high alignment with the population code’s kernel will have quickly rising cumulative power distributions <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, since a large proportion of power is placed in the top modes. Target responses with high <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be learned with fewer training samples than target responses with low <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> since the mode errors <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> are ordered for all <inline-formula><mml:math id="inf115"><mml:mi>P</mml:mi></mml:math></inline-formula> (Appendix Spectral bias and code-task alignment).</p></sec><sec id="s2-4"><title>Probing learning biases in neural data</title><p>Our theory can be used to probe the learning biases of neural populations. Here, we provide various examples of this using publicly available calcium imaging recordings from mouse primary visual cortex (V1). Our examples illustrate how our theory can be used to analyze neural data.</p><p>We first analyzed population responses to static grating stimuli oriented at an angle <inline-formula><mml:math id="inf116"><mml:mi>θ</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>). We found that the kernel eigenfunctions have sinusoidal shape with differing frequency. The ordering of the eigenvalues and eigenfunctions in <xref ref-type="fig" rid="fig3">Figure 3A</xref> (and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) indicates a frequency bias: lower frequency functions of <inline-formula><mml:math id="inf117"><mml:mi>θ</mml:mi></mml:math></inline-formula> are easier to estimate at small sample sizes.</p><p>We tested this idea by constructing two different orientation discrimination tasks shown in <xref ref-type="fig" rid="fig3">Figure 3B and C</xref>, where we assign static grating orientations to positive or negative valence with different frequency square-wave functions of <inline-formula><mml:math id="inf118"><mml:mi>θ</mml:mi></mml:math></inline-formula>. We trained the readout using a subset of the experimentally measured neural responses, and measured the readout’s generalization performance. We found that the cumulative power distribution for the low frequency task has a more rapidly rising <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Using our theory of generalization, we predicted learning curves for these two tasks, which express the generalization error as a function of the number of sampled stimuli <inline-formula><mml:math id="inf120"><mml:mi>P</mml:mi></mml:math></inline-formula>. The error for the low frequency task is lower at all sample sizes than the hard, high-frequency task. The theoretical predictions and numerical experiments show perfect agreement (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). More intuition can be gained by visualizing the projection of the neural response along the top principal axes (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). For the low-frequency task, the two target values are well separated along the top two axes. However, the high-frequency task is not well separated along even the top four axes (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>Using the same ideas, we can use our theory to get insight into tasks which the V1 population code is ill-suited to learn. For the task of identifying mice and birds (<xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>, <xref ref-type="bibr" rid="bib113">Stringer et al., 2018a</xref>) the linear rise in cumulative power indicates that there is roughly equal power along all kernel eigenfunctions, indicating that the representation is poorly aligned to this task (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p>To illustrate how our approach can be used for different learning problems, we evaluate the ability of linear readouts to reconstruct natural images from neural responses to those images (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The ability to reconstruct sensory stimuli from a neural code is an influential normative principle for primary visual cortex (<xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>). Here, we ask which aspects of the presented natural scene stimuli are easiest to learn to reconstruct. Since mouse V1 neurons tend to be selective towards low spatial frequency bands (<xref ref-type="bibr" rid="bib78">Niell and Stryker, 2008</xref><xref ref-type="bibr" rid="bib14">Bonin et al., 2011</xref>; <xref ref-type="bibr" rid="bib123">Vreysen et al., 2012</xref>), we consider reconstruction of band-pass filtered images with spatial frequency wave-vector <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> constrained to an annulus <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msqrt><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> (in units of <inline-formula><mml:math id="inf124"><mml:msup><mml:mtext>pixels</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) and plot the cumulative power <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> associated with each choice of the upper limit <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). The frequency cutoffs were chosen in this way to preserve the volume in Fourier space to <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which quantifies the dimension of the function space. We see that the lower frequency band-limited images are easier to reconstruct, as evidenced by their cumulative power <inline-formula><mml:math id="inf129"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and learning curves <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4D and E</xref>). This reflects the fact that the population preferentially encodes low spatial frequency content in the image (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Experiments with additional values of <inline-formula><mml:math id="inf131"><mml:mi>r</mml:mi></mml:math></inline-formula> are provided in the <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> with additional details found in the Appendix Visual scene reconstruction task.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Reconstructing filtered natural images from V1 responses reveals preference for low spatial frequencies.</title><p>(<bold>A</bold>) Natural scene stimuli <inline-formula><mml:math id="inf132"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> were presented to mice and V1 cells were recorded. (<bold>B</bold>) The images weighted by the top eigenfunctions <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. These “eigenimages&quot; collectively define the difficulty of reconstructing images through readout. (<bold>C</bold>) The kernel spectrum of the V1 code for natural images. (<bold>D</bold>) The cumulative power curves for reconstruction of band-pass filtered images. Filters preserve spatial frequencies in the range <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msqrt><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mn>0.2</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, chosen to preserve volume in Fourier space as <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is varied. (<bold>E</bold>) The learning curves obey the ordering of the cumulative power curves. The images filtered with the lowest band-pass cutoff are easiest to reconstruct from the neural responses. (<bold>F</bold>) Examples of a band-pass filtered image with different preserved frequency bands.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>The inductive bias to reconstruct low spatial frequency components of natural scenes from the population responses holds over several band-pass filters of the form <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msqrt><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</title><p>(<bold>A</bold>) The cumulative power curves for reconstruction of the filtered images with different values of <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Lower values of <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and higher values of <inline-formula><mml:math id="inf140"><mml:mi>c</mml:mi></mml:math></inline-formula> preserve mostly low frequency content in the images and are easier to reconstruct from the neural responses. (<bold>B</bold>) The learning curves respect the ordering of the cumulative power metric <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig4-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Mechanisms of spectral bias and code-task alignment in a simple model of V1</title><p>How do population level inductive biases arise from properties of single neurons? To illustrate that a variety of mechanisms may be involved in a complex manner, we study a simple model of V1 to elucidate neural mechanisms that lead to the low frequency bias at the population level. In particular, we focus on neural nonlinearities and selectivity profiles.</p><p>We model responses of V1 neurons as photoreceptor inputs passed through Gabor filters and a subsequent experimentally motivated power-law nonlinearity (<xref ref-type="bibr" rid="bib3">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib97">Rumyantsev et al., 2020</xref>), modeling a population of orientation selective simple cells (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) (see Appendix A simple feedforward model of V1). In this model, the kernel for static gratings with orientation <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is of the form <inline-formula><mml:math id="inf143"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and, as a consequence, the eigenfunctions of the kernel in this setting are Fourier modes. The eigenvalues, and hence the strength of the spectral bias, are determined by the nonlinearity as we discuss in Appendix Gabor model spectral bias and fit to V1 data. We numerically fit the parameters of the nonlinearity to the V1 responses and use these parameters our investigations in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A model of V1 as a bank of Gabor filters recapitulates experimental inductive bias.</title><p>(<bold>A</bold>) Gabor filtered inputs are mapped through nonlinearity. A grating stimulus (left) with orientation <inline-formula><mml:math id="inf144"><mml:mi>θ</mml:mi></mml:math></inline-formula> and phase <inline-formula><mml:math id="inf145"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is mapped through a circuit of simple and complex cells (middle). Some examples of randomly sampled Gabor filters (right) generate preferred orientation tuning of neurons in the population. (<bold>B</bold>) We plot the top 12 eigenfunctions <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (modes) for pure simple cell population, pure complex cell population and a mixture population with half simple and half complex cells. The pure complex cell population has all eigenfunctions independent of phase <inline-formula><mml:math id="inf147"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. A pure simple cell population <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> or mixture codes <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> depend on both orientation phase in a nontrivial way. (<bold>C</bold>) Three tasks are visualized, where color indicates the binary target value ± 1. The left task only depends on orientation stimulus variable <inline-formula><mml:math id="inf150"><mml:mi>θ</mml:mi></mml:math></inline-formula>, the middle only depends on phase <inline-formula><mml:math id="inf151"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, the hybrid task (right) depends on both. (<bold>D</bold>) (top) Generalization error and cumulative power distributions for the three tasks as a function of the simple-complex cell mixture parameter <inline-formula><mml:math id="inf152"><mml:mi>s</mml:mi></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>The fit of our simple cell model to the mouse V1 code.</title><p>(<bold>A</bold>) The fit nonlinearity has exponent <inline-formula><mml:math id="inf153"><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1.7</mml:mn></mml:mrow></mml:math></inline-formula> and sparsifying threshold <inline-formula><mml:math id="inf154"><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>. These parameters are consistent with similar measurements in the cat visual cortex. We compare this nonlinearity to alternative codes with higher and lower <inline-formula><mml:math id="inf155"><mml:mi>q</mml:mi></mml:math></inline-formula> and a code with the same <inline-formula><mml:math id="inf156"><mml:mi>q</mml:mi></mml:math></inline-formula> but no threshold <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) The fit parameters (blue) accurately fit the experimental kernel (black dots). (<bold>C</bold>) The spectra are also well predicted by the fit model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Nonlinear Rectification and proportion of simple and complex cells influences the inductive bias of the population code.</title><p>(<bold>A</bold>) The choice of nonlinearity has influence on the kernel and its spectrum. If the nonlinearity is <inline-formula><mml:math id="inf158"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>B</bold>) The sparsity can be increased by shifting the nonlinearity <inline-formula><mml:math id="inf160"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Sparser codes have higher dimensionality. Note that <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a special case where the neurons behave in the linear regime for all inputs <inline-formula><mml:math id="inf162"><mml:mi>θ</mml:mi></mml:math></inline-formula> since the currents <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:math></inline-formula> are positive. Thus, for <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the spectrum decays like a Bessel Function <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for a constant <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>C-D</bold>) Easy and hard orientation discrimination tasks with varying nonlinear polynomial order <inline-formula><mml:math id="inf167"><mml:mi>q</mml:mi></mml:math></inline-formula>. At low sample sizes, large <inline-formula><mml:math id="inf168"><mml:mi>q</mml:mi></mml:math></inline-formula> performs better, whereas at large <inline-formula><mml:math id="inf169"><mml:mi>P</mml:mi></mml:math></inline-formula>, the step function nonlinearity <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> achieves the best performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>The modified energy based model with partial selectivity to phase <inline-formula><mml:math id="inf171"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> reproduces the inductive bias of partial invariance.</title><p>(<bold>A</bold>) Some example F1/F0 distributions for various values of <inline-formula><mml:math id="inf172"><mml:mi>α</mml:mi></mml:math></inline-formula> (Appendix Energy model with partially phase-selective cells). As <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>α</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> the code becomes comprised entirely of complex cells. (<bold>B</bold>) Randomly selected tuning curves <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the top 12 eigenfunctions <inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the modified energy model with <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:math></inline-formula>. As expected the code with smaller <inline-formula><mml:math id="inf178"><mml:mi>α</mml:mi></mml:math></inline-formula> gives tuning curves with a variety of selectivity to phase variable <inline-formula><mml:math id="inf179"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, with some cells appearing more complex like and some appearing more like simple cells. At larger <inline-formula><mml:math id="inf180"><mml:mi>α</mml:mi></mml:math></inline-formula>, all cells are approximately phase invariant. This heterogeneity in phase tuning at small <inline-formula><mml:math id="inf181"><mml:mi>α</mml:mi></mml:math></inline-formula> and homogeneity at large <inline-formula><mml:math id="inf182"><mml:mi>α</mml:mi></mml:math></inline-formula> are also reflected in the top eigenfunctions, which are phase dependent in the <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> code but phase-invariant in the <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:math></inline-formula> code. (<bold>C</bold>) The learning curves for the three tasks as a function of <inline-formula><mml:math id="inf185"><mml:mi>α</mml:mi></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig5-figsupp3-v2.tif"/></fig></fig-group><p>Next, to further illustrate the importance of code-task alignment, we study how invariances in the code to stimulus variations may affect the learning performance. We introduce complex cells in addition to simple cells in our model with proportion <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of simple cells (Appendix Gabor model spectral bias and fit to V1 data; <xref ref-type="fig" rid="fig5">Figure 5A</xref>), and allow phase, <inline-formula><mml:math id="inf187"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, variations in static gratings. We use the energy model (<xref ref-type="bibr" rid="bib3">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib106">Simoncelli and Heeger, 1998</xref>) to capture the phase invariant complex cell responses (Appendix Phase variation, complex cells and invariance, complex cell populations are phase invariant). We reason that in tasks that do not depend on phase information, complex cells should improve sample efficiency.</p><p>In this model, the kernel for the V1 population is a convex combination of the kernels for the simple and complex cell populations <inline-formula><mml:math id="inf188"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is the kernel for a pure simple cell population that depends on both orientation and phase, and <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the kernel of a pure complex cell population that is invariant to phase (Appendix Complex cell populations are phase invariant). <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows top kernel eigenfunctions for various values of <inline-formula><mml:math id="inf191"><mml:mi>s</mml:mi></mml:math></inline-formula> elucidating inductive bias of the readout.</p><p><xref ref-type="fig" rid="fig5">Figure 5D and E</xref> show generalization performance on tasks with varying levels of dependence on phase and orientation. On pure orientation discrimination tasks, increasing the proportion of complex cells by decreasing <inline-formula><mml:math id="inf192"><mml:mi>s</mml:mi></mml:math></inline-formula> improves generalization. Increasing the sensitivity to the nuisance phase variable, <inline-formula><mml:math id="inf193"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, only degrades performance. The cumulative power curve is also maximized at <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. However, on a task which only depends on the phase, a pure complex cell population cannot generalize, since variation in the target function due to changes in phase cannot be explained in the codes’ responses. In this setting, a pure simple cell population attains optimal performance. The cumulative power curve is maximized at <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Lastly, in a nontrivial hybrid task which requires utilization of both variables <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></inline-formula>, an optimal mixture <inline-formula><mml:math id="inf197"><mml:mi>s</mml:mi></mml:math></inline-formula> exists for each sample budget <inline-formula><mml:math id="inf198"><mml:mi>P</mml:mi></mml:math></inline-formula> which minimizes the generalization error. The cumulative power curve is maximized at different <inline-formula><mml:math id="inf199"><mml:mi>s</mml:mi></mml:math></inline-formula> values depending on <inline-formula><mml:math id="inf200"><mml:mi>k</mml:mi></mml:math></inline-formula>, the component of the target function. This is consistent with an optimal heterogenous mix, because components of the target are learned successively with increasing sample size. V1 must code for a variety of possible tasks and we can expect a nontrivial optimal simple cell fraction <inline-formula><mml:math id="inf201"><mml:mi>s</mml:mi></mml:math></inline-formula>. We conclude that the degree of invariance required for the set of natural tasks, and the number of samples determine the optimal simple cell, complex cell mix. We also considered a more realistic model where the relative selectivity of each visual cortex neuron to phase <inline-formula><mml:math id="inf202"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, measured with the F1/F0 ratio takes on a continuum of possible values with some cells more invariant to phase and some less invariant. In (Appendix Energy model with partially phase-selective cells, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>) we discuss a simple adaptation of the energy model which can interpolate between a population of entirely simple cells and a population of entirely complex cells, giving diverse selectivity for the intermediate regime. We show that this model reproduces the inductive bias of <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></sec><sec id="s2-6"><title>Small and large sample size behaviors of generalization</title><p>Recently, <xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref> argued that the input-output differentiability of the code, governed by the asymptotic rate of spectral decay, may be enabling better generalization. Our results provide a more nuanced view of the relation between generalization and kernel spectra. First, generalization with low sample sizes crucially depend on the top eigenvalues and eigenfunctions of the code’s kernel, not the tail. Second, generalization requires alignment of the code with the task of interest. Non-differentiable codes can generalize well if there is such an alignment. To illustrate these points, here, we provide examples where asymptotic conditions on the kernel spectrum are insufficient to describe generalization performance for small sample sizes (<xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and Appendix Asymptotic power law scaling of learning curves), and where non-differentiable kernels generalize better than differentiable kernels (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The top eigensystem of a code determines its low-<inline-formula><mml:math id="inf203"><mml:mi>P</mml:mi></mml:math></inline-formula> generalization error.</title><p>(<bold>A</bold>) A periodic variable is coded by a population of neurons with tuning curves of different widths (top). Narrow, wide and optimal refers to the example in C. These codes are all smooth (infinitely differentiable) but have very different feature space representations of the stimulus variable <inline-formula><mml:math id="inf204"><mml:mi>θ</mml:mi></mml:math></inline-formula>, as random projections reveal (below). (<bold>B</bold>) (left) The population codes in the above figure induce von Mises kernels <inline-formula><mml:math id="inf205"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with different bandwidths <inline-formula><mml:math id="inf206"><mml:mi>σ</mml:mi></mml:math></inline-formula>. (right) Eigenvalues of the three kernels. (<bold>C</bold>) (left) As an example learning task, we consider estimating a ‘bump’ target function. The optimal kernel (red, chosen as optimal bandwidth for <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>) achieves a better generalization error than either the wide (green) or narrow (blue) kernels. (middle) A contour plot shows generalization error for varying bandwidth <inline-formula><mml:math id="inf208"><mml:mi>σ</mml:mi></mml:math></inline-formula> and sample size <inline-formula><mml:math id="inf209"><mml:mi>P</mml:mi></mml:math></inline-formula>. (right) The large <inline-formula><mml:math id="inf210"><mml:mi>P</mml:mi></mml:math></inline-formula> generalization error scales in a power law. Solid lines are theory, dots are simulations averaged over 15 repeats, dashed lines are asymptotic power law scalings described in main text. Same color code as B and C-left.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Bandwidth (spectral bulk) also governs generalization in non-smooth codes.</title><p>(<bold>A, B</bold>) Kernel regression experiments are performed with Laplace kernels of varying bandwidth on a non-differentiable target function. The top eigenvalues are modified by changing the bandwidth, but the asymptotic power law scaling is preserved. Generalization at low <inline-formula><mml:math id="inf211"><mml:mi>P</mml:mi></mml:math></inline-formula> is shown in the contour plot while the large <inline-formula><mml:math id="inf212"><mml:mi>P</mml:mi></mml:math></inline-formula> scaling is provided in the generalization. In A-right and B-right, color code is the same as in the main text.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Non-differentiable kernels can generalize better than infinitely differentiable kernels in a variety of contexts.</title><p>(<bold>A</bold>) An infinitely differentiable von-Mises kernel (blue) will be compared to a non-differentiable Laplace kernel (green). (<bold>B</bold>) For a non-differentiable target function <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a cusp, the non-smooth kernel can provide a better fit to the target function at <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> samples. (<bold>C</bold>) The learning curves for this task. Solid lines are theory, circles are simulations. (<bold>D</bold>) The lengthscale of the kernel can be more important than local smoothness. We will now compare a narrow von-Mises kernel (still infinitely differentiable) with a non-differentiable wide kernel. (<bold>E</bold>) The non-smooth kernel generalizes on a smooth task almost perfectly whereas the narrow smooth kernel only locally interpolates. (<bold>F</bold>) The learning curves for this smooth task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig6-figsupp2-v2.tif"/></fig></fig-group><p>Our example demonstrates how a code allowing good generalization for large sample sizes can be disadvantageous for small sizes. In <xref ref-type="fig" rid="fig6">Figure 6A</xref>, we plot three different populations of neurons with smooth (infinitely differentiable) tuning curves that tile a periodic stimulus variable, such as the direction of a moving grating. The tuning width, <inline-formula><mml:math id="inf215"><mml:mi>σ</mml:mi></mml:math></inline-formula>, of the tuning curves strongly influences the structure of these codes: narrower widths have more high frequency content as we illustrate in a random 3D projection of the population code for <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Visualization of the corresponding (von Mises) kernels and their spectra are provided in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. The width of the tuning curves control bandwidths of the kernel spectra <xref ref-type="fig" rid="fig6">Figure 6B</xref>, with narrower curves having an later decay in the spectrum and higher high frequency eigenvalues. These codes can have dramatically different generalization performance, which we illustrate with a simple “bump&quot; target response (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In this example, for illustration purposes, we let the network learn with a delta-rule with a weight decay, leading to a regularized kernel regression solution (Appendix Weight decay and ridge regression). For a sample size of <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, we observe that codes with too wide or too narrow tuning curves (and kernels) do not perform well, and there is a well-performing code with an optimal tuning curve width <inline-formula><mml:math id="inf218"><mml:mi>σ</mml:mi></mml:math></inline-formula>, which is compatible with the width of the target bump, <inline-formula><mml:math id="inf219"><mml:msub><mml:mi>σ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula>. We found that optimal <inline-formula><mml:math id="inf220"><mml:mi>σ</mml:mi></mml:math></inline-formula> is different for each <inline-formula><mml:math id="inf221"><mml:mi>P</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In the large-<inline-formula><mml:math id="inf222"><mml:mi>P</mml:mi></mml:math></inline-formula> regime, the ordering of the performance of the three codes are reversed (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In this regime generalization error scales in a power law (Appendix Asymptotic power law scaling of learning curves) and the narrow code, which performed worst for <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>P</mml:mi><mml:mo>∼</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, performs the best. This example demonstrates that asymptotic conditions on the tail of the spectra are insufficient to understand generalization in the small sample size limit. The bulk of the kernel’s spectrum needs to match the spectral structure of the task to generalize efficiently in the low-sample size regime. However, for large sample sizes, the tail of the eigenvalue spectrum becomes important. We repeat the same exercise and draw the same conclusions for a non-differentiable kernel (Laplace) (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) showing that these results are not an artifact of the infinite differentiability of von Mises kernels. We further provide examples where non-differentiable kernels generalizing better than differentiable kernels in <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>.</p></sec><sec id="s2-7"><title>Time-dependent neural codes</title><p>Our framework can directly be extended to learning of arbitrary time-varying functions of time-varying inputs from an arbitrary spatiotemporal population code (Methods RNN experiment, Appendix Time dependent neural codes). In this setting, the population code <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a function of an input stimulus sequence <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and possibly its entire history, and time <inline-formula><mml:math id="inf226"><mml:mi>t</mml:mi></mml:math></inline-formula>. A downstream linear readout <inline-formula><mml:math id="inf227"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> learns a target sequence <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from a total of <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> examples that can come at any time during any sequence.</p><p>As a concrete example, we focus on readout from a temporal population code generated by a recurrent neural network in a task motivated by a delayed reach task (<xref ref-type="bibr" rid="bib5">Ames et al., 2019</xref>; <xref ref-type="fig" rid="fig7">Figure 7A and B</xref>). In this task, the network is presented for a short time an input cue sequence coding an angular variable which is drawn randomly from a distribution (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). The recurrent neural network must remember this angle and reproduce an output sequence which is a simple step function whose height depends on the angle which begins after a time delay from the cessation of input stimulus and lasts for a short time (<xref ref-type="fig" rid="fig7">Figure 7D</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The performance of time-dependent codes when learning dynamical systems can be understood through spectral bias.</title><p>(<bold>A</bold>) We study the performance of time dependent codes on a delayed response task which requires memory retrieval. A cue (black dot) is presented at an angle <inline-formula><mml:math id="inf230"><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>. After a delay time <inline-formula><mml:math id="inf231"><mml:mi>d</mml:mi></mml:math></inline-formula>, the cursor position (blue triangle) must be moved to the remembered cue position and then subsequently moved back to the origin after a short time. (<bold>B</bold>) The readout weights (blue) of a time dependent code can be learned through a modified delta rule. (<bold>C</bold>) Input is presented to the network as a time series which terminates at <inline-formula><mml:math id="inf232"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The sequences are generated by drawing an angle <inline-formula><mml:math id="inf233"><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and using two step functions as input time-series that code for the cosine and the sine of the angle (Methods RNN experiment, Appendix Time dependent neural codes). We show an example of the one of the variables in a input sequence. (<bold>D</bold>) The target functions for the memory retrieval task are step functions delayed by a time <inline-formula><mml:math id="inf234"><mml:mi>d</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) The kernel <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> compares the code for two sequences at two distinct time points. We show the time dependent kernel for identical sequences (left) and the stimulus dependent kernel for equal time points (middle left) as well as for non-equal stimuli (middle right) and non-equal time (right). (<bold>F</bold>) The kernel can be diagonalized, and the eigenvalues <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> determine the spectral bias of the reservoir computer (left). We see that higher gain <inline-formula><mml:math id="inf237"><mml:mi>g</mml:mi></mml:math></inline-formula> networks have higher dimensional representations. The ‘eigensystems’ <inline-formula><mml:math id="inf238"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are functions of time and cue angle. We plot only <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> components of top systems <inline-formula><mml:math id="inf240"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (right). (<bold>G</bold>) The readout is trained to approximate a target function <inline-formula><mml:math id="inf241"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which requires memory of the presented cue angle. (left) The theoretical (solid) and experimental (vertical errorbar, 100 trials) generalization error <inline-formula><mml:math id="inf242"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> are plotted for the three delays <inline-formula><mml:math id="inf243"><mml:mi>d</mml:mi></mml:math></inline-formula> against training sample size <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. (right) The ordering of <inline-formula><mml:math id="inf245"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> matches the ordering of the <inline-formula><mml:math id="inf246"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> curves as expected.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig7-v2.tif"/></fig><p>The kernel induced by the spatiotemporal code is shown in <xref ref-type="fig" rid="fig7">Figure 7E</xref>. The high dimensional nature of the activity in the recurrent network introduces complex and rich spatiotemporal similarity structure. <xref ref-type="fig" rid="fig7">Figure 7F</xref> shows the kernel’s eigensystem, which consists of stimulus dependent time-series <inline-formula><mml:math id="inf247"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each eigenvalue <inline-formula><mml:math id="inf248"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. An interesting link can be made with this eigensystem and linear low-dimensional manifold dynamics observed in several cortical areas (<xref ref-type="bibr" rid="bib112">Stopfer et al., 2003</xref>; <xref ref-type="bibr" rid="bib55">Kato et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Cunningham and Yu, 2014</xref>; <xref ref-type="bibr" rid="bib98">Sadtler et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Gao and Ganguli, 2015</xref>; <xref ref-type="bibr" rid="bib36">Gallego et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Chapin and Nicolelis, 1999</xref>; <xref ref-type="bibr" rid="bib12">Bathellier et al., 2008</xref>). The kernel eigenfunctions also define the latent variables obtained through a singular value decomposition of the neural activity <inline-formula><mml:math id="inf249"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib35">Gallego et al., 2017</xref>). With enough samples, the readout neuron can learn to output the desired angle with high fidelity (<xref ref-type="fig" rid="fig7">Figure 7G</xref>). Unsurprisingly, tasks involving long time delays are more difficult and exhibit lower cumulative power curves. Consequently, the generalization error for small delay tasks drops much more quickly with increasing samples <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s2-8"><title>Biological codes are metabolically more efficient and more selective than other codes with identical kernels</title><p>Although, the performance of linear readouts may be invariant to rotations that preserve kernels (<xref ref-type="fig" rid="fig2">Figure 2</xref>), metabolic efficiency may favor certain codes over others (<xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib7">Atick and Redlich, 1992</xref>; <xref ref-type="bibr" rid="bib8">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib107">Simoncelli and Olshausen, 2001</xref>), reducing degeneracy in the space of codes with identical kernels. To formalize this idea, we define <inline-formula><mml:math id="inf251"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:math></inline-formula> to be the vector of spontaneous firing rates of a population of neurons, and <inline-formula><mml:math id="inf252"><mml:mrow><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> be the spiking rate vector in response to a stimulus <inline-formula><mml:math id="inf253"><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>. The vector <inline-formula><mml:math id="inf254"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:math></inline-formula> ensures that neural responses are non-negative. The modulation with respect to the spontaneous activity, <inline-formula><mml:math id="inf255"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, gives the population code and defines the kernel, <inline-formula><mml:math id="inf256"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. To avoid confusion with <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we will refer to <inline-formula><mml:math id="inf258"><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> as total spiking activity. We propose that population codes prefer smaller spiking activity subject to a fixed kernel. In other words, because the kernel is invariant to any change of the spontaneous firing rates and left rotations of <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the orientation and shift of the population code <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> should be chosen such that the resulting total spike count <inline-formula><mml:math id="inf261"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is small.</p><p>We tested whether biological codes exhibit lower total spiking activity than others exhibiting the same kernel on mouse V1 recordings, using deconvolved calcium activity as a proxy for spiking events (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Pachitariu et al., 2018</xref>) (Methods Data analysis; <xref ref-type="fig" rid="fig8">Figure 8</xref>). To compare the experimental total spiking activity to other codes with identical kernels, we computed random rotations of the neural responses around spontaneous activity, <inline-formula><mml:math id="inf262"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Qr</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and added the <inline-formula><mml:math id="inf263"><mml:mi>δ</mml:mi></mml:math></inline-formula> that minimizes total spiking activity and maintains its nonnegativity (Methods Generating RROS codes). We refer to such an operation as RROS (random rotation and optimal shift), and a code generated by an RROS operation as an RROS code. The matrix <inline-formula><mml:math id="inf264"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> is a randomly sampled orthogonal matrix (<xref ref-type="bibr" rid="bib6">Anderson et al., 1987</xref>). In other words, we compare the true code to the most metabolically efficient realizations of its random rotations. This procedure may result in an increased or decreased total spike count in the code, and is illustrated in a synthetic dataset in <xref ref-type="fig" rid="fig8">Figure 8A</xref>. We conducted this procedure on subsets of various sizes of mouse V1 neuron populations, as our proposal should hold for any subset of neurons (Methods Generating RROS codes), and found that the true V1 code is much more metabolically efficient than randomly rotated versions of the code (<xref ref-type="fig" rid="fig8">Figure 8B and C</xref>). This finding holds for both responses to static gratings and to natural images as we show in <xref ref-type="fig" rid="fig8">Figure 8B and C</xref> respectively.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The biological code is more metabolically efficient than random codes with same inductive biases.</title><p>(<bold>A</bold>) We illustrate our procedure in a synthetic example. A non-negative population code (left) can be randomly rotated about its spontaneous firing rate (middle), illustrated as a purple dot, and optimally shifted to a new non-negative population code (right). If the kernel is measured about the spontaneous firing rate, these transformations leave the inductive bias of the code invariant but can change the total spiking activity of the neural responses. We refer to such an operation as random rotation + optimal shift (RROS). We also perform gradient descent over rotations and shifts, generating an optimized code (opt). (<bold>B</bold>) Performing RROS on <inline-formula><mml:math id="inf265"><mml:mi>N</mml:mi></mml:math></inline-formula> neuron subsamples of experimental Mouse V1 recordings (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>), shows that the true code has much lower average cost <inline-formula><mml:math id="inf266"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> compared to random rotations of the code. The set of possible RROS transformations (Methods Generating RROS codes, and Methods Comparing sparsity of population codes) generates a distribution over average cost, which has higher mean than the true code. We also optimize metabolic cost over the space of RROS transformations, which resulted in the red dashed lines. We plot the distance (in units of standard deviations) between the cost of the true and optimal codes and the cost of randomly rotated codes for different neuron subsample sizes <inline-formula><mml:math id="inf267"><mml:mi>N</mml:mi></mml:math></inline-formula>. (<bold>C</bold>) The same experiment performed on Mouse V1 responses to ImageNet images from 10 relevant classes (<xref ref-type="bibr" rid="bib113">Stringer et al., 2018a</xref>; <xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>). (<bold>D</bold>) The <italic>lifetime</italic> (LS) and <italic>population sparseness</italic> (PS) levels (Methods Lifetime and population sparseness) are higher for the Mouse V1 code than for a RROS code. The distance between average LS and PS of true code and RROS codes increases with <inline-formula><mml:math id="inf268"><mml:mi>N</mml:mi></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Our metabolic efficiency finding is robust to different pre-processing techniques and upper bounds on neural firing.</title><p>(<bold>A</bold>) We show the same result as <xref ref-type="fig" rid="fig8">Figure 8</xref> except we use raw (non <inline-formula><mml:math id="inf269"><mml:mi>z</mml:mi></mml:math></inline-formula>-scored) estimate of responses for each stimulus. (<bold>B</bold>) Our result is robust to imposition of firing rate upper bounds <italic>u</italic><sub><italic>b</italic></sub> on each neuron. This result uses the <inline-formula><mml:math id="inf270"><mml:mi>z</mml:mi></mml:math></inline-formula>-scored responses to be consistent with the rest of the paper. The biological code achieves a maximum <inline-formula><mml:math id="inf271"><mml:mi>z</mml:mi></mml:math></inline-formula>-score values in the range <inline-formula><mml:math id="inf272"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>3.2</mml:mn><mml:mo>,</mml:mo><mml:mn>4.7</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, which motivated the range of our tested upper bound values <inline-formula><mml:math id="inf273"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Our finding is robust to the number of sampled stimuli <inline-formula><mml:math id="inf274"><mml:mi>P</mml:mi></mml:math></inline-formula> as we show in an experiment where rotations in <inline-formula><mml:math id="inf275"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> dimensional subspace.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>The observation that randomly oriented codes with the same kernel require higher spike counts than the original code is reproduced from electrophysiological recordings of Mouse visual cortex (VISp and VISal) from the AIBO.</title><p>(<bold>A</bold>) Distribution of average spike count for a random selection of <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> neurons. (<bold>B</bold>) The same for <inline-formula><mml:math id="inf277"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> randomly selected neurons. (<bold>C</bold>) Since the distribution of average spike count for the randomly rotated codes concentrates with <inline-formula><mml:math id="inf278"><mml:mi>N</mml:mi></mml:math></inline-formula> the number of standard deviations the true code is from the mean increases with <inline-formula><mml:math id="inf279"><mml:mi>N</mml:mi></mml:math></inline-formula>. We show that this scaling is approximately like <inline-formula><mml:math id="inf280"><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math></inline-formula> which suggests that the variability in average spike count for randomly rotated codes goes scales with neuron count like <inline-formula><mml:math id="inf281"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:math></inline-formula>. This is intuitive since the average spike count over the rotated code is an empirical average over <inline-formula><mml:math id="inf282"><mml:mi>N</mml:mi></mml:math></inline-formula> random variables. (<bold>D</bold>) The lifetime sparseness of neurons in the true code are spread out over a large range. (<bold>E</bold>) The lifetime sparseness distribution for an example RROS code does not have the same range. (<bold>F</bold>) The average (over neurons) lifetime sparseness of RROS codes over random rotations is significantly lower than the average lifetime sparseness of the true code. (<bold>G-I</bold>) The true visual cortex code has much higher population sparseness over each grating stimulus as well.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-fig8-figsupp2-v2.tif"/></fig></fig-group><p>To further explore metabolic efficiency, we posed an optimization problem which identifies the most efficient code with the same kernel as the biological V1 code. This problem searches over rotation matrices <inline-formula><mml:math id="inf283"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> and finds the <inline-formula><mml:math id="inf284"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> matrix and off-set vector <inline-formula><mml:math id="inf285"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:math></inline-formula> which gives the lowest cost <inline-formula><mml:math id="inf286"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> (Methods Comparing sparsity of population codes) (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Although the local optimum identified with the algorithm is lower in cost than the biological code, both the optimal and biological codes are significantly displaced from the distribution of random codes with same kernel. Our findings do not change when data is preprocessed with an alternative strategy, an upper bound on neural responses is imposed on rotated codes, or subsets of stimuli are considered (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). We further verified these results on electrophysiological recordings of mouse visual cortex from the Allen Institute Brain Observatory (<xref ref-type="bibr" rid="bib29">de Vries et al., 2020</xref>), (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). Overall, the large disparity in total spiking activity between the true and randomly generated codes with identical kernels suggests that metabolic constraints may favor the biological code over others that realize the same kernel.</p><p>The disparity between the true biological code and the RROS code is not only manifested in terms of total activity level, but also in terms of single neuron and single stimulus sparseness measures, specifically lifetime and population sparseness distributions (Methods Lifetime and population sparseness) (<xref ref-type="bibr" rid="bib125">Willmore and Tolhurst, 2001</xref>; <xref ref-type="bibr" rid="bib63">Lehky et al., 2005</xref>; <xref ref-type="bibr" rid="bib120">Treves and Rolls, 1991</xref>; <xref ref-type="bibr" rid="bib84">Pehlevan and Sompolinsky, 2014</xref>). In <xref ref-type="fig" rid="fig8">Figure 8D</xref>, we compare the lifetime and population sparseness distributions of the true biological code with a RROS version of the same code, revealing biological neurons have significantly higher lifetime sparseness. In Appendix Necessary conditions for optimally sparse codes, we provide analytical arguments which suggest that tuning curves of optimally sparse non-negative codes with full-rank kernels will have selective tuning.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Elucidating inductive biases of the brain is fundamentally important for understanding natural intelligence (<xref ref-type="bibr" rid="bib118">Tenenbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Lake et al., 2017</xref>; <xref ref-type="bibr" rid="bib108">Sinz et al., 2019</xref>; <xref ref-type="bibr" rid="bib134">Zador, 2019</xref>). These biases are coded into the brain by the dynamics of its neurons, the architecture of its networks, its representations and plasticity rules. Finding ways to extract the inductive biases from neuroscience datasets requires a deeper theoretical understanding of how all these factors shape the biases, and is an open problem. In this work, we attempted to take a step towards filling this gap by focusing on how the structure of static neural population codes shape inductive biases for learning of a linear readout neuron under a biologically plausible learning rule. If the readout neuron’s output is correlated with behavior, and that correlation is known, then our theory could possibly be modified to predict what behavioral tasks can be learned faster.</p><p>Under the delta rule, the generalization performance of the readout is entirely dependent on the code’s inner product kernel; the kernel is a determinant of inductive bias. In its finite dimensional form, the kernel is an example of a representational similarity matrix and is a commonly used tool to study neural representations (<xref ref-type="bibr" rid="bib31">Edelman, 1998</xref>; <xref ref-type="bibr" rid="bib57">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Laakso and Cottrell, 2000</xref>; <xref ref-type="bibr" rid="bib56">Kornblith et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib85">Pehlevan et al., 2018</xref>). Our work elucidates a concrete link between this experimentally measurable mathematical object, and sample-efficient learning.</p><p>We derived an analytical expression for the generalization error as a function of sample-size under very general conditions, for an arbitrary stimulus distribution, arbitrary population code and an arbitrary target stimulus-response map. We used our findings in both theoretical and experimental analysis of primary visual cortex, and temporal codes in a delayed reach task. This generality of our theory is a particular strength.</p><p>Our analysis elucidated two principles that define the inductive bias. The first one is spectral bias: kernel eigenfunctions with large eigenvalues can be estimated using a smaller number of samples. The second principle is the code-task alignment: target functions with most of their power in top kernel eigenfunctions can be estimated efficiently and are compatible with the code. The cumulative power distribution, <inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>), provides a measure of this alignment. These findings define a notion of ‘simplicity’ bias in learning from examples, and provides a solution to the question of what stimulus-response maps are easier to learn. A similar simplicity bias has been also observed in training deep neural networks (<xref ref-type="bibr" rid="bib93">Rahaman et al., 2019</xref>; <xref ref-type="bibr" rid="bib129">Xu et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Kalimeris et al., 2019</xref>). Due to a correspondence between gradient-descent trained neural networks in the infinite-width limit and kernel machines (<xref ref-type="bibr" rid="bib53">Jacot et al., 2018</xref>), results on the spectral bias of kernel machines may shed light onto these findings (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). Though our present analysis focused on learning a single layer weight vector with the biologically plausible delta-rule, future work could explore the learning curves of other learning rules for deep networks (<xref ref-type="bibr" rid="bib16">Bordelon and Pehlevan, 2022a</xref>), such as feedback alignment (<xref ref-type="bibr" rid="bib65">Lillicrap et al., 2016</xref>) or perturbation methods (<xref ref-type="bibr" rid="bib52">Jabri and Flower, 1992</xref>). Such analysis could explore how inductive bias is also shaped by choice of learning rule, as well as the structure of the initial population code.</p><p>We applied our findings in both theoretical and experimental analysis of mouse primary visual cortex. We demonstrated a bias of neural populations towards low frequency orientation discrimination and low spatial frequency reconstruction tasks. The latter finding is consistent with the finding that mouse visual cortex neurons are selective for low spatial frequency (<xref ref-type="bibr" rid="bib78">Niell and Stryker, 2008</xref>; <xref ref-type="bibr" rid="bib123">Vreysen et al., 2012</xref>). The toy model of the visual cortex as a mixture of simple and complex cells demonstrated how invariances, specifically the phase invariance of the complex cells, in the population code can facilitate learning some tasks involving phase invariant responses at the expense of performance on others. The role of invariances in learning with kernel methods and deep networks have recently been investigated in machine learning literature, showing that invariant representations can improve capacity (<xref ref-type="bibr" rid="bib32">Farrell et al., 2021</xref>) and sample efficiency for invariant learning problems (<xref ref-type="bibr" rid="bib71">Mei et al., 2021</xref>; <xref ref-type="bibr" rid="bib64">Li et al., 2019</xref>; <xref ref-type="bibr" rid="bib128">Xiao and Pennington, 2022</xref>).</p><p>A recent proposal considered the possibility that the brain acts as an overparameterized interpolator (<xref ref-type="bibr" rid="bib46">Hasson et al., 2020</xref>). Suitable inductive biases are crucial to prevent overfitting and generalize well in such a regime (<xref ref-type="bibr" rid="bib13">Belkin et al., 2019</xref>). Our theory can explain these inductive biases since, when the kernel is full-rank, which typically is the case when there are more neurons in the population than the number of learning examples, the delta rule without weight decay converges to an interpolator of the learning examples. Modern deep learning architectures also operate in an overparameterized regime, but generalize well (<xref ref-type="bibr" rid="bib135">Zhang et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Belkin et al., 2019</xref>), and an inductive bias towards simple functions has been proposed as an explanation (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Kalimeris et al., 2019</xref>; <xref ref-type="bibr" rid="bib121">Valle-Perez et al., 2018</xref>). However, we also showed that interpolation can be harmful to prediction accuracy when the target function has some variance unexplained by the neural code or if the neural responses are significantly noisy, motivating use of explicit regularization.</p><p>Our work promotes sample efficiency as a general coding principle for neural populations, relating neural representations to the kinds of problems they are well suited to solve. These codes may be shaped through evolution or themselves be learned through prior experience (<xref ref-type="bibr" rid="bib134">Zador, 2019</xref>). Prior related work in this area demonstrated the dependence of sample-efficient learning of a two-angle estimation task on the width of the individual neural tuning curves (<xref ref-type="bibr" rid="bib72">Meier et al., 2020</xref>) and on additive function approximation properties of sparsely connected random networks (<xref ref-type="bibr" rid="bib45">Harris, 2019</xref>).</p><p>A sample efficiency approach to population coding differs from the classical efficient coding theories (<xref ref-type="bibr" rid="bib8">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib7">Atick and Redlich, 1992</xref>; <xref ref-type="bibr" rid="bib111">Srinivasan et al., 1982</xref>; <xref ref-type="bibr" rid="bib122">van Hateren, 1992</xref>; <xref ref-type="bibr" rid="bib94">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib22">Chalk et al., 2018</xref>), which postulate that populations of neurons optimize information content of their code subject to metabolic constraints or noise. While these theories emphasize different aspects of the code’s information content (such as reduced redundancy, predictive power, or sparsity), they do not address sample efficiency demands on learning. Further, recent studies demonstrated hallmarks of redundancy and correlation in population responses (<xref ref-type="bibr" rid="bib23">Chapin and Nicolelis, 1999</xref>; <xref ref-type="bibr" rid="bib12">Bathellier et al., 2008</xref>; <xref ref-type="bibr" rid="bib88">Pitkow and Meister, 2012</xref>; <xref ref-type="bibr" rid="bib37">Gao and Ganguli, 2015</xref>; <xref ref-type="bibr" rid="bib1">Abbasi-Asl et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Gallego et al., 2018</xref>; <xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>), violating a generic prediction of efficient coding theories that responses of different neurons should be uncorrelated across input stimuli in high signal-to-noise regimes to reduce redundancy in the code and maximize information content (<xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib7">Atick and Redlich, 1992</xref>; <xref ref-type="bibr" rid="bib111">Srinivasan et al., 1982</xref>; <xref ref-type="bibr" rid="bib122">van Hateren, 1992</xref>; <xref ref-type="bibr" rid="bib43">Haft and van Hemmen, 1998</xref>; <xref ref-type="bibr" rid="bib50">Huang and Rao, 2011</xref>). In our theory, the structured correlations of neural responses correspond to the decay in the spectrum of the kernel, and play a key role in biasing learned readouts towards simple functions.</p><p>In recent related studies, the asymptotic decay rate of the kernel’s eigenspectrum was argued to be important for generalization (<xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>) and robustness (<xref ref-type="bibr" rid="bib76">Nassar et al., 2020</xref>). The spectral decay rate in the mouse V1 was found to be consistent with a high dimensional (power law) but smooth (differentiable) code, and smoothness was argued to be an enabler of generalization (<xref ref-type="bibr" rid="bib114">Stringer et al., 2018b</xref>). While we also identify power law spectral decays, we show that sample-efficient learning requires more than smoothness conditions in the form of asymptotic decay rates on the kernel’s spectrum. The interplay between the stimulus distribution, target response and the code gives rise to sample efficient learning. Because of spectral bias, the top eigenvalues govern the small sample size behavior. The tail of the spectrum becomes important at large sample sizes.</p><p>Though the kernel is degenerate with respect to rotations of the code in the neural activity space, we demonstrated that the true V1 code has much lower average activity than random codes with the same kernel, suggesting that evolution and learning may be selecting neural codes with low average spike rates which preserve sample-efficiency demands for downstream learning tasks. We predict that metabolic efficiency may be a determinant in the orientation and placement of the ubiquitously observed low-dimensional coding manifolds in neural activity space in other parts of the brain (<xref ref-type="bibr" rid="bib36">Gallego et al., 2018</xref>). The demand of metabolic efficiency is consistent with prior sparse coding theories (<xref ref-type="bibr" rid="bib79">Niven and Laughlin, 2008</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib107">Simoncelli and Olshausen, 2001</xref>; <xref ref-type="bibr" rid="bib49">Hromádka et al., 2008</xref>), however, our theory emphasizes sample-efficient learning as the primary normative objective for the code. As a note of caution, while our analysis holds under the assumption that the neural code is deterministic, real neurons exhibit variability in their responses to repeated stimuli. Such noisy population codes do not generally achieve identical generalization performance under RROS transformations. For example, if each neuron is constrained to produce i.i.d. Poisson noise, then simple shifts of the baseline firing rate reduce the information content of the code. However, if the neural noise is Gaussian (even with stimulus dependent noise covariance), then the generalization error is conserved under RROS operations (Appendix Effect of noise on RROS symmetry). Further studies could focus on revealing the space of codes with equivalent inductive biases under realistic noise models.</p><p>Our work constitutes a first step towards understanding inductive biases in neuronal circuits. To achieve this, we focused on a linear, delta-rule readout of a static population code. More work is need to study other factors that affect inductive bias. Importantly, sensory neuron tuning curves can adapt during perceptual learning tasks (<xref ref-type="bibr" rid="bib40">Gilbert, 1994</xref>; <xref ref-type="bibr" rid="bib42">Goltstein et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Ghose et al., 2002</xref>; <xref ref-type="bibr" rid="bib101">Schoups et al., 2001</xref>) with the strength of adaptation dependent on brain area (<xref ref-type="bibr" rid="bib130">Yang and Maunsell, 2004</xref>; <xref ref-type="bibr" rid="bib2">Adab et al., 2014</xref>; <xref ref-type="bibr" rid="bib81">Op de Beeck et al., 2007</xref>). In many experiments, these changes to tuning in sensory areas are small (<xref ref-type="bibr" rid="bib101">Schoups et al., 2001</xref>; <xref ref-type="bibr" rid="bib39">Ghose et al., 2002</xref>), satisfying the assumptions of our theory. For example monkeys trained on noisy visual motion detection exhibit changes in sensory-motor (LIP) but not sensory areas (MT), consistent with a model of readout from a static sensory population code (<xref ref-type="bibr" rid="bib61">Law and Gold, 2008</xref>; <xref ref-type="bibr" rid="bib103">Shadlen and Newsome, 2001</xref>). However, other perceptual learning tasks and other brain areas can exhibit significant changes in neural tuning (<xref ref-type="bibr" rid="bib96">Recanzone et al., 1993</xref>; <xref ref-type="bibr" rid="bib89">Pleger et al., 2003</xref>; <xref ref-type="bibr" rid="bib34">Furmanski et al., 2004</xref>). This diversity of results motivates more general analysis of learning in multi-layer networks, where representations in each layer can adapt flexibly to task structure (<xref ref-type="bibr" rid="bib104">Shan and Sompolinsky, 2021</xref>; <xref ref-type="bibr" rid="bib69">Mastrogiuseppe et al., 2022</xref>; <xref ref-type="bibr" rid="bib17">Bordelon and Pehlevan, 2022b</xref>; <xref ref-type="bibr" rid="bib4">Ahissar and Hochstein, 2004</xref>). Alternatively, our current analysis of inductive bias can still be consistent with multi-layer learning if the network is sufficiently overparameterized and tuning curves change very little (<xref ref-type="bibr" rid="bib53">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Lee et al., 2018</xref>; <xref ref-type="bibr" rid="bib104">Shan and Sompolinsky, 2021</xref>). In this case, network training is equivalent to kernel learning with a kernel that depends on the learning rule and architecture (<xref ref-type="bibr" rid="bib16">Bordelon and Pehlevan, 2022a</xref>). However, in the regime of neural network training where tuning curves change significantly, more sophisticated analytical tools are needed to predict generalization (<xref ref-type="bibr" rid="bib33">Flesch et al., 2021</xref>; <xref ref-type="bibr" rid="bib133">Yang and Hu, 2021</xref>; <xref ref-type="bibr" rid="bib17">Bordelon and Pehlevan, 2022b</xref>). Although our work focused on linear readouts, arbitrary nonlinear readouts which generate convex learning objectives have been recently studied in the high dimensional limit, giving qualitatively similar learning curves which depend on kernel eigenvalues and task model alignment (<xref ref-type="bibr" rid="bib68">Loureiro et al., 2021b</xref>; <xref ref-type="bibr" rid="bib25">Cui et al., 2022</xref>) (see Appendix Typical case analysis of nonlinear readouts).</p><p>Our work focused on how signal correlations influence inductive bias (<xref ref-type="bibr" rid="bib9">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib24">Cohen and Kohn, 2011</xref>). However, since real neurons do exhibit variability in their responses to identical stimuli, one should consider the effect of neural noise and noise correlations in learning. We provide a preliminary analysis of learning with neural noise in Appendix Impact of neural noise and unlearnable targets on learning, where we show that neural noise can lead to irreducible asymptotic error which depends on the geometry of the signal and noise correlations. Further, if the target function is not fully expressible as linear combinations of neural responses, overfitting peaks in the learning curves are possible, but can be mitigated with regularization implemented by a weight decay in the learning rule (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Future work could extend our analysis to study how signal and noise correlations interact to shape inductive bias and generalization performance in the case where the noise correlation matrices are non-isotropic, including the role of differential correlations (<xref ref-type="bibr" rid="bib75">Moreno-Bote et al., 2014</xref>). Overall, future work could build on the present analysis to incorporate a greater degree of realism in a theory of inductive bias.</p><p>Finally, we discuss possible applications of our work to experimental neuroscience. Our theory has potential implications for experimental studies of task learning. First, in cases where the population selective to stimuli can be measured directly, an experimenter could design easy or difficult tasks for an animal to learn from few examples, under a hypothesis that the behavioral output is a linear function of the observed neurons. Second, in cases where it is unclear which neural population contributes to learning, one could utilize our theory to solve the inverse problem of inferring the relevant kernel from observed learning curves on different tasks (<xref ref-type="bibr" rid="bib126">Wilson et al., 2015</xref>). From these tasks, the experimenter could compare the inferred kernel to those of different recorded populations. For instance, one could compare the kernels from separate populations to the inferred kernel obtained from learning curves on certain visual learning tasks. This could provide new ways to test theories of perceptual learning (<xref ref-type="bibr" rid="bib40">Gilbert, 1994</xref>). Lastly, extensions of our framework could quantify the role of neural variability on task learning and the limitation it imposes on accuracy and sample efficiency.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Generating example codes (Figure 1)</title><p>The two codes in <xref ref-type="fig" rid="fig1">Figure 1</xref> were constructed to produce two different kernels for <inline-formula><mml:math id="inf288"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.25</mml:mn><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>An infinite number of codes could generate either of these kernels. After diagonalizing the kernel into its eigenfunctions on a grid of 120 points, <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, we used a random rotation matrix <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (which satisfies <inline-formula><mml:math id="inf291"><mml:mrow><mml:msup><mml:mi mathvariant="bold">QQ</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:math></inline-formula>) to generate a valid code<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This construction guarantees that <inline-formula><mml:math id="inf292"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">R</mml:mi><mml:mn>1</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf293"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">R</mml:mi><mml:mn>2</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. We plot the tuning curves for the first three neurons. The target function in the first experiment is <inline-formula><mml:math id="inf294"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>0.6</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, while the second experiment used <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>6</mml:mn><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2"><title>Theory of generalization</title><p>Recent work has established analytic results that predict the average case generalization error for kernel regression<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the generalization error for a certain sample <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of size <inline-formula><mml:math id="inf298"><mml:mi>P</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the kernel regression solution for <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (Appendix Convergence of the delta-rule without weight decay) (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). The typical or average case error <inline-formula><mml:math id="inf301"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> is obtained by averaging over all possible datasets of size <inline-formula><mml:math id="inf302"><mml:mi>P</mml:mi></mml:math></inline-formula>. This average case generalization error is determined solely by the decomposition of the target function <inline-formula><mml:math id="inf303"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> along the eigenbasis of the kernel and the eigenspectrum of the kernel. This continuous diagonalization again takes the form (Appendix Singular value decomposition of continuous population responses) (<xref ref-type="bibr" rid="bib95">Rasmussen and Williams, 2005</xref>)<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Our theory is also applicable to discrete stimuli if <inline-formula><mml:math id="inf304"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a Dirac measure as we describe in (Appendix Discrete stimulus spaces: finding eigenfunctions with matrix eigendecomposition). Since the eigenfunctions form a complete set of square integrable functions (<xref ref-type="bibr" rid="bib95">Rasmussen and Williams, 2005</xref>), we expand both the target function <inline-formula><mml:math id="inf305"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the learned function <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in this basis <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf308"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> are understood to be functions of the dataset <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The eigenfunctions are orthonormal <inline-formula><mml:math id="inf310"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which implies that the generalization error for any set of coefficients <inline-formula><mml:math id="inf311"><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>We now introduce the equivalent training error, or empirical loss, written directly in terms of eigenfunction coefficients <inline-formula><mml:math id="inf312"><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, which depends on the random dataset <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:msubsup><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>This loss function is minimized by delta rule updates with weight decay constant <inline-formula><mml:math id="inf314"><mml:mi>λ</mml:mi></mml:math></inline-formula>. It is straightforward to verify that the <inline-formula><mml:math id="inf315"><mml:mi>H</mml:mi></mml:math></inline-formula>-minimizing coefficients are <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, giving the learned function <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where the vectors <inline-formula><mml:math id="inf318"><mml:mi mathvariant="bold">k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf319"><mml:mi mathvariant="bold">y</mml:mi></mml:math></inline-formula> have entries <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for each training stimulus <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> kernel gram matrix <inline-formula><mml:math id="inf324"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> has entries <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The <inline-formula><mml:math id="inf326"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> limit of the minimizer of <inline-formula><mml:math id="inf327"><mml:mi>H</mml:mi></mml:math></inline-formula> coincides with kernel interpolation. This allows us to characterize generalization without reference to learned readout weights <inline-formula><mml:math id="inf328"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>. The generalization error for this optimal function is<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We note that the dependence on the randomly sampled dataset <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> only appears through the matrix <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus to compute the <italic>typical</italic> generalization error we need to average this matrix over realizations of datasets, <italic>i.e</italic>.<inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. There are multiple strategies to perform such an average and we will study one here based on a partial differential equation which was introduced in <xref ref-type="bibr" rid="bib109">Sollich, 1998</xref>; <xref ref-type="bibr" rid="bib110">Sollich, 2002</xref> and studied further in <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>. We describe in detail one method for performing such an average in Appendix Computation of learning curves. After this computation, we find that the generalization error can be approximated at large <inline-formula><mml:math id="inf332"><mml:mi>P</mml:mi></mml:math></inline-formula> as<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mstyle><mml:mpadded width="+5pt"><mml:mstyle displaystyle="false"><mml:mfrac><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mstyle><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf333"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, giving the desired result. We note that (11) defines the function <inline-formula><mml:math id="inf334"><mml:mi>κ</mml:mi></mml:math></inline-formula> implicitly in terms of the sample size <inline-formula><mml:math id="inf335"><mml:mi>P</mml:mi></mml:math></inline-formula>. Taking <inline-formula><mml:math id="inf336"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> gives the generalization error of the minimum norm interpolant, which desribes the generalization error of the solution. This result was recently reproduced using the replica method from statistical mechanics in an asymptotic limit where the number of neurons and samples are large (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). Other recent works have verified our theoretical expressions on a variety of kernels and datasets (<xref ref-type="bibr" rid="bib68">Loureiro et al., 2021b</xref>; <xref ref-type="bibr" rid="bib105">Simon et al., 2021</xref>).</p><p>Additional intuition for the spectral bias phenomenon can be gained from the expected learned function <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is the average readout prediction over possible datasets <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The function <inline-formula><mml:math id="inf339"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined implicitly as <inline-formula><mml:math id="inf340"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and decreases with <inline-formula><mml:math id="inf341"><mml:mi>P</mml:mi></mml:math></inline-formula> from <inline-formula><mml:math id="inf342"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to its asymptotic value <inline-formula><mml:math id="inf343"><mml:mrow><mml:mrow><mml:msub><mml:mo>lim</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula>. The coefficient for the <inline-formula><mml:math id="inf344"><mml:mi>k</mml:mi></mml:math></inline-formula>-th eigenfunction <inline-formula><mml:math id="inf345"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> approaches the true coefficient <italic>v</italic><sub><italic>k</italic></sub> as <inline-formula><mml:math id="inf346"><mml:mrow><mml:mi>P</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="inf347"><mml:mi>k</mml:mi></mml:math></inline-formula>-th eigenfunction <inline-formula><mml:math id="inf348"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is effectively learned when <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>≫</mml:mo><mml:mfrac><mml:mi>κ</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. For large eigenvalues <inline-formula><mml:math id="inf350"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, fewer samples <inline-formula><mml:math id="inf351"><mml:mi>P</mml:mi></mml:math></inline-formula> are needed to satisfy this condition, while small eigenvalue modes will require more samples.</p></sec><sec id="s4-3"><title>RNN experiment</title><p>For the simulations in <xref ref-type="fig" rid="fig7">Figure 7</xref>, we integrated a rate-based recurrent network model with <inline-formula><mml:math id="inf352"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>6000</mml:mn></mml:mrow></mml:math></inline-formula> neurons, time constant <inline-formula><mml:math id="inf353"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and gain <inline-formula><mml:math id="inf354"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula>. Each of the <inline-formula><mml:math id="inf355"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> randomly chosen angles <inline-formula><mml:math id="inf356"><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> generates a trajectory over <inline-formula><mml:math id="inf357"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> equally spaced points in <inline-formula><mml:math id="inf358"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The two dimensional input sequence is simply <inline-formula><mml:math id="inf359"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Target function for a delay <inline-formula><mml:math id="inf360"><mml:mi>d</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf361"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>d</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> which is nonzero for times <inline-formula><mml:math id="inf362"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In each simulation, the activity in the network is initialized to <inline-formula><mml:math id="inf363"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></inline-formula>. The kernel gram matrix <inline-formula><mml:math id="inf364"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is computed by taking inner products of the time varying code at for different inputs <inline-formula><mml:math id="inf365"><mml:msup><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> and at different times. Learning curves represent the generalization error obtained by randomly sampling <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> time points from the <inline-formula><mml:math id="inf367"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> total time points generated in the simulation process and training readout weights <inline-formula><mml:math id="inf368"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula> to convergence with gradient descent.</p></sec><sec id="s4-4"><title>Data analysis</title><sec id="s4-4-1"><title>Data source and processing</title><p>Mouse V1 neuron responses to orientation gratings were obtained from a publicly available dataset (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>). Two-photon calcium microscopy fluorescence traces were deconvolved into spike trains and spikes were counted for each stimulus, as described in <xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>. The presented grating angles were distributed uniformly over <inline-formula><mml:math id="inf369"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> radians. Data pre-processing, which included z-scoring against the mean and standard deviation of null stimulus responses, utilized the provided code for this experiment, which also publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-et-al-2019">https://github.com/MouseLand/stringer-et-al-2019</ext-link> (<xref ref-type="bibr" rid="bib116">Stringer, 2019</xref>). This preprocessing technique was used in all Figures in the paper. To reduce corruption of the estimated kernel from neural noise (trial-to-trial variability), we first trial average responses, binning the grating stimuli oriented at different angles <inline-formula><mml:math id="inf370"><mml:mi>θ</mml:mi></mml:math></inline-formula> into a collection of 100 bins over the interval from <inline-formula><mml:math id="inf371"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and averaging over all of the available responses from each bin. Since grating angles were sampled uniformly, there is a roughly even distribution of about 45 responses in each bin. After trial averaging, SVD was performed on the response matrix <inline-formula><mml:math id="inf372"><mml:mi mathvariant="bold">R</mml:mi></mml:math></inline-formula>, generating the eigenspectrum and kernel eigenfunctions as illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>. <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig8">8</xref>, all used this data anytime responses to grating stimuli were mentioned.</p><p>In <xref ref-type="fig" rid="fig3">Figures 3D</xref>, <xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig8">8C</xref>, the responses of mouse V1 neurons to ImageNet images (<xref ref-type="bibr" rid="bib28">Deng et al., 2009</xref>) were obtained from a different publicly available dataset (<xref ref-type="bibr" rid="bib113">Stringer et al., 2018a</xref>). The images were taken from 15 different classes from the Imagenet dataset with ethological relevance to mice (birds, cats, flowers, hamsters, holes, insects, mice, mushrooms, nests, pellets, snakes, wildcats, other animals, other natural, other man made). In the experiment in <xref ref-type="fig" rid="fig3">Figure 3D</xref> we use all images from the mice and birds category for which responses were recorded. The preprocessing code and image category information were obtained from the publicly available code base at <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-pachitariu-et-al-2018b">https://github.com/MouseLand/stringer-pachitariu-et-al-2018b</ext-link> (<xref ref-type="bibr" rid="bib115">Stringer, 2018c</xref>). Again, spike counts were obtained from deconvolved and z-scored calcium fluorescence traces. In the reconstruction experiment shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> we use the entire set of images for which neural responses were recorded.</p></sec><sec id="s4-4-2"><title>Generating RROS codes</title><p>In <xref ref-type="fig" rid="fig8">Figure 8</xref>, the randomly rotated codes are generated by sampling a matrix <inline-formula><mml:math id="inf373"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> from the Haar measure on the set of <inline-formula><mml:math id="inf374"><mml:mi>N</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf375"><mml:mi>N</mml:mi></mml:math></inline-formula> orthogonal matrices (<xref ref-type="bibr" rid="bib6">Anderson et al., 1987</xref>), and chosing a <inline-formula><mml:math id="inf376"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:math></inline-formula> by solving the following optimization problem:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:munder></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mpadded width="+5pt"><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mtext>s.t.</mml:mtext><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>Qr</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+5pt"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mpadded></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>≥</mml:mo><mml:mpadded width="+5pt"><mml:mn>0</mml:mn></mml:mpadded></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo rspace="7.5pt" stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which minimizes the total spike count subject to the kernel and nonnegativity of firing rates. The solution to this problem is given by <inline-formula><mml:math id="inf377"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">Qr</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-4-3"><title>Comparing sparsity of population codes</title><p>To explore the metabolic cost among the set of codes with the same inductive biases, we estimate the distribution of average spike counts of codes with the same inner product kernel as the biological code. These codes are generated in the form <inline-formula><mml:math id="inf378"><mml:mrow><mml:msup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Qr</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf379"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:math></inline-formula> solves the optimization problem<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>To quantify the distribution of such codes, we randomly sample <inline-formula><mml:math id="inf380"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> from the invariant (Haar) measure for <inline-formula><mml:math id="inf381"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> orthogonal matrices and compute the optimal <inline-formula><mml:math id="inf382"><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:math></inline-formula> as described above. This generates the aqua colored distribution in <xref ref-type="fig" rid="fig8">Figure 8B and C</xref>.</p><p>We also attempt to characterize the most efficient code with the same inner product kernel<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Since this optimization problem is non-convex in <inline-formula><mml:math id="inf383"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, there is no theoretical guarantee that minima are unique. Nonetheless, we attempt to optimize the code by starting <inline-formula><mml:math id="inf384"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> at the identity matrix and conduct gradient descent over orthogonal matrices (<xref ref-type="bibr" rid="bib90">Plumbley, 2004</xref>). Such updates take the form<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mpadded></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mfrac><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf385"><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the matrix exponential. To make the loss function differentiable, we incorporate the non-negativity constraint with a soft-minimum:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mtext>softmin</mml:mtext><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mtext>softmin</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a normalizing constant and <inline-formula><mml:math id="inf387"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">q</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the <inline-formula><mml:math id="inf388"><mml:mrow><mml:mi>β</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> limit, this cost function converges to the exact optimization problem with non-negativity constraint. Finite <inline-formula><mml:math id="inf389"><mml:mi>β</mml:mi></mml:math></inline-formula>, however, allows learning with gradient descent. Gradients are computed with automatic differentiation in JAX (<xref ref-type="bibr" rid="bib18">Bradbury et al., 2018</xref>). This optimization routine is run until convergence and the optimal value is plotted as dashed red lines labeled ‘opt’. in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p><p>We show that our result is robust to different pre-processing techniques and to imposing bounds on neural firing rates in the <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>. To demonstrate that our result is not an artifact of z-scoring the deconvolved signals against the spontaneous baseline activity level, we also conduct the random rotation experiment on the raw deconvolved signals. In addition, we show that imposing realistic constraints on the upper bound of the each neuron’s responses does not change our findings. We used a subset of <inline-formula><mml:math id="inf390"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> neurons and computed random rotations. However, we only accepted a code as valid if it’s maximum value was less than some upper bound <italic>u</italic><sub><italic>b</italic></sub>. Subsets of <inline-formula><mml:math id="inf391"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> neurons in the biological code achieve maxima in the range between 3.2 and 4.7. We performed this experiment for <inline-formula><mml:math id="inf392"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> so that the artificial codes would have maxima that lie in the same range as the biological code.</p></sec><sec id="s4-4-4"><title>Lifetime and population sparseness</title><p>We compute two more refined measures of sparseness in a population code. For each neuron <inline-formula><mml:math id="inf393"><mml:mi>i</mml:mi></mml:math></inline-formula> we compute the lifetime sparseness <inline-formula><mml:math id="inf394"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (also known as selectivity) and for each stimulus <inline-formula><mml:math id="inf395"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> we compute the population sparseness <inline-formula><mml:math id="inf396"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> which are defined as the following two ratios (<xref ref-type="bibr" rid="bib125">Willmore and Tolhurst, 2001</xref>; <xref ref-type="bibr" rid="bib63">Lehky et al., 2005</xref>; <xref ref-type="bibr" rid="bib120">Treves and Rolls, 1991</xref>; <xref ref-type="bibr" rid="bib84">Pehlevan and Sompolinsky, 2014</xref>)<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The normalization factors ensure that these quantities lie in the interval between <inline-formula><mml:math id="inf397"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Intuitively, lifetime sparseness quantifies the variability of each neuron’s responses over the full set of stimuli, whereas population sparseness quantifies the variability of responses in the code for a given stimulus <inline-formula><mml:math id="inf398"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-4-5"><title>Fitting a Gabor model to mouse V1 kernel</title><p>Under the assumption of translation symmetry in the kernel <inline-formula><mml:math id="inf399"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we averaged the elements of the over rows of the empirical mouse V1 kernel (<xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>)<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where angular addition is taken mod <inline-formula><mml:math id="inf400"><mml:mi>π</mml:mi></mml:math></inline-formula>. This generates the black dots in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. We aimed to fit a threshold-power law nonlinearity of the form <inline-formula><mml:math id="inf401"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>q</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> to the kernel. Based on the Gabor model discussed above, we parameterized tuning curves as<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf402"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the preferred angle of the <inline-formula><mml:math id="inf403"><mml:mi>i</mml:mi></mml:math></inline-formula>-th neuron’s tuning curve. Rather than attempting to perform a fit of <inline-formula><mml:math id="inf404"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> of this form to the responses of each of the <inline-formula><mml:math id="inf405"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>-k neurons, we instead simply attempt to fit to the population kernel by optimizing over <inline-formula><mml:math id="inf406"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> under the assumption of uniform tiling of <inline-formula><mml:math id="inf407"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. However, we noticed that two of these variables <inline-formula><mml:math id="inf408"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> are constrained by the sparsity level of the code. If each neuron, on average, fires for only a fraction <inline-formula><mml:math id="inf409"><mml:mi>f</mml:mi></mml:math></inline-formula> of the uniformly sampled angles <inline-formula><mml:math id="inf410"><mml:mi>θ</mml:mi></mml:math></inline-formula>, then the following relationship holds between <inline-formula><mml:math id="inf411"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> and<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Calculation of the coding level <inline-formula><mml:math id="inf412"><mml:mi>f</mml:mi></mml:math></inline-formula> for the recorded responses allowed us to infer <inline-formula><mml:math id="inf413"><mml:mi>a</mml:mi></mml:math></inline-formula> from <inline-formula><mml:math id="inf414"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> during optimization. This reduced the free parameter set to <inline-formula><mml:math id="inf415"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We then solve the following optimization problem<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where integration over <inline-formula><mml:math id="inf416"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is performed numerically. Using the Scipy Trust-Region constrained optimizer, we found <inline-formula><mml:math id="inf417"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1.7</mml:mn><mml:mo>,</mml:mo><mml:mn>5.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which we use as the fit parameters in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></sec></sec><sec id="s4-5"><title>Lead contact</title><p>Requests for information should be directed to the lead contact, Cengiz Pehlevan (cpehlevan@seas.harvard.edu).</p></sec><sec id="s4-6"><title>Data and code availability</title><p>Mouse V1 neuron responses to orientation gratings and preprocessing code were obtained from a publicly available dataset: <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-et-al-2019">https://github.com/MouseLand/stringer-et-al-2019</ext-link>, (<xref ref-type="bibr" rid="bib117">Stringer et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Pachitariu et al., 2019</xref>).</p><p>Responses to ImageNet images and preprocessing code were obtained from another publicly available dataset, <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-pachitariu-et-al-2018b">https://github.com/MouseLand/stringer-pachitariu-et-al-2018b</ext-link> (<xref ref-type="bibr" rid="bib113">Stringer et al., 2018a</xref>).</p><p>The code generated by the authors for this paper is also available <ext-link ext-link-type="uri" xlink:href="https://github.com/Pehlevan-Group/sample_efficient_pop_codes">https://github.com/Pehlevan-Group/sample_efficient_pop_codes</ext-link> (<xref ref-type="bibr" rid="bib86">Pehlevan-Group, 2022</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-78606-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Mouse V1 neuron responses to orientation gratings and preprocessing code were obtained from a publicly available dataset: <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-et-al-2019">https://github.com/MouseLand/stringer-et-al-2019</ext-link>. Responses to ImageNet images and preprocessing code were obtained from another publicly available dataset, <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-pachitariu-et-al-2018b">https://github.com/MouseLand/stringer-pachitariu-et-al-2018b</ext-link>. The code generated by the authors for this paper is also available <ext-link ext-link-type="uri" xlink:href="https://github.com/Pehlevan-Group/sample_efficient_pop_codes">https://github.com/Pehlevan-Group/sample_efficient_pop_codes</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:4a74885520a5f73b8fed3b1e9e6c0be94cfec2af;origin=https://github.com/Pehlevan-Group/sample_efficient_pop_codes;visit=swh:1:snp:05328e6657d451162c6077710ce4a81d80a5de7c;anchor=swh:1:rev:6cd4f0fe7043ae214dd682a9dc035a497ffa2d61">swh:1:rev:6cd4f0fe7043ae214dd682a9dc035a497ffa2d61</ext-link>).</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Carsen</surname><given-names>S</given-names></name><name><surname>Marius</surname><given-names>P</given-names></name><name><surname>Nicholas</surname><given-names>S</given-names></name><name><surname>Matteo</surname><given-names>C</given-names></name><name><surname>Kenneth</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Recordings of ten thousand neurons in visual cortex in response to 2,800 natural images</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.25378/janelia.6845348.v4</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Marius</surname><given-names>P</given-names></name><name><surname>Michalis</surname><given-names>M</given-names></name><name><surname>Carsen</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Recordings of ~20,000 neurons from V1 in response to oriented stimuli</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.25378/janelia.8279387.v3</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Jacob Zavatone-Veth and Abdulkadir Canatar for useful comments and discussions about this manuscript. BB acknowledges the support of the NSF-Simons Center for Mathematical and Statistical Analysis of Biology at Harvard (award #1764269) and the Harvard Q-Bio Initiative. CP and BB were also supported by NSF grant DMS-2134157.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Chklovskii</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Do retinal ganglion cells project natural scenes to their principal subspace and whiten them?</article-title><conf-name>2016 50th Asilomar Conference on Signals, Systems and Computers</conf-name><pub-id pub-id-type="doi">10.1109/ACSSC.2016.7869658</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adab</surname><given-names>HZ</given-names></name><name><surname>Popivanov</surname><given-names>ID</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Perceptual learning of simple stimuli modifies stimulus representations in posterior inferior temporal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>2187</fpage><lpage>2200</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00641</pub-id><pub-id pub-id-type="pmid">24702452</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>2</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1364/josaa.2.000284</pub-id><pub-id pub-id-type="pmid">3973762</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname><given-names>M</given-names></name><name><surname>Hochstein</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The reverse hierarchy theory of visual perceptual learning</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id><pub-id pub-id-type="pmid">15450510</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ames</surname><given-names>KC</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simultaneous motor preparation and execution in a last-moment reach correction task</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-10772-2</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>TW</given-names></name><name><surname>Olkin</surname><given-names>I</given-names></name><name><surname>Underhill</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Generation of random orthogonal matrices</article-title><source>SIAM Journal on Scientific and Statistical Computing</source><volume>8</volume><fpage>625</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1137/0908055</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Redlich</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>What does the retina know about natural scenes?</article-title><source>Neural Computation</source><volume>4</volume><fpage>196</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.2.196</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><volume>61</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1037/h0054663</pub-id><pub-id pub-id-type="pmid">13167245</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1961">1961</year><source>Possible Principles Underlying the Transformation of Sensory Messages</source><publisher-name>Cambridge University</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname><given-names>PL</given-names></name><name><surname>Long</surname><given-names>PM</given-names></name><name><surname>Lugosi</surname><given-names>G</given-names></name><name><surname>Tsigler</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Benign overfitting in linear regression</article-title><source>PNAS</source><volume>117</volume><fpage>30063</fpage><lpage>30070</lpage><pub-id pub-id-type="doi">10.1073/pnas.1907378117</pub-id><pub-id pub-id-type="pmid">32332161</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bathellier</surname><given-names>B</given-names></name><name><surname>Buhl</surname><given-names>DL</given-names></name><name><surname>Accolla</surname><given-names>R</given-names></name><name><surname>Carleton</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic ensemble odor coding in the mammalian olfactory bulb: sensory information at different timescales</article-title><source>Neuron</source><volume>57</volume><fpage>586</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.011</pub-id><pub-id pub-id-type="pmid">18304487</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belkin</surname><given-names>M</given-names></name><name><surname>Hsu</surname><given-names>D</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Mandal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reconciling modern machine-learning practice and the classical bias-variance trade-off</article-title><source>PNAS</source><volume>116</volume><fpage>15849</fpage><lpage>15854</lpage><pub-id pub-id-type="doi">10.1073/pnas.1903070116</pub-id><pub-id pub-id-type="pmid">31341078</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Histed</surname><given-names>MH</given-names></name><name><surname>Yurgenson</surname><given-names>S</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Local diversity and fine-scale organization of receptive fields in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>18506</fpage><lpage>18521</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2974-11.2011</pub-id><pub-id pub-id-type="pmid">22171051</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Canatar</surname><given-names>A</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spectrum dependent learning curves in kernel regression and wide neural networks</article-title><conf-name>Proceedings of the 37th International Conference on Machine Learning of Proceedings of Machine Learning Research</conf-name><fpage>1024</fpage><lpage>1034</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2210.02157">https://arxiv.org/abs/2210.02157</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks</article-title><conf-name>Advances In Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Frostig</surname><given-names>R</given-names></name><name><surname>Hawkins</surname><given-names>P</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Leary</surname><given-names>C</given-names></name><name><surname>Maclaurin</surname><given-names>D</given-names></name><name><surname>Necula</surname><given-names>G</given-names></name><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Wanderman-Milne</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>JAX: composable transformations of python+numpy programs</data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/google/jax">https://github.com/google/jax</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Pinto</surname><given-names>N</given-names></name><name><surname>Ardila</surname><given-names>D</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate it cortex for core visual object recognition</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canatar</surname><given-names>A</given-names></name><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2914</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23103-1</pub-id><pub-id pub-id-type="pmid">34006842</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carey</surname><given-names>S</given-names></name><name><surname>Bartlett</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>Acquiring a Single New Word</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalk</surname><given-names>M</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Toward a unified theory of efficient, predictive, and sparse coding</article-title><source>PNAS</source><volume>115</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1073/pnas.1711114115</pub-id><pub-id pub-id-type="pmid">29259111</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapin</surname><given-names>JK</given-names></name><name><surname>Nicolelis</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Principal component analysis of neuronal ensemble activity reveals multidimensional somatosensory representations</article-title><source>Journal of Neuroscience Methods</source><volume>94</volume><fpage>121</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/s0165-0270(99)00130-2</pub-id><pub-id pub-id-type="pmid">10638820</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Measuring and interpreting neuronal correlations</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>811</fpage><lpage>819</lpage><pub-id pub-id-type="doi">10.1038/nn.2842</pub-id><pub-id pub-id-type="pmid">21709677</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>H</given-names></name><name><surname>Loureiro</surname><given-names>B</given-names></name><name><surname>Krzakala</surname><given-names>F</given-names></name><name><surname>Zdeborová</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Error Rates for Kernel Classification under Source and Capacity Conditions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2201.12655">https://arxiv.org/abs/2201.12655</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1500</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1038/nn.3776</pub-id><pub-id pub-id-type="pmid">25151264</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>LJ</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imagenet: A large-scale hierarchical image database</article-title><conf-name>In 2009 IEEE conference on computer vision and pattern recognition</conf-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Keenan</surname><given-names>T</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>S</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Barber</surname><given-names>C</given-names></name><name><surname>Berbesque</surname><given-names>N</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Bowles</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>SD</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Edwards</surname><given-names>M</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gilbert</surname><given-names>TL</given-names></name><name><surname>Griffin</surname><given-names>F</given-names></name><name><surname>Hargrave</surname><given-names>P</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Jewell</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>N</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>F</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Sjoquist</surname><given-names>N</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Valenza</surname><given-names>R</given-names></name><name><surname>White</surname><given-names>C</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id><pub-id pub-id-type="pmid">31844315</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>J</given-names></name><name><surname>Ohana</surname><given-names>R</given-names></name><name><surname>Rafayelyan</surname><given-names>M</given-names></name><name><surname>Krzakala</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reservoir computing meets recurrent kernels and structured transforms</article-title><conf-name>NeurIPS Proceedings</conf-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Representation is representation of similarities</article-title><source>The Behavioral and Brain Sciences</source><volume>21</volume><fpage>449</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1017/s0140525x98001253</pub-id><pub-id pub-id-type="pmid">10097019</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Farrell</surname><given-names>M</given-names></name><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Trivedi</surname><given-names>S</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Capacity of Group-Invariant Linear Readouts from Equivariant Representations: How Many Objects Can Be Linearly Classified under All Possible Views?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.07472">https://arxiv.org/abs/2110.07472</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.2110.07472</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>T</given-names></name><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rich and Lazy Learning of Task Representations in Brains and Neural Networks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.23.441128</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furmanski</surname><given-names>CS</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Engel</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Learning strengthens the response of primary visual cortex to simple patterns</article-title><source>Current Biology</source><volume>14</volume><fpage>573</fpage><lpage>578</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.03.032</pub-id><pub-id pub-id-type="pmid">15062097</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Naufel</surname><given-names>SN</given-names></name><name><surname>Ethier</surname><given-names>C</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical population activity within a preserved neural manifold underlies multiple motor behaviors</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4233</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06560-z</pub-id><pub-id pub-id-type="pmid">30315158</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.04.003</pub-id><pub-id pub-id-type="pmid">25932978</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Theory of Multineuronal Dimensionality, Dynamics and Measurement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/214262</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghose</surname><given-names>GM</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Physiological correlates of perceptual learning in monkey V1 and V2</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>1867</fpage><lpage>1888</lpage><pub-id pub-id-type="doi">10.1152/jn.00690.2001</pub-id><pub-id pub-id-type="pmid">11929908</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Early perceptual learning</article-title><source>PNAS</source><volume>91</volume><fpage>1195</fpage><lpage>1197</lpage><pub-id pub-id-type="doi">10.1073/pnas.91.4.1195</pub-id><pub-id pub-id-type="pmid">8108386</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girosi</surname><given-names>F</given-names></name><name><surname>Jones</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Regularization theory and neural networks architectures</article-title><source>Neural Computation</source><volume>7</volume><fpage>219</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.2.219</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goltstein</surname><given-names>PM</given-names></name><name><surname>Reinert</surname><given-names>S</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mouse visual cortex areas represent perceptual and semantic features of learned visual categories</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1441</fpage><lpage>1451</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00914-5</pub-id><pub-id pub-id-type="pmid">34545249</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haft</surname><given-names>M</given-names></name><name><surname>van Hemmen</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Theory and implementation of infomax filters for the retina</article-title><source>Network</source><volume>9</volume><fpage>39</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_9_1_003</pub-id><pub-id pub-id-type="pmid">9861978</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>van Vreeswijk</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>How noise contributes to contrast invariance of orientation tuning in cat visual cortex</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5118</fpage><lpage>5128</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-12-05118.2002</pub-id><pub-id pub-id-type="pmid">12077207</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Additive Function Approximation in the Brain</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.02603">https://arxiv.org/abs/1909.02603</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1909.02603</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Goldstein</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Direct fit to nature: an evolutionary perspective on biological and artificial neural networks</article-title><source>Neuron</source><volume>105</volume><fpage>416</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.002</pub-id><pub-id pub-id-type="pmid">32027833</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Montanari</surname><given-names>A</given-names></name><name><surname>Rosset</surname><given-names>S</given-names></name><name><surname>Tibshirani</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Surprises in High-Dimensional Ridgeless Least Squares Interpolation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.08560">https://arxiv.org/abs/1903.08560</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1903.08560</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>J</given-names></name><name><surname>Krogh</surname><given-names>A</given-names></name><name><surname>Palmer</surname><given-names>RG</given-names></name><name><surname>Horner</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Introduction to the theory of neural computation</article-title><source>Physics Today</source><volume>44</volume><elocation-id>70</elocation-id><pub-id pub-id-type="doi">10.1063/1.2810360</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hromádka</surname><given-names>T</given-names></name><name><surname>Deweese</surname><given-names>MR</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e16</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060016</pub-id><pub-id pub-id-type="pmid">18232737</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Rao</surname><given-names>RPN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predictive coding</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>2</volume><fpage>580</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1002/wcs.142</pub-id><pub-id pub-id-type="pmid">26302308</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hume</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>An Enquiry Concerning Human Understanding</source><publisher-name>Oxford University Press</publisher-name><fpage>1711</fpage><lpage>1776</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jabri</surname><given-names>M</given-names></name><name><surname>Flower</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Weight perturbation: an optimal architecture and learning technique for analog vlsi feedforward and recurrent multilayer networks</article-title><source>IEEE Transactions on Neural Networks</source><volume>3</volume><fpage>154</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1109/72.105429</pub-id><pub-id pub-id-type="pmid">18276417</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jacot</surname><given-names>A</given-names></name><name><surname>Gabriel</surname><given-names>F</given-names></name><name><surname>Hongler</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Neural tangent kernel: convergence and generalization in neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Grauman</surname><given-names>K</given-names></name><name><surname>Cesa-Bianchi</surname><given-names>N</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kalimeris</surname><given-names>D</given-names></name><name><surname>Kaplun</surname><given-names>G</given-names></name><name><surname>Nakkiran</surname><given-names>P</given-names></name><name><surname>Edelman</surname><given-names>BL</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Barak</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>SGD on neural networks learns functions of increasing complexity</article-title><conf-name>In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</conf-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kato</surname><given-names>S</given-names></name><name><surname>Kaplan</surname><given-names>HS</given-names></name><name><surname>Schrödel</surname><given-names>T</given-names></name><name><surname>Skora</surname><given-names>S</given-names></name><name><surname>Lindsay</surname><given-names>TH</given-names></name><name><surname>Yemini</surname><given-names>E</given-names></name><name><surname>Lockery</surname><given-names>S</given-names></name><name><surname>Zimmer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Global brain dynamics embed the motor command sequence of <italic>Caenorhabditis elegans</italic></article-title><source>Cell</source><volume>163</volume><fpage>656</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.09.034</pub-id><pub-id pub-id-type="pmid">26478179</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Similarity of Neural Network Representations Revisited</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.00414">https://arxiv.org/abs/1905.00414</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1905.00414</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>HW</given-names></name><name><surname>Tucker</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Nonlinear programming</chapter-title><person-group person-group-type="editor"><name><surname>Kuhn</surname><given-names>HW</given-names></name></person-group><source>Traces and Emergence of Nonlinear Programming</source><publisher-name>Springer</publisher-name><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1007/978-3-0348-0439-4</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laakso</surname><given-names>A</given-names></name><name><surname>Cottrell</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Content and cluster analysis: assessing representational similarity in neural systems</article-title><source>Philosophical Psychology</source><volume>13</volume><fpage>47</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1080/09515080050002726</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Ullman</surname><given-names>TD</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Building machines that learn and think like people</article-title><source>The Behavioral and Brain Sciences</source><volume>40</volume><elocation-id>e253</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X16001837</pub-id><pub-id pub-id-type="pmid">27881212</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Law</surname><given-names>CT</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates of perceptual learning in a sensory-motor, but not a sensory, cortical area</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>505</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1038/nn2070</pub-id><pub-id pub-id-type="pmid">18327253</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Sohl-dickstein</surname><given-names>J</given-names></name><name><surname>Pennington</surname><given-names>J</given-names></name><name><surname>Novak</surname><given-names>R</given-names></name><name><surname>Schoenholz</surname><given-names>S</given-names></name><name><surname>Bahri</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep neural networks as gaussian processes</article-title><conf-name>In International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehky</surname><given-names>SR</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Selectivity and sparseness in the responses of striate complex cells</article-title><source>Vision Research</source><volume>45</volume><fpage>57</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.07.021</pub-id><pub-id pub-id-type="pmid">15571738</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>D</given-names></name><name><surname>Du</surname><given-names>SS</given-names></name><name><surname>Hu</surname><given-names>W</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Arora</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Enhanced Convolutional Neural Tangent Kernels</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1911.00809">https://arxiv.org/abs/1911.00809</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1911.00809</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Cownden</surname><given-names>D</given-names></name><name><surname>Tweed</surname><given-names>DB</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal degrees of synaptic connectivity</article-title><source>Neuron</source><volume>93</volume><fpage>1153</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id><pub-id pub-id-type="pmid">28215558</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loureiro</surname><given-names>B</given-names></name><name><surname>Gerbelot</surname><given-names>C</given-names></name><name><surname>Cui</surname><given-names>H</given-names></name><name><surname>Goldt</surname><given-names>S</given-names></name><name><surname>Krzakala</surname><given-names>F</given-names></name><name><surname>Mezard</surname><given-names>M</given-names></name><name><surname>Zdeborová</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Learning curves of generic features maps for realistic datasets with a teacher-student model</article-title><source>Advances in Neural Information Processing Systems</source><volume>34</volume><fpage>18137</fpage><lpage>18151</lpage><pub-id pub-id-type="doi">10.1088/1742-5468/ac9825</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Loureiro</surname><given-names>B</given-names></name><name><surname>Gerbelot</surname><given-names>C</given-names></name><name><surname>Cui</surname><given-names>H</given-names></name><name><surname>Goldt</surname><given-names>S</given-names></name><name><surname>Krzakala</surname><given-names>F</given-names></name><name><surname>Mézard</surname><given-names>M</given-names></name><name><surname>Zdeborová</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Capturing the Learning Curves of Generic Features Maps for Realistic Data Sets with a Teacher-Student Model</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2102.08127</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Hiratani</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Evolution of Neural Activity in Circuits Bridging Sensory and Abstract Knowledge</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.01.29.478317</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>S</given-names></name><name><surname>Montanari</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The generalization error of random features regression: precise asymptotics and the double descent curve</article-title><source>Communications on Pure and Applied Mathematics</source><volume>75</volume><fpage>667</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1002/cpa.22008</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>S</given-names></name><name><surname>Misiakiewicz</surname><given-names>T</given-names></name><name><surname>Montanari</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning with Invariances in Random Features and Kernel Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2102.13219">https://arxiv.org/abs/2102.13219</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.2102.13219</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>F</given-names></name><name><surname>Dang-Nhu</surname><given-names>R</given-names></name><name><surname>Steger</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Adaptive tuning curve widths improve sample efficient learning</article-title><source>Frontiers in Computational Neuroscience</source><volume>14</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2020.00012</pub-id><pub-id pub-id-type="pmid">32132915</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1909">1909</year><article-title>XVI. Functions of positive and negative type, and their connection the theory of integral equations</article-title><source>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of A Mathematical or Physical Character</source><volume>209</volume><fpage>415</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1098/rsta.1909.0016</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Troyer</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural noise can explain expansive, power-law nonlinearities in neural response functions</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>653</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1152/jn.00425.2001</pub-id><pub-id pub-id-type="pmid">11826034</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>J</given-names></name><name><surname>Sokol</surname><given-names>P</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Harris</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On 1/n neural representation and robustness</article-title><conf-name>Advances in Neural Information Processing Systems 33</conf-name></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Bayesian Learning for Neural Networks</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Highly selective receptive fields in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niven</surname><given-names>JE</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Energy limitation as a selective pressure on the evolution of sensory systems</article-title><source>The Journal of Experimental Biology</source><volume>211</volume><fpage>1792</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1242/jeb.017574</pub-id><pub-id pub-id-type="pmid">18490395</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Effects of perceptual learning in visual backward masking on the responses of macaque inferior temporal neurons</article-title><source>Neuroscience</source><volume>145</volume><fpage>775</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.12.058</pub-id><pub-id pub-id-type="pmid">17293053</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robustness of spike deconvolution for neuronal calcium imaging</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7976</fpage><lpage>7985</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3339-17.2018</pub-id><pub-id pub-id-type="pmid">30082416</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Recordings of 20,000 Neurons from V1 in Response to Oriented Stimuli</source><publisher-name>American Physiological Society</publisher-name></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pehlevan</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Selectivity and sparseness in randomly connected balanced networks</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e89992</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0089992</pub-id><pub-id pub-id-type="pmid">24587172</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pehlevan</surname><given-names>C</given-names></name><name><surname>Sengupta</surname><given-names>AM</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Why do similarity matching objectives lead to hebbian/anti-hebbian networks?</article-title><source>Neural Computation</source><volume>30</volume><fpage>84</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01018</pub-id><pub-id pub-id-type="pmid">28957017</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Pehlevan-Group</collab></person-group><year iso-8601-date="2022">2022</year><data-title>Sample_efficient_pop_codes</data-title><version designator="6cd4f0f">6cd4f0f</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Pehlevan-Group/sample_efficient_pop_codes">https://github.com/Pehlevan-Group/sample_efficient_pop_codes</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>MF</given-names></name><name><surname>Abbey</surname><given-names>CK</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The surprisingly high human efficiency at learning to recognize faces</article-title><source>Vision Research</source><volume>49</volume><fpage>301</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.10.014</pub-id><pub-id pub-id-type="pmid">19000918</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decorrelation and efficient coding by retinal ganglion cells</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>628</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nn.3064</pub-id><pub-id pub-id-type="pmid">22406548</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pleger</surname><given-names>B</given-names></name><name><surname>Foerster</surname><given-names>AF</given-names></name><name><surname>Ragert</surname><given-names>P</given-names></name><name><surname>Dinse</surname><given-names>HR</given-names></name><name><surname>Schwenkreis</surname><given-names>P</given-names></name><name><surname>Malin</surname><given-names>JP</given-names></name><name><surname>Nicolas</surname><given-names>V</given-names></name><name><surname>Tegenthoff</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Functional imaging of perceptual learning in human primary and secondary somatosensory cortex</article-title><source>Neuron</source><volume>40</volume><fpage>643</fpage><lpage>653</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00677-9</pub-id><pub-id pub-id-type="pmid">14642286</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Plumbley</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Lie group methods for optimization with orthogonality constraints</article-title><conf-name>In International Conference on Independent Component Analysis and Signal Separation</conf-name></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priebe</surname><given-names>NJ</given-names></name><name><surname>Mechler</surname><given-names>F</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The contribution of spike threshold to the dichotomy of cortical simple and complex cells</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1113</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1038/nn1310</pub-id><pub-id pub-id-type="pmid">15338009</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priebe</surname><given-names>NJ</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inhibition, spike threshold, and stimulus selectivity in primary visual cortex</article-title><source>Neuron</source><volume>57</volume><fpage>482</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.005</pub-id><pub-id pub-id-type="pmid">18304479</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rahaman</surname><given-names>N</given-names></name><name><surname>Baratin</surname><given-names>A</given-names></name><name><surname>Arpit</surname><given-names>D</given-names></name><name><surname>Draxler</surname><given-names>F</given-names></name><name><surname>Lin</surname><given-names>M</given-names></name><name><surname>Hamprecht</surname><given-names>F</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>On the spectral bias of neural networks</article-title><conf-name>In International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>CE</given-names></name><name><surname>Williams</surname><given-names>CKI</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Gaussian Processes for Machine Learning</source><publisher-name>The MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/3206.001.0001</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname><given-names>GH</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Plasticity in the frequency representation of primary auditory cortex following discrimination training in adult owl monkeys</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>87</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-01-00087.1993</pub-id><pub-id pub-id-type="pmid">8423485</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumyantsev</surname><given-names>OI</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Hernandez</surname><given-names>O</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Savall</surname><given-names>J</given-names></name><name><surname>Chrapkiewicz</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fundamental bounds on the fidelity of sensory cortical coding</article-title><source>Nature</source><volume>580</volume><fpage>100</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2130-2</pub-id><pub-id pub-id-type="pmid">32238928</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural constraints on learning</article-title><source>Nature</source><volume>512</volume><fpage>423</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1038/nature13665</pub-id><pub-id pub-id-type="pmid">25164754</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Herbrich</surname><given-names>R</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A generalized representer theorem</article-title><conf-name>In Proceedings of the 14th Annual Conference on Computational Learning Theory and 5th European Conference on Computational Learning Theory, COLT ’01/EuroCOLT ’01</conf-name></element-citation></ref><ref id="bib100"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoups</surname><given-names>A</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Qian</surname><given-names>N</given-names></name><name><surname>Orban</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Practising orientation identification improves orientation coding in v1 neurons</article-title><source>Nature</source><volume>412</volume><fpage>549</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1038/35087601</pub-id><pub-id pub-id-type="pmid">11484056</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeman</surname><given-names>SC</given-names></name><name><surname>Campagnola</surname><given-names>L</given-names></name><name><surname>Davoudian</surname><given-names>PA</given-names></name><name><surname>Hoggarth</surname><given-names>A</given-names></name><name><surname>Hage</surname><given-names>TA</given-names></name><name><surname>Bosma-Moody</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CA</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Teeter</surname><given-names>C</given-names></name><name><surname>Ko</surname><given-names>AL</given-names></name><name><surname>Ojemann</surname><given-names>JG</given-names></name><name><surname>Gwinn</surname><given-names>RP</given-names></name><name><surname>Silbergeld</surname><given-names>DL</given-names></name><name><surname>Cobbs</surname><given-names>C</given-names></name><name><surname>Phillips</surname><given-names>J</given-names></name><name><surname>Lein</surname><given-names>E</given-names></name><name><surname>Murphy</surname><given-names>G</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sparse recurrent excitatory connectivity in the microcircuit of the adult mouse and human cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e37349</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37349</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shan</surname><given-names>H</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Minimum Perturbation Theory of Deep Perceptual Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.10.05.463260</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>JB</given-names></name><name><surname>Dickens</surname><given-names>M</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name></person-group><article-title>Neural tangent kernel eigenvalues accurately predict generalization</article-title><conf-name>ICLR 2022 Conference</conf-name><year iso-8601-date="2021">2021</year><ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=lycl1GD7fVP">https://openreview.net/forum?id=lycl1GD7fVP</ext-link></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A model of neuronal responses in visual area MT</article-title><source>Vision Research</source><volume>38</volume><fpage>743</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00183-1</pub-id><pub-id pub-id-type="pmid">9604103</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Engineering a less artificial intelligence</article-title><source>Neuron</source><volume>103</volume><fpage>967</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.08.034</pub-id><pub-id pub-id-type="pmid">31557461</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sollich</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Approximate learning curves for Gaussian processes</article-title><conf-name>9th International Conference on Artificial Neural Networks</conf-name><pub-id pub-id-type="doi">10.1049/cp:19991148</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sollich</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Gaussian process regression with mismatched models</chapter-title><person-group person-group-type="editor"><name><surname>Dietterich</surname><given-names>T</given-names></name><name><surname>Becker</surname><given-names>S</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Dubs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Predictive coding: a fresh view of inhibition in the retina</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>216</volume><fpage>427</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1098/rspb.1982.0085</pub-id><pub-id pub-id-type="pmid">6129637</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stopfer</surname><given-names>M</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Intensity versus identity coding in an olfactory system</article-title><source>Neuron</source><volume>39</volume><fpage>991</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2003.08.011</pub-id><pub-id pub-id-type="pmid">12971898</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018a</year><data-title>Recordings of 10,000 neurons in visual cortex in response to 2,800 natural images</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.25378/janelia.6845348</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>High-Dimensional Geometry of Population Responses in Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/374090</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018c</year><data-title>MouseLand / stringer-pachitariu-et-al-2018b</data-title><version designator="79850db">79850db</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-pachitariu-et-al-2018b">https://github.com/MouseLand/stringer-pachitariu-et-al-2018b</ext-link></element-citation></ref><ref id="bib116"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>MouseLand / stringer-et-al-2019</data-title><version designator="bd294c4">bd294c4</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-et-al-2019">https://github.com/MouseLand/stringer-et-al-2019</ext-link></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Tsyboulski</surname><given-names>D</given-names></name><name><surname>Lindo</surname><given-names>SE</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>High-Precision coding in visual cortex</article-title><source>Cell</source><volume>184</volume><fpage>2767</fpage><lpage>2778</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2021.03.042</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How to grow a mind: statistics, structure, and abstraction</article-title><source>Science</source><volume>331</volume><fpage>1279</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1126/science.1192788</pub-id><pub-id pub-id-type="pmid">21393536</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Townsend</surname><given-names>A</given-names></name><name><surname>Trefethen</surname><given-names>LN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Continuous analogues of matrix factorizations</article-title><source>Proceedings of the Royal Society A</source><volume>471</volume><elocation-id>20140585</elocation-id><pub-id pub-id-type="doi">10.1098/rspa.2014.0585</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treves</surname><given-names>A</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>What determines the capacity of autoassociative memories in the brain?</article-title><source>Network</source><volume>2</volume><fpage>371</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_2_4_004</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Valle-Perez</surname><given-names>G</given-names></name><name><surname>Camargo</surname><given-names>CQ</given-names></name><name><surname>Louis</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning generalizes because the parameter-function map is biased towards simple functions</article-title><conf-name>In International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A theory of maximizing sensory information</article-title><source>Biol Cybern</source><volume>68</volume><fpage>23</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1007/BF00203134</pub-id><pub-id pub-id-type="pmid">1486129</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vreysen</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Chino</surname><given-names>YM</given-names></name><name><surname>Arckens</surname><given-names>L</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dynamics of spatial frequency tuning in mouse visual cortex</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>2937</fpage><lpage>2949</lpage><pub-id pub-id-type="doi">10.1152/jn.00022.2012</pub-id><pub-id pub-id-type="pmid">22402662</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Widrow</surname><given-names>B</given-names></name><name><surname>Hoff</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>Adaptive switching circuits</article-title><conf-name>In 1960 IRE WESCON Convention Record</conf-name></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Characterizing the sparseness of neural codes</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1080/net.12.3.255.270</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>AG</given-names></name><name><surname>Dann</surname><given-names>C</given-names></name><name><surname>Lucas</surname><given-names>C</given-names></name><name><surname>Xing</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The human kernel</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The lack of a priori distinctions between learning algorithms</article-title><source>Neural Computation</source><volume>8</volume><fpage>1341</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.7.1341</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>L</given-names></name><name><surname>Pennington</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Synergy and Symmetry in Deep Learning: Interactions between the Data, Model, and Inference Algorithm</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2207.04612">https://arxiv.org/abs/2207.04612</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.2207.04612</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>ZQJ</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Luo</surname><given-names>T</given-names></name><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1901.06523">https://arxiv.org/abs/1901.06523</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1901.06523</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The effect of perceptual learning on neuronal responses in monkey visual area v4</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>1617</fpage><lpage>1626</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4442-03.2004</pub-id><pub-id pub-id-type="pmid">14973244</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Tensor Programs i: Wide Feedforward or Recurrent Neural Networks of Any Architecture Are Gaussian Processes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1910.12478">https://arxiv.org/abs/1910.12478</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1910.12478</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Tensor Programs Ii: Neural Tangent Kernel for Any Architecture</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.14548">https://arxiv.org/abs/2006.14548</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.2006.14548</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Hu</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Tensor programs iv: Feature learning in infinite-width neural networks</article-title><conf-name>In International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A critique of pure learning and what artificial neural networks can learn from animal brains</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-11786-6</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Hardt</surname><given-names>M</given-names></name><name><surname>Recht</surname><given-names>B</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Understanding deep learning requires rethinking generalization</article-title><conf-name>In 5th Int. Conf. on Learning Representations (ICLR 2017)</conf-name></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Singular value decomposition of continuous population responses</title><p>SVD of population responses is usually evaluated with respect to a discrete and finite set of stimuli. In the main paper, we implicitly assumed that a generalization of SVD to a continuum of stimuli. In this section we provide an explicit construction of this generalized SVD using techniques from functional analysis. Our construction is an example of the quasimatrix SVD defined in <xref ref-type="bibr" rid="bib119">Townsend and Trefethen, 2015</xref> and justifies our use of SVD in the main text.</p><p>For our construction, we note that Mercer’s theorem guarantees the existence of an eigendecomposition of any inner product kernel <inline-formula><mml:math id="inf418"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in terms of a complete orthonormal set of functions <inline-formula><mml:math id="inf419"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib95">Rasmussen and Williams, 2005</xref>). In particular, there exist a non-negative (but possibly zero) summable eigenvalues <inline-formula><mml:math id="inf420"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:math></inline-formula> and a corresponding set of orthonormal eigenfunctions such that<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>For a stimulus distribution <inline-formula><mml:math id="inf421"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the set of functions <inline-formula><mml:math id="inf422"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:math></inline-formula> are orthonormal and form a complete basis for square integrable functions <italic>L</italic><sub>2</sub> which means<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Next, we use this basis to construct the SVD. Each of the tuning curves <inline-formula><mml:math id="inf423"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (assumed to be square integrable) can be expressed in this basis with the top <inline-formula><mml:math id="inf424"><mml:mi>N</mml:mi></mml:math></inline-formula> of the functions in the set<disp-formula id="equ24"> <label>(24)</label><mml:math id="m24"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we introduced a matrix <inline-formula><mml:math id="inf425"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of expansion coefficients. Note that <inline-formula><mml:math id="inf426"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We compute the singular value decomposition of the finite matrix <inline-formula><mml:math id="inf427"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula><disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We note that the signal correlation matrix for this population code can be computed in closed form<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>ψ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">AA</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>due to the orthonormality of <inline-formula><mml:math id="inf428"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. Thus the principal axes <inline-formula><mml:math id="inf429"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> of the neural correlations are the left singular vectors of <inline-formula><mml:math id="inf430"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula>. We may similarly express the inner product kernel in terms of the eigenfunctions<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>ψ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The kernel eigenvalue problem demands (<xref ref-type="bibr" rid="bib95">Rasmussen and Williams, 2005</xref>)<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo stretchy="false">⟹</mml:mo><mml:mspace width="thickmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="thickmathspace"/><mml:mo stretchy="false">⟹</mml:mo><mml:mspace width="thickmathspace"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The <inline-formula><mml:math id="inf431"><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> vectors must be identical to <inline-formula><mml:math id="inf432"><mml:mrow><mml:mo>±</mml:mo><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the Cartesian unit vectors, if the eigenvalues are non-degenerate. From this exercise, we find that the SVD for <inline-formula><mml:math id="inf433"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> has the form <inline-formula><mml:math id="inf434"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. With this choice, the population code admits a singular value decomposition<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This singular value decomposition demonstrates the connection between neural manifold structure (principal axes <inline-formula><mml:math id="inf435"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>) and function approximation (kernel eigenfunctions <inline-formula><mml:math id="inf436"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>). This singular value decomposition can be verified by computing the inner product kernel and the correlation matrix, utilizing the orthonormality of <inline-formula><mml:math id="inf437"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf438"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. This exercise has important consequences for the space of learnable functions, which is at most <inline-formula><mml:math id="inf439"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> dimensional since linear readouts lie in <inline-formula><mml:math id="inf440"><mml:mrow><mml:mtext>span</mml:mtext><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s9"><title>Discrete stimulus spaces: finding eigenfunctions with matrix eigendecomposition</title><p>In our discussion so far, our notation suggested that <inline-formula><mml:math id="inf441"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> take a continuum of values. Here we want to point that our theory still applies if <inline-formula><mml:math id="inf442"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> take a discrete set of values. In this case, we can think of a Dirac measure <inline-formula><mml:math id="inf443"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:msubsup><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf444"><mml:mi>i</mml:mi></mml:math></inline-formula> indexes all the <inline-formula><mml:math id="inf445"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> values <inline-formula><mml:math id="inf446"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> can take. With this choice<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:munderover><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Demanding this equality for <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> generates a matrix eigenvalue problem<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The eigenfunctions over the stimuli are identified as the columns of <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> while the eigenvalues are the diagonal elements of <inline-formula><mml:math id="inf450"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><sec sec-type="appendix" id="s9-1"><title>Experimental considerations</title><p>In an experimental setting, a finite number of stimuli are presented and the SVD is calculated over this finite set regardless of the support of <inline-formula><mml:math id="inf451"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This raises the question of the interpretation of this SVD and its relation to the inductive bias theory we presented. Here we provide two interpretations.</p><p>In the first interpretation, we think of the empirical SVD as providing an estimate of the SVD over the full distribution <inline-formula><mml:math id="inf452"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To formalize this notion, we can introduce a Monte-Carlo estimate of the integral eigenvalue problem<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:munderover><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For this interpretation to work, the experimenter must sample the stimuli from <inline-formula><mml:math id="inf453"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which could be the natural stimulus distribution. Measuring responses to a larger number of stimuli gives a more accurate approximation of the integral above, which will provide a better estimate of generalization performance on the true distribution <inline-formula><mml:math id="inf454"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In the second interpretation, we construct an empirical measure on <inline-formula><mml:math id="inf455"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> experimental stimulus values <inline-formula><mml:math id="inf456"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:msubsup><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and consider learning and generalization over this distribution. This allows the application of our theory to an experimental setting where <inline-formula><mml:math id="inf457"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is designed by an experimenter. For example, the experimenter could procure a complicated set of <inline-formula><mml:math id="inf458"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> videos, to which an associated function <inline-formula><mml:math id="inf459"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must be learned. After showing these videos to the animal and measuring neural responses, the experimenter could compute, with our theory, generalization error for a uniform distribution over this full set of <inline-formula><mml:math id="inf460"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> videos. Our theory would predict generalization over this distribution after providing supervisory feedback for only a strict subset of <inline-formula><mml:math id="inf461"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> videos. Under this interpretation, the relationship between the integral eigenvalue problem and matrix eigenvalue problem is exact rather than approximate<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:munderover><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Demanding either of (32) or (33) equalities for <inline-formula><mml:math id="inf462"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>ν</mml:mi></mml:msup></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> generates a matrix eigenvalue problem<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The eigenfunctions restricted to <inline-formula><mml:math id="inf463"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are identified as the columns of <inline-formula><mml:math id="inf464"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> while the eigenvalues are the diagonal elements of <inline-formula><mml:math id="inf465"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the case where <inline-formula><mml:math id="inf466"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf467"><mml:mi>P</mml:mi></mml:math></inline-formula> are finite, the spectrum obtained through eigendecomposition of the kernel <inline-formula><mml:math id="inf468"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> is the same as would be obtained through the finite <inline-formula><mml:math id="inf469"><mml:mi>N</mml:mi></mml:math></inline-formula> signal correlation matrix <inline-formula><mml:math id="inf470"><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, since they are inner and outer products of trial averaged population response matrices <inline-formula><mml:math id="inf471"><mml:mi mathvariant="bold">R</mml:mi></mml:math></inline-formula>.</p></sec></sec><sec sec-type="appendix" id="s10"><title>Translation invariant kernels</title><p>For the special case where the data distribution <inline-formula><mml:math id="inf472"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>V</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> is uniform over volume <inline-formula><mml:math id="inf473"><mml:mi>V</mml:mi></mml:math></inline-formula> and the kernel is translation invariant <inline-formula><mml:math id="inf474"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the kernel can be diagonalized in the basis of plane waves<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>V</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>V</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The eigenvalues are the Fourier components of the Kernel <inline-formula><mml:math id="inf475"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>V</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>V</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> while the eigenfunctions are plane waves <inline-formula><mml:math id="inf476"><mml:mrow><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The set of admissible momenta <inline-formula><mml:math id="inf477"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>±</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>±</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are determined by the boundary conditions. The diagonalized representation of the kernel is therefore<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>For example, if the space is the torus <inline-formula><mml:math id="inf478"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">T</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>×</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, then the space of admissable momenta are the points on the integer lattice <inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Reality and symmetry of the kernel demand that <inline-formula><mml:math id="inf480"><mml:mrow><mml:mrow><mml:mtext>Im</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf481"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Most of the models in this paper consider <inline-formula><mml:math id="inf482"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Unif</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where the kernel has the following Fourier/Mercer decomposition<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where we invoked the simple trigonometric identity <inline-formula><mml:math id="inf483"><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. By recognizing that <inline-formula><mml:math id="inf484"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:math></inline-formula> form a complete orthonormal set of functions with respect to <inline-formula><mml:math id="inf485"><mml:mrow><mml:mtext>Unif</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we have identified this as the collection of kernel eigenfunctions.</p></sec><sec sec-type="appendix" id="s11"><title>Invariant kernels possess invariant eigenfunctions</title><p>Suppose the kernel <inline-formula><mml:math id="inf486"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is invariant to some set of transformations <inline-formula><mml:math id="inf487"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, by which we mean that<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We will now show that any eigenfunction of such a kernel with nonzero eigenvalue must be an invariant function. Let <inline-formula><mml:math id="inf488"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be an eigenfunction with eigenvalue <inline-formula><mml:math id="inf489"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, then<disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This establishes that all functions which depend on <inline-formula><mml:math id="inf490"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> transformations must necessarily lie in the null-space of <inline-formula><mml:math id="inf491"><mml:mi>K</mml:mi></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12"><title>Theory of generalization</title><sec sec-type="appendix" id="s12-1"><title>Convergence of the delta-rule without weight decay</title><p>In this section, we discuss the delta-rule convergence when weight decay parameter is set to <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The next section considers the simpler case where <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Gradient descent training of readout weights <inline-formula><mml:math id="inf494"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula> on a finite sample of size <inline-formula><mml:math id="inf495"><mml:mi>P</mml:mi></mml:math></inline-formula> converges to the kernel regression solution (<xref ref-type="bibr" rid="bib11">Bartlett et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Hastie et al., 2020</xref>). Let <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the dataset with samples <inline-formula><mml:math id="inf497"><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> and target values <inline-formula><mml:math id="inf498"><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>. We introduce a shorthand <inline-formula><mml:math id="inf499"><mml:mrow><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for convenience. The empirical loss we aim to minimize is a sum of the squared losses of each data point in the training set<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Performing gradient descent updates<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>recovers the delta rule that we discussed in the main text (<xref ref-type="bibr" rid="bib124">Widrow and Hoff, 1960</xref>; <xref ref-type="bibr" rid="bib48">Hertz et al., 1991</xref>). Letting the empirical response matrix <inline-formula><mml:math id="inf500"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>P</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> have a SVD <inline-formula><mml:math id="inf501"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:msub><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and expanding the weights <inline-formula><mml:math id="inf502"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and labels <inline-formula><mml:math id="inf503"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in their respective SVD bases, we find<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>For all directions with <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the dynamics converge to the unique fixed point <inline-formula><mml:math id="inf505"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:msqrt><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mrow></mml:math></inline-formula>, while for all modes with <inline-formula><mml:math id="inf506"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the weights remain at <inline-formula><mml:math id="inf507"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Thus<disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where K<sup>+</sup> is the Moore-Penrose inverse of the kernel matrix <inline-formula><mml:math id="inf508"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The predictions of the learned function are given by <inline-formula><mml:math id="inf509"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> which can be expressed as<disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The fact that the solution can be written in terms of a linear combination of <inline-formula><mml:math id="inf510"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:math></inline-formula> is known as the representer theorem (<xref ref-type="bibr" rid="bib99">Schölkopf et al., 2001</xref>; <xref ref-type="bibr" rid="bib95">Rasmussen and Williams, 2005</xref>). A similar analysis for nonlinear readouts where <inline-formula><mml:math id="inf511"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is provided in Appendix Convergence of delta-rule for nonlinear readouts.</p></sec><sec sec-type="appendix" id="s12-2"><title>Weight decay and ridge regression</title><p>We can introduce a regularization term in our learning problem which penalizes the size of the readout weights. This leads to a modified learning objective of the form<disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Inclusion of this regularization alters the learning rule through <italic>weight decay</italic><inline-formula><mml:math id="inf512"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which multiplies the existing weight value by a factor of <inline-formula><mml:math id="inf513"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> before adding the data dependent update. The fixed point of these dynamics is <inline-formula><mml:math id="inf514"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">RR</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">Ry</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This learning problem and gradient descent dynamics have a closed form solution<disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The generalization benefits of explicit regularization through weight decay is known to be related to the noise statistics in the learning problem (<xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). This is visible in the <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> , where unlearnable target functions demand nonzero optimal regularization. We simulate weight decay only in <xref ref-type="fig" rid="fig6">Figure 6C</xref>, where we use <inline-formula><mml:math id="inf515"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to improve numerical stability at large <inline-formula><mml:math id="inf516"><mml:mi>P</mml:mi></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12-3"><title>Computation of learning curves</title><p>Recent work has established analytic results that predict the average case generalization error for kernel regression<disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the generalization error for a certain sample <inline-formula><mml:math id="inf518"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of size <inline-formula><mml:math id="inf519"><mml:mi>P</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf520"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the kernel regression solution for <inline-formula><mml:math id="inf521"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). The typical or average case error <inline-formula><mml:math id="inf522"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> is obtained by averaging over all possible datasets of size <inline-formula><mml:math id="inf523"><mml:mi>P</mml:mi></mml:math></inline-formula>. This average case generalization error is determined solely by the decomposition of the target function <inline-formula><mml:math id="inf524"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> along the eigenbasis of the kernel and the eigenspectrum of the kernel. This diagonalization takes the form<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the eigenfunctions form a complete set of square integrable functions, we expand both the target function <inline-formula><mml:math id="inf525"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the learned function <inline-formula><mml:math id="inf526"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in this basis<disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Due to the orthonormality of the kernel eigenfunctions <inline-formula><mml:math id="inf527"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, the generalization error for any set of coefficients <inline-formula><mml:math id="inf528"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is<disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula></p><p>We now introduce training error, or empirical loss, which depends on the disorder in the dataset <inline-formula><mml:math id="inf529"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:msubsup><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>It is straightforward to verify that the optimal <inline-formula><mml:math id="inf530"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> which minimizes <inline-formula><mml:math id="inf531"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the kernel regression solution for kernel with eigenvalues <inline-formula><mml:math id="inf532"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The optimal weights <inline-formula><mml:math id="inf534"><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> can be identified through the first order condition <inline-formula><mml:math id="inf535"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> which gives<disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf536"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the eigenfunctions evaluated on the training data and <inline-formula><mml:math id="inf537"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a a diagonal matrix containing the kernel eigenvalues. The generalization error for this optimal solution is<disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>We note that the dependence on the randomly sampled dataset <inline-formula><mml:math id="inf538"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> only appears through the matrix <inline-formula><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus to compute the <italic>typical</italic> generalization error we need to average over this matrix <inline-formula><mml:math id="inf540"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. There are multiple strategies to perform such an average and we will study one here based on a partial differential equation which was introduced in <xref ref-type="bibr" rid="bib109">Sollich, 1998</xref>; <xref ref-type="bibr" rid="bib110">Sollich, 2002</xref> and studied further in <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>. In this setting, we denote the average matrix <inline-formula><mml:math id="inf541"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for a dataset of size <inline-formula><mml:math id="inf542"><mml:mi>P</mml:mi></mml:math></inline-formula>. We first will derive a recursion relationship using the Sherman Morrison formula for a rank-1 update to an inverse matrix. We imagine adding a new sampled feature vector <inline-formula><mml:math id="inf543"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> to a dataset <inline-formula><mml:math id="inf544"><mml:mi>ψ</mml:mi></mml:math></inline-formula> with size <inline-formula><mml:math id="inf545"><mml:mi>P</mml:mi></mml:math></inline-formula>. The average matrix <inline-formula><mml:math id="inf546"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at <inline-formula><mml:math id="inf547"><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> samples can be related to <inline-formula><mml:math id="inf548"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through the Sherman Morrison rule<disp-formula id="equ54"><label>(54)</label><mml:math id="m54"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>ψ</mml:mi><mml:msup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:msup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>ψ</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where in the last step we approximated the average of the ratio with the ratio of averages. This operation, is of course, unjustified theoretically, but has been shown to produce accurate learning curves (<xref ref-type="bibr" rid="bib110">Sollich, 2002</xref>; <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>). Since the chosen basis of kernel eigenfunctions are orthonormal, the average over the new sample is trivial <inline-formula><mml:math id="inf549"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:msup><mml:mi>ψ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We thus arrive at the following recursion relationship for <inline-formula><mml:math id="inf550"><mml:mi mathvariant="bold">G</mml:mi></mml:math></inline-formula><disp-formula id="equ55"><label>(55)</label><mml:math id="m55"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mtext>Tr</mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>By introducing an additional source <inline-formula><mml:math id="inf551"><mml:mi>J</mml:mi></mml:math></inline-formula> so that <inline-formula><mml:math id="inf552"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can relate <inline-formula><mml:math id="inf553"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>’s first and second moments through differentiation<disp-formula id="equ56"><label>(56)</label><mml:math id="m56"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus the recursion relation simplifies to<disp-formula id="equ57"><label>(57)</label><mml:math id="m57"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we approximated the finite difference in <inline-formula><mml:math id="inf554"><mml:mi>P</mml:mi></mml:math></inline-formula> as a derivative, treating <inline-formula><mml:math id="inf555"><mml:mi>P</mml:mi></mml:math></inline-formula> as a continuous variable. Taking the trace of both sides and defining <inline-formula><mml:math id="inf556"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> we arrive at the following quasilinear PDE<disp-formula id="equ58"><label>(58)</label><mml:math id="m58"><mml:mrow><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with the initial condition <inline-formula><mml:math id="inf557"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mtext>Tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Using the method of characteristics, we arrive at the solution <inline-formula><mml:math id="inf558"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mtext>Tr</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Using this solution to <inline-formula><mml:math id="inf559"><mml:mi>κ</mml:mi></mml:math></inline-formula>, we can identify the solution to <inline-formula><mml:math id="inf560"><mml:mi mathvariant="bold">G</mml:mi></mml:math></inline-formula><disp-formula id="equ59"><label>(59)</label><mml:math id="m59"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>P</mml:mi><mml:mi>κ</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The generalization error, therefore can be written as<disp-formula id="equ60"><label>(60)</label><mml:math id="m60"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ61"><label>(61)</label><mml:math id="m61"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>P</mml:mi><mml:mi>κ</mml:mi></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mstyle><mml:mstyle displaystyle="false"><mml:mfrac><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf561"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, giving the desired result. Note that <inline-formula><mml:math id="inf562"><mml:mi>κ</mml:mi></mml:math></inline-formula> depends on <inline-formula><mml:math id="inf563"><mml:mi>J</mml:mi></mml:math></inline-formula> implicitly, which is the source of the <inline-formula><mml:math id="inf564"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> factor. This result was recently reproduced using techniques from statistical mechanics (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>).</p></sec><sec sec-type="appendix" id="s12-4"><title>Spectral bias and code-task alignment</title><p>Through implicit differentiation it is straightforward to verify that the ordering of the mode errors <inline-formula><mml:math id="inf565"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> matches the ordering of the eigenvalues (<xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). Let <inline-formula><mml:math id="inf566"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, then we have<disp-formula id="equ62"><label>(62)</label><mml:math id="m62"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>-</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>κ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>-</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf567"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the first bracket must be negative and the second bracket must be positive. Further, it is straightforward to compute that <inline-formula><mml:math id="inf568"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>κ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>κ</mml:mi><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Therefore <inline-formula><mml:math id="inf569"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> implies <inline-formula><mml:math id="inf570"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf571"><mml:mi>P</mml:mi></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf572"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> at <inline-formula><mml:math id="inf573"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> we therefore have that <inline-formula><mml:math id="inf574"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf575"><mml:mi>P</mml:mi></mml:math></inline-formula> and consequently <inline-formula><mml:math id="inf576"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Modes with larger eigenvalues <inline-formula><mml:math id="inf577"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> have lower normalized mode errors <inline-formula><mml:math id="inf578"><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. This observation can be used to prove that target functions acting on the same data distribution with higher cumulative power distributions <inline-formula><mml:math id="inf579"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf580"><mml:mi>k</mml:mi></mml:math></inline-formula> will have lower generalization error normalized by total target power, <inline-formula><mml:math id="inf581"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for all <inline-formula><mml:math id="inf582"><mml:mi>P</mml:mi></mml:math></inline-formula>. Proof can be found in <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>.</p></sec><sec sec-type="appendix" id="s12-5"><title>Asymptotic power law scaling of learning curves</title><sec sec-type="appendix" id="s12-5-1"><title><italic>Exponential spectral decays</italic>:</title><p>First, we will study the setting relevant to the von-Mises kernel where <inline-formula><mml:math id="inf583"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf584"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf585"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. This exponential behavior accounts for differences in bandwidth between kernels which modulates the base <inline-formula><mml:math id="inf586"><mml:mi>β</mml:mi></mml:math></inline-formula> of the exponential scaling of <inline-formula><mml:math id="inf587"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id="inf588"><mml:mi>k</mml:mi></mml:math></inline-formula>. We will approximate the sum over all mode errors with an integral<disp-formula id="equ63"><label>(63)</label><mml:math id="m63"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:mfrac><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If we include a regularization parameter <inline-formula><mml:math id="inf589"><mml:mi>λ</mml:mi></mml:math></inline-formula>, then <inline-formula><mml:math id="inf590"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>∼</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf591"><mml:mrow><mml:mi>P</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. Making the change of variables <inline-formula><mml:math id="inf592"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we transform the above integral into<disp-formula id="equ64"><mml:math id="m64"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>P</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The remaining integral over <inline-formula><mml:math id="inf593"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is either dominated near <inline-formula><mml:math id="inf594"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf595"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and behaves as a <inline-formula><mml:math id="inf596"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-independent constant or else is dominated near <inline-formula><mml:math id="inf597"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>≈</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, in which case the integral scales as <inline-formula><mml:math id="inf598"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Multiplying these resulting functions with the prefactor, we find the following scaling laws for generalization.<disp-formula id="equ65"> <label>(64)</label><mml:math id="m65"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, we obtain a power law scaling of the learning curve <inline-formula><mml:math id="inf599"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> which is dominated at large <inline-formula><mml:math id="inf600"><mml:mi>P</mml:mi></mml:math></inline-formula> by <inline-formula><mml:math id="inf601"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. For the von-Mises kernel we can approximate the spectra with <inline-formula><mml:math id="inf602"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf603"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> giving rise to a generalization scaling scaling <inline-formula><mml:math id="inf604"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12-5-2"><title>Power law spectral decays</title><p>The same arguments can be applied for power law kernels <inline-formula><mml:math id="inf605"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and power law targets <inline-formula><mml:math id="inf606"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which is of interest due to its connection to nonlinear rectified neural populations. In this setting, the generalization error is<disp-formula id="equ66"><label>(65)</label><mml:math id="m66"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>1</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msup></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We see that there are two possible power law scalings for <inline-formula><mml:math id="inf607"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> with the exponents <inline-formula><mml:math id="inf608"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> and 2. At large <inline-formula><mml:math id="inf609"><mml:mi>P</mml:mi></mml:math></inline-formula> this formula will be dominated by the term with minimum exponent so <inline-formula><mml:math id="inf610"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec sec-type="appendix" id="s12-6"><title>Laplace kernel generalization</title><p>We calculate similar learning curves as we did for the von-Mises kernel but with Laplace kernels to show that our results is not an artifact of the infinite differentiability of the Von Mises kernel. Each of these Laplace kernels has the same asymptotic power law spectrum <inline-formula><mml:math id="inf611"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, exhibiting a discontinuous first derivative (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Despite having the same spectral scaling at large <inline-formula><mml:math id="inf612"><mml:mi>k</mml:mi></mml:math></inline-formula>, these kernels can give dramatically different performance in learning tasks, again indicating the influence of the top eigenvalues on generalization at small <inline-formula><mml:math id="inf613"><mml:mi>P</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Again, the trend for which kernels perform best at low <inline-formula><mml:math id="inf614"><mml:mi>P</mml:mi></mml:math></inline-formula> can be reversed at large <inline-formula><mml:math id="inf615"><mml:mi>P</mml:mi></mml:math></inline-formula>. In this case, all generalization errors scale with <inline-formula><mml:math id="inf616"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). More generally, our theory shows that if the task power spectrum and kernel eigenspectrum are both falling as power laws with exponents <inline-formula><mml:math id="inf617"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf618"><mml:mi>b</mml:mi></mml:math></inline-formula> respectively, then the generalization error asymptotically falls with a power law, <inline-formula><mml:math id="inf619"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (Methods) (<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>). This decay is fastest when <inline-formula><mml:math id="inf620"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> for which <inline-formula><mml:math id="inf621"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Therefore, the tail of the kernel’s eigenvalue spectrum determines the large sample size behavior of the generalization error for power law kernels. Small sample size limit is still governed by the bulk of the spectrum.</p></sec><sec sec-type="appendix" id="s12-7"><title>Learning with multiple output channels</title><p>Our theory is not limited to scalar target functions but rather can be easily extended to multiple output functions <inline-formula><mml:math id="inf622"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from the same data, if for example the task requires computing class membership for <inline-formula><mml:math id="inf623"><mml:mi>C</mml:mi></mml:math></inline-formula> categories. In this setting, each data point has the form <inline-formula><mml:math id="inf624"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf625"><mml:mrow><mml:msup><mml:mi mathvariant="bold">y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. For these <inline-formula><mml:math id="inf626"><mml:mi>C</mml:mi></mml:math></inline-formula> classes, the generalization error takes the form<disp-formula id="equ67"><label>(66)</label><mml:math id="m67"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We therefore find that the generalization error in the multi-class setting is the same as the <inline-formula><mml:math id="inf627"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> obtained for a single scalar target function with power spectrum <inline-formula><mml:math id="inf628"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). The relevant cumulative power distribution measures the fraction of total output variance captured by the first <inline-formula><mml:math id="inf629"><mml:mi>k</mml:mi></mml:math></inline-formula> eigenfunctions of the population code<disp-formula id="equ68"><label>(67)</label><mml:math id="m68"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s12-8"><title>Convergence of Delta-rule for nonlinear readouts</title><p>In this section, we consider gradient descent dynamics of an arbitrary convex loss function. For instance, we can consider a binary classification problem where <inline-formula><mml:math id="inf630"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by outputting a prediction of <inline-formula><mml:math id="inf631"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mtext>sign</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We could, for example, train a model using the hinge loss <inline-formula><mml:math id="inf632"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>max</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> so that the classifier will converge to a kernel support vector machine (SVM) (<xref ref-type="bibr" rid="bib100">Schölkopf et al., 2002</xref>). The generalization of the classifier would be the error rate of <inline-formula><mml:math id="inf633"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>sign</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> compared to the ground truth <inline-formula><mml:math id="inf634"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Let <inline-formula><mml:math id="inf635"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> be the dataset with samples <inline-formula><mml:math id="inf636"><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> and target values <inline-formula><mml:math id="inf637"><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>. We introduce a shorthand <inline-formula><mml:math id="inf638"><mml:mrow><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for convenience. The loss we aim to minimize is the sum of the losses of each data point in the training set with an additional weight decay parameter<disp-formula id="equ69"><label>(68)</label><mml:math id="m69"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For convex <inline-formula><mml:math id="inf639"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> and nonzero <inline-formula><mml:math id="inf640"><mml:mi>λ</mml:mi></mml:math></inline-formula>, the above objective is strongly convex, indicating the existence of a unique minimizer which can be found from simple first order learning rules. For <inline-formula><mml:math id="inf641"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> the initial condition for <inline-formula><mml:math id="inf642"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula> does not influence the final result.</p><p>We will now show that the dynamics will converge to a function which only depends on the code <inline-formula><mml:math id="inf643"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through the kernel <inline-formula><mml:math id="inf644"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To simplify the argument, we consider starting from an initial condition of <inline-formula><mml:math id="inf645"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></inline-formula> and performing gradient descent updates. Under such an assumption, the weights <inline-formula><mml:math id="inf646"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> will always be in the span of the population vectors on the training set <inline-formula><mml:math id="inf647"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> since<disp-formula id="equ70"><label>(69)</label><mml:math id="m70"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msup><mml:mi>ℓ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The derivative in the final term is taken with respect to the first argument <inline-formula><mml:math id="inf648"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>. This update is still local and recovers the delta rule that we discussed in the main text for <inline-formula><mml:math id="inf649"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib124">Widrow and Hoff, 1960</xref>; <xref ref-type="bibr" rid="bib48">Hertz et al., 1991</xref>). We can express <inline-formula><mml:math id="inf650"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> in terms of the population vectors <inline-formula><mml:math id="inf651"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>μ</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> so that <inline-formula><mml:math id="inf652"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> defines the linear weighting of each sample. The dynamics of these coefficients are<disp-formula id="equ71"><label>(70)</label><mml:math id="m71"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf653"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the kernel Gram matrix evaluated on the training points. We multiply both sides of this equation by <inline-formula><mml:math id="inf654"><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:math></inline-formula>, and define <inline-formula><mml:math id="inf655"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which satisfy the following simplified dynamics<disp-formula id="equ72"><label>(71)</label><mml:math id="m72"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>RK</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where K<sup>+</sup> is the pseudo-inverse of <inline-formula><mml:math id="inf656"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>. The nonlinear fixed point condition is <inline-formula><mml:math id="inf657"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>λ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which transparently only depends on the kernel <inline-formula><mml:math id="inf658"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> rather than the full code <inline-formula><mml:math id="inf659"><mml:mi mathvariant="bold">R</mml:mi></mml:math></inline-formula>. The above equation recovers the correct linear equation <inline-formula><mml:math id="inf660"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for square loss. For an arbitrary test point <inline-formula><mml:math id="inf661"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>, the model makes prediction using <inline-formula><mml:math id="inf662"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">RK</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which also only depends on the kernel <inline-formula><mml:math id="inf663"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> on test point <inline-formula><mml:math id="inf664"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> and train points <inline-formula><mml:math id="inf665"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, as well as the kernel gram matrix <inline-formula><mml:math id="inf666"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>To illustrate a specific case with a square error and nonlinear readout, consider output neurons which produce activity <inline-formula><mml:math id="inf667"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for invertible nonlinear function <inline-formula><mml:math id="inf668"><mml:mi>g</mml:mi></mml:math></inline-formula> with non-vanishing gradient, and gradient based learning on <inline-formula><mml:math id="inf669"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This gives <inline-formula><mml:math id="inf670"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>-</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mtext>span</mml:mtext><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is still a local learning rule. Thus the weights at convergence can be written as <inline-formula><mml:math id="inf671"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the learned function can be written as <inline-formula><mml:math id="inf672"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf673"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. To see this, first note that <inline-formula><mml:math id="inf674"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mtext>span</mml:mtext><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf675"><mml:mi>t</mml:mi></mml:math></inline-formula> so that <inline-formula><mml:math id="inf676"><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The interpolation condition can be expressed as <inline-formula><mml:math id="inf677"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula>, giving the desired result <inline-formula><mml:math id="inf678"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The predictions of the model on a test stimulus <inline-formula><mml:math id="inf679"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> are given by <inline-formula><mml:math id="inf680"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mo>∗</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We see that this solution only depends on the kernel (directly and indirectly through <inline-formula><mml:math id="inf681"><mml:msup><mml:mi>α</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>), rather than the full code.</p></sec><sec sec-type="appendix" id="s12-9"><title>Typical case analysis of nonlinear readouts</title><p>The analysis of typical case generalization can be extended to nonlinear predictors and loss functions described by (68) which depend on the scalar prediction variable <inline-formula><mml:math id="inf682"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib67">Loureiro et al., 2021a</xref>). Thanks Further, if <inline-formula><mml:math id="inf683"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is well approximated as a Gaussian process, then the generalization performance can still be characterized using statistical mechanics methods (<xref ref-type="bibr" rid="bib67">Loureiro et al., 2021a</xref>). Many qualitative features of our results continue to hold, including that the kernel’s diagonalization governs training and generalization and that improvements in code task alignment lead to improvements in generalization.</p><p>In a later work by <xref ref-type="bibr" rid="bib25">Cui et al., 2022</xref>, SVM and ridge classifiers trained on codes and tasks with power law spectra were analyzed asymptotically, showing power law generalization error decay rates <inline-formula><mml:math id="inf684"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. These classification learning curves for power law spectra were shown to follow power laws with exponents <inline-formula><mml:math id="inf685"><mml:mi>β</mml:mi></mml:math></inline-formula> which are qualitatively similar to the exponents obtained with the square loss which we describe in our section titled Small and Large Sample Size Behaviors of Generalization. Just as in our theory, decay rate exponents <inline-formula><mml:math id="inf686"><mml:mi>β</mml:mi></mml:math></inline-formula> are larger for codes which are well aligned to the task and are smaller for codes which are non-aligned.</p></sec></sec><sec sec-type="appendix" id="s13"><title>Visual scene reconstruction task</title><sec sec-type="appendix" id="s13-1"><title>Reconstruction of natural scenes from neural responses</title><p>Using the mouse V1 responses to natural scenes, we attempt to reconstruct original images from the neural codes using different numbers of images. The presented natural scenes are taken from ten classes of imagenet which can be downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/stringer-pachitariu-et-al-2018b">https://github.com/MouseLand/stringer-pachitariu-et-al-2018b</ext-link>. Let <inline-formula><mml:math id="inf687"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> be a <inline-formula><mml:math id="inf688"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional flattened vector containing the pixel values of the μ-th image and let <inline-formula><mml:math id="inf689"><mml:mrow><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represent the neural response to the μ-th image. The goal in the problem is to learn a collection of weights <inline-formula><mml:math id="inf690"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> which map neural responses <inline-formula><mml:math id="inf691"><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> to images <inline-formula><mml:math id="inf692"><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula><disp-formula id="equ73"><label>(72)</label><mml:math id="m73"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>≈</mml:mo><mml:msup><mml:mi>Wr</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The generalization error <inline-formula><mml:math id="inf693"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> again measures the average error on all points, averaged over all possible datasets <inline-formula><mml:math id="inf694"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> of size <inline-formula><mml:math id="inf695"><mml:mi>P</mml:mi></mml:math></inline-formula>. If the optimal weights for dataset <inline-formula><mml:math id="inf696"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is <inline-formula><mml:math id="inf697"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> then the generalization error is<disp-formula id="equ74"><label>(73)</label><mml:math id="m74"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>After identifying eigenfunctions <inline-formula><mml:math id="inf698"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we expand the images in this basis <inline-formula><mml:math id="inf699"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf700"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The generalization error is therefore <inline-formula><mml:math id="inf701"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the cumulative power is <inline-formula><mml:math id="inf702"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. We perform this reconstruction task on many filtered versions of the natural scenes. To construct a filter, we first compute the Fourier transform of the image. Let <inline-formula><mml:math id="inf703"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt><mml:mo>×</mml:mo><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represent the non-flattened image and let <inline-formula><mml:math id="inf704"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt><mml:mo>×</mml:mo><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represent the Fourier transform of the image, computed explicitly as<disp-formula id="equ75"><label>(74)</label><mml:math id="m75"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To develop the band-pass filter, we calculate <inline-formula><mml:math id="inf705"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> for each of the indices in the matrix. For a band-pass filter with parameters <inline-formula><mml:math id="inf706"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> we simply zero out the entries in <inline-formula><mml:math id="inf707"><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> which correspond to states with frequencies outside the appropriate band: for any <inline-formula><mml:math id="inf708"><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf709"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>∉</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>-</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> then <inline-formula><mml:math id="inf710"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. We then perform the inverse Fourier transform on <inline-formula><mml:math id="inf711"><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> to obtain a filtered version of the original image.</p></sec></sec><sec sec-type="appendix" id="s14"><title>A simple feedforward model of V1</title><sec sec-type="appendix" id="s14-1"><title>Linear neurons</title><p>We consider a simplified but instructive model of the V1 population code as a linear-nonlinear map from photoreceptor responses through Gabor filters and then nonlinearity (<xref ref-type="bibr" rid="bib3">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib97">Rumyantsev et al., 2020</xref>). Let <inline-formula><mml:math id="inf712"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> represent the two-dimensional retinotopic position of photoreceptors. The firing rates of the photoreceptor at position <inline-formula><mml:math id="inf713"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula> to a static grating stimulus oriented at angle <inline-formula><mml:math id="inf714"><mml:mi>θ</mml:mi></mml:math></inline-formula> is<disp-formula id="equ76"><label>(75)</label><mml:math id="m76"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mpadded width="+5pt"><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mpadded></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We model each V1 neuron’s receptive field as a Gabor filter of the receptor responses <inline-formula><mml:math id="inf715"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="inf716"><mml:mi>i</mml:mi></mml:math></inline-formula>-th V1 neuron has preferred wavevector <inline-formula><mml:math id="inf717"><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, generating the following set of weights between photoreceptors and the <inline-formula><mml:math id="inf718"><mml:mi>i</mml:mi></mml:math></inline-formula>-th V1 neuron<disp-formula id="equ77"><label>(76)</label><mml:math id="m77"><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The V1 population code is obtained by filtering the photoreceptor responses. By approximating the resulting sum over all retinal photoreceptors with an integral, we find the response of neuron <inline-formula><mml:math id="inf719"><mml:mi>i</mml:mi></mml:math></inline-formula> to grating stimulus with wavenumber <inline-formula><mml:math id="inf720"><mml:mi mathvariant="bold">k</mml:mi></mml:math></inline-formula> is<disp-formula id="equ78"><label>(77)</label><mml:math id="m78"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The response of neuron <inline-formula><mml:math id="inf721"><mml:mi>i</mml:mi></mml:math></inline-formula> is computed through nonlinear rectification of this input current <inline-formula><mml:math id="inf722"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For a linear neuron <inline-formula><mml:math id="inf723"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>, the kernel has the following form<disp-formula id="equ79"><label>(78)</label><mml:math id="m79"><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the kernel is normalized to have maximum value of 1. Note that this normalization of the kernel is completely legitimate since it merely rescales each eigenvalue by a constant and does not change the learning curves.</p><p>Since the kernel only depends on the difference between angles <inline-formula><mml:math id="inf724"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, it is said to posess translation invariance. Such translation invariant kernels admit a Mercer decomposition in terms of Fourier modes <inline-formula><mml:math id="inf725"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> since the Fourier modes diagonalize shift invariant integral operators on <inline-formula><mml:math id="inf726"><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:math></inline-formula>. For the linear neuron, the kernel eigenvalues scale like <inline-formula><mml:math id="inf727"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mfrac><mml:msup><mml:mi>β</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, indicating infinite differentiability of the tuning curves. Since <inline-formula><mml:math id="inf728"><mml:msub><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> decays rapidly with <inline-formula><mml:math id="inf729"><mml:mi>n</mml:mi></mml:math></inline-formula>, we find that this Gabor code has an inductive bias that favors low frequency functions of orientation <inline-formula><mml:math id="inf730"><mml:mi>θ</mml:mi></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s14-2"><title>Nonlinear simple cells</title><p>Introducing nonlinear functions <inline-formula><mml:math id="inf731"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that map input currents <inline-formula><mml:math id="inf732"><mml:mi>z</mml:mi></mml:math></inline-formula> into the V1 population into firing rates, we can obtain a non-linear kernel <inline-formula><mml:math id="inf733"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which has the following definition<disp-formula id="equ80"><label>(79)</label><mml:math id="m80"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="false"><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In this setting, it is convenient to restrict <inline-formula><mml:math id="inf734"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and assume that the preferred wavevectors <inline-formula><mml:math id="inf735"><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are uniformly distributed over the circle. In this case, it suffices to identify a decomposition of the composed function <inline-formula><mml:math id="inf736"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the basis of Chebyshev polynomials <inline-formula><mml:math id="inf737"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which satisfy <inline-formula><mml:math id="inf738"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ81"><label>(80)</label><mml:math id="m81"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:msup><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which can be computed efficiently with an appropriate quadrature scheme. Once the coefficients <italic>a</italic><sub><italic>n</italic></sub> are determined, we can compute the kernel by first letting <inline-formula><mml:math id="inf739"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to be the angle between <inline-formula><mml:math id="inf740"><mml:mi mathvariant="bold">k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf741"><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and letting <inline-formula><mml:math id="inf742"><mml:mi>θ</mml:mi></mml:math></inline-formula> be the angle between <inline-formula><mml:math id="inf743"><mml:mi mathvariant="bold">k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf744"><mml:msup><mml:mi mathvariant="bold">k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula><disp-formula id="equ82"><label>(81)</label><mml:math id="m82"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Thus the kernel eigenvalues are <inline-formula><mml:math id="inf745"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><sec sec-type="appendix" id="s14-2-1"><title>Asymptotic scaling of spectra</title><p>Activation functions that encourage sparsity have slower eigenvalue decays. If the nonlinear activation function has the form <inline-formula><mml:math id="inf746"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>q</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, then the spectrum decays like <inline-formula><mml:math id="inf747"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. A simple argument justifies this scaling: if the function <inline-formula><mml:math id="inf748"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is only <inline-formula><mml:math id="inf749"><mml:mrow><mml:mi>q</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> times differentiable then <inline-formula><mml:math id="inf750"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mi>q</mml:mi></mml:msup></mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> since <inline-formula><mml:math id="inf751"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mi>q</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> must diverge. Therefore <inline-formula><mml:math id="inf752"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Note that this scaling is independent of the threshold. Examples of these scalings can be found in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref>.</p></sec></sec><sec sec-type="appendix" id="s14-3"><title>Phase variation, complex cells and invariance</title><p>We can consider a slightly more complicated model where Gabors and stimuli have phase shifts<disp-formula id="equ83"><label>(82)</label><mml:math id="m83"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The simple cells are generated by nonlinearity<disp-formula id="equ84"><label>(83)</label><mml:math id="m84"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The input currents into the simple V1 cells can be computed exactly<disp-formula id="equ85"><label>(84)</label><mml:math id="m85"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>When <inline-formula><mml:math id="inf753"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the simple cell tuning curves <inline-formula><mml:math id="inf754"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> only depend on <inline-formula><mml:math id="inf755"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf756"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, allowing a Fourier decomposition<disp-formula id="equ86"><label>(85)</label><mml:math id="m86"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The simple cell kernel <inline-formula><mml:math id="inf757"><mml:msub><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, therefore decomposes into Fourier modes over <inline-formula><mml:math id="inf758"><mml:mi>θ</mml:mi></mml:math></inline-formula><disp-formula id="equ87"><label>(86)</label><mml:math id="m87"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf759"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. It therefore suffices to solve the infinite sequence of integral eigenvalue problems over <inline-formula><mml:math id="inf760"><mml:mi>ϕ</mml:mi></mml:math></inline-formula><disp-formula id="equ88"><label>(87)</label><mml:math id="m88"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>⟹</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>With this choice it is straightforward to verify that the kernel eigenfunctions are <inline-formula><mml:math id="inf761"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with corresponding eigenvalue <inline-formula><mml:math id="inf762"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Since <italic>b</italic><sub><italic>n</italic></sub> is not translation invariant in <inline-formula><mml:math id="inf763"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, the eigenfunctions <inline-formula><mml:math id="inf764"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are not necessarily Fourier modes. These eigenvalue problems for <italic>b</italic><sub><italic>n</italic></sub> must be solved numerically when using arbitrary nonlinearity <inline-formula><mml:math id="inf765"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The top eigenfunctions of the simple cell kernel depend heavily on the phase of the two grating stimuli <inline-formula><mml:math id="inf766"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. Thus, a pure orientation discrimination task which is independent of phase requires a large number of samples to learn with the simple cell population.</p></sec><sec sec-type="appendix" id="s14-4"><title>Complex cell populations are phase invariant</title><p>V1 also contains complex cells which possess invariance to the phase <inline-formula><mml:math id="inf767"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> of the stimulus.<disp-formula id="equ89"><label>(88)</label><mml:math id="m89"><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Again using Gabor filters we model the complex cell responses with a quadratic nonlinearity and sum over two squared filters which are phase shifted by <inline-formula><mml:math id="inf768"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula><disp-formula id="equ90"><label>(89)</label><mml:math id="m90"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which we see is independent of the phase <inline-formula><mml:math id="inf769"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> of the grating stimulus. Integrating over the set of possible Gabor filters <inline-formula><mml:math id="inf770"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf771"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> again gives the following kernel for the complex cells<disp-formula id="equ91"><label>(90)</label><mml:math id="m91"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Remarkably, this kernel is independent of the phase <inline-formula><mml:math id="inf772"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> of the grating stimulus. Thus, complex cell populations possess good inductive bias for vision tasks where the target function only depends on the orientation of the stimulus rather than it’s phase. In reality, V1 is a mixture of simple and complex cells. Let <inline-formula><mml:math id="inf773"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the relative proportion of neurons which are simple cells and <inline-formula><mml:math id="inf774"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the relative proportion of complex cells. The kernel for the mixed V1 population is given by a simple convex combination of the simple and complex cell kernels<disp-formula id="equ92"><label>(91)</label><mml:math id="m92"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf775"><mml:mi>n</mml:mi></mml:math></inline-formula> denotes neuron type (simple vs complex, tuning etc) and <inline-formula><mml:math id="inf776"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are probability distributions over the V1 neuron identities, the simple cell identities and the complex cell identities respectively. Increasing <inline-formula><mml:math id="inf777"><mml:mi>s</mml:mi></mml:math></inline-formula> increases the phase dependence of the code by giving greater weight to the simple cell population. Decreasing <inline-formula><mml:math id="inf778"><mml:mi>s</mml:mi></mml:math></inline-formula> gives weight to the complex cell population, encouraging phase invariance of readouts.</p></sec><sec sec-type="appendix" id="s14-5"><title>Visualization of feedforward Gabor V1 model and induced kernels</title><p>Examples of the induced kernels for the Gabor-bank V1 model are provided in <xref ref-type="fig" rid="fig5">Figure 5</xref>. We show how choice of rectifying nonlinearity <inline-formula><mml:math id="inf779"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and sparsifying threshold <inline-formula><mml:math id="inf780"><mml:mi>a</mml:mi></mml:math></inline-formula> influence the kernel and their spectra. Learning curves for simple orientation tasks are provided.</p></sec><sec sec-type="appendix" id="s14-6"><title>Gabor model spectral bias and fit to V1 data</title><p>Motivated by findings in the primary visual cortex (<xref ref-type="bibr" rid="bib44">Hansel and van Vreeswijk, 2002</xref>; <xref ref-type="bibr" rid="bib74">Miller and Troyer, 2002</xref>; <xref ref-type="bibr" rid="bib91">Priebe et al., 2004</xref>; <xref ref-type="bibr" rid="bib92">Priebe and Ferster, 2008</xref>), we studied the spectral bias induced by rectified power-law nonlinearities of the form <inline-formula><mml:math id="inf781"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>q</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. From theory, such a power-law activation function arises in a spiking neuron when firing is driven by input fluctuations (<xref ref-type="bibr" rid="bib44">Hansel and van Vreeswijk, 2002</xref>; <xref ref-type="bibr" rid="bib74">Miller and Troyer, 2002</xref>). Further, this activation is observed in intracellular recordings over the dynamic range of neurons in primary visual cortex (<xref ref-type="bibr" rid="bib92">Priebe and Ferster, 2008</xref>). For example, in cats, the power, <inline-formula><mml:math id="inf782"><mml:mi>q</mml:mi></mml:math></inline-formula>, ranges from 2.7 to 3.9 (<xref ref-type="bibr" rid="bib91">Priebe et al., 2004</xref>). We fit parameters of our model to the Mouse V1 kernel and compared to other parameter sets in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. Our best fit value of <inline-formula><mml:math id="inf783"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1.7</mml:mn></mml:mrow></mml:math></inline-formula> is lower but on par with the estimates from the cat and reproduces the observed kernel. Computation of the kernel and its eigenvalues (Appendix Nonlinear simple cells) indicates a low frequency bias: the eigenvalues for low frequency modes are higher than those for high frequency modes, indicating a strong inductive bias to learn functions of low frequency in the orientation. Decreasing sparsity (lower <inline-formula><mml:math id="inf784"><mml:mi>a</mml:mi></mml:math></inline-formula>) leads to a faster decay in the spectrum (but similar asymptotic scaling at the tail, see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref>) and a stronger bias towards lower frequency functions (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The effect of the power of nonlinearity <inline-formula><mml:math id="inf785"><mml:mi>q</mml:mi></mml:math></inline-formula> is more nuanced: increasing power may increase spectra at lower frequencies, but may also lead to a faster decay at the tail (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref> ). In general, an exponent <inline-formula><mml:math id="inf786"><mml:mi>q</mml:mi></mml:math></inline-formula> implies a power-law asymptotic spectral decay <inline-formula><mml:math id="inf787"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf788"><mml:mrow><mml:mi>k</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> (Appendix Nonlinear simple cells). The behavior at low frequencies may have significant impact for learning with few samples. Overall, our findings show that the spectral bias of a population code can be determined in non-trivial ways by its biophysical parameters, including neural thresholds and nonlinearities.</p></sec><sec sec-type="appendix" id="s14-7"><title>Energy model with partially phase-selective cells</title><p>The model of the V1 population as a mixture of purely simple and purely complex cells is an idealization which fails to capture the variability in phase selectivity of cells observed in experiment. In this section, we describe a simple model which can interpolate between an invariant code and a code which has high alignment with phase-dependent eigenfunctions. Further, a single scalar parameter <inline-formula><mml:math id="inf789"><mml:mi>α</mml:mi></mml:math></inline-formula> will control how strongly the population is biased towards invariance. We define <inline-formula><mml:math id="inf790"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for nonlinear function <inline-formula><mml:math id="inf791"><mml:mi>g</mml:mi></mml:math></inline-formula> and scalar <inline-formula><mml:math id="inf792"><mml:mi>z</mml:mi></mml:math></inline-formula> which is constructed as follows<disp-formula id="equ93"> <label>(92)</label><mml:math id="m93"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mo lspace="12.5pt">+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This linear combination is inspired by the construction of simple cells in Dayan &amp; Abbot Chapter 2 (<xref ref-type="bibr" rid="bib27">Dayan and Abbott, 2001</xref>). If all <inline-formula><mml:math id="inf793"><mml:mi>β</mml:mi></mml:math></inline-formula> are equal, then this tuning curve is invariant to phase <inline-formula><mml:math id="inf794"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. To generate variability in selectivity to phase <inline-formula><mml:math id="inf795"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, we will draw <inline-formula><mml:math id="inf796"><mml:mi mathvariant="bold-italic">β</mml:mi></mml:math></inline-formula> from a Dirichlet distrbution on the simplex with concentration parameter <inline-formula><mml:math id="inf797"><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:math></inline-formula> so that <inline-formula><mml:math id="inf798"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:msubsup><mml:mi>β</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf799"><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. In the <inline-formula><mml:math id="inf800"><mml:mrow><mml:mi>α</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> limit, the probability density concentrates on <inline-formula><mml:math id="inf801"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:math></inline-formula>, leading to a code comprised entirely of complex cells which are invariant to phase <inline-formula><mml:math id="inf802"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. In the <inline-formula><mml:math id="inf803"><mml:mrow><mml:mi>α</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> limit, the density is concentrated around the “edges” of the simplex such as <inline-formula><mml:math id="inf804"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where only one preferred phase is present per neuron. For intermediate values, neurons are partially selective to phase. As before, the selectivity or invariance to phase is manifested in the kernel decomposition and leads to similar learning curves for the three tasks of the main paper (Orientation, Phase, Hybrid). We provide an illustration of tuning curves, F1/F0 distributions, eigenfunctions, and learning curves in <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>.</p></sec></sec><sec sec-type="appendix" id="s15"><title>Time dependent neural codes</title><sec sec-type="appendix" id="s15-1"><title>RNN model and decomposition</title><p>In this setting, the population code <inline-formula><mml:math id="inf805"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a function of an input stimulus sequence <inline-formula><mml:math id="inf806"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and time <inline-formula><mml:math id="inf807"><mml:mi>t</mml:mi></mml:math></inline-formula>. In general the neural code <inline-formula><mml:math id="inf808"><mml:mi mathvariant="bold">r</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf809"><mml:mi>t</mml:mi></mml:math></inline-formula> can depend on the entire history of the stimulus input <inline-formula><mml:math id="inf810"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf811"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>≤</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, as is the case for recurrent neural networks. We denote dependence of a function <inline-formula><mml:math id="inf812"><mml:mi>f</mml:mi></mml:math></inline-formula> on <inline-formula><mml:math id="inf813"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in this causal manner with the notation <inline-formula><mml:math id="inf814"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In a learning task, a set of readout weights <inline-formula><mml:math id="inf815"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula> are chosen so that a downstream linear readout <inline-formula><mml:math id="inf816"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> approximates a target sequence <inline-formula><mml:math id="inf817"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which maps input stimulus sequences to output scalar sequences. The quantity of interest is the generalization <inline-formula><mml:math id="inf818"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula>, which in this case is an average over both input sequences and time, <inline-formula><mml:math id="inf819"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The average is computed over a distribution of input stimulus sequences <inline-formula><mml:math id="inf820"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To train the readout, <inline-formula><mml:math id="inf821"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>, the network is given a sample of <inline-formula><mml:math id="inf822"><mml:mi>P</mml:mi></mml:math></inline-formula> stimulus sequences <inline-formula><mml:math id="inf823"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For the μ-th training input sequence, the target system <inline-formula><mml:math id="inf824"><mml:mi>y</mml:mi></mml:math></inline-formula> is evaluated at a set of discrete time points <inline-formula><mml:math id="inf825"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> giving a collection of target values <inline-formula><mml:math id="inf826"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and a total dataset of size <inline-formula><mml:math id="inf827"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The <italic>average case generalization</italic> computes a further average of the generalization error <inline-formula><mml:math id="inf828"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> over randomly sampled datasets of size <inline-formula><mml:math id="inf829"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Learning is again achieved through iterated weight updates with delta-rule form, but now have contributions from both sequence index and time <inline-formula><mml:math id="inf830"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. As before, optimization of the readout weights is equivalent to kernel regression with a kernel that computes inner products of neural population vectors at different times <inline-formula><mml:math id="inf831"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for different input sequences <inline-formula><mml:math id="inf832"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> . This kernel depends on details of the time varying population code including its recurrent intrinsic dynamics as well as its encoding of the time-varying input stimuli. The optimization problem and delta rule described above converge to the kernel regression solution for kernel gram matrix <inline-formula><mml:math id="inf833"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>t</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib30">Dong et al., 2020</xref>; <xref ref-type="bibr" rid="bib131">Yang, 2019</xref>; <xref ref-type="bibr" rid="bib132">Yang, 2020</xref>). The learned function has the form <inline-formula><mml:math id="inf834"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf835"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for kernel gram matrix <inline-formula><mml:math id="inf836"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> which is computed for the entire set of training sequences, and the vector <inline-formula><mml:math id="inf837"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the vector containing the desired target outputs for each sequence. Assuming a probability distribution over sequences <inline-formula><mml:math id="inf838"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the kernel can be diagonalized with orthonormal eigenfunctions <inline-formula><mml:math id="inf839"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Our theory carries over from the static case: kernels whose top eigenfunctions have high alignment with the target dynamical system <inline-formula><mml:math id="inf840"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will achieve the best average case generalization performance.</p></sec></sec><sec sec-type="appendix" id="s16"><title>Alternative neural codes with same kernel</title><sec sec-type="appendix" id="s16-1"><title>Orthogonal transformations are sufficient for linear kernel-preserving transformations</title><p>We will now show that for any linear transformation <inline-formula><mml:math id="inf841"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Ar</mml:mi></mml:mrow></mml:math></inline-formula> which preserves the inner product kernel <inline-formula><mml:math id="inf842"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, there exists an orthogonal matrix <inline-formula><mml:math id="inf843"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf844"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Qr</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p><italic>Proof</italic>.</p><p>Let <inline-formula><mml:math id="inf845"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ar</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for all stimuli <inline-formula><mml:math id="inf846"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>. To preserve the kernel, we must have<disp-formula id="equ94"><label>(93)</label><mml:math id="m94"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟹</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">Ar</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Taking projections against each of the orthonormal eigenfunctions <inline-formula><mml:math id="inf847"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see Appendix Singular value decomposition of continuous population responses), we define vectors <inline-formula><mml:math id="inf848"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> as <inline-formula><mml:math id="inf849"><mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, allowing us to express the SVD of the population code <inline-formula><mml:math id="inf850"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. These vectors <inline-formula><mml:math id="inf851"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are orthonormal <inline-formula><mml:math id="inf852"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> since, by the definition of the kernel eigenfunctions <inline-formula><mml:math id="inf853"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>,<disp-formula id="equ95"><label>(94)</label><mml:math id="m95"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf854"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf855"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> have the same inner product kernel, they must posess the same kernel eigenfunctions <inline-formula><mml:math id="inf856"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and kernel eigenvalues <inline-formula><mml:math id="inf857"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, which are identified through the eigenvalue problem<disp-formula id="equ96"><label>(95)</label><mml:math id="m96"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We therefore have the following two singular value decompositions for <inline-formula><mml:math id="inf858"><mml:mi mathvariant="bold">r</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf859"><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula><disp-formula id="equ97"><label>(96)</label><mml:math id="m97"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf860"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf861"><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:math></inline-formula> are both complete sets of orthonormal vectors (the sums above run over possible zero eigenvalues). Taking the equation <inline-formula><mml:math id="inf862"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ar</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we multiply both sides of the equation by <inline-formula><mml:math id="inf863"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and average over <inline-formula><mml:math id="inf864"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> giving<disp-formula id="equ98"><label>(97)</label><mml:math id="m98"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">Au</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>For an eigenmode <inline-formula><mml:math id="inf865"><mml:mi>k</mml:mi></mml:math></inline-formula> with positive eigenvalue <inline-formula><mml:math id="inf866"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, this implies <inline-formula><mml:math id="inf867"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">Au</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, while there is no corresponding constraint for the null modes with <inline-formula><mml:math id="inf868"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. However, the action of <inline-formula><mml:math id="inf869"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> on the nullspace of the code has no influence on <inline-formula><mml:math id="inf870"><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> so there is no loss in generality to restrict consideration to transformations <inline-formula><mml:math id="inf871"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> which satisfy <inline-formula><mml:math id="inf872"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">Au</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf873"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (rather than just the <inline-formula><mml:math id="inf874"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> modes). This choice gives <inline-formula><mml:math id="inf875"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">u</mml:mi><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, the space of codes <inline-formula><mml:math id="inf876"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with equivalent kernels to <inline-formula><mml:math id="inf877"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> generated through linear transformations is equivalent to all possible orthogonal transformations of the original code <inline-formula><mml:math id="inf878"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold">Qr</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">QQ</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. ∎</p></sec><sec sec-type="appendix" id="s16-2"><title>Effect of noise on RROS symmetry</title><p>The random rotation and optimal shift (RROS) operations introduced in the main text preserve generalization performance under the assumption of a deterministic neural code. However, for noisy codes, the presence of RROS symmetry is dependent on the noise distribution. Below we discuss two commonly analyzed distributions: the Gaussian distribution and the Poisson distribution. For Gaussian noise, the RROS operations preserve the generalization performance and the local Fisher information. However, if noise is constrained to be Poisson then RROS operations do not preserve generalization or Fisher information.</p><p>First, we will analyze stimulus dependent Gaussian noise, where generalization performance is preserved under rotations and baseline shifts. Note that if the code at <inline-formula><mml:math id="inf879"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> obeyed <inline-formula><mml:math id="inf880"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, then the rotated and shifted code follows <inline-formula><mml:math id="inf881"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This rotated and shifted code <inline-formula><mml:math id="inf882"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Qr</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:math></inline-formula>, when centered, will exhibit identical generalization performance as the original code. This is true both for learning from a trial averaged or non-trial averaged code. In the case of Gaussian noise on a centered code, the dataset transforms under a rotation as <inline-formula><mml:math id="inf883"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The optimal weights for a linear model similarly transform as <inline-formula><mml:math id="inf884"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Under these transformations the predictor on test point <inline-formula><mml:math id="inf885"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> is unchanged since<disp-formula id="equ99"><label>(98)</label><mml:math id="m99"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Further, the local Fisher information matrix is <inline-formula><mml:math id="inf886"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is unchanged under the transformation <inline-formula><mml:math id="inf887"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mi mathvariant="bold">Qr</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Under this transformation, the covariance simply transforms linearly <inline-formula><mml:math id="inf888"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the <inline-formula><mml:math id="inf889"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> matrices will annihilate under the trace. This shows that, for some noise models, our assumption that rotations and baseline shifts preserve generalization performance will be valid.</p><p>However, for Poisson noise, where the variance is tied to the mean firing rate, the RROS operations will not preserve noise structure or information content. The Fisher information at scalar stimulus <inline-formula><mml:math id="inf890"><mml:mi>θ</mml:mi></mml:math></inline-formula> for a Poisson neuron is <inline-formula><mml:math id="inf891"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. A baseline shift <inline-formula><mml:math id="inf892"><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to the tuning curve will not change the numerator since the derivative of the tuning curve is invariant to this transformation, but it will increase the denominator.</p></sec><sec sec-type="appendix" id="s16-3"><title>Necessary conditions for optimally sparse codes</title><p>Next we argue why optimally sparse codes should be lifetime and population selective. We consider the following optimization problem: find a non-negative neural responses <inline-formula><mml:math id="inf893"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and baseline vector <inline-formula><mml:math id="inf894"><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> so that baseline subtracted responses <inline-formula><mml:math id="inf895"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> realize a desired inner product kernel <inline-formula><mml:math id="inf896"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and have minimal total firing. This is equivalent to finding the most metabolically efficient code among the space of codes with equivalent inductive bias. Mathematically, we formulate this problem as<disp-formula id="equ100"><label>(99)</label><mml:math id="m100"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mtext>s.t.</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mrow><mml:mpadded width="+7.8pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>To enforce the constraints for the definition of the kernel and the non-negativity of the responses, we introduce the following Lagrangian<disp-formula id="equ101"><label>(100)</label><mml:math id="m101"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>S1</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">V</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where 1 is the vector containing all ones, the Lagrange multiplier matrix <inline-formula><mml:math id="inf897"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> enforces the definition of the kernel and the KKT multiplier matrix <inline-formula><mml:math id="inf898"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula> enforces the non-negativity constraints for each element of <inline-formula><mml:math id="inf899"><mml:mi mathvariant="bold">S</mml:mi></mml:math></inline-formula>. The KKT conditions require that any local optimum of the objective would have to satisfy the following equations (<xref ref-type="bibr" rid="bib58">Kuhn and Tucker, 2014</xref>)<disp-formula id="equ102"><label>(101)</label><mml:math id="m102"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mn>11</mml:mn><mml:mo>⊤</mml:mo></mml:msup><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">A1</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>⊙</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf900"><mml:mo>⊙</mml:mo></mml:math></inline-formula> denotes the element-wise Hadamard product. Using the complementary slackness condition <inline-formula><mml:math id="inf901"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></inline-formula>, and the first optimality condition <inline-formula><mml:math id="inf902"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></inline-formula>, we have<disp-formula id="equ103"><label>(102)</label><mml:math id="m103"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊙</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Therefore, for any neuron-stimulus pair <inline-formula><mml:math id="inf903"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, either <inline-formula><mml:math id="inf904"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf905"><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Further, under the condition that K is full rank, we conclude that for any stimulus μ, <inline-formula><mml:math id="inf906"><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> from the equation <inline-formula><mml:math id="inf907"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf908"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represent the set of stimuli for which neuron <inline-formula><mml:math id="inf909"><mml:mi>i</mml:mi></mml:math></inline-formula> fires. We will call this the <italic>receptive field set</italic> for neuron <inline-formula><mml:math id="inf910"><mml:mi>i</mml:mi></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf911"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> have entries<disp-formula id="equ104"><label>(103)</label><mml:math id="m104"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>μ</mml:mi><mml:mo>∉</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mtext>or</mml:mtext><mml:mtext> </mml:mtext><mml:mi>ν</mml:mi><mml:mo>∉</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the matrix <inline-formula><mml:math id="inf912"><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf913"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> minor of <inline-formula><mml:math id="inf914"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> obtained by taking all rows and columns with indices <inline-formula><mml:math id="inf915"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf916"><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> denotes pseudo-inverse of <inline-formula><mml:math id="inf917"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula>. Then the <inline-formula><mml:math id="inf918"><mml:mi>i</mml:mi></mml:math></inline-formula>-th neuron’s tuning curve is a function of the index set <inline-formula><mml:math id="inf919"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the baseline <inline-formula><mml:math id="inf920"><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and the neuron-independent <inline-formula><mml:math id="inf921"><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf922"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The non-negativity constraint for neuron <inline-formula><mml:math id="inf923"><mml:mi>i</mml:mi></mml:math></inline-formula>’s tuning curve implies that <inline-formula><mml:math id="inf924"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>ν</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf925"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. To satisfy the definition of the kernel, we have the following constraint on the matrix <inline-formula><mml:math id="inf926"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula>, the index sets <inline-formula><mml:math id="inf927"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and baselines <inline-formula><mml:math id="inf928"><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula><disp-formula id="equ105"><label>(104)</label><mml:math id="m105"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>K</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This equation implictly defines the index sets <inline-formula><mml:math id="inf929"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the baselines <inline-formula><mml:math id="inf930"><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and the KKT matrix <inline-formula><mml:math id="inf931"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula>. We see that, in order to fit an arbitrary kernel, the receptive field sets <inline-formula><mml:math id="inf932"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and baselines <inline-formula><mml:math id="inf933"><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> for each neuron must be sufficiently diverse since otherwise only a low rank kernel matrix can be achieved from the optimally sparse code. As a concrete example, suppose that <inline-formula><mml:math id="inf934"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi></mml:mrow></mml:math></inline-formula> so that <inline-formula><mml:math id="inf935"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf936"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf937"><mml:mi>i</mml:mi></mml:math></inline-formula>. For example, this could occur if each neuron fired for every possible stimulus. In this case, the kernel would be rank one: <inline-formula><mml:math id="inf938"><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mo>,</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mo>,</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. In order to achieve a higher rank code there must be sufficient diversity of the receptive fields <inline-formula><mml:math id="inf939"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. Thus the only way for optimally sparse codes to realize high rank kernels <inline-formula><mml:math id="inf940"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> is to have neurons to have different receptive field sets <inline-formula><mml:math id="inf941"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. The necessary optimality conditions thus reveal a preference for sparse neural tuning curves to have high <italic>lifetime sparseness</italic>; to achieve diverse index sets <inline-formula><mml:math id="inf942"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, any given neuron will fire only for a unique subset of the possible stimuli.</p></sec></sec><sec sec-type="appendix" id="s17"><title>Impact of neural noise and unlearnable targets on learning</title><p>While our analysis so far has focused on deterministic population codes, our theory can be extended to neural populations which exhibit variability in responses to identical stimuli. For each stimulus <inline-formula><mml:math id="inf943"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>, we let the population response <inline-formula><mml:math id="inf944"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be a random vector with mean <inline-formula><mml:math id="inf945"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and covariance <inline-formula><mml:math id="inf946"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>The (deterministic) target function can be decomposed in terms of the mean response as <inline-formula><mml:math id="inf947"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (the usual decomposition <inline-formula><mml:math id="inf948"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> gives an unphysical target function which fluctuates with the variability in neural responses). For a given configuration of weights <inline-formula><mml:math id="inf949"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>, the generalization error (which is an average over the joint distribution of <inline-formula><mml:math id="inf950"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math></inline-formula>) is determined only by the signal <inline-formula><mml:math id="inf951"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and noise <inline-formula><mml:math id="inf952"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> correlation matrices:<disp-formula id="equ106"><label>(105)</label><mml:math id="m106"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where we utilized the fact that <inline-formula><mml:math id="inf953"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to eliminate the cross-term. The two terms in the final expression can be thought of as a bias-variance decomposition over the noise in neural responses. The minimum achievable loss can be obtained by differentiation of the generalization error expression with respect to <inline-formula><mml:math id="inf954"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>, giving <inline-formula><mml:math id="inf955"><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mi>g</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. We note that any noise correlation matrix with noise orthogonal to coding direction <inline-formula><mml:math id="inf956"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> will give the minimal (zero) asymptotic error. Alignment of the noise <inline-formula><mml:math id="inf957"><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id="inf958"><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> gives higher asymptotic error.</p><p>In addition to the irreducible error, the presence of neural noise can alter the learning curve at finite <inline-formula><mml:math id="inf959"><mml:mi>P</mml:mi></mml:math></inline-formula>. An analytical study of this is difficult, which we leave for future work. We numerically study the effect of neural variability on generalization performance in the orientation discrimination tasks for non-trial-averaged Mouse V1 code in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> . We note that the generalization error is worse at each finite value of <inline-formula><mml:math id="inf960"><mml:mi>P</mml:mi></mml:math></inline-formula> when compared to trial averaged (noise free) learning curves. We varied the regularization parameter and did not find an obvious non-zero optimal weight decay <inline-formula><mml:math id="inf961"><mml:mi>λ</mml:mi></mml:math></inline-formula>, consistent with small noise levels.</p><p>Neural noise is not the only phenomenon that can degrade task learning. Codes which are incapable of expressing the target function through linear readouts are also susceptible to overfitting. As explained in <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>, the components of the target function that are inexpressible act as a source of noise on the learning process which can overfit this noise. Such a scenario can occur, for example, when the readout neuron only gets input from a sparse subset of the coding neural population (<xref ref-type="bibr" rid="bib102">Seeman et al., 2018</xref>). We show in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C-D</xref> that using subsampled populations of size <inline-formula><mml:math id="inf962"><mml:mi>N</mml:mi></mml:math></inline-formula> can indeed lead to a regime where more data can hurt performance leading to an overfitting error peak, a subsequent non-vanishing asymptotic error, and an optimal weight decay parameter <inline-formula><mml:math id="inf963"><mml:mi>λ</mml:mi></mml:math></inline-formula>. This phenomenon is known as double descent in machine learning literature (<xref ref-type="bibr" rid="bib13">Belkin et al., 2019</xref>; <xref ref-type="bibr" rid="bib70">Mei and Montanari, 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). At small <inline-formula><mml:math id="inf964"><mml:mi>N</mml:mi></mml:math></inline-formula>, these codes are not sufficiently expressive to learn the target function through linear readout. The overfitting peak occurs near the interpolation threshold, the largest value of <inline-formula><mml:math id="inf965"><mml:mi>P</mml:mi></mml:math></inline-formula> where all training sets could be perfectly fit in the <inline-formula><mml:math id="inf966"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> limit (<xref ref-type="bibr" rid="bib20">Canatar et al., 2021</xref>). At infinite <inline-formula><mml:math id="inf967"><mml:mi>P</mml:mi></mml:math></inline-formula>, generalization error asymptotes to the amount of unexplained variance in the target function.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Neural noise and subsampled neural codes can lead to overfitting.</title><p>(<bold>A</bold>) The learning curves without trial averaging (solid) and with trial averaging (dashed) for the high and low frequency orientation discrimination task. In principle, neural noise could limit asymptotic performance and lead to the existence of an optimal weight decay parameter <inline-formula><mml:math id="inf968"><mml:mi>λ</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Performance at <inline-formula><mml:math id="inf969"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> vs ridge <inline-formula><mml:math id="inf970"><mml:mi>λ</mml:mi></mml:math></inline-formula> shows that there is not an optimal weight decay parameter. (<bold>C</bold>) Generalization of readouts trained on subsets of <inline-formula><mml:math id="inf971"><mml:mi>N</mml:mi></mml:math></inline-formula> V1 neurons exhibit non-monotonic learning curves with an overfitting peak around <inline-formula><mml:math id="inf972"><mml:mrow><mml:mi>P</mml:mi><mml:mo>≈</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. (<bold>D</bold>) The performance of subsamples of <inline-formula><mml:math id="inf973"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons as a function of the weight decay parameter <inline-formula><mml:math id="inf974"><mml:mi>λ</mml:mi></mml:math></inline-formula> at <inline-formula><mml:math id="inf975"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> samples show that, for sufficiently small <inline-formula><mml:math id="inf976"><mml:mi>N</mml:mi></mml:math></inline-formula>, there is a non-zero optimal <inline-formula><mml:math id="inf977"><mml:mi>λ</mml:mi></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78606-app1-fig1-v2.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78606.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>This important study presents a theory of generalization in neural population codes and proposes sample efficiency as a new normative principle. The theory can be used to identify the set of 'easily learnable' stimulus-response mappings from neural data and makes strong behavioral predictions that can be evaluated experimentally. Overall, the new method for elucidating inductive biases of the brain is highly compelling and will be of interest to theoretical and experimental neuroscientists working towards understanding how the cortex works.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78606.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Anselmi</surname><given-names>Fabio</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Baylor College of Medicine</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Beck</surname><given-names>Jeff</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00py81415</institution-id><institution>Duke University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/%2010.1101/2021.03.30.437743">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.03.30.437743v3">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Population codes enable learning from few examples by shaping inductive bias&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Fabio Anselmi (Reviewer #1); Jeff Beck (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The consensus among all 3 reviewers is that the manuscript describes an insightful and novel mathematical framework for evaluating the ability of a downstream linear decoder to learn new stimulus-response mappings from relatively few training examples. However, there is also broad consensus among the 3 reviewers that (1) the manuscript is too technical and written in a way that may not be palatable to the <italic>eLife</italic> readership (to paraphrase one of the reviewers &quot;I don't think that many experimentalists will immediately see the sheer utility of having a tool like this in their arsenal&quot; and that 2) some of the assumptions made are overly simplistic and the theory needs to be extended with more realistic neural assumptions regarding noise, readout and kernels.</p><p>Wrt (1), before the manuscript can be accepted for publication, it is necessary that the authors expand on possible applications to neural and behavioral experiments. The authors also need to distill the math better, provide more intuitive explanations, and reduce the jargon. Wrt (2) R2 provided a detailed lists of comments with suggestions for more realistic assumptions.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors provide a simple and clear way to understand an aspect of the implicit bias of a neural population code linking it with well-known machine learning methods and concepts such as kernel regression, sample complexity and efficiency.</p><p>Although the mathematical results the authors employ are not novel, the way they apply them to the problem of neural coding is novel and interesting to a broad audience.</p><p>In particular, the computational neuroscience community can benefit from this work being it is one of the few dealing with the impact of the model implicit bias in explaining real data.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>It is my opinion that the principle utility of this approach lies in its ability to identify the set of 'easily learnable' stimulus-response mappings from neural data which makes strong behavioral predictions that can be easily evaluated. I envision a simple experiment in which empirically obtained kernel functions are used to rank stimulus-response mappings according to their learnability which can then be plotted against measures of performance like the observed learning rate and saturated performance. Because kernel functions are empirically obtained, there is even the potential for meaningful cross-species comparisons. If behaviorally validated, one could also use this approach to label cortical populations by the set of easily learned stimulus-response mappings for that population. This allows for the identification of task-relevant neurons or regions which can be subsequently manipulated to enhance or degrade learning rates.</p><p>Of course, any theoretical approach is only as good as the underlying assumptions and so while the primary strength is the simplicity and generality of this approach, the primary weakness is its neglect of some very real and very relevant aspects of neural data in particular and statistical learning in general. In particular, the three principle limitations of this work are tied to its reliance on the assumptions that (1) neurons are noiseless, (2) decoders are linear, and (3) learned weights are unbiased.</p><p>(1) Within this framework, a realistic stimulus-dependent noise model can be easily introduced and its effects on the kernel and set of easily learned stimulus-response mappings investigated. So while the kernel would be substantially altered via the addition of a realistic noise model, the applications of the approach outlined above would not be affected. The same cannot be said for the efficient coding application described in this manuscript. There, the authors note that rotations and constant shifts of neural activity do not affect the kernel and thus do not affect the generalization error. This kernel invariance is not present when a non-trivial (i.e. non-isotropic) noise model is added. For example, suppose that neurons are independent and Poisson so that noise scales with the mean of the neural response. In this case, adding a baseline firing rate to a population of unimodal neurons representing orientation necessarily reduces the information content of the population while rotations can affect the fidelity with which certain stimulus values are represented. It is important to note, however, that while this particular efficiency result is not compelling, I believe that it is possible to perform a similar analysis that takes into account realistic noise models and focuses on a broad set of 'biologically plausible' kernels instead of particular invariant ones. For example, one could consider noise covariance structures with differential correlations (Moreno-Bote 2014). Since the magnitude of differential correlations controls the redundancy of the population code this would enable an analysis of the role of redundancy in suppressing (or enhancing) generalization error.</p><p>(2) Similarly, the linearity assumption is somewhat restrictive. Global linear decoders of neural activity are known to be highly inefficient and completely fail when decoding orientation in the primary visual cortex in the presence of contrast fluctuations. This is because contrast modulates the amplitude of the neural response and doubling the amplitude means doubling an estimate obtained from a linear decoder even when the underlying orientation has not changed. While the contrast issue could be partially addressed by simply considering normalized neural responses, it is not yet clear how to extend this approach to account for other sources of neural variability and co-variability that cause global linear decoders to fail so badly.</p><p>(3) This analysis relies on the assumption that decoder weights learned in the presence of finite data are efficient and unbiased. This assumption is problematic particularly when it comes to inductive bias and generalization error. This is because a standard way to reduce generalization error is to introduce bias into the learned decoder weights through a penalization scheme that privileges decoder weights with small magnitudes. This kind of regularization is particularly important when neurons are noisy. Fortunately, this issue could be addressed by parameterizing changes in the kernel function by the degree and type of regularization potentially leading to a more general result.</p><p>Finally, I would like to conclude by explicitly stating that while the limitations imposed by the assumptions listed above temper my enthusiasm in regards to conclusions drawn in this work, I do not believe there is some fundamental problem with the general theoretical framework. Indeed, items 1 and 3 above can be easily addressed through straightforward extensions of the authors approach and I look forward to their implementation. Item 2 is a bit more troublesome, but my intuition tells me that an information-theoretic extension based upon Fisher information may be capable of eliminating all three of these limiting assumptions by exploiting the relationship between FI(\theta) and FI(y=f(\theta)).</p><p>Ultimately, all I felt the need to say is in the public part of the review. But I wanted to use this space to insure that it was clear that I feel that this approach has a lot of potential and that I very much hope to see it extended/generalized.</p><p>If I were to make any suggestions they would be:</p><p>(1) a discussion of the consequences of biologically plausible noise models. I believe that this only requires simply augmenting K by a stimulus dependent noise covariance matrix.</p><p>(2) using this addition of a noise model to enable a discussion of the role of redundancy in generalization. This could be accomplished by considering perturbations to the noise covariance matrix that introduce differential correlations (Moreno-Bote 2014) of varying magnitude. More differential correlations means more redundancy and, I suspect, better generalization.</p><p>(3) the addition of a paragraph or two outlining some of the other ways that this learnability/generalization measure could be of use to physiologists and behavioral scientists. Training a mouse to do anything more complicated simple orientation detection is challenging to say the least. So having any tool that can be used to identify functions $y=f(\theta)$ that a mouse has a good chance of learning quickly would be highly advantageous. Similarly, by strategically subsampling neurons by tuning properties I suspect it may be possible to identify subpopulations of neurons that are particular important for learning certain functions. This is also cool because it allows physiologists to then target those populations for manipulation.</p><p>Of these (3) is probably the most important for this manuscript, because I don't think that many experimentalists will immediately see the sheer utility of having a tool like this in their arsenal.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The manuscript presents a theory of generalization performance in deterministic population codes, that applies to the case of small numbers of training examples. The main technical result, as far as I understand, is that generalization performance (the expected classification or regression error) of a population code depends exclusively on the 'kernel', i.e. a measure of the pairwise similarity between population activity patterns corresponding to different inputs. The main conceptual results are that, using this theory, one can understand the inductive biases of the code just from analyzing the kernel, particularly the top eigenfunctions; and that sample-efficient learning (low generalization performance with few samples) depends on whether the task is aligned with the population's inductive bias, that is, whether the target function (i.e. the true map from inputs to outputs) is aligned with the top eigenfunctions of the kernel. For instance, in mouse V1 data, they show that the top eigenfunctions correspond to low frequency functions of visual orientation (i.e. functions that map a broad range of similar orientations to similar output value), and that consistent with the theory, the generalization performance for small sample sizes is better for tasks defined by low frequency target functions. In my opinion, perhaps the most significant finding from a neuroscience perspective, is that the conditions for good generalization at low samples are markedly different from those in the large-sample asymptotic regime studies in Stringer et al. 2018 Nature: rather than a trade-off between high-dimensionality and differentiability proposed by Stringer et al., this manuscript shows that in the low-sample regime such codes can be disadvantageous for small sample sizes, that differentiability is not required, that the top eigenvalues matter more than the tail of the spectrum, and what matters is the alignment between the task and the top eigenfunctions. The authors propose sample-efficient learning/generalization as a new principle of neural coding, replacing or complementing efficient coding.</p><p>Overall, in my opinion this is a remarkable manuscript, presenting truly innovative theory with somewhat limited but convincing application to neural data. My main concern is that this is highly technical, dense, and long; the mathematical proofs for the theory are buried in the supplement and require knowledge of disparate techniques from statistical physics. Although some of that material on the theory of generalization is covered in previous publications by the authors, it was not clear to me if that is true for all of the technical results or only some.</p><p>Fixed population code, learnable linear readout: the authors acknowledge in the very last sentences of the manuscript that this is a limitation, given that neural tuning curves (the population neural code) are adaptable. I imagine extending the theory to both learnable codes and learnable readouts is hard and I understand it's beyond the scope of this paper. But perhaps the authors could motivate and discuss this choice, not just because of its mathematical convenience but also in relation to actual neural systems: when are these assumptions expected to be a good approximation of the real system?</p><p>The analysis of V1 data, showing a bias for low-frequency functions of orientation is convincing. But it could help if the authors provided some considerations on the kind of ethological behavioral context where this is relevant, or at least the design of an experimental behavioral task to probe it. Also related, it would be useful to construct and show a counter-example, a synthetic code for which the high-frequency task is easier.</p><p>Line 519, data preprocessing: related to the above, is it possible that binning together the V1 responses to gratings with different orientations (a range of 3.6 deg per bin, if I understood correctly) influences the finding of a low-frequency bias?</p><p>I found the study of invariances interesting, where the theory provides a normative prediction for the proportion of simple and complex cells. However, I would suggest the authors attempt to bring this analysis a step closer to the actual data: there are no pure simple and complex cells, usually the classification is based on responses to gratings phases (F1/F0) and real neurons take a continuum of values. Could the theory qualitatively predict that distribution?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78606.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The consensus among all 3 reviewers is that the manuscript describes an insightful and novel mathematical framework for evaluating the ability of a downstream linear decoder to learn new stimulus-response mappings from relatively few training examples. However, there is also broad consensus among the 3 reviewers that (1) the manuscript is too technical and written in a way that may not be palatable to the eLife readership to paraphrase one of the reviewers &quot;I don't think that many experimentalists will immediately see the sheer utility of having a tool like this in their arsenal&quot; and that (2) some of the assumptions made are overly simplistic and the theory needs to be extended with more realistic neural assumptions regarding noise, readout and kernels.</p><p>Wrt (1), before the manuscript can be accepted for publication, it is necessary that the authors expand on possible applications to neural and behavioral experiments. The authors also need to distill the math better, provide more intuitive explanations, and reduce the jargon. Wrt (2) R2 provided a detailed lists of comments with suggestions for more realistic assumptions.</p></disp-quote><p>We thank the editor for these points of advice. We respond to these points in detail below, in our point-by-point rebuttal to the reviewers. Here, we provide a summary of the measures taken to address each of these problems.</p><p>– Writing style too technical: We attempted to eliminate mathematical jargon and keep only the crucial mathematical components in the main text. The detailed discussion of the RNN model in the main text was shortened and the reference to Lie groups was removed. We added a paragraph at the beginning of the Results section where we define our mathematical notation explicitly. We also removed unnecessary detail about the V1 Gabor model from the main text. We added more intuition about what a kernel is and how it relates to distances between points in neural space. We also tried adding more intuition about the spectral bias result by giving the simpler expression for the average learned function in Methods Theory of Generalization starting at line 617.</p><p>– Contextualize Results for Experimental Audience: To describe how our theory of inductive bias may be useful for experimental neuroscience and could lead to new neural and behavioral experiments, we added a new paragraph at the end of the discussion.</p><p>– Assumptions Overly Simplistic: To address our previous model’s failure to model neural noise, we added a Appendix Section Impact of Neural Noise and Unlearnable Targets on Learning, where we discuss effects of noise on the learning curve. We allow for L2 regularization of the readout weights with a ridge penalty <italic>λ</italic> and show that there can be an optimal <italic>λ</italic> to prevent overfitting. We discuss nonlinear readouts in greater detail in App Typical Case Analysis of Nonlinear Readouts, showing that the kernel still governs the learned function for a wider set of learning problems. We discuss recent work, which computes learning curves for nonlinear readouts with a similar technique. Lastly, we provide a more realistic model of V1 tuning curves where neurons are neither completely simple or complex, but have a distribution of F1/F0 values (see App Energy Model with Partially Phase-Selective Cells). We discuss limitations of our work brought up in the reviews in our new discussion. Overall, we view extensions to address these present limitations as promising areas of future research.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The authors provide a simple and clear way to understand an aspect of the implicit bias of a neural population code linking it with well-known machine learning methods and concepts such as kernel regression, sample complexity and efficiency.</p><p>Although the mathematical results the authors employ are not novel, the way they apply them to the problem of neural coding is novel and interesting to a broad audience.</p><p>In particular, the computational neuroscience community can benefit from this work being it is one of the few dealing with the impact of the model implicit bias in explaining real data.</p></disp-quote><p>We thank the reviewer for insightful comments and the support. While addressing these comments, we had the chance to clarify some subtle points.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>It is my opinion that the principle utility of this approach lies in its ability to identify the set of 'easily learnable' stimulus-response mappings from neural data which makes strong behavioral predictions that can be easily evaluated. I envision a simple experiment in which empirically obtained kernel functions are used to rank stimulus-response mappings according to their learnability which can then be plotted against measures of performance like the observed learning rate and saturated performance. Because kernel functions are empirically obtained, there is even the potential for meaningful cross-species comparisons. If behaviorally validated, one could also use this approach to label cortical populations by the set of easily learned stimulus-response mappings for that population. This allows for the identification of task-relevant neurons or regions which can be subsequently manipulated to enhance or degrade learning rates.</p></disp-quote><p>We thank the reviewer for appreciating the possible applications of our work for future empirical studies on stimulus-response learning and inductive bias in neural systems. We are also excited by these possibilities for neuroscience experiments. We added a this as a discussion item where we suggest some ways our framework could be used in future empirical work. We discuss this addition further below.</p><disp-quote content-type="editor-comment"><p>Of course, any theoretical approach is only as good as the underlying assumptions and so while the primary strength is the simplicity and generality of this approach, the primary weakness is its neglect of some very real and very relevant aspects of neural data in particular and statistical learning in general. In particular, the three principle limitations of this work are tied to its reliance on the assumptions that (1) neurons are noiseless, (2) decoders are linear, and (3) learned weights are unbiased.</p></disp-quote><p>We thank this reviewer for encouraging us to explore additional realism in our model of sample efficient learning and acknowledge the limitations of our current analysis. Below we summarize each of the changes we made in response.</p><p>1. We updated the manuscript to allow for biased weights through ridge regression (regression with an additional L2 norm penalty <italic>λ</italic>|<bold>w</bold>|<sup>2</sup> on the weights) with ridge parameter <italic>λ</italic>. The effect of this parameter on generalization is easy to identify as it merely alters the implicit definition of <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:mfrac><mml:mtext>λk</mml:mtext><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Equation 11).</p><p>2. We provide a preliminary analysis of neuron noise in the new Appendix Section Impact of Neural Noise and Unlearnable Targets on Learning. Although we currently do not have a data-averaged theory of generalization (which is involves a very complicated computation), we did attempt empirically to explore the effect of neural variability on task learning. In the new Appendix Figure 1, we show the performance of the non-trial averaged V1 code on the orientation discrimination tasks. While the trial averaged code has much better sample efficiency, the optimal weight decay parameter <italic>λ</italic> for the noisy code is still small.</p><p>3. We also studied the effect of training readouts on subsamples of <italic>N</italic> neurons from the total V1 code. For sufficiently small <italic>N</italic>, the target function becomes unexpressible as a linear combination of the <italic>N</italic> selected neurons and the learning rule is likely to overfit. This leads to an optimal weight decay parameter <italic>λ</italic>. We mention how this is related to fitting noisy target functions [37].</p><p>4. For the behavior of nonlinear readouts we discuss the results of some recent works which allow for arbitrary nonlinear readout functions in the Discussion and Appendix Convergence of Δ Rule for Nonlinear Readouts. For classification, the power law exponents for the generalization convergence may change, but the phenonenon of code-task alignment is very qualitatively similar [2].</p><p>5. We add a Discussion section item about how the presence of neural noise impacts our metabolic efficiency argument. If the neural noise were Gaussian (perhaps with stimulus dependent covariance), then the rotations and baselines would still be legitimate and our argument still obtains. However, for many distributions, such as the Poisson distribution, the variance at a given stimulus is tied to the mean value at that stimulus so simple transformations such as shifts by a baseline or rotation (while preserving the mean response manifold) alter the noise structure and thus the information content of the code. This item is added to the Discussion section.</p><p>We will now proceed to provide more detail on each of these topics.</p><disp-quote content-type="editor-comment"><p>(1) Within this framework, a realistic stimulus-dependent noise model can be easily introduced and its effects on the kernel and set of easily learned stimulus-response mappings investigated. So while the kernel would be substantially altered via the addition of a realistic noise model, the applications of the approach outlined above would not be affected. The same cannot be said for the efficient coding application described in this manuscript. There, the authors note that rotations and constant shifts of neural activity do not affect the kernel and thus do not affect the generalization error. This kernel invariance is not present when a non-trivial (i.e. non-isotropic) noise model is added. For example, suppose that neurons are independent and Poisson so that noise scales with the mean of the neural response. In this case, adding a baseline firing rate to a population of unimodal neurons representing orientation necessarily reduces the information content of the population while rotations can affect the fidelity with which certain stimulus values are represented. It is important to note, however, that while this particular efficiency result is not compelling, I believe that it is possible to perform a similar analysis that takes into account realistic noise models and focuses on a broad set of 'biologically plausible' kernels instead of particular invariant ones. For example, one could consider noise covariance structures with differential correlations (Moreno-Bote 2014). Since the magnitude of differential correlations controls the redundancy of the population code this would enable an analysis of the role of redundancy in suppressing (or enhancing) generalization error.</p></disp-quote><p>Impact of Neural Noise on Learning Curve</p><p>We agree with the reviewer that any model of learning in a realistic neural system will have to consider the effect of noise (trial-to-trial variability) on the accuracy of learned decoders/predictions. We now mention this explicitly at the beginning of the paper. At the beginning of the Results section on line 93, we added</p><p>“These responses define the population code. Throughout this work, we will mostly assume that this population code is determinstic: that identical stimuli generate identical neural responses.&quot;</p><p>The reviewer is indeed correct that an analysis of generalization can be performed for noisy population codes. Interestingly, even if the noise is stimulus dependent the generalization error can still be expressed in terms of only two correlation matrices, rather than the Fisher information at every value of the stimulus <italic>θ</italic>.</p><p>To justify this claim, first let r(<italic>θ</italic>) be stochastic with mean <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mtext> </mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mtext>rθ</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and covariance <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> For a given configuration of weights w, the generalization error (which is an average over the joint distribution of r<italic>,θ</italic>) is determined only by the signal <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and (average) noise <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> correlation matrices:<disp-formula id="sa2equ1"> <label>(R.9)</label><mml:math id="sa2m6"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∙</mml:mo><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∙</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>w</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where the (deterministic) target function is given by <inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:mtext> </mml:mtext><mml:mover><mml:mi>r</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The above two expressions can be thought of as a bias-variance decomposition over the noise in neural responses. As before, the challenge is determining the average/typical case behavior of <italic>E<sub>g</sub></italic> when <bold>w</bold> is learned from the δ rule on <italic>P</italic> samples, this time with noisy responses <bold>r</bold>(<italic>θ<sub>µ</sub></italic>). The minimum achievable loss can be obtained by differentiation of the population risk expression, giving <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>While we have not yet computed the full learning curve for arbitrary noise correlation structure <bold>Σ</bold><italic><sub>n</sub></italic> (which is quite involved), we hope to pursue this calculation for future work. We want to extend our generalization analysis to arbitrary signal + arbitrary noise correlation structure, including an analysis of the role of differential correlations, but the same computation is made much more difficult by the fact that <bold>Σ</bold><italic><sub>n</sub></italic> and <bold>Σ</bold><italic><sub>s</sub></italic> are not generally co-diagonalizable.</p><p>Further, we empirically tested the effect of neural variability on task learning by training readouts from the V1 code without trial averaging. We studied both the low and high frequency orientation discrimination tasks. The result is provided in R.3 and included in the paper as Appendix Figure 1. We found that the error for the trial-averaged code (dashed) is much lower than the error of the noisy code. However, the error tends to increase with weight decay parameter <italic>λ</italic> without any obvious optimal regularization choice.</p><p>We also tested subsampling the neural population, keeping only <italic>N</italic> neurons to model a sparsely connected readout. The result in Figure R.3 (C-D) shows that for small <italic>N</italic>, the learning curves can exhibit overfitting peaks. Further, the generalization error is minimized for an optimal choice of the weight decay parameter.</p><p>We address and discuss these points by a new Appendix Section Impact of Neural Noise and Unlearnable Targets on Learning starting on line 1861 together with Figure R.3 (included in the paper as Appendix Figure 1). This section reads:</p><p>“While our analysis so far has focused on deterministic population codes, our theory can be extended to neural populations which exhibit variability in responses to identical stimuli. […] At infinite P, generalization error asymptotes to the amount of unexplained variance in the target function.”</p><p>We added the following to the Discussion section (line 546)</p><p>“Our work focused on how signal correlations influence inductive bias [3, 4]. However, since real neurons do exhibit variability in their responses to identical stimuli, one should consider the effect of neural noise and noise correlations in learning. We provide a preliminary analysis of learning with neural noise in Appendix Impact of Neural Noise and Unlearnable Targets on Learning, where we show that neural noise can lead to irreducible asymptotic error which depends on the geometry of the signal and noise correlations. Further, if the target function is not fully expressible as linear combinations of neural responses, overfitting peaks in the learning curves are possible, but can be mitigated with regularization implemented by a weight decay in the learning rule (see Appendix Figure 1). Future work could extend our analysis to study how signal and noise correlations interact to shape inductive bias and generalization performance in the case where the noise correlation matrices are non-isotropic, including the role of differential correlations [41]. Overall, future work could build on the present analysis to incorporate a greater degree of realism in a theory of inductive bias. ”</p><p>Impact of Neural Noise Metabolic Efficiency Results</p><p>The reviewer brings up a very good point about how neural noise can prevent decoding accuracy from being preserved under rotations and baseline shifts. To respond to this critique, we will first point out an example (stimulus dependent Gaussian noise), where generalization performance is preserved under rotations and baseline shifts. Note that if the code at <italic>θ</italic> obeyed <bold>r</bold>(<italic>θ</italic>) ∼ N(¯<bold>r</bold>(<italic>θ</italic>)<italic>,</italic><bold>Σ</bold><italic><sub>n</sub></italic>(<italic>θ</italic>)), then the rotated and shifted code follows <bold>Qr</bold>(<italic>θ</italic>) + <italic>δ</italic> ∼ N(<bold>Q</bold>¯<bold>r</bold>(<italic>θ</italic>) + <italic>δ,</italic><bold>QΣ</bold><italic><sub>n</sub></italic>(<italic>θ</italic>)<bold>Q</bold><sup>&gt;</sup>). This rotated and shifted code <bold>Qr</bold>(<italic>θ</italic>) + <italic>δ</italic>, when centered, will exhibit identical generalization performance as the original code. This is true both for learning from a trial averaged or non-trial averaged code. In the case of Gaussian noise on a centered code, the dataset transforms under a rotation as</p><p>D = {<bold>r</bold><italic><sub>µ</sub>,y<sub>µ</sub></italic>} → D<sup>0</sup> = {<italic>Q</italic><bold>r</bold><italic><sub>µ</sub>,y<sub>µ</sub></italic>}. The optimal weights for a linear model similarly transform as <bold>w</bold>(D) → <italic>Q</italic><bold>w</bold>(D). Under these transformations the predictor on test point <italic>θ</italic> is unchanged since<disp-formula id="sa2equ2"><label>(R.10)</label><mml:math id="sa2m9"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold">⋅</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold">⋅</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Further, the local Fisher information matrix<disp-formula id="sa2equ3"><mml:math id="sa2m10"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>Tr</mml:mtext><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is unchanged under the transformation <bold>r</bold> → <bold>Qr</bold> + <italic>δ</italic> since <bold>Σ</bold><italic><sub>n</sub></italic>(<italic>θ</italic>) → <bold>QΣ</bold><italic><sub>n</sub></italic>(<italic>θ</italic>)<bold>Q</bold>. This shows that, for some noise models, our assumption that rotations and baseline shifts preserve generalization performance will be valid.</p><p>However, we agree with the reviewer, that for Poisson noise, where the variance is tied to the mean firing rate, the RROS operations will not preserve noise structure or information content. The Fisher information at scalar stimulus <italic>θ</italic> for a Poisson neuron is <inline-formula><mml:math id="sa2m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>A baseline shift <italic>r</italic> → <italic>r</italic> + <italic>δ</italic> to the tuning curve will not change the numerator since the derivative of the tuning curve is invariant to this transformation, but it will increase the denominator.</p><p>We provide a detailed discussion of these points in Appendix Effect of Noise on RROS Symmetry (line 1772) and added a paragraph toward the end of our Discussion section, line 511, which reads:</p><p>“As a note of caution, while this analysis holds under the assumption that the neural code is deterministic, real neurons exhibit variability in their responses to repeated stimuli. Such noisy population codes do not, in general, have identical generalization performance under RROS transformations. For example, if each neuron is constrained to produce i.i.d. Poisson noise, then simple shifts of the baseline firing rate reduce the information content of the code. However, if the neural noise is Gaussian (even with stimulus dependent noise covariance), then the generalization error is conserved under RROS operations (App. Effect of Noise on RROS Symmetry). Further studies could focus on revealing the space of codes with equivalent inductive biases under realistic noise models.&quot;</p><disp-quote content-type="editor-comment"><p>(2) Similarly, the linearity assumption is somewhat restrictive. Global linear decoders of neural activity are known to be highly inefficient and completely fail when decoding orientation in the primary visual cortex in the presence of contrast fluctuations. This is because contrast modulates the amplitude of the neural response and doubling the amplitude means doubling an estimate obtained from a linear decoder even when the underlying orientation has not changed. While the contrast issue could be partially addressed by simply considering normalized neural responses, it is not yet clear how to extend this approach to account for other sources of neural variability and co-variability that cause global linear decoders to fail so badly.</p></disp-quote><p>We appreciate the reviewer’s critique of global linear decoders. Our intention here is not to design the best decoder, but rather study a decoder’s performance set by the inductive bias of the population code. From this point of view, a linear decoder is a great choice for its analytical tractability. However, we agree that we should at least acknowledge the restrictions that come due to focusing on a linear decoder.</p><p>We discuss below some recent works which have extended our methods to nonlinear readouts such as binary classifiers which are invariant to scale transformations <bold>r</bold> → <italic>c</italic> <bold>r</bold>. Other possible nonlinear functions of <bold>w</bold> · <bold>r</bold> can also be handled within this framework under certain conditions. We have added an acknowledgement of this limitation of our present work and ways it can be extended to the discussion and the new Appendix Typical Case Analysis of Nonlinear Readouts (line 1471). The main text starting on line 541 reads:</p><p>“Though our work focused on linear readouts, arbitrary nonlinear readouts which generate convex learning objectives have been recently studied in the high dimensional limit, giving qualitatively similar learning curves which depend on kernel eigenvalues and task model alignment [1, 2] (see App. Typical Case Analysis of Nonlinear Readouts).”</p><p>Next, we provide an overview of what is discussed in Appendix Typical Case Analysis of Nonlinear Readouts. The analysis of typical case generalization can be extended to nonlinear predictors and loss functions which depend on the scalar prediction variable <italic>f</italic>(<italic>θ</italic>) = <bold>w</bold> · <bold>r</bold>(<italic>θ</italic>) [1], however the resulting equations are less interpretable. For instance, we can consider binary classification problem where <italic>y</italic> ∈ {±1} by outputting a prediction of <italic>y</italic>ˆ = sign(<bold>w</bold>·<bold>r</bold>) which is invariant to simple rescaling of <bold>r</bold> → <italic>c</italic><bold>r</bold>. We can train a model using the hinge loss <italic>`</italic>(<bold>w</bold> · <bold>r</bold><italic>,y</italic>) = [1 − <bold>w</bold> · <bold>r</bold><italic>y</italic>]<sub>+</sub> so that the classifier will converge to a kernel support vector machine (SVM) [42]. In general, we can describe a loss function which depends on <italic>f</italic>(<italic>θ</italic>) and the ground truth function <italic>y</italic>(<italic>θ</italic>) as <italic>`</italic>(<italic>f,y</italic>).<disp-formula id="sa2equ4"><label>(R.11)</label><mml:math id="sa2m12"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:msub><mml:mi mathvariant="normal">n</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mtext> </mml:mtext><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">w</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Provided that <italic>l</italic> is convex in <bold>w</bold> · <bold>r</bold>, gradient descent will still converge to the optimum. Further, if <bold>r</bold>(<italic>θ</italic>) is approximately Gaussian, then the generalization performance can still be characterized using statistical mechanics methods [1]. While this set of results is more general, the solution for arbitrary code and task structure has some shortcomings when compared to least squares minimization including the requirement of Monte-carlo sampling to compute test loss and the non-decomposability of the error into separate mode errors.</p><p>However, many qualitative features of our results continue to hold, including that the kernel’s diagonalization governs training (Appendix Typical Case Analysis of Nonlinear Readouts) and generalization and that improvements in code task alignment lead to improvements in generalization. In the follow-up work by Cui et al. [2], codes and tasks with power law spectra were analyzed asymptotically, showing power law generalization error decay rates <italic>E<sub>g</sub></italic> ∼ <italic>P</italic><sup>−<italic>β</italic></sup>. The classification learning curves for power law spectra were shown to follow power laws with exponents <italic>β</italic> that are qualitatively similar to the exponents obtained with the square loss which we describe in our section titled Small and Large Sample Size Behaviors of Generalization. Just as in our theory, decay rate exponents <italic>β</italic> are larger for codes which are well aligned to the task and are smaller for codes which are non-aligned.</p><disp-quote content-type="editor-comment"><p>(3) This analysis relies on the assumption that decoder weights learned in the presence of finite data are efficient and unbiased. This assumption is problematic particularly when it comes to inductive bias and generalization error. This is because a standard way to reduce generalization error is to introduce bias into the learned decoder weights through a penalization scheme that privileges decoder weights with small magnitudes. This kind of regularization is particularly important when neurons are noisy. Fortunately, this issue could be addressed by parameterizing changes in the kernel function by the degree and type of regularization potentially leading to a more general result.</p></disp-quote><p>This is a great point. We now discuss inclusion of a ridge parameter <italic>λ</italic> in our calculation which privileges <bold>w</bold> with low <italic>L</italic>2 norm. The <italic>λ</italic> → 0<sup>+</sup> limit recovers our previous result (see Equations 9 to 11 in Methods, and Appendix Weight Decay and Ridge Regression starting on line 1235).</p><p>We explored the effect of neural noise on the learning curve for the V1 population in orientation discrimination (see Figure R.3). The error of the noisy code (solid) is higher than the trial averaged code (dashed). For this code and tasks, the error tends to increase monotonically with regularization strength, indicating a weak effect of noise. However, in (C-D), we study the learning curves for subsamples of <italic>N</italic> neurons from the entire neural population. In this case, overfitting peaks are visible at certain values of <italic>P</italic>. Similarly, there exist non-zero optimal values of weight decay <italic>λ</italic>.</p><disp-quote content-type="editor-comment"><p>Finally, I would like to conclude by explicitly stating that while the limitations imposed by the assumptions listed above temper my enthusiasm in regards to conclusions drawn in this work, I do not believe there is some fundamental problem with the general theoretical framework. Indeed, items 1 and 3 above can be easily addressed through straightforward extensions of the authors approach and I look forward to their implementation. Item 2 is a bit more troublesome, but my intuition tells me that an information-theoretic extension based upon Fisher information may be capable of eliminating all three of these limiting assumptions by exploiting the relationship between FI(\theta) and FI(y=f(\theta)).</p></disp-quote><p>We are also excited by the possibility to analyze sample-efficiency of learning for neural codes with arbitrary signal and noise structure. Though we have only provided a formula for the asymptotic/irreducible error of learning with noisy neurons, we intend to analyze full learning curves in future work. As the reviewer mentions, this is a promising future direction which could even potentially benefit experimentalists in designing learning tasks which can be learned more quickly.</p><disp-quote content-type="editor-comment"><p>Ultimately, all I felt the need to say is in the public part of the review. But I wanted to use this space to insure that it was clear that I feel that this approach has a lot of potential and that I very much hope to see it extended/generalized.</p><p>If I were to make any suggestions they would be:</p><p>(1) a discussion of the consequences of biologically plausible noise models. I believe that this only requires simply augmenting K by a stimulus dependent noise covariance matrix.</p><p>(2) using this addition of a noise model to enable a discussion of the role of redundancy in generalization. This could be accomplished by considering perturbations to the noise covariance matrix that introduce differential correlations (Moreno-Bote 2014) of varying magnitude. More differential correlations means more redundancy and, I suspect, better generalization.</p><p>(3) the addition of a paragraph or two outlining some of the other ways that this learnability/generalization measure could be of use to physiologists and behavioral scientists. Training a mouse to do anything more complicated simple orientation detection is challenging to say the least. So having any tool that can be used to identify functions $y=f(\theta)$ that a mouse has a good chance of learning quickly would be highly advantageous. Similarly, by strategically subsampling neurons by tuning properties I suspect it may be possible to identify subpopulations of neurons that are particular important for learning certain functions. This is also cool because it allows physiologists to then target those populations for manipulation.</p><p>Of these (3) is probably the most important for this manuscript, because I don't think that many experimentalists will immediately see the sheer utility of having a tool like this in their arsenal.</p></disp-quote><p>In conclusion, we added the following to address these three points</p><p>1. Noise Model: We added the Appendix Section Impact of Neural Noise and Unlearnable Targets on Learning. We discussed this addition in our response above.</p><p>2. Structured Noise: We have not yet analyzed the average case learning curves for arbitrary noise correlation structure <bold>Σ</bold><italic><sub>n</sub></italic>(<italic>θ</italic>) (since it is much more of an involved calculation), but provide a formula for the irreducible loss. We also discuss our aspiration to pursue an analysis of differential correlations as an important future direction in the discussion.</p><p>3. Learnability for Physiologists/Behavioral Scientists: We added the possible applications to experimental neuroscience as a discussion item (see last paragraph) which we quote here:</p><p>“Finally, we discuss possible applications of our work to experimental neuroscience. Our theory has potential implications for experimental studies of task learning. First, in cases where the population selective to stimuli can be measured directly, an experimenter could design easy or difficult tasks for an animal to learn from few examples, under a hypothesis that the behavioral output is a linear function of the observed neurons. Second, in cases where it is unclear which neural population contributes to learning, one could utilize our theory to solve the inverse problem of inferring the relevant kernel from observed learning curves on different tasks [43]. From these tasks, the experimenter could compare the inferred kernel to those of different recorded populations. For instance one could compare the kernels from V1, V4, IT in visual ventral stream to the inferred kernel obtained from learning curves on certain visual learning tasks. This could provide new ways to test theories of perceptual learning [7]. Lastly, extensions of our framework could quantify the role of neural variability on task learning and the limitation it imposes on accuracy and sample efficiency.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The manuscript presents a theory of generalization performance in deterministic population codes, that applies to the case of small numbers of training examples. The main technical result, as far as I understand, is that generalization performance (the expected classification or regression error) of a population code depends exclusively on the 'kernel', i.e. a measure of the pairwise similarity between population activity patterns corresponding to different inputs. The main conceptual results are that, using this theory, one can understand the inductive biases of the code just from analyzing the kernel, particularly the top eigenfunctions; and that sample-efficient learning (low generalization performance with few samples) depends on whether the task is aligned with the population's inductive bias, that is, whether the target function (i.e. the true map from inputs to outputs) is aligned with the top eigenfunctions of the kernel. For instance, in mouse V1 data, they show that the top eigenfunctions correspond to low frequency functions of visual orientation (i.e. functions that map a broad range of similar orientations to similar output value), and that consistent with the theory, the generalization performance for small sample sizes is better for tasks defined by low frequency target functions. In my opinion, perhaps the most significant finding from a neuroscience perspective, is that the conditions for good generalization at low samples are markedly different from those in the large-sample asymptotic regime studies in Stringer et al. 2018 Nature: rather than a trade-off between high-dimensionality and differentiability proposed by Stringer et al., this manuscript shows that in the low-sample regime such codes can be disadvantageous for small sample sizes, that differentiability is not required, that the top eigenvalues matter more than the tail of the spectrum, and what matters is the alignment between the task and the top eigenfunctions. The authors propose sample-efficient learning/generalization as a new principle of neural coding, replacing or complementing efficient coding.</p></disp-quote><p>We thank the reviewer for such a careful reading and impressively accurate summary of our work.</p><disp-quote content-type="editor-comment"><p>Overall, in my opinion this is a remarkable manuscript, presenting truly innovative theory with somewhat limited but convincing application to neural data. My main concern is that this is highly technical, dense, and long; the mathematical proofs for the theory are buried in the supplement and require knowledge of disparate techniques from statistical physics. Although some of that material on the theory of generalization is covered in previous publications by the authors, it was not clear to me if that is true for all of the technical results or only some.</p></disp-quote><p>Thank you for your encouraging words! It is true that our presentation was technically dense. Here is what we did to address this:</p><p>– Throughout the paper, we tried eliminating unnecessary jargon and heavy mathematical notation, especially in the V1 model and time-dependent codes sections.</p><p>– We tried providing additional intuition and simple derivations in the main text. In the problem setup, we provided a more detailed argument showing that the learned function only depends on the kernel. We also connected kernels to the idea of a distance metric in neural space. We additionally included a simple expression for the average learned function <inline-formula><mml:math id="sa2m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the Methods Theory of Generalization starting on line 617, showing how the coefficients for each eigenmode are learned at different rates.</p><p>– On the question of novelty, the main result on learning curve theory was derived in our prior works [44, 37], but the application of the theory to different neural codes or real neural data had not yet been performed.</p><disp-quote content-type="editor-comment"><p>Fixed population code, learnable linear readout: the authors acknowledge in the very last sentences of the manuscript that this is a limitation, given that neural tuning curves (the population neural code) are adaptable. I imagine extending the theory to both learnable codes and learnable readouts is hard and I understand it's beyond the scope of this paper. But perhaps the authors could motivate and discuss this choice, not just because of its mathematical convenience but also in relation to actual neural systems: when are these assumptions expected to be a good approximation of the real system?</p></disp-quote><p>This is a great point. Below we argue that our theory will apply to learning and generalization dynamics under one of the following two conditions</p><p>– Only the readout weights of a certain sensory layer (with code r(θ)) are optimized by a learning rule (such as δ rule in Figure 1). This is the simplest interpretation of our present results. It relies on an assumption that the readout weights converge more quickly than underlying low level sensory representation r(θ) evolves. We would expect this to be the case when learning a new simple task such as a Macaque learning to distinguish two new faces. The IT cortical representation will likely not need to change much in response to the classification of two new stimuli. However, if the animal is learning to classify entirely new types of stimuli over a very long period of training, it may be the case that cortical representations evolve significantly. Research in deep and recurrent networks has revealed operating regimes where the output function can still be written as f(θ) = w · ψ(θ) for a different static feature map ψ(θ) which depends on the architecture of the entire network [23, 24, 45]. For gradient flow, the kernel for this feature map is known as the neural tangent kernel [23]. Deep neural networks operating in this limit have scaling laws which are well predicted by our theory [44, 46, 47, 48]. This operating regime could be relevant for certain learning tasks in biological neural circuits.</p><p>While the above cases define the ways that our theory could accurately apply to learning in real sensory systems, there are also regimes of neural network learning where features cannot be modeled as static (in machine learning, this is known as the rich feature learning regime). Ideally a future analysis could extend our results to this setting, but it would be much more challenging. Such feature learning in a deep network is dependent on every layer of processing as well as the specific learning rule one assumes [27, 21]. Since the brain’s solution to the credit assignment problem is still unknown, it is currently unclear which learning rule should be analyzed (back-propagation, feedback alignment, global error broadcasting, gated linear networks, etc). Comparing the typical case performance and learned representations for each of these different learning rules would be a good first step towards understanding the possible inductive biases of networks in the rich regime. Provided a sufficiently refined analysis of these rules, one could perhaps even identify which learning rules are operating in real neural systems based on their measured learning dynamics.</p><p>We added the following to the discussion (starting line 520) about when we expect our theory to be accurate</p><p>“Our work constitutes a first step towards understanding inductive biases in neuronal circuits. To achieve this, we focused on a linear, δ-rule readout of a static population code. More work is need to study other factors that affect inductive bias. Importantly, sensory neuron tuning curves can adapt during perceptual learning tasks [7, 14, 9, 8] with the strength of adaptation dependent on brain area [10, 11, 12]. However, in many experiments, these changes to tuning in sensory areas are small [8, 9], satisfying the assumptions of our theory. For example monkeys trained on noisy visual motion detection exhibit changes in sensory-motor (LIP) but not sensory areas (MT), consistent with a model of readout from a static sensory population code [13, 15]. However, other perceptual learning tasks and other brain areas can exhibit significant changes in neural tuning [16, 17, 18]. This diversity of results motivates more general analysis of learning in multilayer networks, where the representations in each layer can adapt flexibly to task structure [19, 20, 21, 22]. Alternatively, our current analysis of inductive bias can still be consistent with multilayer learning if the network is sufficiently overparameterized and tuning curves change very little [23, 24, 19]. In this case, network training is equivalent to kernel learning with a kernel that depends on the learning rule and architecture [25]. However, in the regime of neural network training where tuning curves change significantly, more sophisticated analytical tools are needed to predict generalization [26, 27, 21]. Though our work focused on linear readouts, arbitrary nonlinear readouts which generate convex learning objectives have been recently studied in the high dimensional limit, giving qualitatively similar learning curves which depend on kernel eigenvalues and task model alignment [1, 2].”</p><disp-quote content-type="editor-comment"><p>The analysis of V1 data, showing a bias for low-frequency functions of orientation is convincing. But it could help if the authors provided some considerations on the kind of ethological behavioral context where this is relevant, or at least the design of an experimental behavioral task to probe it. Also related, it would be useful to construct and show a counter-example, a synthetic code for which the high-frequency task is easier.</p></disp-quote><p>We thank the reviewer for this comment.</p><p>Applications to experimental neuroscience: We added the possible applications to experimental neuroscience as a discussion item (see last paragraph) which we quote here:</p><p>“Finally, we discuss possible applications of our work to experimental neuroscience. Our theory has potential implications for experimental studies of task learning. First, in cases where the population selective to stimuli can be measured directly, an experimenter could design easy or difficult tasks for an animal to learn from few examples, under a hypothesis that the behavioral output is a linear function of the observed neurons. Second, in cases where it is unclear which neural population contributes to learning, one could utilize our theory to solve the inverse problem of inferring the relevant kernel from observed learning curves on different tasks [43]. From these tasks, the experimenter could compare the inferred kernel to those of different recorded populations. For instance one could compare the kernels from V1, V4, IT in visual ventral stream to the inferred kernel obtained from learning curves on certain visual learning tasks. This could provide new ways to test theories of perceptual learning [7]. Lastly, extensions of our framework could quantify the role of neural variability on task learning and the limitation it imposes on accuracy and sample efficiency.”</p><p>Orientation discrimination task: We agree that this orientation discrimination task isn’t perhaps the most ethologically relevant one, but given that the data consists of responses to oriented gratings, it is a reasonable task to consider. Since neurons are selective to bars at particular orientations, this kind of task is intuitively expected to have high alignment with the V1 code. Also, because the eigenfuntions are close to Fourier modes, the inductive is more interpretable, and, thus, this task serves as a good starter example.</p><p>We also analyzed two ethologically more relevant tasks. First was a scene reconstruction task from V1 (Figure 4), and found that low-pass filtered images are easier to reconstruct, consistent with low spatial frequency selectivity of mouse V1 neurons. Second was a more ethologically natural scene categorization task (Figure 3D), but the V1 code did not generalize well, suggesting that later stages of the visual ventral stream may be better suited for categorization tasks. We suspect that other regions in visual stream may be more aligned to these ethologically relevant categorization/classification tasks. For example, primate IT codes may be more aligned with object classification.</p><p>Synthetic, counter-example code: This is a fantastic idea! Our Figure 1 actually does this. Figure 1B has two different codes, one for which a low-frequency task is easier and another one for which a high-frequency task is easier. These codes are synthetically created by generating tuning curves from a procedure we describe in Methods Generating Example Codes.</p><disp-quote content-type="editor-comment"><p>Line 519, data preprocessing: related to the above, is it possible that binning together the V1 responses to gratings with different orientations (a range of 3.6 deg per bin, if I understood correctly) influences the finding of a low-frequency bias?</p></disp-quote><p>Thanks for this comment. Based on the reviewer’s suggestion, we computed the eigendecomposition of trial averaged responses for different bin numbers bins ∈ {50<italic>,</italic>100<italic>,</italic>200} which correspond to angular windows of <inline-formula><mml:math id="sa2m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mn>3.6</mml:mn><mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>1.8</mml:mn><mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>0.9</mml:mn><mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The spectrum and eigenfunctions (especially the top modes) are very consistent across different bin sizes. The eigenfunctions become noisier with smaller bins since there are fewer trials to average over.</p><p>This figure is included as Figure 3—figure supplement 1 and referred to in the Main text line number 209.</p><disp-quote content-type="editor-comment"><p>I found the study of invariances interesting, where the theory provides a normative prediction for the proportion of simple and complex cells. However, I would suggest the authors attempt to bring this analysis a step closer to the actual data: there are no pure simple and complex cells, usually the classification is based on responses to gratings phases (F1/F0) and real neurons take a continuum of values. Could the theory qualitatively predict that distribution?</p></disp-quote><p>We thank the reviewer for pointing out that our model of simple/complex V1 cells was overly simplified. In reality, as the reviewer mentions there is continuous variability in F1/F0 across the neural population. In response, we modified our energy based model which produces a continuum of F1/F0 values. Further, a single scalar parameter <italic>α</italic> controls how strongly the population is biased towards invariance.</p><p>Concretely, we define the response of a cell <inline-formula><mml:math id="sa2m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for nonlinear function <italic>g</italic> and scalar <italic>z</italic> which is constructed as follows<disp-formula id="sa2equ5"><label>(R.12)</label><mml:math id="sa2m16"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This linear combination is inspired by the construction of complex cells in Dayan and Abbot Chapter 2. If only one of the <italic>β</italic>s were 1 and the rest were zero, this would be a perfect simple cell. If all the <italic>β</italic>s were equal, this would a perfect complex cell that is invariant to stimulus phase <italic>φ</italic>. To generate variability in tuning to phase <italic>φ</italic>, we will draw <italic>β</italic> from a Dirichlet distribution on the simplex with concentration parameter <italic>α</italic><bold>1</bold> so that <italic>p</italic>(<italic>β</italic>) ∝ <sup>Q4</sup><sub><italic>j</italic>=1</sub> <italic>β<sub>j</sub><sup>α</sup></italic><sup>−1</sup> with <sup>P4</sup><sub><italic>j</italic>=1</sub> <italic>β<sub>j</sub></italic> = 1. In the <italic>α</italic> → ∞ limit, the probability density concentrates on &lt;inline-graphic mimetype=&quot;image&quot; mime-subtype=&quot;png&quot; xlink:href=&quot;media/image1.png&quot; /&gt;, leading to a code comprised entirely of complex cells which are invariant to phase <italic>φ</italic>. In the <italic>α</italic> → 0 limit, the density is concentrated around the “edges” of the simplex such as (1<italic>,</italic>0<italic>,</italic>0<italic>,</italic>0)<italic>,</italic>(0<italic>,</italic>1<italic>,</italic>0<italic>,</italic>0), etc, giving a population of pure simple cells. For intermediate values, neurons are partially selective to phase. As before, the selectivity or invariance to phase is manifested in the kernel decomposition and leads to similar learning curves for the three tasks of the main paper (Orientation, Phase, Hybrid). We provide an illustration of tuning curves, eigenfunctions, and learning curves in Figure R.5. This model is discussed in detail the new Appendix Energy Model with Partially Phase-Selective Cells (line 1677). The Figure is included as Figure 5—figure supplement 3.</p><p>References</p><list list-type="bullet"><list-item><p>Bruno Loureiro, Cédric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mézard, and Lenka Zdeborová. Capturing the learning curves of generic features maps for realistic data sets with a teacher-student model. <italic>CoRR</italic>, abs/2102.08127, 2021.</p></list-item><list-item><p>Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová. Error rates for kernel classification under source and capacity conditions, 2022.</p></list-item><list-item><p>Bruno Averbeck, Peter Latham, and Alexandre Pouget. Neural correlations, population coding and computation. <italic>Nature Reviews Neuroscience</italic>, 7, 2006.</p></list-item><list-item><p>Marlene R Cohen and Adam Kohn. Measuring and interpreting neuronal correlations. <italic>Nature neuroscience</italic>, 14(7):811, 2011.</p></list-item><list-item><p>H S Seung and H Sompolinsky. Simple models for reading neuronal population codes. <italic>Proceedings of the National Academy of Sciences</italic>, 90(22):10749–10753, 1993.</p></list-item><list-item><p>Uri Cohen, Sueyeon Chung, Daniel Lee, and Haim Sompolinsky. Separability and geometry of object manifolds in deep neural networks. <italic>Nature Communications</italic>, 11(746), 2020.</p></list-item><list-item><p>Charles D Gilbert. Early perceptual learning. <italic>Proceedings of the National Academy of Sciences of the United States of America</italic>, 91(4):1195, 1994.</p></list-item><list-item><p>Aniek Schoups, Rufin Vogels, Ning Qian, and Guy Orban. Practising orientation identification improves orientation coding in v1 neurons. <italic>Nature</italic>, 412(6846):549–553, 2001.</p></list-item><list-item><p>Geoffrey M Ghose, Tianming Yang, and John HR Maunsell. Physiological correlates of perceptual learning in monkey v1 and v2. <italic>Journal of neurophysiology</italic>, 87(4):1867–1888, 2002.</p></list-item><list-item><p>Tianming Yang and John HR Maunsell. The effect of perceptual learning on neuronal responses in monkey visual area v4. <italic>Journal of Neuroscience</italic>, 24(7):1617–1626, 2004.</p></list-item><list-item><p>Hamed Zivari Adab, Ivo D Popivanov, Wim Vanduffel, and Rufin Vogels. Perceptual learning of simple stimuli modifies stimulus representations in posterior inferior temporal cortex. <italic>Journalof cognitive neuroscience</italic>, 26(10):2187–2200, 2014.</p></list-item> </list><p><italic>neuroscience</italic>, 24(10):1441–1451, 2021.</p><list list-type="bullet"><list-item><p>Michael N Shadlen and William T Newsome. Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey. <italic>Journal of neurophysiology</italic>, 86(4):1916–1936, 2001.</p></list-item><list-item><p>Gregg H Recanzone, Christoph E Schreiner, and Michael M Merzenich. Plasticity in the frequency representation of primary auditory cortex following discrimination training in adult owl monkeys. <italic>Journal of Neuroscience</italic>, 13(1):87–103, 1993.</p></list-item><list-item><p>Burkhard Pleger, Ann-Freya Foerster, Patrick Ragert, Hubert R Dinse, Peter Schwenkreis, Jean-Pierre Malin, Volkmar Nicolas, and Martin Tegenthoff. Functional imaging of perceptual learning in human primary and secondary somatosensory cortex. <italic>Neuron</italic>, 40(3):643–653, 2003.</p></list-item><list-item><p>Christopher S Furmanski, Denis Schluppeck, and Stephen A Engel. Learning strengthens the response of primary visual cortex to simple patterns. <italic>Current Biology</italic>, 14(7):573–578, 2004.</p></list-item><list-item><p>Haozhe Shan and Haim Sompolinsky. A minimum perturbation theory of deep perceptual learning, 2021.</p></list-item><list-item><p>Francesca Mastrogiuseppe, Naoki Hiratani, and Peter Latham. Evolution of neural activity in circuits bridging sensory and abstract knowledge. <italic>bioRxiv</italic>, 2022.</p></list-item><list-item><p>Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. <italic>arXiv preprint arXiv:2205.09653</italic>, 2022.</p></list-item><list-item><p>Merav Ahissar and Shaul Hochstein. The reverse hierarchy theory of visual perceptual learning. <italic>Trends in cognitive sciences</italic>, 8(10):457–464, 2004.</p></list-item><list-item><p>Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, <italic>Advances in Neural Information Processing Systems</italic>, volume 31, pages 8571–8580. Curran Associates, Inc, 2018.</p></list-item><list-item><p>Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In <italic>International Conference on Learning Representations</italic>, 2018.</p></list-item><list-item><p>Blake Bordelon and Cengiz Pehlevan. The influence of learning rule on representation dynamics in wide neural networks, 2022.</p></list-item><list-item><p>Timo Flesch, Keno Juechems, Tsvetomira Dumbalska, Andrew Saxe, and Christopher Summerfield. Rich and lazy learning of task representations in brains and neural networks. <italic>bioRxiv</italic>, 2021.</p></list-item><list-item><p>Greg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In <italic>International Conference on Machine Learning</italic>, pages 11727–11737. PMLR, 2021.</p></list-item><list-item><p>D Hansel and C Van Vreeswijk. How noise contributes to contrast invariance of orientation tuning in cat visual cortex. <italic>Journal of Neuroscience</italic>, 22(12):5118–5128, 2002.</p></list-item><list-item><p>Kenneth D Miller and Todd W Troyer. Neural noise can explain expansive, power-law nonlinearities in neural response functions. <italic>Journal of neurophysiology</italic>, 87(2):653–659, 2002.</p></list-item><list-item><p>Nicholas J Priebe, Ferenc Mechler, Matteo Carandini, and David Ferster. The contribution of spike threshold to the dichotomy of cortical simple and complex cells. <italic>Nature neuroscience</italic>, 7(10):1113–1122, 2004.</p></list-item><list-item><p>Nicholas J Priebe and David Ferster. Inhibition, spike threshold, and stimulus selectivity in primary visual cortex. <italic>Neuron</italic>, 57(4):482–497, 2008.</p></list-item><list-item><label>32.</label><p>Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random features and kernel models, 2021.</p></list-item><list-item><label>33.</label><p>Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev Arora. Enhanced convolutional neural tangent kernels, 2019.</p></list-item><list-item><label>34.</label><p>Matthew Farrell, Blake Bordelon, Shubhendu Trivedi, and Cengiz Pehlevan. Capacity of groupinvariant linear readouts from equivariant representations: How many objects can be linearly classified under all possible views? <italic>arXiv preprint arXiv:2110.07472</italic>, 2021.</p></list-item><list-item><label>35.</label><p>Lechao Xiao and Jeffrey Pennington. Synergy and symmetry in deep learning: Interactions between the data, model, and inference algorithm, 2022.</p></list-item><list-item><label>36.</label><p>Carsen Stringer, Marius Pachitariu, Nicholas Steinmetz, Matteo Carandini, and Kenneth D.</p></list-item></list><p>Harris. High-dimensional geometry of population responses in visual cortex. <italic>Nature</italic>, 571, 6 2018.</p><list list-type="bullet"><list-item><label>37.</label><p>Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. <italic>Nature Communications</italic>, in press, 2021.</p></list-item></list></body></sub-article></article>