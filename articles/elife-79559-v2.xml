<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79559</article-id><article-id pub-id-type="doi">10.7554/eLife.79559</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Circular and unified analysis in network neuroscience</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-136800"><name><surname>Rubinov</surname><given-names>Mika</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4787-7075</contrib-id><email>mika.rubinov@vanderbilt.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vm5rt34</institution-id><institution>Departments of Biomedical Engineering, Computer Science, and Psychology, Vanderbilt University</institution></institution-wrap><addr-line><named-content content-type="city">Nashville</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013sk6x84</institution-id><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution></institution-wrap><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Verstynen</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e79559</elocation-id><history><date date-type="received" iso-8601-date="2022-04-18"><day>18</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-10-18"><day>18</day><month>10</month><year>2023</year></date></history><permissions><copyright-statement>© 2023, Rubinov</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Rubinov</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79559-v2.pdf"/><abstract><p>Genuinely new discovery transcends existing knowledge. Despite this, many analyses in systems neuroscience neglect to test new speculative hypotheses against benchmark empirical facts. Some of these analyses inadvertently use circular reasoning to present existing knowledge as new discovery. Here, I discuss that this problem can confound key results and estimate that it has affected more than three thousand studies in network neuroscience over the last decade. I suggest that future studies can reduce this problem by limiting the use of speculative evidence, integrating existing knowledge into benchmark models, and rigorously testing proposed discoveries against these models. I conclude with a summary of practical challenges and recommendations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>network neuroscience</kwd><kwd>systems neuroscience</kwd><kwd>computational neuroscience</kwd><kwd>statistical models</kwd><kwd>explanatory models</kwd><kwd>benchmark models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RF1MH125933</award-id><principal-award-recipient><name><surname>Rubinov</surname><given-names>Mika</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>2207891</award-id><principal-award-recipient><name><surname>Rubinov</surname><given-names>Mika</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Circular analyses of knowledge are a common, serious, but often manageable problem in systems and network neuroscience.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p><italic>You do not know anything until you know why you know it.</italic></p><p>Clovis Andersen, <italic>The Principles of Private Detection</italic> (<xref ref-type="bibr" rid="bib109">McCall Smith, 2007</xref>), cited in <xref ref-type="bibr" rid="bib160">Sokal, 2010</xref>.</p><sec id="s1" sec-type="intro"><title>Introduction</title><p>Scientific models are explanations of reality (<xref ref-type="bibr" rid="bib154">Shmueli, 2010</xref>; <xref ref-type="bibr" rid="bib56">Frigg and Hartmann, 2020</xref>). Models come in many forms, from sentences to equations, and in many kinds, from hypotheses to theories. All models are false, but some models are truer than others (<xref ref-type="bibr" rid="bib115">Mizrahi, 2020</xref>). Specifically, all else being equal, models that are more explanatorily successful — that explain the data more accurately or with fewer assumptions — are likely to be truer than rival models (Appendix 1).</p><p>Efforts to find truer models drive scientific progress but command relatively little neuroscientific attention. Neuroscience devotes greater efforts to produce better data or more replicable analyses (<xref ref-type="bibr" rid="bib55">Frégnac, 2017</xref>). A study by <xref ref-type="bibr" rid="bib83">Jonas and Kording, 2017</xref> implicitly critiqued this imbalance of effort. The study showed that popular neuroscientific analyses of ideal data cannot explain the workings of a computer chip, a toy model of the nervous system. The study implied, in this way, that neuroscience must devote greater efforts to find truer models.</p><p>Science finds truer and truer models relative to stronger and stronger rival models. By contrast, many analyses in neuroscience test new speculative models against weak null models. Some of these analyses use circular reasoning to redundantly explain existing knowledge. These circular analyses of knowledge violate the principle of parsimony and, in this way, accept models that are less true relative to the strongest rival models. Here, I discuss the nature and prevalence of this problem in systems and network neuroscience. I show that the problem can confound key results and estimate that it is common in the network-neuroscience literature.</p><p>I suggest that studies can reduce this problem in three main ways. First, they can limit the use of speculative evidence. Second, they can integrate all important existing knowledge into benchmark models. Third, they can rigorously test the significance of proposed discoveries against these models. Together, these steps can reduce circular analyses, formalize existing knowledge, and benchmark future progress.</p><p>Much of the following discussion stresses the importance of unambiguous definitions. Accordingly, <xref ref-type="table" rid="table1">Table 1</xref> defines the use of several potentially ambiguous technical terms.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Definitions of terms.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Term</th><th align="left" valign="bottom">Definition</th></tr></thead><tbody><tr><td align="left" valign="top">Principle of parsimony (Occam’s razor)</td><td align="left" valign="top">An assertion that all else being equal, models with fewer redundant features are likely to be truer than rival models (<xref ref-type="bibr" rid="bib6">Baker, 2022</xref>). This assertion reflects an objective preference for parsimony rather than a subjective preference for simplicity or elegance. In this way, and contrary to misconception, the principle of parsimony does not imply that reality, or its truest models, are simple or elegant.</td></tr><tr><td align="left" valign="top">Trueness (bias)</td><td align="left" valign="top">Distance between expected and true estimates of model parameters (<xref ref-type="bibr" rid="bib82">ISO, 1994</xref>). True values of model parameters are typically inaccessible, and trueness (bias) can therefore be defined only in relative terms. The principle of parsimony asserts that all else being equal, models with fewer redundant features have truer (less biased) parameter estimates relative to rival models.</td></tr><tr><td align="left" valign="top">Precision (variance)</td><td align="left" valign="top">Expected distance between repeated estimates of model parameters (<xref ref-type="bibr" rid="bib82">ISO, 1994</xref>). Precision (variance) does not require knowledge of the true values of model parameters and can therefore be defined in absolute terms. The problem of irreplicable results (<xref ref-type="bibr" rid="bib80">Ioannidis, 2005</xref>) is primarily a problem of precision (variance).</td></tr><tr><td align="left" valign="top">Circular analysis</td><td align="left" valign="top">An analysis that first tests a model in a way that almost invariably accepts the model and then accepts the model on the basis of this test. This definition includes circular analyses of knowledge that accept overspecified models or redundant (less true) explanations. It also includes circular analyses of noise that accept overfitted models or irreplicable (less precise) explanations (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>).</td></tr><tr><td align="left" valign="top">Neural circuits or brain networks</td><td align="left" valign="top">Groups of connected neurons or brain regions that mediate function. This definition does not intend to make analogies between groups of neurons or brain regions, and electronic circuits or artificial neural networks (<xref ref-type="bibr" rid="bib143">Rubinov, 2015</xref>).</td></tr><tr><td align="left" valign="top">Function</td><td align="left" valign="top">Behavior and other action that helps animals to survive and reproduce (<xref ref-type="bibr" rid="bib142">Roux, 2014</xref>). This definition excludes physiological phenomena that lack such useful action.</td></tr><tr><td align="left" valign="top">Structure</td><td align="left" valign="top">Anatomical or physiological organization. This definition encompasses all physiological phenomena, including phenomena that lack known function.</td></tr><tr><td align="left" valign="top">Development</td><td align="left" valign="top">Formation of structure before and after birth. This definition includes plasticity and therefore encompasses learning and memory.</td></tr></tbody></table></table-wrap><sec id="s1-1"><title>General definitions</title><p>Analyses of complex datasets are vulnerable to distortions by extraneous features. Such distortions may include corruption by noise or confounding by existing knowledge. Statistical science, machine learning, and other fields have developed rigorous tests to mitigate the risk of these distortions. Analyses of complex datasets that neglect such tests, however, will almost invariably be distorted by extraneous features to some extent.</p><p>These distortions can generally lead to inflated agreement between model and data and to inappropriate model acceptance on the basis of this inflated agreement. The nature of individual distortions, however, will ultimately determine the individual consequences of this problem. On the one hand, corruption of analyses by noise can lead to the well-known problem of model overfitting and to irreplicable explanations (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>; <xref ref-type="bibr" rid="bib179">Vul et al., 2009</xref>). On the other hand, confounding of analyses by existing knowledge can lead to a distinct, and less well-known, problem of model overspecification and to redundant explanations.</p><p>This work describes analyses that neglect to test speculative models against existing knowledge and that consequently accept overspecified models and redundant explanations. This section first defines the nature of this problem and then outlines a general solution.</p><sec id="s1-1-1"><title>Toy analogy</title><p>We can get an intuition for the problem with a toy analysis of a biological image (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The image is ambiguous, but our existing biological knowledge tells us that it most likely shows a duck — specifically a male duck doing a head-throw, its signature courting move. Sometimes our analyses may neglect such knowledge. This neglect will not make knowledge disappear. Instead, it will inflate the importance of hypotheses redundant with this knowledge.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Speculative models.</title><p>Speculative hypotheses that rest on apparent similarities between (<bold>a</bold>) an ambiguous duck-rabbit animal and (<bold>b</bold>) a skvader, a type of winged hare; (<bold>c</bold>) networks of neurons and (<bold>d</bold>) networks of galaxies; (<bold>e</bold>) a cortical visual system and (<bold>f</bold>) a convolutional neural network, a machine learning model for classifying images; (<bold>g</bold>) large-scale brain networks and (<bold>h</bold>) global friendship networks. Panel (a) is reproduced from Tim Zurowski (Shutterstock). Panel (b) is reproduced from Gösta Knochenhauer. Panel (c) is reproduced from Figure 4.2 of <xref ref-type="bibr" rid="bib165">Stangor and Walinga, 2014</xref>. Panel (d) is adapted from the Illustris Collaboration (<xref ref-type="bibr" rid="bib178">Vogelsberger et al., 2014</xref>). Panel (e) is reproduced from Figure 1 of <xref ref-type="bibr" rid="bib180">Wallisch and Movshon, 2008</xref>. Panel (f) is adapted from Figure 2 of <xref ref-type="bibr" rid="bib92">Krizhevsky et al., 2012</xref>. Panel (g) is reproduced from the USC Laboratory of NeuroImaging and Athinoula A. Martinos Center for Biomedical Imaging Human Connectome Project Consortium. Panel (h) is reproduced from Paul Butler (Facebook).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-fig1-v2.tif"/><permissions><copyright-statement>© 2017, Tim Zurowski (Shutterstock)</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Tim Zurowski (Shutterstock)</copyright-holder><license><license-p>Panel (a) is reproduced from Tim Zurowski (Shutterstock). It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions><permissions><copyright-statement>© 2015, Gösta Knochenhauer</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Gösta Knochenhauer</copyright-holder><license><license-p>Panel (b) is reproduced from Gösta Knochenhauer with permission. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig><p>We may propose, for example, that the image shows a skvader, a type of winged hare (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Our existing knowledge makes this hypothesis redundant — ducks doing head-throws almost always look like skvaders. Our neglect of this knowledge, however, can make the hypothesis seem important. We may accept the hypothesis on the basis of this perceived importance. This acceptance, however, will lead to redundant explanations. We will implicitly “double dip” or explain the same image twice — first as a duck and second as a skvader.</p></sec><sec id="s1-1-2"><title>Circular analysis</title><p>We can define the problem more formally with three types of models.</p><p><italic>Benchmark models (well-specified models)</italic>. These models represent all important existing knowledge about our phenomenon of interest. They include all benchmark features, features of known importance to this phenomenon, and they exclude all other features. In systems neuroscience, benchmark features often represent existing knowledge about the function, structure, development, and evolution of neural circuits. Distinct phenomena may have distinct benchmark models, and one phenomenon may have several competing benchmark models.</p><p><italic>Speculative models</italic>. These models represent new hypotheses about some phenomenon of interest. They include one or more speculative features, features of possible but uncertain importance to this phenomenon. Some speculative features may turn out to be redundant with benchmark features. For example, consider the similarity of the human brain and the universe (<xref ref-type="fig" rid="fig1">Figure 1c–d</xref>). Both systems have billions of nested, spatially embedded, and interacting elements: neurons and galaxies (<xref ref-type="bibr" rid="bib177">Vazza and Feletti, 2020</xref>). Let the feature of <italic>cosmicity</italic> denote the resemblance of a complex system to the universe. The human brain has high cosmicity. A speculative model may propose, on this basis, that brain dynamics resemble cosmic dynamics. Note, however, that brain cosmicity is likely to be redundant with our existing knowledge about the structure of neural circuits.</p><p><italic>Strawman models (underspecified models).</italic> These models represent weak null hypotheses. They typically exclude the benchmark features with which the speculative features are redundant. In our example, a strawman model excludes the known structure of neural circuits with which cosmicity is redundant.</p><p><italic>Circular analyses.</italic> These analyses almost invariably accept speculative models against strawman models (<xref ref-type="box" rid="box1">Box 1</xref>, Appendix 2). They comprise circular analyses of noise and circular analyses of knowledge (Appendix 3). Circular analyses of noise, the focus of previous work (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>), result in acceptance of noisy or irreplicable explanations. By contrast, circular analyses of knowledge, the focus of this work, result in acceptance of redundant explanations. In our example, a circular analysis of knowledge will almost invariably accept the significance of cosmicity against our strawman model. The analysis will be circular because our strawman model excludes the known structure of neural circuits with which cosmicity is redundant.</p><boxed-text id="box1"><label>Box 1.</label><caption><title>A classification of circular analyses.</title></caption><p><bold>General definition (weak evidence of progress)</bold></p><p>Circular analyses are analyses that use circular reasoning. These analyses:</p><list list-type="order"><list-item><p>Test a model in a way that almost invariably accepts the model.</p></list-item><list-item><p>Accept the model on the basis of this test.</p></list-item></list><p>In general, circular analyses denote weak evidence of progress but do not necessarily preclude progress. In this way, these analyses do not necessarily denote strong evidence of stagnation. These analyses also violate Mayo’s weak-severity requirement of “bad evidence, no test” (<xref ref-type="bibr" rid="bib106">Mayo and Spanos, 2011</xref>; <xref ref-type="bibr" rid="bib107">Mayo, 2018</xref>; Appendix 2).</p><p><bold>Specific definition (strong evidence of stagnation)</bold></p><p>This work describes circular analyses of knowledge. These analyses:</p><list list-type="order"><list-item><p>Test a speculative model in a way that almost invariably accepts it against a strawman model. Specifically, these analyses test the statistical significance of speculative features in a way that almost invariably shows the significance of these features against a strawman model because:</p><list list-type="alpha-lower"><list-item><p>The speculative features are redundant with one or more benchmark features.</p></list-item><list-item><p>The strawman model excludes the benchmark features with which the speculative features are redundant.</p></list-item></list></list-item><list-item><p>Accept the speculative model on the basis of this test.</p></list-item></list><p>Circular analyses of knowledge explain the same aspect of the data twice: first, as one or more benchmark features and second, as a speculative feature redundant with these benchmark features. In this way, these analyses necessarily denote strong evidence of stagnation. Note that in principle, the acceptance of redundant explanations may signify regress rather than mere stagnation. In practice, however, the relatively transient nature of many such explanations suggests that stagnation is a more apt description of the problem, cf. “[w]hen we examine the history of favored stories for any particular adaptation, we do not trace a tale of increasing truth as one story replaces the last, but rather a chronicle of shifting fads and fashions.” (<xref ref-type="bibr" rid="bib63">Gould, 1978</xref>)</p><p><italic>Analyses of noise and analyses of knowledge</italic>. Previous work has described circular analyses of noise (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>). These analyses have deep similarities with circular analyses of knowledge. Both analyses center on the problem of false discovery and are equivalent in other important respects (Appendix 3).</p></boxed-text><p><italic>Redundant explanations (overspecified models).</italic> Studies sometimes conclude that speculative features should replace or overturn the benchmark features with which they are redundant. Circular analyses of knowledge cannot support such conclusions because they never test the speculative features against a benchmark model. Such analyses must therefore accept, often implicitly, a model that includes all the existing benchmark features and the redundant speculative features.</p><p>In our example, we do not test cosmicity against existing knowledge with which it is redundant and so cannot overturn this existing knowledge. Our analysis implies, therefore, that cosmicity enriches, but does not replace, our existing knowledge. In this way, we must accept the importance of cosmicity and simultaneously accept the importance of existing knowledge with which cosmicity is redundant.</p><p>This problem extends to the acceptance of many, potentially countless, speculative models against the same strawman model. Such acceptance implicitly proposes the simultaneous importance of many, potentially countless, redundant features. Moreover, the circular acceptance of one speculative model after another can give an impression of progress even as it leads to stagnation.</p></sec><sec id="s1-1-3"><title>Unified analysis</title><p>A general solution to this problem centers on significance tests of speculative features against benchmark models (<xref ref-type="fig" rid="fig2">Figure 2</xref>). These tests represent unified analyses of existing knowledge and proposed discovery. They form controlled experiments that test the importance of one feature by controlling for the effects of all known confounding features (<xref ref-type="bibr" rid="bib155">Sibbald and Roland, 1998</xref>; <xref ref-type="box" rid="box2">Box 2</xref>). They also form a type of <italic>severe (model) selection</italic> within Mayo’s framework of <italic>severe testing (</italic><xref ref-type="bibr" rid="bib106">Mayo and Spanos, 2011</xref>; <xref ref-type="bibr" rid="bib107">Mayo, 2018</xref>; Appendix 2). Finally, they parallel controls for model overfitting (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>; Appendix 3).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Tests against benchmark models.</title><p>(<bold>a</bold>) An empirical data sample. The diagram (left) shows a network representation of this sample. This example shows only one empirical data sample, but in general there could be many such samples. (<bold>b</bold>) A speculative feature computed on empirical data. In this example, the feature has the same size as the data, but in general it could have an arbitrary size. Colors denote values of feature elements. (<bold>c</bold>–<bold>d</bold>) Corresponding (<bold>c</bold>) benchmark data samples and (<bold>d</bold>) speculative features computed on these data. (<bold>e</bold>) Empirical test statistic (large black dot) and corresponding benchmark test statistics (small red dots). The effect size reflects the deviation of the empirical test statistic from the benchmark test statistic. The uncertainty (confidence) interval and p-value reflect the statistical significance of this deviation. This panel shows a non-significant effect and thus implies that the speculative feature does not transcend the benchmark model of existing knowledge.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-fig2-v2.tif"/></fig><boxed-text id="box2"><label>Box 2.</label><caption><title>Tests against benchmark models and randomized controlled trials</title></caption><p>Tests against benchmark models have deep similarities with randomized controlled trials, controlled experiments in medical research (<xref ref-type="bibr" rid="bib155">Sibbald and Roland, 1998</xref>). Randomized controlled trials comprise three main steps. The first step randomly splits a sample of people into a treated group and a control group. The second step gives the treatment to people in the treated group and gives a placebo to people in the control group. The third step compares the medical outcomes of the two groups.</p><p>The following list shows that tests against benchmark models (or <italic>tests</italic>) have essentially the same structure as randomized controlled trials (or <italic>trials</italic>), even as they differ in implementational details.</p><list list-type="bullet"><list-item><p>Samples of empirical data (in <italic>tests</italic>) parallel people in the treated group (in <italic>trials</italic>).</p></list-item><list-item><p>Samples of benchmark-model data (in <italic>tests</italic>) parallel people in the control group (in <italic>trials</italic>).</p></list-item><list-item><p>Comparison of test statistics (in <italic>tests</italic>) parallels comparison of medical outcomes (in <italic>trials</italic>).</p></list-item><list-item><p>Maximally random, or unbiased, sampling of benchmark-model data (in <italic>tests</italic>) parallels maximally random, or unbiased, split into the treated and control groups (in <italic>trials</italic>). Both approaches allow, in principle, to control for all known (<italic>tests</italic>) or all possible (<italic>trials</italic>) confounding explanations.</p></list-item></list><p>Despite these similarities, these two types of experiments have one basic difference. Randomized controlled trials can test causality because the treatment always precedes the outcome in time (<xref ref-type="bibr" rid="bib156">Siddiqi et al., 2022</xref>). By contrast, tests against benchmark models can test non-redundancy but do not test causality unless we have additional information about the temporal precedence of speculative and benchmark features.</p></boxed-text><p>In practice, these analyses center on the sampling of data from benchmark-model distributions and on the testing of speculative features against these data. We can describe these analyses in three steps.</p><p>First, we can consider a sample of empirical data. The sample could be as small as a single dataset (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) or it could be larger. We can compute a speculative feature of interest on this sample and summarize this feature with a test statistic (<xref ref-type="fig" rid="fig2">Figure 2b and e</xref>). The empirical test statistic reflects the importance of the corresponding speculative feature. It can also reflect, by extension, the importance of the speculative model that centers on this feature.</p><p>Second, we can get many data samples from a benchmark-model distribution (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). These samples should match the statistics of all benchmark features but be maximally random in all other respects. We can compute the test statistic on these samples and in this way estimate the null distribution — the distribution of the test statistic under the null hypothesis of existing knowledge (<xref ref-type="fig" rid="fig2">Figure 2d and e</xref>).</p><p>Third, we can test the significance of the empirical test statistic against this null distribution by estimating the effect size, uncertainty (confidence) interval, and p-value (<xref ref-type="bibr" rid="bib106">Mayo and Spanos, 2011</xref>). The p-value can reflect the probability that the empirical test statistic does not exceed the benchmark test statistic. In this way, and with appropriate definitions of the test statistic and the benchmark model, the p-value can reflect the probability that our proposed discovery does not transcend existing knowledge.</p><p>In our cosmicity example, we can do this analysis in three steps. First, we can define a test statistic of cosmicity and estimate the empirical value of this statistic. Second, we can define a benchmark model that includes all important existing knowledge about the structure of neural circuits. We can then sample data from this model distribution and estimate the null distribution of the test statistic. Third, we can use this null distribution to estimate the effect size, the uncertainty interval, and the p-value, and in this way test the significance of cosmicity against our existing knowledge of neural circuits.</p><p>As we discussed above, cosmicity is likely to be redundant with our existing knowledge. This likely redundancy suggests that our result is unlikely to be statistically significant. In this context, a finding of statistical significance can serve as genuine evidence for the importance of cosmicity and, by extension, for the importance of cosmic dynamics to brain function.</p></sec></sec><sec id="s1-2"><title>Specific examples</title><p>Previous work has noted that circular analyses of noise can be “hard to understand, imagine, or predict” and “when it’s hard to see how, it can still be happening” (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>). This section shows that circular analyses of knowledge can often be similarly inconspicuous. It first describes possible examples of these analyses in systems neuroscience and probable examples in network neuroscience. It then walks through the details of the problem with a toy analysis. It finally estimates the prevalence of the problem in the network-neuroscience literature.</p><sec id="s1-2-1"><title>Possible circular analyses of knowledge</title><p>Systems neuroscience broadly studies the structure and function of interacting groups of neurons or brain regions. The field variously terms these groups assemblies, populations, circuits, systems, or networks. It has acquired considerable, albeit somewhat scattered, knowledge about the structure and function of these groups. It has also proposed many speculative hypotheses that seek to transcend this existing knowledge.</p><p>We can show how circular analyses of knowledge can lurk in this environment using the example of the systems neuroscience of (mammalian) vision. In line with our discussion, we can first consider the benchmark, speculative, and strawman models of this phenomenon.</p><p><italic>Benchmark model.</italic> Systems neuroscience lacks a benchmark model that captures our existing knowledge about the nature and origin of vision (<xref ref-type="bibr" rid="bib133">Poggio and Serre, 2013</xref>; <xref ref-type="bibr" rid="bib61">Golan et al., 2023</xref>). Despite this lack of a benchmark model, we know many benchmark features relevant to vision. We know, for example, that the visual system tightly balances the activity of inhibitory and excitatory neurons (<xref ref-type="bibr" rid="bib81">Isaacson and Scanziani, 2011</xref>). This balance prevents overinhibition and overexcitation and thus allows animals to sense light and not get seizures (<xref ref-type="bibr" rid="bib101">Ma et al., 2019</xref>). We also know that this balance rests, in part, on the fast-spiking response of inhibitory neurons to excitatory visual stimulation (<xref ref-type="bibr" rid="bib159">Sohal, 2016</xref>). Finally, we know that vision evolved, in virtually all animals, to support visuo-motor interactions, that is, to help animals interact with their environments through movement (<xref ref-type="bibr" rid="bib62">Goodale, 1996</xref>; <xref ref-type="bibr" rid="bib122">Nilsson, 2021</xref>). These basic features do not necessarily form a benchmark model, but they will suffice for our discussion.</p><p><italic>Speculative models.</italic> Systems neuroscience has many speculative models of vision. Many of these models center on the importance of elegant features and often rest on analogies with other natural and synthetic systems. We can consider three prominent examples of these models.</p><p>The first model centers on the importance of <italic>internal representations</italic>, patterns of neuronal activity that internally represent visual stimuli (<xref ref-type="bibr" rid="bib40">Craik, 1943</xref>; <xref ref-type="bibr" rid="bib78">Hubel and Wiesel, 1959</xref>). Studies have proposed that the visual system interprets the meaning of internal representations much like an artificial neural network decodes the nature of input images (<xref ref-type="bibr" rid="bib90">Kriegeskorte, 2015</xref>; <xref ref-type="bibr" rid="bib139">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Cichy and Kaiser, 2019</xref>; <xref ref-type="fig" rid="fig1">Figure 1e–f</xref>). Despite these intuitions, we have no evidence that patterns of neuronal activity actually denote internal representations (<xref ref-type="bibr" rid="bib87">Kenny, 1971</xref>; <xref ref-type="bibr" rid="bib20">Brette, 2019</xref>; <xref ref-type="bibr" rid="bib14">Bennett and Hacker, 2022</xref>). Moreover, in many cases, we may be able to explain these patterns as substrates of visuo-motor interactions without the need to assume that they internally represent anything at all (<xref ref-type="bibr" rid="bib54">Freeman and Skarda, 1990</xref>; <xref ref-type="bibr" rid="bib28">Cao, 2020</xref>; <xref ref-type="bibr" rid="bib48">Driscoll et al., 2022</xref>).</p><p>The second model centers on the importance of <italic>gamma oscillations</italic>, fast rhythms of neuronal activity that correlate with visual perception (<xref ref-type="bibr" rid="bib65">Gray et al., 1989</xref>; <xref ref-type="bibr" rid="bib24">Burwick, 2014</xref>). Studies have proposed that gamma oscillations bind simple visual stimuli into complex perception, much like orchestra conductors weave the sounds of individual musicians into complex music (<xref ref-type="bibr" rid="bib157">Singer, 2001</xref>; <xref ref-type="bibr" rid="bib25">Buzsáki and Draguhn, 2004</xref>). Despite these intuitions, we know that gamma oscillations are absent during the perception of some images, and so may not be necessary to bind stimuli into perception (<xref ref-type="bibr" rid="bib74">Hermes et al., 2015b</xref>; <xref ref-type="bibr" rid="bib73">Hermes et al., 2015a</xref>). Moreover, in many cases, we may be able to explain these oscillations as the inevitable outcomes of inhibitory responses to visual stimulation without the need to assume that they bind anything at all (<xref ref-type="bibr" rid="bib137">Ray and Maunsell, 2015</xref>; <xref ref-type="bibr" rid="bib158">Singer, 2018</xref>).</p><p>The third model centers on the importance of neural criticality, collective neuronal activity that balances on the edge of order and disorder. Studies have proposed that criticality optimizes our sensitivity to visual stimuli, much like the critical (neither shallow nor steep) angle of a sand pile optimizes its responsiveness to tactile stimuli (<xref ref-type="bibr" rid="bib152">Shew et al., 2009</xref>; <xref ref-type="bibr" rid="bib153">Shew and Plenz, 2013</xref>). Despite these intuitions, we know that signatures of criticality can occur in the absence of any visual stimuli and so may not necessarily be related to optimized visual sensation (<xref ref-type="bibr" rid="bib52">Fontenele et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Destexhe and Touboul, 2021</xref>). Moreover, in many cases, we may be able to explain these signatures as inevitable outcomes of balanced inhibitory and excitatory activity without the need to assume that they optimize anything at all (<xref ref-type="bibr" rid="bib118">Nanda et al., 2023</xref>).</p><p><italic>Strawman models.</italic> We cannot summarize the full range of null models in the expansive literature of representations, oscillations, and criticality. We can still do justice to this literature, however, by considering some of its strongest models. One such model can test the significance of representations against correlations of neuronal activity across space and time (<xref ref-type="bibr" rid="bib49">Elsayed and Cunningham, 2017</xref>). Another model can test the significance of oscillations against non-oscillatory activity of similar amplitude (<xref ref-type="bibr" rid="bib47">Donoghue et al., 2022</xref>). A third model can test the significance of critical neuronal activity against mimicking non-critical (lognormal) phenomena (<xref ref-type="bibr" rid="bib26">Buzsáki and Mizuseki, 2014</xref>). Together, all these models can test representations, oscillations, and criticality against important confounders. Despite this, none of these models test these speculative features against the benchmark features with which they may be redundant.</p><p><italic>Circular analyses and redundant explanations.</italic> Tests against strawman models often accept the importance of representations, oscillations, and criticality. Separately, these tests cannot reject the importance of benchmark features with which these speculative features may be redundant. It follows that these tests may implicitly explain the same aspects of brain activity twice — first as a basic benchmark feature and second as a redundant speculative feature. In the study of vision, these analyses may therefore conclude the simultaneous importance of:</p><list list-type="order"><list-item><p>Visuo-motor interactions and internal representations possibly redundant with these interactions.</p></list-item><list-item><p>Inhibitory responses to stimulation and gamma oscillations possibly redundant with these responses.</p></list-item><list-item><p>Balance of inhibition and excitation and critical activity possibly redundant with this balance.</p></list-item></list><p>Individually, these analyses accept simple or elegant models. Collectively, however, they may accept a needlessly complicated model that assumes the simultaneous importance of several redundant features.</p></sec><sec id="s1-2-2"><title>Probable circular analyses of knowledge</title><p>Many parts of systems neuroscience, such as the study of vision, lack well-defined benchmark models or the ability to test speculative models against these benchmarks. These limitations make it hard to show the presence of circular analyses of knowledge, even when they exist.</p><p>Some parts of systems neuroscience, however, have relatively well-defined benchmark models and the ability to test speculative models against these benchmarks. These strengths make it possible to show the presence of circular analyses of knowledge when they exist. Here, we can describe the probable presence of such analyses in network neuroscience.</p><p>Network neuroscience is a subfield of systems neuroscience that studies the structure and function of extensive, including whole-brain, networks (<xref ref-type="bibr" rid="bib10">Bassett and Sporns, 2017</xref>). Nodes in these networks typically denote cells or regions, while links typically denote synapses, axonal projections, or activity correlations. We can show probable circular analyses in this field using the example of the network neuroscience of (mammalian) cortex. In line with our previous discussion, we can first consider the benchmark, speculative, and strawman models of this structure.</p><p><italic>Benchmark model.</italic> We have considerable knowledge of evolution, development, structure, and function of cortical networks. First, <italic>evolutionary</italic> analyses of extensive mapping studies suggest that essentially all mammals share a common cortical blueprint (<xref ref-type="bibr" rid="bib84">Kaas, 1995</xref>; <xref ref-type="bibr" rid="bib93">Krubitzer, 1995</xref>; <xref ref-type="fig" rid="fig3">Figure 3a</xref>). Second, the commonality of this blueprint likely stems from strongly conserved <italic>developmental</italic> processes. These processes include an initial establishment of spatial concentration gradients of developmental molecules and a subsequent discretization of these gradients (<xref ref-type="fig" rid="fig3">Figure 3b–c</xref>). Third, signatures of these developmental processes show through in the <italic>structure</italic> of the adult cortex. To a first approximation, this structure reflects a gradual transition along the cortical sheet (<xref ref-type="fig" rid="fig3">Figure 3d</xref>):</p><list list-type="bullet"><list-item><p>from a relatively well-delineated, clustered, and poorly connected sensory-motor cortex.</p></list-item><list-item><p>to a relatively ill-delineated, distributed, and highly connected association cortex.</p></list-item></list><p>(The sensory-motor cortex is well-delineated in large part because it comprises cortical areas that form spatial mappings of entire sensory or motor fields. For example, the primary somatosensory area comprises a spatial mapping of all body parts that can receive somatic input. By contrast, the association cortex is ill-delineated in large part because it lacks areas that form similarly clear mappings of complete sensory or motor fields [<xref ref-type="bibr" rid="bib22">Buckner and Krienen, 2013</xref>; <xref ref-type="bibr" rid="bib131">Patel et al., 2014</xref>].)</p><p>Fourth, this cortical structure constrains known cortical <italic>function</italic>. Specifically, a gradual transition from a relatively well-delineated sensory-motor cortex to a relatively ill-delineated association cortex reflects a corresponding transition from relatively well-defined sensory-motor function to relatively ambiguous cognitive function (<xref ref-type="bibr" rid="bib12">Bayne et al., 2019</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>A blueprint of large-scale cortical networks.</title><p>(<bold>a</bold>) Rostrocaudal (nose-to-tail) maps of shared cortical regions in three popular mammalian model organisms. Virtually all mammals have well-delineated primary and other sensory areas, and an ill-delineated posterior parietal association cortex. In addition, most mammals have well-delineated primary and other motor areas (not highlighted in this panel). (<bold>b</bold>–<bold>c</bold>) Gradients of cortical development. (<bold>b</bold>) Spatial gradients of morphogen concentration induce corresponding (<bold>c</bold>) spatial gradients of transcription-factor and gene expression. Morphogens are signaling molecules that establish spatial concentration gradients through extracellular diffusion from specific sites. Transcription factors (names in italics) are intracellular proteins that establish spatial gradients of gene expression. The discretization of these gradients during development results in the formation of discrete cortical areas and systems (colors in b). (<bold>d</bold>) A schematized blueprint of a macaque cortical network reflects a gradual transition of a relatively clustered sensory-motor cortex (red and green) into a relatively distributed association cortex (gray). Circles denote cortical regions, while lines denote interregional projections. V1 and A1 denote primary visual and auditory areas, while PPC denotes posterior parietal association cortex. Panel (a) is adapted from Figure 3 of <xref ref-type="bibr" rid="bib94">Krubitzer and Prescott, 2018</xref>. Panel (b) is adapted from Figure 1.3b of <xref ref-type="bibr" rid="bib67">Grove and Monuki, 2020</xref>. Panel (c) is adapted from Figure 2 of <xref ref-type="bibr" rid="bib15">Borello and Pierani, 2010</xref>. Panel (d) is adapted from Figure 2d of <xref ref-type="bibr" rid="bib112">Mesulam, 1998</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-fig3-v2.tif"/></fig><p>Network neuroscience has a well-known model that captures the basic features of this cortical blueprint (<xref ref-type="bibr" rid="bib162">Sporns, 2013</xref>). This model includes two types of benchmark features. First, it includes network modules (clusters) that capture the clustered sensory-motor cortex. Second, it includes node connectivity (number of connections) that captures the gradual transition from the poorly connected sensory-motor cortex to the well-connected association cortex. We can adopt this basic benchmark model for our subsequent discussion.</p><p><italic>Speculative models.</italic> Speculative models in network neuroscience broadly resemble other speculative models in systems neuroscience. These models center on the importance of elegant features and often rest on analogies with other natural and synthetic systems, such as metabolic, transport, and friendship networks (<xref ref-type="bibr" rid="bib8">Barabási, 2016</xref>; <xref ref-type="fig" rid="fig1">Figure 1g–h</xref>). In contrast to broader systems neuroscience, however, speculative features in network neuroscience are often more clearly redundant with benchmark features. We can show this redundancy in three speculative models that reflect some of the best-known results in network neuroscience.</p><p>The first model centers on the importance of small-world structure (<xref ref-type="bibr" rid="bib166">Stephan et al., 2000</xref>; <xref ref-type="bibr" rid="bib1">Achard et al., 2006</xref>). This structure denotes the simultaneous presence of many network triangles (triplets of fully connected nodes) and many network shortcuts (connections between different network parts). Studies have proposed that small-world cortical structure optimizes the competing demands of functional segregation and integration (<xref ref-type="bibr" rid="bib161">Sporns and Zwi, 2004</xref>). We also know, however, that this structure is redundant with connected sensory-motor modules: “modular systems are small-world but not all small-world systems are modular” (<xref ref-type="bibr" rid="bib113">Meunier et al., 2010</xref>).</p><p>The second model centers on the importance of cores or clubs (<xref ref-type="bibr" rid="bib70">Hagmann et al., 2008</xref>; <xref ref-type="bibr" rid="bib190">Zamora-López et al., 2010</xref>; <xref ref-type="bibr" rid="bib175">van den Heuvel and Sporns, 2011</xref>). These structures denote groups of highly connected nodes. Studies have proposed that cores or clubs of the association cortex form the backbone of functional integration and may underpin the global workspace, a theoretical substrate of consciousness (<xref ref-type="bibr" rid="bib66">Griffa and van den Heuvel, 2018</xref>). We also know, however, that these structures are redundant with sensory-motor modules and highly connected association nodes (hubs): “clubs are structural byproducts of modules and hubs” (<xref ref-type="bibr" rid="bib144">Rubinov, 2016</xref>).</p><p>The third model centers on the importance of node controllability (<xref ref-type="bibr" rid="bib169">Tang et al., 2012</xref>; <xref ref-type="bibr" rid="bib68">Gu et al., 2015</xref>). High-control nodes in dynamical systems mediate switches between network activity (system states). Studies have proposed that high-control cortical nodes may support internal cognitive control and may serve as levers for external cortical control (<xref ref-type="bibr" rid="bib170">Tang and Bassett, 2018</xref>). We also know, however, that node control-lability is roughly equivalent with node connectivity (degree) (<xref ref-type="bibr" rid="bib173">Tu et al., 2018</xref>) or related features (<xref ref-type="bibr" rid="bib130">Patankar et al., 2020</xref>): “a strong […] correlation between node degree and average controllability is mathematically expected” (<xref ref-type="bibr" rid="bib68">Gu et al., 2015</xref>).</p><p><italic>Strawman models.</italic> Studies of small worlds, cores/clubs, and controllability use a relatively limited set of null models. First, tests of small worlds and cores/clubs tend to follow the broader network-science literature and use null models that include node connectivity but not network modules (<xref ref-type="bibr" rid="bib182">Watts and Strogatz, 1998</xref>; <xref ref-type="bibr" rid="bib38">Colizza et al., 2006</xref>). Second, many tests of controllability use abstract null models that lack node connectivity or network modules (<xref ref-type="bibr" rid="bib129">Pasqualetti et al., 2019</xref>). Third, many studies also use null models that include the empirical decay of connectivity with spatial distance (<xref ref-type="bibr" rid="bib104">Markov et al., 2013</xref>). These spatial models can account for much variance in the data and are perhaps the strongest network-neuroscience null models in common use today (<xref ref-type="bibr" rid="bib85">Kaiser and Hilgetag, 2006</xref>). Despite these strengths, these models lack node connectivity or network modules and cannot compete with benchmark models that include these features (<xref ref-type="bibr" rid="bib144">Rubinov, 2016</xref>).</p><p><italic>Circular analyses and redundant explanations.</italic> Tests of small worlds, cores/clubs, and controllability against strawman models will almost invariably accept the importance of these speculative features. Separately, these tests cannot reject the importance of benchmark features with which these speculative features are redundant. It follows that these circular analyses implicitly explain the same aspects of network structure twice: first as a basic benchmark feature, and second as a redundant speculative feature. Individually, these analyses accept simple or elegant models. Collectively, however, they accept a needlessly complicated model that assumes the simultaneous importance of sensory-motor modules, highly connected association nodes, small worlds, cores/clubs, and controllability.</p></sec><sec id="s1-2-3"><title>Walkthrough circular analysis of knowledge</title><p>We can show the details of this problem with a walkthrough analysis of a toy cortical network. This network has an accentuated transition from clustered to distributed cortical connectivity (<xref ref-type="fig" rid="fig4">Figure 4a–b</xref>, left). We can propose a speculative model of this network that centers on a toy feature of a controllable core. This hybrid feature represents a core of cortical regions whose activity can be induced with relatively little stimulation. Theory suggests that this controllable core may support a stable state of cortical activity and thus play an important role in cortical function. Despite these considerations, the existence and importance of this feature remain speculative without tests against a benchmark model.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Example analysis.</title><p>(<bold>a</bold>) Left: A toy cortical network. Right: A matrix that reflects the controllability of specific network states (a one-rank approximation of the controllability Gramian [<xref ref-type="bibr" rid="bib21">Brunton and Kutz, 2019</xref>]). Dashed lines delineate the controllable core. The test statistic is the logarithm of the sum of all matrix elements within this core. (<bold>b</bold>) Left: Data samples from a benchmark-model distribution. The benchmark model includes empirical network modules and node connectivity (red overlays). Right: Controllable cores in benchmark-model data. Rightmost: Empirical (large black dot) and benchmark test statistics (small red dots). (<bold>c</bold>) Left: Data samples from a strawman model distribution. The strawman model includes node connectivity but not empirical network modules (red overlay). Right: Controllable cores in strawman-model data. Rightmost: Empirical test statistic (large black dot) and strawman test statistics (small red dots).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-fig4-v2.tif"/></fig><p>We can test this feature against a benchmark model in three steps. First, we can define a test statistic that reflects the importance of this feature. In our example, we can define this statistic to be the core density of controllable network nodes (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, right). Second, we can compute the value of this statistic on empirical and benchmark-model data (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). Third, we can use these values to quantify the effect size, uncertainty interval, and p-value. In our analysis, the empirical test statistic is 3.02, while the median [95% uncertainty interval] benchmark-test statistic is 3.00 [2.87, 3.12] (arbitrary units). The corresponding effect size of 0.02 [−0.11, 0.15] and p=0.36 (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, right) suggest that the empirical test statistic is not significant against benchmark-model data. This analysis suggests that the controllable core is redundant with our existing knowledge.</p><p>Separately, we can test the significance of a controllable core against a strawman model. (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). In our analysis, the strawman-model statistic is 2.72 [2.62, 2.88]. The corresponding effect size of 0.30 [0.14, 0.40] and p&lt;0.01 (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, right) suggest a rejection of this strawman model. This rejection is circular because the strawman model excludes the benchmark feature with which the controllable core is redundant.</p></sec><sec id="s1-2-4"><title>Prevalence of probable circular analyses of knowledge</title><p>I quantified the fraction and number of probable circular analyses of knowledge in the network-neuroscience literature. I did this by evaluating network-neuroscience studies published during five recent years in ten journals. Appendix 4 describes the details of this evaluation.</p><p>This evaluation shows that 56% of evaluated studies had at least one circular analysis of knowledge. A simple extrapolation suggests that this problem may have affected more than three thousand original studies published over the last decade. This extrapolation is necessarily a rough estimate. It may be upwardly biased if my sample is unrepresentative of the broader literature or downwardly biased if my search criteria missed other affected articles. Despite these limitations, this extrapolation forms a useful indicator of the magnitude of this problem in the literature.</p><p>I did not try to assess the effects of this problem on individual results. These effects will depend on the aims and conclusions of individual studies. For example, circular analyses of knowledge in some studies may be tangential to the main results and may not affect the main conclusions. Separately, circular analyses in other studies may make the main results seem falsely novel or important and, in this way, may severely distort the main conclusions. Overall, I agree with a previous similar evaluation of the literature (<xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref>) that such effects should be assessed through systematic community efforts.</p><p>To facilitate these efforts, I created a semi-automated analysis pipeline that downloads and curates all published studies that match some specified search criteria (Appendix 4). The curation includes the extraction of the Methods and Results sections and the highlighting of possible descriptions of benchmark, speculative, or strawman models. This basic curation cannot replace the careful evaluation of individual articles, but it may help to make such an evaluation standardized and more objective.</p></sec></sec><sec id="s1-3"><title>Speculative evidence</title><p>The commonness of circular analyses of knowledge may reflect, in part, the intuitive importance of many speculative models. This importance often rests on the misleading suggestiveness of speculative evidence. The ability to spot such evidence can help to shift the focus from speculative intuitions to rigorous tests and, in this way, alleviate much of this problem in the literature.</p><p>This section discusses how suggestive terminology, suggestive structure, and suggestive narratives can all falsely signal the importance of speculative features. This discussion aligns with similar perspectives in neuroscience (<xref ref-type="bibr" rid="bib88">Krakauer et al., 2017</xref>), machine learning (<xref ref-type="bibr" rid="bib100">Lipton and Steinhardt, 2019</xref>), and psychology (<xref ref-type="bibr" rid="bib188">Yarkoni, 2020</xref>).</p><sec id="s1-3-1"><title>Suggestive terminology: Deepities</title><p>The term <italic>deepity</italic> denotes a word or phrase that has two distinct meanings (<xref ref-type="bibr" rid="bib44">Dennett, 2013</xref>). The first meaning is direct and undisputed but bland, while the second is profound but indirect and speculative. Deepities do damage when they lead us to conflate the two meanings and, in this way, make speculative or redundant features seem well-supported.</p><p>Many bedrock terms or ideas in systems neuroscience are deepities because they conflate facts with speculations (<xref ref-type="table" rid="table2">Table 2</xref>). Here, we can show this conflation using three especially consequential terms: <italic>function</italic>, <italic>emergence</italic>, and <italic>significance</italic>. We can do so using a toy example of “lub-dub” heart sounds, features that arise as byproducts of turbulent blood flow.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Example deepities.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Deepity</th><th align="left" valign="bottom">Direct meaning</th><th align="left" valign="bottom">Implicit allusion</th></tr></thead><tbody><tr><td align="left" valign="top">Neural computation (<xref ref-type="bibr" rid="bib31">Churchland and Sejnowski, 2016</xref>)</td><td align="left" valign="top">Transformation of sensory input to behavioral output.</td><td align="left" valign="top">Computer-like transformation of sensory input to behavioral output.</td></tr><tr><td align="left" valign="top">Neural representation, code, or information (<xref ref-type="bibr" rid="bib7">Baker et al., 2022</xref>; <xref ref-type="bibr" rid="bib20">Brette, 2019</xref>; <xref ref-type="bibr" rid="bib123">Nizami, 2019</xref>)</td><td align="left" valign="top">Patterns of neuronal activity that correlate with, or change in response to, sensory input.</td><td align="left" valign="top">Internal representations or encodings of information about the external world.</td></tr><tr><td align="left" valign="top">Neural networks (<xref ref-type="bibr" rid="bib19">Bowers et al., 2022</xref>)</td><td align="left" valign="top">Artificial neural networks (machine-learning models).</td><td align="left" valign="top">Biological neural networks.</td></tr><tr><td align="left" valign="top">Necessity and sufficiency (<xref ref-type="bibr" rid="bib189">Yoshihara and Yoshihara, 2018</xref>)</td><td align="left" valign="top">The induction or suppression of behavior through stimulation or inhibition of neural substrate.</td><td align="left" valign="top">Logical equivalence between behavior and neural substrate.</td></tr><tr><td align="left" valign="top">Functional connectivity (<xref ref-type="bibr" rid="bib138">Reid et al., 2019</xref>)</td><td align="left" valign="top">Correlated neural activity.</td><td align="left" valign="top">Neural connectivity that causes function.</td></tr><tr><td align="left" valign="top">Complexity (<xref ref-type="bibr" rid="bib111">Merker et al., 2022</xref>)</td><td align="left" valign="top">Patterns of neural structure that are neither ordered nor disordered.</td><td align="left" valign="top">Patterns of neural structure that are fundamentally important.</td></tr><tr><td align="left" valign="top">Motifs</td><td align="left" valign="top">Repeating patterns of brain-network connectivity.</td><td align="left" valign="top">Motifs of neural computation.</td></tr><tr><td align="left" valign="top">Efficiency</td><td align="left" valign="top">Communication between pairs of brain nodes via algorithmic sequences of connections.</td><td align="left" valign="top">Efficiency of neural communication.</td></tr><tr><td align="left" valign="top">Modularity</td><td align="left" valign="top">Propensity of brain networks to be divided into clusters.</td><td align="left" valign="top">Propensity of brain networks to be robust or evolvable.</td></tr><tr><td align="left" valign="top">Flexibility</td><td align="left" valign="top">Propensity for brain nodes to dynamically switch their cluster affiliations.</td><td align="left" valign="top">Propensity for cognitive flexibility.</td></tr><tr><td align="left" valign="top">The brain is a network, like many other natural and synthetic systems.</td><td align="left" valign="top">The brain consists of connected elements, like many other natural and synthetic systems.</td><td align="left" valign="top">The brain shares functional network principles with many natural and synthetic systems.</td></tr><tr><td align="left" valign="top">Brain disorders are disconnection syndromes.</td><td align="left" valign="top">Brain disorders are correlated with brain-network abnormalities.</td><td align="left" valign="top">Brain disorders are caused by brain-network abnormalities.</td></tr></tbody></table></table-wrap><p>First, <italic>function</italic> can denote physiological activity and also signal functional utility (<xref ref-type="bibr" rid="bib142">Roux, 2014</xref>). The conflation of these two meanings may falsely attribute utility to all physiological phenomena. The heart pumps blood and makes lub-dub sounds, but only one of these actions is useful.</p><p>Second, <italic>emergent</italic> phenomena can denote higher-order structures in complex systems and also signal the importance of these structures (<xref ref-type="bibr" rid="bib13">Bedau, 1997</xref>). The conflation of these two meanings may falsely attribute functional importance to higher-order structures. The structure of turbulent blood flow is emergent, but this flow plays no important role in heart function.</p><p>Third, <italic>significance</italic> can denote the rejection of a null hypothesis and also signal scientific importance (<xref ref-type="bibr" rid="bib181">Wasserstein and Lazar, 2016</xref>). The conflation of these two meanings may falsely attribute importance to statistically significant features, especially if these features are also <italic>functional</italic> and <italic>emergent</italic>. In practice, the importance of a statistically significant result is strongly tied to the nature of the null hypothesis. A weak null hypothesis may propose, for example, that heart sounds are equally loud in still and beating hearts. We will always reject this null hypothesis, but such rejection will tell us little about the importance of heart sounds.</p><p>Collectively, the use of deepities can make speculative features seem useful or important. Moreover, the ability to fall back on the direct meanings of deepities in response to criticism, and to promote their implicit allusions at other times, can make deepities easy to defend and thus hard to eliminate. (This defense of deepities is known as “motte and bailey”, by analogy with a defense of a medieval castle [<xref ref-type="bibr" rid="bib151">Shackel, 2005</xref>]. The motte is a hill with a tower — it is easily defensible but not particularly enjoyable to spend time in. The bailey is an outside court — it is enjoyable but not particularly defensible. The motte-and-bailey defense denotes a retreat to the motte in response to attacks and enjoyment of the bailey during more peaceful times.)</p></sec><sec id="s1-3-2"><title>Suggestive structure: Spandrels</title><p>In architecture, spandrels denote triangular spaces of building arches (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). These spaces arise as byproducts of the contours of the arch, but their intricate decoration may suggest that they have important (decorative) function. In biology, spandrels are phenotypes that have intricate and similarly suggestive structure (<xref ref-type="bibr" rid="bib64">Gould and Lewontin, 1979</xref>). For example, the intricate structure of turbulent lub-dub flow, and the ability of this flow to predict heart activity and physical exertion, may all suggest that lub-dub sounds play an important role in heart function. The intricate structure and predictive success of many features in systems neuroscience may likewise suggest that these features play an important role in brain function (<xref ref-type="fig" rid="fig5">Figure 5b–c</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Example spandrels.</title><p>(<bold>a</bold>) Spandrels in architecture denote triangular spaces of building arches (left, orange). Existing knowledge (gray) may explain these spaces as byproducts, but their intricate structure (right, orange) may suggest that they have important function. (<bold>b</bold>) An illustrative depiction of a “manifold” representation of neuronal population activity (orange). Axes denote directions of neuronal population activity in low-dimensional space. The intricate structure and predictive success of this feature may suggest that it plays an important role in neural function. The difficulty of testing this importance against existing knowledge (not shown) can make this importance speculative. (<bold>c</bold>) An illustrative depiction of a cortical core (orange). The intricate structure of this feature may suggest that it plays an important role in neural function. The relative ease of testing this importance against existing knowledge (gray) makes it possible to show that this feature is ultimately redundant. Panels (a) and (c) are adapted from (respectively) Figure 2b and Figure 1a of <xref ref-type="bibr" rid="bib144">Rubinov, 2016</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-fig5-v2.tif"/></fig><p>The concept of spandrels helps to show the value of tests against benchmark models. For example, the lack of a benchmark model of vision makes it difficult to test the significance of internal representations against visuo-motor interactions (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This difficulty can make the existence and importance of internal representations inconclusive. Such inconclusiveness, in turn, may help to explain the vigorous and unsettled debates over the nature of this and other speculative features in systems neuroscience (<xref ref-type="bibr" rid="bib97">Langdon et al., 2023</xref>; <xref ref-type="bibr" rid="bib159">Sohal, 2016</xref>; <xref ref-type="bibr" rid="bib45">Destexhe and Touboul, 2021</xref>). By contrast, well-defined benchmark models of cortical networks make it relatively easy to show the redundancy of cores or clubs against these models (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). This ease may help explain the lack of comparable debates over the nature of these and other redundant features in network neuroscience (<xref ref-type="bibr" rid="bib99">Liao et al., 2017</xref>; <xref ref-type="bibr" rid="bib163">Sporns, 2018</xref>; <xref ref-type="bibr" rid="bib129">Pasqualetti et al., 2019</xref>).</p></sec><sec id="s1-3-3"><title>Suggestive narratives: Just-so stories</title><p>Just-so stories in biology are intriguing but speculative narratives that suggest the presence of theoretically elegant or optimal biological function (<xref ref-type="bibr" rid="bib63">Gould, 1978</xref>; <xref ref-type="bibr" rid="bib18">Bowers and Davis, 2012</xref>). A just-so-story may suggest, for example, that heart sounds exist to warn of overexertion and thus help minimize energy expenditure. Just-so stories can be difficult to falsify because it is often easy to reexplain some evident non-optimality as a globally optimal tradeoff between competing objectives (<xref ref-type="bibr" rid="bib64">Gould and Lewontin, 1979</xref>). <xref ref-type="table" rid="table3">Table 3</xref> shows examples of such stories in the recent systems-neuroscience literature.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Example stories.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Concept</th><th align="left" valign="bottom">Initial narrative of optimality</th><th align="left" valign="bottom">Evidence of suboptimality (strong but unviable null model)</th><th align="left" valign="bottom">Restoration of optimality through the inclusion of an ad hoc tradeoff</th><th align="left" valign="bottom">Alternative benchmark narrative (strong and viable null model)</th></tr></thead><tbody><tr><td align="left" valign="top">Criticality (<xref ref-type="bibr" rid="bib52">Fontenele et al., 2019</xref>; <xref ref-type="bibr" rid="bib185">Wilting and Priesemann, 2019</xref>; <xref ref-type="bibr" rid="bib118">Nanda et al., 2023</xref>)</td><td align="left" valign="top">Brain activity always and exactly balances between order and disorder. This allows it to optimize information transmission and storage.</td><td align="left" valign="top">Brain activity does not always or exactly balance between order and disorder.</td><td align="left" valign="top">Brain activity optimizes the tradeoffs between the benefits of criticality and the competing benefits of flexibility or stability.</td><td align="left" valign="top">Brain activity avoids the extremes of overinhibition and overexcitation and is not optimal over and above this avoidance-of-extremes baseline.</td></tr><tr><td align="left" valign="top">Predictive coding (<xref ref-type="bibr" rid="bib168">Sun and Firestone, 2020</xref>; <xref ref-type="bibr" rid="bib174">Van de Cruys et al., 2020</xref>; <xref ref-type="bibr" rid="bib149">Seth et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Cao, 2020</xref>)</td><td align="left" valign="top">Brain activity aims to optimally predict incoming sensory input.</td><td align="left" valign="top">Brain activity optimally predicts sensory input in dark and quiet spaces. Despite this, animals tend not to seek out such spaces.</td><td align="left" valign="top">Brain activity aims to optimize the tradeoffs between predictions that are accurate and predictions that are motivational.</td><td align="left" valign="top">Brain activity reacts to sensory input but does not aim to optimally predict this input.</td></tr><tr><td align="left" valign="top">Wiring minimization (<xref ref-type="bibr" rid="bib104">Markov et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Bullmore and Sporns, 2012</xref>; <xref ref-type="bibr" rid="bib144">Rubinov, 2016</xref>)</td><td align="left" valign="top">Brain-network structure globally minimizes wiring cost and therefore optimizes wiring economy.</td><td align="left" valign="top">Brain-network structure does not globally minimize wiring cost.</td><td align="left" valign="top">Brain-network structure optimizes the tradeoffs between wiring cost and communication efficiency.</td><td align="left" valign="top">Brain networks have long connections that enable specific sensory-motor function but do not optimize global communication.</td></tr></tbody></table></table-wrap><p>The difficulty of falsifying just-so stories also helps to show the value of tests against benchmark models. Assertions of suboptimality form strong but unviable null models (<xref ref-type="table" rid="table3">Table 3</xref>, third column). Acceptance of these models, in other words, does not offer a viable alternative explanation to replace the original narrative. Without such a viable alternative, it becomes easy to hold on to the original narrative, typically by introducing an ad hoc tradeoff that restores optimality (<xref ref-type="table" rid="table3">Table 3</xref>, fourth column). This process may help to explain why just-so stories can hold sway in the field long after they are rejected against strong null models. By contrast, benchmark models form strong and viable null models (<xref ref-type="table" rid="table3">Table 3</xref>, fifth column). The acceptance of these models offers viable alternative explanations of brain function and, in this way, makes it easier to eliminate the original narrative (Appendix 2).</p></sec></sec><sec id="s1-4"><title>Stagnation and progress</title><p>The commonness of circular analyses of knowledge can help explain a seeming disconnect between the fast pace of everyday discovery and the slow pace of real progress. <xref ref-type="bibr" rid="bib37">Cobb, 2020</xref> described the nature of this disconnect in neuroscience:</p><list list-type="simple"><list-item><p>“There are now tens of thousands of brain researchers around the world, beavering away in a bewildering range of new subdisciplines […] each with their own questions, methods and approaches. Thousands of research articles relating to brain function appear each year.” Despite this, “[i]n reality, no major conceptual innovation has been made in our overall understanding of how the brain works for over half a century.”</p></list-item></list><p>On the one hand, circular analyses of knowledge can enable a fast pace of intriguing, and often replicable, everyday discoveries. On the other hand, the speculative and redundant nature of these discoveries does not lead to revisions of benchmark models and, in this way, results in a lack of real progress. <xref ref-type="bibr" rid="bib77">Horgan, 2015</xref> introduced the term “ironic science” to describe the nature of this process:</p><list list-type="simple"><list-item><p>“Ironic science [acceptance of intriguing but speculative models] offers points of view, opinions, which are, at best, interesting, which provoke further comment. But it does not converge on the truth [lead to acceptance of truer models]. It cannot achieve empirically verifiable surprises that force scientists to make substantial revisions in their basic description of reality [make substantial revisions to benchmark models].”</p></list-item></list><p>Tests against benchmark models can help resolve this disconnect by ultimately linking the value of proposed discovery with revisions of benchmark models. Particle physics provides a good example of these tests in action. This field has the Standard Model, perhaps the most successful benchmark model in all of science today. The field seeks to revise this model but refreshingly accepts, and indeed embraces, the everyday failure to do so. <xref ref-type="bibr" rid="bib39">Cousins, 2017</xref> aptly summarized the nature of this practice:</p><list list-type="simple"><list-item><p>“In many searches in [particle physics], there is a hope to reject the [Standard Model] and make a major discovery […]. But there is nonetheless high (or certainly non-negligible) prior belief in the null hypothesis. The literature, including the most prestigious journals, has many papers […] that report no significant evidence for the sought-for [beyond-the-Standard-Model] physics. Often these publications provide useful constraints on theoretical speculation, and offer guidance for future searches.”</p></list-item></list><p>In contrast to particle physics, benchmark models are often ill-defined in more expansive fields, such as psychology or sociology. The difficulty of evaluating real progress in these fields can make practitioners throw up their hands in despair (<xref ref-type="bibr" rid="bib188">Yarkoni, 2020</xref> gives an example from psychology). It may also make them avoid tests against null models altogether. For example, <xref ref-type="bibr" rid="bib59">Gelman et al., 2020</xref> noted:</p><list list-type="simple"><list-item><p>“We do not generally use null hypothesis significance testing in our own work. In the fields in which we work [social science and public health], we do not generally think null hypotheses can be true [cf. strawman models can be truer than speculative models]. We do not find it particularly helpful to formulate and test null hypotheses that we know ahead of time cannot be true [cf. almost invariably accept speculative models against strawman models].”</p></list-item></list><p>Systems neuroscience probably lies somewhere between particle physics and social science. Some parts of the field, such as network neuroscience, are sufficiently circumscribed to allow tests of new models against well-delineated benchmark models. To be clear, it is unlikely that the field can converge on benchmark models that resemble the Standard Model or even remotely approach the explanatory success of this model. Despite these limitations, the adoption of routine tests against benchmark models can help place the field on a rigorous foundation and in this way facilitate real progress.</p></sec><sec id="s1-5"><title>Practical details</title><p>This section describes the practical details of testing new models against benchmark models. It first describes steps to integrate existing knowledge into benchmark models. It then discusses methods to sample data from benchmark-model distributions. It finally proposes practical steps to establish a culture of rigorous tests.</p><sec id="s1-5-1"><title>Integrating knowledge</title><p>Benchmark models should include all aspects of important existing knowledge about some phenomenon of interest. The need to include <italic>all</italic> knowledge reflects not dogma but the objective importance of control for <italic>all</italic> known confounding explanations. This need parallels the need to control for <italic>all</italic> aspects of the noise in tests on independent data (Appendix 3) or the need to control for <italic>all</italic> confounding explanations in randomized controlled trials (<xref ref-type="box" rid="box2">Box 2</xref>).</p><p>In principle, the inclusion of all important existing knowledge can seem daunting. In practice, however, this inclusion already happens routinely, albeit often informally, in books, reviews, and detailed Introduction sections of original articles. For example, an Introduction section that describes the importance of features <italic>a</italic>, <italic>b,</italic> and <italic>c</italic> to some phenomenon of interest, informally includes all these features in a benchmark model of this phenomenon.</p><p>Features that comprise important existing knowledge should rest on rigorous evidence from extensive observations or controlled experiments. Such evidence generally points to strong similarities between the nervous system and other body systems, and to strong similarities between the nervous systems of distantly related species. These similarities span functional objectives, structural building blocks, and developmental processes.</p><p>We know, for example, that other body systems use effective but often inelegant tricks to solve diverse but always specific problems of survival and reproduction. We also know that nervous systems use similarly effective but inelegant tricks to feed, fight, flee, mate, and solve other diverse but similarly specific problems (<xref ref-type="bibr" rid="bib136">Ramachandran, 1985</xref>; <xref ref-type="bibr" rid="bib103">Marcus, 2009</xref>). We also know that the specific details of these tricks are similar in distantly related species (<xref ref-type="bibr" rid="bib121">Nieuwenhuys and Puelles, 2016</xref>; <xref ref-type="bibr" rid="bib172">Tosches, 2017</xref>; <xref ref-type="bibr" rid="bib34">Cisek, 2019</xref>). These similarities include homologies of specific circuits (<xref ref-type="bibr" rid="bib146">Sanes and Zipursky, 2010</xref>; <xref ref-type="bibr" rid="bib16">Borst and Helmstaedter, 2015</xref>; <xref ref-type="bibr" rid="bib35">Clark and Demb, 2016</xref>), systems (<xref ref-type="bibr" rid="bib167">Strausfeld and Hirth, 2013</xref>; <xref ref-type="bibr" rid="bib51">Fiore et al., 2015</xref>; <xref ref-type="bibr" rid="bib140">Riebli and Reichert, 2016</xref>) and developmental processes (<xref ref-type="bibr" rid="bib29">Carroll, 1995</xref>; <xref ref-type="bibr" rid="bib4">Arthur, 2010</xref>; <xref ref-type="bibr" rid="bib72">Held, 2017</xref>) in flies and mice, organisms that diverged about 600 million years ago (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The importance, specificity, and conservation of these features make them natural candidates for inclusion in benchmark models (Appendix 5).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Similarities of development and structure in mice and flies.</title><p>(<bold>a</bold>) Conserved rostrocaudal (nose-to-tail, left panels) and dorsoventral (back-to-belly, right panels) patterns of neural gene expression in developing flies and mice. Matching colors denote homologous genes. Gene names not shown. (<bold>b</bold>) Conserved gross organization of regional modules in adult flies and mice. Note that, relative to flies, the organization of (<bold>a</bold>) expressed neural genes and (<bold>b</bold>) visual, auditory, and olfactory modules in mice is inverted dorsoventrally. This is a known developmental quirk (<xref ref-type="bibr" rid="bib72">Held, 2017</xref>). (<bold>c</bold>) Similarities in the motion-detection circuits of flies and mice. R1–R6 photoreceptors in flies, and cone photoreceptors in mice, convert light into neural activity. Each photoreceptor has a distinct receptive field that responds to spatially distinct light stimuli. Parallel ON and OFF pathways in both animals extract motion signals from this activity. These pathways start with L1/L2 lamina monopolar cells in flies, and directly with photoreceptors in mice. Cells in the ON pathway depolarize, and cells in the OFF pathway repolarize, in response to increased visual input. Moreover, distinct cells within each pathway may respond to input on fast or slow timescales. T4/T5 interneurons in flies, and starburst amacrine interneurons (SACs) in mice, detect motion in each pathway by integrating fast and slow responses associated with specific receptive fields. Finally, lobular plate tangential cells (LPTCs) in flies, and ON-OFF direction-selective ganglion cells (DSGCs) in mice, recombine motion signals from the ON and OFF pathways. +/− denote excitation/inhibition, and yellow arrows denote four directions of motion. (<bold>d</bold>) Proposed homologies between the action-selection circuits of flies and mice. The alignment emphasizes the shared function of individual areas and of excitatory or modulatory (blue), inhibitory (red), dopaminergic (black), and descending (green) projections. In flies, action selection centers on the central complex. The central complex includes the protocerebral bridge (PB), the fan-shaped body (FB), and the ellipsoid body (EB). In mice, action selection centers on the basal ganglia. The basal ganglia include the striatum (ST) and the external and internal globus pallidi (GPe and GPi). The central complex receives direct projections from sensory areas, the intermediate and inferior lateral protocerebra (IMP and ILP). It also receives direct projections from an association area, the superior medial protocerebrum (SMP). Finally, it receives indirect projections, via the SMP, from a learning area, the mushroom body (MB). Correspondingly, the basal ganglia receive direct projections from sensory and association areas in the cortex and indirect projections, via association cortex, from learning areas (the amygdala and hippocampus, Am and Hp). The central complex projects to the ventral cord via the lateral accessory lobes (LAL) and the motor ventrolateral protocerebra (VLP). Similarly, the basal ganglia project to the spinal cord via the thalamus and the motor cortex. Finally, in both cases, dopamine plays an important modulatory role. It acts via PPL1 and PPM3 neurons in flies, and via the substantia nigra pars compacta (SNc) in mice. Note also that the gall (not shown) may be a fly homolog of the mouse suprathalamic nucleus (STN, <xref ref-type="bibr" rid="bib51">Fiore et al., 2015</xref>). Panel (a) is reproduced from Figure 1 of <xref ref-type="bibr" rid="bib5">Bailly et al., 2013</xref>. Panel (b) is adapted from Figure 1b of <xref ref-type="bibr" rid="bib144">Rubinov, 2016</xref>. Panel (c) is reproduced from Figure 5 of <xref ref-type="bibr" rid="bib16">Borst and Helmstaedter, 2015</xref>. Panel (d) is adapted from Figure 2 of <xref ref-type="bibr" rid="bib167">Strausfeld and Hirth, 2013</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-fig6-v2.tif"/><permissions><copyright-statement>© 2015, Springer Nature</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Springer Nature</copyright-holder><license><license-p>Panel (c) is reproduced from Figure 5 of <xref ref-type="bibr" rid="bib16">Borst and Helmstaedter, 2015</xref>, with permission from Springer Nature. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions><permissions><copyright-statement>© 2013, Science</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Science</copyright-holder><license><license-p>Panel (d) is reproduced from Figure 2 of <xref ref-type="bibr" rid="bib167">Strausfeld and Hirth, 2013</xref>. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig></sec><sec id="s1-5-2"><title>Defining models</title><p><italic>Models of the phenomena.</italic> Benchmark models of relatively simple or circumscribed phenomena can sometimes take the form of parametric equations. In neuroscience, perhaps the best-known example of such a benchmark model is the Hodgkin-Huxley model of the action potential (<xref ref-type="bibr" rid="bib76">Hodgkin and Huxley, 1952</xref>). By contrast, benchmark models of complex or expansive phenomena, such as whole-brain networks, are often hard to express in parametric form. These models can instead be defined pragmatically on the basis of benchmark features in empirical data (<xref ref-type="table" rid="table4">Table 4</xref>). Such data-driven definitions can resemble dimensionality reduction (<xref ref-type="bibr" rid="bib42">Cunningham and Ghahramani, 2015</xref>) and force studies to formalize the often-vague theoretical concepts as quantifiable model features.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Example features and statistics.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model feature</th><th align="left" valign="bottom">Example statistic</th></tr></thead><tbody><tr><td align="left" valign="bottom">Sensory-motor interactions</td><td align="left" valign="bottom">Connectivity and activity statistics of functional circuits.</td></tr><tr><td align="left" valign="bottom">Excitation/inhibition balance</td><td align="left" valign="bottom">1/<italic>f</italic> power-spectral slopes (<xref ref-type="bibr" rid="bib57">Gao et al., 2017</xref>).</td></tr><tr><td align="left" valign="bottom">Node connectivity</td><td align="left" valign="bottom">Degree-distribution statistics (<xref ref-type="bibr" rid="bib36">Clauset et al., 2009</xref>).</td></tr><tr><td align="left" valign="bottom">Network clusters</td><td align="left" valign="bottom">Within-module densities (<xref ref-type="bibr" rid="bib53">Fortunato, 2010</xref>).</td></tr><tr><td align="left" valign="bottom">Tuning representations</td><td align="left" valign="bottom">Tuning-curve statistics (<xref ref-type="bibr" rid="bib91">Kriegeskorte and Wei, 2021</xref>).</td></tr><tr><td align="left" valign="bottom">Manifold representations</td><td align="left" valign="bottom">Persistent-homology barcodes (<xref ref-type="bibr" rid="bib60">Ghrist, 2008</xref>).</td></tr><tr><td align="left" valign="bottom">Oscillations</td><td align="left" valign="bottom">Frequency-specific amplitudes and phases (<xref ref-type="bibr" rid="bib46">Donoghue et al., 2020</xref>).</td></tr><tr><td align="left" valign="bottom">Criticality</td><td align="left" valign="bottom">Avalanche exponents (<xref ref-type="bibr" rid="bib150">Sethna et al., 2001</xref>).</td></tr><tr><td align="left" valign="bottom">Small worlds</td><td align="left" valign="bottom">Small-world statistics (<xref ref-type="bibr" rid="bib9">Bassett and Bullmore, 2017</xref>).</td></tr><tr><td align="left" valign="bottom">Cores/clubs</td><td align="left" valign="bottom">Within-core densities (<xref ref-type="bibr" rid="bib41">Csermely et al., 2013</xref>).</td></tr><tr><td align="left" valign="bottom">Network controllability</td><td align="left" valign="bottom">Network-controllability statistics (<xref ref-type="bibr" rid="bib128">Pasqualetti et al., 2014</xref>).</td></tr></tbody></table></table-wrap><p>Many applied or clinical fields seek to explain the nature of altered brain development, structure, or function. Formulation of benchmark models is equally important in these fields. Benchmark models of altered phenomena should correspondingly be defined in terms of altered, rather than absolute, values of empirical features. For example, benchmark models of neuropsychiatric disorders could be defined in terms of altered development and structure that coherently delineate specific patient populations (<xref ref-type="bibr" rid="bib79">Insel and Cuthbert, 2015</xref>; <xref ref-type="bibr" rid="bib71">Hampel et al., 2023</xref>).</p><p><italic>Models of the data.</italic> In practice, benchmark models should also include features that represent data limitations or biases. For example, limitations of neural-activity data may include acquisition artifacts, physiological confounders and indirectness of neural-activity markers (<xref ref-type="bibr" rid="bib75">Hillman, 2014</xref>; <xref ref-type="bibr" rid="bib184">Wei et al., 2020</xref>). The inclusion of these data features in benchmark models can help to mitigate their confounding effects. The interactions of these features with other aspects of the signal, however, makes it ultimately impossible to fully eliminate these effects (Appendix 3).</p></sec><sec id="s1-5-3"><title>Sampling data</title><p>Tests against benchmark models rest on the ability to sample data from benchmark-model distributions. This sampling should ideally be unbiased: the data samples should match the model statistics but be maximally random otherwise. Unbiased sampling allows us to make valid statistical inferences. For example, the opinions of an unbiased sample of people allow us to make valid statistical inferences about the opinions of the whole population.</p><p>In practice, fully unbiased sampling is often intractable, but approximately unbiased sampling is often possible for many interesting benchmark-model distributions. For clarity, this section distinguishes between specific and general methods for doing such sampling.</p><p>Specific sampling methods typically first express benchmark-model distributions as solution spaces of data that satisfy benchmark statistics (<xref ref-type="bibr" rid="bib147">Schellenberger and Palsson, 2009</xref>). They then randomly draw data samples from these solution spaces. Important examples of these methods can sample data with spatial and temporal correlations (<xref ref-type="bibr" rid="bib135">Prichard and Theiler, 1994</xref>; <xref ref-type="bibr" rid="bib141">Roberts et al., 2016</xref>; <xref ref-type="bibr" rid="bib118">Nanda et al., 2023</xref>). The main strength of these methods is in the ability to sample data in fast and unbiased ways. Their main weakness is the inability to sample data with general or arbitrary features and their consequent restriction to a relatively narrow set of benchmark models.</p><p>General sampling methods have a complementary set of strengths and weaknesses. The main strength of these methods is the ability to sample data with general or arbitrary benchmark features. Their main weakness is the slow or biased nature of the sampling.</p><p>General sampling methods comprise two broad types. The first type of general sampling typically begins with an initial data sample that typically matches the dimensionality and other basic properties of empirical data. It then iteratively randomizes this initial sample in a way that satisfies the benchmark statistics of empirical data, usually by minimizing an error function (<xref ref-type="bibr" rid="bib148">Schreiber, 1998</xref>). Unbiased sampling requires that this randomization could, in principle, reach all possible samples and that randomization at each iteration could, in principle, be reversible (<xref ref-type="bibr" rid="bib120">Newman and Barkema, 1999</xref>). These conditions imply that this randomization must be “non-greedy” or not necessarily lower the error at each iteration.</p><p>The second type of general sampling typically uses statistical inference methods, such as the principle of maximum entropy. It first defines and fits parametric data distributions and then randomly draws data samples from these distributions (<xref ref-type="bibr" rid="bib164">Squartini and Garlaschelli, 2011</xref>). In contrast to other sampling methods, this approach preserves the benchmark statistics in the population average but not necessarily in each individual data sample. Fully unbiased sampling with this approach is often intractable for large datasets. Assumptions of independence can make this sampling tractable for many benchmark models but likely at the expense of considerable bias (<xref ref-type="bibr" rid="bib33">Cimini et al., 2019</xref>).</p></sec><sec id="s1-5-4"><title>Making progress</title><p>The importance of tests against benchmark models reflects the broader importance of scientific progress. In modern science, the notion of progress is intertwined with the concept of impact. Formally, impact often denotes the number of papers and citations. Implicitly, impact signals real progress. Circular analyses of knowledge enable speculative and redundant results that can lead to many intriguing, replicable, and highly cited papers. Such papers satisfy the formal meaning of impact even as they fail to make real progress (<xref ref-type="bibr" rid="bib98">Lawrence, 2007</xref>; <xref ref-type="bibr" rid="bib3">Alberts, 2013</xref>).</p><p>Tests against benchmark models can help to align the formal and intuitive definitions of impact. A narrow perspective on genuine impact could equate impact with direct revisions of benchmark models. A broader and more realistic perspective can also emphasize advances that indirectly facilitate revisions of benchmark models (<xref ref-type="table" rid="table5">Table 5</xref>).</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Examples of impactful advances.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Advance</th><th align="left" valign="bottom">Nature of impact</th></tr></thead><tbody><tr><td align="left" valign="bottom">Discoveries</td><td align="left" valign="bottom">Revisions of benchmark models (typically rare).</td></tr><tr><td align="left" valign="bottom">Null results</td><td align="left" valign="bottom">Rejections of previously promising speculative models.</td></tr><tr><td align="left" valign="bottom">Exploratory advances</td><td align="left" valign="bottom">Formulations of newly promising speculative models.</td></tr><tr><td align="left" valign="bottom">Conceptual advances</td><td align="left" valign="bottom">Discoveries of explanatory gaps that enable exploratory advances.</td></tr><tr><td align="left" valign="bottom">Methodological advances</td><td align="left" valign="bottom">Improvements in data or analysis that support all the other advances.</td></tr></tbody></table></table-wrap><p>Separately, the adoption of benchmarking best practices from predictive modeling fields, including machine learning (<xref ref-type="bibr" rid="bib183">Weber et al., 2019</xref>; <xref ref-type="bibr" rid="bib102">Mangul et al., 2019</xref>; <xref ref-type="bibr" rid="bib114">Mitchell et al., 2019</xref>; <xref ref-type="bibr" rid="bib86">Kapoor and Narayanan, 2023</xref>), can help facilitate progress in explanatory modeling. The following list describes three important examples of these practices:</p><list list-type="order"><list-item><p>High-quality and publicly accessible data can advance discovery in several ways. First, such data can serve as a reference for formulating consensus benchmark models. Second, such data can help reveal explanatory gaps in existing benchmark models. Third, such data can help formulate new and promising speculative models.</p></list-item><list-item><p>Standardized summaries of models and tests can help replace imprecise narratives with quantitative summaries of individual results. Machine-readable versions of these summaries can help facilitate automated integration of such results across studies.</p></list-item><list-item><p>A centralized integration of results can help to formalize discovery through continuous revisions of benchmark models. It can also help to collate and standardize null results and, in this way, eliminate rejected speculative models from future tests.</p></list-item></list><p>Together, this change in focus can help motivate systems neuroscientists to carefully formulate new models and to rigorously test these models against benchmark models. Such testing can lead to a welcome decrease in publications of speculative and redundant results. And collectively, the resulting alignment of formal and intuitive definitions of impact can give the field a better chance to make real progress.</p></sec></sec><sec id="s1-6"><title>Concluding recommendations</title><p>Circular analyses of noise, and the resulting problem of irreplicable results, form a known impediment to progress in systems neuroscience. This work described that circular analyses of knowledge, and the resulting problem of redundant results, form a less-well known but similarly serious impediment. This concluding section summarizes my overall suggestions for resolving this problem. Appendix 6 discusses objections to some of these suggestions.</p><p><italic>Raise awareness.</italic> Few scientists and funding bodies formally discuss the problem of redundant results. The lack of this discussion contrasts with extensive parallel discussions of the problem of irreplicable results. The start of this discussion, including in research and policy papers, will be an important first step towards the development of principled solutions.</p><p><italic>Reevaluate discoveries.</italic> Systematic community efforts should establish the genuine novelty of discoveries in systems and network neuroscience. These efforts may benefit from the experience of similar efforts to establish the replicability of discoveries in psychological and social science (<xref ref-type="bibr" rid="bib126">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="bib27">Camerer et al., 2018</xref>). These efforts face specific challenges, however, including establishing consensus on definitions of reference datasets, benchmark models, and test statistics.</p><p><italic>Delimit speculation.</italic> Speculation often helps to formulate promising new models. At the same time, misuse of speculation can lead to the neglect of rigorous tests and to the inappropriate acceptance of speculative models. Studies should minimize this misuse by delimiting all suggestively speculative terms (deepities), structure (spandrels), and narratives (just-so stories). Ideally, these delimitations should be prominently made in Introduction sections.</p><p><italic>Define benchmarks.</italic> Many parts of systems and network neuroscience lack benchmark models. The field should formulate such models to integrate all important existing knowledge and rigorously test proposed discovery. Challenges in the formulation of benchmark models include collation and curation of existing knowledge, consensus definition of model features and test statistics, and development of distinct models for individual phenomena.</p><p><italic>Advance sampling.</italic> A dearth of powerful sampling methods limits the adoption of rigorous model tests. The field should develop unbiased and scalable methods for sampling data from diverse benchmark-model distributions. Challenges in the development of these methods include competing demands of unbiased sampling and scalability (for general methods) and extensions to diverse benchmark-model distributions (for specific methods).</p><p><italic>Reclaim impact.</italic> The divergence of formal and intuitive meanings of impact can hinder scientific progress. A multifaceted assessment of direct or indirect impact that centers on revisions of benchmark models, and that discourages redundant explanations, can help to reduce this divergence. Research and funding bodies can emphasize this multifaceted assessment and downplay the use of publication metrics as indicators of progress.</p></sec></sec></body><back><sec sec-type="additional-information" id="s2"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s3"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79559-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><ack id="ack"><title>Acknowledgements</title><p>I thank Maarten Zwart, Mark Hanin, Linda Douw, Mona Zimmermann, Lucas Breedt, Eduarda Centeno, Vera Belgers, Jantine Röttgering, Maxine Gorter, Casey Schneider-Mizell, Alex Goulas, Dalton Nelson, as well as journal editors and anonymous reviewers for detailed constructive critique of previous versions of the manuscript; Roman Frigg, Galit Shmueli, Moti Mizrahi, Robert Cousins, John Kaas, Alex Maier, Paul Cisek, and Anton Schulmann for helpful discussions on scientific models and modeling; Lindsay Bremner for thorough and thoughtful scientific editing; Gösta Knochenhauer, Moritz Helmstaedter, and Axel Borst for sharing high-resolution images.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achard</surname><given-names>S</given-names></name><name><surname>Salvador</surname><given-names>R</given-names></name><name><surname>Whitcher</surname><given-names>B</given-names></name><name><surname>Suckling</surname><given-names>J</given-names></name><name><surname>Bullmore</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A resilient, low-frequency, small-world human brain functional network with highly connected association cortical hubs</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3874-05.2006</pub-id><pub-id pub-id-type="pmid">16399673</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aho</surname><given-names>K</given-names></name><name><surname>Derryberry</surname><given-names>D</given-names></name><name><surname>Peterson</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Model selection for ecologists: the worldviews of AIC and BIC</article-title><source>Ecology</source><volume>95</volume><fpage>631</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1890/13-1452.1</pub-id><pub-id pub-id-type="pmid">24804445</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alberts</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Impact factor distortions</article-title><source>Science</source><volume>340</volume><elocation-id>787</elocation-id><pub-id pub-id-type="doi">10.1126/science.1240319</pub-id><pub-id pub-id-type="pmid">23744781</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Arthur</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Evolution: A Developmental Approach</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bailly</surname><given-names>X</given-names></name><name><surname>Reichert</surname><given-names>H</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The urbilaterian brain revisited: novel insights into old questions from new flatworm clades</article-title><source>Development Genes and Evolution</source><volume>223</volume><fpage>149</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1007/s00427-012-0423-7</pub-id><pub-id pub-id-type="pmid">23143292</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>Simplicity</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name></person-group><source>Stanford Encyclopedia of Philosophy</source><publisher-name>Metaphysics Research Lab, Stanford University</publisher-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>B</given-names></name><name><surname>Lansdell</surname><given-names>B</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Three aspects of representation in neuroscience</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>942</fpage><lpage>958</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.08.014</pub-id><pub-id pub-id-type="pmid">36175303</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barabási</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Network Science</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Small-world brain networks revisited</article-title><source>The Neuroscientist</source><volume>23</volume><fpage>499</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1177/1073858416667720</pub-id><pub-id pub-id-type="pmid">27655008</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Network neuroscience</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>353</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1038/nn.4502</pub-id><pub-id pub-id-type="pmid">28230844</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bateson</surname><given-names>P</given-names></name><name><surname>Laland</surname><given-names>KN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tinbergen’s four questions: an appreciation and an update</article-title><source>Trends in Ecology &amp; Evolution</source><volume>28</volume><fpage>712</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2013.09.013</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayne</surname><given-names>T</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Clayton</surname><given-names>N</given-names></name><name><surname>Heyes</surname><given-names>C</given-names></name><name><surname>Mather</surname><given-names>J</given-names></name><name><surname>Ölveczky</surname><given-names>B</given-names></name><name><surname>Shadlen</surname><given-names>M</given-names></name><name><surname>Suddendorf</surname><given-names>T</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>What is cognition?</article-title><source>Current Biology</source><volume>29</volume><fpage>R608</fpage><lpage>R615</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.05.044</pub-id><pub-id pub-id-type="pmid">31287972</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedau</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Weak emergence</article-title><source>Noûs</source><volume>31</volume><fpage>375</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1111/0029-4624.31.s11.17</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bennett</surname><given-names>MR</given-names></name><name><surname>Hacker</surname><given-names>PMS</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>The mereological fallacy in neuroscience</chapter-title><source>Philosophical Foundations of Neuroscience</source><publisher-name>Wiley</publisher-name><fpage>79</fpage><lpage>93</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borello</surname><given-names>U</given-names></name><name><surname>Pierani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Patterning the cerebral cortex: traveling with morphogens</article-title><source>Current Opinion in Genetics &amp; Development</source><volume>20</volume><fpage>408</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/j.gde.2010.05.003</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Common circuit design in fly and mammalian motion vision</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1067</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1038/nn.4050</pub-id><pub-id pub-id-type="pmid">26120965</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourget</surname><given-names>D</given-names></name><name><surname>Chalmers</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>What do philosophers believe?</article-title><source>Philosophical Studies</source><volume>170</volume><fpage>465</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1007/s11098-013-0259-7</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowers</surname><given-names>JS</given-names></name><name><surname>Davis</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bayesian just-so stories in psychology and neuroscience</article-title><source>Psychological Bulletin</source><volume>138</volume><fpage>389</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1037/a0026450</pub-id><pub-id pub-id-type="pmid">22545686</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowers</surname><given-names>JS</given-names></name><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>Dujmović</surname><given-names>M</given-names></name><name><surname>Montero</surname><given-names>ML</given-names></name><name><surname>Tsvetkov</surname><given-names>C</given-names></name><name><surname>Biscione</surname><given-names>V</given-names></name><name><surname>Puebla</surname><given-names>G</given-names></name><name><surname>Adolfi</surname><given-names>F</given-names></name><name><surname>Hummel</surname><given-names>JE</given-names></name><name><surname>Heaton</surname><given-names>RF</given-names></name><name><surname>Evans</surname><given-names>BD</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name><name><surname>Blything</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep problems with neural network models of human vision</article-title><source>Behavioral and Brain Sciences</source><volume>2022</volume><fpage>1</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1017/S0140525X22002813</pub-id><pub-id pub-id-type="pmid">36453586</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Is coding a relevant metaphor for the brain?</article-title><source>Behavioral and Brain Sciences</source><volume>42</volume><elocation-id>e21</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X19000049</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>SL</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Controllability and observability</chapter-title><source>Data-Driven Science and Engineering</source><publisher-name>Cambridge University Press</publisher-name><fpage>287</fpage><lpage>291</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Krienen</surname><given-names>FM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The evolution of distributed association networks in the human brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>648</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.09.017</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullmore</surname><given-names>E</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The economy of brain network organization</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>336</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1038/nrn3214</pub-id><pub-id pub-id-type="pmid">22498897</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burwick</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The binding problem</article-title><source>Wiley Interdisciplinary Reviews: Cognitive Science</source><volume>5</volume><fpage>305</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1002/wcs.1279</pub-id><pub-id pub-id-type="pmid">26308565</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Draguhn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal oscillations in cortical networks</article-title><source>Science</source><volume>304</volume><fpage>1926</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1126/science.1099745</pub-id><pub-id pub-id-type="pmid">15218136</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Mizuseki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>264</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1038/nrn3687</pub-id><pub-id pub-id-type="pmid">24569488</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Ho</surname><given-names>T-H</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Nave</surname><given-names>G</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Pfeiffer</surname><given-names>T</given-names></name><name><surname>Altmejd</surname><given-names>A</given-names></name><name><surname>Buttrick</surname><given-names>N</given-names></name><name><surname>Chan</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Forsell</surname><given-names>E</given-names></name><name><surname>Gampa</surname><given-names>A</given-names></name><name><surname>Heikensten</surname><given-names>E</given-names></name><name><surname>Hummer</surname><given-names>L</given-names></name><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Isaksson</surname><given-names>S</given-names></name><name><surname>Manfredi</surname><given-names>D</given-names></name><name><surname>Rose</surname><given-names>J</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>637</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0399-z</pub-id><pub-id pub-id-type="pmid">31346273</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>New labels for old ideas: Predictive processing and the interpretation of neural signals</article-title><source>Review of Philosophy and Psychology</source><volume>11</volume><fpage>517</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1007/s13164-020-00481-x</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carroll</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Homeotic genes and the evolution of arthropods and chordates</article-title><source>Nature</source><volume>376</volume><fpage>479</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1038/376479a0</pub-id><pub-id pub-id-type="pmid">7637779</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chakravartty</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Scientific realism</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name></person-group><source>Stanford Encyclopedia of Philosophy</source><publisher-name>Metaphysics Research Lab, Stanford University</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>PS</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Computational overview</chapter-title><source>The Computational Brain</source><publisher-name>MIT Press</publisher-name><fpage>61</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.7551/mitpress/11207.003.0006</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep neural networks as scientific models</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>305</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.009</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cimini</surname><given-names>G</given-names></name><name><surname>Squartini</surname><given-names>T</given-names></name><name><surname>Saracco</surname><given-names>F</given-names></name><name><surname>Garlaschelli</surname><given-names>D</given-names></name><name><surname>Gabrielli</surname><given-names>A</given-names></name><name><surname>Caldarelli</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The statistical physics of real-world networks</article-title><source>Nature Reviews Physics</source><volume>1</volume><fpage>58</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/s42254-018-0002-6</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Resynthesizing behavior through phylogenetic refinement</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>81</volume><fpage>2265</fpage><lpage>2287</lpage><pub-id pub-id-type="doi">10.3758/s13414-019-01760-1</pub-id><pub-id pub-id-type="pmid">31161495</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Demb</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Parallel computations in insect and mammalian visual motion processing</article-title><source>Current Biology</source><volume>26</volume><fpage>R1062</fpage><lpage>R1072</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.003</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clauset</surname><given-names>A</given-names></name><name><surname>Shalizi</surname><given-names>CR</given-names></name><name><surname>Newman</surname><given-names>MEJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Power-law distributions in empirical data</article-title><source>SIAM Review</source><volume>51</volume><fpage>661</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1137/070710111</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cobb</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>The Idea of the Brain: The Past and Future of Neuroscience</source><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colizza</surname><given-names>V</given-names></name><name><surname>Flammini</surname><given-names>A</given-names></name><name><surname>Serrano</surname><given-names>MA</given-names></name><name><surname>Vespignani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Detecting rich-club ordering in complex networks</article-title><source>Nature Physics</source><volume>2</volume><fpage>110</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1038/nphys209</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cousins</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Jeffreys–Lindley paradox and discovery criteria in high energy physics</article-title><source>Synthese</source><volume>194</volume><fpage>395</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1007/s11229-014-0525-z</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Craik</surname><given-names>KJW</given-names></name></person-group><year iso-8601-date="1943">1943</year><chapter-title>Hypothesis on the nature of thought</chapter-title><source>The Nature of Explanation</source><publisher-name>Cambridge University Press</publisher-name><fpage>50</fpage><lpage>61</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Csermely</surname><given-names>P</given-names></name><name><surname>London</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>LY</given-names></name><name><surname>Uzzi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Structure and dynamics of core/periphery networks</article-title><source>Journal of Complex Networks</source><volume>1</volume><fpage>93</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1093/comnet/cnt016</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Linear dimensionality reduction: Survey, insights, and generalizations</article-title><source>Journal of Machine Learning Research</source><volume>16</volume><fpage>2859</fpage><lpage>2900</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Danks</surname><given-names>D</given-names></name><name><surname>London</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Algorithmic Bias in Autonomous Systems</article-title><conf-name>Twenty-Sixth International Joint Conference on Artificial Intelligence</conf-name><fpage>4691</fpage><lpage>4697</lpage><pub-id pub-id-type="doi">10.24963/ijcai.2017/654</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dennett</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>What is a Deepity?</chapter-title><source>Intuition Pumps and Other Tools for Thinking</source><publisher-loc>New York, United States</publisher-loc><publisher-name>W. W. Norton</publisher-name><fpage>56</fpage><lpage>57</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destexhe</surname><given-names>A</given-names></name><name><surname>Touboul</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Is there sufficient evidence for criticality in cortical systems?</article-title><source>eNeuro</source><volume>8</volume><elocation-id>ENEURO.0551-20.2021</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0551-20.2021</pub-id><pub-id pub-id-type="pmid">33811087</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoghue</surname><given-names>T</given-names></name><name><surname>Haller</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>EJ</given-names></name><name><surname>Varma</surname><given-names>P</given-names></name><name><surname>Sebastian</surname><given-names>P</given-names></name><name><surname>Gao</surname><given-names>R</given-names></name><name><surname>Noto</surname><given-names>T</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Shestyuk</surname><given-names>A</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parameterizing neural power spectra into periodic and aperiodic components</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1655</fpage><lpage>1665</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00744-x</pub-id><pub-id pub-id-type="pmid">33230329</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoghue</surname><given-names>T</given-names></name><name><surname>Schaworonkow</surname><given-names>N</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Methodological considerations for studying neural oscillations</article-title><source>The European Journal of Neuroscience</source><volume>55</volume><fpage>3502</fpage><lpage>3527</lpage><pub-id pub-id-type="doi">10.1111/ejn.15361</pub-id><pub-id pub-id-type="pmid">34268825</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>LN</given-names></name><name><surname>Duncker</surname><given-names>L</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Representational drift: Emerging theories for continual learning and experimental future directions</article-title><source>Current Opinion in Neurobiology</source><volume>76</volume><elocation-id>102609</elocation-id><pub-id pub-id-type="doi">10.1016/j.conb.2022.102609</pub-id><pub-id pub-id-type="pmid">35939861</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Structure in neural population recordings: an expected byproduct of simpler phenomena?</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1310</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1038/nn.4617</pub-id><pub-id pub-id-type="pmid">28783140</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname><given-names>D</given-names></name><name><surname>Costas</surname><given-names>R</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Meta-assessment of bias in science</article-title><source>PNAS</source><volume>114</volume><fpage>3714</fpage><lpage>3719</lpage><pub-id pub-id-type="doi">10.1073/pnas.1618569114</pub-id><pub-id pub-id-type="pmid">28320937</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiore</surname><given-names>VG</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Strausfeld</surname><given-names>NJ</given-names></name><name><surname>Hirth</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Evolutionarily conserved mechanisms for the selection and maintenance of behavioural activity</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>370</volume><elocation-id>20150053</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0053</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontenele</surname><given-names>AJ</given-names></name><name><surname>de Vasconcelos</surname><given-names>NAP</given-names></name><name><surname>Feliciano</surname><given-names>T</given-names></name><name><surname>Aguiar</surname><given-names>LAA</given-names></name><name><surname>Soares-Cunha</surname><given-names>C</given-names></name><name><surname>Coimbra</surname><given-names>B</given-names></name><name><surname>Dalla Porta</surname><given-names>L</given-names></name><name><surname>Ribeiro</surname><given-names>S</given-names></name><name><surname>Rodrigues</surname><given-names>AJ</given-names></name><name><surname>Sousa</surname><given-names>N</given-names></name><name><surname>Carelli</surname><given-names>PV</given-names></name><name><surname>Copelli</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Criticality between Cortical States</article-title><source>Physical Review Letters</source><volume>122</volume><elocation-id>208101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.122.208101</pub-id><pub-id pub-id-type="pmid">31172737</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fortunato</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Community detection in graphs</article-title><source>Physics Reports</source><volume>486</volume><fpage>75</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.physrep.2009.11.002</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>WJ</given-names></name><name><surname>Skarda</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="1990">1990</year><chapter-title>Representations: who needs them?</chapter-title><person-group person-group-type="editor"><name><surname>Lynch</surname><given-names>G</given-names></name><name><surname>McGaugh</surname><given-names>JL</given-names></name><name><surname>Weinberger</surname><given-names>NM</given-names></name></person-group><source>Brain Organization and Memory</source><publisher-name>Oxford University Press</publisher-name><fpage>375</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1093/oso/9780195077124.003.0022</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frégnac</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Big data and the industrialization of neuroscience: A safe roadmap for understanding the brain?</article-title><source>Science</source><volume>358</volume><fpage>470</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1126/science.aan8866</pub-id><pub-id pub-id-type="pmid">29074766</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frigg</surname><given-names>R</given-names></name><name><surname>Hartmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Models in science</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name></person-group><source>Stanford Encyclopedia of Philosophy</source><publisher-name>Metaphysics Research Lab, Stanford University</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>R</given-names></name><name><surname>Peterson</surname><given-names>EJ</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inferring synaptic excitation/inhibition balance from field potentials</article-title><source>NeuroImage</source><volume>158</volume><fpage>70</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.078</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><name><surname>Jacobsen</surname><given-names>JH</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Zemel</surname><given-names>R</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Shortcut learning in deep neural networks</article-title><source>Nature Machine Intelligence</source><volume>2</volume><fpage>665</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00257-z</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hill</surname><given-names>J</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Statistical inference</chapter-title><source>Regression and Other Stories</source><publisher-name>Cambridge University Press</publisher-name><fpage>49</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1017/9781139161879</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghrist</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Barcodes: The persistent topology of data</article-title><source>Bulletin of the American Mathematical Society</source><volume>45</volume><fpage>61</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1090/S0273-0979-07-01191-3</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Golan</surname><given-names>T</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Schütt</surname><given-names>HH</given-names></name><name><surname>Peters</surname><given-names>B</given-names></name><name><surname>Sommers</surname><given-names>RP</given-names></name><name><surname>Seeliger</surname><given-names>K</given-names></name><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Linton</surname><given-names>P</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name><name><surname>Kording</surname><given-names>K</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Deep Neural Networks Are Not a Single Hypothesis but a Language for Expressing Computational Hypotheses</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/tr7gx</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Visuomotor modules in the vertebrate brain</article-title><source>Canadian Journal of Physiology and Pharmacology</source><volume>74</volume><fpage>390</fpage><lpage>400</lpage><pub-id pub-id-type="pmid">8828886</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gould</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Sociobiology: the art of storytelling</article-title><source>New Scientist</source><volume>80</volume><fpage>530</fpage><lpage>533</lpage><pub-id pub-id-type="pmid">11664947</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gould</surname><given-names>SJ</given-names></name><name><surname>Lewontin</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>The spandrels of San Marco and the Panglossian paradigm: a critique of the adaptationist programme</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>205</volume><fpage>581</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1098/rspb.1979.0086</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>CM</given-names></name><name><surname>König</surname><given-names>P</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties</article-title><source>Nature</source><volume>338</volume><fpage>334</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1038/338334a0</pub-id><pub-id pub-id-type="pmid">2922061</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffa</surname><given-names>A</given-names></name><name><surname>van den Heuvel</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rich-club neurocircuitry: function, evolution, and vulnerability</article-title><source>Dialogues in Clinical Neuroscience</source><volume>20</volume><fpage>121</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.31887/DCNS.2018.20.2/agriffa</pub-id><pub-id pub-id-type="pmid">30250389</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grove</surname><given-names>EA</given-names></name><name><surname>Monuki</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Morphogens, Patterning centers, and their mechanisms of action</chapter-title><person-group person-group-type="editor"><name><surname>Rubenstein</surname><given-names>J</given-names></name><name><surname>Rakic</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Kwan</surname><given-names>KY</given-names></name></person-group><source>Patterning and Cell Type Specification in the Developing CNS and PNS. 2nd Ed</source><publisher-name>Academic Press</publisher-name><fpage>3</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-814405-3.00001-1</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>S</given-names></name><name><surname>Pasqualetti</surname><given-names>F</given-names></name><name><surname>Cieslak</surname><given-names>M</given-names></name><name><surname>Telesford</surname><given-names>QK</given-names></name><name><surname>Yu</surname><given-names>AB</given-names></name><name><surname>Kahn</surname><given-names>AE</given-names></name><name><surname>Medaglia</surname><given-names>JD</given-names></name><name><surname>Vettel</surname><given-names>JM</given-names></name><name><surname>Miller</surname><given-names>MB</given-names></name><name><surname>Grafton</surname><given-names>ST</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Controllability of structural brain networks</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8414</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9414</pub-id><pub-id pub-id-type="pmid">26423222</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Elisseeff</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>An introduction to variable and feature selection</article-title><source>Journal of Machine Learning Research</source><volume>3</volume><fpage>1157</fpage><lpage>1182</lpage></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagmann</surname><given-names>P</given-names></name><name><surname>Cammoun</surname><given-names>L</given-names></name><name><surname>Gigandet</surname><given-names>X</given-names></name><name><surname>Meuli</surname><given-names>R</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Wedeen</surname><given-names>VJ</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mapping the structural core of human cerebral cortex</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e159</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060159</pub-id><pub-id pub-id-type="pmid">18597554</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hampel</surname><given-names>H</given-names></name><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Cummings</surname><given-names>J</given-names></name><name><surname>Toschi</surname><given-names>N</given-names></name><name><surname>Thompson</surname><given-names>PM</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Cho</surname><given-names>M</given-names></name><name><surname>Vergallo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The foundation and architecture of precision medicine in neurology and psychiatry</article-title><source>Trends in Neurosciences</source><volume>46</volume><fpage>176</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2022.12.004</pub-id><pub-id pub-id-type="pmid">36642626</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Held</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Deep Homology? Uncanny Similarities of Humans and Flies Uncovered by Evo-Devo</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/9781316550175</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermes</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Gamma oscillations in visual cortex: the stimulus matters</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>57</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.009</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermes</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Stimulus dependence of gamma oscillations in human visual cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>2951</fpage><lpage>2959</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu091</pub-id><pub-id pub-id-type="pmid">24855114</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillman</surname><given-names>EMC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Coupling mechanism and significance of the BOLD signal: A status report</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>161</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014111</pub-id><pub-id pub-id-type="pmid">25032494</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodgkin</surname><given-names>AL</given-names></name><name><surname>Huxley</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title><source>Journal of Physiology</source><volume>117</volume><fpage>500</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1952.sp004764</pub-id><pub-id pub-id-type="pmid">12991237</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Horgan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>The End Of Science: Facing The Limits Of Knowledge In The Twilight Of The Scientific Age</source><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title><source>Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Insel</surname><given-names>TR</given-names></name><name><surname>Cuthbert</surname><given-names>BN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Medicine: brain disorders? Precisely</article-title><source>Science</source><volume>348</volume><fpage>499</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1126/science.aab2358</pub-id><pub-id pub-id-type="pmid">25931539</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Why most published research findings are false</article-title><source>PLOS Medicine</source><volume>2</volume><elocation-id>e124</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id><pub-id pub-id-type="pmid">16060722</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isaacson</surname><given-names>JS</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How inhibition shapes cortical activity</article-title><source>Neuron</source><volume>72</volume><fpage>231</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.027</pub-id><pub-id pub-id-type="pmid">22017986</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="report"><person-group person-group-type="author"><collab>ISO</collab></person-group><year iso-8601-date="1994">1994</year><source>Accuracy (trueness and precision) of measurement methods and results</source><publisher-name>ISO</publisher-name></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonas</surname><given-names>E</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Could a neuroscientist understand a microprocessor?</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005268</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005268</pub-id><pub-id pub-id-type="pmid">28081141</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>The segregation of function in the nervous system: Why do sensory systems have so many subdivisions</chapter-title><person-group person-group-type="editor"><name><surname>Neff</surname><given-names>WD</given-names></name></person-group><source>Contributions to Sensory Physiology</source><publisher-name>Elsevier</publisher-name><fpage>201</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-151807-3.50012-4</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>M</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Nonoptimal component placement, but short processing paths, due to long-distance projections in neural systems</article-title><source>PLOS Computational Biology</source><volume>2</volume><elocation-id>e95</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020095</pub-id><pub-id pub-id-type="pmid">16848638</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kapoor</surname><given-names>S</given-names></name><name><surname>Narayanan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Leakage and the reproducibility crisis in machine-learning-based science</article-title><source>Patterns</source><volume>4</volume><elocation-id>100804</elocation-id><pub-id pub-id-type="doi">10.1016/j.patter.2023.100804</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kenny</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1971">1971</year><chapter-title>The homunculus fallacy</chapter-title><person-group person-group-type="editor"><name><surname>Grene</surname><given-names>MG</given-names></name><name><surname>Prigogine</surname><given-names>I</given-names></name></person-group><source>Interpretations of Life and Mind</source><publisher-name>Humanities Press</publisher-name><fpage>65</fpage><lpage>74</lpage></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: Correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Simmons</surname><given-names>WK</given-names></name><name><surname>Bellgowan</surname><given-names>PSF</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Circular analysis in systems neuroscience: the dangers of double dipping</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>535</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1038/nn.2303</pub-id><pub-id pub-id-type="pmid">19396166</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks: A new framework for modeling biological vision and brain information processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural tuning and representational geometry</article-title><source>Nature Reviews Neuroscience</source><volume>22</volume><fpage>703</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1038/s41583-021-00502-3</pub-id><pub-id pub-id-type="pmid">34522043</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krubitzer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The organization of neocortex in mammals: are species differences really so different?</article-title><source>Trends in Neurosciences</source><volume>18</volume><fpage>408</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(95)93938-t</pub-id><pub-id pub-id-type="pmid">7482807</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krubitzer</surname><given-names>LA</given-names></name><name><surname>Prescott</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The combinatorial creature: Cortical phenotypes within and across lifetimes</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>744</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.08.002</pub-id><pub-id pub-id-type="pmid">30274608</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1976">1976</year><chapter-title>Falsification and the methodology of scientific research programmes</chapter-title><person-group person-group-type="editor"><name><surname>Harding</surname><given-names>SG</given-names></name></person-group><source>Can Theories Be Refuted? Essays on the Duhem-Quine Thesis</source><publisher-name>Springer</publisher-name><fpage>205</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1007/978-94-010-1863-0</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laland</surname><given-names>KN</given-names></name><name><surname>Sterelny</surname><given-names>K</given-names></name><name><surname>Odling-Smee</surname><given-names>J</given-names></name><name><surname>Hoppitt</surname><given-names>W</given-names></name><name><surname>Uller</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cause and effect in biology revisited: is Mayr’s proximate-ultimate dichotomy still useful?</article-title><source>Science</source><volume>334</volume><fpage>1512</fpage><lpage>1516</lpage><pub-id pub-id-type="doi">10.1126/science.1210879</pub-id><pub-id pub-id-type="pmid">22174243</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langdon</surname><given-names>C</given-names></name><name><surname>Genkin</surname><given-names>M</given-names></name><name><surname>Engel</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A unifying perspective on neural manifolds and circuits for cognition</article-title><source>Nature Reviews Neuroscience</source><volume>24</volume><fpage>363</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00693-x</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The mismeasurement of science</article-title><source>Current Biology</source><volume>17</volume><fpage>R583</fpage><lpage>R585</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.06.014</pub-id><pub-id pub-id-type="pmid">17686424</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>X</given-names></name><name><surname>Vasilakos</surname><given-names>AV</given-names></name><name><surname>He</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Small-world human brain networks: Perspectives and challenges</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>77</volume><fpage>286</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.03.018</pub-id><pub-id pub-id-type="pmid">28389343</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>ZC</given-names></name><name><surname>Steinhardt</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Troubling trends in machine learning scholarship</article-title><source>Queue</source><volume>17</volume><fpage>45</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1145/3317287.3328534</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>Z</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Wessel</surname><given-names>R</given-names></name><name><surname>Hengen</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical circuit dynamics are homeostatically tuned to criticality in vivo</article-title><source>Neuron</source><volume>104</volume><fpage>655</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.08.031</pub-id><pub-id pub-id-type="pmid">31601510</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangul</surname><given-names>S</given-names></name><name><surname>Martin</surname><given-names>LS</given-names></name><name><surname>Hill</surname><given-names>BL</given-names></name><name><surname>Lam</surname><given-names>AKM</given-names></name><name><surname>Distler</surname><given-names>MG</given-names></name><name><surname>Zelikovsky</surname><given-names>A</given-names></name><name><surname>Eskin</surname><given-names>E</given-names></name><name><surname>Flint</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Systematic benchmarking of omics computational tools</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1393</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09406-4</pub-id><pub-id pub-id-type="pmid">30918265</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Kluge: The Haphazard Evolution of the Human Mind</source><publisher-name>Houghton Mifflin</publisher-name></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>NT</given-names></name><name><surname>Ercsey-Ravasz</surname><given-names>M</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Knoblauch</surname><given-names>K</given-names></name><name><surname>Toroczkai</surname><given-names>Z</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical high-density counterstream architectures</article-title><source>Science</source><volume>342</volume><elocation-id>1238406</elocation-id><pub-id pub-id-type="doi">10.1126/science.1238406</pub-id><pub-id pub-id-type="pmid">24179228</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262514620.001.0001</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mayo</surname><given-names>DG</given-names></name><name><surname>Spanos</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Error Statistics</chapter-title><person-group person-group-type="editor"><name><surname>Bandyopadhyay</surname><given-names>PS</given-names></name><name><surname>Forster</surname><given-names>MR</given-names></name></person-group><source>Handbook of the Philosophy of Science</source><publisher-loc>Amsterdam</publisher-loc><publisher-name>North-Holland</publisher-name><fpage>153</fpage><lpage>198</lpage></element-citation></ref><ref id="bib107"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mayo</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/9781107286184</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayr</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Cause and effect in biology</article-title><source>Science</source><volume>134</volume><fpage>1501</fpage><lpage>1506</lpage><pub-id pub-id-type="doi">10.1126/science.134.3489.1501</pub-id><pub-id pub-id-type="pmid">14471768</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McCall Smith</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>The Good Husband of Zebra Drive</source><publisher-name>Knopf Doubleday Publishing Group</publisher-name></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menditto</surname><given-names>A</given-names></name><name><surname>Patriarca</surname><given-names>M</given-names></name><name><surname>Magnusson</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Understanding the meaning of accuracy, trueness and precision</article-title><source>Accreditation and Quality Assurance</source><volume>12</volume><fpage>45</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1007/s00769-006-0191-z</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merker</surname><given-names>B</given-names></name><name><surname>Williford</surname><given-names>K</given-names></name><name><surname>Rudrauf</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The integrated information theory of consciousness: a case of mistaken identity</article-title><source>Behavioral and Brain Sciences</source><volume>45</volume><elocation-id>e41</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X21000881</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesulam</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>From sensation to cognition</article-title><source>Brain</source><volume>121 (Pt 6)</volume><fpage>1013</fpage><lpage>1052</lpage><pub-id pub-id-type="doi">10.1093/brain/121.6.1013</pub-id><pub-id pub-id-type="pmid">9648540</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meunier</surname><given-names>D</given-names></name><name><surname>Lambiotte</surname><given-names>R</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modular and hierarchically modular organization of brain networks</article-title><source>Frontiers in Neuroscience</source><volume>4</volume><elocation-id>200</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2010.00200</pub-id><pub-id pub-id-type="pmid">21151783</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Zaldivar</surname><given-names>A</given-names></name><name><surname>Barnes</surname><given-names>P</given-names></name><name><surname>Vasserman</surname><given-names>L</given-names></name><name><surname>Hutchinson</surname><given-names>B</given-names></name><name><surname>Spitzer</surname><given-names>E</given-names></name><name><surname>Raji</surname><given-names>ID</given-names></name><name><surname>Gebru</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Model cards for model reporting</article-title><conf-name>FAT* ’19</conf-name><fpage>220</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1145/3287560.3287596</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mizrahi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>The Relativity of Theory: Key Positions and Arguments in the Contemporary Scientific Realism/Antirealism Debate</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-030-58047-6</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobbs</surname><given-names>D</given-names></name><name><surname>Trimmer</surname><given-names>PC</given-names></name><name><surname>Blumstein</surname><given-names>DT</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Foraging for foundations in decision neuroscience: insights from ethology</article-title><source>Nature Reviews Neuroscience</source><volume>19</volume><fpage>419</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0010-7</pub-id><pub-id pub-id-type="pmid">29752468</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musgrave</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Falsification and its critics</article-title><source>Studies in Logic and the Foundations of Mathematics</source><volume>74</volume><fpage>393</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/S0049-237X(09)70374-X</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nanda</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>GW</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Chang</surname><given-names>C</given-names></name><name><surname>Englot</surname><given-names>DJ</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Rubinov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Time-resolved correlation of distributed brain activity tracks E-I balance and accounts for diverse scale-free phenomena</article-title><source>Cell Reports</source><volume>42</volume><elocation-id>112254</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2023.112254</pub-id><pub-id pub-id-type="pmid">36966391</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nesse</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tinbergen’s four questions, organized: a response to Bateson and Laland</article-title><source>Trends in Ecology &amp; Evolution</source><volume>28</volume><fpage>681</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2013.10.008</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Newman</surname><given-names>MEJ</given-names></name><name><surname>Barkema</surname><given-names>GT</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Monte Carlo Methods in Statistical Physics</source><publisher-name>Clarendon Press</publisher-name><pub-id pub-id-type="doi">10.1093/oso/9780198517962.001.0001</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nieuwenhuys</surname><given-names>R</given-names></name><name><surname>Puelles</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Towards a New Neuromorphology</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-25693-1</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The diversity of eyes and vision</article-title><source>Annual Review of Vision Science</source><volume>7</volume><fpage>19</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-121820-074736</pub-id><pub-id pub-id-type="pmid">34086478</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nizami</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Information theory is abused in neuroscience</article-title><source>Cybernetics &amp; Human Knowing</source><volume>26</volume><fpage>47</fpage><lpage>97</lpage></element-citation></ref><ref id="bib124"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oddie</surname><given-names>G</given-names></name><name><surname>Cevolani</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>Truthlikeness</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name><name><surname>Nodelman</surname><given-names>U</given-names></name></person-group><source>Stanford Encyclopedia of Philosophy</source><publisher-name>Metaphysics Research Lab, Stanford University</publisher-name></element-citation></ref><ref id="bib125"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oerter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The Theory of Almost Everything: The Standard Model, the Unsung Triumph of Modern Physics</source><publisher-name>Penguin</publisher-name></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>6251</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orr</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fitness and its role in evolutionary genetics</article-title><source>Nature Reviews Genetics</source><volume>10</volume><fpage>531</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1038/nrg2603</pub-id><pub-id pub-id-type="pmid">19546856</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasqualetti</surname><given-names>F</given-names></name><name><surname>Zampieri</surname><given-names>S</given-names></name><name><surname>Bullo</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Controllability metrics, limitations and algorithms for complex networks</article-title><source>IEEE Transactions on Control of Network Systems</source><volume>1</volume><fpage>40</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1109/TCNS.2014.2310254</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasqualetti</surname><given-names>F</given-names></name><name><surname>Gu</surname><given-names>S</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>RE: Warnings and caveats in brain controllability</article-title><source>NeuroImage</source><volume>197</volume><fpage>586</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.001</pub-id><pub-id pub-id-type="pmid">31075390</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patankar</surname><given-names>SP</given-names></name><name><surname>Kim</surname><given-names>JZ</given-names></name><name><surname>Pasqualetti</surname><given-names>F</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Path-dependent connectivity, not modularity, consistently predicts controllability of structural brain networks</article-title><source>Network Neuroscience</source><volume>4</volume><fpage>1091</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00157</pub-id><pub-id pub-id-type="pmid">33195950</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>GH</given-names></name><name><surname>Kaplan</surname><given-names>DM</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Topographic organization in the brain: searching for general principles</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>351</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.03.008</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The Levels of Understanding framework, revised</article-title><source>Perception</source><volume>41</volume><fpage>1017</fpage><lpage>1023</lpage><pub-id pub-id-type="doi">10.1068/p7299</pub-id><pub-id pub-id-type="pmid">23409366</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Models of visual cortex</article-title><source>Scholarpedia</source><volume>8</volume><elocation-id>3516</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.3516</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Popper</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Conjectures and Refutations: The Growth of Scientific Knowledge</source><publisher-name>Routledge</publisher-name></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prichard</surname><given-names>D</given-names></name><name><surname>Theiler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Generating surrogate data for time series with several simultaneously measured variables</article-title><source>Physical Review Letters</source><volume>73</volume><fpage>951</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.73.951</pub-id><pub-id pub-id-type="pmid">10057582</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandran</surname><given-names>VS</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>The neurobiology of perception</article-title><source>Perception</source><volume>14</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1068/p140097</pub-id><pub-id pub-id-type="pmid">4069953</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Do gamma oscillations play a role in cerebral cortex?</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>78</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.002</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reid</surname><given-names>AT</given-names></name><name><surname>Headley</surname><given-names>DB</given-names></name><name><surname>Mill</surname><given-names>RD</given-names></name><name><surname>Sanchez-Romero</surname><given-names>R</given-names></name><name><surname>Uddin</surname><given-names>LQ</given-names></name><name><surname>Marinazzo</surname><given-names>D</given-names></name><name><surname>Lurie</surname><given-names>DJ</given-names></name><name><surname>Valdés-Sosa</surname><given-names>PA</given-names></name><name><surname>Hanson</surname><given-names>SJ</given-names></name><name><surname>Biswal</surname><given-names>BB</given-names></name><name><surname>Calhoun</surname><given-names>V</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Cole</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Advancing functional connectivity research from association to causation</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1751</fpage><lpage>1760</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0510-4</pub-id><pub-id pub-id-type="pmid">31611705</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Riebli</surname><given-names>N</given-names></name><name><surname>Reichert</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>The first nervous system</chapter-title><person-group person-group-type="editor"><name><surname>Shepherd</surname><given-names>SV</given-names></name></person-group><source>The Wiley Handbook of Evolutionary Neuroscience</source><publisher-name>Wiley</publisher-name><fpage>125</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1002/9781118316757</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>JA</given-names></name><name><surname>Perry</surname><given-names>A</given-names></name><name><surname>Lord</surname><given-names>AR</given-names></name><name><surname>Roberts</surname><given-names>G</given-names></name><name><surname>Mitchell</surname><given-names>PB</given-names></name><name><surname>Smith</surname><given-names>RE</given-names></name><name><surname>Calamante</surname><given-names>F</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The contribution of geometry to the human connectome</article-title><source>NeuroImage</source><volume>124</volume><fpage>379</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.09.009</pub-id><pub-id pub-id-type="pmid">26364864</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roux</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The concept of function in modern physiology</article-title><source>Journal of Physiology</source><volume>592</volume><fpage>2245</fpage><lpage>2249</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2014.272062</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural networks in the future of neuroscience research</article-title><source>Nature Reviews Neuroscience</source><volume>16</volume><elocation-id>767</elocation-id><pub-id pub-id-type="doi">10.1038/nrn4042</pub-id><pub-id pub-id-type="pmid">26486186</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Constraints and spandrels of interareal connectomes</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13812</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13812</pub-id><pub-id pub-id-type="pmid">27924867</pub-id></element-citation></ref><ref id="bib145"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Pipeline for semi-automated analysis of network-neuroscience literature</data-title><version designator="swh:1:rev:dc5a056851b7b6ba4d6aae34447eb759f62db9e1">swh:1:rev:dc5a056851b7b6ba4d6aae34447eb759f62db9e1</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:94844f44be5568182054c9c70665c57be12a4810;origin=https://github.com/mikarubi/litrev;visit=swh:1:snp:d42d9bc6260b2448a6fb718496a4f1070e525492;anchor=swh:1:rev:dc5a056851b7b6ba4d6aae34447eb759f62db9e1">https://archive.softwareheritage.org/swh:1:dir:94844f44be5568182054c9c70665c57be12a4810;origin=https://github.com/mikarubi/litrev;visit=swh:1:snp:d42d9bc6260b2448a6fb718496a4f1070e525492;anchor=swh:1:rev:dc5a056851b7b6ba4d6aae34447eb759f62db9e1</ext-link></element-citation></ref><ref id="bib146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Zipursky</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Design principles of insect and vertebrate visual systems</article-title><source>Neuron</source><volume>66</volume><fpage>15</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.018</pub-id><pub-id pub-id-type="pmid">20399726</pub-id></element-citation></ref><ref id="bib147"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schellenberger</surname><given-names>J</given-names></name><name><surname>Palsson</surname><given-names>BØ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Use of randomized sampling for analysis of metabolic networks</article-title><source>Journal of Biological Chemistry</source><volume>284</volume><fpage>5457</fpage><lpage>5461</lpage><pub-id pub-id-type="doi">10.1074/jbc.R800048200</pub-id><pub-id pub-id-type="pmid">18940807</pub-id></element-citation></ref><ref id="bib148"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiber</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Constrained randomization of time series data</article-title><source>Physical Review Letters</source><volume>80</volume><fpage>2105</fpage><lpage>2108</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.80.2105</pub-id></element-citation></ref><ref id="bib149"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Millidge</surname><given-names>B</given-names></name><name><surname>Buckley</surname><given-names>CL</given-names></name><name><surname>Tschantz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curious inferences: Reply to Sun and Firestone on the Dark Room Problem</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>681</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.05.011</pub-id></element-citation></ref><ref id="bib150"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sethna</surname><given-names>JP</given-names></name><name><surname>Dahmen</surname><given-names>KA</given-names></name><name><surname>Myers</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Crackling noise</article-title><source>Nature</source><volume>410</volume><fpage>242</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/35065675</pub-id><pub-id pub-id-type="pmid">11258379</pub-id></element-citation></ref><ref id="bib151"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shackel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The vacuity of postmodernist methodology</article-title><source>Metaphilosophy</source><volume>36</volume><fpage>295</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9973.2005.00370.x</pub-id></element-citation></ref><ref id="bib152"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shew</surname><given-names>WL</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Petermann</surname><given-names>T</given-names></name><name><surname>Roy</surname><given-names>R</given-names></name><name><surname>Plenz</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuronal avalanches imply maximum dynamic range in cortical networks at criticality</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>15595</fpage><lpage>15600</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3864-09.2009</pub-id><pub-id pub-id-type="pmid">20007483</pub-id></element-citation></ref><ref id="bib153"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shew</surname><given-names>WL</given-names></name><name><surname>Plenz</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The functional benefits of criticality in the cortex</article-title><source>The Neuroscientist</source><volume>19</volume><fpage>88</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1177/1073858412445487</pub-id><pub-id pub-id-type="pmid">22627091</pub-id></element-citation></ref><ref id="bib154"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shmueli</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>To explain or to predict?</article-title><source>Statistical Science</source><volume>25</volume><fpage>289</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1214/10-STS330</pub-id></element-citation></ref><ref id="bib155"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sibbald</surname><given-names>B</given-names></name><name><surname>Roland</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Understanding controlled trials: why are randomised controlled trials important?</article-title><source>BMJ</source><volume>316</volume><elocation-id>201</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.316.7126.201</pub-id><pub-id pub-id-type="pmid">9468688</pub-id></element-citation></ref><ref id="bib156"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siddiqi</surname><given-names>SH</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Fox</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Causal mapping of human brain function</article-title><source>Nature Reviews Neuroscience</source><volume>23</volume><fpage>361</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/s41583-022-00583-8</pub-id><pub-id pub-id-type="pmid">35444305</pub-id></element-citation></ref><ref id="bib157"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Consciousness and the binding problem</article-title><source>Annals of the New York Academy of Sciences</source><volume>929</volume><fpage>123</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2001.tb05712.x</pub-id><pub-id pub-id-type="pmid">11349422</pub-id></element-citation></ref><ref id="bib158"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neuronal oscillations: unavoidable and useful?</article-title><source>The European Journal of Neuroscience</source><volume>48</volume><fpage>2389</fpage><lpage>2398</lpage><pub-id pub-id-type="doi">10.1111/ejn.13796</pub-id><pub-id pub-id-type="pmid">29247490</pub-id></element-citation></ref><ref id="bib159"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohal</surname><given-names>VS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How close are we to understanding what (if anything) γ oscillations do in cortical circuits?</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>10489</fpage><lpage>10495</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0990-16.2016</pub-id><pub-id pub-id-type="pmid">27733600</pub-id></element-citation></ref><ref id="bib160"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sokal</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Beyond the Hoax: Science, Philosophy and Culture</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib161"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Zwi</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The small world of the cerebral cortex</article-title><source>Neuroinformatics</source><volume>2</volume><fpage>145</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1385/NI:2:2:145</pub-id><pub-id pub-id-type="pmid">15319512</pub-id></element-citation></ref><ref id="bib162"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Network attributes for segregation and integration in the human brain</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>162</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.11.015</pub-id><pub-id pub-id-type="pmid">23294553</pub-id></element-citation></ref><ref id="bib163"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Graph theory methods: applications in brain networks</article-title><source>Dialogues in Clinical Neuroscience</source><volume>20</volume><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.31887/DCNS.2018.20.2/osporns</pub-id><pub-id pub-id-type="pmid">30250388</pub-id></element-citation></ref><ref id="bib164"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squartini</surname><given-names>T</given-names></name><name><surname>Garlaschelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Analytical maximum-likelihood method to detect patterns in real networks</article-title><source>New Journal of Physics</source><volume>13</volume><elocation-id>083001</elocation-id><pub-id pub-id-type="doi">10.1088/1367-2630/13/8/083001</pub-id></element-citation></ref><ref id="bib165"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stangor</surname><given-names>C</given-names></name><name><surname>Walinga</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Introduction to Psychology</source><publisher-name>BCcampus</publisher-name></element-citation></ref><ref id="bib166"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Burns</surname><given-names>GA</given-names></name><name><surname>O’Neill</surname><given-names>MA</given-names></name><name><surname>Young</surname><given-names>MP</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Computational analysis of functional connectivity between areas of primate cerebral cortex</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>355</volume><fpage>111</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1098/rstb.2000.0552</pub-id><pub-id pub-id-type="pmid">10703047</pub-id></element-citation></ref><ref id="bib167"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strausfeld</surname><given-names>NJ</given-names></name><name><surname>Hirth</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deep homology of arthropod central complex and vertebrate basal ganglia</article-title><source>Science</source><volume>340</volume><fpage>157</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1126/science.1231828</pub-id><pub-id pub-id-type="pmid">23580521</pub-id></element-citation></ref><ref id="bib168"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Z</given-names></name><name><surname>Firestone</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Dark Room Problem</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>346</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.02.006</pub-id></element-citation></ref><ref id="bib169"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Zou</surname><given-names>W</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Identifying controlling nodes in neuronal networks in different scales</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e41375</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0041375</pub-id><pub-id pub-id-type="pmid">22848475</pub-id></element-citation></ref><ref id="bib170"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>E</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Colloquium: Control of dynamics in brain networks</article-title><source>Reviews of Modern Physics</source><volume>90</volume><elocation-id>31003</elocation-id><pub-id pub-id-type="doi">10.1103/RevModPhys.90.031003</pub-id></element-citation></ref><ref id="bib171"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinbergen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>On aims and methods of ethology</article-title><source>Zeitschrift Für Tierpsychologie</source><volume>20</volume><fpage>410</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0310.1963.tb01161.x</pub-id></element-citation></ref><ref id="bib172"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tosches</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Developmental and genetic mechanisms of neural circuit evolution</article-title><source>Developmental Biology</source><volume>431</volume><fpage>16</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.ydbio.2017.06.016</pub-id><pub-id pub-id-type="pmid">28645748</pub-id></element-citation></ref><ref id="bib173"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tu</surname><given-names>C</given-names></name><name><surname>Rocha</surname><given-names>RP</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Zampieri</surname><given-names>S</given-names></name><name><surname>Zorzi</surname><given-names>M</given-names></name><name><surname>Suweis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Warnings and caveats in brain controllability</article-title><source>NeuroImage</source><volume>176</volume><fpage>83</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.04.010</pub-id><pub-id pub-id-type="pmid">29654874</pub-id></element-citation></ref><ref id="bib174"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Cruys</surname><given-names>S</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Controlled optimism: Reply to Sun and Firestone on the Dark Room Problem</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>680</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.05.012</pub-id></element-citation></ref><ref id="bib175"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Rich-club organization of the human connectome</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>15775</fpage><lpage>15786</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3539-11.2011</pub-id><pub-id pub-id-type="pmid">22049421</pub-id></element-citation></ref><ref id="bib176"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>van Fraassen</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="1980">1980</year><source>The Scientific Image</source><publisher-name>Clarendon Press</publisher-name><pub-id pub-id-type="doi">10.1093/0198244274.001.0001</pub-id></element-citation></ref><ref id="bib177"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vazza</surname><given-names>F</given-names></name><name><surname>Feletti</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The quantitative comparison between the neuronal network and the cosmic web</article-title><source>Frontiers in Physics</source><volume>8</volume><elocation-id>525731</elocation-id><pub-id pub-id-type="doi">10.3389/fphy.2020.525731</pub-id></element-citation></ref><ref id="bib178"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogelsberger</surname><given-names>M</given-names></name><name><surname>Genel</surname><given-names>S</given-names></name><name><surname>Springel</surname><given-names>V</given-names></name><name><surname>Torrey</surname><given-names>P</given-names></name><name><surname>Sijacki</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name><name><surname>Snyder</surname><given-names>G</given-names></name><name><surname>Bird</surname><given-names>S</given-names></name><name><surname>Nelson</surname><given-names>D</given-names></name><name><surname>Hernquist</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Properties of galaxies reproduced by a hydrodynamic simulation</article-title><source>Nature</source><volume>509</volume><fpage>177</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1038/nature13316</pub-id><pub-id pub-id-type="pmid">24805343</pub-id></element-citation></ref><ref id="bib179"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vul</surname><given-names>E</given-names></name><name><surname>Harris</surname><given-names>C</given-names></name><name><surname>Winkielman</surname><given-names>P</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Puzzlingly high correlations in fMRI studies of emotion, personality, and social cognition</article-title><source>Perspectives on Psychological Science</source><volume>4</volume><fpage>274</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1111/j.1745-6924.2009.01125.x</pub-id><pub-id pub-id-type="pmid">26158964</pub-id></element-citation></ref><ref id="bib180"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallisch</surname><given-names>P</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Structure and function come unglued in the visual cortex</article-title><source>Neuron</source><volume>60</volume><fpage>195</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.008</pub-id><pub-id pub-id-type="pmid">18957212</pub-id></element-citation></ref><ref id="bib181"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wasserstein</surname><given-names>RL</given-names></name><name><surname>Lazar</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The ASA statement on <italic>p</italic>-values: Context, process, and purpose</article-title><source>The American Statistician</source><volume>70</volume><fpage>129</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1080/00031305.2016.1154108</pub-id></element-citation></ref><ref id="bib182"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watts</surname><given-names>DJ</given-names></name><name><surname>Strogatz</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Collective dynamics of “small-world” networks</article-title><source>Nature</source><volume>393</volume><fpage>440</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1038/30918</pub-id><pub-id pub-id-type="pmid">9623998</pub-id></element-citation></ref><ref id="bib183"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>LM</given-names></name><name><surname>Saelens</surname><given-names>W</given-names></name><name><surname>Cannoodt</surname><given-names>R</given-names></name><name><surname>Soneson</surname><given-names>C</given-names></name><name><surname>Hapfelmeier</surname><given-names>A</given-names></name><name><surname>Gardner</surname><given-names>PP</given-names></name><name><surname>Boulesteix</surname><given-names>AL</given-names></name><name><surname>Saeys</surname><given-names>Y</given-names></name><name><surname>Robinson</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Essential guidelines for computational method benchmarking</article-title><source>Genome Biology</source><volume>20</volume><elocation-id>125</elocation-id><pub-id pub-id-type="doi">10.1186/s13059-019-1738-8</pub-id><pub-id pub-id-type="pmid">31221194</pub-id></element-citation></ref><ref id="bib184"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Lin</surname><given-names>BJ</given-names></name><name><surname>Chen</surname><given-names>TW</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A comparison of neuronal population dynamics measured with calcium imaging and electrophysiology</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008198</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008198</pub-id><pub-id pub-id-type="pmid">32931495</pub-id></element-citation></ref><ref id="bib185"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilting</surname><given-names>J</given-names></name><name><surname>Priesemann</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>25 years of criticality in neuroscience - established results, open controversies, novel concepts</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>105</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.08.002</pub-id><pub-id pub-id-type="pmid">31546053</pub-id></element-citation></ref><ref id="bib186"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Worrall</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1978">1978</year><chapter-title>The ways in which the methodology of scientific research programmes improves on Popper’s methodology</chapter-title><person-group person-group-type="editor"><name><surname>Radnitzky</surname><given-names>G</given-names></name><name><surname>Andersson</surname><given-names>G</given-names></name></person-group><source>Progress and Rationality in Science</source><publisher-name>Springer</publisher-name><fpage>45</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1007/978-94-009-9866-7</pub-id></element-citation></ref><ref id="bib187"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wray</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Selection and predictive success</article-title><source>Erkenntnis</source><volume>72</volume><fpage>365</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1007/s10670-009-9206-6</pub-id></element-citation></ref><ref id="bib188"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The generalizability crisis</article-title><source>Behavioral and Brain Sciences</source><volume>45</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1017/S0140525X20001685</pub-id><pub-id pub-id-type="pmid">33342451</pub-id></element-citation></ref><ref id="bib189"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshihara</surname><given-names>M</given-names></name><name><surname>Yoshihara</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>“Necessary and sufficient” in biology is not necessarily necessary - confusions and erroneous conclusions resulting from misapplied logic in the field of biology, especially neuroscience</article-title><source>Journal of Neurogenetics</source><volume>32</volume><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1080/01677063.2018.1468443</pub-id><pub-id pub-id-type="pmid">29757057</pub-id></element-citation></ref><ref id="bib190"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zamora-López</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>C</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical hubs form a module for multisensory integration on top of the hierarchy of cortical networks</article-title><source>Frontiers in Neuroinformatics</source><volume>4</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.001.2010</pub-id><pub-id pub-id-type="pmid">20428515</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><p>This section provides two complementary perspectives on the concept of relative trueness.</p><sec sec-type="appendix" id="s4"><title>Philosophical perspective</title><p>This work posits that all else being equal, models that are more explanatorily successful — that explain the data more accurately or with fewer assumptions — are likely to be truer than rival models. This position is largely compatible with the two main philosophical perspectives on model trueness (known in philosophy as truthlikeness [<xref ref-type="bibr" rid="bib124">Oddie and Cevolani, 2022</xref>]). The most popular perspective, scientific realism, broadly posits that the most successful scientific models are likely to be approximately true (<xref ref-type="bibr" rid="bib17">Bourget and Chalmers, 2014</xref>; <xref ref-type="bibr" rid="bib30">Chakravartty, 2017</xref>). The main alternative perspective, scientific antirealism, broadly disagrees with this position. This disagreement forms the basis of a longstanding and possibly irreconcilable debate. Despite this disagreement, however, both perspectives broadly agree that all else being equal, more successful models are likely to be truer than rival models (<xref ref-type="bibr" rid="bib187">Wray, 2010</xref>).</p><p>The position in this work is largely compatible with both perspectives because it narrowly centers on this point of agreement and because it avoids taking sides in the disagreement. <xref ref-type="bibr" rid="bib115">Mizrahi, 2020</xref> described a very similar middle-ground position of <italic>relative realism</italic>:</p><list list-type="simple"><list-item><p>“[W]e have adequate grounds for believing that, from a set of competing scientific theories, the more empirically successful theory is <italic>comparatively</italic> true, that is, closer to the truth relative to its competitors in the set, rather than approximately true.” and “[A] scientific theory can be […] comparatively true, but still be quite far off from the truth.”</p></list-item></list><p>Relative trueness resembles the biological concept of relative fitness or reproductive success. Much as it is more meaningful to study the relative, rather than absolute, fitness of individual organisms (<xref ref-type="bibr" rid="bib127">Orr, 2009</xref>), so it is often more meaningful to study the relative, rather than absolute, trueness of scientific models.</p><p>Note also that model trueness differs from model utility. For example, many models in neuroscience can make accurate predictions but be biologically unconstrained, artificially structured, or altogether uninterpretable. Such models are useful predictive tools (have high utility) but do not accurately explain biological reality (have relatively low trueness).</p></sec><sec sec-type="appendix" id="s5"><title>Statistical perspective</title><p>This work focuses on scientific analyses that estimate parameters of explanatory models (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, top). Many such analyses cannot express explanatory models as parametric equations and cannot perform formal parameter estimation. Instead, these analyses often use significance tests to informally estimate parameters of underlying explanatory models.</p><p>This work assumes that all parameter estimates are directly comparable across all models. It makes this assumption without loss of generality because any two models can be nested within a more general model. It adopts the terminology of the International Organization for Standardization (<xref ref-type="bibr" rid="bib82">ISO, 1994</xref>; <xref ref-type="bibr" rid="bib110">Menditto et al., 2007</xref>) to quantify the accuracy of these parameter estimates. It defines trueness as the inverse of the estimation bias and precision as the inverse of the estimation variance (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, bottom). Note that trueness has a single and clear meaning, whereas bias is often a catch-all term, especially outside statistics (<xref ref-type="bibr" rid="bib43">Danks and London, 2017</xref>; <xref ref-type="bibr" rid="bib50">Fanelli et al., 2017</xref>).</p><p>This work focuses on the problem of redundant features in explanatory models. Successful explanatory models tend to have relatively low estimation bias. The inclusion of redundant features tends to increase this bias and thereby reduce explanatory success. By contrast, the work does not consider the problem of redundant features in predictive models. Successful predictive models have relatively low prediction bias but may not necessarily have low estimation bias, as we saw in the above distinction between trueness and utility. The inclusion of redundant features in these models does not necessarily increase their prediction bias (<xref ref-type="bibr" rid="bib69">Guyon and Elisseeff, 2003</xref>) and, in this way, does not necessarily reduce their predictive success. (Note that this distinction between explanatory and predictive modeling differs from the treatment of <xref ref-type="bibr" rid="bib154">Shmueli, 2010</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Statistical perspective on trueness and precision.</title><p>Top (flowchart): Analyses as parameter estimates of explanatory models. Bottom (target): Four example parameter estimates with distinct precision and trueness profiles (colored dots). True parameter values denote true explanations and not true predictions. High-bias estimates denote explanations that have low relative trueness. By contrast, high-variance estimates denote explanations that have low precision.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-app1-fig1-v2.tif"/></fig></sec></app><app id="appendix-2"><title>Appendix 2</title><p>This section relates circular and unified analyses to Mayo’s framework of severe testing (<xref ref-type="bibr" rid="bib106">Mayo and Spanos, 2011</xref>; <xref ref-type="bibr" rid="bib107">Mayo, 2018</xref>). First, it shows that circular analyses of knowledge form a specific violation of Mayo’s weak-severity requirement. Second, it shows that unified analyses form a specific adherence to Mayo’s strong-severity requirement. Third, it describes unified analyses as a type of <italic>severe selection</italic>: a hybrid approach that combines severe testing with model selection.</p><sec sec-type="appendix" id="s6"><title>Weak-severity requirement and circular analysis</title><p><xref ref-type="bibr" rid="bib107">Mayo, 2018</xref> defines her weak-severity requirement as follows:</p><list list-type="simple"><list-item><p>“One does not have evidence for a claim if nothing has been done to rule out ways the claim may be false. If data […] agree with a claim C but the method used is practically guaranteed to find such agreement, and had little or no capability of finding flaws with C even if they exist, then we have bad evidence, no test (BENT).”</p></list-item></list><p><xref ref-type="table" rid="app2table1">Appendix 2—table 1</xref> contrasts this definition with our definitions of circular analysis.</p><table-wrap id="app2table1" position="float"><label>Appendix 2—table 1.</label><caption><title>Weak-severity requirement and circular analysis.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Weak-severity requirement (Mayo)</th><th align="left" valign="bottom">Circular analysis (this work)</th></tr></thead><tbody><tr><td align="left" valign="top"><italic>Bad evidence, no test</italic>.<list list-type="order"><list-item><p>Use a test that practically guarantees to find agreement between data and claim and has little or no capability of finding flaws with the claim even if they exist.</p></list-item><list-item><p>Show that data agree with the claim on the basis of this test.</p></list-item></list></td><td align="left" valign="top"><italic>General definition (weak evidence of progress</italic>).<list list-type="order"><list-item><p>Test a model in a way that almost invariably accepts the model.</p></list-item><list-item><p>Accept the model on the basis of this test.</p></list-item></list></td></tr><tr><td align="left" valign="top"><italic>N/A</italic>.<break/>The framework of severe testing, and the weak-severity requirement, do not specifically consider the problem of redundant explanations.</td><td align="left" valign="top"><italic>Specific definition (strong evidence of stagnation</italic>).<list list-type="order"><list-item><p>Test the statistical significance of redundant features in a way that almost invariably shows this significance against a strawman model.</p></list-item><list-item><p>Accept the corresponding model on the basis of this test.</p></list-item></list></td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s7"><title>Strong-severity requirement and unified analysis</title><p><xref ref-type="bibr" rid="bib107">Mayo, 2018</xref> defines her strong-severity requirement as follows:</p><list list-type="simple"><list-item><p>“We have evidence for a claim C just to the extent it survives a stringent scrutiny. If C passes a test that was highly capable of finding flaws or discrepancies from C, and yet none or few are found, then the passing result […] is evidence for C.”</p></list-item></list><p><xref ref-type="table" rid="app2table2">Appendix 2—table 2</xref> contrasts this definition with our discussion of unified analysis.</p><table-wrap id="app2table2" position="float"><label>Appendix 2—table 2.</label><caption><title>Strong severity and unified analysis.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Strong-severity requirement (Mayo)</th><th align="left" valign="bottom">Unified analysis (this work)</th></tr></thead><tbody><tr><td align="left" valign="top"><italic>Evidence from survival of stringent scrutiny</italic>.<list list-type="order"><list-item><p>Use a test that is highly capable of finding flaws or discrepancies with a claim if they exist.</p></list-item><list-item><p>Show that this test does not find flaws or discrepancies with the claim.</p></list-item></list></td><td align="left" valign="top"><italic>Evidence of genuinely new discovery</italic>.<list list-type="order"><list-item><p>Define a benchmark model that includes all important existing knowledge about some phenomenon of interest.</p></list-item><list-item><p>Show the statistical significance of a speculative feature against this model.</p></list-item></list></td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s8"><title>Severe selection</title><p>Severe testing largely builds on Popper’s ideas and terminology. <xref ref-type="bibr" rid="bib134">Popper, 2002</xref> considered that falsification centers on genuine or severe tests:</p><list list-type="simple"><list-item><p>“A theory which is not refutable by any conceivable event is nonscientific. Irrefutability is not a virtue of a theory (as people often think) but a vice. Every genuine [or severe] test of a theory is an attempt to falsify it, or to refute it. Testability is falsifiability.”</p></list-item></list><p>At the same time, Popper did not consider that severe tests need to involve testable, or viable, rival models, “the negation of a testable (or falsifiable) statement need not be testable” (<xref ref-type="bibr" rid="bib134">Popper, 2002</xref>). For example, a wrong prediction can falsify a speculative model but need not accept a testable or viable rival model.</p><p>This lack of viable rival models can make it hard to eliminate falsified explanations (<xref ref-type="table" rid="table3">Table 3</xref>). <xref ref-type="bibr" rid="bib95">Lakatos, 1976</xref>, among others (<xref ref-type="bibr" rid="bib117">Musgrave, 1973</xref>), has made this point:</p><list list-type="simple"><list-item><p>“‘Falsification’ […] (corroborated counterevidence) is not a sufficient condition for eliminating a specific theory: in spite of hundreds of known anomalies we do not regard [a theory] as falsified (that is, eliminated) until we have a better one.”</p></list-item></list><p>By contrast, tests against benchmark models always lead to the acceptance of viable rival models. In this sense, these tests form a type of severe selection: they combine aspects of severe testing (stringent scrutiny) with aspects of model selection (viable rival models). This severe selection between strong rival models may also resemble, more closely than severe testing, the natural selection of strong rival organisms (Appendix 1).</p></sec></app><app id="appendix-3"><title>Appendix 3</title><p>This section provides a unified perspective on the circular analyses of noise in <xref ref-type="bibr" rid="bib89">Kriegeskorte et al., 2009</xref> and the circular analyses of knowledge in this work. It shows that these two problems, and their corresponding solutions, share deep similarities but also have basic differences. The differences reflect the basic distinctions between overfitting and overspecification.</p><sec sec-type="appendix" id="s9"><title>Similarities of circular analyses</title><p>Kriegeskorte et al. described circular analyses that lead to overfitting — the corruption of results by noise. This work describes circular analyses that lead to overspecification — the confounding of results by existing knowledge. We can show that these two problems are equivalent in important respects, by translating parts of Kriegeskorte et al. (slightly edited for clarity) into the language of this work. For simplicity, this translation focuses on an extreme version of overfitting, the full redundancy (rather than the mere non-independence) of model features with noise. Underlined text in this translation highlights the differences with the main text.</p><p>As in the main text, we can formally define this problem with three types of models.</p><p><italic>Benchmark models.</italic> These models represent all important <underline>assumptions about noise in the data</underline>. In most cases, we may simply assume that the data have noise. In some other cases, we may also assume that noise in the data follows a specific distribution.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic>: “Data are always a composite of true effects and noise.”</p></list-item><list-item><p><italic>Translation</italic>: The benchmark model assumes that the data have noise.</p></list-item></list><p><italic>Speculative models.</italic> These models include one or more speculative features of possible but uncertain importance. Some of these speculative features may be <underline>redundant with benchmark (noise) features</underline>. In practice, these redundant features will strongly correlate with noise in the data.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic>: “A model may capture the noise to some extent as its parameters are fitted to the data.”</p></list-item><list-item><p><italic>Translation</italic>: A model may include speculative features that are redundant with benchmark (noise) features.</p></list-item></list><p><italic>Strawman models</italic>. These models represent weak null hypotheses. Kriegeskorte et al. do not consider strawman models in their discussion. Here, we can equate the absence of strawman models with the presence of maximally weak strawman models.</p><p>These definitions allow us to express the problem in Kriegeskorte et al. in our language.</p><p><italic>Circular analyses and irreplicable explanations (overfitted models</italic>). Circular analyses almost invariably show the significance of speculative features against a strawman model because:</p><list list-type="simple"><list-item><p>The speculative feature is <underline>redundant with one or more benchmark (noise) features</underline>.</p></list-item><list-item><p>The <underline>maximally weak (absent) strawman model</underline> excludes the benchmark (noise) features with which the speculative features are redundant.</p></list-item></list><p>These analyses implicitly accept a new benchmark model that includes all existing benchmark (noise) features as well as the new redundant speculative features. In this way, these analyses explain the same aspect of the data twice: first as assumptions about noise and second as a new discovery redundant with these assumptions.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic> (Supplementary Discussion): “Using the same data set to generate and test a hypothesis is circular unless [we] address the question: If the data contained only noise and we searched for an effect the way we did, with what probability would we find an effect as strong as (or stronger than) the one we observed?”</p></list-item><list-item><p><italic>Translation</italic>: Accepting a feature known to be redundant with noise is circular unless we test the significance of the feature against a benchmark (noise) model.</p></list-item></list></sec><sec sec-type="appendix" id="s10"><title>Similarities of unified analyses</title><p>Kriegeskorte et al. described two tests to counter circular analyses of noise. Both tests center on the sampling of data and on testing the significance of speculative features against these sampled data. We can likewise translate these descriptions (slightly edited for clarity) into the language of this work.</p><p><italic>Tests of non-redundancy.</italic> These tests are equivalent to our tests against benchmark models. First, they sample data from benchmark-model distributions: data in which all benchmark (noise) features are preserved, and all other aspects of the data are maximally random. Second, they test the statistical significance of speculative features against these data. Third, the finding of statistical significance implies that the tested speculative features are not redundant with benchmark (noise) features.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic> (Supplementary Discussion): “Modeling the effect of assumptions may not be tractable analytically, but could be achieved by simulation of null data.”</p></list-item><list-item><p><italic>Translation</italic>: Testing the non-redundancy of speculative features against benchmark (noise) features may not be tractable analytically, but could be achieved by sampling data from benchmark (noise) model distributions.</p></list-item></list><p><italic>Tests of independence.</italic> These tests are not discussed in the main text. First, they sample data in which <underline>all benchmark (noise) features are maximally random, and all other aspects of the data are preserved</underline>. Second, they test the statistical significance of speculative features against these data. Third, the finding of statistical significance implies that the tested speculative features are <underline>independent of benchmark (noise) features</underline>.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic> (Supplementary Discussion): “Independent data can ensure independence of the results under the null hypothesis.”</p></list-item><list-item><p><italic>Translation</italic>: Data in which all benchmark (noise) features are maximally random (independent), and all other aspects of the data are preserved can ensure the independence of speculative features from benchmark (noise) features.</p></list-item></list><p><italic>Conceptual considerations.</italic> Kriegeskorte et al. primarily advocate tests of independence to prevent circular analyses of noise. Our discussion helps us to appreciate the reason for this advocacy. Noise is, by definition, an unwanted feature of the data. Therefore, it is important to show that a speculative feature is independent of noise rather than merely not redundant with it. Tests of independence, but not tests of non-redundancy, can allow us to achieve this goal.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic> (Supplementary Discussion): “Tests on null data from a random generator [of noise] can help catch statistical circularities. Unfortunately, the absence of a bias in such tests does not indicate that analyses are noncircular.”</p></list-item><list-item><p><italic>Translation</italic>: Tests of non-redundancy can help catch statistical circularities by showing that the speculative features are redundant with the noise features. Unfortunately, the absence of redundancy in such tests does not indicate that the speculative features are independent of the noise features.</p></list-item></list><p><italic>Practical considerations.</italic> The irreplicable nature of noise and the replicable nature of existing knowledge have practical implications for tests of independence. Specifically, the acquisition of new data under the same experimental conditions simulates the sampling of data in which all benchmark (noise) features are maximally random (independent), and all other aspects of the data are preserved. It follows that such data can be used to test the independence of speculative features from noise but not the independence of these features from existing knowledge.</p><list list-type="simple"><list-item><p><italic>Kriegeskorte et al.</italic> (Supplementary Discussion): “Independence in this context means the noise is statistically independent between the two data sets but real effects in the data will replicate.”</p></list-item><list-item><p><italic>Translation</italic>: Independent data amounts to the sampling of data in which all benchmark (noise) features are maximally random, and all other aspects of the data are preserved.</p></list-item></list></sec><sec sec-type="appendix" id="s11"><title>Summary of similarities</title><p>Circular analyses of noise and circular analyses of knowledge have the same basic structure. First, these analyses are vulnerable to distortions by extraneous features. Second, these analyses neglect to test for these distortions. Third, and due to this neglect, both analyses almost invariably explain the same aspect of the data twice.</p><p><xref ref-type="table" rid="app3table1">Appendix 3—table 1</xref> summarizes these basic similarities (underlined text highlights the main differences).</p><table-wrap id="app3table1" position="float"><label>Appendix 3—table 1.</label><caption><title>Two types of circular analysis.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Circular analysis of noise</th><th align="left" valign="bottom">Circular analysis of knowledge</th></tr></thead><tbody><tr><td align="left" valign="top">Conceptual problem</td><td align="left" valign="top">Explanation of the same aspect of the data twice: first as <underline>noise</underline>, and second as a new discovery <underline>non-independent of this noise</underline>.</td><td align="left" valign="top">Explanation of the same aspect of the data twice: first as <underline>existing knowledge</underline>, and second as new discovery <underline>redundant with this knowledge</underline>.</td></tr><tr><td align="left" valign="top">Statistical problem</td><td align="left" valign="top">Model <underline>overfitting</underline> that results in <underline>high variance</underline> (<underline>low precision</underline>) of estimated model parameters.</td><td align="left" valign="top">Model <underline>overspecification</underline> that results in <underline>high bias</underline> (<underline>low trueness</underline>) of estimated model parameters.</td></tr><tr><td align="left" valign="top">Statistical solution</td><td align="left" valign="top"><underline>Tests of independence</underline> against sampled data in which all benchmark (noise) features are <underline>maximally random</underline> and all other aspects of the data are <underline>preserved</underline>.</td><td align="left" valign="top"><underline>Tests of non-redundancy</underline> against sampled data in which all benchmark (existing knowledge) features are <underline>preserved</underline> and all other aspects of the data are <underline>maximally random</underline>.</td></tr></tbody></table></table-wrap><p><italic>Use novelty, theoretical novelty, and double dipping.</italic> Kriegeskorte et al. described circular analyses as a form of double dipping — the use of the same aspect of the data to formulate and test new models. This process violates the requirement for (data) use novelty (<xref ref-type="bibr" rid="bib186">Worrall, 1978</xref>). Our discussion described a more general problem of double dipping: the explanation of the same aspect of the data twice — as a benchmark feature and as a redundant feature. This more general problem violates the requirement for theoretical novelty — the need to transcend existing explanations of the data (<xref ref-type="bibr" rid="bib107">Mayo, 2018</xref>).</p></sec><sec sec-type="appendix" id="s12"><title>Analyses of artifact</title><p>We can consider data artifact as another extraneous feature. Artifact has distinct properties to noise and to existing knowledge. On the one hand, artifact is like noise because it is an unwanted feature of the data. On the other hand, artifact is like existing knowledge because it can replicate under the same experimental conditions. Together, these properties suggest that neither of the above tests can fully show the independence of results from artifact. In practice, and to mitigate this problem, we can try to remove artifact from data or test results on data recorded under different experimental conditions (<xref ref-type="bibr" rid="bib58">Geirhos et al., 2020</xref>).</p></sec><sec sec-type="appendix" id="s13"><title>Terminology</title><p>Finally, note that assumptions about artifact or noise also form a type of existing knowledge. In this sense, a more accurate, but somewhat more cumbersome, terminology of circular analyses could distinguish between:</p><list list-type="bullet"><list-item><p>Circular analyses of [existing knowledge of] artifact.</p></list-item><list-item><p>Circular analyses of [existing knowledge of] noise (the focus of Kriegeskorte et al.).</p></list-item><list-item><p>Circular analyses of [existing knowledge of] signal (the focus of this work).</p></list-item></list></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s14"><title>Literature selection</title><p>I first searched the Web of Science database in January 2019 for all original articles that contained the following topic terms (TS):</p><p><code xml:space="preserve">(TS=(&quot;network neuroscience&quot;))
 OR 
((TS=(&quot;connectom*&quot;))
 AND 
(TS=(&quot;analy*&quot;) OR TS=(&quot;model*&quot;)))
 OR 
((TS=(&quot;*brain*&quot;) OR TS=(&quot;*cort*&quot;))
 AND 
(TS=(&quot;network theor*&quot;) OR TS=(&quot;network analy*&quot;) OR TS=(&quot;network topolog*&quot;) OR TS=(&quot;network control*&quot;) OR TS=(&quot;graph theor*&quot;) OR TS=(&quot;complex network*&quot;)))</code></p><p>The initial search produced thousands of articles. I restricted this search to all articles that were published between 2014 and 2018 in neuroscience (<italic>Nature Neuroscience</italic>, <italic>Neuron</italic>), life science (<italic>eLife</italic>, <italic>PLOS Biology</italic>), clinical (<italic>Brain</italic>, <italic>Biological Psychiatry</italic>), or multidisciplinary (<italic>Nature</italic>, <italic>Science</italic>, <italic>Nature Communications</italic>, <italic>PNAS</italic>) journals. This restricted search produced 235 articles.</p></sec><sec sec-type="appendix" id="s15"><title>Literature evaluation</title><p>I manually evaluated the Methods and Results sections of all structured articles, or the full text of all unstructured articles, for the presence of circular analyses of knowledge. This evaluation centered on the following three conditions.</p><list list-type="bullet"><list-item><p>Condition 1: Presence of at least one network-neuroscience model. Network neuroscience models are network-science models of brain networks. By convention, I excluded standard dimension-reduction models of networks, such as principal component analysis, and standard network-inference models, such as dynamic causal models.</p></list-item><list-item><p>Condition 2: Acceptance of at least one <italic>M</italic><sub>1</sub>, where:</p><list list-type="bullet"><list-item><p><italic>M</italic><sub>1</sub> is a network-neuroscience model of the studied data.</p></list-item><list-item><p><italic>M</italic><sub>1</sub> includes a feature <italic>X</italic><sub>1</sub> that represents some function <italic>F</italic><sub>1</sub>.</p></list-item><list-item><p>There is no strong known mechanistic link between <italic>X</italic><sub>1</sub> and <italic>F</italic><sub>1</sub>.</p></list-item></list></list-item></list><list list-type="bullet"><list-item><p>Condition 3: No test of <italic>M</italic><sub>1</sub> against at least one <italic>M</italic><sub>0</sub>, where such a test is possible, and where:</p><list list-type="bullet"><list-item><p><italic>M</italic><sub>0</sub> is a model of the same studied data.</p></list-item><list-item><p><italic>M</italic><sub>0</sub> includes only features with known mechanistic links to function.</p></list-item><list-item><p><italic>M</italic><sub>0</sub> is known or likely to explain <italic>X</italic><sub>1</sub> as a redundant feature.</p></list-item></list></list-item></list></sec><sec sec-type="appendix" id="s16"><title>Results</title><p>This analysis found that 61% of all evaluated studies had at least one network-neuroscience model (satisfied Condition 1). These studies were suitable for further evaluation. Of these studies, 56% had at least one circular analysis of knowledge (satisfied Conditions 2–3). This estimate has a 95% bootstrap uncertainty interval of [48%, 64%]. Another 10% of studies may or may not have had such analyses. I could not say with certainty if these additional studies accepted an <italic>M</italic><sub>1</sub> or tested it against a possible <italic>M</italic><sub>0</sub>.</p><p>I revisited this search in January 2023 to include all journal articles that contained the same topic terms and that were published between 2013 and 2022. This additional search yielded 11,395 articles. I extrapolated the above percentages to estimate that more than three thousand (11 thousand × 0.61 × 0.56) studies in this larger set had at least one circular analysis of knowledge.</p></sec><sec sec-type="appendix" id="s17"><title>Auxiliary pipeline</title><p>To facilitate future systematic assessment of circular analyses of knowledge, I created an extensible semi-automated analysis pipeline in Python, a popular programming language. The following text summarizes the individual steps in this pipeline:</p><list list-type="bullet"><list-item><p>Step 1: Environment set-up and loading of previously analyzed data.</p></list-item><list-item><p>Step 2: Specification of the full literature-search query and instructions for manually downloading all reference records that match this query from the <italic>Web of Science</italic>.</p></list-item><list-item><p>Step 3: Automated download of all full-text articles that match the specified search query.</p></list-item><list-item><p>Step 4: Automated curation and cleaning of study text for all downloaded articles.</p></list-item><list-item><p>Step 5: Automated extraction of relevant text segments and emphasis of potential keywords.</p></list-item><list-item><p>Step 6: Automated scoring of the presence or absence of circular analyses based on manual evaluation of specified criteria.</p></list-item><list-item><p>Step 7: Automated storage of collated evaluations and scores in a simple database and a summary table.</p></list-item></list></sec><sec sec-type="appendix" id="s18"><title>Resource availability</title><p>Data and code are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mikarubi/litrev">https://github.com/mikarubi/litrev</ext-link> (copy archived at <xref ref-type="bibr" rid="bib145">Rubinov, 2023</xref>).</p></sec></app><app id="appendix-5"><title>Appendix 5</title><p>This section describes a framework for integrating existing knowledge about the function, structure, development, and evolution of individual biological features or traits (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref>). A benchmark model of a specific phenomenon should include, where possible, important existing knowledge from all aspects of this framework.</p><p>This framework is based on classifications of Mayr and Tinbergen (<xref ref-type="bibr" rid="bib108">Mayr, 1961</xref>; <xref ref-type="bibr" rid="bib171">Tinbergen, 1963</xref>; <xref ref-type="bibr" rid="bib96">Laland et al., 2011</xref>; <xref ref-type="bibr" rid="bib11">Bateson and Laland, 2013</xref>; <xref ref-type="bibr" rid="bib119">Nesse, 2013</xref>) and is organized along two dimensions. The first dimension reflects the nature and origin of a trait. It distinguishes what the trait is (structure and function) from how it came to be (development and evolution). The second dimension reflects the mechanistic timescales of this nature and origin. It distinguishes the proximate mechanisms of a single lifetime (structure and development) from the distal mechanisms of many generations (function and evolution).</p><p>This framework is widely accepted in biology but less well-known in neuroscience. Instead, more neuroscientists seem to know about Marr’s (and Poggio’s) three-level framework for studying the brain as a computer (<xref ref-type="bibr" rid="bib105">Marr, 2010</xref>). The first level of this framework (Marr 1) denotes the aim of brain “computation”. The second level (Marr 2) denotes the “algorithms” that achieve this aim. The third level (Marr 3) denotes the “hardware” that implements the algorithms.</p><p>The focus on computation alone is somewhat restrictive because it separates the brain from the rest of the body. We have no rigorous evidence to support this separation (see the main text for more discussion). Interestingly, and in line with this observation, Poggio recently updated Marr’s framework to include development and evolution (<xref ref-type="bibr" rid="bib132">Poggio, 2012</xref>). In this way, he seems to have independently converged on Tinbergen’s more general biological classification.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>A framework for integrating existing knowledge.</title><p>Tinbergen’s four levels of analysis (blue boxes) organized along dimensions that reflect Mayr’s distinction between proximal and distal mechanisms. Arrows denote interactions (pressures or constraints) between individual levels. <xref ref-type="bibr" rid="bib96">Laland et al., 2011</xref>, <xref ref-type="bibr" rid="bib11">Bateson and Laland, 2013</xref>, <xref ref-type="bibr" rid="bib88">Krakauer et al., 2017</xref>, and <xref ref-type="bibr" rid="bib116">Mobbs et al., 2018</xref> provide additional discussions of this framework.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79559-app5-fig1-v2.tif"/></fig></app><app id="appendix-6"><title>Appendix 6</title><p>This section discusses possible objections to the main recommendations in this work.</p><sec sec-type="appendix" id="s19"><title>Benchmark models are complicated</title><p><italic>Objection.</italic> Benchmark models that include all important existing knowledge will often be complicated. The acceptance of such models violates the scientific preference for simplicity.</p><p><italic>Clarification.</italic> There is an important difference between the preference for simplicity and the preference for parsimony. The preference for simplicity asserts that successful scientific models should be simple or elegant. This preference can be appealing but is not objectively defensible. Simple or elegant models can be aesthetically pleasing and can help formulate speculative models, but we have no objective evidence that they provide the most successful explanations of reality. <xref ref-type="bibr" rid="bib176">van Fraassen, 1980</xref> made this point more forcefully:</p><list list-type="simple"><list-item><p>“[S]ome writings […] suggest that simple theories are more likely to be true. But it is surely absurd to think that the world is more likely to be simple than complicated (unless one has certain metaphysical or theological views not usually accepted as legitimate factors in scientific inference).”</p></list-item></list><p>By contrast, the preference for parsimony asserts that all else being equal, models with fewer redundant features are likely to be more explanatory successful (or truer) than rival models. This principle makes no assumptions about simplicity or elegance and, in this way, merely embodies aspects of rational thinking.</p><p>The distinction between simplicity and parsimony has real implications for scientific practice. For example, the Standard Model of particle physics is parsimonious insofar as it lacks redundant features. Despite this, this model is neither simple nor elegant — instead, particle physicists have called it “ugly”, “repulsive”, and “awkward” (<xref ref-type="bibr" rid="bib125">Oerter, 2006</xref>). On this basis, proponents of simplicity should eliminate the Standard Model from scientific practice. By contrast, proponents of parsimony can accept this model as a successful benchmark.</p><p>Simplicity and parsimony may coexist in benchmark models of simple or circumscribed phenomena. The Hodgkin-Huxley model is both relatively simple and parsimonious. By contrast, simplicity and parsimony are less likely to coexist in benchmark models of complex or expansive phenomena, including in models of whole-brain networks.</p></sec><sec sec-type="appendix" id="s20"><title>Benchmark models favor reductionist explanations</title><p><italic>Objection.</italic> Benchmark models ignore that the same biological structure can have many functions. Specifically, these models tend to favor reductionist features and prevent the acceptance of emergent features. In practice, however, reductionist features can coexist with or give rise to important emergent features. For example, sensory-motor circuits can coexist with or give rise to important internal representations.</p><p><italic>Clarification.</italic> Benchmark models can attribute multiple functions to the same biological structure as long as these attributions produce non-redundant explanations of the data. The showing of such non-redundancy, in turn, requires rigorous evidence. We often have such evidence for reductionist features. By contrast, we often lack such evidence for emergent features, because emergent features are often hard to test in controlled ways.</p><p>A more general objection of this sort may appeal to intuitions. It may assert, for example, that emergent brain function is intuitively distinct from reductionist body function or that emergent human cognition is intuitively distinct from reductionist insect cognition. All such intuitions likewise require rigorous evidence. This evidence can be experimental or computational. It cannot, however, be solely speculative.</p></sec><sec sec-type="appendix" id="s21"><title>Benchmark models are ill-defined</title><p><italic>Objection.</italic> Benchmark models are ill-defined for many expansive phenomena. In basic neuroscience, for example, a benchmark model of cognition is ill-defined because cognition has a wealth of distinct meanings. Similarly, in clinical neuroscience, a benchmark model of schizophrenia is ill-defined because this disorder has a wealth of heterogeneous pathology.</p><p><italic>Clarification.</italic> The ill-defined nature of benchmark models reflects the ill-defined nature of many expansive phenomena. Improved definitions of these phenomena can naturally lead to improved definitions of the corresponding benchmark models. One approach to improve these definitions could focus on narrower and better-delineated portions of individual expansive phenomena. This approach could, for example, focus on well-delineated cognition linked to specific sensory-motor function or on well-delineated developmental changes linked to symptoms of schizophrenia. Iterative revisions of these narrower definitions can ultimately converge on well-defined phenomena and well-defined benchmark models.</p></sec><sec sec-type="appendix" id="s22"><title>Descriptive, explanatory, generative and null models are distinct</title><p><italic>Objection.</italic> On the one hand, many speculative models are descriptive and not explanatory models and therefore need not be rigorously tested. On the other hand, benchmark models are generative and not null models and therefore should not be used to test other models.</p><p><italic>Clarification.</italic> The nature of outwardly different model types is often similar or equivalent. The assignment of distinct roles to these equivalent models is redundant in much the same way as the assignment of distinct function to equivalent features.</p><p>First, descriptive models in neuroscience should, in many cases, be more properly termed explanatory models. Strictly speaking, descriptive models should provide neutral summaries of data. In neuroscience, however, “descriptive” summaries often represent hypotheses about important aspects of brain structure or function. In these cases, descriptive models essentially do the work of explanatory models.</p><p>Second, explanatory models can typically generate data with few or no additional assumptions. For example, an explanatory benchmark model specifies a distribution of data samples that match the empirical benchmark statistics. The sampling of data from these distributions generates data.</p><p>Third, generated data of explanatory benchmark models can be used to test null hypotheses. This finally underscores the equivalence of explanatory, generative, and null models.</p><p>In practice, “generative models” in the literature often have features that allow the sampling of data relatively easily (without need for computationally intensive methods). In some cases, these features may reflect existing knowledge. In other cases, they do not reflect existing knowledge and therefore reflect sampling bias. Separately, “null models” in the literature often lack existing knowledge and, as a consequence, are easy to reject. The exclusion of existing knowledge from these models underpins the problem of circular analyses of knowledge.</p></sec><sec sec-type="appendix" id="s23"><title>Tests against benchmark models favor old knowledge</title><p><italic>Objection.</italic> Tests against benchmark models prevent interesting new discoveries. In an extreme case, a parsimonious benchmark model that perfectly explains some phenomenon of interest will be very hard to reject. The inability to reject this model will stifle progress.</p><p><italic>Clarification.</italic> Tests against benchmark models help to prevent false new discoveries. This is not a weakness but a strength of these tests. Greater and greater knowledge can make it harder and harder to make new discoveries (because a previously made discovery cannot be new again). The difficulty of rejecting stronger and stronger benchmark models merely formalizes this process. The inability to reject benchmark models can still be impactful, however, as null results that may facilitate future discoveries (see the main text for additional discussion).</p></sec><sec sec-type="appendix" id="s24"><title>Tests against benchmark models are hard</title><p><italic>Objection.</italic> Tests against benchmark models require the sampling of data from complex benchmark-model distributions. Such sampling is often slow and sometimes intractable.</p><p><italic>Clarification.</italic> The slowness of data sampling needs to be placed in context. Many current tests against strawman models take negligible time, especially relative to data acquisition or analysis. The commonness of speculative and redundant explanations suggests that this negligible allocation of time is unjustified. Tests against benchmark models, like other controlled experiments (<xref ref-type="box" rid="box2">Box 2</xref>), provide rigorous evidence of new discovery. The slowness of such tests is generally compensated by the strength of this evidence.</p><p>In some cases, sampling from benchmark-model distributions may be simply intractable. In such cases, it may still be possible to rigorously test speculative models in other ways. For example:</p><list list-type="order"><list-item><p>We could test if speculative models can make specific and surprising predictions. This approach may allow us to severely test individual models but does not necessarily offer viable alternative models or test explanatory model success (<xref ref-type="table" rid="table3">Table 3</xref>, Appendix 1–2). In addition, the ability to devise surprising and testable predictions may be nontrivial for complex neuroscience phenomena.</p></list-item><list-item><p>We could estimate maximum likelihoods and quantify the trade-offs between complexity (number of parameters) and agreement with data (likelihood) of competing models (<xref ref-type="bibr" rid="bib2">Aho et al., 2014</xref>). This approach may allow us to select between models but does not necessarily offer severe tests if all the competing models are strawmen (Appendix 2). In addition, the estimation of maximum likelihoods may often be nontrivial, especially for models of large datasets.</p></list-item><list-item><p>We could bypass numerical tests by showing analytical equivalences of outwardly distinct model features. The approximate equivalence between node connectivity and average controllability is one example of such an analytical equivalence. Analytical equivalences can be elegant and instructive, but their discovery is generally idiosyncratic and is often intractable, especially for complex or highly nonlinear models.</p></list-item></list><p>In many cases, we lack the ability to rigorously test interesting models. In these cases, we need to acknowledge that such untestable models — no matter how elegant, intuitive, or appealing — ultimately remain speculative.</p></sec></app></app-group></back></article>