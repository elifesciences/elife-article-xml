<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">71601</article-id><article-id pub-id-type="doi">10.7554/eLife.71601</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Cancer Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Investigating the replicability of preclinical cancer biology</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-16276"><name><surname>Errington</surname><given-names>Timothy M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4959-5143</contrib-id><email>tim@cos.io</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-90485"><name><surname>Mathur</surname><given-names>Maya</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6698-2607</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-244994"><name><surname>Soderberg</surname><given-names>Courtney K</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" id="author-244996"><name><surname>Denis</surname><given-names>Alexandria</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-252111"><name><surname>Perfito</surname><given-names>Nicole</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf4"/></contrib><contrib contrib-type="author" id="author-16376"><name><surname>Iorns</surname><given-names>Elizabeth</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5515-1258</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf5"/></contrib><contrib contrib-type="author" id="author-244999"><name><surname>Nosek</surname><given-names>Brian A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6797-5476</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf6"/></contrib><aff id="aff1"><label>1</label><institution>Center for Open Science</institution><addr-line><named-content content-type="city">Charlottesville</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Quantitative Sciences Unit, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Science Exchange</institution><addr-line><named-content content-type="city">Palo Alto</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>University of Virginia</institution><addr-line><named-content content-type="city">Charlottesville</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Pasqualini</surname><given-names>Renata</given-names></name><role>Reviewing Editor</role><aff><institution>Rutgers University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Franco</surname><given-names>Eduardo</given-names></name><role>Senior Editor</role><aff><institution>McGill University</institution><country>Canada</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Fordham University School of Law, New York, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Rarebase, Palo Alto, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>12</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e71601</elocation-id><history><date date-type="received" iso-8601-date="2021-06-24"><day>24</day><month>06</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-10-16"><day>16</day><month>10</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Errington et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Errington et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-71601-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-71601-figures-v3.pdf"/><abstract><p>Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">Reproducibility Project: Cancer Biology</ext-link> was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85% smaller than the median effect size in the original experiments, and 92% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Reproducibility Project: Cancer Biology</kwd><kwd>replication</kwd><kwd>reproducibility</kwd><kwd>meta-analysis</kwd><kwd>transparency</kwd><kwd>reproducibility in cancer biology</kwd><kwd>credibility</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014848</institution-id><institution>Arnold Ventures</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Errington</surname><given-names>Timothy M</given-names></name><name><surname>Nosek</surname><given-names>Brian A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A project to repeat experiments from high-impact papers in cancer biology found that the effects observed in replications were frequently weaker than, or inconsistent with, the effects reported in the original papers.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Science is a system for accumulating knowledge. Independent researchers and teams study topics and make claims about nature based on the evidence that they gather. They also share their work so that others can evaluate, extend, or challenge the evidence and claims. The accumulation of evidence weeds out claims that are unreliable, and supports the development of new models and theories. Over time, uncertainty declines and the accumulated knowledge provides useful descriptions, effective predictions, and a better understanding of nature. Humanity applies this system in service of creating knowledge, treating disease, and advancing society.</p><p>An important feature of this system is replication (<xref ref-type="bibr" rid="bib31">Hempel, 1968</xref>; <xref ref-type="bibr" rid="bib52">Musgrave, 1970</xref>; <xref ref-type="bibr" rid="bib57">Nosek and Errington, 2020a</xref>; <xref ref-type="bibr" rid="bib70">Salmon and Glymour, 1999</xref>). A scientific claim is said to be replicable if it is supported by new data. However, it is often not straightforward to decide if a claim is supported by new data or not. Moreover, the success or failure of an attempt to replicate rarely provides a definitive answer about the credibility of an original claim. When the replication attempt is successful, confidence in the reliability of the claim increases, but that does not mean that the claim is valid: a finding can be both replicable and invalid at the same time. Repeated successful replications can help to eliminate alternative explanations and potential confounding influences, and therefore increase confidence in both reliability and validity, but they might not eliminate all confounding influences. It is possible that the original experiment and all the replication attempts could be invalidated by a common shortcoming in experimental design.</p><p>When a replication attempt is not successful, it is possible that the original was a false positive – noise mistaken as a signal. It is possible that the original claim was overly generalized and is only replicable under a much narrower range of conditions than was originally believed. It is also possible that the methodology necessary to produce the evidence is not sufficiently defined or understood, or that the theoretical explanation for why the finding occurred is incorrect. Failures in implementing experimental protocols may also result in replication attempts being uninformative.</p><p>All of these possibilities are ordinary and can occur when researchers have been rigorous in their work. What should not be ordinary is persistent failure to recognize that some scientific claims are not replicable. Science advances via self-correction, the progressive identification and elimination of error. Self-correction requires a healthy verification process that recognizes non-replicability, eliminates unproductive paths, and redirects attention and resources to promising directions. A failure to recognize that some claims are not replicable can foster overconfidence, underestimate uncertainty, and hinder scientific progress.</p><p>There is accumulating evidence that non-replicability may be occurring at higher rates than recognized, potentially undermining credibility and self-correction. Theoretical analyses point to a system of incentives that prioritizes innovation over verification, leading to infrequent efforts to replicate findings and to behaviors that could reduce the replicability of published findings such as selective reporting, presenting exploratory findings as confirmatory tests, and failures of documentation, transparency, and sharing (<xref ref-type="bibr" rid="bib12">Casadevall and Fang, 2012</xref>; <xref ref-type="bibr" rid="bib25">Gelman and Loken, 2013</xref>; <xref ref-type="bibr" rid="bib27">Greenwald, 1975</xref>; <xref ref-type="bibr" rid="bib37">Kimmelman et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Makel et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Nosek et al., 2012</xref>; <xref ref-type="bibr" rid="bib69">Rosenthal, 1979</xref>). For instance, one theoretical analysis estimated that more than half of research findings are false (<xref ref-type="bibr" rid="bib33">Ioannidis, 2005</xref>). And in a survey, 60% of biologists who responded reported that they had failed to replicate their own results, and more than 75% had failed to replicate results from a different lab (<xref ref-type="bibr" rid="bib4">Baker, 2016</xref>).</p><p>Large-scale replication studies in the social and behavioral sciences provide evidence of replicability challenges (<xref ref-type="bibr" rid="bib10">Camerer et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Camerer et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Ebersole et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Ebersole et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Klein et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Open Science Collaboration, 2015</xref>). In psychology, across 307 systematic replications and multisite replications, 64% reported statistically significant evidence in the same direction and effect sizes 68% as large as the original experiments (<xref ref-type="bibr" rid="bib59">Nosek et al., 2021</xref>).</p><p>In the biomedical sciences, the ALS Therapy Development Institute observed no effectiveness of more than 100 potential drugs in a mouse model in which prior research reported effectiveness in slowing down disease, and eight of those compounds were tried and failed in clinical trials costing millions and involving thousands of participants (<xref ref-type="bibr" rid="bib64">Perrin, 2014</xref>). Of 12 replications of preclinical spinal cord injury research in the FORE-SCI program, only two clearly replicated the original findings – one under constrained conditions of the injury and the other much more weakly than the original (<xref ref-type="bibr" rid="bib77">Steward et al., 2012</xref>). And, in cancer biology and related fields, two drug companies (Bayer and Amgen) reported failures to replicate findings from promising studies that could have led to new therapies (<xref ref-type="bibr" rid="bib65">Prinz et al., 2011</xref>; <xref ref-type="bibr" rid="bib6">Begley and Ellis, 2012</xref>). Their success rates (25% for the Bayer report, and 11% for the Amgen report) provided disquieting initial evidence that preclinical research may be much less replicable than recognized. Unfortunately, because of proprietary concerns, very little information was made available on the studies that failed to replicate, on the replication methodology, or on the particular barriers encountered for replicating the findings. This lack of transparency makes it difficult to ascertain the reasons for failures to replicate and critique the basis of the claims.</p><p>In the <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">Reproducibility Project: Cancer Biology,</ext-link> we sought to acquire evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from 53 high-impact papers published in 2010, 2011, and 2012 (<xref ref-type="bibr" rid="bib21">Errington et al., 2014</xref>). We describe in a companion paper (<xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>) the challenges we encountered while repeating these experiments. These barriers include: shortcomings in documentation of the original methodology; failures of transparency in original findings and protocols; failures to share original data, reagents, and other materials; methodological challenges encountered during the execution of the replication experiments. These challenges meant that we only completed 50 of the 193 experiments (26%) we planned to repeat. The 50 experiments that we were able to complete included a total of 158 effects that could be compared with the same effects in the original paper. It was common for experiments to have multiple effects, such as assessing whether an intervention affected both tumor burden and overall survival, or assessing the impact that depleting different genes has on cellular proliferation.</p><p>In this paper, we report the results of a meta-analysis of all these comparisons. There is no single method for assessing the success or failure of replication attempts (<xref ref-type="bibr" rid="bib48">Mathur and VanderWeele, 2019</xref>; <xref ref-type="bibr" rid="bib61">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="bib80">Valentine et al., 2011</xref>), so we used seven different methods to compare the effect reported in the original paper and the effect observed in the replication attempt (see Results). Six of these methods were dichotomous (i.e., replication success/failure) and one was not.</p><p>In total, 136 of the 158 effects (86%) reported in the original papers were positive effects – the original authors interpreted their data as showing that a relationship between variables existed or that an intervention had an impact on the biological system being studied. The other 22 (14%) were null effects – the original authors interpreted their data as not showing evidence for a meaningful relationship or impact of an intervention. Furthermore, 117 of the effects reported in the original papers (74%) were supported by a numerical result (such as graphs of quantified data or statistical tests), and 41 (26%) were supported by a representative image or similar. For effects where the original paper reported a numerical result for a positive effect, it was possible to use all seven methods of comparison. However, for cases where the original paper relied on a representative image (without a numerical result) as evidence for a positive effect, or when the original paper reported a null effect, it was not possible to use all seven methods.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this section we discuss the seven different methods that we used to assess replication attempts, and report what we found when we used these methods to compare the effects reported in the original papers and the effects observed in the replications. The results are reported in <xref ref-type="table" rid="table1">Table 1</xref>. We display the results of original positive effects and original null effects separately; we also display cases where the original effect was reported as a numerical value separate from cases where the original effect was reported as a representative image. In some cases we conducted two or more internal replication experiments for the same original effect, which increased the total number of outcomes from 158 to 188 (see Materials and methods). In the text of this article we mostly report and discuss our results in terms of effects, the relevant tables and figures report the results by outcome, effect, experiment, and paper.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Replication rates according to seven criteria.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Papers</th><th align="left" valign="bottom">Experiments</th><th align="left" valign="bottom">Effects</th><th align="left" valign="bottom">All outcomes</th></tr></thead><tbody><tr><td align="left" valign="bottom">Total number</td><td align="left" valign="bottom">23</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">158</td><td align="left" valign="bottom">188</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL POSITIVE RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold><italic>Numerical results</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">17 of 19 (89%)</td><td align="left" valign="bottom">26 of 35 (74%)</td><td align="left" valign="bottom">80 of 101 (79%)</td><td align="left" valign="bottom">95 of 116 (82%)</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">8 of 19 (42%)</td><td align="left" valign="bottom">17 of 33 (52%)</td><td align="left" valign="bottom">42 of 97 (43%)</td><td align="left" valign="bottom">44 of 112 (39%)</td></tr><tr><td align="left" valign="bottom">Original ES in replication CI</td><td align="left" valign="bottom">5 of 19 (26%)</td><td align="left" valign="bottom">3 of 33 (9%)</td><td align="left" valign="bottom">17 of 97 (18%)</td><td align="left" valign="bottom">26 of 112 (23%)</td></tr><tr><td align="left" valign="bottom">Replication ES in original CI</td><td align="left" valign="bottom">5 of 19 (26%)</td><td align="left" valign="bottom">11 of 33 (33%)</td><td align="left" valign="bottom">42 of 97 (43%)</td><td align="left" valign="bottom">50 of 112 (45%)</td></tr><tr><td align="left" valign="bottom">Replication ES in PI (p<sub>orig</sub>)</td><td align="left" valign="bottom">6 of 19 (32%)</td><td align="left" valign="bottom">13 of 33 (39%)</td><td align="left" valign="bottom">56 of 97 (58%)</td><td align="left" valign="bottom">67 of 112 (60%)</td></tr><tr><td align="left" valign="bottom">Replication ES≥ original ES</td><td align="left" valign="bottom">1 of 19 (5%)</td><td align="left" valign="bottom">1 of 33 (3%)</td><td align="left" valign="bottom">3 of 97 (3%)</td><td align="left" valign="bottom">3 of 112 (3%)</td></tr><tr><td align="left" valign="bottom">Meta-analysis (p &lt; 0.05)</td><td align="left" valign="bottom">15 of 19 (79%)</td><td align="left" valign="bottom">26 of 33 (79%)</td><td align="left" valign="bottom">60 of 97 (62%)</td><td align="left" valign="bottom">75 of 112 (67%)</td></tr><tr><td align="left" valign="bottom"><bold><italic>Representative images</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">9 of 10 (90%)</td><td align="left" valign="bottom">12 of 16 (75%)</td><td align="left" valign="bottom">28 of 35 (80%)</td><td align="left" valign="bottom">34 of 45 (76%)</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">3 of 8 (40%)</td><td align="left" valign="bottom">7 of 12 (58%)</td><td align="left" valign="bottom">14 of 22 (64%)</td><td align="left" valign="bottom">14 of 22 (64%)</td></tr><tr><td align="left" valign="bottom">Original image in replication CI</td><td align="left" valign="bottom">5 of 7 (71%)</td><td align="left" valign="bottom">3 of 11 (27%)</td><td align="left" valign="bottom">10 of 21 (48%)</td><td align="left" valign="bottom">10 of 21 (48%)</td></tr><tr><td align="left" valign="bottom">Replication effect ≥ original image</td><td align="left" valign="bottom">3 of 7 (43%)</td><td align="left" valign="bottom">5 of 11 (45%)</td><td align="left" valign="bottom">7 of 21 (33%)</td><td align="left" valign="bottom">7 of 21 (33%)</td></tr><tr><td align="left" valign="bottom"><bold><italic>Sample sizes</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Median [IQR] of original</td><td align="left" valign="bottom">46.0 [20.0–100]</td><td align="left" valign="bottom">20.0 [8.5–48.0]</td><td align="left" valign="bottom">8.0 [6.0–13.0]</td><td align="left" valign="bottom">8.0 [6.0–18.0]</td></tr><tr><td align="left" valign="bottom">Median [IQR] of replication</td><td align="left" valign="bottom">50.0 [28.0–128]</td><td align="left" valign="bottom">24.0 [11.5–50.0]</td><td align="left" valign="bottom">12.0 [8.0–22.2]</td><td align="left" valign="bottom">12.0 [8.0–18.0]</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL NULL RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold><italic>Numerical results</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">9 of 11 (82%)</td><td align="left" valign="bottom">10 of 12 (83%)</td><td align="left" valign="bottom">11 of 15 (73%)</td><td align="left" valign="bottom">10 of 20 (50%)</td></tr><tr><td align="left" valign="bottom">Original ES in replication CI</td><td align="left" valign="bottom">8 of 11 (73%)</td><td align="left" valign="bottom">9 of 12 (75%)</td><td align="left" valign="bottom">11 of 15 (73%)</td><td align="left" valign="bottom">12 of 20 (60%)</td></tr><tr><td align="left" valign="bottom">Replication ES in original CI</td><td align="left" valign="bottom">9 of 11 (82%)</td><td align="left" valign="bottom">10 of 12 (83%)</td><td align="left" valign="bottom">12 of 15 (80%)</td><td align="left" valign="bottom">13 of 20 (65%)</td></tr><tr><td align="left" valign="bottom">Replication ES in PI (p<sub>orig</sub>)</td><td align="left" valign="bottom">9 of 11 (82%)</td><td align="left" valign="bottom">10 of 12 (83%)</td><td align="left" valign="bottom">12 of 15 (80%)</td><td align="left" valign="bottom">14 of 20 (70%)</td></tr><tr><td align="left" valign="bottom">Replication ES ≤ original ES</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Meta-analysis (p &gt; 0.05)</td><td align="left" valign="bottom">8 of 11 (73%)</td><td align="left" valign="bottom">10 of 12 (83%)</td><td align="left" valign="bottom">10 of 15 (67%)</td><td align="left" valign="bottom">11 of 20 (55%)</td></tr><tr><td align="left" valign="bottom"><bold><italic>Representative images</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">3 of 3 (100%)</td><td align="left" valign="bottom">3 of 3 (100%)</td><td align="left" valign="bottom">4 of 5 (80%)</td><td align="left" valign="bottom">4 of 5 (80%)</td></tr><tr><td align="left" valign="bottom">Original image in replication CI</td><td align="left" valign="bottom">1 of 3 (33%)</td><td align="left" valign="bottom">1 of 3 (33%)</td><td align="left" valign="bottom">3 of 5 (60%)</td><td align="left" valign="bottom">3 of 5 (60%)</td></tr><tr><td align="left" valign="bottom">Replication effect ≤ original image</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom"><bold><italic>Sample sizes</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Median [IQR] of original</td><td align="left" valign="bottom">16.0 [8.0–25.0]</td><td align="left" valign="bottom">12.0 [6.0–20.0]</td><td align="left" valign="bottom">15.0 [7.5–31.0]</td><td align="left" valign="bottom">18.0 [8.0–514]</td></tr><tr><td align="left" valign="bottom">Median [IQR] of replication</td><td align="left" valign="bottom">24.0 [16.0–69.0]</td><td align="left" valign="bottom">21.0 [8.0–54.0]</td><td align="left" valign="bottom">27.0 [8.0–66.8]</td><td align="left" valign="bottom">24.0 [16.0–573]</td></tr></tbody></table><table-wrap-foot><fn><p>Summary of consistency between original and replication findings for original positive results (top) and null results (bottom), and by treating internal replications individually (all outcomes; column 5) and aggregated by effects (column 4), experiments (column 3), and papers (column 2). All findings coded in terms of consistency with original findings. If original results were null, then a positive result is counted as inconsistent with the original finding. For statistical significance, if original results were interpreted as a positive result but were not statistically significant at p &lt; 0.05, then they were treated as a positive result (seven effects); likewise, if they were interpreted as a null result but were statistically significant at p &lt; 0.05, they were treated as a null result (two effects). For original positive results, replications were deemed successful if they were statistically significant and in the same direction as the original finding; for original null results, replications were deemed successful if they were not statistically significant, regardless of direction. The ‘same direction’ criterion is not applicable for original null results because ‘null’ is an interpretation in null hypothesis significance testing and most null results still have a direction (as the effect size is almost always non-zero). Likewise, comparing direction of effect sizes is not meaningful for original null results if their variation was interpreted as noise. Mean differences were estimated from the image for original effects based on representative images. Original positive and null effects were kept separate when aggregating into experiments and papers. That is, if a single experiment had both positive and null effects, then the positive effects are summarized in ‘original positive results’ and the null outcomes are summarized in ‘original null results’. Very similar results are obtained when alternative strategies are used to aggregate the data (see Tables S1–S3 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Standardized mean difference (SMD) effect sizes are reported. CI = 95% confidence interval; PI = 95% prediction interval; ES = effect size; IQR = interquartile range.</p></fn></table-wrap-foot></table-wrap><p>The nested structure of outcomes within effects, effects within experiments, and experiments within papers provides different ways to characterize the results, and it is possible for some effects within an experiment to replicate successfully while other effects in the same experiment fail to replicate. However, the results are similar irrespective of whether we look at them by paper (23 in total), by experiment (50 in total), by effect (158 in total), or by outcome (188 in total). We also use a number of strategies for aggregating data across effects and experiments, but observe very similar findings regardless of method used for aggregation (see Tables S1–S3 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><sec id="s2-1"><title>Evaluating replications with the ‘same direction’ criterion</title><p>According to our first criterion, a replication attempt is successful if the original effect and the replication effect are in the same direction. This is inclusive of original effects that are reported as a representative image without numerical values.</p><p>Among the 136 effects that were reported as being positive in the original experiments, 108 (79%) were likewise in the positive direction in the replications (<xref ref-type="table" rid="table1">Table 1</xref>). Moreover, the replication rate for the 101 cases where the original effect was based on a numerical result (80%) and the 35 cases where the original effect was based on a representative image (79%) were essentially the same. A weakness of the ‘same direction’ criterion is that it is a ‘low bar’ for determining replication success. If there were no true effects and all original and replication experiments were just investigating noise, the direction of original and replication effects would be random, and we would expect a 50% replication success rate. That makes 50% the lowest expected success rate with this criterion. Also, some findings have only a single direction – either the phenomenon is absent or present. As such, any detection of an effect would be labeled a success no matter the magnitude.</p><p>The other 22 cases in the original experiments were reported as null effects. However, because of random error, few truly null effects are nil, literally zero, meaning that they have a direction. But, this means that there is no obvious interpretation for success or failure on the ‘same direction’ criterion for original null effects. Given this, we do not use the ‘same direction’ criterion to assess replications of original null effects.</p></sec><sec id="s2-2"><title>Evaluating replications against a null hypothesis</title><p>Null hypothesis significance testing is used to test for evidence that an observed effect size or something larger would have been unlikely to occur under the null hypothesis. For positive original results, it is straightforward to assess whether a replication effect observes a statistically significant result in the same direction as the original effect. The simplicity of this indicator has led to it being a common replication criterion despite its dichotomous nature, its dependence on the power of the replication experiment, and challenges for proper interpretation (<xref ref-type="bibr" rid="bib3">Andrews and Kasy, 2019</xref>; <xref ref-type="bibr" rid="bib10">Camerer et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Camerer et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="bib62">Patil et al., 2016</xref>; <xref ref-type="bibr" rid="bib80">Valentine et al., 2011</xref>). Of the 112 original effects with associated statistical significance tests, 97 were interpreted as positive effects, and 15 were interpreted as null effects (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>p-value density plots for original and replication results.</title><p>p-alue density plots for original and replication results treating internal replications individually (top row), and aggregated by effects (second row), experiments (third row), and papers (fourth row). Left column presents all data for which p-values could be calculated for both original and replication results; the other two columns present data for when the original result was interpreted as positive (middle column) or as a null result (right column). Some original effects (n = 7) were interpreted as positive results with p-values &gt; 0.05, and some original effects (n = 2) were interpreted as null results with p-values &lt; 0.05. Replication p-values ignore whether the result was in the same or opposite direction as the original result (n = 7 effects had p-values &lt; 0.05 in the opposite direction as the original effect).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig1-v3.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>p-value distributions for original and replication effects.</title><p>Cumulative distribution functions (CDF; left) and probability distribution functions (PDF; right) for p-values for the 112 effects for which the original and replications had an associated statistical significance test. The vertical dashed line indicates p = 0.05. The difference between the means of the two p-value distributions (0.064 for the original effects; 0.259 for the replications) was significant: paired t-test: t(111) = –6.14, p = 1.33 × 10<sup>–8</sup>; Wilcoxon rank sum test: W = 3358, p = 1.88 × 10<sup>–9</sup>. Quantities are 0.00034, 0.0048, 0.0198 for the original effects, and 0.0075, 0.0757, 0.528 for the replications.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig1-figsupp1-v3.tif"/></fig></fig-group><p>For original positive effects, 42 of the 97 (43%) replication effects were statistically significant and in the same direction as the original effect; 48 (49%) were null results; and 7 (7%) were statistically significant in the opposite direction. Based on the power of the experiments, if the replications were all statistically consistent with the original experiments, we would expect approximately 87% of replications to be statistically significant and positive (<xref ref-type="bibr" rid="bib50">Mathur and VanderWeele, 2020b</xref>), which is considerably higher than what we observed (43% [95% CI: 25%, 62%]). A sensitivity analysis that approximately accounts for possible heterogeneity within pairs also yielded a value (85%) that was considerably higher than what was observed.</p><p>For the original null effects, 11 (73%) replication effects were null results and 4 (27%) were statistically significant. Combining positive effects that remained positive and null effects that remained null, 53 of 112 (47%) of the replications were consistent with the original effects.</p><p>For cases in which the original findings were reported as representative images, we were able to conduct statistical significance tests for the replications: of the 22 effects that were positive in the original experiments, 14 (64%) replications were statistically significant in the same direction. And of the five null effects in the original experiments, 4 replications were also null (<xref ref-type="table" rid="table1">Table 1</xref>).</p><p>A weakness of this approach to assessing replication results is that it treats p = 0.05 as a bright-line criterion between replication success and failure. For example, if an excess of findings fell just above p = 0.05 it could indicate false negatives are present in the non-statistically significant outcomes of original positive results. p-values for non-statistically significant replication effects were widely distributed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), and do not statistically differ from the approximately uniform distribution that would be expected if all were true null results whether examining the findings that had p-values for both original and replication effects (Fisher’s exact test: χ<sup>2</sup>(118) = 135.7, p = 0.127), or also including the replication effects for which the original effects were based on a representative image (χ<sup>2</sup>(138) = 155.1, p = 0.152). Therefore, we cannot reject the hypothesis that the observed null effects come from a population of true negatives.</p></sec><sec id="s2-3"><title>Comparing original effect size with the 95% confidence interval of the replication effect size</title><p>Another approach based on the logic of null hypothesis significance testing is to assess whether the original effect size is contained within the 95% confidence interval of the replication effect size. In this approach the null hypothesis is the original effect size, and we are testing if the replication effect size is significantly different. We found that 17 of the 97 (18%) original positive effect sizes were in the 95% confidence interval of the replication effect size, as were 11 of the 15 (73%) original null effect sizes. Therefore, according to this criterion, 75% of original effect sizes were inconsistent with the replications, even if both observed an effect in the same direction. Note that the precision of the replication estimate is influenced by the sample size, which was larger in the replications than in the original experiments for both positive effects (12.0 vs. 8.9 for the median) and null effects (27.0 vs. 15.0).</p><p>This criterion can also be applied to cases in which the original experiment reported only a representative image with an effect size that could be estimated from that image. Of these, 10 of the 21 (48%) original positive effect sizes, and 3 of the 5 (60%) original null effect sizes, were in the 95% confidence interval of the replication effect size, meaning that half of the original effect sizes were in the confidence interval of the replication effect sizes, and half of the original images were inconsistent with the replications. Combining all numerical and image-only data, 41 of 138 (30%) replications were consistent with original effects on this criterion.</p></sec><sec id="s2-4"><title>Comparing the replication effect size with the 95% confidence interval of the original effect size</title><p>A complementary criterion is to assess whether the replication effect size falls within the 95% confidence interval of the original effect size. When the original effect was positive, 42 of the 97 (43%) replication effect sizes were within the 95% confidence interval of original effect size; and when the original effect was null, 12 of the 15 (80%) replication effect sizes were within the 95% confidence interval of the original effect sizes. This success rate is low but it is almost double the rate reported for a seemingly similar approach in the previous section. This is attributable to the smaller sample sizes in the original experiments leading to wider confidence intervals, thus making it ‘easier’ for the replication to achieve an effect size that counts as a success.</p><p>A more complete picture of the consistency between the original findings and the replications in the null hypothesis significance testing framework can be obtained by combining the three criteria we have just discussed. Were the original results and the replications consistent on zero, one, two, or all three of these criteria? For the 97 effects that were positive in the original experiments, we find that just 13 were successful on all three criteria, 18 were successful on two, 26 were successful on one, and 40 failed to replicate on all three criteria. For the 15 effects that were null in the original experiments, eight were successful on all three criteria, four were successful on two, two were successful on one, and only one failed on all three criteria (see <xref ref-type="table" rid="table2">Table 2</xref> and Tables S4–S6 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Replication rates according to three criteria involving null hypothesis significance testing.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" colspan="2" valign="bottom">Papers</th><th align="left" colspan="2" valign="bottom">Experiments</th><th align="left" colspan="2" valign="bottom">Effects</th><th align="left" colspan="2" valign="bottom">All outcomes</th></tr></thead><tbody><tr><td align="left" valign="bottom">Total number</td><td align="left" colspan="2" valign="bottom">23</td><td align="left" colspan="2" valign="bottom">50</td><td align="left" colspan="2" valign="bottom">158</td><td align="left" colspan="2" valign="bottom">188</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL POSITIVE RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Succeeded on all three criteria</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">6%</td><td align="left" valign="bottom">13</td><td align="left" valign="bottom">13%</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">18%</td></tr><tr><td align="left" valign="bottom">[1]Failed only on significance and direction</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">3%</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">4%</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">5%</td></tr><tr><td align="left" valign="bottom">[2]Failed only on original in replication confidence interval</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">5%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">14%</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">9%</td></tr><tr><td align="left" valign="bottom">[3]Failed only on replication in original confidence interval</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td></tr><tr><td align="left" valign="bottom">Failed only on [1] and [2]</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">9%</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">13%</td></tr><tr><td align="left" valign="bottom">Failed only on [2] and [3]</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">26%</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">30%</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">13%</td></tr><tr><td align="left" valign="bottom">Failed only on [1] and [3]</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">5%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td></tr><tr><td align="left" valign="bottom">Failed on all three criteria [1], [2], and [3]</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">42%</td><td align="left" valign="bottom">12</td><td align="left" valign="bottom">36%</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">41%</td><td align="left" valign="bottom">48</td><td align="left" valign="bottom">43%</td></tr><tr><td align="left" valign="bottom">Total</td><td align="left" valign="bottom">19</td><td align="left" valign="bottom"/><td align="left" valign="bottom">33</td><td align="left" valign="bottom"/><td align="left" valign="bottom">97</td><td align="left" valign="bottom"/><td align="left" valign="bottom">112</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL NULL RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Succeeded on all three criteria</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">55%</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">58%</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">53%</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">35%</td></tr><tr><td align="left" valign="bottom">[1]Failed only on significance and direction</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">18%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">17%</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">20%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">25%</td></tr><tr><td align="left" valign="bottom">[2]Failed only on original in replication confidence interval</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">9%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">8%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">7%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">5%</td></tr><tr><td align="left" valign="bottom">[3]Failed only on replication in original confidence interval</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td></tr><tr><td align="left" valign="bottom">Failed only on [1] and [2]</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td></tr><tr><td align="left" valign="bottom">Failed only on [2] and [3]</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">18%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">17%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">13%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">10%</td></tr><tr><td align="left" valign="bottom">Failed only on [1] and [3]</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td></tr><tr><td align="left" valign="bottom">Failed on all three criteria [1], [2], and [3]</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">7%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">25%</td></tr><tr><td align="left" valign="bottom">Total</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom"/><td align="left" valign="bottom">12</td><td align="left" valign="bottom"/><td align="left" valign="bottom">15</td><td align="left" valign="bottom"/><td align="left" valign="bottom">20</td><td align="left" valign="bottom"/></tr></tbody></table><table-wrap-foot><fn><p>Number of replications that succeeded or failed to replicate results in original experiments according to three criteria within the null hypothesis significance testing framework: statistical significance (p &lt; 0.05) and same direction; original effect size inside 95% confidence interval of replication effect size using standardized mean difference (SMD) effect sizes; replication effect size inside 95% confidence interval of original effect size using SMD effect sizes. Data for original positive results and original null results are shown separately, as are data for all outcomes and aggregated by effect, experiment, and paper. Very similar results are obtained when alternative strategies are used to aggregate the data (see Tables S4–S6 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-5"><title>Comparing the replication effect size with the 95% prediction interval of the original effect size</title><p>A 95% prediction interval is the range of values inside which a future observation will fall with 95% probability, given what has been observed already. Prediction intervals are sometimes preferred over confidence intervals when presenting new results because they do not assume that the future observation has infinite sample size. As a consequence, they more appropriately represent the (greater) uncertainty around the future estimate.</p><p>However, as a criterion for evaluating replication success or failure, prediction intervals are more liberal than criteria based on confidence intervals. If, for example, the original finding was close to p = 0.05, then the prediction interval will often overlap with zero. If the true effect size is near zero, a replication might never provide evidence inconsistent with the prediction interval unless random error leads to the effect size being estimated in the opposite direction of the original finding. In other words, somewhat ironically, the more uncertain an original finding, the harder it is for a replication to provide disconfirming evidence. Nevertheless, the prediction interval has been used in at least one case to estimate replication success (<xref ref-type="bibr" rid="bib62">Patil et al., 2016</xref>).</p><p>For the 97 effects that were positive in the original experiments, 56 effects (58%, 95% CI: [44%, 72%]) could be considered successful according to this criterion (<xref ref-type="table" rid="table1">Table 1</xref>). A sensitivity analysis that approximately accounts for possible heterogeneity within pairs yields a higher value (65%, 95% CI: [51%, 79%]). And for the 15 effects that were null in the original experiments, 12 effects (80%) could be considered successful. Combining these results, 68 of 112 (61%) replications were successful according to prediction interval criterion.</p><p>Related to prediction intervals, the degree of statistical inconsistency between each replication and the corresponding original effect can be represented with a metric called p<sub>orig</sub>, which is a p-value for the hypothesis that the original and the replication had the same population effect size (<xref ref-type="bibr" rid="bib50">Mathur and VanderWeele, 2020b</xref>). p<sub>orig</sub> thus assesses whether each replication effect was similar to the corresponding original effect, with small values of p<sub>orig</sub> indicating less similarity and larger values indicating more similarity. For original positive effects, the median p<sub>orig</sub> was 0.064, suggesting some evidence for inconsistency on average. Of the 97 original positive effects, 42% (95% CI: [28%, 56%]) had p<sub>orig</sub> &lt; 0.05, and 14% (95% CI: [5%, 24%]) had p<sub>orig</sub> &lt; 0.005 (<xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>).</p><p>We then aggregated the values of p<sub>orig</sub> using the harmonic mean p-value to test the global null hypothesis that none of the pairs were statistically inconsistent (<xref ref-type="bibr" rid="bib85">Wilson, 2019</xref>), yielding an aggregated p<sub>orig</sub> of 0.0005, which is strong evidence of some inconsistency in the tested pairs. The aggregated value of p<sub>orig</sub> accommodates correlations among p-values due to nested data. In a sensitivity analysis that additionally accounted for possible effect heterogeneity within each of the 97 original-replication pairs, the median p<sub>orig</sub> was 0.087: 35% of effects (95% CI: [21%, 49%]) had p<sub>orig</sub> &lt; 0.05, and 12% of effects (95% CI: [2%, 23%]) had p<sub>orig</sub> &lt; 0.005.</p></sec><sec id="s2-6"><title>Comparing effect sizes in the original experiments and the replications</title><p>Another way to assess replications is to compare the original effect size and the replication effect size. For the 97 effects that were positive in the original experiments, the effect size was lower in 94 (97%) of the replications (<xref ref-type="table" rid="table1">Table 1</xref>): if the original effect sizes were accurately estimated, one would expect this percentage to be about 50%, and the probability of 94 of the 97 replication effect sizes being lower than the original effect sizes would be vanishingly low (binomial test: p = 1.92 × 10<sup>–24</sup>). And for the 21 cases in which the original evidence for a positive effect was a representative image, the effect size in the replication was smaller than the effect size estimated for the original in 14 cases (67%). Combining these results, the effect sizes in the replications were smaller than the effect sizes in the original findings in 108 of 118 (92%) cases.</p><p>We also compared the mean and median values of the effect sizes for positive effects (<xref ref-type="table" rid="table3">Table 3</xref>): in both cases the value was considerably larger for the original effect. Comparing means, the value for the original effects was 6.15 (SD = 12.39, 95% CI: [1.83, 10.47]), and the value for the replications was 1.37 (SD = 3.01, 95% CI: [0.42, 2.32]). Comparing medians, the value for the original effects was 2.96 (interquartile range [IQR] = 1.71–5.70), and the value for the replications was 0.43 (IQR = 0.15–2.06).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Comparing effect sizes in the original results and the replications.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Papers</th><th align="left" valign="bottom">Experiments</th><th align="left" valign="bottom">Effects</th><th align="left" valign="bottom">All outcomes</th></tr></thead><tbody><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL POSITIVE RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Number of outcomes</td><td align="left" valign="bottom">19</td><td align="left" valign="bottom">33</td><td align="left" valign="bottom">97</td><td align="left" valign="bottom">112</td></tr><tr><td align="left" valign="bottom">Mean (SD) original experiment effect size</td><td align="left" valign="bottom">7.35 (18.77)</td><td align="left" valign="bottom">6.36 (14.62)</td><td align="left" valign="bottom">6.15 (12.39)</td><td align="left" valign="bottom">5.56 (11.63)</td></tr><tr><td align="left" valign="bottom">Median [IQR] original experiment effect size</td><td align="left" valign="bottom">2.07 [1.68–5.03]</td><td align="left" valign="bottom">2.45 [1.42–4.58]</td><td align="left" valign="bottom">2.96 [1.71–5.70]</td><td align="left" valign="bottom">2.57 [1.60–5.49]</td></tr><tr><td align="left" valign="bottom">Mean (SD) replication experiment effect size</td><td align="left" valign="bottom">1.38 (2.02)</td><td align="left" valign="bottom">1.55 (3.31)</td><td align="left" valign="bottom">1.37 (3.01)</td><td align="left" valign="bottom">1.30 (2.83)</td></tr><tr><td align="left" valign="bottom">Median [IQR] replication experiment effect size</td><td align="left" valign="bottom">0.53 [0.18–1.80]</td><td align="left" valign="bottom">0.37 [0.10–1.31]</td><td align="left" valign="bottom">0.43 [0.15–2.06]</td><td align="left" valign="bottom">0.47 [0.17–1.67]</td></tr><tr><td align="left" valign="bottom">Meta-analytic mean (SD) estimate</td><td align="left" valign="bottom">1.68 (1.81)</td><td align="left" valign="bottom">1.79 (2.90)</td><td align="left" valign="bottom">1.66 (2.47)</td><td align="left" valign="bottom">1.61 (2.32)</td></tr><tr><td align="left" valign="bottom">Meta-analytic median [IQR] estimate</td><td align="left" valign="bottom">0.98 [0.57–2.20]</td><td align="left" valign="bottom">1.00 [0.28–2.03]</td><td align="left" valign="bottom">0.92 [0.36–2.43]</td><td align="left" valign="bottom">1.05 [0.36–2.11]</td></tr><tr><td align="left" valign="bottom"><bold><italic>Sample sizes</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Median [IQR] of original</td><td align="left" valign="bottom">46.0 [20.0–100]</td><td align="left" valign="bottom">24.0 [9.0–48.0]</td><td align="left" valign="bottom">8.0 [6.0–13.0]</td><td align="left" valign="bottom">8.5 [6.0–18.0]</td></tr><tr><td align="left" valign="bottom">Median [IQR] of replication</td><td align="left" valign="bottom">50.0 [28.0–128]</td><td align="left" valign="bottom">32.0 [12.0–50.0]</td><td align="left" valign="bottom">12.0 [8.0–23.0]</td><td align="left" valign="bottom">12.0 [8.0–18.0]</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL NULL RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Number of outcomes</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">12</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">20</td></tr><tr><td align="left" valign="bottom">Mean (SD) original experiment effect size</td><td align="left" valign="bottom">0.70 (0.64)</td><td align="left" valign="bottom">0.72 (0.61)</td><td align="left" valign="bottom">0.63 (0.59)</td><td align="left" valign="bottom">0.51 (0.55)</td></tr><tr><td align="left" valign="bottom">Median [IQR] original experiment effect size</td><td align="left" valign="bottom">0.61 [0.15–1.03]</td><td align="left" valign="bottom">0.68 [0.15–1.03]</td><td align="left" valign="bottom">0.61 [0.16–0.97]</td><td align="left" valign="bottom">0.18 [0.15–0.79]</td></tr><tr><td align="left" valign="bottom">Mean (SD) replication experiment effect size</td><td align="left" valign="bottom">–0.08 (0.75)</td><td align="left" valign="bottom">–0.02 (0.74)</td><td align="left" valign="bottom">0.02 (0.69)</td><td align="left" valign="bottom">0.01 (0.86)</td></tr><tr><td align="left" valign="bottom">Median [IQR] replication experiment effect size</td><td align="left" valign="bottom">0.13 [-0.27–0.24]</td><td align="left" valign="bottom">0.13 [-0.23–0.39]</td><td align="left" valign="bottom">0.16 [-0.24–0.47]</td><td align="left" valign="bottom">0.16 [-0.21–0.39]</td></tr><tr><td align="left" valign="bottom">Meta-analytic mean (SD) estimate</td><td align="left" valign="bottom">0.20 (0.31)</td><td align="left" valign="bottom">0.25 (0.34)</td><td align="left" valign="bottom">0.24 (0.34)</td><td align="left" valign="bottom">0.20 (0.39)</td></tr><tr><td align="left" valign="bottom">Meta-analytic median [IQR] estimate</td><td align="left" valign="bottom">0.17 [0.06–0.40]</td><td align="left" valign="bottom">0.23 [0.07–0.43]</td><td align="left" valign="bottom">0.16 [0.06–0.44]</td><td align="left" valign="bottom">0.16 [0.07–0.43]</td></tr><tr><td align="left" valign="bottom"><bold><italic>Sample sizes</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Median [IQR] of original</td><td align="left" valign="bottom">16.0 [8.0–25.0]</td><td align="left" valign="bottom">12.0 [7.0–22.5]</td><td align="left" valign="bottom">18.0 [8.0–32.0]</td><td align="left" valign="bottom">19.0 [11.0–514]</td></tr><tr><td align="left" valign="bottom">Median [IQR] of replication</td><td align="left" valign="bottom">24.0 [16.0–69.0]</td><td align="left" valign="bottom">22.5 [8.0–61.5]</td><td align="left" valign="bottom">30.0 [12.0–72.5]</td><td align="left" valign="bottom">27.0 [17.5–573]</td></tr></tbody></table><table-wrap-foot><fn><p>Comparing original effect sizes and effect sizes in the replications for original positive results (top) and null results (bottom) when treating internal replications individually (all outcomes; column 5) and aggregated by effects (column 4), experiments (column 3), and papers (column 2). The mean and median of the effect sizes in the original results were considerably larger than those for the replications. SD = standard deviation; IQR = interquartile range.</p></fn></table-wrap-foot></table-wrap><p>The pattern was similar when we compared mean and median values for null effects. Comparing means, the value for the original effects was 0.63 (SD = 0.59), and the value for replications was 0.02 (SD = 0.69). Comparing medians, the value for the original effects was 0.61 (IQR = 0.16–0.97), and the value for replications was 0.16 (IQR = –0.24–0.47).</p><p>Although the original and replication effect sizes had very different effect magnitudes, larger effect sizes in the original results tended to be associated with larger effect sizes in the replication (Spearman’s r = 0.47, p = 1.83 × 10<sup>–7</sup>; <xref ref-type="fig" rid="fig2">Figure 2</xref>). This indicates that observed effect sizes are not all random, and that some findings retain their rank ordering in effect size, despite the clear differences between the original and replication effect sizes. To illustrate the comparability of these findings across different levels of aggregation, <xref ref-type="fig" rid="fig3">Figure 3</xref> presents density plots of original effect sizes compared to replication effect sizes by individual outcomes, effects, experiments, and papers.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Replication effect sizes compared with original effect sizes.</title><p>(<bold>A</bold>) Graph in which each circle represents an effect for which an SMD effect size could be computed for both the original effect and the replication (n = 110). Blue circles indicate effects for which p &lt; 0.05 in the replication, and red circles indicate p &gt; 0.05. Two effects for which the original effects size was &gt;80 are not shown. The median effect size in the replications was 85% smaller than the median effect size in the original experiments, and 97% of replication effect sizes were smaller than original effect sizes (below the gray diagonal line). (<bold>B</bold>) An expanded view of panel A for effect sizes &lt; 5 (gray outline in panel A). SMD: standardized mean difference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Replication effect sizes compared with original effect sizes for all effects (treating internal replications individually).</title><p>(<bold>A</bold>) Graph in which each circle represents an effect for which an SMD effect size could be computed for both the original effect and the replication: all effects, including internal replications, are shown (n = 130). Blue circles indicate effects for which p &lt; 0.05 in the replication, and red circles indicate p &gt; 0.05. Two effects for which the original effects size was &gt;80 are not shown. (<bold>B</bold>) An expanded view of panel A for effect sizes &lt; 5 (gray outline in panel A). SMD: standardized mean difference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig2-figsupp1-v3.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Replication effect sizes compared with original effect sizes for experiments (combining effects).</title><p>(<bold>A</bold>) Graph in which each circle represents an experiment (n = 44). The SMD effect size for each experiment was determined by meta-analytically combining positive or null effects from each unique experiment with random-effect models. Blue circles indicate experiments for which p &lt; 0.05 in the replication, and red circles indicate p &gt; 0.05. One experiment for which two original effect sizes were &gt;80 are not shown. (<bold>B</bold>) An expanded view of panel A for effect sizes &lt; 5 (gray outline in panel A). SMD: standardized mean difference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig2-figsupp2-v3.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Replication effect sizes compared with original effect sizes for papers (combining experiments).</title><p>(<bold>A</bold>) Graph in which each circle represents a paper (n = 29). The SMD effect size for each paper was determined by meta-analytically combining positive or null results from each unique experiment with random-effect models. Blue circles indicate experiments for which p &lt; 0.05 in the replication, and red circles indicate p &gt; 0.05. One paper for which two original effect sizes were &gt;80 are not shown. (<bold>B</bold>) An expanded view of panel A for effect sizes &lt; 5 (gray outline in panel A). SMD: standardized mean difference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig2-figsupp3-v3.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Effect size density plots for original and replication results.</title><p>Effect size density plots for original and replication findings for all results treating internal replications individually (top row) and aggregated by effects (second row), experiments (third row), and papers (fourth row). Left column presents all data for which SMD effect sizes could be calculated for both original and replication results; the other two columns present data for when the original result was interpreted as positive (middle column) or as a null result (right column). Effect sizes &gt; 80 (two for all outcomes and effects, and one for experiments and papers) are not shown.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Effect size distributions for original and replication effects.</title><p>Histogram (left) and cumulative distribution function (right) for SMD effect sizes for the 112 effects for which the original and replications had an associated statistical significance test. The difference between the means of the two effect size distributions (5.41 [SD = 11.7] for the original effects; 1.19 [SD = 2.85] for the replications) was significant: paired t-test: t(111) = 3.93, p = 1.48 × 10<sup>–4</sup>; Wilcoxon rank sum test: W = 9898, p = 7.68 × 10<sup>–14</sup>. Two effects for which the original effects size was &gt;80 are not shown. SMD: standardized mean difference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig3-figsupp1-v3.tif"/></fig></fig-group></sec><sec id="s2-7"><title>Combining the original and replication effect sizes</title><p>Combining the original and replication findings provides an assessment of the cumulative evidence for a phenomenon. In general, cumulative or meta-analytic evidence obtained from multiple independently conducted experiments provides a better basis for assessing the reliability of findings than evidence from a single experiment. However, the credibility of such results is contingent on a lack of selective reporting or on the ability to effectively correct for missing evidence (<xref ref-type="bibr" rid="bib51">McShane et al., 2016</xref>; <xref ref-type="bibr" rid="bib76">Stanley and Doucouliagos, 2014</xref>). If, for example, original experiments were influenced by publication bias, with null results being ignored at greater rates, then the meta-analytic evidence would be biased. The use of preregistration and complete outcome reporting in the project eliminated the possibility of publication bias in the replication experiments (<xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>), but it may be present in the original experiments. Nevertheless, we combined the two sets of results by weighting each finding by the inverse of its variance to estimate the effect size and effect precision. Using a fixed-effect model for each original-replication pair where the original result was positive, 60 of the 97 effects (62%) were statistically significant at p &lt; 0.05 (<xref ref-type="table" rid="table1">Table 1</xref>), and 39 effects (40% of the total) were statistically significant according to the stricter criterion of p &lt; 0.005. <xref ref-type="table" rid="table3">Table 3</xref> reports mean and median values for the original effect size, the replication effect size, and the meta-analytic combination of the two. According to the combined results the mean effect size was 1.66 (95% CI: [0.92, 2.41]).</p><p>For cases in which the original was a null effect, 10 of the 15 (67%) meta-analytic effects were likewise null (p &gt; 0.05), meaning that combining the data led to a third of cases showing statistically significant effects, even though the original reported a null finding. This can occur when the original experiment was underpowered to detect the true effect size, but combining data increases precision to detect smaller effects. This can be important when, for example, an experiment is evaluating whether an intervention increases toxicity. An original null result based on a small sample may provide misplaced confidence in safety. Considering all data, 70 of 112 (63%) of meta-analytic combinations showed effects consistent with the effect reported for the original experiment.</p></sec><sec id="s2-8"><title>Comparing animal vs. non-animal experiments</title><p>Animal experiments have special significance in understanding biological mechanisms and in translating basic science into potential clinical application. We explored whether the patterns of replicability differed between the animal and non-animal experiments included in this meta-analysis (<xref ref-type="table" rid="table4">Table 4</xref>). Descriptively, animal experiments with positive effects were less likely to replicate than non-animal experiments with positive effects on every replication criterion. For example, 12% of replication effects were in the same direction as the original and statistically significant for animal experiments, compared with 54% for non-animal experiments. Likewise, 44% of replication effects were in the 95% prediction interval for animal experiments, compared with 63% for non-animal experiments.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Replication rates for animal and non-animal experiments.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Animal</th><th align="left" valign="bottom">Non-animal</th><th align="left" valign="bottom">Total</th></tr></thead><tbody><tr><td align="left" valign="bottom">Total number of effects</td><td align="left" valign="bottom">36</td><td align="left" valign="bottom">122</td><td align="left" valign="bottom">158</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL POSITIVE EFFECTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold><italic>Numerical results</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">17 of 27 (63%)</td><td align="left" valign="bottom">63 of 74 (85%)</td><td align="left" valign="bottom">80 of 101 (79%)</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">3 of 25 (12%)</td><td align="left" valign="bottom">39 of 72 (54%)</td><td align="left" valign="bottom">42 of 97 (43%)</td></tr><tr><td align="left" valign="bottom">Original ES in replication CI</td><td align="left" valign="bottom">4 of 25 (16%)</td><td align="left" valign="bottom">13 of 72 (18%)</td><td align="left" valign="bottom">17 of 97 (18%)</td></tr><tr><td align="left" valign="bottom">Replication ES in original CI</td><td align="left" valign="bottom">9 of 25 (36%)</td><td align="left" valign="bottom">33 of 72 (46%)</td><td align="left" valign="bottom">42 of 97 (43%)</td></tr><tr><td align="left" valign="bottom">Replication ES in PI (p<sub>orig</sub>)</td><td align="left" valign="bottom">11 of 25 (44%)</td><td align="left" valign="bottom">45 of 72 (63%)</td><td align="left" valign="bottom">56 of 97 (58%)</td></tr><tr><td align="left" valign="bottom">Replication ES≥ original ES</td><td align="left" valign="bottom">0 of 25 (0%)</td><td align="left" valign="bottom">3 of 72 (4%)</td><td align="left" valign="bottom">3 of 97 (3%)</td></tr><tr><td align="left" valign="bottom">Meta-analysis (p &lt; 0.05)</td><td align="left" valign="bottom">13 of 25 (52%)</td><td align="left" valign="bottom">47 of 72 (65%)</td><td align="left" valign="bottom">60 of 97 (62%)</td></tr><tr><td align="left" valign="bottom"><bold><italic>Representative images</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">1 of 4 (25%)</td><td align="left" valign="bottom">27 of 31 (87%)</td><td align="left" valign="bottom">28 of 35 (80%)</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">0 of 2 (0%)</td><td align="left" valign="bottom">14 of 20 (70%)</td><td align="left" valign="bottom">14 of 22 (64%)</td></tr><tr><td align="left" valign="bottom">Original image in replication CI</td><td align="left" valign="bottom">0 of 1 (0%)</td><td align="left" valign="bottom">10 of 20 (50%)</td><td align="left" valign="bottom">10 of 21 (48%)</td></tr><tr><td align="left" valign="bottom">Replication effect ≥ original image</td><td align="left" valign="bottom">0 of 1 (0%)</td><td align="left" valign="bottom">7 of 20 (35%)</td><td align="left" valign="bottom">7 of 21 (33%)</td></tr><tr><td align="left" valign="bottom"><bold><italic>Sample sizes</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Median [IQR] of original</td><td align="left" valign="bottom">14.0 [10.0–20.0]</td><td align="left" valign="bottom">7.0 [6.0–11.2]</td><td align="left" valign="bottom">8.0 [6.0–13.0]</td></tr><tr><td align="left" valign="bottom">Median [IQR] of replication</td><td align="left" valign="bottom">15.0 [13.0–21.8]</td><td align="left" valign="bottom">10.0 [8.0–22.0]</td><td align="left" valign="bottom">12.0 [8.0–22.2]</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL NULL EFFECTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold><italic>Numerical results</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">4 of 5 (80%)</td><td align="left" valign="bottom">7 of 10 (70%)</td><td align="left" valign="bottom">11 of 15 (73%)</td></tr><tr><td align="left" valign="bottom">Original ES in replication CI</td><td align="left" valign="bottom">4 of 5 (80%)</td><td align="left" valign="bottom">7 of 10 (70%)</td><td align="left" valign="bottom">11 of 15 (73%)</td></tr><tr><td align="left" valign="bottom">Replication ES in original CI</td><td align="left" valign="bottom">5 of 5 (100%)</td><td align="left" valign="bottom">7 of 10 (70%)</td><td align="left" valign="bottom">12 of 15 (80%)</td></tr><tr><td align="left" valign="bottom">Replication ES in PI (p<sub>orig</sub>)</td><td align="left" valign="bottom">5 of 5 (100%)</td><td align="left" valign="bottom">7 of 10 (70%)</td><td align="left" valign="bottom">12 of 15 (80%)</td></tr><tr><td align="left" valign="bottom">Replication ES≤ original ES</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Meta-analysis (p &gt; 0.05)</td><td align="left" valign="bottom">3 of 5 (60%)</td><td align="left" valign="bottom">7 of 10 (70%)</td><td align="left" valign="bottom">10 of 15 (67%)</td></tr><tr><td align="left" valign="bottom"><bold><italic>Representative images</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Same direction</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Direction and statistical significance</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">7 of 5 (80%)</td><td align="left" valign="bottom">4 of 5 (80%)</td></tr><tr><td align="left" valign="bottom">Original image in replication CI</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">3 of 5 (60%)</td><td align="left" valign="bottom">3 of 5 (60%)</td></tr><tr><td align="left" valign="bottom">Replication effect ≤ original image</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom"><bold><italic>Sample sizes</italic></bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Median [IQR] of original</td><td align="left" valign="bottom">21.0 [20.0–30.0]</td><td align="left" valign="bottom">8.0 [5.0–266]</td><td align="left" valign="bottom">15.0 [7.5–31.0]</td></tr><tr><td align="left" valign="bottom">Median [IQR] of replication</td><td align="left" valign="bottom">35.0 [30.0–61.0]</td><td align="left" valign="bottom">16.0 [7.0–604]</td><td align="left" valign="bottom">27.0 [8.0–66.8]</td></tr></tbody></table><table-wrap-foot><fn><p>Comparing replication rates for animal experiments (column 2) and non-animal experiments (column 3) according to the seven criteria used in <xref ref-type="table" rid="table1">Table 1</xref>. For statistical significance, if original effects were interpreted as a positive effect but were not significant at p &lt; 0.05, then they were treated as a positive effect (7 cases), and likewise if they were interpreted as a null effect but were significant at p &lt; 0.05 they were treated as a null effect (3 cases). Standardized mean difference (SMD) effect sizes are reported. CI = 95% confidence interval; PI = 95% prediction interval; ES = effect size; IQR = interquartile range.</p></fn></table-wrap-foot></table-wrap><p>We used multi-level models to explore the association of five possible moderators with the replication rate: (1) animal experiments vs. non-animal (i.e., in vitro) experiments; (2) the use of contract research organizations to conduct replications; (3) the use of academic research core facilities to conduct replications; (4) whether the original authors shared materials with the replicating labs; (5) the quality of methodological clarifications made by the original authors upon request from the replicating labs (<xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>). None of the five moderators showed a consistent, significant association with replication success (see Table S7 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>), though the moderators were variably correlated with one another (ranging from r = –0.68 to r = 0.53; <xref ref-type="fig" rid="fig4">Figure 4</xref>). We cannot say whether any of these moderators influence replication success in general, but this analysis suggests that they do not account for much variation in replication success in this sample.</p><p>Other factors have been identified that could improve replicability, such as blinding, randomization, and sample size planning (<xref ref-type="bibr" rid="bib40">Landis et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Macleod and Mohan, 2019</xref>). However, these aspects were very rarely reported in the original experiments so they could not be examined as candidate moderators. For example, for the 36 animal effects across 15 experiments, none of the original experiments reported blinding, one experiment (for two effects) reported randomization, and none reported determining sample size a priori. By comparison, the replications reported blinding for five experiments (11 effects), randomization for 13 experiments (28 effects), and all 15 experiments (36 effects) reported calculating sample size a priori.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Correlations between five candidate moderators.</title><p>Point-biserial correlations among five candidate moderators for predicting replication success for the 97 original positive effects with replication pairs. The five moderators were: (i) animal experiments vs. non-animal (i.e., in vitro) experiments (animal expt); (ii) the use of contract research organizations to conduct replications (CRO lab); (iii) the use of academic research core facilities to conduct replications (core lab); (iv) whether the original authors shared materials with the replicating labs (materials shared); (v) the quality of methodological clarifications made by the original authors (clarifications quality); see Materials and methods for more details. Correlations are color-coded (blue = positive; red = negative; see color bar), with the size of the circle being proportional to the magnitude of the correlation. None of the five moderators showed a consistent, significant association with replication rate (see Table S7 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig4-v3.tif"/></fig><p>The multi-level analysis does not support the conclusion that there is a meaningful difference in the replication rates of animal and non-animal experiments. As can be seen in <xref ref-type="table" rid="table5">Table 5</xref>, median effect sizes were 84% smaller than the original findings for animal replications, and 78% smaller for non-animal replications. The reason that animal experiments had such a low replication rate, particularly according to the statistical significance criterion (12%), is that the effect sizes in the original experiments (Mdn = 1.61) were notably smaller than the effect sizes in the original non-animal experiments (Mdn = 3.65; <xref ref-type="fig" rid="fig5">Figure 5</xref>). In sum, original findings with smaller effect sizes were less likely to replicate than original findings with larger effect sizes, and animal experiments tended to have smaller effect sizes than non-animal experiments. In other words, when seeking to predict if a replication will be successful it is more useful to know the original effect size than to know whether the original experiment was an animal experiment or not.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Replication effect sizes compared with original effect sizes for animal and non-animal experiments.</title><p>Graphs for animal experiments (n = 30 effects; left) and non-animal experiments (n = 70 effects; right) in which each circle represents an effect for which an SMD effect size could be computed for both the original effects and the replication. Blue circles indicate effects for which p &lt; 0.05 in the replication, and red circles indicate p &gt; 0.05. Animal experiments were less likely to replicate than non-animal experiments and this may be a consequence of animal experiments eliciting smaller effect sizes on average than non-animal experiments (see main text for further discussion). Twelve effects in the non-animal experiments for which the original effects size was &gt;10 are not shown. SMD: standardized mean difference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig5-v3.tif"/></fig><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Effect sizes for animal and non-animal experiments.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Animal</th><th align="left" valign="bottom">Non-animal</th><th align="left" valign="bottom">Total</th></tr></thead><tbody><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL POSITIVE EFFECTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Number of outcomes</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">72</td><td align="left" valign="bottom">97</td></tr><tr><td align="left" valign="bottom">Mean (SD) original experiment effect size</td><td align="left" valign="bottom">1.88 (1.61)</td><td align="left" valign="bottom">7.63 (14.07)</td><td align="left" valign="bottom">6.15 (12.39)</td></tr><tr><td align="left" valign="bottom">Median [IQR] original experiment effect size</td><td align="left" valign="bottom">1.61 [0.81–2.30]</td><td align="left" valign="bottom">3.65 [2.45–6.43]</td><td align="left" valign="bottom">2.96 [1.71–5.70]</td></tr><tr><td align="left" valign="bottom">Mean (SD) replication experiment effect size</td><td align="left" valign="bottom">0.19 (0.50)</td><td align="left" valign="bottom">1.78 (3.39)</td><td align="left" valign="bottom">1.37 (3.01)</td></tr><tr><td align="left" valign="bottom">Median [IQR] replication experiment effect size</td><td align="left" valign="bottom">0.25 [−0.06–0.41]</td><td align="left" valign="bottom">0.79 [0.20–2.27]</td><td align="left" valign="bottom">0.43 [0.15–2.06]</td></tr><tr><td align="left" valign="bottom">Meta-analytic mean (SD) estimate</td><td align="left" valign="bottom">0.65 (0.54)</td><td align="left" valign="bottom">2.02 (2.77)</td><td align="left" valign="bottom">1.66 (2.47)</td></tr><tr><td align="left" valign="bottom">Meta-analytic median [IQR] estimate</td><td align="left" valign="bottom">0.83 [0.11–1.05]</td><td align="left" valign="bottom">1.06 [0.46–2.79]</td><td align="left" valign="bottom">0.92 [0.36–2.43]</td></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL NULL EFFECTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Number of outcomes</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">15</td></tr><tr><td align="left" valign="bottom">Mean (SD) original experiment effect size</td><td align="left" valign="bottom">0.34 (0.29)</td><td align="left" valign="bottom">0.78 (0.65)</td><td align="left" valign="bottom">0.63 (0.59)</td></tr><tr><td align="left" valign="bottom">Median [IQR] original experiment effect size</td><td align="left" valign="bottom">0.19 [0.10–0.61]</td><td align="left" valign="bottom">0.84 [0.17–1.08]</td><td align="left" valign="bottom">0.61 [0.16–0.97]</td></tr><tr><td align="left" valign="bottom">Mean (SD) replication experiment effect size</td><td align="left" valign="bottom">0.21 (0.48)</td><td align="left" valign="bottom">–0.08 (0.78)</td><td align="left" valign="bottom">0.02 (0.69)</td></tr><tr><td align="left" valign="bottom">Median [IQR] replication experiment effect size</td><td align="left" valign="bottom">0.13 [−0.18–0.65]</td><td align="left" valign="bottom">0.16 [−0.27–0.28]</td><td align="left" valign="bottom">0.16 [−0.24–0.47]</td></tr><tr><td align="left" valign="bottom">Meta-analytic mean (SD) estimate</td><td align="left" valign="bottom">0.21 (0.31)</td><td align="left" valign="bottom">0.25 (0.37)</td><td align="left" valign="bottom">0.24 (0.34)</td></tr><tr><td align="left" valign="bottom">Meta-analytic median [IQR] estimate</td><td align="left" valign="bottom">0.12 [0.04–0.37]</td><td align="left" valign="bottom">0.17 [0.10–0.45]</td><td align="left" valign="bottom">0.16 [0.06–0.44]</td></tr></tbody></table><table-wrap-foot><fn><p>Comparing original and replication effect sizes (means and medians) for animal experiments (column 2) and non-animal experiments (column 3), along with meta-analytic means and medians for the effect size obtained by combining data from the original effects and the replications. SD = standard deviation; IQR = interquartile range.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-9"><title>Summarizing replications across five criteria</title><p>The criteria described above returned a range of replication rates due to the different assumptions made by each, particularly how they handle the estimation of uncertainty. To provide an overall picture, we combined the replication rates by five of these criteria, selecting criteria that could be meaningfully applied to both positive and null effects, which meant excluding the ‘same direction’ and ‘comparing effect size’ criteria, as neither works for null effects.</p><p>For replications of original positive effects, 13 of 97 (13%) replications succeeded on all five criteria, 15 succeeded on four, 11 succeeded on three, 22 failed on three, 15 failed on four, and 21 (22%) failed on all five (<xref ref-type="table" rid="table6">Table 6</xref> and <xref ref-type="fig" rid="fig6">Figure 6</xref>). For original null effects, 7 of 15 (47%) replications succeeded on all five criteria, 2 succeeded on four, 3 succeeded on three, 0 failed on three, 2 failed on four, and 1 (7%) failed on all five. If we consider a replication to be successful overall if it succeeded on a majority of criteria (i.e., three or more), original null effects (80%) were twice as likely to replicate as original positive effects (40%). Combining positive and null effects, 51 of 112 (46%) replications succeeded on more criteria than they failed, and 61 (54%) replications failed on more criteria than they succeeded. We also found these five criteria to be positively correlated with one another ranging from 0.15 to 0.78 and a median of 0.345 suggesting that they provide related but distinct information (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Assessing replications of positive and null effects across five criteria.</title><p>Five of the criteria we used to assess replications could be used for both positive (n = 97) and null effects (n = 15). The number of effects where the replication was successful on all five criteria is shown by the top bar of each panel, with the second bar showing the number of effects where the replications were successful on four criteria, and so on: positive effects are shown in the left panel (blue bars), and null effects are shown in the right panel (green bars). The five criteria were: (i) direction and statistical significance (p &lt; 0.05); (ii) original effect size in replication 95% confidence interval; (iii) replication effect size in original 95% confidence interval; (iv) replication effect size in original 95% prediction interval; (v) meta-analysis combining original and replication effect sizes is statistically significant (p &lt; 0.05). Standardized mean difference (SMD) effect sizes are reported.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig6-v3.tif"/></fig><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Assessing replications of positive and null results across five criteria.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" colspan="2" valign="bottom">Papers</th><th align="left" colspan="2" valign="bottom">Experiments</th><th align="left" colspan="2" valign="bottom">Effects</th><th align="left" colspan="2" valign="bottom">All outcomes</th></tr></thead><tbody><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL POSITIVE RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Successful replication on all five criteria</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">6%</td><td align="left" valign="bottom">13</td><td align="left" valign="bottom">13%</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">18%</td></tr><tr><td align="left" valign="bottom">Success on 4; failure on 1</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">5%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom">13</td><td align="left" valign="bottom">12%</td></tr><tr><td align="left" valign="bottom">Success on 3; failure on 2</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">16%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">3%</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">13</td><td align="left" valign="bottom">12%</td></tr><tr><td align="left" valign="bottom">Success on 2; failure on 3</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">26%</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">45%</td><td align="left" valign="bottom">22</td><td align="left" valign="bottom">23%</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">23%</td></tr><tr><td align="left" valign="bottom">Success on 1, failure on 4</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">32%</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">18%</td><td align="left" valign="bottom">15</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom">19</td><td align="left" valign="bottom">17%</td></tr><tr><td align="left" valign="bottom">Success on 0, failure on 5</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">12%</td><td align="left" valign="bottom">21</td><td align="left" valign="bottom">22%</td><td align="left" valign="bottom">21</td><td align="left" valign="bottom">19%</td></tr><tr><td align="left" valign="bottom">Total</td><td align="left" valign="bottom">19</td><td align="left" valign="bottom"/><td align="left" valign="bottom">33</td><td align="left" valign="bottom"/><td align="left" valign="bottom">97</td><td align="left" valign="bottom"/><td align="left" valign="bottom">112</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"> </td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>ORIGINAL NULL RESULTS</bold></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Successful replication on all five criteria</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">45%</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">58%</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">47%</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">30%</td></tr><tr><td align="left" valign="bottom">Success on 4; failure on 1</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">18%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">8%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">13%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">10%</td></tr><tr><td align="left" valign="bottom">Success on 3; failure on 2</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">18%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">17%</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">20%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">25%</td></tr><tr><td align="left" valign="bottom">Success on 2; failure on 3</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">18%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">17%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">13%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">10%</td></tr><tr><td align="left" valign="bottom">Success on 1; failure on 4</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">15%</td></tr><tr><td align="left" valign="bottom">Success on 0; failure on 5</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0%</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">7%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">10%</td></tr><tr><td align="left" valign="bottom">Total</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom"/><td align="left" valign="bottom">12</td><td align="left" valign="bottom"/><td align="left" valign="bottom">15</td><td align="left" valign="bottom"/><td align="left" valign="bottom">20</td><td align="left" valign="bottom"/></tr></tbody></table><table-wrap-foot><fn><p>Five of the criteria we used to assess replications could be used for both positive results and null results. The number of papers, experiments, effects, and outcomes where replications were successful on various numbers of these criteria are shown for positive results (top) and null results (bottom). The five criteria were: (i) direction and statistical significance (p &lt; 0.05); (ii) original effect size in replication 95% confidence interval; (iii) replication effect size in original 95% confidence interval; (iv) replication effect size in original 95% prediction interval; (v) meta-analysis combining original and replication effect sizes is statistically significant (p &lt; 0.05). The data in this table are based on standardized mean difference (SMD) effect sizes. Very similar results are obtained when alternative strategies are used to aggregate the data (see Tables S8–S10 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></fn></table-wrap-foot></table-wrap><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Correlations between five criteria for replication success.</title><p>Point-biserial correlations among five criteria for evaluating replication success for the 112 original-replication pairs that could be evaluated on all five criteria: (i) same direction and statistical significance (Dir &amp; Sig); (ii) original effect size in replication 95% confidence interval (Orig ES in rep CI); (iii) replication effect size in original 95% confidence interval (Rep ES in orig CI); (iv) replication effect size in 95% prediction interval (Rep ES in PI); (v) meta-analysis combining original and replication effect sizes gives significant effect (p &lt; 0.05) (Meta sig). Correlations are color-coded (blue = positive; red = negative; see color bar), with the size of the circle being proportional to the magnitude of the correlation. The five criteria were all positively correlated with one another.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71601-fig7-v3.tif"/></fig><p>The ‘same direction’ and ‘comparing effect size’ criteria were not included as neither works for null effects. Also, these two criteria cannot be directly compared with the other five criteria because they both have a minimum replication success rate of 50% under ordinary assumptions, compared with 0% for the other criteria. For example, if all the original effects were due to noise, then the ‘same direction’ criterion would return a value of 50% for the replication rate indicating the worst possible performance. The observation that 79% of replications were in the same direction as the original effect indicates some signal being detected. Conversely, if all the original and replication effect sizes were equivalent and estimated without bias, then the ‘comparing effect size’ criterion would return a value of 50% indicating the best possible performance. The observation that just 3% of replications had larger effect sizes than original positive effects indicates that the original effect sizes were overestimated.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We used seven criteria to assess the replicability of 158 effects in a selection of 23 papers reporting the results of preclinical research in cancer biology. Across multiple criteria, the replications provided weaker evidence for the findings than the original papers. For original positive effects that were reported as numerical values, the median effect size for the replications was 0.43, which was 85% smaller than the median of the original effect sizes (2.96). And although 79% of the replication effects were in the same direction as the original finding (random would be 50%), 92% of replication effect sizes were smaller than the original (combining numeric and representative images). Across five dichotomous criteria for assessing replicability, original null results were twice as likely as original positive results to mostly replicate successfully (80% vs. 40%). Combining original positive and null effects for each of the five criteria, the replication success rates were 47% for same direction and statistical significance, 25% for the original effect size being inside the 95% confidence interval (CI) of the replication, 48% for the replication effect size being inside the 95% CI of the original, 61% for the replication effect size being inside the 95% prediction interval, and 63% for a criterion based on a meta-analytic combination of the data from the original experiment and the replication. Replication rates were relatively consistent whether examining the effects in isolation, combining across internal replications, combining across all the effects of each experiment, or combining across all the experiments of each paper. Animal and non-animal replications both had similarly weaker effect sizes compared to original findings, but animal experiments were much less likely to replicate (probably because the original effect size tended to be smaller in animal experiments). In this section we discuss some of the implications of our results.</p><sec id="s3-1"><title>What does a failure to replicate mean?</title><p>A single failure to replicate a finding does not render a verdict on its replicability or credibility. A failure to replicate could occur because the original finding was a false positive. Indeed, there is accumulating evidence of the deleterious impacts of low power and small sample sizes, ignoring null results, failures of transparency of methodology, failing to publish all experimental data collected, and questionable research practices such as p-hacking on the inflation of false positives in the published literature (<xref ref-type="bibr" rid="bib12">Casadevall and Fang, 2012</xref>; <xref ref-type="bibr" rid="bib15">Chalmers et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Gelman and Loken, 2013</xref>; <xref ref-type="bibr" rid="bib27">Greenwald, 1975</xref>; <xref ref-type="bibr" rid="bib33">Ioannidis, 2005</xref>; <xref ref-type="bibr" rid="bib34">John et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Kaplan and Irvin, 2015</xref>; <xref ref-type="bibr" rid="bib40">Landis et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Macleod et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Macleod et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">van der Naald et al., 2020</xref>; <xref ref-type="bibr" rid="bib69">Rosenthal, 1979</xref>; <xref ref-type="bibr" rid="bib73">Simmons et al., 2011</xref>). This evidence suggests that published findings might often be false positives or have exaggerated effect sizes, potentially adding noise and friction to the accumulation of knowledge.</p><p>A failure to replicate could also occur because the replication was a false negative. This can occur if the replication was underpowered or the design or execution was flawed. Such failures are uninteresting but important. Minimizing them requires attention to quality and rigor. We contracted independent laboratories with appropriate instrumentation and expertise to conduct the experiments. This has the virtue of ostensibly removing biasing influences of self-interest (pro or con) on the outcomes of the experiments. A skeptic, however, might suggest that original authors are essential for conducting the experiments because of particular skills or tacit knowledge they have for conducting the experiments. Indeed, this was part of a critique of the Reproducibility Project: Psychology (<xref ref-type="bibr" rid="bib26">Gilbert et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Open Science Collaboration, 2015</xref>). In that case, follow-up investigations did not support the claim that replication failures were due to deficiencies in replication quality. For example, Ebersole et al. repeated the potentially flawed replication protocols and developed revised protocols that were peer reviewed in advance by domain experts and original authors; they found that the replicability of original findings did not improve with the revised vs. original replication protocols (<xref ref-type="bibr" rid="bib20">Ebersole et al., 2020</xref>; see also <xref ref-type="bibr" rid="bib2">Anderson et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Nosek and Gilbert, 2017</xref>). Nevertheless, the possibility of flaws in research is always present.</p><p>For the present replications, we attempted to minimize the likelihood of replication errors by using original materials whenever possible, employing large sample sizes, engaging expert peer review of the methodology in advance, and by preregistering the experiment design and analysis plan. Despite all this, we cannot rule out the possibility of methodological error in the replications. To facilitate further review, critique, and follow-up investigation, all replications in this meta-analysis are reported transparently with digital materials, data, and code openly available on OSF.</p><p>A failure to replicate could also occur even when both original and replication findings are ‘correct’. Experimental contexts inevitably differ by innumerable factors such as the samples used, reagent and instrument suppliers, climate, software version, time of year, and physical environment. An experiment is a replication if the many differences between original and new experimental context are theoretically presumed to be irrelevant for observing evidence for the finding (<xref ref-type="bibr" rid="bib57">Nosek and Errington, 2020a</xref>). The replication experiments underwent peer review in advance to arrive at a precommitment that they were good faith tests based on present understanding of the phenomena and the conditions necessary to observe evidence supporting them (<xref ref-type="bibr" rid="bib58">Nosek and Errington, 2020b</xref>). However, differences that were deemed inconsequential during a priori peer review may be more critical than presently understood. After the replication results were known, some reviewers and commentators offered hypotheses for why the findings might have differed from the original (<xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>). This generative hypothesizing can be productive if it spurs additional investigations to test the new hypotheses. It can also be counterproductive if it is just rationalizing to preserve confidence in the original findings. <italic>Hypothesizing ideas to test</italic> is easily conflated with <italic>providing evidence to explain</italic>. Without follow-up investigation to test the hypotheses, that mix-up can promote overconfidence in original findings.</p></sec><sec id="s3-2"><title>What does a successful replication mean?</title><p>Successfully replicating a finding also does not render a verdict on its credibility. Successful replication increases confidence that the finding is repeatable, but it is mute to its meaning and validity. For example, if the finding is a result of unrecognized confounding influences or invalid measures, then the interpretation may be wrong even if it is easily replicated. Also, the interpretation of a finding may be much more general than is justified by the evidence. The particular experimental paradigm may elicit highly replicable findings, but also apply only to very specific circumstances that are much more circumscribed than the interpretation.</p><p>These possibilities are ordinary and unavoidable. Science makes progress by systematically producing and evaluating claims. Sometimes this leads to discoveries with broad generalizability and impact. Sometimes this leads to an understanding that is much more limited than the initial discovery. And, sometimes this leads to abandoning the effort because of persistent non-replicability or illumination of invalidity. Research produces its share of exhilaration with new discoveries and disappointments as some of them fade, but it is the continuous march away from ignorance that gets many scientists up each day excited to see which ideas will flourish.</p></sec><sec id="s3-3"><title>What replicates and what does not?</title><p>If we had better insights into the causes of replicability, or at least into the factors that correlate with replicability, we might be able to develop interventions that improve replicability. We explored five candidate moderators of replication success and did not find strong evidence to indicate that any of them account for variation in replication rates we observed in our sample. The clearest indicator of replication success was that smaller effects were less likely to replicate than larger effects, and this was particularly notable for animal experiments because they tended to have smaller original effect sizes than did non-animal experiments. Research into replicability in other disciplines has also found that findings with stronger initial evidence (such as larger effect sizes and/or smaller p-values) is more likely to replicate (<xref ref-type="bibr" rid="bib59">Nosek et al., 2021</xref>; <xref ref-type="bibr" rid="bib61">Open Science Collaboration, 2015</xref>), and it may be worth exploring if other findings from other disciplines – such as more surprising findings being less likely to replicate – can be generalized to cancer biology. There are also unique qualities of research in cancer biology that could be related to replicability, and a number of ongoing projects exploring replication in preclinical research (<xref ref-type="bibr" rid="bib1">Amaral et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Drude et al., 2021</xref>) will add to the data presented here and increase our understanding of replication and translational success (<xref ref-type="bibr" rid="bib14">Chakroborty et al., 2020</xref>). To facilitate such investigation, we have made all of the data for our meta-analysis available for reanalysis. Exploratory analyses with the dataset can help generate hypotheses about correlates of replicability that could be subjected to additional investigation.</p></sec><sec id="s3-4"><title>What have we learned about these findings?</title><p>After conducting dozens of replications, we can declare definitive understanding of precisely zero of the original findings. That may seem a dispiriting conclusion from such an intense effort, but it is the reality of doing research. Original findings provided initial evidence, replications provide additional evidence. Sometimes the replications increased confidence in the original findings, sometimes they decreased confidence. In all cases, we now have more information than we had. In no cases, do we have all the information that we need. Science makes progress by progressively identifying error and reducing uncertainty. Replication actively confronts current understanding, sometimes with affirmation, other times signaling caution and a call to investigate further. In science, that’s progress.</p></sec><sec id="s3-5"><title>What have we learned about replicability of preclinical cancer biology?</title><p>We adopted a wide and shallow approach with mostly single replications of many findings. We learned a little about each finding and more about replicability of preclinical cancer biology in general. If we had conducted a narrow and deep approach with the same resources, many replications of few findings, we would have learned more about the individual findings and less about cancer biology in general. The present study provides substantial evidence about the replicability of findings in a sample of high-impact papers published in the field of cancer biology in 2010, 2011, and 2012. The evidence suggests that replicability is lower than one might expect of the published literature. Causes of non-replicability could be due to factors in conducting and reporting the original research, conducting the replication experiments, or the complexity of the phenomena being studied. The present evidence cannot parse between these possibilities for any particular finding. But, there is substantial evidence of how the present research culture creates and maintains dysfunctional incentives and practices that can reduce research credibility in general. There are also reforms emerging that could address those challenges and potentially improve replicability.</p><p>It is unlikely that the challenges for replicability have a single cause or a single solution. Selective reporting, questionable research practices, and low-powered research all contribute to the unreliability of findings in a range of disciplines (<xref ref-type="bibr" rid="bib9">Button et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Franco et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Ioannidis, 2005</xref>; <xref ref-type="bibr" rid="bib34">John et al., 2012</xref>). These challenges may be compounded by researchers not receiving sufficient training in statistical inference and research methodology (<xref ref-type="bibr" rid="bib81">Van Calster et al., 2021</xref>). Moreover, as reported in the companion paper (<xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>)<ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?cTUheJ">,</ext-link> failures to document research protocols and to share data, materials (digital and physical), and code are hindering efforts to increase replicability (see also <xref ref-type="bibr" rid="bib42">Lemmon et al., 2014</xref>; <xref ref-type="bibr" rid="bib72">Serghiou et al., 2021</xref>; <xref ref-type="bibr" rid="bib78">Stodden et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Vines et al., 2014</xref>). These issues are exacerbated by dysfunctional incentives in the research culture that favor positive, novel, tidy results, even at the expense of rigor, accuracy, and transparency. In a system that rewards researchers with publications, grants, and employment for producing exciting and innovative results, it is no surprise that the literature is filled with exciting and innovative results. A system that also rewarded rigor, accuracy, and transparency as counterweights might stem some of the most unproductive impacts on research credibility, improve the culture, and accelerate progress.</p><p>There are solutions available that could have a substantial positive impact on improving research practices. Technologies provide mechanisms for research planning, preregistration, and sharing data, materials, and code (<xref ref-type="bibr" rid="bib5">Baker, 2019</xref>; <xref ref-type="bibr" rid="bib17">Cragin et al., 2010</xref>; <xref ref-type="bibr" rid="bib30">Heinl et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Horai et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Lindsay et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Soderberg, 2018</xref>). Training services can assist researchers in maximizing the value of these technologies and advance their understanding of research methodology and statistical inference (<xref ref-type="bibr" rid="bib13">Casadevall et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Teal et al., 2015</xref>; <xref ref-type="bibr" rid="bib84">Wilson, 2014</xref>). Incentive focused innovations such as Registered Reports can shift reward away from achieving exciting results and toward asking important questions and designing rigorous studies to investigate them (<xref ref-type="bibr" rid="bib16">Chambers, 2019</xref>; <xref ref-type="bibr" rid="bib71">Scheel et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Soderberg et al., 2020</xref>). Signals such as badges to acknowledge open practices can increase visibility of these behaviors to facilitate changing norms within research communities (<xref ref-type="bibr" rid="bib36">Kidwell et al., 2016</xref>). Finally, journals, funders, and institutions can assess the research cultures and incentives that they create and maintain and introduce new policies that are values-aligned to promote research rigor and transparency (<xref ref-type="bibr" rid="bib29">Hatch and Curry, 2020</xref>; <xref ref-type="bibr" rid="bib55">Nosek et al., 2015</xref>). Improvements to infrastructure, training, norms, incentives, and policies are each necessary, and none individually sufficient, to foster a research culture that rewards rigor, transparency, and sharing, to ultimately reduce friction and accelerate progress.</p></sec><sec id="s3-6"><title>Limitations</title><p>The present investigation provides evidence about the challenges involved in replicating a sample of experiments from high-impact papers in cancer biology. This could have implications for research on cancer biology and preclinical life sciences research more generally. However, there are important cautions about the selection and replication process that make the generalizability of these findings unclear. The systematic selection process identified 193 experiments from 53 high-impact papers published in 2010, 2011, and 2012. We experienced a variety of barriers to preparing and conducting the replications (<xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>). We do not know if those barriers produced a selection bias that altered our likelihood of successful replication, for better or worse. Moreover, replications were conducted on a subset of experiments from the papers. We cannot rule out the possibility that we inadvertently selected experiments from within the papers that were less likely to replicate.</p><p>One protection against the possibility of selection bias was the fact that all the experimental designs and protocols for the replications were peer reviewed by experts in advance, which sometimes resulted in changes to the experiments selected for replication. There was no systematic observation of biased selection of experiments that were more or less likely to replicate. More generally, selecting high-impact papers may result in biasing influence: for example, if papers reporting findings that have lower prior odds of being correct are more likely to gain attention and citations, then selecting these papers may overestimate failures to replicate for findings in general. Alternatively, if high-impact papers, most of which are published in ‘prestigious’ journals, are more likely to receive and withstand scrutiny prior to publication, then selecting these papers may underestimate failures to replicate for findings in general. In any case, compared to the baseline presumption that the published literature is credible and replicable, our findings from a systematically selected sample of the literature suggest that there is room for improvement.</p></sec><sec id="s3-7"><title>Conclusion</title><p>No single effect, experiment, or paper provides definitive evidence about its claims. Innovation identifies possibilities. Verification interrogates credibility. Progress depends on both. Innovation without verification is likely to accumulate incredible results at the expense of credible ones and create friction in the creation of knowledge, solutions, and treatments. Replication is important for research progress because it helps to separate what we know from what we think we know.</p><p>The surprisingly high rate of failures to replicate in this study might be an indicator of friction in the research process inhibiting the successful translation of basic and preclinical research. For example, promising animal experiments are often the basis of launching clinical trials. The low replication success rates and small effect sizes we found reinforce prior calls for improving the rigor and transparency of preclinical research to improve the allocation of limited resources and to protect human participants from being enrolled in trials that are based on weak evidence (<xref ref-type="bibr" rid="bib40">Landis et al., 2012</xref>; <xref ref-type="bibr" rid="bib64">Perrin, 2014</xref>; <xref ref-type="bibr" rid="bib77">Steward et al., 2012</xref>).</p><p>Stakeholders from across the research community have been raising concerns and generating evidence about dysfunctional incentives and research practices that could slow the pace of discovery. This paper is just one contribution to the community’s self-critical examination of its own practices. Science pursuing and exposing its own flaws is just science being science. Science is trustworthy because it does not trust itself. Science earns that trustworthiness through publicly, transparently, and continuously seeking out and eradicating error in its own culture, methods, and findings. Increasing awareness and evidence of the deleterious effects of reward structures and research practices will spur one of science’s greatest strengths, self-correction.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Paper, experiment, and effect selection strategy</title><p>Of the 193 experiments from 53 papers selected for replication, a total of 50 experiments from 23 papers were completed with 158 unique effects. See <xref ref-type="bibr" rid="bib21">Errington et al., 2014</xref> and <xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref> for full descriptions of the sampling strategy, methodology, and challenges in conducting the replications. Briefly, we asked the original authors for data, materials, and advice on the protocols to maximize the quality of the replications. If we were able to design complete replication protocols, identify labs to conduct the experiments, and obtain materials and reagents to perform the experiments, we then submitted the protocols for peer review at <italic>eLife</italic> as Registered Reports, a publishing format in which peer review is conducted prior to observing the experimental outcomes to maximize quality of methodology, ensure precommitment and transparency of registered methods and analysis plans, and to minimize publication bias and selective reporting (<xref ref-type="bibr" rid="bib16">Chambers, 2019</xref>; <xref ref-type="bibr" rid="bib54">Nosek and Lakens, 2014</xref>). Thus, all selected experiments and effects of interest, protocol details, material choices, and analytical strategies were peer reviewed in advance. In total, we published 29 Registered Reports including 87 planned experiments. For each experiment, effects of interest were identified with sample sizes determined a priori for at least 80% power to detect the original effect size. For 18 of the original papers we were able to implement modifications to complete the proposed experiments resulting in a total of 17 published Replication Studies with the one rejected Replication Study posted as a preprint (<xref ref-type="bibr" rid="bib63">Pelech et al., 2021</xref>). For the five original papers where we completed some of the experiments the results of the completed replications were reported in an aggregate paper (<xref ref-type="bibr" rid="bib22">Errington et al., 2021a</xref>). In total there were 158 effects with replication outcomes from 50 experiments (effects per experiment: M = 3.2; SD = 2.4; range = 1–13) that came from 23 papers (effects per paper: M = 3.9; SD = 2.8; range = 1–13). Some of the 158 effects had internal replications (n = 19) in which we conducted multiple replications (range = 2–3) of the same original experiment leading to 188 total outcomes considering the internal replications separately. All of these outcomes are available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/e5nvr/">https://osf.io/e5nvr/</ext-link>). That database is the basis of the meta-analysis reported in this paper. All replication protocols, materials, data, and outcomes are documented, archived, and publicly accessible to maximize transparency, accountability, and reproducibility of this project and are available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/collections/rpcb/">https://osf.io/collections/rpcb/</ext-link>. All individual papers published as part of this project are available at <italic>eLife</italic> (<ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology</ext-link>).</p></sec><sec id="s4-2"><title>Calculation and extraction of statistical data</title><p>Papers and experiments were coded as described in <xref ref-type="bibr" rid="bib23">Errington et al., 2021b</xref>. All original outcome data were calculated using either the shared original raw data, shared original summary data, extracted summary data from original papers, or original statistical variables from original papers. Summary data from all original experiments were reported in the associated Registered Reports and used for power calculations. Replication outcome data were calculated using the replication raw data with the outcomes and summary data reported in the associated Replication Studies. There was variation in reporting across outcomes that constrained what kinds of comparisons could be made between original and replication findings (e.g., 117 [74%] of the 158 original effects reported numerical results [e.g., graphs of quantified data or statistical tests] while other effects may have been reported as a representative image without any information about variability or an associated statistical inference). And of the 158 original effects, 86% were positive (i.e., interpreted as observing a relationship or impact of an intervention) and 14% were null (i.e., interpreted as not observing a meaningful relationship or impact of an intervention). For each outcome the statistical tests, when possible, were calculated based on the native structure of the original or replication data with a common shared effect size calculated for each original and replication pair. In 19 cases (5 where the original was a numerical result and 14 where the original was representative), this was not possible for the replication, meaning only the ‘same direction’ criterion was able to be determined. There was also one case where the original point estimate from the representative image in the original study was not able to be determined, meaning only the ‘same direction’ and ‘significance agreement’ criteria could be assessed, but not others (e.g., if the original image estimate was within the 95% confidence interval of the replication). For the cases where there were statistical tests and effect sizes, they ranged from types of standardized mean differences (SMDs) (e.g., Cohen’s d) and non-parametric equivalents (e.g., Cliff’s delta) across the range of outcomes. To facilitate effect-size conversions to approximate the SMDs scale we calculated, or approximately converted, all effect sizes to the SMD scale and recalculated statistical tests where necessary. This allows for meta-analysis and aggregation across a wider range of outcomes, although has the risk of distorting the results. As such, we conducted analyses below using the outcomes in the native or SMD scale, which gave similar patterns (see Tables S1–S3, S4–S6, S8–S10 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Data dictionaries describing all of the variables are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/e5nvr/">https://osf.io/e5nvr/</ext-link>.</p></sec><sec id="s4-3"><title>Effect size conversions</title><p>For numerical results, we extracted effect sizes on the following SMD scales for all outcomes: Cohen’s d (110 outcomes), Cohen’s dz (15 outcomes), Glass’ delta (24 outcomes), hazard ratio ( outcomes), and Pearson’s r (6 outcomes). The first three effect size scales are types of SMDs, although their interpretations are somewhat different from one another. We approximately converted hazard ratios (<xref ref-type="bibr" rid="bib28">Hasselblad and Hedges, 1995</xref>) and Pearson’s correlations (<xref ref-type="bibr" rid="bib49">Mathur and VanderWeele, 2020a</xref>) to the SMD scale. Converting Pearson’s correlations calculated with continuous independent variables to SMDs requires specifying the size of contrast in the independent variable that is to be considered. We selected a contrast size of 1 standard deviation on the independent variable throughout.</p></sec><sec id="s4-4"><title>Accounting for nested data</title><p>The data had a hierarchical structure, with effects nested within experiments nested with papers. We first calculated five pairwise metrics of replication success (detailed below) at the effect level, which was the finest-grained level of analysis. To account for this nesting structure, we first used fixed-effects meta-analysis to combine internal replications within a given paper, experiment, and effect, reflecting the assumption that they were testing the same effect size. We report aggregated findings at each level of analysis for ease of comprehension and multi-level analysis that account for the nested structure. Effects were meta-analytically combined into experiments with random-effect models reflecting the fact that the effects could be heterogeneous within an experiment. And experiments were meta-analytically combined into papers with random-effect models for the same reason. Original positive and null effects were kept separate in aggregating into experiments and papers. This was done for effects on the SMD scale (main paper results presented this way) and for effects on the native scale (where possible; Tables S1, S4, S8 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). We also summarized these effect-level metrics at the experiment and paper level by calculating percentages to summarize binary metrics of replication success, and by using harmonic mean p-values to summarize the continuous metric (<xref ref-type="bibr" rid="bib85">Wilson, 2019</xref>). This was done for effects on the native scale (Tables S2, S5 and S9 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) and the SMD scale (Tables S3, S6, and S10 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). When conducting meta-regression analyses across pairs (i.e., when effect sizes could be determined for both original and replication effects), we accounted for the hierarchical structure using robust inference (<xref ref-type="bibr" rid="bib7">Bell and McCaffrey, 2002</xref>; <xref ref-type="bibr" rid="bib66">Pustejovsky and Tipton, 2017</xref>) to account for the nesting structure as described in ‘Assessing candidate moderators of replication success metrics’ below.</p></sec><sec id="s4-5"><title>Pairwise metrics of replication success</title><p>We evaluated replication success with a variety of outcomes that have been employed in prior replication studies: (1) We evaluated whether the sign of the replication estimate agreed with that of the original effect (‘same direction’). This rudimentary metric does not account for effect sizes nor statistical precision, but was useful because we could compute it for even the non-quantitative pairs such as when original experiments reported only representative images. (2) We assessed whether the replication had a p-value less than 0.05 and an estimate in the same direction as the original (‘significance agreement’). (3 and 4) We assessed whether the original and replication estimates were inside the 95% confidence intervals of the other. (5) We assessed whether the replication estimate was inside the 95% prediction interval of the original, and formally assessed the degree of statistical inconsistency between the replication and the original using the metric p<sub>orig</sub>, which can be viewed as a p-value for the hypothesis that the original and the replication had the same population effect size (<xref ref-type="bibr" rid="bib50">Mathur and VanderWeele, 2020b</xref>). (6) We estimated the difference in estimates between the replication and the original after transforming all effect sizes to a comparable scale. Finally, regarding evidence strength for the effects under investigation, (7) we calculated a pooled estimate from combining the replication and original via fixed-effects meta-analysis. This is equivalent to pooling individual observations from the replication and the original data and can be viewed as an updated estimate of the effect size if the original experiment and the replication experiment are treated as equally informative. We chose a fixed-effect model rather than a random-effects model primarily for consistency with our assumption throughout all main analyses that there was no within-pair heterogeneity. Also, a random-effects model typically cannot adequately estimate heterogeneity with only two effects (<xref ref-type="bibr" rid="bib41">Langan et al., 2019</xref>). Additionally, in the specific context of pooling a single replication effect with a single original effect, we view the natural target of statistical inference as the mean effect size for those two effects rather than for a hypothetical larger population of effects on the same topic from which the original and replication were drawn. For this purpose, inference from a fixed-effect model is appropriate (<xref ref-type="bibr" rid="bib68">Rice et al., 2017</xref>). In any case, reanalysis with a random-effects model has modest impact on the estimates and does not alter the substantive conclusions. Rationales, including the strengths and limitations of each of these metrics, are presented in context of the outcomes in the Results section.</p><p>We conducted all statistical analyses with R software (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001905">SCR_001905</ext-link>), version 4.0.3 (<xref ref-type="bibr" rid="bib67">R Development Core Team, 2021</xref>). All statistical analyses were determined <italic>post hoc</italic>. We also used metafor software (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_003450">SCR_003450</ext-link>), version 2.4–0 to conduct the meta-analyses.</p></sec><sec id="s4-6"><title>Assessing candidate moderators of replication success metrics</title><p>We assessed whether five candidate moderators were associated with each indicator of replication success. The moderators were: (1) animal experiments vs. non-animal (i.e., in vitro) experiments; (2) whether at least one of the replication labs was a contract research organization; (3) whether at least one of the replication labs was an academic research core facility; (4) whether the original authors shared materials with the replicating labs (coded as ‘no’, ‘yes’, or ‘not requested’, with the latter indicating that we did not need to request materials); and (5) the quality of methodological clarifications made by the original authors upon request from the replicating labs (coded as 0 = ‘no response or not helpful’ to 5 = ‘extremely helpful’ and analyzed as a continuous variable). We intended to include a sixth moderator, which described the extent to which we were successful at implementing any needed protocol changes (analyzed as a categorical variable with categories ‘changes were moderately, mostly, or completely implemented’, ‘changes were less than moderately implemented’, and ‘no changes needed’). However, all but one outcome were rated in the first category, making the model inestimable with this variable included. We therefore excluded this moderator from the model.</p><p>To assess whether the five moderators were associated with these metrics of replication success across quantitative pairs, we used multi-level models to regress each pairwise metric on all candidate moderators simultaneously, accounting for clustering within experiments and papers. Some pairwise metrics had variances associated with them (i.e., the difference in estimate and the pooled estimate), whereas the others did not. For the metrics that did not have variances, we obtained point estimates from a standard multi-level model containing random intercepts of experiments nested within papers. For the metrics that did have variances, we obtained point estimates from an equivalent model that also weighted pairs by the inverse variance of the outcome variable (i.e., the replication success metric); this model is a form of random-effects meta-analysis. In both cases, we used CR2 robust standard errors in order to relax the distributional assumptions of parametric mixed models.</p><p>These models estimated the average difference in each metric of replication success that was associated with each candidate moderator, while holding constant all the other moderators. We report inference both with and without Bonferroni corrections for multiplicity. The Bonferroni corrections adjusted for multiplicity across the five moderator coefficients per metric of replication success, but did not adjust for the multiple metrics of replication success because these metrics were of course highly correlated with one another, and were sometimes arithmetically related to one another.</p></sec><sec id="s4-7"><title>Sensitivity analysis</title><p>All of the replication success metrics assume that there is no heterogeneity within quantitative pairs. This is a limitation because there could be substantive differences, including unmeasured moderators, between a given original effect and its replication that could produce genuine substantive differences between the two estimates. Such heterogeneity is not uncommon, even if it is relatively small, in multisite replications in which heterogeneity can be directly estimated (<xref ref-type="bibr" rid="bib20">Ebersole et al., 2020</xref>; <xref ref-type="bibr" rid="bib39">Klein et al., 2018</xref>). As such, the metrics will typically underestimate replication success when there is heterogeneity within pairs (<xref ref-type="bibr" rid="bib50">Mathur and VanderWeele, 2020b</xref>). As a sensitivity analysis, we also evaluated evidence for inconsistency by calculating p<sub>orig</sub>, constructing prediction intervals, and estimating the expected significance agreement across pairs under the assumption that there was within-pair heterogeneity with standard deviation 0.21 on the SMDs scale, an estimate we obtained from reanalyzing a review of multisite replications (<xref ref-type="bibr" rid="bib50">Mathur and VanderWeele, 2020b</xref>; <xref ref-type="bibr" rid="bib60">Olsson-Collentine et al., 2020</xref>). This sensitivity analysis yielded similar results and conclusions to the main analyses for these three metrics of replication success (see Tables S7 and S11 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>), likely because the estimated heterogeneity was small relative to the original and replication standard errors. Future research could attempt to directly estimate heterogeneity in these research contexts.</p></sec><sec id="s4-8"><title>Note</title><p>All <italic>eLife</italic> content related to the Reproducibility Project: Cancer Biology is available at: <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology</ext-link>.</p><p>All underlying data, code, and digital materials for the project is available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/collections/rpcb/">https://osf.io/collections/rpcb/</ext-link>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>Employed by the Center for Open Science, a non-profit organization that has a mission to increase openness, integrity, and reproducibility of research</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf3"><p>Was employed by the Center for Open Science, a non-profit organization that has a mission to increase openness, integrity, and reproducibility of research</p></fn><fn fn-type="COI-statement" id="conf4"><p>Was employed by and holds shares in Science Exchange Inc</p></fn><fn fn-type="COI-statement" id="conf5"><p>Employed by and holds shares in Science Exchange Inc</p></fn><fn fn-type="COI-statement" id="conf6"><p>Employed by the nonprofit Center for Open Science that has a mission to increase openness, integrity, and reproducibility of research</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Supervision, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Methodology, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Methodology, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Funding acquisition, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Supervision, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Tables S1–S11.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-71601-supp1-v3.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-71601-transrepform1-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All experimental details (e.g., additional protocol details, data, analysis files) of the individual replications and data, code, and materials for the overall project are openly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/collections/rpcb/">https://osf.io/collections/rpcb/</ext-link>. Master data files, containing the aggregate coded variables, are available for exploratory analysis at <ext-link ext-link-type="uri" xlink:href="https://osf.io/e5nvr/">https://osf.io/e5nvr/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Replication Data from the Reproducibility Project: Cancer Biology</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/osf.io/e5nvr</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Fraser Tan, Joelle Lomax, Rachel Tsui, and Stephen Williams for helping in coordination efforts during the course of the project. We thank all Science Exchange providers who provided their services and all employees at Science Exchange and the Center for Open Science who contributed to administrative and platform development efforts that enabled this project to occur.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amaral</surname><given-names>OB</given-names></name><name><surname>Neves</surname><given-names>K</given-names></name><name><surname>Wasilewska-Sampaio</surname><given-names>AP</given-names></name><name><surname>Carneiro</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Brazilian Reproducibility Initiative</article-title><source>eLife</source><volume>8</volume><elocation-id>e41602</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41602</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>CJ</given-names></name><name><surname>Bahník</surname><given-names>Š</given-names></name><name><surname>Barnett-Cowan</surname><given-names>M</given-names></name><name><surname>Bosco</surname><given-names>FA</given-names></name><name><surname>Chandler</surname><given-names>J</given-names></name><name><surname>Chartier</surname><given-names>CR</given-names></name><name><surname>Cheung</surname><given-names>F</given-names></name><name><surname>Christopherson</surname><given-names>CD</given-names></name><name><surname>Cordes</surname><given-names>A</given-names></name><name><surname>Cremata</surname><given-names>EJ</given-names></name><name><surname>Della Penna</surname><given-names>N</given-names></name><name><surname>Estel</surname><given-names>V</given-names></name><name><surname>Fedor</surname><given-names>A</given-names></name><name><surname>Fitneva</surname><given-names>SA</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name><name><surname>Grange</surname><given-names>JA</given-names></name><name><surname>Hartshorne</surname><given-names>JK</given-names></name><name><surname>Hasselman</surname><given-names>F</given-names></name><name><surname>Henninger</surname><given-names>F</given-names></name><name><surname>van der Hulst</surname><given-names>M</given-names></name><name><surname>Jonas</surname><given-names>KJ</given-names></name><name><surname>Lai</surname><given-names>CK</given-names></name><name><surname>Levitan</surname><given-names>CA</given-names></name><name><surname>Miller</surname><given-names>JK</given-names></name><name><surname>Moore</surname><given-names>KS</given-names></name><name><surname>Meixner</surname><given-names>JM</given-names></name><name><surname>Munafò</surname><given-names>MR</given-names></name><name><surname>Neijenhuijs</surname><given-names>KI</given-names></name><name><surname>Nilsonne</surname><given-names>G</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Plessow</surname><given-names>F</given-names></name><name><surname>Prenoveau</surname><given-names>JM</given-names></name><name><surname>Ricker</surname><given-names>AA</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Spies</surname><given-names>JR</given-names></name><name><surname>Stieger</surname><given-names>S</given-names></name><name><surname>Strohminger</surname><given-names>N</given-names></name><name><surname>Sullivan</surname><given-names>GB</given-names></name><name><surname>van Aert</surname><given-names>RCM</given-names></name><name><surname>van Assen</surname><given-names>M</given-names></name><name><surname>Vanpaemel</surname><given-names>W</given-names></name><name><surname>Vianello</surname><given-names>M</given-names></name><name><surname>Voracek</surname><given-names>M</given-names></name><name><surname>Zuni</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Response to Comment on “Estimating the reproducibility of psychological science.”</article-title><source>Science</source><volume>351</volume><elocation-id>1037</elocation-id><pub-id pub-id-type="doi">10.1126/science.aad9163</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname><given-names>I</given-names></name><name><surname>Kasy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Identification of and Correction for Publication Bias</article-title><source>American Economic Review</source><volume>109</volume><fpage>2766</fpage><lpage>2794</lpage><pub-id pub-id-type="doi">10.1257/aer.20180310</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Is there a reproducibility crisis</article-title><source>Nature</source><volume>533</volume><fpage>353</fpage><lpage>366</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Animal registries aim to reduce bias</article-title><source>Nature</source><volume>573</volume><fpage>297</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1038/d41586-019-02676-4</pub-id><pub-id pub-id-type="pmid">31501583</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begley</surname><given-names>CG</given-names></name><name><surname>Ellis</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Drug development: Raise standards for preclinical cancer research</article-title><source>Nature</source><volume>483</volume><fpage>531</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/483531a</pub-id><pub-id pub-id-type="pmid">22460880</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>RM</given-names></name><name><surname>McCaffrey</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Bias reduction in standard errors for linear regression with multi-stage samples</article-title><source>Survey Methodology</source><volume>28</volume><fpage>169</fpage><lpage>182</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>DJ</given-names></name><name><surname>Berger</surname><given-names>JO</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><name><surname>Berk</surname><given-names>R</given-names></name><name><surname>Bollen</surname><given-names>KA</given-names></name><name><surname>Brembs</surname><given-names>B</given-names></name><name><surname>Brown</surname><given-names>L</given-names></name><name><surname>Camerer</surname><given-names>C</given-names></name><name><surname>Cesarini</surname><given-names>D</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Clyde</surname><given-names>M</given-names></name><name><surname>Cook</surname><given-names>TD</given-names></name><name><surname>De Boeck</surname><given-names>P</given-names></name><name><surname>Dienes</surname><given-names>Z</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Easwaran</surname><given-names>K</given-names></name><name><surname>Efferson</surname><given-names>C</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name><name><surname>Fidler</surname><given-names>F</given-names></name><name><surname>Field</surname><given-names>AP</given-names></name><name><surname>Forster</surname><given-names>M</given-names></name><name><surname>George</surname><given-names>EI</given-names></name><name><surname>Gonzalez</surname><given-names>R</given-names></name><name><surname>Goodman</surname><given-names>S</given-names></name><name><surname>Green</surname><given-names>E</given-names></name><name><surname>Green</surname><given-names>DP</given-names></name><name><surname>Greenwald</surname><given-names>AG</given-names></name><name><surname>Hadfield</surname><given-names>JD</given-names></name><name><surname>Hedges</surname><given-names>LV</given-names></name><name><surname>Held</surname><given-names>L</given-names></name><name><surname>Hua Ho</surname><given-names>T</given-names></name><name><surname>Hoijtink</surname><given-names>H</given-names></name><name><surname>Hruschka</surname><given-names>DJ</given-names></name><name><surname>Imai</surname><given-names>K</given-names></name><name><surname>Imbens</surname><given-names>G</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Jeon</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>JH</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Laibson</surname><given-names>D</given-names></name><name><surname>List</surname><given-names>J</given-names></name><name><surname>Little</surname><given-names>R</given-names></name><name><surname>Lupia</surname><given-names>A</given-names></name><name><surname>Machery</surname><given-names>E</given-names></name><name><surname>Maxwell</surname><given-names>SE</given-names></name><name><surname>McCarthy</surname><given-names>M</given-names></name><name><surname>Moore</surname><given-names>DA</given-names></name><name><surname>Morgan</surname><given-names>SL</given-names></name><name><surname>Munafó</surname><given-names>M</given-names></name><name><surname>Nakagawa</surname><given-names>S</given-names></name><name><surname>Nyhan</surname><given-names>B</given-names></name><name><surname>Parker</surname><given-names>TH</given-names></name><name><surname>Pericchi</surname><given-names>L</given-names></name><name><surname>Perugini</surname><given-names>M</given-names></name><name><surname>Rouder</surname><given-names>J</given-names></name><name><surname>Rousseau</surname><given-names>J</given-names></name><name><surname>Savalei</surname><given-names>V</given-names></name><name><surname>Schönbrodt</surname><given-names>FD</given-names></name><name><surname>Sellke</surname><given-names>T</given-names></name><name><surname>Sinclair</surname><given-names>B</given-names></name><name><surname>Tingley</surname><given-names>D</given-names></name><name><surname>Van Zandt</surname><given-names>T</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Watts</surname><given-names>DJ</given-names></name><name><surname>Winship</surname><given-names>C</given-names></name><name><surname>Wolpert</surname><given-names>RL</given-names></name><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Young</surname><given-names>C</given-names></name><name><surname>Zinman</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>VE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Redefine statistical significance</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>6</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0189-z</pub-id><pub-id pub-id-type="pmid">30980045</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname><given-names>KS</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Mokrysz</surname><given-names>C</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Flint</surname><given-names>J</given-names></name><name><surname>Robinson</surname><given-names>ESJ</given-names></name><name><surname>Munafò</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Power failure: why small sample size undermines the reliability of neuroscience</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1038/nrn3475</pub-id><pub-id pub-id-type="pmid">23571845</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname><given-names>C. F</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Forsell</surname><given-names>E</given-names></name><name><surname>Ho</surname><given-names>TH</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Almenberg</surname><given-names>J</given-names></name><name><surname>Altmejd</surname><given-names>A</given-names></name><name><surname>Chan</surname><given-names>T</given-names></name><name><surname>Heikensten</surname><given-names>E</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Isaksson</surname><given-names>S</given-names></name><name><surname>Nave</surname><given-names>G</given-names></name><name><surname>Pfeiffer</surname><given-names>T</given-names></name><name><surname>Razen</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evaluating replicability of laboratory experiments in economics</article-title><source>Science</source><volume>351</volume><fpage>1433</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0918</pub-id><pub-id pub-id-type="pmid">26940865</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Ho</surname><given-names>T-H</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Nave</surname><given-names>G</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Pfeiffer</surname><given-names>T</given-names></name><name><surname>Altmejd</surname><given-names>A</given-names></name><name><surname>Buttrick</surname><given-names>N</given-names></name><name><surname>Chan</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Forsell</surname><given-names>E</given-names></name><name><surname>Gampa</surname><given-names>A</given-names></name><name><surname>Heikensten</surname><given-names>E</given-names></name><name><surname>Hummer</surname><given-names>L</given-names></name><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Isaksson</surname><given-names>S</given-names></name><name><surname>Manfredi</surname><given-names>D</given-names></name><name><surname>Rose</surname><given-names>J</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>637</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0399-z</pub-id><pub-id pub-id-type="pmid">31346273</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casadevall</surname><given-names>A</given-names></name><name><surname>Fang</surname><given-names>FC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reforming science: methodological and cultural reforms</article-title><source>Infection and Immunity</source><volume>80</volume><fpage>891</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1128/IAI.06183-11</pub-id><pub-id pub-id-type="pmid">22184414</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casadevall</surname><given-names>A</given-names></name><name><surname>Ellis</surname><given-names>LM</given-names></name><name><surname>Davies</surname><given-names>EW</given-names></name><name><surname>McFall-Ngai</surname><given-names>M</given-names></name><name><surname>Fang</surname><given-names>FC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A Framework for Improving the Quality of Research in the Biological Sciences</article-title><source>MBio</source><volume>7</volume><elocation-id>e01256-16</elocation-id><pub-id pub-id-type="doi">10.1128/mBio.01256-16</pub-id><pub-id pub-id-type="pmid">27578756</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chakroborty</surname><given-names>S</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Mancevska</surname><given-names>K</given-names></name><name><surname>Martin</surname><given-names>Z</given-names></name><name><surname>Yuan</surname><given-names>J</given-names></name><name><surname>Sheffield</surname><given-names>C</given-names></name><name><surname>Petanceska</surname><given-names>SS</given-names></name><name><surname>Refolo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>AlzPED: Improving the predictive power and translational validity of preclinical testing of candidate therapeutics in Alzheimer’s disease animal models: Development of new models and analysis methods/validation of pre‐clinical methods</article-title><source>Alzheimer’s &amp; Dementia</source><volume>16</volume><elocation-id>S3</elocation-id><pub-id pub-id-type="doi">10.1002/alz.036763</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalmers</surname><given-names>I</given-names></name><name><surname>Bracken</surname><given-names>MB</given-names></name><name><surname>Djulbegovic</surname><given-names>B</given-names></name><name><surname>Garattini</surname><given-names>S</given-names></name><name><surname>Grant</surname><given-names>J</given-names></name><name><surname>Gülmezoglu</surname><given-names>AM</given-names></name><name><surname>Howells</surname><given-names>DW</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Oliver</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How to increase value and reduce waste when research priorities are set</article-title><source>The Lancet</source><volume>383</volume><fpage>156</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62229-1</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>What’s next for Registered Reports?</article-title><source>Nature</source><volume>573</volume><fpage>187</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1038/d41586-019-02674-6</pub-id><pub-id pub-id-type="pmid">31506624</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cragin</surname><given-names>MH</given-names></name><name><surname>Palmer</surname><given-names>CL</given-names></name><name><surname>Carlson</surname><given-names>JR</given-names></name><name><surname>Witt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Data sharing, small science and institutional repositories</article-title><source>Philosophical Transactions of the Royal Society A</source><volume>368</volume><fpage>4023</fpage><lpage>4038</lpage><pub-id pub-id-type="doi">10.1098/rsta.2010.0165</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drude</surname><given-names>NI</given-names></name><name><surname>Martinez Gamboa</surname><given-names>L</given-names></name><name><surname>Danziger</surname><given-names>M</given-names></name><name><surname>Dirnagl</surname><given-names>U</given-names></name><name><surname>Toelch</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Improving preclinical studies through replications</article-title><source>eLife</source><volume>10</volume><elocation-id>e62101</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.62101</pub-id><pub-id pub-id-type="pmid">33432925</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebersole</surname><given-names>CR</given-names></name><name><surname>Atherton</surname><given-names>OE</given-names></name><name><surname>Belanger</surname><given-names>AL</given-names></name><name><surname>Skulborstad</surname><given-names>HM</given-names></name><name><surname>Allen</surname><given-names>JM</given-names></name><name><surname>Banks</surname><given-names>JB</given-names></name><name><surname>Baranski</surname><given-names>E</given-names></name><name><surname>Bernstein</surname><given-names>MJ</given-names></name><name><surname>Bonfiglio</surname><given-names>DBV</given-names></name><name><surname>Boucher</surname><given-names>L</given-names></name><name><surname>Brown</surname><given-names>ER</given-names></name><name><surname>Budiman</surname><given-names>NI</given-names></name><name><surname>Cairo</surname><given-names>AH</given-names></name><name><surname>Capaldi</surname><given-names>CA</given-names></name><name><surname>Chartier</surname><given-names>CR</given-names></name><name><surname>Chung</surname><given-names>JM</given-names></name><name><surname>Cicero</surname><given-names>DC</given-names></name><name><surname>Coleman</surname><given-names>JA</given-names></name><name><surname>Conway</surname><given-names>JG</given-names></name><name><surname>Davis</surname><given-names>WE</given-names></name><name><surname>Devos</surname><given-names>T</given-names></name><name><surname>Fletcher</surname><given-names>MM</given-names></name><name><surname>German</surname><given-names>K</given-names></name><name><surname>Grahe</surname><given-names>JE</given-names></name><name><surname>Hermann</surname><given-names>AD</given-names></name><name><surname>Hicks</surname><given-names>JA</given-names></name><name><surname>Honeycutt</surname><given-names>N</given-names></name><name><surname>Humphrey</surname><given-names>B</given-names></name><name><surname>Janus</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>DJ</given-names></name><name><surname>Joy-Gaba</surname><given-names>JA</given-names></name><name><surname>Juzeler</surname><given-names>H</given-names></name><name><surname>Keres</surname><given-names>A</given-names></name><name><surname>Kinney</surname><given-names>D</given-names></name><name><surname>Kirshenbaum</surname><given-names>J</given-names></name><name><surname>Klein</surname><given-names>RA</given-names></name><name><surname>Lucas</surname><given-names>RE</given-names></name><name><surname>Lustgraaf</surname><given-names>CJN</given-names></name><name><surname>Martin</surname><given-names>D</given-names></name><name><surname>Menon</surname><given-names>M</given-names></name><name><surname>Metzger</surname><given-names>M</given-names></name><name><surname>Moloney</surname><given-names>JM</given-names></name><name><surname>Morse</surname><given-names>PJ</given-names></name><name><surname>Prislin</surname><given-names>R</given-names></name><name><surname>Razza</surname><given-names>T</given-names></name><name><surname>Re</surname><given-names>DE</given-names></name><name><surname>Rule</surname><given-names>NO</given-names></name><name><surname>Sacco</surname><given-names>DF</given-names></name><name><surname>Sauerberger</surname><given-names>K</given-names></name><name><surname>Shrider</surname><given-names>E</given-names></name><name><surname>Shultz</surname><given-names>M</given-names></name><name><surname>Siemsen</surname><given-names>C</given-names></name><name><surname>Sobocko</surname><given-names>K</given-names></name><name><surname>Weylin Sternglanz</surname><given-names>R</given-names></name><name><surname>Summerville</surname><given-names>A</given-names></name><name><surname>Tskhay</surname><given-names>KO</given-names></name><name><surname>van Allen</surname><given-names>Z</given-names></name><name><surname>Vaughn</surname><given-names>LA</given-names></name><name><surname>Walker</surname><given-names>RJ</given-names></name><name><surname>Weinberg</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>JP</given-names></name><name><surname>Wirth</surname><given-names>JH</given-names></name><name><surname>Wortman</surname><given-names>J</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Many Labs 3: Evaluating participant pool quality across the academic semester via replication</article-title><source>Journal of Experimental Social Psychology</source><volume>67</volume><fpage>68</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2015.10.012</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebersole</surname><given-names>CR</given-names></name><name><surname>Mathur</surname><given-names>MB</given-names></name><name><surname>Baranski</surname><given-names>E</given-names></name><name><surname>Bart-Plange</surname><given-names>D-J</given-names></name><name><surname>Buttrick</surname><given-names>NR</given-names></name><name><surname>Chartier</surname><given-names>CR</given-names></name><name><surname>Corker</surname><given-names>KS</given-names></name><name><surname>Corley</surname><given-names>M</given-names></name><name><surname>Hartshorne</surname><given-names>JK</given-names></name><name><surname>IJzerman</surname><given-names>H</given-names></name><name><surname>Lazarević</surname><given-names>LB</given-names></name><name><surname>Rabagliati</surname><given-names>H</given-names></name><name><surname>Ropovik</surname><given-names>I</given-names></name><name><surname>Aczel</surname><given-names>B</given-names></name><name><surname>Aeschbach</surname><given-names>LF</given-names></name><name><surname>Andrighetto</surname><given-names>L</given-names></name><name><surname>Arnal</surname><given-names>JD</given-names></name><name><surname>Arrow</surname><given-names>H</given-names></name><name><surname>Babincak</surname><given-names>P</given-names></name><name><surname>Bakos</surname><given-names>BE</given-names></name><name><surname>Baník</surname><given-names>G</given-names></name><name><surname>Baskin</surname><given-names>E</given-names></name><name><surname>Belopavlović</surname><given-names>R</given-names></name><name><surname>Bernstein</surname><given-names>MH</given-names></name><name><surname>Białek</surname><given-names>M</given-names></name><name><surname>Bloxsom</surname><given-names>NG</given-names></name><name><surname>Bodroža</surname><given-names>B</given-names></name><name><surname>Bonfiglio</surname><given-names>DBV</given-names></name><name><surname>Boucher</surname><given-names>L</given-names></name><name><surname>Brühlmann</surname><given-names>F</given-names></name><name><surname>Brumbaugh</surname><given-names>CC</given-names></name><name><surname>Casini</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Chiorri</surname><given-names>C</given-names></name><name><surname>Chopik</surname><given-names>WJ</given-names></name><name><surname>Christ</surname><given-names>O</given-names></name><name><surname>Ciunci</surname><given-names>AM</given-names></name><name><surname>Claypool</surname><given-names>HM</given-names></name><name><surname>Coary</surname><given-names>S</given-names></name><name><surname>Čolić</surname><given-names>MV</given-names></name><name><surname>Collins</surname><given-names>WM</given-names></name><name><surname>Curran</surname><given-names>PG</given-names></name><name><surname>Day</surname><given-names>CR</given-names></name><name><surname>Dering</surname><given-names>B</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Edlund</surname><given-names>JE</given-names></name><name><surname>Falcão</surname><given-names>F</given-names></name><name><surname>Fedor</surname><given-names>A</given-names></name><name><surname>Feinberg</surname><given-names>L</given-names></name><name><surname>Ferguson</surname><given-names>IR</given-names></name><name><surname>Ford</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name><name><surname>Fryberger</surname><given-names>E</given-names></name><name><surname>Garinther</surname><given-names>A</given-names></name><name><surname>Gawryluk</surname><given-names>K</given-names></name><name><surname>Ashbaugh</surname><given-names>K</given-names></name><name><surname>Giacomantonio</surname><given-names>M</given-names></name><name><surname>Giessner</surname><given-names>SR</given-names></name><name><surname>Grahe</surname><given-names>JE</given-names></name><name><surname>Guadagno</surname><given-names>RE</given-names></name><name><surname>Hałasa</surname><given-names>E</given-names></name><name><surname>Hancock</surname><given-names>PJB</given-names></name><name><surname>Hilliard</surname><given-names>RA</given-names></name><name><surname>Hüffmeier</surname><given-names>J</given-names></name><name><surname>Hughes</surname><given-names>S</given-names></name><name><surname>Idzikowska</surname><given-names>K</given-names></name><name><surname>Inzlicht</surname><given-names>M</given-names></name><name><surname>Jern</surname><given-names>A</given-names></name><name><surname>Jiménez-Leal</surname><given-names>W</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Joy-Gaba</surname><given-names>JA</given-names></name><name><surname>Kauff</surname><given-names>M</given-names></name><name><surname>Kellier</surname><given-names>DJ</given-names></name><name><surname>Kessinger</surname><given-names>G</given-names></name><name><surname>Kidwell</surname><given-names>MC</given-names></name><name><surname>Kimbrough</surname><given-names>AM</given-names></name><name><surname>King</surname><given-names>JPJ</given-names></name><name><surname>Kolb</surname><given-names>VS</given-names></name><name><surname>Kołodziej</surname><given-names>S</given-names></name><name><surname>Kovacs</surname><given-names>M</given-names></name><name><surname>Krasuska</surname><given-names>K</given-names></name><name><surname>Kraus</surname><given-names>S</given-names></name><name><surname>Krueger</surname><given-names>LE</given-names></name><name><surname>Kuchno</surname><given-names>K</given-names></name><name><surname>Lage</surname><given-names>CA</given-names></name><name><surname>Langford</surname><given-names>EV</given-names></name><name><surname>Levitan</surname><given-names>CA</given-names></name><name><surname>de Lima</surname><given-names>TJS</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Lins</surname><given-names>S</given-names></name><name><surname>Loy</surname><given-names>JE</given-names></name><name><surname>Manfredi</surname><given-names>D</given-names></name><name><surname>Markiewicz</surname><given-names>Ł</given-names></name><name><surname>Menon</surname><given-names>M</given-names></name><name><surname>Mercier</surname><given-names>B</given-names></name><name><surname>Metzger</surname><given-names>M</given-names></name><name><surname>Meyet</surname><given-names>V</given-names></name><name><surname>Millen</surname><given-names>AE</given-names></name><name><surname>Miller</surname><given-names>JK</given-names></name><name><surname>Montealegre</surname><given-names>A</given-names></name><name><surname>Moore</surname><given-names>DA</given-names></name><name><surname>Muda</surname><given-names>R</given-names></name><name><surname>Nave</surname><given-names>G</given-names></name><name><surname>Nichols</surname><given-names>AL</given-names></name><name><surname>Novak</surname><given-names>SA</given-names></name><name><surname>Nunnally</surname><given-names>C</given-names></name><name><surname>Orlić</surname><given-names>A</given-names></name><name><surname>Palinkas</surname><given-names>A</given-names></name><name><surname>Panno</surname><given-names>A</given-names></name><name><surname>Parks</surname><given-names>KP</given-names></name><name><surname>Pedović</surname><given-names>I</given-names></name><name><surname>Pękala</surname><given-names>E</given-names></name><name><surname>Penner</surname><given-names>MR</given-names></name><name><surname>Pessers</surname><given-names>S</given-names></name><name><surname>Petrović</surname><given-names>B</given-names></name><name><surname>Pfeiffer</surname><given-names>T</given-names></name><name><surname>Pieńkosz</surname><given-names>D</given-names></name><name><surname>Preti</surname><given-names>E</given-names></name><name><surname>Purić</surname><given-names>D</given-names></name><name><surname>Ramos</surname><given-names>T</given-names></name><name><surname>Ravid</surname><given-names>J</given-names></name><name><surname>Razza</surname><given-names>TS</given-names></name><name><surname>Rentzsch</surname><given-names>K</given-names></name><name><surname>Richetin</surname><given-names>J</given-names></name><name><surname>Rife</surname><given-names>SC</given-names></name><name><surname>Rosa</surname><given-names>AD</given-names></name><name><surname>Rudy</surname><given-names>KH</given-names></name><name><surname>Salamon</surname><given-names>J</given-names></name><name><surname>Saunders</surname><given-names>B</given-names></name><name><surname>Sawicki</surname><given-names>P</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Schuepfer</surname><given-names>K</given-names></name><name><surname>Schultze</surname><given-names>T</given-names></name><name><surname>Schulz-Hardt</surname><given-names>S</given-names></name><name><surname>Schütz</surname><given-names>A</given-names></name><name><surname>Shabazian</surname><given-names>AN</given-names></name><name><surname>Shubella</surname><given-names>RL</given-names></name><name><surname>Siegel</surname><given-names>A</given-names></name><name><surname>Silva</surname><given-names>R</given-names></name><name><surname>Sioma</surname><given-names>B</given-names></name><name><surname>Skorb</surname><given-names>L</given-names></name><name><surname>de Souza</surname><given-names>LEC</given-names></name><name><surname>Steegen</surname><given-names>S</given-names></name><name><surname>Stein</surname><given-names>LAR</given-names></name><name><surname>Sternglanz</surname><given-names>RW</given-names></name><name><surname>Stojilović</surname><given-names>D</given-names></name><name><surname>Storage</surname><given-names>D</given-names></name><name><surname>Sullivan</surname><given-names>GB</given-names></name><name><surname>Szaszi</surname><given-names>B</given-names></name><name><surname>Szecsi</surname><given-names>P</given-names></name><name><surname>Szöke</surname><given-names>O</given-names></name><name><surname>Szuts</surname><given-names>A</given-names></name><name><surname>Thomae</surname><given-names>M</given-names></name><name><surname>Tidwell</surname><given-names>ND</given-names></name><name><surname>Tocco</surname><given-names>C</given-names></name><name><surname>Torka</surname><given-names>A-K</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name><name><surname>Vanpaemel</surname><given-names>W</given-names></name><name><surname>Vaughn</surname><given-names>LA</given-names></name><name><surname>Vianello</surname><given-names>M</given-names></name><name><surname>Viganola</surname><given-names>D</given-names></name><name><surname>Vlachou</surname><given-names>M</given-names></name><name><surname>Walker</surname><given-names>RJ</given-names></name><name><surname>Weissgerber</surname><given-names>SC</given-names></name><name><surname>Wichman</surname><given-names>AL</given-names></name><name><surname>Wiggins</surname><given-names>BJ</given-names></name><name><surname>Wolf</surname><given-names>D</given-names></name><name><surname>Wood</surname><given-names>MJ</given-names></name><name><surname>Zealley</surname><given-names>D</given-names></name><name><surname>Žeželj</surname><given-names>I</given-names></name><name><surname>Zrubka</surname><given-names>M</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>3</volume><fpage>309</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1177/2515245920958687</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Gunn</surname><given-names>W</given-names></name><name><surname>Tan</surname><given-names>FE</given-names></name><name><surname>Lomax</surname><given-names>J</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An open investigation of the reproducibility of cancer biology research</article-title><source>eLife</source><volume>3</volume><elocation-id>e04333</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04333</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Allison</surname><given-names>AB</given-names></name><name><surname>Araiza</surname><given-names>R</given-names></name><name><surname>Aza-Blanc</surname><given-names>P</given-names></name><name><surname>Bower</surname><given-names>LR</given-names></name><name><surname>Campos</surname><given-names>J</given-names></name><name><surname>Chu</surname><given-names>H</given-names></name><name><surname>Denson</surname><given-names>S</given-names></name><name><surname>Donham</surname><given-names>C</given-names></name><name><surname>Harr</surname><given-names>K</given-names></name><name><surname>Haven</surname><given-names>B</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Kwok</surname><given-names>J</given-names></name><name><surname>McDonald</surname><given-names>E</given-names></name><name><surname>Pelech</surname><given-names>S</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Pike</surname><given-names>A</given-names></name><name><surname>Sampey</surname><given-names>D</given-names></name><name><surname>Settles</surname><given-names>M</given-names></name><name><surname>Scott</surname><given-names>DA</given-names></name><name><surname>Sharma</surname><given-names>V</given-names></name><name><surname>Tolentino</surname><given-names>T</given-names></name><name><surname>Trinh</surname><given-names>A</given-names></name><name><surname>Tsui</surname><given-names>R</given-names></name><name><surname>Willis</surname><given-names>B</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Young</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Experiments from unfinished Registered Reports in the Reproducibility Project: Cancer Biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e73430</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.73430</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Challenges for assessing replicability in preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e67995</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67995</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franco</surname><given-names>A</given-names></name><name><surname>Malhotra</surname><given-names>N</given-names></name><name><surname>Simonovits</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Social science. Publication bias in the social sciences: unlocking the file drawer</article-title><source>Science</source><volume>345</volume><fpage>1502</fpage><lpage>1505</lpage><pub-id pub-id-type="doi">10.1126/science.1255484</pub-id><pub-id pub-id-type="pmid">25170047</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Loken</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time</article-title><source>Department of Statistics, Columbia University</source><ext-link ext-link-type="uri" xlink:href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf</ext-link><date-in-citation iso-8601-date="2021-10-22">October 22, 2021</date-in-citation></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>DT</given-names></name><name><surname>King</surname><given-names>G</given-names></name><name><surname>Pettigrew</surname><given-names>S</given-names></name><name><surname>Wilson</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comment on “Estimating the reproducibility of psychological science.”</article-title><source>Science</source><volume>351</volume><elocation-id>1037</elocation-id><pub-id pub-id-type="doi">10.1126/science.aad7243</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenwald</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Consequences of prejudice against the null hypothesis</article-title><source>Psychological Bulletin</source><volume>82</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1037/h0076157</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselblad</surname><given-names>V</given-names></name><name><surname>Hedges</surname><given-names>LV</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Meta-analysis of screening and diagnostic tests</article-title><source>Psychological Bulletin</source><volume>117</volume><fpage>167</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.117.1.167</pub-id><pub-id pub-id-type="pmid">7870860</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hatch</surname><given-names>A</given-names></name><name><surname>Curry</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Changing how we evaluate research is difficult, but not impossible</article-title><source>eLife</source><volume>9</volume><elocation-id>e58654</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58654</pub-id><pub-id pub-id-type="pmid">32782065</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinl</surname><given-names>C</given-names></name><name><surname>Chmielewska</surname><given-names>J</given-names></name><name><surname>Olevska</surname><given-names>A</given-names></name><name><surname>Grune</surname><given-names>B</given-names></name><name><surname>Schönfelder</surname><given-names>G</given-names></name><name><surname>Bert</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rethinking the incentive system in science: animal study registries: Preregistering experiments using animals could greatly improve transparency and reliability of biomedical studies and improve animal welfare</article-title><source>EMBO Reports</source><volume>21</volume><elocation-id>e49709</elocation-id><pub-id pub-id-type="doi">10.15252/embr.201949709</pub-id><pub-id pub-id-type="pmid">31867805</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hempel</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Maximal Specificity and Lawlikeness in Probabilistic Explanation</article-title><source>Philosophy of Science</source><volume>35</volume><fpage>116</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1086/288197</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horai</surname><given-names>H</given-names></name><name><surname>Arita</surname><given-names>M</given-names></name><name><surname>Kanaya</surname><given-names>S</given-names></name><name><surname>Nihei</surname><given-names>Y</given-names></name><name><surname>Ikeda</surname><given-names>T</given-names></name><name><surname>Suwa</surname><given-names>K</given-names></name><name><surname>Ojima</surname><given-names>Y</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>S</given-names></name><name><surname>Aoshima</surname><given-names>K</given-names></name><name><surname>Oda</surname><given-names>Y</given-names></name><name><surname>Kakazu</surname><given-names>Y</given-names></name><name><surname>Kusano</surname><given-names>M</given-names></name><name><surname>Tohge</surname><given-names>T</given-names></name><name><surname>Matsuda</surname><given-names>F</given-names></name><name><surname>Sawada</surname><given-names>Y</given-names></name><name><surname>Hirai</surname><given-names>MY</given-names></name><name><surname>Nakanishi</surname><given-names>H</given-names></name><name><surname>Ikeda</surname><given-names>K</given-names></name><name><surname>Akimoto</surname><given-names>N</given-names></name><name><surname>Maoka</surname><given-names>T</given-names></name><name><surname>Takahashi</surname><given-names>H</given-names></name><name><surname>Ara</surname><given-names>T</given-names></name><name><surname>Sakurai</surname><given-names>N</given-names></name><name><surname>Suzuki</surname><given-names>H</given-names></name><name><surname>Shibata</surname><given-names>D</given-names></name><name><surname>Neumann</surname><given-names>S</given-names></name><name><surname>Iida</surname><given-names>T</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Funatsu</surname><given-names>K</given-names></name><name><surname>Matsuura</surname><given-names>F</given-names></name><name><surname>Soga</surname><given-names>T</given-names></name><name><surname>Taguchi</surname><given-names>R</given-names></name><name><surname>Saito</surname><given-names>K</given-names></name><name><surname>Nishioka</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>MassBank: a public repository for sharing mass spectral data for life sciences</article-title><source>Journal of Mass Spectrometry</source><volume>45</volume><fpage>703</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1002/jms.1777</pub-id><pub-id pub-id-type="pmid">20623627</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Why Most Published Research Findings Are False</article-title><source>PLOS Medicine</source><volume>2</volume><elocation-id>e124</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname><given-names>LK</given-names></name><name><surname>Loewenstein</surname><given-names>G</given-names></name><name><surname>Prelec</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Measuring the prevalence of questionable research practices with incentives for truth telling</article-title><source>Psychological Science</source><volume>23</volume><fpage>524</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1177/0956797611430953</pub-id><pub-id pub-id-type="pmid">22508865</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>RM</given-names></name><name><surname>Irvin</surname><given-names>VL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0132382</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0132382</pub-id><pub-id pub-id-type="pmid">26244868</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidwell</surname><given-names>MC</given-names></name><name><surname>Lazarević</surname><given-names>LB</given-names></name><name><surname>Baranski</surname><given-names>E</given-names></name><name><surname>Hardwicke</surname><given-names>TE</given-names></name><name><surname>Piechowski</surname><given-names>S</given-names></name><name><surname>Falkenberg</surname><given-names>L-S</given-names></name><name><surname>Kennett</surname><given-names>C</given-names></name><name><surname>Slowik</surname><given-names>A</given-names></name><name><surname>Sonnleitner</surname><given-names>C</given-names></name><name><surname>Hess-Holden</surname><given-names>C</given-names></name><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Fiedler</surname><given-names>S</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Macleod</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002456</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002456</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmelman</surname><given-names>J</given-names></name><name><surname>Mogil</surname><given-names>JS</given-names></name><name><surname>Dirnagl</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distinguishing between exploratory and confirmatory preclinical research will improve translation</article-title><source>PLOS Biology</source><volume>12</volume><elocation-id>e1001863</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001863</pub-id><pub-id pub-id-type="pmid">24844265</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>RA</given-names></name><name><surname>Ratliff</surname><given-names>KA</given-names></name><name><surname>Vianello</surname><given-names>M</given-names></name><name><surname>Adams</surname><given-names>RB</given-names></name><name><surname>Bahník</surname><given-names>Š</given-names></name><name><surname>Bernstein</surname><given-names>MJ</given-names></name><name><surname>Bocian</surname><given-names>K</given-names></name><name><surname>Brandt</surname><given-names>MJ</given-names></name><name><surname>Brooks</surname><given-names>B</given-names></name><name><surname>Brumbaugh</surname><given-names>CC</given-names></name><name><surname>Cemalcilar</surname><given-names>Z</given-names></name><name><surname>Chandler</surname><given-names>J</given-names></name><name><surname>Cheong</surname><given-names>W</given-names></name><name><surname>Davis</surname><given-names>WE</given-names></name><name><surname>Devos</surname><given-names>T</given-names></name><name><surname>Eisner</surname><given-names>M</given-names></name><name><surname>Frankowska</surname><given-names>N</given-names></name><name><surname>Furrow</surname><given-names>D</given-names></name><name><surname>Galliani</surname><given-names>EM</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Investigating Variation in Replicability: A “Many Labs” Replication Project</article-title><source>Social Psychology</source><volume>45</volume><fpage>142</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1027/1864-9335/a000178</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>RA</given-names></name><name><surname>Vianello</surname><given-names>M</given-names></name><name><surname>Hasselman</surname><given-names>F</given-names></name><name><surname>Adams</surname><given-names>BG</given-names></name><name><surname>Adams</surname><given-names>RB</given-names></name><name><surname>Alper</surname><given-names>S</given-names></name><name><surname>Aveyard</surname><given-names>M</given-names></name><name><surname>Axt</surname><given-names>JR</given-names></name><name><surname>Babalola</surname><given-names>MT</given-names></name><name><surname>Bahník</surname><given-names>Š</given-names></name><name><surname>Batra</surname><given-names>R</given-names></name><name><surname>Berkics</surname><given-names>M</given-names></name><name><surname>Bernstein</surname><given-names>MJ</given-names></name><name><surname>Berry</surname><given-names>DR</given-names></name><name><surname>Bialobrzeska</surname><given-names>O</given-names></name><name><surname>Binan</surname><given-names>ED</given-names></name><name><surname>Bocian</surname><given-names>K</given-names></name><name><surname>Brandt</surname><given-names>MJ</given-names></name><name><surname>Busching</surname><given-names>R</given-names></name><name><surname>Rédei</surname><given-names>AC</given-names></name><name><surname>Cai</surname><given-names>H</given-names></name><name><surname>Cambier</surname><given-names>F</given-names></name><name><surname>Cantarero</surname><given-names>K</given-names></name><name><surname>Carmichael</surname><given-names>CL</given-names></name><name><surname>Ceric</surname><given-names>F</given-names></name><name><surname>Chandler</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>JH</given-names></name><name><surname>Chatard</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>EE</given-names></name><name><surname>Cheong</surname><given-names>W</given-names></name><name><surname>Cicero</surname><given-names>DC</given-names></name><name><surname>Coen</surname><given-names>S</given-names></name><name><surname>Coleman</surname><given-names>JA</given-names></name><name><surname>Collisson</surname><given-names>B</given-names></name><name><surname>Conway</surname><given-names>MA</given-names></name><name><surname>Corker</surname><given-names>KS</given-names></name><name><surname>Curran</surname><given-names>PG</given-names></name><name><surname>Cushman</surname><given-names>F</given-names></name><name><surname>Dagona</surname><given-names>ZK</given-names></name><name><surname>Dalgar</surname><given-names>I</given-names></name><name><surname>Dalla Rosa</surname><given-names>A</given-names></name><name><surname>Davis</surname><given-names>WE</given-names></name><name><surname>de Bruijn</surname><given-names>M</given-names></name><name><surname>De Schutter</surname><given-names>L</given-names></name><name><surname>Devos</surname><given-names>T</given-names></name><name><surname>de Vries</surname><given-names>M</given-names></name><name><surname>Doğulu</surname><given-names>C</given-names></name><name><surname>Dozo</surname><given-names>N</given-names></name><name><surname>Dukes</surname><given-names>KN</given-names></name><name><surname>Dunham</surname><given-names>Y</given-names></name><name><surname>Durrheim</surname><given-names>K</given-names></name><name><surname>Ebersole</surname><given-names>CR</given-names></name><name><surname>Edlund</surname><given-names>JE</given-names></name><name><surname>Eller</surname><given-names>A</given-names></name><name><surname>English</surname><given-names>AS</given-names></name><name><surname>Finck</surname><given-names>C</given-names></name><name><surname>Frankowska</surname><given-names>N</given-names></name><name><surname>Freyre</surname><given-names>MÁ</given-names></name><name><surname>Friedman</surname><given-names>M</given-names></name><name><surname>Galliani</surname><given-names>EM</given-names></name><name><surname>Gandi</surname><given-names>JC</given-names></name><name><surname>Ghoshal</surname><given-names>T</given-names></name><name><surname>Giessner</surname><given-names>SR</given-names></name><name><surname>Gill</surname><given-names>T</given-names></name><name><surname>Gnambs</surname><given-names>T</given-names></name><name><surname>Gómez</surname><given-names>Á</given-names></name><name><surname>González</surname><given-names>R</given-names></name><name><surname>Graham</surname><given-names>J</given-names></name><name><surname>Grahe</surname><given-names>JE</given-names></name><name><surname>Grahek</surname><given-names>I</given-names></name><name><surname>Green</surname><given-names>EGT</given-names></name><name><surname>Hai</surname><given-names>K</given-names></name><name><surname>Haigh</surname><given-names>M</given-names></name><name><surname>Haines</surname><given-names>EL</given-names></name><name><surname>Hall</surname><given-names>MP</given-names></name><name><surname>Heffernan</surname><given-names>ME</given-names></name><name><surname>Hicks</surname><given-names>JA</given-names></name><name><surname>Houdek</surname><given-names>P</given-names></name><name><surname>Huntsinger</surname><given-names>JR</given-names></name><name><surname>Huynh</surname><given-names>HP</given-names></name><name><surname>IJzerman</surname><given-names>H</given-names></name><name><surname>Inbar</surname><given-names>Y</given-names></name><name><surname>Innes-Ker</surname><given-names>ÅH</given-names></name><name><surname>Jiménez-Leal</surname><given-names>W</given-names></name><name><surname>John</surname><given-names>MS</given-names></name><name><surname>Joy-Gaba</surname><given-names>JA</given-names></name><name><surname>Kamiloğlu</surname><given-names>RG</given-names></name><name><surname>Kappes</surname><given-names>HB</given-names></name><name><surname>Karabati</surname><given-names>S</given-names></name><name><surname>Karick</surname><given-names>H</given-names></name><name><surname>Keller</surname><given-names>VN</given-names></name><name><surname>Kende</surname><given-names>A</given-names></name><name><surname>Kervyn</surname><given-names>N</given-names></name><name><surname>Knežević</surname><given-names>G</given-names></name><name><surname>Kovacs</surname><given-names>C</given-names></name><name><surname>Krueger</surname><given-names>LE</given-names></name><name><surname>Kurapov</surname><given-names>G</given-names></name><name><surname>Kurtz</surname><given-names>J</given-names></name><name><surname>Lakens</surname><given-names>D</given-names></name><name><surname>Lazarević</surname><given-names>LB</given-names></name><name><surname>Levitan</surname><given-names>CA</given-names></name><name><surname>Lewis</surname><given-names>NA</given-names></name><name><surname>Lins</surname><given-names>S</given-names></name><name><surname>Lipsey</surname><given-names>NP</given-names></name><name><surname>Losee</surname><given-names>JE</given-names></name><name><surname>Maassen</surname><given-names>E</given-names></name><name><surname>Maitner</surname><given-names>AT</given-names></name><name><surname>Malingumu</surname><given-names>W</given-names></name><name><surname>Mallett</surname><given-names>RK</given-names></name><name><surname>Marotta</surname><given-names>SA</given-names></name><name><surname>Međedović</surname><given-names>J</given-names></name><name><surname>Mena-Pacheco</surname><given-names>F</given-names></name><name><surname>Milfont</surname><given-names>TL</given-names></name><name><surname>Morris</surname><given-names>WL</given-names></name><name><surname>Murphy</surname><given-names>SC</given-names></name><name><surname>Myachykov</surname><given-names>A</given-names></name><name><surname>Neave</surname><given-names>N</given-names></name><name><surname>Neijenhuijs</surname><given-names>K</given-names></name><name><surname>Nelson</surname><given-names>AJ</given-names></name><name><surname>Neto</surname><given-names>F</given-names></name><name><surname>Lee Nichols</surname><given-names>A</given-names></name><name><surname>Ocampo</surname><given-names>A</given-names></name><name><surname>O’Donnell</surname><given-names>SL</given-names></name><name><surname>Oikawa</surname><given-names>H</given-names></name><name><surname>Oikawa</surname><given-names>M</given-names></name><name><surname>Ong</surname><given-names>E</given-names></name><name><surname>Orosz</surname><given-names>G</given-names></name><name><surname>Osowiecka</surname><given-names>M</given-names></name><name><surname>Packard</surname><given-names>G</given-names></name><name><surname>Pérez-Sánchez</surname><given-names>R</given-names></name><name><surname>Petrović</surname><given-names>B</given-names></name><name><surname>Pilati</surname><given-names>R</given-names></name><name><surname>Pinter</surname><given-names>B</given-names></name><name><surname>Podesta</surname><given-names>L</given-names></name><name><surname>Pogge</surname><given-names>G</given-names></name><name><surname>Pollmann</surname><given-names>MMH</given-names></name><name><surname>Rutchick</surname><given-names>AM</given-names></name><name><surname>Saavedra</surname><given-names>P</given-names></name><name><surname>Saeri</surname><given-names>AK</given-names></name><name><surname>Salomon</surname><given-names>E</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Schönbrodt</surname><given-names>FD</given-names></name><name><surname>Sekerdej</surname><given-names>MB</given-names></name><name><surname>Sirlopú</surname><given-names>D</given-names></name><name><surname>Skorinko</surname><given-names>JLM</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Smith-Castro</surname><given-names>V</given-names></name><name><surname>Smolders</surname><given-names>K</given-names></name><name><surname>Sobkow</surname><given-names>A</given-names></name><name><surname>Sowden</surname><given-names>W</given-names></name><name><surname>Spachtholz</surname><given-names>P</given-names></name><name><surname>Srivastava</surname><given-names>M</given-names></name><name><surname>Steiner</surname><given-names>TG</given-names></name><name><surname>Stouten</surname><given-names>J</given-names></name><name><surname>Street</surname><given-names>CNH</given-names></name><name><surname>Sundfelt</surname><given-names>OK</given-names></name><name><surname>Szeto</surname><given-names>S</given-names></name><name><surname>Szumowska</surname><given-names>E</given-names></name><name><surname>Tang</surname><given-names>ACW</given-names></name><name><surname>Tanzer</surname><given-names>N</given-names></name><name><surname>Tear</surname><given-names>MJ</given-names></name><name><surname>Theriault</surname><given-names>J</given-names></name><name><surname>Thomae</surname><given-names>M</given-names></name><name><surname>Torres</surname><given-names>D</given-names></name><name><surname>Traczyk</surname><given-names>J</given-names></name><name><surname>Tybur</surname><given-names>JM</given-names></name><name><surname>Ujhelyi</surname><given-names>A</given-names></name><name><surname>van Aert</surname><given-names>RCM</given-names></name><name><surname>van Assen</surname><given-names>M</given-names></name><name><surname>van der Hulst</surname><given-names>M</given-names></name><name><surname>van Lange</surname><given-names>PAM</given-names></name><name><surname>van ’t Veer</surname><given-names>AE</given-names></name><name><surname>Vásquez- Echeverría</surname><given-names>A</given-names></name><name><surname>Ann Vaughn</surname><given-names>L</given-names></name><name><surname>Vázquez</surname><given-names>A</given-names></name><name><surname>Vega</surname><given-names>LD</given-names></name><name><surname>Verniers</surname><given-names>C</given-names></name><name><surname>Verschoor</surname><given-names>M</given-names></name><name><surname>Voermans</surname><given-names>IPJ</given-names></name><name><surname>Vranka</surname><given-names>MA</given-names></name><name><surname>Welch</surname><given-names>C</given-names></name><name><surname>Wichman</surname><given-names>AL</given-names></name><name><surname>Williams</surname><given-names>LA</given-names></name><name><surname>Wood</surname><given-names>M</given-names></name><name><surname>Woodzicka</surname><given-names>JA</given-names></name><name><surname>Wronska</surname><given-names>MK</given-names></name><name><surname>Young</surname><given-names>L</given-names></name><name><surname>Zelenski</surname><given-names>JM</given-names></name><name><surname>Zhijia</surname><given-names>Z</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Many Labs 2: Investigating Variation in Replicability Across Samples and Settings</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>1</volume><fpage>443</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1177/2515245918810225</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landis</surname><given-names>SC</given-names></name><name><surname>Amara</surname><given-names>SG</given-names></name><name><surname>Asadullah</surname><given-names>K</given-names></name><name><surname>Austin</surname><given-names>CP</given-names></name><name><surname>Blumenstein</surname><given-names>R</given-names></name><name><surname>Bradley</surname><given-names>EW</given-names></name><name><surname>Crystal</surname><given-names>RG</given-names></name><name><surname>Darnell</surname><given-names>RB</given-names></name><name><surname>Ferrante</surname><given-names>RJ</given-names></name><name><surname>Fillit</surname><given-names>H</given-names></name><name><surname>Finkelstein</surname><given-names>R</given-names></name><name><surname>Fisher</surname><given-names>M</given-names></name><name><surname>Gendelman</surname><given-names>HE</given-names></name><name><surname>Golub</surname><given-names>RM</given-names></name><name><surname>Goudreau</surname><given-names>JL</given-names></name><name><surname>Gross</surname><given-names>RA</given-names></name><name><surname>Gubitz</surname><given-names>AK</given-names></name><name><surname>Hesterlee</surname><given-names>SE</given-names></name><name><surname>Howells</surname><given-names>DW</given-names></name><name><surname>Huguenard</surname><given-names>J</given-names></name><name><surname>Kelner</surname><given-names>K</given-names></name><name><surname>Koroshetz</surname><given-names>W</given-names></name><name><surname>Krainc</surname><given-names>D</given-names></name><name><surname>Lazic</surname><given-names>SE</given-names></name><name><surname>Levine</surname><given-names>MS</given-names></name><name><surname>Macleod</surname><given-names>MR</given-names></name><name><surname>McCall</surname><given-names>JM</given-names></name><name><surname>Narasimhan</surname><given-names>K</given-names></name><name><surname>Noble</surname><given-names>LJ</given-names></name><name><surname>Perrin</surname><given-names>S</given-names></name><name><surname>Porter</surname><given-names>JD</given-names></name><name><surname>Steward</surname><given-names>O</given-names></name><name><surname>Unger</surname><given-names>E</given-names></name><name><surname>Utz</surname><given-names>U</given-names></name><name><surname>Silberberg</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A call for transparent reporting to optimize the predictive value of preclinical research</article-title><source>Nature</source><volume>490</volume><fpage>187</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1038/nature11556</pub-id><pub-id pub-id-type="pmid">23060188</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langan</surname><given-names>D</given-names></name><name><surname>Higgins</surname><given-names>JPT</given-names></name><name><surname>Jackson</surname><given-names>D</given-names></name><name><surname>Bowden</surname><given-names>J</given-names></name><name><surname>Veroniki</surname><given-names>AA</given-names></name><name><surname>Kontopantelis</surname><given-names>E</given-names></name><name><surname>Viechtbauer</surname><given-names>W</given-names></name><name><surname>Simmonds</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses</article-title><source>Research Synthesis Methods</source><volume>10</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1002/jrsm.1316</pub-id><pub-id pub-id-type="pmid">30067315</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemmon</surname><given-names>VP</given-names></name><name><surname>Abeyruwan</surname><given-names>S</given-names></name><name><surname>Visser</surname><given-names>U</given-names></name><name><surname>Bixby</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Facilitating transparency in spinal cord injury studies using data standards and ontologies</article-title><source>Neural Regeneration Research</source><volume>9</volume><fpage>6</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.4103/1673-5374.125322</pub-id><pub-id pub-id-type="pmid">25206736</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>DS</given-names></name><name><surname>Simons</surname><given-names>DJ</given-names></name><name><surname>Lilienfeld</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Research preregistration 101</article-title><source>APS Observer</source><volume>29</volume><elocation-id>10</elocation-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname><given-names>MR</given-names></name><name><surname>Michie</surname><given-names>S</given-names></name><name><surname>Roberts</surname><given-names>I</given-names></name><name><surname>Dirnagl</surname><given-names>U</given-names></name><name><surname>Chalmers</surname><given-names>I</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Salman</surname><given-names>RAS</given-names></name><name><surname>Chan</surname><given-names>AW</given-names></name><name><surname>Glasziou</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Biomedical research: increasing value, reducing waste</article-title><source>The Lancet</source><volume>383</volume><fpage>101</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62329-6</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname><given-names>MR</given-names></name><name><surname>Lawson McLean</surname><given-names>A</given-names></name><name><surname>Kyriakopoulou</surname><given-names>A</given-names></name><name><surname>Serghiou</surname><given-names>S</given-names></name><name><surname>de Wilde</surname><given-names>A</given-names></name><name><surname>Sherratt</surname><given-names>N</given-names></name><name><surname>Hirst</surname><given-names>T</given-names></name><name><surname>Hemblade</surname><given-names>R</given-names></name><name><surname>Bahor</surname><given-names>Z</given-names></name><name><surname>Nunes-Fonseca</surname><given-names>C</given-names></name><name><surname>Potluru</surname><given-names>A</given-names></name><name><surname>Thomson</surname><given-names>A</given-names></name><name><surname>Baginskitae</surname><given-names>J</given-names></name><name><surname>Egan</surname><given-names>K</given-names></name><name><surname>Vesterinen</surname><given-names>H</given-names></name><name><surname>Currie</surname><given-names>GL</given-names></name><name><surname>Churilov</surname><given-names>L</given-names></name><name><surname>Howells</surname><given-names>DW</given-names></name><name><surname>Sena</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Risk of Bias in Reports of In Vivo Research: A Focus for Improvement</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002273</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002273</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname><given-names>M</given-names></name><name><surname>Mohan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reproducibility and Rigor in Animal-Based Research</article-title><source>ILAR Journal</source><volume>60</volume><fpage>17</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1093/ilar/ilz015</pub-id><pub-id pub-id-type="pmid">31687758</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makel</surname><given-names>MC</given-names></name><name><surname>Plucker</surname><given-names>JA</given-names></name><name><surname>Hegarty</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Replications in Psychology Research: How Often Do They Really Occur?</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>537</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1177/1745691612460688</pub-id><pub-id pub-id-type="pmid">26168110</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathur</surname><given-names>MB</given-names></name><name><surname>VanderWeele</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Challenges and suggestions for defining replication “success” when effects may be heterogeneous: Comment on Hedges and Schauer (2019)</article-title><source>Psychological Methods</source><volume>24</volume><fpage>571</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1037/met0000223</pub-id><pub-id pub-id-type="pmid">31580141</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathur</surname><given-names>MB</given-names></name><name><surname>VanderWeele</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>A Simple, Interpretable Conversion from Pearson’s Correlation to Cohen’s for d Continuous Exposures</article-title><source>Epidemiology</source><volume>31</volume><fpage>e16</fpage><lpage>e18</lpage><pub-id pub-id-type="doi">10.1097/EDE.0000000000001105</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathur</surname><given-names>MB</given-names></name><name><surname>VanderWeele</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>New statistical metrics for multisite replication projects</article-title><source>Journal of the Royal Statistical Society</source><volume>183</volume><fpage>1145</fpage><lpage>1166</lpage><pub-id pub-id-type="doi">10.1111/rssa.12572</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McShane</surname><given-names>BB</given-names></name><name><surname>Böckenholt</surname><given-names>U</given-names></name><name><surname>Hansen</surname><given-names>KT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes</article-title><source>Perspectives on Psychological Science</source><volume>11</volume><fpage>730</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1177/1745691616662243</pub-id><pub-id pub-id-type="pmid">27694467</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Musgrave</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1970">1970</year><source>Criticism and the Growth of Knowledge</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.2307/2412896</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Spies</surname><given-names>JR</given-names></name><name><surname>Motyl</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>615</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1177/1745691612459058</pub-id><pub-id pub-id-type="pmid">26168121</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Registered Reports</article-title><source>Social Psychology</source><volume>45</volume><fpage>137</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1027/1864-9335/a000192</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Alter</surname><given-names>G</given-names></name><name><surname>Banks</surname><given-names>GC</given-names></name><name><surname>Borsboom</surname><given-names>D</given-names></name><name><surname>Bowman</surname><given-names>SD</given-names></name><name><surname>Breckler</surname><given-names>SJ</given-names></name><name><surname>Buck</surname><given-names>S</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Chin</surname><given-names>G</given-names></name><name><surname>Christensen</surname><given-names>G</given-names></name><name><surname>Contestabile</surname><given-names>M</given-names></name><name><surname>Dafoe</surname><given-names>A</given-names></name><name><surname>Eich</surname><given-names>E</given-names></name><name><surname>Freese</surname><given-names>J</given-names></name><name><surname>Glennerster</surname><given-names>R</given-names></name><name><surname>Goroff</surname><given-names>D</given-names></name><name><surname>Green</surname><given-names>DP</given-names></name><name><surname>Hesse</surname><given-names>B</given-names></name><name><surname>Humphreys</surname><given-names>M</given-names></name><name><surname>Ishiyama</surname><given-names>J</given-names></name><name><surname>Karlan</surname><given-names>D</given-names></name><name><surname>Kraut</surname><given-names>A</given-names></name><name><surname>Lupia</surname><given-names>A</given-names></name><name><surname>Mabry</surname><given-names>P</given-names></name><name><surname>Madon</surname><given-names>T</given-names></name><name><surname>Malhotra</surname><given-names>N</given-names></name><name><surname>Mayo-Wilson</surname><given-names>E</given-names></name><name><surname>McNutt</surname><given-names>M</given-names></name><name><surname>Miguel</surname><given-names>E</given-names></name><name><surname>Paluck</surname><given-names>EL</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name><name><surname>Soderberg</surname><given-names>C</given-names></name><name><surname>Spellman</surname><given-names>BA</given-names></name><name><surname>Turitto</surname><given-names>J</given-names></name><name><surname>VandenBos</surname><given-names>G</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Wilson</surname><given-names>R</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Gilbert</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mischaracterizing Replication Studies Leads to Erroneous Conclusions</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.31234/osf.io/nt4d3">https://doi.org/10.31234/osf.io/nt4d3</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Errington</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>What is replication?</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000691</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000691</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Errington</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The best time to argue about what a replication means? Before you do it</article-title><source>Nature</source><volume>583</volume><fpage>518</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1038/d41586-020-02142-6</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Hardwicke</surname><given-names>TE</given-names></name><name><surname>Moshontz</surname><given-names>H</given-names></name><name><surname>Allard</surname><given-names>A</given-names></name><name><surname>Corker</surname><given-names>KS</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Fidler</surname><given-names>F</given-names></name><name><surname>Hilgard</surname><given-names>J</given-names></name><name><surname>Struhl</surname><given-names>MK</given-names></name><name><surname>Nuijten</surname><given-names>MB</given-names></name><name><surname>Rohrer</surname><given-names>JM</given-names></name><name><surname>Romero</surname><given-names>F</given-names></name><name><surname>Scheel</surname><given-names>AM</given-names></name><name><surname>Scherer</surname><given-names>LD</given-names></name><name><surname>Schönbrodt</surname><given-names>FD</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Replicability, Robustness, and Reproducibility in Psychological Science</article-title><source>Annual Review of Psychology</source><volume>73</volume><elocation-id>114157</elocation-id><pub-id pub-id-type="doi">10.1146/annurev-psych-020821-114157</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsson-Collentine</surname><given-names>A</given-names></name><name><surname>Wicherts</surname><given-names>JM</given-names></name><name><surname>van Assen</surname><given-names>MALM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Heterogeneity in direct replications in psychology and its association with effect size</article-title><source>Psychological Bulletin</source><volume>146</volume><fpage>922</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1037/bul0000294</pub-id><pub-id pub-id-type="pmid">32700942</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patil</surname><given-names>P</given-names></name><name><surname>Peng</surname><given-names>RD</given-names></name><name><surname>Leek</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What Should Researchers Expect When They Replicate Studies? A Statistical View of Replicability in Psychological Science</article-title><source>Perspectives on Psychological Science</source><volume>11</volume><fpage>539</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1177/1745691616646366</pub-id><pub-id pub-id-type="pmid">27474140</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pelech</surname><given-names>S</given-names></name><name><surname>Gallagher</surname><given-names>C</given-names></name><name><surname>Sutter</surname><given-names>C</given-names></name><name><surname>Yue</surname><given-names>L</given-names></name><name><surname>Kerwin</surname><given-names>J</given-names></name><name><surname>Bhargava</surname><given-names>A</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Tsui</surname><given-names>R</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Errington</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Replication Study: RAF Inhibitors Prime Wild-Type RAF to Activate the MAPK Pathway and Enhance Growth</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.30.470372</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Preclinical research: Make mouse studies work</article-title><source>Nature</source><volume>507</volume><fpage>423</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1038/507423a</pub-id><pub-id pub-id-type="pmid">24678540</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname><given-names>F</given-names></name><name><surname>Schlange</surname><given-names>T</given-names></name><name><surname>Asadullah</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Believe it or not: how much can we rely on published data on potential drug targets?</article-title><source>Nature Reviews Drug Discovery</source><volume>10</volume><elocation-id>712</elocation-id><pub-id pub-id-type="doi">10.1038/nrd3439-c1</pub-id><pub-id pub-id-type="pmid">21892149</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pustejovsky</surname><given-names>JE</given-names></name><name><surname>Tipton</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Small-Sample Methods for Cluster-Robust Variance Estimation and Hypothesis Testing in Fixed Effects Models</article-title><source>Journal of Business &amp; Economic Statistics</source><volume>36</volume><fpage>672</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1080/07350015.2016.1247004</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2021">2021</year><data-title>R: A language and environment for statistical computing</data-title><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rice</surname><given-names>K</given-names></name><name><surname>Higgins</surname><given-names>JPT</given-names></name><name><surname>Lumley</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A re‐evaluation of fixed effect(s) meta‐analysis</article-title><source>Journal of the Royal Statistical Society</source><volume>181</volume><fpage>205</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1111/rssa.12275</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>The file drawer problem and tolerance for null results</article-title><source>Psychological Bulletin</source><volume>86</volume><fpage>638</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.86.3.638</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Salmon</surname><given-names>MH</given-names></name><name><surname>Glymour</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Troduction to the Philosophy of Science</source><publisher-name>Hackett Publishing</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Scheel</surname><given-names>AM</given-names></name><name><surname>Schijen</surname><given-names>M</given-names></name><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An Excess of Positive Results: Comparing the Standard Psychology Literature with Registered Reports</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.31234/osf.io/p6e9c">https://doi.org/10.31234/osf.io/p6e9c</ext-link></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serghiou</surname><given-names>S</given-names></name><name><surname>Contopoulos-Ioannidis</surname><given-names>DG</given-names></name><name><surname>Boyack</surname><given-names>KW</given-names></name><name><surname>Riedel</surname><given-names>N</given-names></name><name><surname>Wallach</surname><given-names>JD</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Assessment of transparency indicators across the biomedical literature: How open is open?</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001107</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001107</pub-id><pub-id pub-id-type="pmid">33647013</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>JP</given-names></name><name><surname>Nelson</surname><given-names>LD</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title><source>Psychological Science</source><volume>22</volume><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soderberg</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Using OSF to Share Data: A Step-by-Step Guide</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>1</volume><fpage>115</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1177/2515245918757689</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Soderberg</surname><given-names>CK</given-names></name><name><surname>Errington</surname><given-names>T</given-names></name><name><surname>Schiavone</surname><given-names>SR</given-names></name><name><surname>Bottesini</surname><given-names>JG</given-names></name><name><surname>Thorn</surname><given-names>FS</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Esterling</surname><given-names>KM</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Research Quality of Registered Reports Compared to the Traditional Publishing Model</article-title><source>MetaArXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.31222/osf.io/7x9vy6">https://doi.org/10.31222/osf.io/7x9vy6</ext-link></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname><given-names>TD</given-names></name><name><surname>Doucouliagos</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Meta-regression approximations to reduce publication selection bias</article-title><source>Research Synthesis Methods</source><volume>5</volume><fpage>60</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1002/jrsm.1095</pub-id><pub-id pub-id-type="pmid">26054026</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steward</surname><given-names>O</given-names></name><name><surname>Popovich</surname><given-names>PG</given-names></name><name><surname>Dietrich</surname><given-names>WD</given-names></name><name><surname>Kleitman</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Replication and reproducibility in spinal cord injury research</article-title><source>Experimental Neurology</source><volume>233</volume><fpage>597</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1016/j.expneurol.2011.06.017</pub-id><pub-id pub-id-type="pmid">22078756</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stodden</surname><given-names>V</given-names></name><name><surname>Seiler</surname><given-names>J</given-names></name><name><surname>Ma</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An empirical analysis of journal policy effectiveness for computational reproducibility</article-title><source>PNAS</source><volume>115</volume><fpage>2584</fpage><lpage>2589</lpage><pub-id pub-id-type="doi">10.1073/pnas.1708290115</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teal</surname><given-names>TK</given-names></name><name><surname>Cranston</surname><given-names>KA</given-names></name><name><surname>Lapp</surname><given-names>H</given-names></name><name><surname>White</surname><given-names>E</given-names></name><name><surname>Wilson</surname><given-names>G</given-names></name><name><surname>Ram</surname><given-names>K</given-names></name><name><surname>Pawlik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Data Carpentry: Workshops to Increase Data Literacy for Researchers</article-title><source>International Journal of Digital Curation</source><volume>10</volume><fpage>135</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.2218/ijdc.v10i1.351</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>JC</given-names></name><name><surname>Biglan</surname><given-names>A</given-names></name><name><surname>Boruch</surname><given-names>RF</given-names></name><name><surname>Castro</surname><given-names>FG</given-names></name><name><surname>Collins</surname><given-names>LM</given-names></name><name><surname>Flay</surname><given-names>BR</given-names></name><name><surname>Kellam</surname><given-names>S</given-names></name><name><surname>Mościcki</surname><given-names>EK</given-names></name><name><surname>Schinke</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Replication in prevention science</article-title><source>Prevention Science</source><volume>12</volume><fpage>103</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1007/s11121-011-0217-6</pub-id><pub-id pub-id-type="pmid">21541692</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Calster</surname><given-names>B</given-names></name><name><surname>Wynants</surname><given-names>L</given-names></name><name><surname>Riley</surname><given-names>RD</given-names></name><name><surname>van Smeden</surname><given-names>M</given-names></name><name><surname>Collins</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Methodology over metrics: current scientific standards are a disservice to patients and society</article-title><source>Journal of Clinical Epidemiology</source><volume>138</volume><fpage>219</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.jclinepi.2021.05.018</pub-id><pub-id pub-id-type="pmid">34077797</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Naald</surname><given-names>M</given-names></name><name><surname>Wenker</surname><given-names>S</given-names></name><name><surname>Doevendans</surname><given-names>PA</given-names></name><name><surname>Wever</surname><given-names>KE</given-names></name><name><surname>Chamuleau</surname><given-names>SAJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Publication rate in preclinical research: a plea for preregistration</article-title><source>BMJ Open Science</source><volume>4</volume><elocation-id>e100051</elocation-id><pub-id pub-id-type="doi">10.1136/bmjos-2019-100051</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vines</surname><given-names>TH</given-names></name><name><surname>Albert</surname><given-names>AYK</given-names></name><name><surname>Andrew</surname><given-names>RL</given-names></name><name><surname>Débarre</surname><given-names>F</given-names></name><name><surname>Bock</surname><given-names>DG</given-names></name><name><surname>Franklin</surname><given-names>MT</given-names></name><name><surname>Gilbert</surname><given-names>KJ</given-names></name><name><surname>Moore</surname><given-names>JS</given-names></name><name><surname>Renaut</surname><given-names>S</given-names></name><name><surname>Rennison</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The availability of research data declines rapidly with article age</article-title><source>Current Biology</source><volume>24</volume><fpage>94</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.11.014</pub-id><pub-id pub-id-type="pmid">24361065</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Software Carpentry: lessons learned</article-title><source>F1000Research</source><volume>3</volume><elocation-id>62</elocation-id><pub-id pub-id-type="doi">10.12688/f1000research.3-62.v2</pub-id><pub-id pub-id-type="pmid">24715981</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The harmonic mean p -value for combining dependent tests</article-title><source>PNAS</source><volume>116</volume><fpage>1195</fpage><lpage>1200</lpage><pub-id pub-id-type="doi">10.1073/pnas.1814092116</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71601.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Pasqualini</surname><given-names>Renata</given-names></name><role>Reviewing Editor</role><aff><institution>Rutgers University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Investigating the Replicability of Cancer Biology&quot; to <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by an <italic>eLife</italic> Reviewing Editor (Renata Pasqualini), an <italic>eLife</italic> Senior Editor (Eduardo Franco), and the <italic>eLife</italic> Features Editor (Peter Rodgers). The following individuals involved in review of your submission have agreed to reveal their identity: Paul Glasziou (Reviewer #1); Malcolm MacLeod (Reviewer #2); Tudor Oprea (Reviewer #3).</p><p>The reviewers and editors have discussed the reviews and we have drafted this decision letter to help you prepare a revised submission.</p><p>Summary</p><p>This is an excellent summary of a landmark set of replication research studies which builds on the large-scale replication studies done in psychology and extends them to cancer biology. The authors have replicated 50 important experiments published in high impact Cancer biology papers and have done so in both a rigorous and transparent way. These important findings should trigger a rethink in how we do cancer biology research and indeed all research. Of particular note is their finding that &quot;original null results were twice as likely to mostly replicate successfully (80%) as original positive results (40%).&quot; However, there are a number of points that need to be addressed to make the article suitable for publication. The presentation of the results could also be improved.</p><p>Essential revisions</p><p>1. The violin plots (eg Figure 1) do not indicate the number of experiments contributing to each panel. Eg for Original null papers, I think n=11 … yet the violin plot implies greater precision in the shape of the distribution. Is there a way to fix this? And are quantiles best described as &quot;25&quot;,&quot;50&quot; etc or Q1, Q2 …</p><p>2. The choice of ES measure will introduce error when n is small. You say that you re-do the analysis with native data, but in the supplementary table the n for native data appears to be the same as for the variance based measure. If so, why not just use the native data? If not, can you clarify? I may have missed something …</p><p>3. For the ~30 animal studies which were included, could you report the proportion that were randomised, blinded etc, and report whether this was associated with differences in observed replication rates? I was going to do this myself, but thought you'd rather have the review sooner!</p><p>4. Figure 2: The distribution curves seem wrong – it appears (to my eye) that these include some points from the full dataset which are not included in the panel (because of cropping axes), and this distracted me. For instance, replication effect size p&gt;0.05 there are two &quot;bumps&quot; above the central spike, only one of which appears to relate to a point shown on the graph.</p><p>5. Interquartile ranges are offered as a number rather than a range … and if the distribution is not symmetrical, this is losing information.</p><p>6. Line 59 – some would quibble that knowing that lab does indeed change your confidence in the reported findings. That is, a lab with a track record of getting things wrong should not start from the same position of credibility as the obverse.</p><p>7. Line 62 – can you define validity?</p><p>8. Line 67. &quot;successive replications …&quot; a faithful replication will presumably share the same confounding influences, if these are part of the design (ie features not bugs). But I agree successive replications do reduce the possibility that the findings are due to some latent confounding variable.</p><p>9. Line 70 et seq: I liked this, but I wonder if the component &quot;misunderstanding or underdeveloped theory&quot; will land with life scientists unless more richly described.</p><p>10. Line 179: I think this depends on your scale – if it is &quot;detected&quot; or &quot;not detected&quot; and you have very low sensitivity, most will be nil. Of course, the chances that the underlying biology does not change one iota is close to zero.</p><p>11. Measures of replication – is the most relevant not the question of whether the ES(rep) lies within the 95% prediction interval of the first experiment? I know that this &quot;advantages&quot; imprecise originator experiments, but we take that into account when we use the information therein. This is somewhat related to Hanno Wuerbul's claim that heterogenous experiments are better, because they are better predictors of what will happen in a further experiment – largely, but not exclusively, because their prediction intervals tend to be wider.</p><p>12. The authors write: &quot; After conducting dozens of replications, we can declare a definitive understanding of precisely zero of the original findings.&quot;</p><p>– In my opinion, there should have been more consequences. For example, those papers that failed the reproducibility test. Shouldn't there be retraction notices? Or at least, &quot;expressions of concern&quot; to the Publishers? If there is no real consequence, the only thing we learn is that there's monkey business in science. [This point is taken from the report of Tudor Oprea: please see below for the full report].</p><p>Please comment on what you think authors and journals should do when a paper they have published fails to replicate.</p><p>Points related to presentation</p><p>A. Figures 1, 2, and 3 are the P-value densities for the original and replication experiments which is extremely informative, but I would prefer the axis for the p-values to the more conventional x-axis. This may be a personal preference, but I found rotating the figure by 90 degrees made it much easier to read – please consider making this change.</p><p>B. It is not clear that figure 2B adds anything to the article – please consider deleting this panel.</p><p>C. Figure 5 (and similar figure supplements) should use the same scaling on both axes so that the diagonal &quot;equality&quot; line is at 45 degrees.</p><p>D. Tables 1, 2, and 3 set out the results in great detail, but are very dense with numbers. I wondered if these might be better as a hybrid figure/table (which can include the raw numbers inside the bars of the boxes or histograms). For example, see – https://stackoverflow.com/questions/6644997/showing-data-values-on-stacked-bar-chart-in-ggplot2.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71601.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions</p><p>1. The violin plots (eg Figure 1) do not indicate the number of experiments contributing to each panel. Eg for Original null papers, I think n=11 … yet the violin plot implies greater precision in the shape of the distribution. Is there a way to fix this? And are quantiles best described as &quot;25&quot;,&quot;50&quot; etc or Q1, Q2 …</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>2. The choice of ES measure will introduce error when n is small. You say that you re-do the analysis with native data, but in the supplementary table the n for native data appears to be the same as for the variance based measure. If so, why not just use the native data? If not, can you clarify? I may have missed something …</p></disp-quote><p>We look at the data two ways: one where we keep the native effect size type for each individual effect and one where we convert effect size types to a standardized mean difference (SMD) scale. The sample size n does not change during these, rather we are converting effect sizes to a SMD scale (e.g., hazard ratios to an SMD scale as described in Hasselblad and Hedges, 1995). We report both as conversions can potentially distort the results; however, we find the outcome is similar regardless of which approach is used. The reason we report the SMD scale in the main results is to keep all tables, figures, etc comparable to each other since some tables (Table 3, 5, and 6) and figures (Figures 2, 3, and 5) include effect sizes that are reported relative to each other, which can only be done when effect sizes are on the SMD scale.</p><disp-quote content-type="editor-comment"><p>3. For the ~30 animal studies which were included, could you report the proportion that were randomised, blinded etc, and report whether this was associated with differences in observed replication rates? I was going to do this myself, but thought you'd rather have the review sooner!</p></disp-quote><p>The reviewer rightly points out that randomization, blinding, and sample size planning are important factors that have been identified as potentially improving replicability of findings. Unfortunately, in this case, there is not enough variation in the original findings to examine these as potential moderators. For the 36 animal effects across 15 experiments, none reported blinding, none reported determining sample size a priori, and one experiment (for two effects) reported randomization.</p><p>For completeness sake, blinding and randomization were not reported for all replication studies. For the 36 animal effects, 11 reported blinding (5 of 15 experiments), 28 reported randomization (13 of 15 experiments), and all 36 (15 of 15 experiments) reported determining sample size a priori. We added a brief description of these data at the end of the second paragraph of the section “Comparing animal versus non-animal experiments”.</p><disp-quote content-type="editor-comment"><p>4. Figure 2: The distribution curves seem wrong – it appears (to my eye) that these include some points from the full dataset which are not included in the panel (because of cropping axes), and this distracted me. For instance, replication effect size p&gt;0.05 there are two &quot;bumps&quot; above the central spike, only one of which appears to relate to a point shown on the graph.</p></disp-quote><p>Thank you for catching this. These have been fixed so the distribution curves are only for the cropped data.</p><disp-quote content-type="editor-comment"><p>5. Interquartile ranges are offered as a number rather than a range … and if the distribution is not symmetrical, this is losing information.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>6. Line 59 – some would quibble that knowing that lab does indeed change your confidence in the reported findings. That is, a lab with a track record of getting things wrong should not start from the same position of credibility as the obverse.</p></disp-quote><p>We agree that, in practice, researchers can gain a reputation for high or low credibility research. We refer here to scientific ideals and provide the qualifier “supposed to” for the quibble:</p><p>“Credibility of research claims are not supposed to be contingent on the reputation of their originator; credibility is supposed to be based on the quality of the evidence itself, including replicability.”</p><disp-quote content-type="editor-comment"><p>7. Line 62 – can you define validity?</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>8. Line 67. &quot;successive replications …&quot; a faithful replication will presumably share the same confounding influences, if these are part of the design (ie features not bugs). But I agree successive replications do reduce the possibility that the findings are due to some latent confounding variable.</p></disp-quote><p>Yes, we agree. We added “But successive replications may never eliminate some confounding influences or invalidity in design.” to the end of the paragraph.</p><disp-quote content-type="editor-comment"><p>9. Line 70 et seq: I liked this, but I wonder if the component &quot;misunderstanding or underdeveloped theory&quot; will land with life scientists unless more richly described.</p></disp-quote><p>We elaborated this statement to say: “It is possible that the methodology necessary to produce the evidence is not sufficiently defined or understood. This could mean that the original authors misunderstand what is necessary or sufficient to observe the finding, or that the theoretical explanation for why the finding occurred is incorrect.”</p><disp-quote content-type="editor-comment"><p>10. Line 179: I think this depends on your scale – if it is &quot;detected&quot; or &quot;not detected&quot; and you have very low sensitivity, most will be nil. Of course, the chances that the underlying biology does not change one iota is close to zero.</p></disp-quote><p>Yes, it is possible to have low sensitivity measures that detect no variation at all even if there might be some variation that is undetectable. This does not have direct implications for our use of “nil” here to refer to literally zero because we observed variation in measurements for virtually all outcome variables.</p><disp-quote content-type="editor-comment"><p>11. Measures of replication – is the most relevant not the question of whether the ES(rep) lies within the 95% prediction interval of the first experiment? I know that this &quot;advantages&quot; imprecise originator experiments, but we take that into account when we use the information therein. This is somewhat related to Hanno Wuerbul's claim that heterogenous experiments are better, because they are better predictors of what will happen in a further experiment – largely, but not exclusively, because their prediction intervals tend to be wider.</p></disp-quote><p>Patil et al., (2016) agree with this argument, but there is substantial disagreement among methodologists about the best metric for interpreting replication success. For example, with extremely high precision tests, it becomes very easy to fail to replicate using prediction intervals even when the effect is observed reliably and precisely and is just a bit smaller or larger than the original. For many research applications, a bit of variation in overall effect size would not be consequential for theoretical interpretation. In any case, our position is that there is no singular indicator of replication success--which is preferred is somewhat dependent on setting a liberal versus conservative criterion, and somewhat dependent on which specific question one is aiming to answer. As such, we present a variety of outcome criteria and avoid asserting that one is the best.</p><disp-quote content-type="editor-comment"><p>12. The authors write: &quot; After conducting dozens of replications, we can declare a definitive understanding of precisely zero of the original findings.&quot;</p></disp-quote><p>In my opinion, there should have been more consequences. For example, those papers that failed the reproducibility test. Shouldn't there be retraction notices? Or at least, &quot;expressions of concern&quot; to the Publishers? If there is no real consequence, the only thing we learn is that there's monkey business in science. [This point is taken from the report of Tudor Oprea: please see below for the full report].</p><disp-quote content-type="editor-comment"><p>Please comment on what you think authors and journals should do when a paper they have published fails to replicate.</p></disp-quote><p>We don’t believe that a failure to replicate on its own requires any action by the journal. The primary reason we believe that is because of the selected quote. A single replication study is not (maybe we could agree to ‘rarely’) a definitive statement of the confidence we should or should not have in the original finding. As much as we attempted to conduct rigorous, high-powered, transparent replication studies, it is possible that something went wrong and the original study findings are replicable under other circumstances. So, our position is that a failure to replicate should prompt a closer look and, often, additional investigation to demonstrate replicability if researchers perceive possible improvements that could establish it.</p><p>Failures to replicate do increase our scepticism of the original findings, but if journals are perceived as definitive arbiters of truth - accepted if replicable, retracted if unreplicable - it would foster further dysfunction in the research culture because individual studies do not provide definitive evidence. A more functional approach by journals would be to embrace uncertainty of every paper and publish replication studies and accumulation of evidence to support resolving uncertainty rather than demand innovation in every publication.</p><p>More generally, we do appreciate Tudor Opera’s comments about the need to improve self-correction in the literature. However, we believe that the impact of this particular paper will be strongest with a focus on the general implication of unreplicability rather than focusing on the replicability of the particular papers and findings that happened to be sampled.</p><disp-quote content-type="editor-comment"><p>Points related to presentation</p><p>A. Figures 1, 2, and 3 are the p-value densities for the original and replication experiments which is extremely informative, but I would prefer the axis for the p-values to the more conventional x-axis. This may be a personal preference, but I found rotating the figure by 90 degrees made it much easier to read – please consider making this change.</p></disp-quote><p>Figures 1 (p-value densities) and 3 (effect size densities) were flipped so the values are along the x-axis opposed to the y-axis.</p><disp-quote content-type="editor-comment"><p>B. It is not clear that figure 2B adds anything to the article – please consider deleting this panel.</p></disp-quote><p>Panel 2A shows most of the entire distribution of effect sizes. As a consequence, it is visually dominated by the most extreme original effect sizes. Figure 2B zooms in on where most of the original and replication studies effect sizes occurred so that the reader can perceive the pattern of decline more clearly where most of the data are. We find it useful for our own understanding of the findings and anticipate that some readers will likewise.</p><disp-quote content-type="editor-comment"><p>C. Figure 5 (and similar figure supplements) should use the same scaling on both axes so that the diagonal &quot;equality&quot; line is at 45 degrees.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>D. Tables 1, 2, and 3 set out the results in great detail, but are very dense with numbers. I wondered if these might be better as a hybrid figure/table (which can include the raw numbers inside the bars of the boxes or histograms). For example, see – https://stackoverflow.com/questions/6644997/showing-data-values-on-stacked-bar-chart-in-ggplot2.</p></disp-quote><p>Thank you for this suggestion. We kept each table as is for full transparency and have included a new figure (Figure 7) which summarizes the main findings from Table 7 for effects.</p></body></sub-article></article>