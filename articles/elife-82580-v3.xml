<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82580</article-id><article-id pub-id-type="doi">10.7554/eLife.82580</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-99408"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7257-428X</contrib-id><email>hebart@cbs.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-290596"><name><surname>Contier</surname><given-names>Oliver</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2983-4709</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-225310"><name><surname>Teichmann</surname><given-names>Lina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8040-5686</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-290593"><name><surname>Rockter</surname><given-names>Adam H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2446-717X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-203643"><name><surname>Zheng</surname><given-names>Charles Y</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-290594"><name><surname>Kidder</surname><given-names>Alexis</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-290595"><name><surname>Corriveau</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-258535"><name><surname>Vaziri-Pashkam</surname><given-names>Maryam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1830-2501</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-28129"><name><surname>Baker</surname><given-names>Chris I</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6861-8964</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf3"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Vision and Computational Cognition Group, Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Department of Medicine, Justus Liebig University Giessen</institution></institution-wrap><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck School of Cognition, Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>Machine Learning Core, National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82580</elocation-id><history><date date-type="received" iso-8601-date="2022-08-09"><day>09</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-02-25"><day>25</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-07-23"><day>23</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.22.501123"/></event></pub-history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82580-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82580-figures-v3.pdf"/><abstract><p>Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-data, a multimodal collection of large-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-data is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-driven and data-driven applications. THINGS-data constitutes the core public release of the THINGS initiative (<ext-link ext-link-type="uri" xlink:href="https://things-initiative.org">https://things-initiative.org</ext-link>) for bridging the gap between disciplines and the advancement of cognitive neuroscience.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fMRI</kwd><kwd>MEG</kwd><kwd>behavior</kwd><kwd>research data</kwd><kwd>objects</kwd><kwd>vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIA-MH-002909</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIC-MH002968</award-id><principal-award-recipient><name><surname>Zheng</surname><given-names>Charles Y</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><award-id>Max Planck Research Group M.TN.A.NEPF0009</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Starting Grant StG-2021-101039712</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003495</institution-id><institution>Hessisches Ministerium für Wissenschaft und Kunst</institution></institution-wrap></funding-source><award-id>LOEWE Start Professorship</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100018668</institution-id><institution>Max Planck School of Cognition</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Contier</surname><given-names>Oliver</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003495</institution-id><institution>Hessisches Ministerium für Wissenschaft und Kunst</institution></institution-wrap></funding-source><award-id>Tha Adaptive Mind</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. Open access funding provided by Max Planck Society.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>THINGS-data reflects three large-scale neuroimaging and behavioral datasets of object processing in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1854 objects.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A central goal of cognitive neuroscience is to attain a detailed characterization of the recognition and understanding of objects in the world. Over the past few decades, there has been tremendous progress in revealing the basic building blocks of human visual and semantic object processing. For example, numerous functionally selective clusters have been identified in ventral and lateral occipitotemporal cortex that respond selectively to images of faces, scenes, objects, or body parts (<xref ref-type="bibr" rid="bib33">Downing et al., 2001</xref>; <xref ref-type="bibr" rid="bib36">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="bib70">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib88">Malach et al., 1995</xref>). Likewise, several coarse-scale gradients have been revealed that span across these functionally selective regions and that reflect low-level visual properties such as eccentricity or curvature (<xref ref-type="bibr" rid="bib3">Arcaro et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="bib126">Yue et al., 2020</xref>), mid-to-high-level properties such as animacy or size (<xref ref-type="bibr" rid="bib19">Caramazza and Shelton, 1998</xref>; <xref ref-type="bibr" rid="bib77">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib76">Konkle and Oliva, 2012</xref>; <xref ref-type="bibr" rid="bib81">Kriegeskorte et al., 2008b</xref>), or high-level semantics (<xref ref-type="bibr" rid="bib64">Huth et al., 2012</xref>). These results have been complemented by studies in the temporal domain, revealing a temporal cascade of object-related responses that become increasingly invariant over time to visually specific features such as size and position (<xref ref-type="bibr" rid="bib67">Isik et al., 2014</xref>), that reflect differences between visual and more abstract semantic properties (<xref ref-type="bibr" rid="bib6">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Clarke et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Clarke et al., 2015</xref>), and that reveal the dynamics of feedforward and feedback processing (<xref ref-type="bibr" rid="bib13">Boring et al., 2022</xref>; <xref ref-type="bibr" rid="bib73">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib92">Mohsenzadeh et al., 2018</xref>). These spatial and temporal patterns of object-related brain activity have been linked to categorization behavior (<xref ref-type="bibr" rid="bib50">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib107">Ritchie et al., 2015</xref>) and perceived similarity (<xref ref-type="bibr" rid="bib6">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="bib93">Mur et al., 2013</xref>), indicating their direct relevance for overt behavior.</p><p>Despite these advances, our general understanding of the processing of visually-presented objects has remained incomplete. One major limitation stems from the enormous variability of the visual world and the thousands of objects that we can identify and distinguish (<xref ref-type="bibr" rid="bib11">Biederman, 1985</xref>; <xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>). Different objects are characterized by a large and often correlated set of features (<xref ref-type="bibr" rid="bib46">Groen et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Naselaris et al., 2021</xref>), making it challenging to determine the overarching properties that govern the representational structure in visual cortex and behavior. A more complete understanding of visual and semantic object processing will almost certainly require a high-dimensional account (<xref ref-type="bibr" rid="bib97">Naselaris et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Haxby et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>; <xref ref-type="bibr" rid="bib85">Lehky et al., 2014</xref>), which is impossible to derive from traditional experiments that are based only on a small number of stimuli or a small number of categories. Likewise, even large-scale datasets remain limited in the insights they can yield about object representations when they lack a systematic sampling of object categories and images.</p><p>To overcome these limitations, here we introduce THINGS-data, which consists of three multimodal large-scale datasets of brain and behavioral responses to naturalistic object images. There are three key aspects of THINGS-data that maximize its utility and set it apart from other large-scale datasets using naturalistic images (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="bib63">Horikawa and Kamitani, 2017</xref>; <xref ref-type="bibr" rid="bib71">Kay et al., 2008</xref>). First, THINGS-data is unique in that it offers a broad, comprehensive and systematic sampling of object representations for up to 1854 diverse nameable manmade and natural object concepts. This is in contrast to previous large-scale neuroimaging datasets that focused primarily on dataset size, not sampling, and that often contain biases towards specific object categories (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>). Second, THINGS-data is multimodal, containing functional MRI, magnetoencephalography (MEG) and behavioral datasets allowing analyses of both the spatial patterns and temporal dynamics of brain responses (<xref ref-type="bibr" rid="bib39">Ghuman and Martin, 2019</xref>) as well as their relationship to behavior. In particular, THINGS-data comes with 4.70 million behavioral responses that capture the perceived similarity between objects with considerable detail and precision. Third, the THINGS database of object concepts and images (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>) comes with a growing body of rich annotations and metadata, allowing for direct comparisons of representations across domains, an extension to other methods and species (<xref ref-type="bibr" rid="bib80">Kriegeskorte et al., 2008a</xref>), streamlined incorporation of computational modeling frameworks (<xref ref-type="bibr" rid="bib82">Kriegeskorte and Douglas, 2018</xref>), and direct testing of diverse hypotheses on these large-scale datasets.</p><p>In this paper, we provide a detailed account of all aspects of THINGS-data, from acquisition and data quality checks to exemplary analyses demonstrating the potential utility of the data. These exemplary analyses primarily serve to highlight potential research directions that could be explored with these data. In addition, the analyses of the neuroimaging data reveal high reliability of findings across individual participants, underscoring the utility of densely sampling a small number of individuals. Finally, they replicate a large number of research findings, suggesting that these data can be used for revealing new insights into visual and semantic processing in human brain and behavior. We expect that THINGS-data will serve as an important resource for the community, enabling novel analyses to provide significant insights into visual object processing as well as validation and extension of existing findings. THINGS-data reflects the core release of datasets as part of the THINGS initiative (<ext-link ext-link-type="uri" xlink:href="https://things-initiative.org">https://things-initiative.org</ext-link>), which will provide numerous multimodal and multispecies behavioral, neurophysiology, and neuroimaging datasets based on the same images, offering an important general resource that bridges the gap between disciplines for the advancement of the cognitive neurosciences.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A multimodal collection of datasets of object representations in brain and behavior</title><p>We collected three datasets that extensively sampled object representations using functional MRI (fMRI), magnetoencephalography (MEG), and behavior (<xref ref-type="fig" rid="fig1">Figure 1</xref>). To this end, we drew on the THINGS database (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>), a richly-annotated database of 1854 object concepts representative of the American English language which contains 26,107 manually curated naturalistic object images. The comprehensive set of object categories, the large number of high-quality naturalistic images, and the rich set of semantic and image annotations make THINGS ideally suited for the large-scale collection of imaging and behavioral datasets.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview over datasets.</title><p>(<bold>A</bold>) THINGS-data comprises MEG, fMRI and behavioral responses to large samples of object images taken from the THINGS database. (<bold>B</bold>) In the fMRI and MEG experiment, participants viewed object images while performing an oddball detection task (synthetic image). (<bold>C</bold>) The behavioral dataset comprises human similarity judgements from an odd-one-out task where participants chose the most dissimilar object amongst three options. (<bold>D</bold>) The fMRI dataset contains extensive additional imaging data. (<bold>E</bold>) The MEG dataset provides high temporal resolution of neural response measurements in 272 channels. The butterfly plot shows the mean stimulus-locked response in each channel for four example sessions in one of the participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig1-v3.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Effects of ICA denoising on fMRI noise ceiling estimates, for all three fMRI participants.</title><p>Each data point represents a voxel in a visual mask determined based on the localizer experiment. The x-axis shows the test data noise ceiling in % explainable variance after standard preprocessing. The y-axis shows the respective noise ceiling when the data is additionally denoised with the ICA noise components. All voxels falling above the diagonal show an improved noise ceiling due to ICA denoising.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig1-figsupp1-v3.tif"/></fig></fig-group><p>During the fMRI and MEG experiments, participants were shown a representative subset of THINGS images, spread across 12 separate sessions (fMRI: N=3, 8740 unique images of 720 objects; MEG: N=4, 22,448 unique images of 1854 objects). Images were shown in fast succession (fMRI: 4.5 s; MEG: 1.5±0.2 s; <xref ref-type="fig" rid="fig1">Figure 1B</xref>), and participants were instructed to maintain central fixation. Please note that for the MEG and fMRI experiments, we chose non-overlapping sets of participants to ensure they had not seen individual images before and thus to minimize potential memory effects on measured object representations. To ensure engagement, participants performed an oddball detection task responding to occasional artificially-generated images. A subset of images (fMRI: n=100; MEG: n=200) were shown repeatedly in each session to estimate noise ceilings (<xref ref-type="bibr" rid="bib84">Lage-Castellanos et al., 2019</xref>) and to provide a test set for model evaluation (see Appendix 1 for details on the concept and image selection strategy).</p><p>Beyond the core functional imaging data in response to THINGS images, additional structural and functional imaging data as well as eye-tracking and physiological responses were gathered. Specifically, for MEG, we acquired T1-weighted MRI scans to allow for cortical source localization. Eye movements were monitored in the MEG to ensure participants maintained central fixation (see Appendix 2 and <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> for extensive eye-movement related analyses). For MRI, we collected high-resolution anatomical images (T1- and T2-weighted), measures of brain vasculature (Time-of-Flight angiography, T2*-weighted), and gradient-echo field maps. In addition, we ran a functional localizer to identify numerous functionally specific brain regions, a retinotopic localizer for estimating population receptive fields, and an additional run without external stimulation for estimating resting-state functional connectivity. Finally, each MRI session was accompanied by physiological recordings (heartbeat and respiration) to support data denoising. Based on these additional data, we computed a variety of data derivatives for users to refine their analyses. These derivatives include cortical flatmaps which allow for visualizing statistical results on the entire cortical surface (<xref ref-type="bibr" rid="bib38">Gao et al., 2015</xref>), independent-component based noise regressors which can be used for improving the reliability of obtained results, regions of interest for category-selective and early visual brain areas which allow for anatomically-constrained research questions, and estimates of retinotopic parameters, such as population receptive field location and size.</p><p>THINGS-data also includes 4.70 million human similarity judgements collected via online crowdsourcing for 1854 object images. In a triplet odd-one-out task, participants (N=12,340) were presented with three objects from the THINGS database and were asked to indicate which object is the most dissimilar. The triplet odd-one-out task assesses the similarity of two objects in the context imposed by a third object. With a broad set of objects, this offers a principled approach for measuring context-independent perceived similarity with minimal response bias, but also allows for estimating context-dependent similarity, for example by constraining similarity to specific superordinate categories, such as animals or vehicles. An initial subset of 1.46 million of these odd-one-out judgments were reported in previous work (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>; <xref ref-type="bibr" rid="bib127">Zheng et al., 2019</xref>), and the 4.70 million trials reported here represent a substantial increase in dataset size and the ability to draw inferences about fine-grained similarity judgments. Beyond dataset size, two notable additions are included. First, we collected age information, providing a cross-sectional sample for how mental representations may change with age. Second, we collected a set of 37,000 within-subject triplets to estimate variability at the subject level. Taken together, the behavioral dataset provides a massive set of perceived similarity judgements of object images and can be linked to neural responses measured in MEG and fMRI, opening the door to studying the neural processes underlying perceived similarity at scale, for a wide range of objects.</p><p>The remaining results section will be structured as follows: We will first describe the quality and reliability of both neuroimaging datasets, followed by the description of the quality of the behavioral dataset. Then, we will showcase the validity and suitability of the datasets for studying questions about behavioral and neural object representations. This will include multivariate pairwise decoding of hundreds of object categories, encoding analyses serving as a large-scale replication of the animacy and size organization in occipitotemporal cortex, representational similarity analysis of patterns of brain activity and perceived similarity, and a novel MEG-fMRI fusion approach based on directly regressing MEG responses onto fMRI voxel activation patterns.</p></sec><sec id="s2-2"><title>Data quality and data reliability in the fMRI and MEG datasets</title><p>To be useful for addressing diverse research questions, we aimed at providing neuroimaging datasets with excellent data quality and high reliability. To reduce variability introduced through head motion and alignment between sessions, fMRI and MEG participants wore custom head casts throughout all sessions. <xref ref-type="fig" rid="fig2">Figure 2</xref> demonstrates that overall head motion was, indeed, very low in both neuroimaging datasets. In the fMRI dataset, the mean framewise displacement per run was consistently below 0.2 mm. In the MEG, head position was recorded between runs and showed consistently low head motion for all participants during sessions (median &lt;1.5 mm). Between sessions, changes in MEG head position were slightly higher but remained overall low (median &lt;3 mm). A visual comparison of the evoked responses for each participant across sessions in different sensor groups highlights that the extent of head motion we observed does not appear to be detrimental for data quality (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Quality metrics for fMRI and MEG datasets.</title><p>fMRI participants are labeled F1-F3 and MEG participants M1-M4 respectively. (<bold>A</bold>) Head motion in the fMRI experiment as measured by the mean framewise displacement in each functional run of each participant. (<bold>B</bold>) Median change in average MEG head coil position as a function of the Euclidean distance of all pairwise comparisons between all runs. Results are reported separately for comparisons within sessions and between sessions (see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> for all pairwise distances). (<bold>C</bold>) fMRI voxel-wise noise ceilings in the test dataset as an estimate of explainable variance visualized on the flattened cortical surface. The labeled outlines show early visual (V1–V3) and category-selective brain regions identified based on the population receptive field mapping and localizer data, respectively. (<bold>D</bold>) MEG time-resolved noise ceilings similarly show high reliability, especially for occipital, parietal, and temporal sensors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Event-related fields for occipital, temporal, and parietal sensors.</title><p>After preprocessing, event-related fields were calculated for each participant (columns 1–4). Every row shows a different sensor group, as depicted in column 5. Thin lines correspond to the average response to the 200 test images per session, while the thick line corresponds to the average across sessions. The high consistency in the evoked signal highlights the comparability of the data between sessions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig2-figsupp1-v3.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>MEG noise ceilings for all sensors.</title><p>Every column shows the noise ceiling for a given participant. The last column highlights which sensors were considered for each sensor group (row). Noise ceilings were calculated for each sensor (thin lines) and averaged across sensors in a given sensor group (thick line).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig2-figsupp2-v3.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>fMRI voxel-wise noise ceilings per participant projected onto the flattened cortical surface.</title><p>(<bold>A</bold>) The noise ceiling estimate on the level of single trial responses. (<bold>B</bold>) Noise ceiling estimate in the test dataset where responses from 12 trial repetitions can be averaged. Note that the range of noise ceiling values represented by the color map is higher for the test dataset (0–80%) compared to the single trial responses (0–20%).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig2-figsupp3-v3.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Head coil positioning across runs in the MEG experiment.</title><p>Head position was recorded with three marker coils attached at the nasion, left preauricular, and right preauricular. The coil positions were recorded before and after each run. To calculate the distance between runs and sessions, we took the mean of pre- and post-run measurements and calculated the Euclidean distance between all pairs of run measurements. Runs with failed localization were excluded. Overall, head coil positioning was consistent across sessions and runs. However, for participant M4 there were two sessions (S4 and S5) where the left marker coil may not have been attached at the same location as in other sessions, evidenced by low within-session and high between-session distances. Additionally, there may have been a failed measurement in session 8, characterized by large distances to all other measurements. While this indicates that head motion estimates we provided in the main text are rather conservative, researchers should be careful when using the head coil localization from these runs (e.g. for source reconstruction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig2-figsupp4-v3.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Example visualization used for the manual labeling of independent components.</title><p>For the ICA-based denoising, two raters manually labeled a subset of all independent components as signal or noise based on these visualizations. For the depicted example component, both raters labeled it as a noise component related to head motion. The top two rows show the spatial map (thresholded at 0.9) of the independent component overlayed on the mean functional image of that run. The frequency spectrum (third row left) was presented alongside the high-frequency content. The remaining plots show the expected time course of the experimental design (green), physiological noise (blue), and head motion related noise (orange) alongside the time course of the independent component (black) as well as the correlation between the component’s and these expected time courses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig2-figsupp5-v3.tif"/></fig></fig-group><p>To further improve fMRI data quality and provide easily usable data, we conducted two additional processing steps. First, since fMRI data contains diverse sources of noise including head motion, pulse, heartbeat, and other sources of physiological and scanner-related noise, we developed a custom denoising method based on independent component analysis (<xref ref-type="bibr" rid="bib8">Beckmann and Smith, 2004</xref>), which involved hand-labeling a subset of components and a set of simple heuristics to separate signal from noise components (see Methods for details). This approach yielded strong and consistent improvements in the reliability of single trial BOLD response estimates (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Second, we estimated the BOLD response amplitude to each object image by fitting a single-trial regularized general linear model on the preprocessed fMRI time series with voxel-specific estimates of the HRF shape (see Methods). Together, these methods yielded much higher data reliability and provided a format that is much smaller than the original time series and that is amenable to a wider range of analysis techniques, including data-driven analyses. This reduced set of BOLD parameter estimates is used for all analyses showcased in this manuscript and is part of the publicly available data (see Data availability).</p><p>To provide a quantitative assessment of the reliability of the fMRI and MEG datasets, we computed noise ceilings. Noise ceilings are defined as the maximum performance any model can achieve given the noise in the data (<xref ref-type="bibr" rid="bib84">Lage-Castellanos et al., 2019</xref>) and are based on the variability across repeated measurements. Since noise ceiling estimates depend on the number of trials averaged in a given analysis, we estimated them separately for the 12 trial repeats of the test set and for single trial estimates. Noise ceilings in the test set were high (<xref ref-type="fig" rid="fig2">Figure 2</xref>), with up to 80% explainable variance in early visual cortex for fMRI (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) and up to 70% explainable variance in MEG (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Individual differences between participants indicated that performance was particularly high for fMRI participants F1 and F2 and MEG participants M2 and M3 but qualitatively similar for all participants. For single-trial estimates, as expected, noise ceilings were lower and varied more strongly across participants (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). This suggests that these neuroimaging datasets are ideally suited for analyses that incorporate estimates across multiple trials, such as encoding or decoding models or data-driven analyses at the level of object concepts.</p></sec><sec id="s2-3"><title>Data quality and data reliability in the behavioral odd-one out dataset: A 66-dimensional embedding captures fine-grained perceived similarity judgments</title><p>To achieve a full estimate of a behavioral similarity matrix for all 1854 objects, we would have to collect 1.06 billion triplet odd-one-out judgments. We previously demonstrated (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>) that 1.46 million trials were sufficient to generate a sparse positive similarity embedding (SPoSE) (<xref ref-type="bibr" rid="bib127">Zheng et al., 2019</xref>) that approached noise ceiling in predicting choices in left-out trials and pairwise similarity. SPoSE yielded 49 interpretable behavioral dimensions reflecting perceptual and conceptual object properties (e.g. colorful, animal-related) and thus identified what information may be used by humans to judge the similarity of objects in this task. Yet, several important questions about the general utility of these data could not be addressed with this original dataset.</p><p>First, how much data is enough to capture the core dimensions underlying human similarity judgments? Previously, we had shown that performance of our embedding at predicting triplet choices had saturated even with the original 1.46 million trials, yet dimensionality continued to increase with dataset size (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>). Before collecting additional data and using different subsets of the original dataset, we estimated that model dimensionality would saturate around 67.5 dimensions and would reach ~66.5 dimensions for 4.5–5 million trials (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Indeed, when re-running the model with the full dataset of 4.70 million trials (4.10 million for training), embedding dimensionality turned out as predicted: from a set of 72 randomly-initialized models, we chose the most reliable embedding as the final embedding, revealing 66 interpretable dimensions underlying perceived similarity judgments (see Methods for details). Thus, increasing dataset size beyond this large dataset may no longer yield noticeable improvements in predictive performance or changes in embedding dimensionality at the global level of similarity, and potential improvements may not justify the cost of collecting additional data. Thus, rather than continuing to increase dataset size, future research on representational object dimensions may focus more strongly on individual differences, within category similarity, different sensory domains, or abstracted stimuli.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Behavioral similarity dataset.</title><p>(<bold>A</bold>) How much data is required to capture the core representational dimensions underlying human similarity judgments? Based on the original dataset of 1.46 million triplets (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>), it was predicted that around 4.5–5 million triplets would be required for the curve to saturate. Indeed, for the full dataset, the dimensionality was found to be 66, in line with the extrapolation. Red bars indicate histograms for dimensionality across several random model initializations, while the final model was chosen to be the most stable among this set. (<bold>B</bold>) Within-category pairwise similarity ratings were predicted better for diverse datasets using the new, larger dataset of 4.70 million triplets (4.10 million training samples), indicating that this dataset contains more fine-grained similarity information. Error bars reflect standard errors of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Changes in embedding dimensions between original embedding (49 dimensions) and the new embedding (66 dimensions) based on the full dataset.</title><p>Lines correspond to Pearson correlations between old and new dimensions, only showing cases with <italic>r</italic>&gt;0.3 for dimensions that already have a strong pairing (e.g. ‘artificial/hard’ with ‘metallic/artificial’) and <italic>r</italic>&gt;0.3 for dimensions without a strong pairing (after correcting for baseline cross-correlation between the original 49 dimensions). These cutoffs were chosen arbitrarily to provide a trade-off between maximizing the information contained in this figure while still effectively visualizing changes in dimensions. 46 out of 49 original dimensions showed strong correlations with new dimensions (all <italic>r</italic>&gt;0.63), demonstrating that the original embedding was reproduced well. In addition, several dimensions were split up, either revealing more fine-grained distinctions (e.g. ‘sweet/dessert’ rather than ‘food’), disentangling dimensions further (e.g. ‘plant-related/green’ to separate dimensions for ‘plant-related’ and ‘green’), or sometimes remixing them (e.g. ‘tool-related’ and ‘long/thin’ led to ‘pointed/spiky’). Finally, there were a number of dimensions that previously had not been found and also showed no strong relationship to previous dimensions (e.g. ‘fluffy/soft’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig3-figsupp1-v3.tif"/></fig></fig-group><p>In the final 66-dimensional embedding, many dimensions were qualitatively very similar to the original 49 dimensions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), and some new dimensions were splits derived from previously mixed dimensions (e.g. plant-related and green) or highlighted more fine-grained aspects of previous dimensions (e.g. dessert rather than food). Overall model performance was similar yet slightly lower for the new and larger as compared to the original and smaller dataset (original: 64.60 ± 0.23%, new: 64.13 ± 0.18%), while noise ceilings were comparable (original noise ceiling dataset: 68.91 ± 1.07%, new noise ceiling datasets: 68.74 ± 1.07% and 67.67 ± 1.08%), indicating that the larger dataset was of similar quality. However, these noise ceilings were based on between-subject variability, leaving open a second question: how strongly did within-subject variability contribute to overall variability in the data? To estimate the within-subject noise ceiling, we inspected the consistency of within-subject triplet repeats. The within-subject noise ceiling was at 86.34 ± 0.46%. Even though this estimate constitutes an upper bound of the noise ceiling, since identical trials were repeated after only 16–20 triplets to compute reliability estimates, these results indicate that a lot of additional variance may be captured when accounting for differences between individuals. Thus, participant-specific modeling based on this new large-scale behavioral dataset may yield additional, novel insights into the nature of mental object representations.</p><p>Third, while increases in dataset size did not lead to notable improvements in overall performance, did increasing the dataset size improve more fine-grained predictions of similarity? To address this question, we used several existing datasets of within-category similarity ratings (<xref ref-type="bibr" rid="bib4">Avery et al., 2022</xref>; <xref ref-type="bibr" rid="bib66">Iordan et al., 2022</xref>; <xref ref-type="bibr" rid="bib103">Peterson et al., 2018</xref>) and computed similarity predictions. Rather than computing similarity across all possible triplets, these predictions were constrained to triplet contexts within superordinate categories (e.g. animals, vehicles). We expected the overall predictive performance to vary, given that these existing similarity rating datasets were based on a different similarity task or used different images. Yet, improvements are expected if fine-grained similarity can be estimated better with the large dataset than the original dataset. Indeed, as shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, seven out of eight datasets showed an improvement in predicted within-category similarity (mean improvement M=0.041 ± 0.007, p&lt;0.001, bootstrap difference test). This demonstrates that within-category similarity could be estimated more reliably with the larger dataset, indicating that the estimated embedding indeed contained more fine-grained information.</p></sec><sec id="s2-4"><title>Robustly decodable neural representations of objects</title><p>Having demonstrated the quality and overall reliability of the neuroimaging datasets, we aimed at validating their general suitability for studying questions about the neural representation of objects. To this end, we performed multivariate decoding on both the fMRI and MEG datasets, both at the level of individual object images, using the repeated image sets, and at the level of object category, using the 12 example images per category. Demonstrating the possibility to decode image identity and object category thus serves as a baseline analysis for more specific future research analyses.</p><p>When decoding the identity of object images, for fMRI we found above chance decoding accuracies in all participants throughout large parts of early visual and occipitotemporal cortices (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), with peak accuracies in early visual cortex, reaching 80% in participants F1 and F2. In MEG, we found above-chance decoding within an extended time-window (~80–1000ms) peaking ~100ms after stimulus onset, approaching 70–80% in participants M2 and M3 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Object image decoding in fMRI and MEG.</title><p>(<bold>A</bold>) Decoding accuracies in the fMRI data from a searchlight-based pairwise classification analysis visualized on the cortical surface. (<bold>B</bold>) Analogous decoding accuracies in the MEG data plotted over time. The arrow marks the onset of the largest time window where accuracies exceed the threshold which was defined as the maximum decoding accuracy observed during the baseline period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig4-v3.tif"/></fig><p>Moving from the level of decoding of individual images to the decoding of object category, for fMRI, accuracies now peaked in high-level visual cortex (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Likewise, for MEG the early decoding accuracies were less pronounced in absolute magnitude as compared to object image decoding (<xref ref-type="fig" rid="fig5">Figure 5C &amp; D</xref>). Together, these results confirm that both object image and object category can be read out reliably from both neuroimaging datasets, demonstrating their general usefulness for addressing more specific research questions about object identity.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Object category decoding and multidimensional scaling of object categories in fMRI and MEG.</title><p>(<bold>A</bold>) Decoding accuracies in the fMRI data from a searchlight-based pairwise classification analysis visualized on the cortical surface. (<bold>B</bold>) Multidimensional scaling of fMRI response patterns in occipito-temporal category-selective brain regions for each individual subject. Each data point reflects the average response pattern of a given object category. Colors reflect superordinate categories. (<bold>C</bold>) Pairwise decoding accuracies of object category resolved over time in MEG for each individual subject. (<bold>D</bold>) Group average of subject-wise MEG decoding accuracies. Error bars reflect standard error of the mean across participants (n = 4). (<bold>E</bold>) Multidimensional scaling for the group-level response pattern at different timepoints. Colors reflect superordinate categories and highlight that differential responses can emerge at different stages of processing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig5-v3.tif"/></fig><p>To demonstrate the utility of the datasets for exploring the representational structure in the neural response patterns evoked by different object categories, we additionally visualized their relationships in a data-driven fashion using multidimensional scaling (MDS) and highlighted clusters formed by superordinate categories. In fMRI, spatial response patterns across voxels in object-selective brain areas formed distinct clusters for the superordinate categories animals vs. food (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). MEG sensor patterns showed differences between categorical clustering at early and late time points (e.g. early differences for vehicles vs. tools, late differences for animals vs. food), indicating that information about superordinate categories arise at different times (<xref ref-type="fig" rid="fig5">Figure 5E</xref>).</p></sec><sec id="s2-5"><title>Large-scale replication of experimental findings: The case of animacy and size</title><p>The large-scale neuroimaging datasets can be used for addressing an abundance of new questions by pairing them with existing or new metadata for object categories, object concepts, or object images. However, they can also be used to test the degree to which previously shown findings hold for a broader set of objects. To this end, we aimed at replicating the seminal findings of cortical gradients of animacy and size tuning in occipitotemporal cortex (<xref ref-type="bibr" rid="bib77">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib76">Konkle and Oliva, 2012</xref>; <xref ref-type="bibr" rid="bib122">Welbourne et al., 2021</xref> ) and the temporal dynamics of object animacy and size representation (<xref ref-type="bibr" rid="bib51">Grootswagers et al., 2019</xref>; <xref ref-type="bibr" rid="bib72">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="bib120">Wang et al., 2022</xref>). We used animacy and size ratings available for each object in the THINGS concept metadata (<xref ref-type="bibr" rid="bib116">Stoinski et al., 2022</xref>) and used them to predict single-trial fMRI and MEG responses.</p><p>In line with previous findings, the fMRI results (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>) replicated the well-known alternating and spoke-like preference for animate vs. inanimate and small vs. big objects in occipitotemporal cortex (<xref ref-type="bibr" rid="bib77">Konkle and Caramazza, 2013</xref>). As expected, we found a strong preference for animate objects in fusiform gyrus and a transition along the mid-fusiform sulcus to inanimate preference in parahippocampal cortex (<xref ref-type="bibr" rid="bib45">Grill-Spector and Weiner, 2014</xref>). Regarding real-world size, place-selective brain areas (parahippocampal place area, occipital place area, and medial place area) showed a preference for big objects, and sections of lateral occipital cortex, which partly overlap with the extrastriate body area, showed a preference for small objects. While the results so far replicate the known topography of object animacy and size, in contrast to previous studies (<xref ref-type="bibr" rid="bib77">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib76">Konkle and Oliva, 2012</xref>; <xref ref-type="bibr" rid="bib122">Welbourne et al., 2021</xref>), we found a preference for large objects in parts of the fusiform gyrus, as well as a preference for small objects in a stretch of cortex in-between fusiform and parahippocampal gyrus. While the reasons for these diverging results are unclear, previous studies used a smaller range of sizes, and objects in the present dataset excluded certain stimuli that serve the purpose of navigation (e.g. houses) or that tend to be small (e.g. food), which may have affected these results. Disentangling the functional topography of object size at these different scales is a subject for future research.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Functional topography and temporal dynamics of object animacy and size.</title><p>(<bold>A</bold>) Voxel-wise regression weights for object animacy and size as predictors of trial-wise fMRI responses. The results replicate the characteristic spoke-like topography of functional tuning to animacy and size in occipitotemporal cortex. (<bold>B</bold>) Time courses for the animacy (top) and size (bottom) information in the MEG signal. The time courses were obtained from a cross-validated linear regression and show the correlation between the predicted and true animacy and size labels. Shaded areas reflect the largest time window exceeding the maximum correlation during the baseline period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig6-v3.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Functional topography of object animacy.</title><p>fMRI single trial responses averaged per object concept were predicted with animacy and size ratings obtained from human observers using ordinary least squares linear regression. Voxel-wise regression weights were resampled to an inflated representation of the participant’s individual cortical surface. The animacy regressor was z-scored such that positive weights (purple) indicate a preference for animate objects and negative weights (green) for inanimate objects at a given cortical location. Results are shown for all three participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig6-figsupp1-v3.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Functional topography of object size.</title><p>fMRI single-trial responses averaged per object concept were predicted with animacy and size ratings obtained from human observers using ordinary least squares linear regression. Voxel-wise regression weights were resampled to an inflated representation of the participant’s individual cortical surface reconstruction. The size regressor was z-scored such that positive weights (blue) indicate a preference for big objects and negative weights (orange) for small objects at a given cortical location. Results are shown for all three participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig6-figsupp2-v3.tif"/></fig></fig-group><p>With regard to the temporal dynamics, our data support previous findings (<xref ref-type="bibr" rid="bib23">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib107">Ritchie et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Grootswagers et al., 2019</xref>; <xref ref-type="bibr" rid="bib72">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="bib120">Wang et al., 2022</xref>; <xref ref-type="bibr" rid="bib20">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Grootswagers et al., 2017</xref>). For animacy, previous small-scale studies varied in reported decoding peaks between 140 and 350ms, with most results around 140–190ms. Our large-scale data corroborate this overall trend, showing a pronounced peak for animacy information at ~180ms in all participants (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Similarly, object size information was reliably present in the neural signal for all participants, albeit weaker than animacy and peaking later, further supporting previous findings. Thus, while previous findings were based on a small number of objects cropped from their natural background, our data generalize these findings by demonstrating that they also hold for a comprehensive range of thousands of objects and by extending previous findings to object images embedded in a natural background.</p></sec><sec id="s2-6"><title>Linking object representations between fMRI, MEG, and behavior</title><p>To demonstrate avenues for integrating neuroimaging and behavioral datasets, we performed representational similarity analysis (<xref ref-type="bibr" rid="bib80">Kriegeskorte et al., 2008a</xref>) to identify how well human similarity judgements reflected spatial and temporal brain responses. To this end, we correlated the behavioral similarity matrix with similarity matrices derived from fMRI searchlight voxel patterns across space and MEG sensor patterns across time. For fMRI, we found representational similarities in large parts of occipito-temporal cortex, with the strongest correspondence in ventral temporal and lateral occipital areas (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), in line with previous findings (<xref ref-type="bibr" rid="bib25">Cichy et al., 2019</xref>). For MEG, representational similarities with behavior were present as early as 80–100ms after stimulus onset in all participants, which is earlier than reported in previous studies (<xref ref-type="bibr" rid="bib6">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Cichy et al., 2019</xref>). Correlations exceeding the maximum value during the baseline period were sustained in all participants for at least 500ms (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Together, these results showcase how the behavioral and neuroimaging data can be linked for studying the large-scale cortical topography and temporal response dynamics underlying subjectively perceived object similarities, from small sets of individual objects all the way to a comprehensive evaluation based on thousands of objects.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Identifying shared representations between brain and behavior.</title><p>(<bold>A</bold>) Pearson correlation between perceived similarity in behavior and local fMRI activity patterns using searchlight representational similarity analysis. Similarity patterns are confined mostly to higher visual cortex. (<bold>B</bold>) Pearson correlation between the perceived similarity in behavioral and time-resolved MEG activation patterns across sensors using representational similarity analysis. The largest time window of timepoints exceeding a threshold are shaded. The threshold was defined as the maximum correlation found during the baseline period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig7-v3.tif"/></fig></sec><sec id="s2-7"><title>Direct regression-based MEG-fMRI fusion</title><p>One advantage of a multimodal collection of datasets is that we can combine fMRI and MEG to reveal the spatiotemporal dynamics underlying object processing. An existing popular approach for combining MEG and fMRI (<xref ref-type="bibr" rid="bib26">Cichy and Oliva, 2020</xref>) relies on correlating representational dissimilarity matrices (RDMs) obtained from fMRI for specific ROIs with time-resolved RDMs recorded with MEG. Thus, while this approach allows for comparisons at the population level both for MEG and fMRI, it is indirect and introduces additional assumptions about the spatial distribution of activity patterns and their representational similarity metric. Specifically, MEG-fMRI fusion based on classical representational similarity analysis (RSA; <xref ref-type="bibr" rid="bib80">Kriegeskorte et al., 2008a</xref>) requires the a priori selection of sensors and/or voxels to include into the computation of an RDM, additionally assumes that all voxels and MEG sensors contribute equally to the representational similarity (<xref ref-type="bibr" rid="bib69">Kaniuth and Hebart, 2022</xref>), and requires the selection of a similarity metric (<xref ref-type="bibr" rid="bib12">Bobadilla-Suarez et al., 2020</xref>; <xref ref-type="bibr" rid="bib106">Ramírez et al., 2020</xref>). In contrast, the size of THINGS-data allows using the MEG data directly to predict responses in fMRI ROIs or even individual voxels without having to rely on these assumptions. To showcase this analysis approach, we focused on two ROIs, V1 and FFA, and predicted average ROI responses recorded with fMRI from time-resolved multivariate pattern responses recorded with MEG using conventional multiple linear regression (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Predicting fMRI regional activity with MEG responses.</title><p>(<bold>A</bold>) Pearson correlation between predicted and true regression labels using mean FFA and V1 responses as dependent and multivariate MEG sensor activation pattern as independent variable. Shaded areas around the mean show bootstrapped confidence intervals (n=10,000) based on a 12-fold cross-validation scheme, leaving one session out for testing in each iteration. The mean across the cross-validation iterations is smoothed over 5 timepoints. (<bold>B</bold>) Difference between V1 and FFA time-resolved correlations with the peak highlighting when the correlation with V1 is higher than that of FFA. Error bars reflect boostrapped 95% confidence intervals (10,000 iterations).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-fig8-v3.tif"/></fig><p>The results from all four MEG participants showed that V1 responses could be predicted by MEG activity starting within the first 100ms, corresponding to earlier MEG work (<xref ref-type="bibr" rid="bib23">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Cichy et al., 2015</xref>) and work in non-human primates (<xref ref-type="bibr" rid="bib17">Bullier, 2001</xref>; <xref ref-type="bibr" rid="bib112">Schmolesky et al., 1998</xref>). In contrast, the FFA response could only be predicted from later timepoints of the MEG signal (~180ms). This finding is in line with many studies showing face-specific effects measured with fMRI in FFA (<xref ref-type="bibr" rid="bib70">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib44">Grill-Spector et al., 2004</xref>; <xref ref-type="bibr" rid="bib119">Tong et al., 2000</xref>) and a later dominance of high-level face responses (<xref ref-type="bibr" rid="bib10">Bentin et al., 1996</xref>; <xref ref-type="bibr" rid="bib32">Deffke et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Eimer, 2011</xref>; <xref ref-type="bibr" rid="bib121">Wardle et al., 2020</xref>). Contrasting the correlation time courses of V1 and FFA (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), we found that the correlation with V1 was larger than that of FFA between 105 and 130ms. Together, these analyses highlight the potential for combining larger datasets to provide a detailed spatiotemporally-resolved account of object processing.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>THINGS-data provides researchers in cognitive and computational neuroscience with a unique large-scale multimodal collection of neuroimaging and behavioral datasets in response to thousands of images of up to 1854 diverse objects. We have demonstrated the high quality of these datasets and we have provided five examples for potential research directions, including information-based multivariate decoding at the image and category level, data-driven visualization of response patterns across space and time, large-scale hypothesis testing by evaluating the reproducibility of previous research findings, revealing the relevance of the neuroimaging datasets for learning about behavioral similarity judgments, and regression-based fusion of MEG and fMRI data for uncovering a spatiotemporally resolved information flow in the human brain.</p><p>Two key strengths that set THINGS-data apart from other public datasets are its multimodality and size, offering fMRI and MEG responses to up to 22,448 object images collected over 12 sessions per participant and 4.70 million behavioral similarity judgments in response to natural object images, allowing countless new hypotheses to be tested at scale. For example, how are behaviorally relevant object dimensions reflected in patterns of brain activity, both in space and in time? What is the interplay of animacy, size, curvature, object color, object spikiness (<xref ref-type="bibr" rid="bib7">Bao et al., 2020</xref>), and other dimensions for a broader set of natural objects (<xref ref-type="bibr" rid="bib116">Stoinski et al., 2022</xref>)? How is object memorability represented in the human brain, and what other factors, such as object category or typicality, affect this representation (<xref ref-type="bibr" rid="bib78">Kramer et al., 2022</xref>)? How stable are object representations across days, and how does this effect depend on object category? What are the differences in perceived object similarity across gender and age, both within category and between categories? By offering multiple datasets targeting semantically broad spatial and temporal brain responses as well as behavioral judgments in response to images, THINGS-data fills an important gap in our ability as researchers to bring together different data domains. Among others, our exemplary analyses demonstrate a new method for directly combining MEG and fMRI responses without the assumptions imposed by representational similarity analysis (<xref ref-type="bibr" rid="bib23">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Hebart and Baker, 2018</xref>) or source modeling (<xref ref-type="bibr" rid="bib5">Baillet, 2017</xref>).</p><p>Beyond the multimodality and size, by virtue of being based on the THINGS object concept and image database (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>), THINGS-data comes with a rich and growing source of metadata specifically targeting cognitive and computational neuroscientists, including high-level categories, typicality, object feature ratings (<xref ref-type="bibr" rid="bib116">Stoinski et al., 2022</xref>), as well as memorability scores for each individual image (<xref ref-type="bibr" rid="bib78">Kramer et al., 2022</xref>). THINGS-data also provides numerous additional measures beyond the core datasets, including diverse structural and functional MRI data including resting state fMRI and functional localizers, physiological recordings for MRI data, and eye-tracking for MEG data. While eye-tracking data will be of limited use for studying natural eye movement dynamics, given the instruction of maintaining central fixation, these data can be used for targeted removal of MEG time periods involving occasional involuntary eye movements and eye blinks. Together with the large-scale behavioral dataset, these extensive measures and their breadth promise key new insights into the cortical processing of objects in vision, semantics, and memory.</p><p>In line with a growing body of literature highlighting the strengths of densely sampled datasets (<xref ref-type="bibr" rid="bib97">Naselaris et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="bib114">Smith and Little, 2018</xref>), for the fMRI and MEG datasets we targeted our efforts at extensive samples of a small number of participants instead of broadly sampling the population. Our key exemplary results replicate largely across participants, highlighting their generality and demonstrating that it is possible to treat each participant as a separate replication, with the aim of producing results that may generalize to the population. Another key benefit of extensively sampling individual brains is the ability to provide insights that generalize across objects and images (<xref ref-type="bibr" rid="bib97">Naselaris et al., 2021</xref>). Our data quality assessments and exemplary results replicate a large number of existing research findings, demonstrating that THINGS-data yields replicable results and highlighting that, despite known issues in the replicability of neuroscience and behavioral research (<xref ref-type="bibr" rid="bib89">Marek et al., 2022</xref>; <xref ref-type="bibr" rid="bib18">Button et al., 2013</xref>; <xref ref-type="bibr" rid="bib99">Open Science Collaboration, 2015</xref>), many previously reported findings are reliable and generalize across objects and images. At the same time, our exemplary analyses aimed at replicating previous fMRI and MEG work of size and animacy representation indeed only partially reproduced these findings (<xref ref-type="bibr" rid="bib77">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib76">Konkle and Oliva, 2012</xref>), highlighting the importance of extensive and representative sampling of object images. In order to confidently generalize results of specific hypotheses derived from THINGS-data to the population, additional focused studies in a larger set of participants may be conducted to strengthen these conclusions. THINGS-data thus offers an important testbed not only for new hypotheses but also for assessing the replicability and robustness of previous research findings.</p><p>The fMRI dataset published as part of THINGS-data provides important unique value beyond existing densely-sampled unimodal fMRI datasets targeting natural images. The present fMRI dataset contains responses to 720 objects and 8740 object images that were sampled to be representative of our visual diet (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>). In contrast, three other publicly available, large fMRI datasets of natural images (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="bib63">Horikawa and Kamitani, 2017</xref>) use images sampled from large machine learning databases, including Imagenet and MSCOCO (<xref ref-type="bibr" rid="bib86">Lin et al., 2014</xref>; <xref ref-type="bibr" rid="bib110">Russakovsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib125">Xiao et al., 2010</xref>), or are focused more strongly on natural scenes. While the advantage of these other datasets is the direct comparability with neural network models trained on these machine learning databases, this complicates the assessment of individual objects in scenes and comes with specific category biases that may affect the interpretation of results. For example, ImageNet contains a lot of dog images while lacking a person category, and MSCOCO is dominated by 80 categories (e.g. ‘giraffe’, ‘toilet’) that also often co-occur in images (e.g. ‘dog’ and ‘frisbee’). Beyond selection bias, these existing fMRI datasets are limited in their coverage of object categories or provide only a single exemplar per category, which limits their utility for the study of object invariance specifically and object semantics more generally. One existing fMRI dataset provides individual images for a broad set of objects, yet without multiple exemplars per category (<xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>), another dataset offers data with 8 object exemplars yet is restricted to 150 categories in the training set (<xref ref-type="bibr" rid="bib63">Horikawa and Kamitani, 2017</xref>), and two datasets strongly sample the 80 MSCOCO categories (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>). In contrast, THINGS-data offers individual objects from the THINGS database from 720 to 1854 carefully-curated object categories (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>), with 12 unique images per object. Finally, in THINGS-data, all images are presented only once to each participant, with the exception of test images, which precludes changes in brain responses across image repeats related to adaptation or memory, but which leads to lower reliability of BOLD estimates at the individual image level than slow event-related designs (<xref ref-type="bibr" rid="bib22">Chang et al., 2019</xref>), block designs (<xref ref-type="bibr" rid="bib63">Horikawa and Kamitani, 2017</xref>), or designs with repeated image presentations (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>). Thus, while existing unimodal datasets may be particularly useful for comparing results to machine learning models, for exploratory data analyses or for modeling of natural scenes, it is unclear how well results from these previous datasets will generalize to the entire breadth of objects in our environment. In contrast, the fMRI dataset of THINGS-data offers a uniquely broad, comprehensive and balanced sampling of objects for investigating visual and semantic representations across the human brain.</p><p>Despite the semantic breadth of the datasets provided and the use of naturalistic images, there are important limitations of our datasets. First, while the datasets reflect a broad sampling strategy for object categories and object images, object categories in the THINGS database had been selected as being representative of the American English language, which may continue to yield residual biases in the frequency of object categories that may lead to a biased assessment of object representations in brain and behavior. However, THINGS allows evaluating how robust findings are when changing the sampling strategy, which may help overcome this limitation. An additional limitation is the fact that our fMRI and MEG datasets relied on a small number of participants for making general statements about brain function. However, as discussed above, small-n designs have a long tradition in vision science and many of the studied effects replicate across participants, indicating that each participant can be treated as a replication of the same experiment (<xref ref-type="bibr" rid="bib114">Smith and Little, 2018</xref>). Further, stimuli were chosen to be naturalistic images presented at fixation, yet our world does not consist of static snapshots of images viewed through a restricted frame but instead of a continuously moving world present at a much wider viewing angle. Due to the absence of well-controlled short naturalistic movies of objects, the technical limitations of presenting larger stimuli, and the added complexity of recording and analyzing datasets consisting of movies, we decided to rely on object images. Future studies may extend these efforts to naturalistic movies of objects embedded in scenes (<xref ref-type="bibr" rid="bib65">Huth et al., 2022</xref>), yet with a similar level of semantic control as imposed by THINGS, and potentially in a wide angle environment, yielding important additional insights into object processing. Finally, any results may be affected by the choice of the task, which may have less of an effect on earlier processing stages yet more strongly affect object representations in anterior temporal lobe, as well as parietal prefrontal cortex (<xref ref-type="bibr" rid="bib54">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Hebart et al., 2018</xref>). Future efforts should compare the effect of different tasks on object representations across cerebral cortex.</p><p>THINGS-data reflects the core release of the THINGS initiative (<ext-link ext-link-type="uri" xlink:href="https://things-initiative.org">https://things-initiative.org</ext-link>), a global initiative bringing together researchers around the world for multimodal and multispecies collection of neuroimaging, electrophysiological, and behavioral datasets based on THINGS objects. As part of the THINGS initiative, two electroencephalography (EEG) datasets have recently been made available (<xref ref-type="bibr" rid="bib40">Gifford et al., 2022</xref>; <xref ref-type="bibr" rid="bib52">Grootswagers et al., 2022</xref>). In contrast to our temporally spaced MEG dataset that offers non-overlapping and unobstructed responses to stimuli, these datasets used a rapid serial visual presentation design, which allows presenting more images in a shorter time window, yet which leads to a strong overlap in neighboring responses and interactions between stimuli that are known to affect high-level processing (<xref ref-type="bibr" rid="bib108">Robinson et al., 2019</xref>). While this and the improved spatial fidelity afforded by MEG promise significant unique value of our MEG dataset, the datasets that are available or will be made available as part of the THINGS initiative offer a unique opportunity for convergence across multiple methods, species and paradigms. In this context, THINGS-data lays the groundwork for understanding object representations in vision, semantics, and memory with unprecedented detail, promising strong advances for the cognitive and computational neurosciences.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>For the onsite MRI and MEG studies, 3 healthy volunteers (2 female, 1 male, mean age at beginning of study: 25.33 years) took part in the MRI study and 4 different healthy volunteers (2 female, 2 male, mean age at beginning of study: 23.25 years) in the MEG study. Sample size of n=4 was determined in advance based on a trade-off between number of participants and effort of data collection. A fourth participant for the MRI study was planned but due to repeated technical issues and the ensuing lack of time was canceled. All on-site participants were screened prior to participation for suitability and availability, all with prior experience in studies that required keeping their eyes on fixation for prolonged periods of time. All participants were right-handed and had normal or corrected-to-normal visual acuity. Participants provided informed consent in participation and data sharing, and they received financial compensation for taking part in the respective studies. The research was approved by the NIH Institutional Review Board as part of the study protocol 93 M-0170 (NCT00001360).</p><p>For the online study, behavioral data were collected through the crowdsourcing platform Amazon Mechanical Turk. 14,025 workers participated in the triplet odd-one out experiment, for a total of 5,517,400 triplet choices. The sample size was determined based on the number of triplets expected to be sufficient for reaching threshold in data dimensionality, which was estimated to be ~5 million triplets. We collected an additional 10% to compensate for assumed partial exclusion of the data. A subset of 1.46 million triplets had been used in previous work (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>; <xref ref-type="bibr" rid="bib127">Zheng et al., 2019</xref>; <xref ref-type="bibr" rid="bib96">Muttenthaler et al., 2022b</xref>). Data quality was assessed separately across four batches. Exclusion criteria were in part pre-established based on previous work (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>), in part updated to yield a decent trade-off between data quality and the amount of excluded data. Workers in a given batch were excluded if they were overly fast in at least five separate assignments of 20 trials each (&gt;25% responses faster than 800ms and &gt;50% responses faster than 1,100ms), overly repetitive in a set of ≥200 trials (deviating from the central 95% distribution), or very inconsistent in providing demographic information (&gt;3 ages provided). These criteria led to the exclusion of 818,240 triplets (14.83%). The final dataset consisted of 12,340 workers (6619 female, 4400 male, 56 other, 1065 not reported; mean age: 36.71, std: 11.87, n=5170 no age reported) and 4,699,160 triplets, of which 4,574,059 triplets comprised the training and test data for computational modeling and 125,101 triplets the four datasets used for computing noise ceilings. Workers received financial compensation for their study participation ($0.10 for 20 trials, median RT per trial: 2846ms). Workers provided informed consent for the participation in the study. The online study was conducted in accordance with all relevant ethical regulations and approved by the NIH Office of Human Research Subject Protection (OHSRP).</p></sec><sec id="s4-2"><title>Stimuli</title><p>Images for all three datasets were taken from the THINGS object concept and image database (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>). THINGS comprises 1854 object concepts from a comprehensive list of nameable living and non-living objects and things, including non-countable substances (e.g. ‘grass’, ‘sand’), faces (e.g. ‘baby’, ‘boy’, ‘face’), as well as body and face parts (e.g. ‘eye’, ‘leg’). For each concept, THINGS contains a minimum of 12 high-quality colored images of objects embedded in a natural background (total number of images: 26,107).</p><p>For the MEG dataset, all 1854 object concepts were sampled, with the first 12 exemplars per concept, for a total of 22,248 unique images presented once throughout the study. For the MRI dataset, given time limitation for the planned 12 sessions, sampling was restricted to a subset of 720 representative object concepts, again with the first 12 exemplars per concept, for a total of 8640 unique images (for the concept and image selection strategy, see Appendix 1). In addition, for the MEG dataset, there were 200 separate THINGS images that were among the remaining THINGS images. These images were presented repeatedly and served as a separate test set for model evaluation. For MRI, there were 100 separate test images that were a representative subset of the 200. Finally, there were 100 unique catch images that were created using the generative adversarial neural network BigGAN (<xref ref-type="bibr" rid="bib16">Brock et al., 2019</xref>). These images were generated by interpolating between two latent vectors, yielding novel objects that were not recognizable. All presented images subtended 10 degrees of visual angle and were presented on a mid-grey background, and a fixation crosshair (<xref ref-type="bibr" rid="bib117">Thaler et al., 2013</xref>) subtending 0.5 degrees was overlaid onto the image.</p><p>For the behavioral dataset, the 1854 images were used that had been shown during evaluation of the concepts included in the THINGS database (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>), of which 1663 images ended up overlapping with the THINGS images (other images had been excluded from the database because of small image size). The images were cropped to square size, with the exception of a small number of images for which objects didn’t fit inside a square and which were padded with white background.</p></sec><sec id="s4-3"><title>Experimental procedure</title><sec id="s4-3-1"><title>MRI study procedure</title><p>MRI participants wore custom fitted head casts (Caseforge Inc, USA) to minimize head motion and improve alignment between sessions. Stimuli were presented on a 32” BOLD screen (Cambridge Research Systems Ltd, UK) that was placed behind the bore of the scanner and viewed through a surface mirror attached to the head coil. Respiration and pulse were recorded at 500 Hz using a breathing belt and a photoplethysmograph, respectively (Biopac System Inc, USA).</p><p>Participants took part in a total of 15–16 scanning sessions. All sessions of a given participant took place roughly at the same time of day (+/-2 hours) to avoid non-specific effects associated with changes during the day (<xref ref-type="bibr" rid="bib100">Orban et al., 2020</xref>; <xref ref-type="bibr" rid="bib115">Steel et al., 2019</xref>). The first 1–2 sessions were used for testing the fit of the individualized head casts (see below) and for acquiring functional localizers for measuring retinotopic maps using population receptive field (pRF) mapping (4–6 runs, ~8 min each) as well as attaining category-selective functionally localized clusters in response to images of faces, body parts, scenes, words, and objects (6 runs, ~4.5 min each; for details, see Appendix 3). In the next 12 sessions, functional data was acquired for the main experiment using THINGS images. In the last two sessions, two separate datasets were acquired that are planned to be published separately. During each session, if there was sufficient time, additional anatomical images were acquired (see MRI data acquisition). At the end of each session, a resting state run was conducted (~6 min, eyes closed).</p><p>Each of the 12 main fMRI sessions consisted of 10 functional runs (~7 min each). In each run, 72 object images were presented, as well as 10 test and 10 catch images. Participants’ task was to keep their eyes on fixation and report the presence of a catch image with a button press on a fiber-optic diamond-shaped button box (Current Designs Inc, USA). Stimuli were presented for 500ms, followed by 4 s of fixation (SOA: 4.5 s). This amounted to a total of 92 trials per run, 920 trials per session, and 11,040 trials in total per participant. The 720 object images in a given session were chosen such that each of the 720 object concepts were present, while all 100 test images were shown in each session once and the catch images were chosen randomly. The order of trials was randomized within each functional run, with the constraint that the minimum distance between two subsequent catch images was three trials. Stimulus presentation was controlled using MATLAB with Psychtoolbox (<xref ref-type="bibr" rid="bib15">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib75">Kleiner et al., 2007</xref>).</p></sec><sec id="s4-3-2"><title>MEG study procedure</title><p>MEG participants wore an individually molded head cast (Chalk Studios Ltd, UK) to minimize head motion and improve alignment between sessions. Head position was measured with three marker coils attached to the head casts (nasion, as well as anterior to the left and right preauricular pits). Head position was recorded at the beginning and end of each run. Stimuli were presented on a back projection screen using a ProPixx projector (VPixx Technologies Inc, Canada). Eye position and pupil size was tracked at 1200 Hz throughout the study using an EyeLink 1000 Plus (SR Research, Canada).</p><p>Each MEG participant attended one MRI session and 14 MEG sessions. In the MRI session, a T1-weighted structural brain image (MPRAGE, 0.8 mm isotropic resolution, 208 sagittal slices) was collected without head padding to allow for the construction of a custom head cast and as part of the dataset to allow for improved MEG source modeling. The next 12 sessions were the main MEG sessions using THINGS images, while in the final two sessions, two separate datasets were acquired that are planned to be published separately. Within each of the 12 main sessions, the overall procedure was very similar to the MRI study, with the main difference that 1854 objects were presented in each session and that the stimulus presentation rate was faster. Each session consisted of 10 runs (~5 min each). In each run, 185–186 object images were presented, as well as 20 test and 20 catch images. Stimuli were presented for 500ms, followed by a variable fixation period of 1000±200ms (SOA: 1500±200ms). Jitter was included to reduce the effect of alpha synchronization with trial onset. This amounted to 225–226 trials per run, 2,254 trials per session, and 27,048 trials per participant. Stimulus presentation was controlled using MATLAB with Psychtoolbox (<xref ref-type="bibr" rid="bib15">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib75">Kleiner et al., 2007</xref>).</p></sec><sec id="s4-3-3"><title>Online crowdsourcing study procedure</title><p>The triplet odd-one out task was collected using the online crowdsourcing platform Amazon Mechanical Turk. The task was carried out in a browser window. On a given trial, participants saw three images of objects side by side and were asked to indicate with a mouse click which object they perceived as the odd-one out. Then, the next trial was initiated after 500ms. To reduce bias, participants were told to focus on the object but no additional instructions were provided as to what constitutes the odd-one out. Each task consisted of 20 trials, and workers could choose to participate as often as they liked. This had the advantage that workers could stop whenever they no longer felt motivated to continue. After completing the 20 trials, workers were prompted to fill in demographic information. For the first set of ~1.46 million trials, workers could voluntarily report gender and ethnicity, while for the remaining dataset, workers could voluntarily report gender, ethnicity, and also age. Triplets and stimulus order were chosen randomly, but were selected in a way that each cell of the final 1,854×1,854 similarity matrix was sampled roughly equally often. In the final dataset, each cell was sampled on average 7.99 times, with all cells sampled at least once and 98.48% of all cells sampled 6 times or more. For a small subset of 40,000 trials, participants were shown the same set of 1,000 triplets twice within the same task (i.e. 40 per triplet), with a minimum distance of 16 trials to reduce short-term memory effects. The purpose of this manipulation was to estimate an upper bound for the consistency of participants’ choices. For another subset of 40,000 trials this same set of triplets was shown but this time to different participants, to estimate the lower bound for the consistency of participants’ choices. Finally, two other subsets of trials were generated with two other sets of 1000 triplets (25,000 and 40,000 trials, respectively), to ensure that data quality remained stable across data acquisition time periods. Stimulus presentation was controlled with custom HTML and Javascript code.</p></sec></sec><sec id="s4-4"><title>MRI acquisition and preprocessing</title><sec id="s4-4-1"><title>MRI data acquisition</title><p>All magnetic resonance images were collected at the NIH in Bethesda, MD (USA) using a 3 Tesla Siemens Magnetom Prisma scanner and a 32-channel head coil. During the main task of the fMRI experiment involving the THINGS images, we collected whole-brain functional MRI data with 2 mm isotropic resolution (60 axial slices, 2 mm slice thickness, no slice gap, matrix size 96×96, FOV = 192 × 192 mm, TR = 1.5 s, TE = 33ms, flip angle = 75°<italic>,</italic> echo spacing 0.55ms, bandwidth 2,264 Hz/pixel, multi-band slice acceleration factor 3, phase encoding posterior-to-anterior).</p><p>We collected additional high-resolution data of each participant’s individual anatomy (2–3 T1-weighted and one T2-weighted images per participant), vasculature (Time-of-Flight and T2*-weighed), and functional connectivity (resting state functional data), as well as gradient echo field maps to account for image distortions due to inhomogeneities in the magnetic field. The resting state functional MRI data was acquired using the reverse phase encoding direction (anterior-to-posterior) compared to the main functional runs to allow for an alternative method for distortion correction (<xref ref-type="bibr" rid="bib2">Andersson et al., 2003</xref>). A detailed description of the MRI imaging parameters can be found in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></sec><sec id="s4-4-2"><title>MRI data preprocessing</title><p>Functional magnetic resonance imaging data was deidentified (<xref ref-type="bibr" rid="bib53">Gulban et al., 2019</xref>), converted to the Brain Imaging Data Structure (<xref ref-type="bibr" rid="bib42">Gorgolewski et al., 2016</xref>) and preprocessed with fMRIPrep (<xref ref-type="bibr" rid="bib37">Esteban et al., 2019</xref>) (version 20.2.0). A detailed description of this procedure can be found in the online dataset on figshare (see Data Availability). In short, the preprocessing pipeline for the functional images included slice timing correction, rigid body head motion correction, correction of susceptibility distortions based on the field maps, spatial alignment to each participant’s T1-weighted anatomical reference images, and brain tissue segmentation and reconstruction of pial and white matter surfaces. Since the default pipeline of fMRIPrep does not allow the inclusion of multiple T1-weighted and T2-weighted anatomical images, which can improve each participant’s surface reconstruction and all downstream processing steps, we manually ran Freesurfer’s recon-all (<xref ref-type="bibr" rid="bib31">Dale et al., 1999</xref>) and passed the output to fMRIPrep. Finally, we visually inspected the cortical surface reconstruction and manually placed relaxation cuts along anatomical landmarks including the calcarine sulcus to generate cortical flat maps for visualization purposes (<xref ref-type="bibr" rid="bib38">Gao et al., 2015</xref>). Preprocessing and analysis of retinotopic mapping data yielded retinotopic visual regions V1-V3, hV4, VO1/VO2, LO1/LO2, TO1/TO2, and V3a/V3b (see Appendix 3). Preprocessing and analysis of functional localizer data yielded fusiform face area (FFA), occipital face area (OFA), posterior superior temporal sulcus (pSTS), extrastriate body area (EBA), parahippocampal place area (PPA), medial place area / retrosplenial complex (MPA), occipital place area (OPA), and lateral occipital cortex (LOC). For subject F3, pSTS could not be defined because no significant cluster of face-selective activation was localized in that area.</p></sec><sec id="s4-4-3"><title>fMRI ICA denoising</title><p>fMRI data contains noise due to head motion, pulse and heartbeat, respiration, as well as other physiological and scanner-related factors that can negatively impact downstream data analysis (<xref ref-type="bibr" rid="bib94">Murphy et al., 2013</xref>). Independent component analysis (ICA) has been shown to reliably separate many signal and noise components (<xref ref-type="bibr" rid="bib8">Beckmann and Smith, 2004</xref>). However, common existing automatic or semi-autom atic ICA classification approaches are based either on a complex classification pipeline (<xref ref-type="bibr" rid="bib111">Salimi-Khorshidi et al., 2014</xref>) which may be prone to overfitting, or they are focused on head motion-related artifacts alone (<xref ref-type="bibr" rid="bib105">Pruim et al., 2015</xref>). Therefore, we developed a heuristic semi-automated classification approach to capture a broad set of physiological and scanner-related artifacts based on independent component analysis (ICA).</p><p>For attaining stable independent components, each functional run was additionally preprocessed with spatial smoothing (FWHM = 4 mm) and a high-pass filter (cut-off=120 s). Decomposing the preprocessed data of each run with MELODIC ICA (<xref ref-type="bibr" rid="bib8">Beckmann and Smith, 2004</xref>) yielded a total of 20,389 independent components for all sessions of all 3 participants. For each independent component, we quantified a set of features which we hypothesized to be related to its potential classification as signal or noise, which are explained in more detail below: The correlation with the experimental design, the correlation with physiological parameters, the correlation with head motion parameters, its edge fraction, and its high-frequency content.</p><p>The correlation with the experimental design was estimated by convolving the stimulus onsets with a canonical hemodynamic response function and computing the Pearson correlation with the component time series. The correlation with physiological parameters was taken as the maximum correlation of the component time series with a set of physiological regressors derived from the raw cardiac and respiratory recordings (see code make_physio_regressors.m). Similarly, the correlation with head motion was taken as the maximum correlation of the component time series with any of the head motion estimates produced by fMRIPrep. The edge fraction reflects the presence of high independent component weights near the edge of the brain and was estimated as the sum of absolute weights in the edge mask, divided by the sum of absolute weights within the entire brain mask. The edge mask was generated by subtracting an eroded brain mask (eroded by 4 mm) from the original whole-brain mask. High-frequency content was defined as the frequency at which the higher frequencies explain 50% of the total power between 0.01 Hz and the Nyquist frequency (<xref ref-type="bibr" rid="bib105">Pruim et al., 2015</xref>).</p><p>Once these features had been derived, two independent raters manually labeled a subset of all independent components. We randomly sampled a set of 2472 components (1614 for rater 1; 1665 for rater 2; 807 of which were rated by both). Raters gave each component a unique label for either signal, head motion noise, physiological noise, MR-scanner noise, other noise source, or unknown, as well as a confidence rating from 1 (not confident) to 3 (very confident). Interrater agreement for labeling components as signal vs. not signal was 87%. Raters made their choices based on summary visualizations (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>), which showed each component’s respective spatial map, time series, and temporal frequency spectrum as well as additional information including (1) the time course of the experimental design, (2) the expected time course of physiological noise and (3) the expected time course of head motion related noise. The time course of the experimental design was created by convolving the stimulus onsets with a canonical HRF. We estimated the expected time course of physiological noise by regressing the physiological confounds against the component time series and visualized the resulting prediction. Similarly, we estimated the expected time course of head motion related noise by regressing head motion parameters against the component time course and visualized the resulting prediction. The head motion parameters used included rotation and translation along the three axes, as well as their square value and first derivative. Finally, these visualizations also showed the highest Pearson correlation of the component time series with the physiological confounds and the head motion parameters as well as the correlation with the experimental design, the high frequency content and the edge fraction.</p><p>We then visually inspected the distributions of the labeled data along the estimated feature dimensions. The results showed that the signal distribution was reliably separable from the noise distributions based on edge fraction and high-frequency content alone. For robustness, we defined univariate thresholds in these features (edge fraction: 0.225, high-frequency content: 0.4) and classified each of the 20,388 originally estimated components accordingly (rater 1: 61% noise sensitivity, 98% signal specificity; rater 2: 69% noise sensitivity, 98% signal specificity). The resulting noise component time series were then used as noise regressors for the single trial response estimation in downstream analyses. Incorporating these noise regressors strongly improved the reliability of single trial response estimates (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s4-4-4"><title>fMRI single-trial response estimates</title><p>Beyond the many sources of noise in fMRI data, another challenge lies in the fact that fMRI voxel-wise time series consist of a lot of data, making analyses of the entire dataset computationally challenging and potentially inaccessible to researchers with fewer resources. The time series format is also not ideal for some data-driven techniques such as multivariate pattern analysis (<xref ref-type="bibr" rid="bib55">Haxby et al., 2001</xref>), representational similarity analysis (<xref ref-type="bibr" rid="bib80">Kriegeskorte et al., 2008a</xref>), or dimensionality reduction techniques that require discrete data samples as inputs. To overcome these challenges, we estimated the BOLD response amplitude to each object image by fitting a single-trial general linear model on the preprocessed fMRI time series. Our procedure was similar to the recently-developed GLMsingle approach (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib104">Prince et al., 2022</xref>), but we adopted an approach to better suit (1) our experimental design which contained image repeats only across sessions and (2) the use of ICA noise regressors which varied in number between runs.</p><p>First, we converted data from each functional run to percent signal change. We then regressed the resulting time series data against a set of noise regressors comprising the ICA noise components for that run and a set of polynomial regressors up to degree 4. The residuals of this step were then kept for downstream analyses. Although a stepwise regression approach can be suboptimal (<xref ref-type="bibr" rid="bib87">Lindquist et al., 2019</xref>), we chose it in order to avoid regularizing the noise regressors when optimizing the single trial beta estimates, and post-hoc analyses demonstrated that resulting noise ceilings were, indeed, slightly higher with our approach. To account for differences in the shape of the hemodynamic response function (HRF), we used a library of 20 HRFs that had previously been shown to capture a broad range of HRF shapes (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib104">Prince et al., 2022</xref>) and determined the best fitting HRF for each voxel. To this end, we generated a separate on-off design matrix for each of the 20 HRFs, fit each design matrix to the fMRI time series separately, and determined the best HRF per voxel by the largest amount of explained variance. Since the regressors of neighboring trials are highly correlated in a fast event-related design, we used fractional ridge regression to mitigate overfitting and to identify the regularization parameter for each voxel that maximized the predictive performance of left-out data (<xref ref-type="bibr" rid="bib109">Rokem and Kay, 2020</xref>). We used a range of regularization parameters from 0.1 to 0.9 in steps of 0.1 and from 0.9 to 1.0 in steps of 0.01 to sample the hyperparameter space more finely for values which correspond to less regularization. We evaluated the performance based on the consistency of beta estimates over repeatedly presented trials in a leave-one-session-out cross-validation. To this end, we determined the sum of squared differences between the mean of the regularized betas in the 11 training sessions and the unregularized betas in the held-out session. We then fit a single-trial model with the best hyperparameter combination per voxel (HRF and fractional ridge parameter) to obtain the set of single-trial beta coefficients. Since ridge regression leads to biases in the overall scale of the beta coefficients, we linearly rescaled them by regressing the regularized against the unregularized coefficients and keeping the predictions as the final single-trial response amplitudes.</p><p>The resulting beta weights of the single-trial model represent an estimate of the BOLD response amplitude to each object image as a single number per voxel. This data format is much smaller than the original time series, is amenable to a wider range of analysis techniques, and was used for all analyses showcased in this manuscript. Both the voxel-wise time series and single-trial response estimates are publicly available such that users may choose the data format that best suits their research purposes (see Data availability).</p></sec></sec><sec id="s4-5"><title>MEG acquisition and preprocessing</title><sec id="s4-5-1"><title>MEG data acquisition</title><p>The MEG data were recorded with a CTF 275 MEG system (CTF Systems, Canada) which incorporates a whole-head array of 275 radial 1st order gradiometer/SQUID channels. The MEG was located inside a magnetically shielded room (Vacuumschmelze, Germany). Data were recorded at 1,200 Hz. 3rd gradient balancing was used to remove background noise online. Recordings were carried out in a seated position. Since three MEG channels were dysfunctional (MLF25, MRF43, and MRO13), data were available from 272 channels only. Eye-tracking data (position and pupil) were saved as signals in miscellaneous MEG channels (x-coordinate: UADC009, y-coordinate: UADC010, pupil size: UADC013). Parallel port triggers were used to mark the stimulus onset in real time (channel: UPPT001). To account for temporal delays between the computer and the stimulus display, we used an optical sensor which detects light changes (channel: UADC016).</p></sec><sec id="s4-5-2"><title>MEG data preprocessing and cleaning</title><p>We used MNE-python (<xref ref-type="bibr" rid="bib43">Gramfort et al., 2013</xref>) to preprocess the MEG data. We bandpass filtered the raw data for each run between 0.1 and 40 Hz. For two participants (M1 and M2), there was a complete MEG signal dropout in one run which lasted for less than 200ms. We replaced this segment of the data with the median sensor response before epoching. To mark trial onsets in the continuous MEG recordings, we used parallel port triggers and the signal of an optical sensor which detects light changes on the display and can thus account for temporal delays between the computer and the projector. We used the signal from the optical sensor to epoch the continuous data from –100ms to 1300ms relative to stimulus onset. We then baseline corrected the epoched data by subtracting the mean and dividing by the standard deviation of the data during baseline (100ms before stimulus onset).Next, we excluded one sensor (MRO11) for all participants which was regularly unlocked and thus yielded very noisy responses. After all preprocessing steps were completed, data were downsampled to 200 Hz to reduce computational load for downstream analyses.</p></sec><sec id="s4-5-3"><title>MEG head motion</title><p>Continuous head localization did not allow for stable MEG recordings, so it was deactivated. However, given the use of individualized head casts, we expected head motion to be minimal during runs. We recorded snapshots of head position via the three marker coils before and after every run. To examine how much of a concern head motion is, we calculated the within- and the cross-session changes in head position for every participant using the data from the average of the pre- and post-run measurements. We found that within-session head motion was minimal (median &lt;1.5 mm for all participants), with slightly larger but still small head motion across sessions (median &lt;3 mm for all participants). Note that differences in measured head position could also be due to the positioning of the marker coils inside the head cast. For one participant (M4), it seems likely that the marker coils in two sessions were not positioned in the exact same location as in other sessions (see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p></sec><sec id="s4-5-4"><title>MEG event-related fields</title><p>To examine the stability of evoked responses across sessions, we inspected the event-related fields for the 200 test images that were repeated in each session. As the 200 images showed different objects, we expected a stable visual response across sessions. Thus, we averaged the response across all 200 test images for occipital, temporal, and parietal sensors, respectively, and plotted the evoked responses for each sensor and sensor group separately (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Overall, the visualization indicates that the evoked responses for each participant were similar across the twelve sessions, highlighting that differences between sessions (e.g. in head motion or cognitive state) were likely not detrimental to the overall results.</p></sec></sec><sec id="s4-6"><title>Noise ceiling estimation</title><p>We estimated noise ceilings for all datasets as an indicator of data reliability. The noise ceiling is defined as the maximum performance any model can be expected to achieve in terms of explainable variance (<xref ref-type="bibr" rid="bib84">Lage-Castellanos et al., 2019</xref>). For the fMRI and MEG data, we estimated the noise ceiling based on the variability of responses to the subset of object images which were presented repeatedly in each session (fMRI: 100 images, 12 repetitions; MEG: 200 images, 12 repetitions). To this end, we used the analytical approach introduced recently (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>). The noise variance was estimated as the pooled variance over repeated responses to the same image. The signal variance was estimated by first taking the mean response over repetitions of the same image and then computing the variance over the resulting image-wise average responses. Finally, the total variance was taken as the sum of the noise and signal variance. Thus, the noise ceiling was defined as the ratio between the signal variance and the total variance. This expression of the noise ceiling can be adjusted to account for the number of trials researchers might want to average in their analysis (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>). We computed two noise ceiling estimates: one for the test set (adjusted for averaging over 12 trial repeats) and one for single trial responses (unadjusted). Note that the noise ceiling was estimated independently for each measurement channel (i.e. each voxel in fMRI and each timepoint and sensor in MEG).</p><p>For the behavioral dataset, there were three noise ceiling datasets where triplets were sampled repeatedly between participants, and one where they were sampled within participants. For the three between-subject noise ceiling datasets, a different set of 1000 random triplets were chosen, while the within-subject noise ceiling triplets were the same as the second dataset. Several noise ceiling datasets were acquired to test for non-stationarity in the acquired behavioral dataset, since the first 1.46 million triplets had been acquired much earlier than the later datasets. For a given triplet, across all participants that had taken part, the choice consistency was computed in percent. The noise ceiling was then defined as the average choice consistency across all triplets. Note that this procedure slightly overestimates the true noise ceiling since it is always based on the most consistent choice in the sample.</p></sec><sec id="s4-7"><title>Behavioral modeling procedure</title><p>The procedure for deriving a low-dimensional embedding from triplet odd-one out judgments has been described in detail previously (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>). The computational model is available online (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViCCo-Group/SPoSE">https://github.com/ViCCo-Group/SPoSE</ext-link>; <xref ref-type="bibr" rid="bib95">Muttenthaler et al., 2022a</xref>), which was implemented in PyTorch 1.6 (<xref ref-type="bibr" rid="bib101">Paszke et al., 2019</xref>). First, we split up the triplets into a training and test set, using a 90–10 split. The SPoSE model is designed to identify a set of sparse, non-negative and interpretable dimensions underlying similarity judgments. The embedding was first initialized with 90 random dimensions (range 0–1). Next, for a given triplet of images, the dot product of the 90-dimensional embedding vectors between all three pairs of images was computed, followed by a softmax function (no temperature parameter), yielding three predicted choice probabilities. The highest choice probability was then used as a basis for the computation of the loss. The loss function for updating the embedding weights consisted of the cross-entropy, which is the logarithm of the softmax function, and a separate sparsity-inducing L-1 norm of the weights, where the trade-off between both loss terms is determined by a regularization parameter <italic>λ</italic> that was identified with cross-validation on the training set (final <italic>λ</italic>: 0.00385). Optimization was carried out using Adam (<xref ref-type="bibr" rid="bib74">Kingma and Ba, 2017</xref>) with default parameters and minibatch size of 100 triplets. Once optimization had completed, all dimensions with weights exclusively &lt;0.1 were removed. We then sorted the dimensions in descending order based on the sum of the weights.</p><p>Since the optimization procedure is stochastic, the resulting embedding will change slightly depending on how it is initialized. To identify a highly reproducible embedding, we ran the entire procedure 72 times with a different random seed, yielding 72 separate embeddings. For a given embedding and a given dimension, we then iterated across the remaining 71 embeddings, identified the most similar dimension, and used this to compute a reproducibility index (average Fisher-z transformed Pearson correlation). Repeating this process for each dimension in an embedding provided us with a mean reproducibility for each embedding. We then picked the embedding with the best reproducibility, yielding the final embedding with 66 dimensions. One of the authors (MNH) then visually inspected and hand labeled all 66 dimensions. Note that the same author had generated the labels for all 49 dimensions in the original model, which mostly agreed with participants’ labels to these dimensions (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>).</p></sec><sec id="s4-8"><title>Extrapolation from small dataset to predict saturation of dimensionality</title><p>While it has been shown previously that the modeling procedure approached peak performance already with a smaller dataset (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>), the embedding dimensionality kept increasing, indicating a benefit of collecting a larger dataset for a more refined representational embedding. To determine how large a dataset was required until model dimensionality no longer grew noticeably, we estimated the growth in model dimensionality as a function of dataset size by extrapolating the original dataset of 1.46 million trials. To achieve this aim, we first took the estimated dimensionality of 4 embedding that had been computed at each step of 100,000 trials up until 1.4 million trials, making it a total of 14 steps and 56 embeddings. Next, we fitted an exponential decay function with the shape of <inline-formula><mml:math id="inf1"><mml:mi>a</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>b</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> to the mean dimensionality across all 14 steps and extrapolated this function. Finally, we computed 1000 bootstrap estimates by resampling the means from all 4 embeddings per position and repeating this fitting procedure. This was used to identify 95% confidence intervals of the estimated model dimensionality given the dataset size. In the limit, the embedding saturated at 67.54 dimensions (95% CI: 61.94–74.82). The final dataset size was determined as a trade-off between approaching the final model dimensionality and data acquisition cost.</p></sec><sec id="s4-9"><title>Fine-grained prediction of perceived similarity</title><p>To identify the degree to which the updated embedding yielded improved prediction of fine-grained similarity, we used 8 existing datasets from three studies (<xref ref-type="bibr" rid="bib4">Avery et al., 2022</xref>; <xref ref-type="bibr" rid="bib66">Iordan et al., 2022</xref>; <xref ref-type="bibr" rid="bib103">Peterson et al., 2018</xref>) that had examined within category similarity. Note that predicted similarities are likely underestimated, given that the original similarity datasets were collected using different image examples and/or tasks. First, we took the labels from these datasets and identified the overlap with the THINGS concepts, while adjusting small differences (e.g. ‘chairs’ was changed to ‘chair’, ‘mandrill’ to ‘monkey’). Several datasets contained multiple images per object concept, i.e. not all used concepts were unique. This yielded datasets of the high-level categories animal (n=10, all concepts unique concepts, Iordan; n=104, 39 unique, Peterson), food (n=30, all unique, Avery), fruit (n=72, 24 unique, Peterson), furniture (n=81, 12 unique, Peterson), vegetable (n=69, 23 unique, Peterson), and vehicle (2 datasets, n=10, all unique, Iordan; n=78, 13 unique, Peterson). Since the SPoSE model allows for computing similarity within a constrained context, we used category-constrained similarity estimates, using all examples of a given superordinate category to generate similarity estimates for a given model. The representational similarity was then computed using the lower triangular part of each matrix and using Pearson correlation between the similarity matrices derived from the original 49-dimensional embedding and the measured similarity matrix, as well as the new 66-dimensional embedding and the measured similarity matrix. Finally, we computed 100,000 bootstrap estimates for each representational similarity to attain confidence estimates, by repeatedly sampling rows and columns in each similarity matrix referring to different individual objects. To test if across all eight datasets there was an overall improvement, we determined the fraction of bootstrap examples yielding a mean improvement in predicted similarity. When five or more similarity matrices showed an improvement, this was counted as an improvement (&gt;50%), while if 4 or fewer similarity matrices showed an improvement, this was counted as no improvement (≤50%). The fraction of cases where there was no improvement was then taken as the p-value.</p></sec><sec id="s4-10"><title>fMRI and MEG multivariate decoding analyses</title><p>To validate the usefulness of the neuroimaging datasets for studying object representations, we conducted two sets of multivariate decoding analyses (<xref ref-type="bibr" rid="bib59">Hebart and Baker, 2018</xref>; <xref ref-type="bibr" rid="bib57">Haynes, 2015</xref>) focused at revealing object-related information content in brain activity patterns. One set of analyses was conducted at the level of object images, while the other was carried out at the level of object concepts. The object image analyses were based on the 100 test images for fMRI and the 200 test images for MEG, respectively, of which each had been repeated 12 times each. The object concept analyses were based on all 12 unique exemplars per object concept that had not been repeated (fMRI: 720 concepts; MEG: 1,854 concepts). All analyses were conducted using leave-one-session-out cross-validation. For fMRI, we used spatially-resolved searchlight decoding on the beta weights (radius = 10 mm), implemented in The Decoding Toolbox (<xref ref-type="bibr" rid="bib58">Hebart et al., 2014</xref>), while for MEG, we used time-resolved decoding at the sensor level implemented in the CoSMoMVPA toolbox (<xref ref-type="bibr" rid="bib98">Oosterhof et al., 2016</xref>). FMRI analyses were based on pairwise linear support vector machine classification (258,840 pairwise classifiers per searchlight and cross-validation step) using default hyperparameters in LIBSVM (<xref ref-type="bibr" rid="bib21">Chang and Lin, 2011</xref>), while MEG analyses were based on pairwise linear discriminant analysis (1,717,731 pairwise classifiers per timepoint and cross-validation step) with default hyperparameters. Iteratively training classifiers on 11 sessions and testing them on the 12th session yielded representational dissimilarity matrices based on classification accuracy for all pairs of comparisons. The reported accuracies reflect the mean of the lower triangular part of these matrices, which corresponds to the mean pairwise decoding accuracy (chance: 50%).</p></sec><sec id="s4-11"><title>fMRI and MEG multidimensional scaling</title><p>To explore the representational structure in fMRI and MEG response patterns evoked by different objects, we visualized their relationships using multidimensional scaling (MDS). For demonstrating the utility of the dataset for identifying meaningful structure from patterns of brain activity alone, we specifically focused on the spatial clustering of known superordinate category information in the datasets. For fMRI, we extracted image-specific parameter estimates from lateral occipital complex and all previously defined category-selective ROIs, with the exception of medial place area, and averaged them across exemplars for each object concept, yielding 720 voxel response patterns at the object concept level. We then fit 2D-MDS based on the correlation distance between these response patterns (10 initializations, 5000 iterations, implemented in scikit-learn <xref ref-type="bibr" rid="bib102">Pedregosa et al., 2011</xref>). For MEG, we directly used the time-resolved pairwise decoding accuracy matrices for all 1854 object concepts from the previous analysis step, fit 2D-MDS in a time-resolved fashion, and iteratively aligned results across time using Procrustes transformation (implemented in the functions cmdscale and procrustes in MATLAB). For plotting the resulting two-dimensional embeddings (<xref ref-type="fig" rid="fig5">Figure 5B and E</xref>), we highlighted superordinate categories with different colors, and for MEG we visualized equally spaced time points with 200ms distance.</p></sec><sec id="s4-12"><title>Object animacy and size analyses</title><p>We aimed at identifying the degree to which previously reported neuroimaging findings regarding object animacy and size generalize to our larger neuroimaging datasets. To this end, we used human animacy and size ratings for all 1854 object concepts, obtained as part of the extended THINGS+ metadata (<xref ref-type="bibr" rid="bib116">Stoinski et al., 2022</xref>). In short, animacy ratings for each object concept in the THINGS database (<xref ref-type="bibr" rid="bib61">Hebart et al., 2019</xref>) were collected by presenting raters with the respective noun and asking them to respond to the property ‘something that lives’ on a Likert scale. Real-world size ratings for each object concept were obtained in two steps. First, raters were instructed to indicate the size of a given object noun on a continuous scale, defined by nine reference objects spanning the size range of all objects (from ‘grain of sand’ to ‘aircraft carrier’). In each trial, raters first indicated the approximate size. In a second step, the rating scale zoomed in on the interval between the closest two anchor points in order to allow raters to give a more refined answer.</p><p>For fMRI, we first fit a simple ordinary least squares linear regression model to the average fMRI response for each object concept (smoothed with FWHM = 3 mm), using z-scored ratings as predictors. Then, we visualized the voxel-wise regression weights on the cortical surface as indicators for the preferred tuning to animate vs. inanimate and big vs. small objects, respectively. For MEG, we ran time-resolved cross-validated ordinary least squares linear regression predicting size and animacy ratings from MEG sensor activation patterns. Note that the direction of inference here is reversed as compared to fMRI for better comparability to previous research findings. Cross-validation was implemented in a leave-one-session-out fashion (11 training sessions, 1 test session) and was based on the correlation between the predicted and the true animacy and size ratings.</p></sec><sec id="s4-13"><title>Multimodal analyses</title><sec id="s4-13-1"><title>Relating neuroimaging data to behavioral similarity judgments</title><p>To demonstrate a use case for integrating the behavioral dataset with the neuroimaging datasets, we conducted representational similarity analysis (<xref ref-type="bibr" rid="bib80">Kriegeskorte et al., 2008a</xref>), comparing representational dissimilarity matrices from patterns of fMRI voxel activity and MEG sensor activity with those obtained for behavioral similarity judgments. To this end, we first computed a large-scale behavioral similarity matrix for all 1854 objects, where object similarity for a given pair of objects <italic>i</italic> and <italic>j</italic> was defined as the triplet choice probability for choosing object <italic>k</italic> as the odd-one out, averaged across all 1852 possible <italic>k</italic>, which was estimated from the choices predicted from the 66-dimensional SPoSE embedding. Next, we converted this matrix to a dissimilarity matrix and extracted its lower triangular part, separately for the 720 concepts for fMRI and all 1854 concepts for MEG. We then took the existing pairwise decoding matrices for all fMRI searchlights and MEG time points that had been computed for the pairwise decoding analyses at the object concept level (see <italic>fMRI and MEG multivariate decoding analyses</italic>), extracted their lower triangular part, and compared it to the behavioral similarity matrix using Pearson’s correlation. This resulted in a representational similarity estimate for each fMRI searchlight location and MEG time point, highlighting the spatial and temporal distribution of effects related to perceived similarity of objects.</p></sec><sec id="s4-13-2"><title>Regression-based MEG-fMRI fusion</title><p>We aimed at demonstrating the usefulness of integrating the multimodal neuroimaging datasets for revealing insights into spatio-temporal evolution of object-related information in the human brain. To this end, the sheer size of the datasets allowed us to combine MEG and fMRI data directly using multiple linear regression (regression-based MEG-fMRI fusion). For our demonstration, we focused on two regions of interest, V1 and FFA, and used the MEG data to predict the univariate BOLD response in these regions. First, we averaged the responses in V1 and FFA across all three fMRI participants. Next, for every timepoint separately, we trained an ordinary least squares linear regression model on MEG sensor data from 11 sessions to predict the response for each trial in V1 and FFA. Then, we used the parameter estimates to predict the fMRI response using the left-out MEG data. We then correlated the predicted V1/FFA response with the true V1/FFA response to examine at what timepoints the image-specific effects observed in V1 and FFA emerged in the MEG data.</p></sec></sec><sec id="s4-14"><title>Code availability</title><p>Code for implementing the main neuroimaging analyses described in this manuscript is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViCCo-Group/THINGS-data">https://github.com/ViCCo-Group/THINGS-data</ext-link>; <xref ref-type="bibr" rid="bib29">Contier et al., 2023</xref>). All relevant code for reproducing results of the behavioral dataset can be found on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/f5rn6/">https://osf.io/f5rn6/</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/F5RN6">https://doi.org/10.17605/OSF.IO/F5RN6</ext-link>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf3"><p>Senior editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Software, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Clinical trial registration NCT00001360.</p></fn><fn fn-type="other"><p>Human subjects: All research participants for the fMRI and MEG studies provided informed consent in participation and data sharing, and they received financial compensation for taking part in the respective studies. The research was approved by the NIH Institutional Review Board as part of the study protocol 93-M-0170 (NCT00001360). All research participants taking part in the online behavioral study provided informed consent for the participation in the study. The online study was conducted in accordance with all relevant ethical regulations and approved by the NIH Office of Human Research Subject Protection (OHSRP).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Acquisition parameters for all MRI sequences used in the fMRI dataset.</title></caption><media xlink:href="elife-82580-supp1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82580-mdarchecklist1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All parts of the THINGS-data collection are freely available on scientific data repositories. We provide the raw MRI (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds004192.v1.0.5">https://doi.org/10.18112/openneuro.ds004192.v1.0.5</ext-link>) and raw MEG (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds004212.v2.0.0">https://doi.org/10.18112/openneuro.ds004212.v2.0.0</ext-link>) datasets in BIDS format (<xref ref-type="bibr" rid="bib42">Gorgolewski et al., 2016</xref>) on OpenNeuro (<xref ref-type="bibr" rid="bib90">Markiewicz et al., 2021</xref>). In addition to these raw datasets, we provide the raw and preprocessed MEG data as well as the raw and derivative MRI data on Figshare (<xref ref-type="bibr" rid="bib118">Thelwall and Kousha, 2016</xref>) at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.6161151">https://doi.org/10.25452/figshare.plus.c.6161151</ext-link>. The MEG data derivatives include preprocessed and epoched data that are compatible with MNE-python and CoSMoMVPA in MATLAB. The MRI data derivatives include single trial response estimates, category-selective and retinotopic regions of interest, cortical flatmaps, independent component based noise regressors, voxel-wise noise ceilings, and estimates of subject specific retinotopic parameters. In addition, we included the preprocessed and epoched eyetracking data that were recorded during the MEG experiment in the OpenNeuro repository. The behavioral triplet odd-one-out dataset can be accessed on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/f5rn6/">https://osf.io/f5rn6/</ext-link>).</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Contier</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Rockter</surname><given-names>AH</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>THINGS-fMRI</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds004192.v1.0.5</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Contier</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Rockter</surname><given-names>AH</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>THINGS-MEG</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds004212.v2.0.0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Contier</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Rockter</surname><given-names>AH</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>THINGS-odd-one-out</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/F5RN6</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset4"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Contier</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Rockter</surname><given-names>A</given-names></name><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>THINGS-data: A multimodal collection of large-scale datasets for investigating object representations in brain and behavior</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.25452/figshare.plus.c.6161151.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Elissa Aminoff, Kendrick Kay, Alex Martin, Thomas Naselaris, Francisco Pereira, and Michael Tarr for useful discussions in the design stage of these datasets. Additional thanks to Tom Holroyd, Sean Marrett, Frank Sutak, and Dardo Tomasi for technical support with the fMRI and MEG facilities and to Govind Bhagavatheeshwaran and Sean Marrett for support designing MRI sequences. Thanks to James Gao for continued support with generating and using custom MRI head cases. Special thanks to Ed Silson for sharing the functional and retinotopic localizer code, to Christian Büchel for allowing us to use and share their code for converting raw physiological recordings to physiological regressors, and to Lukas Muttenthaler for creating a faster and more versatile version of the SPoSE embedding code. Thanks to Ülkühan Tonbuloglu and Julia Norman for manual labeling of independent components. We are grateful for useful discussions with Kendrick Kay and Jacob Prince on single trial parameter estimates and with Talia Konkle on object animacy and size effects. We would like to thank Jason Avery, Marius Cătălin Iordan, and Joshua Peterson for sharing their object similarity matrices. To run the MEG analysis we utilized the computational resources of the NIH HPC Biowulf cluster. (<ext-link ext-link-type="uri" xlink:href="http://hpc.nih.gov">http://hpc.nih.gov</ext-link>). We are grateful to Jeff Stout, Ashita Basavaraj, and Anthony Galassi for their help with using the HPC. This work was supported by the Intramural Research Program of the National Institutes of Health (ZIA-MH-002909, ZIC-MH002968), under National Institute of Mental Health Clinical Study Protocol 93 M-1070 (NCT00001360), a research group grant by the Max Planck Society awarded to MNH, the ERC Starting Grant project COREDIM (101039712), and the Hessian Ministry of Higher Education, Science, Research and Art (LOEWE Start Professorship to MNH and Excellence Program &quot;The Adaptive Mind&quot;). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00962-x</pub-id><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Skare</surname><given-names>S</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Mruczek</surname><given-names>REB</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Widespread correlation patterns of fMRI signal across visual cortex reflect eccentricity organization</article-title><source>eLife</source><volume>4</volume><elocation-id>e03952</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03952</pub-id><pub-id pub-id-type="pmid">25695154</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Avery</surname><given-names>J</given-names></name><name><surname>Carrington</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>Representation of naturalistic food categories in the human brain</article-title><conf-name>22nd Annual Meeting of the Vision Sciences Society</conf-name><year iso-8601-date="2022">2022</year><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/jov.22.14.3426">https://doi.org/10.1167/jov.22.14.3426</ext-link><pub-id pub-id-type="doi">10.1167/jov.22.14.3426</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Magnetoencephalography for brain electrophysiology and imaging</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>327</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1038/nn.4504</pub-id><pub-id pub-id-type="pmid">28230841</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title><source>NeuroImage</source><volume>178</volume><fpage>172</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.037</pub-id><pub-id pub-id-type="pmid">29777825</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Probabilistic independent component analysis for functional magnetic resonance imaging</article-title><source>IEEE Transactions on Medical Imaging</source><volume>23</volume><fpage>137</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1109/TMI.2003.822821</pub-id><pub-id pub-id-type="pmid">14964560</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian analysis of retinotopic maps</article-title><source>eLife</source><volume>7</volume><elocation-id>e40224</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.40224</pub-id><pub-id pub-id-type="pmid">30520736</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Perez</surname><given-names>E</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Electrophysiological studies of face perception in humans</article-title><source>Journal of Cognitive Neuroscience</source><volume>8</volume><fpage>551</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.551</pub-id><pub-id pub-id-type="pmid">20740065</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Human image understanding: recent research and a theory</article-title><source>Computer Vision, Graphics, and Image Processing</source><volume>32</volume><fpage>29</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/0734-189X(85)90002-7</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bobadilla-Suarez</surname><given-names>S</given-names></name><name><surname>Ahlheim</surname><given-names>C</given-names></name><name><surname>Mehrotra</surname><given-names>A</given-names></name><name><surname>Panos</surname><given-names>A</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Measures of neural similarity</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>369</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1007/s42113-019-00068-5</pub-id><pub-id pub-id-type="pmid">33225218</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Boring</surname><given-names>MJ</given-names></name><name><surname>Richardson</surname><given-names>RM</given-names></name><name><surname>Ghuman</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Interacting Cortical Gradients of Neural Timescales and Functional Connectivity and Their Relationship to Perceptual Behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.05.490070</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Daniels</surname><given-names>N</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task context overrules object- and category-related representational content in the human parietal cortex</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>310</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw419</pub-id><pub-id pub-id-type="pmid">28108492</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Brock</surname><given-names>A</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large scale GAN training for high fidelity natural image synthesis</article-title><conf-name>International Conference on Learning Representations</conf-name><pub-id pub-id-type="doi">10.48550/arXiv.1809.11096</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullier</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Integrated model of visual processing</article-title><source>Brain Research Reviews</source><volume>36</volume><fpage>96</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/s0165-0173(01)00085-6</pub-id><pub-id pub-id-type="pmid">11690606</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname><given-names>KS</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Mokrysz</surname><given-names>C</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Flint</surname><given-names>J</given-names></name><name><surname>Robinson</surname><given-names>ESJ</given-names></name><name><surname>Munafò</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Power failure: why small sample size undermines the reliability of neuroscience</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1038/nrn3475</pub-id><pub-id pub-id-type="pmid">23571845</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Shelton</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Domain-specific knowledge systems in the brain: the animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>10</volume><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1162/089892998563752</pub-id><pub-id pub-id-type="pmid">9526080</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>N</given-names></name><name><surname>Pyles</surname><given-names>JA</given-names></name><name><surname>Marcus</surname><given-names>A</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Aminoff</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>BOLD5000, a public fMRI dataset while viewing 5000 visual images</article-title><source>Scientific Data</source><volume>6</volume><elocation-id>49</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-019-0052-3</pub-id><pub-id pub-id-type="pmid">31061383</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Ramirez</surname><given-names>FM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Can visual information encoded in cortical columns be decoded from magnetoencephalography data in humans?</article-title><source>NeuroImage</source><volume>121</volume><fpage>193</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.011</pub-id><pub-id pub-id-type="pmid">26162550</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The spatiotemporal neural dynamics underlying perceived similarity for real-world objects</article-title><source>NeuroImage</source><volume>194</volume><fpage>12</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.031</pub-id><pub-id pub-id-type="pmid">30894333</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A M/EEG-fmri fusion primer: resolving human brain responses in space and time</article-title><source>Neuron</source><volume>107</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id><pub-id pub-id-type="pmid">32721379</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>KI</given-names></name><name><surname>Devereux</surname><given-names>B</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From perception to conception: how meaningful objects are processed over time</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>187</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs002</pub-id><pub-id pub-id-type="pmid">22275484</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting the time course of individual objects with MEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3602</fpage><lpage>3612</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu203</pub-id><pub-id pub-id-type="pmid">25209607</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Contier</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><collab>Vision and Computational Cognition Group</collab></person-group><year iso-8601-date="2023">2023</year><data-title>THINGS-data</data-title><version designator="swh:1:rev:2d95c15d3a2cc5984ffd4a9a2c4ad3496847ca9d">swh:1:rev:2d95c15d3a2cc5984ffd4a9a2c4ad3496847ca9d</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:103c132af8acae684ffb969752a28f0beb4c1c2a;origin=https://github.com/ViCCo-Group/THINGS-data;visit=swh:1:snp:4c1e0fab18d6e2d8038137bab93d0cb9721ba358;anchor=swh:1:rev:2d95c15d3a2cc5984ffd4a9a2c4ad3496847ca9d">https://archive.softwareheritage.org/swh:1:dir:103c132af8acae684ffb969752a28f0beb4c1c2a;origin=https://github.com/ViCCo-Group/THINGS-data;visit=swh:1:snp:4c1e0fab18d6e2d8038137bab93d0cb9721ba358;anchor=swh:1:rev:2d95c15d3a2cc5984ffd4a9a2c4ad3496847ca9d</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deffke</surname><given-names>I</given-names></name><name><surname>Sander</surname><given-names>T</given-names></name><name><surname>Heidenreich</surname><given-names>J</given-names></name><name><surname>Sommer</surname><given-names>W</given-names></name><name><surname>Curio</surname><given-names>G</given-names></name><name><surname>Trahms</surname><given-names>L</given-names></name><name><surname>Lueschow</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>MEG/EEG sources of the 170-ms response to faces are co-localized in the fusiform gyrus</article-title><source>NeuroImage</source><volume>35</volume><fpage>1495</fpage><lpage>1501</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.034</pub-id><pub-id pub-id-type="pmid">17363282</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The face-sensitivity of the N170 component</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>119</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00119</pub-id><pub-id pub-id-type="pmid">22022313</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>FMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>JS</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Gallant</surname><given-names>JLP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Pycortex: an interactive surface visualizer for fMRI</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fninf.2015.00023</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghuman</surname><given-names>AS</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dynamic neural representations: an inferential challenge for fMRI</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>534</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.04.004</pub-id><pub-id pub-id-type="pmid">31103440</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gifford</surname><given-names>AT</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A large and rich EEG dataset for modeling human visual object recognition</article-title><source>NeuroImage</source><volume>264</volume><elocation-id>119754</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119754</pub-id><pub-id pub-id-type="pmid">36400378</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Auer</surname><given-names>T</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name><name><surname>Craddock</surname><given-names>RC</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Duff</surname><given-names>EP</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Glatard</surname><given-names>T</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Handwerker</surname><given-names>DA</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Keator</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Michael</surname><given-names>Z</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>Nichols</surname><given-names>BN</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Pellman</surname><given-names>J</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Schaefer</surname><given-names>G</given-names></name><name><surname>Sochat</surname><given-names>V</given-names></name><name><surname>Triplett</surname><given-names>W</given-names></name><name><surname>Turner</surname><given-names>JA</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title><source>Scientific Data</source><volume>3</volume><elocation-id>160044</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id><pub-id pub-id-type="pmid">27326542</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG and EEG data analysis with MNE-python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Knouf</surname><given-names>N</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The fusiform face area subserves face perception, not generic within-category identification</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>555</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/nn1224</pub-id><pub-id pub-id-type="pmid">15077112</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contributions of low- and high-level properties to neural processing of visual scenes in the human brain</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160102</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0102</pub-id><pub-id pub-id-type="pmid">28044013</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Theta-burst TMS of lateral occipital cortex reduces BOLD responses across category-selective areas in ventral temporal cortex</article-title><source>NeuroImage</source><volume>230</volume><elocation-id>117790</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117790</pub-id><pub-id pub-id-type="pmid">33497776</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Dekker</surname><given-names>TM</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visuospatial coding as ubiquitous scaffolding for human cognition</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>81</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.10.011</pub-id><pub-id pub-id-type="pmid">34799253</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Wardle</surname><given-names>SG</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Asymmetric compression of representational space for object animacy categorization under degraded viewing conditions</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>1995</fpage><lpage>2010</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01177</pub-id><pub-id pub-id-type="pmid">28820673</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Finding decodable information that can be read out in behaviour</article-title><source>NeuroImage</source><volume>179</volume><fpage>252</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.022</pub-id><pub-id pub-id-type="pmid">29886145</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Shatek</surname><given-names>SM</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Untangling featural and conceptual object representations</article-title><source>NeuroImage</source><volume>202</volume><elocation-id>116083</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116083</pub-id><pub-id pub-id-type="pmid">31400529</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Zhou</surname><given-names>I</given-names></name><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams</article-title><source>Scientific Data</source><volume>9</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-021-01102-7</pub-id><pub-id pub-id-type="pmid">35013331</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gulban</surname><given-names>OF</given-names></name><name><surname>Nielson</surname><given-names>D</given-names></name><name><surname>Poldrack</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Gorgolewski</surname><given-names>C</given-names></name><name><surname>Ghosh</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Poldracklab/pydeface</data-title><version designator="v2.0.0">v2.0.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3524401">https://doi.org/10.5281/zenodo.3524401</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task context impacts visual object processing differentially across the cortex</article-title><source>PNAS</source><volume>111</volume><fpage>E962</fpage><lpage>E971</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312567111</pub-id><pub-id pub-id-type="pmid">24567402</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Conroy</surname><given-names>BR</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Ramadge</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A common, high-dimensional model of the representational space in human ventral temporal cortex</article-title><source>Neuron</source><volume>72</volume><fpage>404</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.026</pub-id><pub-id pub-id-type="pmid">22017997</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives</article-title><source>Neuron</source><volume>87</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.025</pub-id><pub-id pub-id-type="pmid">26182413</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deconstructing multivariate decoding for the study of brain function</article-title><source>NeuroImage</source><volume>180</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.005</pub-id><pub-id pub-id-type="pmid">28782682</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The representational dynamics of task and object processing in humans</article-title><source>eLife</source><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id><pub-id pub-id-type="pmid">29384473</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Dickter</surname><given-names>AH</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Kwok</surname><given-names>WY</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Van Wicklin</surname><given-names>C</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>THINGS: a database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0223792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0223792</pub-id><pub-id pub-id-type="pmid">31613926</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</article-title><source>Nature Human Behaviour</source><volume>4</volume><fpage>1173</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00951-3</pub-id><pub-id pub-id-type="pmid">33046861</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15037</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id><pub-id pub-id-type="pmid">28530228</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id><pub-id pub-id-type="pmid">23259955</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Dupre La Tour</surname><given-names>T</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Gallant lab natural short clips 3T fmri data</data-title><source>G-Node</source><pub-id pub-id-type="doi">10.12751/G-NODE.VY1ZJD</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iordan</surname><given-names>MC</given-names></name><name><surname>Giallanza</surname><given-names>T</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Beckage</surname><given-names>NM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Context matters: recovering human semantic structure from machine learning analysis of large-scale text corpora</article-title><source>Cognitive Science</source><volume>46</volume><elocation-id>e13085</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.13085</pub-id><pub-id pub-id-type="pmid">35146779</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Webster</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An algorithmic method for functionally defining regions of interest in the ventral visual pathway</article-title><source>NeuroImage</source><volume>60</volume><fpage>2357</fpage><lpage>2364</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.055</pub-id><pub-id pub-id-type="pmid">22398396</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaniuth</surname><given-names>P</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Feature-reweighted representational similarity analysis: a method for improving the fit between computational models, brains, and behavior</article-title><source>NeuroImage</source><volume>257</volume><elocation-id>119294</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119294</pub-id><pub-id pub-id-type="pmid">35580810</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/jneurosci.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><volume>452</volume><fpage>352</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature06713</pub-id><pub-id pub-id-type="pmid">18322462</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tracking the spatiotemporal neural dynamics of real-world object size and animacy in the human brain</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1559</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01290</pub-id><pub-id pub-id-type="pmid">29877767</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id><pub-id pub-id-type="pmid">31591217</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A real-world size organization of object responses in occipitotemporal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>1114</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.036</pub-id><pub-id pub-id-type="pmid">22726840</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tripartite organization of the ventral stream by animacy and object size</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>10235</fpage><lpage>10242</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0983-13.2013</pub-id><pub-id pub-id-type="pmid">23785139</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>MA</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Bainbridge</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The Features Underlying the Memorability of Objects</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.04.29.490104</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>Sjak-Shie</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Preprocessing pupil size data: guidelines and code</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>1336</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1075-y</pub-id><pub-id pub-id-type="pmid">29992408</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name><name><surname>Douglas</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cognitive computational neuroscience</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1148</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0210-5</pub-id><pub-id pub-id-type="pmid">30127428</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain-like object recognition with high-performing shallow recurrent ANNs</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lage-Castellanos</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Methods for computing the maximum performance of computational models of fmri responses</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006397</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006397</pub-id><pub-id pub-id-type="pmid">30849071</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehky</surname><given-names>SR</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensionality of object representations in monkey inferotemporal cortex</article-title><source>Neural Computation</source><volume>26</volume><fpage>2135</fpage><lpage>2162</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00648</pub-id><pub-id pub-id-type="pmid">25058707</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Maire</surname><given-names>M</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Microsoft COCO: common objects in context</chapter-title><person-group person-group-type="editor"><name><surname>Tuytelaars</surname><given-names>T</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name></person-group><source>Computer Vision - ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</source><publisher-name>Springer International Publishing</publisher-name><fpage>740</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10602-1</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindquist</surname><given-names>MA</given-names></name><name><surname>Geuter</surname><given-names>S</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Caffo</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modular preprocessing pipelines can reintroduce artifacts into fMRI data</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>2358</fpage><lpage>2376</lpage><pub-id pub-id-type="doi">10.1002/hbm.24528</pub-id><pub-id pub-id-type="pmid">30666750</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Benson</surname><given-names>RR</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Kennedy</surname><given-names>WA</given-names></name><name><surname>Ledden</surname><given-names>PJ</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>PNAS</source><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.18.8135</pub-id><pub-id pub-id-type="pmid">7667258</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marek</surname><given-names>S</given-names></name><name><surname>Tervo-Clemmens</surname><given-names>B</given-names></name><name><surname>Calabro</surname><given-names>FJ</given-names></name><name><surname>Montez</surname><given-names>DF</given-names></name><name><surname>Kay</surname><given-names>BP</given-names></name><name><surname>Hatoum</surname><given-names>AS</given-names></name><name><surname>Donohue</surname><given-names>MR</given-names></name><name><surname>Foran</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>RL</given-names></name><name><surname>Hendrickson</surname><given-names>TJ</given-names></name><name><surname>Malone</surname><given-names>SM</given-names></name><name><surname>Kandala</surname><given-names>S</given-names></name><name><surname>Feczko</surname><given-names>E</given-names></name><name><surname>Miranda-Dominguez</surname><given-names>O</given-names></name><name><surname>Graham</surname><given-names>AM</given-names></name><name><surname>Earl</surname><given-names>EA</given-names></name><name><surname>Perrone</surname><given-names>AJ</given-names></name><name><surname>Cordova</surname><given-names>M</given-names></name><name><surname>Doyle</surname><given-names>O</given-names></name><name><surname>Moore</surname><given-names>LA</given-names></name><name><surname>Conan</surname><given-names>GM</given-names></name><name><surname>Uriarte</surname><given-names>J</given-names></name><name><surname>Snider</surname><given-names>K</given-names></name><name><surname>Lynch</surname><given-names>BJ</given-names></name><name><surname>Wilgenbusch</surname><given-names>JC</given-names></name><name><surname>Pengo</surname><given-names>T</given-names></name><name><surname>Tam</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Newbold</surname><given-names>DJ</given-names></name><name><surname>Zheng</surname><given-names>A</given-names></name><name><surname>Seider</surname><given-names>NA</given-names></name><name><surname>Van</surname><given-names>AN</given-names></name><name><surname>Metoki</surname><given-names>A</given-names></name><name><surname>Chauvin</surname><given-names>RJ</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Greene</surname><given-names>DJ</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Garavan</surname><given-names>H</given-names></name><name><surname>Thompson</surname><given-names>WK</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Luna</surname><given-names>B</given-names></name><name><surname>Fair</surname><given-names>DA</given-names></name><name><surname>Dosenbach</surname><given-names>NUF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reproducible brain-wide association studies require thousands of individuals</article-title><source>Nature</source><volume>603</volume><fpage>654</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-04492-9</pub-id><pub-id pub-id-type="pmid">35296861</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Feingold</surname><given-names>F</given-names></name><name><surname>Blair</surname><given-names>R</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Miller</surname><given-names>E</given-names></name><name><surname>Hardcastle</surname><given-names>N</given-names></name><name><surname>Wexler</surname><given-names>J</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Goncavles</surname><given-names>M</given-names></name><name><surname>Jwa</surname><given-names>A</given-names></name><name><surname>Poldrack</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The openneuro resource for sharing of neuroscience data</article-title><source>eLife</source><volume>10</volume><elocation-id>e71774</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71774</pub-id><pub-id pub-id-type="pmid">34658334</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Jones</surname><given-names>EC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An ecologically motivated image dataset for deep learning yields better models of human vision</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2011417118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2011417118</pub-id><pub-id pub-id-type="pmid">33593900</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>S</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title><source>eLife</source><volume>7</volume><elocation-id>e36329</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id><pub-id pub-id-type="pmid">29927384</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Meys</surname><given-names>M</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>128</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00128</pub-id><pub-id pub-id-type="pmid">23525516</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Birn</surname><given-names>RM</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Resting-State fMRI confounds and cleanup</article-title><source>NeuroImage</source><volume>80</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.001</pub-id><pub-id pub-id-type="pmid">23571418</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Muttenthaler</surname><given-names>L</given-names></name><name><surname>Hansen</surname><given-names>H</given-names></name><collab>Vision and Computational Cognition Group</collab></person-group><year iso-8601-date="2022">2022a</year><data-title>SPoSE</data-title><version designator="1d7b152">1d7b152</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ViCCo-Group/SPoSE">https://github.com/ViCCo-Group/SPoSE</ext-link></element-citation></ref><ref id="bib96"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Muttenthaler</surname><given-names>L</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>McClure</surname><given-names>P</given-names></name><name><surname>Vandermeulen</surname><given-names>RA</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>VICE: Variational Interpretable Concept Embeddings</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2205.00756</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extensive sampling for complete models of individual brains</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>45</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.12.008</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in matlab/GNU octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orban</surname><given-names>C</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chee</surname><given-names>MWL</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Time of day is associated with paradoxical reductions in global signal fluctuation and functional connectivity</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000602</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000602</pub-id><pub-id pub-id-type="pmid">32069275</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Kopf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title><source>Advances in Neural Information Processing Systems</source><volume>32</volume><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Brucher</surname><given-names>M</given-names></name><name><surname>Perrot</surname><given-names>M</given-names></name><name><surname>Duchesnau</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>JC</given-names></name><name><surname>Abbott</surname><given-names>JT</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Evaluating (and improving) the correspondence between deep neural networks and human representations</article-title><source>Cognitive Science</source><volume>42</volume><fpage>2648</fpage><lpage>2669</lpage><pub-id pub-id-type="doi">10.1111/cogs.12670</pub-id><pub-id pub-id-type="pmid">30178468</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kurzawski</surname><given-names>JW</given-names></name><name><surname>Pyles</surname><given-names>JA</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Improving the accuracy of single-trial fMRI response estimates using glmsingle</article-title><source>eLife</source><volume>11</volume><elocation-id>e77599</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.77599</pub-id><pub-id pub-id-type="pmid">36444984</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pruim</surname><given-names>RHR</given-names></name><name><surname>Mennes</surname><given-names>M</given-names></name><name><surname>van Rooij</surname><given-names>D</given-names></name><name><surname>Llera</surname><given-names>A</given-names></name><name><surname>Buitelaar</surname><given-names>JK</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ICA-AROMA: a robust ICA-based strategy for removing motion artifacts from fMRI data</article-title><source>NeuroImage</source><volume>112</volume><fpage>267</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.02.064</pub-id><pub-id pub-id-type="pmid">25770991</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramírez</surname><given-names>FM</given-names></name><name><surname>Revsine</surname><given-names>C</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>What do across-subject analyses really tell us about neural coding?</article-title><source>Neuropsychologia</source><volume>143</volume><elocation-id>107489</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107489</pub-id><pub-id pub-id-type="pmid">32437761</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Emerging object representations in the visual system predict reaction times for categorization</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004316</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004316</pub-id><pub-id pub-id-type="pmid">26107634</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The influence of image masking on object representations during rapid serial visual presentation</article-title><source>NeuroImage</source><volume>197</volume><fpage>224</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.050</pub-id><pub-id pub-id-type="pmid">31009746</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fractional ridge regression: a fast, interpretable reparameterization of ridge regression</article-title><source>GigaScience</source><volume>9</volume><elocation-id>giaa133</elocation-id><pub-id pub-id-type="doi">10.1093/gigascience/giaa133</pub-id><pub-id pub-id-type="pmid">33252656</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Automatic denoising of functional MRI data: combining independent component analysis and hierarchical fusion of classifiers</article-title><source>NeuroImage</source><volume>90</volume><fpage>449</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.046</pub-id><pub-id pub-id-type="pmid">24389422</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmolesky</surname><given-names>MT</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Hanes</surname><given-names>DP</given-names></name><name><surname>Thompson</surname><given-names>KG</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Schall</surname><given-names>JD</given-names></name><name><surname>Leventhal</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Signal timing across the macaque visual system</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>3272</fpage><lpage>3278</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.6.3272</pub-id><pub-id pub-id-type="pmid">9636126</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Chan</surname><given-names>AWY</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A retinotopic basis for the division of high-level scene processing between lateral and ventral human occipitotemporal cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11921</fpage><lpage>11935</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0137-15.2015</pub-id><pub-id pub-id-type="pmid">26311774</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>PL</given-names></name><name><surname>Little</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Small is beautiful: in defense of the small-N design</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>2083</fpage><lpage>2101</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1451-8</pub-id><pub-id pub-id-type="pmid">29557067</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steel</surname><given-names>A</given-names></name><name><surname>Thomas</surname><given-names>C</given-names></name><name><surname>Trefler</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Finding the baby in the bath water - evidence for task-specific changes in resting state functional connectivity evoked by training</article-title><source>NeuroImage</source><volume>188</volume><fpage>524</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.12.038</pub-id><pub-id pub-id-type="pmid">30578926</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stoinski</surname><given-names>LM</given-names></name><name><surname>Perkuhn</surname><given-names>J</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>THINGSplus: new norms and metadata for the THINGS database of 1,854 object Concepts and 26,107 natural object images</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/exu9f</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname><given-names>L</given-names></name><name><surname>Schütz</surname><given-names>AC</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>What is the best fixation target? the effect of target shape on stability of fixational eye movements</article-title><source>Vision Research</source><volume>76</volume><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id><pub-id pub-id-type="pmid">23099046</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thelwall</surname><given-names>M</given-names></name><name><surname>Kousha</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Figshare: a universal repository for academic resource sharing?</article-title><source>Online Information Review</source><volume>40</volume><fpage>333</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1108/OIR-06-2015-0190</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Weinrib</surname><given-names>O</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Response properties of the human fusiform face area</article-title><source>Cognitive Neuropsychology</source><volume>17</volume><fpage>257</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1080/026432900380607</pub-id><pub-id pub-id-type="pmid">20945183</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Janini</surname><given-names>D</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mid-level feature differences support early animacy and object size distinctions: evidence from electroencephalography decoding</article-title><source>Journal of Cognitive Neuroscience</source><volume>34</volume><fpage>1670</fpage><lpage>1680</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01883</pub-id><pub-id pub-id-type="pmid">35704550</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wardle</surname><given-names>SG</given-names></name><name><surname>Taubert</surname><given-names>J</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rapid and dynamic processing of face pareidolia in the human brain</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4518</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18325-8</pub-id><pub-id pub-id-type="pmid">32908146</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welbourne</surname><given-names>LE</given-names></name><name><surname>Jonnalagadda</surname><given-names>A</given-names></name><name><surname>Giesbrecht</surname><given-names>B</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The transverse occipital sulcus and intraparietal sulcus show neural selectivity to object-scene size relationships</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>768</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02294-9</pub-id><pub-id pub-id-type="pmid">34158579</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Ripley</surname><given-names>BD</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporal autocorrelation in univariate linear modeling of fMRI data</article-title><source>NeuroImage</source><volume>14</volume><fpage>1370</fpage><lpage>1386</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0931</pub-id><pub-id pub-id-type="pmid">11707093</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multilevel linear modelling for fMRI group analysis using Bayesian inference</article-title><source>NeuroImage</source><volume>21</volume><fpage>1732</fpage><lpage>1747</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.12.023</pub-id><pub-id pub-id-type="pmid">15050594</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>J</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Ehinger</surname><given-names>KA</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>SUN database: Large-scale scene recognition from abbey to zoo</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2010.5539970</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Robert</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curvature processing in human visual cortical areas</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117295</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117295</pub-id><pub-id pub-id-type="pmid">32835823</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Revealing interpretable object representations from human behavior</article-title><conf-name>7th International Conference on Learning Representations (ICLR</conf-name><fpage>1</fpage><lpage>16</lpage></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Concept and image selection strategies</title><p>The 720 categories as well as the representative test sets of 100 and 200 images were selected based on two criteria: to maximize overlap with the concepts included in the machine learning image dataset Ecoset (<xref ref-type="bibr" rid="bib91">Mehrer et al., 2021</xref>) and to be visually and conceptually as representative of the overall THINGS image set as possible. Ecoset offers a more natural distribution of object concepts than typically used machine learning image databases and has a strong overlap in concepts with those used in THINGS. Maximal overlap with Ecoset was thus chosen to allow for better training of neural networks using the same concepts and thus better comparability with THINGS-data. To select images that are visually and conceptually representative of the overall image set, we first selected the intersection of concepts between Ecoset and THINGS and included these concepts (n=470). Next, we ran spectral clustering (k=80) on all THINGS concepts and images using activations separately in two layers of the brain-inspired neural network CorNet (<xref ref-type="bibr" rid="bib83">Kubilius et al., 2019</xref>): Layer V1 and layer IT, with the aim of being representative of early and high-level visual processing. Finally, we additionally ran spectral clustering (k=80) on the 49 dimensions from the original computational model derived from behavioral odd-one-out choices (<xref ref-type="bibr" rid="bib62">Hebart et al., 2020</xref>), with the aim of being representative of mental representations of objects in humans. For the selection of the subsets of 720 concepts, we next identified the concepts that were as representative as possible of all 240 clusters and their cluster sizes, using a greedy selection approach iteratively swapping in and out pairs of images until all clusters were sampled representatively. Once the 720 categories had been determined, we repeated this approach for the 200 images for the MEG dataset, this time based on all remaining images of the 720 concepts that are not used as main experimental stimuli. Finally, from these 200 images, we selected the 100 most representative for the MRI dataset.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Eye-tracking processing and results</title><p>During the MEG sessions, we recorded eye-tracking data using an EyeLink 1000 Plus Eye-Tracking System as a means for controlling that participants were able to maintain central fixation. Eye movements were recorded from one eye, with x-coordinates, y- coordinates, and pupil size being fed directly into miscellaneous sensors of the MEG (sensors UADC009-2104, UADC010-2101, UADC013-2104, respectively) with a sampling rate of 1200 Hz. We preprocessed the continuous eye-tracking data before epoching in the same way as the MEG data (−100–1300ms relative to stimulus onset) to assess how well participants fixated.</p></sec><sec sec-type="appendix" id="s10"><title>Eye-tracking preprocessing</title><p>We preprocessed the eye-tracking data separately for each run (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A</xref>) and based our pipeline on previous work (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib79">Kret and Sjak-Shie, 2019</xref>). First, we removed invalid samples which we defined as x- and y- eye positions beyond the stimulus edges (10°). Then, we removed samples based on pupil dilation speed to detect eyeblinks. We calculated the pupil dilation changes from one timepoint to the next and examined when the dilation speed changed more than a threshold. The threshold was determined by examining the median absolute deviation from all dilation speeds in the run multiplied by a constant of 16 (<xref ref-type="bibr" rid="bib79">Kret and Sjak-Shie, 2019</xref>). As the dilation speed threshold may not detect initial phases of the eyelid closure, we expanded the gap by 100ms before and 150ms after the blink occurred (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>). We then removed samples that were temporally isolated (≥40ms away from other valid measurements) and only had a few consecutive valid measurements around them (max. 100ms). In addition, we fitted a smooth line to the pupil size data and excluded samples with a larger deviation (<xref ref-type="bibr" rid="bib79">Kret and Sjak-Shie, 2019</xref>). Finally, we ran linear detrending on the x- and y-coordinates as well as the pupil size data to account for slow drifts over the run. On average, we removed ~10% of the eye-tracking samples during preprocessing (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1B</xref>).</p></sec><sec sec-type="appendix" id="s11"><title>Eye-tracking results</title><p>The results (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1C</xref>) show that all four MEG participants fixated well. The gaze position of all participants was within 1 degree of the stimulus in more than 95% of all valid samples (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1C</xref>). Looking at the time-resolved data, it seems that on average there was only minimal time-locked eye movement (max. 0.3 degrees, see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1D</xref>). In addition, we found no consistent pattern of pupil size changes across time (see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1E</xref>). Together, this indicates that participants mostly fixated during the MEG experiment.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Eye-tracking preprocessing and results.</title><p>(<bold>A</bold>) Visual illustration of the eye-tracking preprocessing routine. Raw data for one example run (top row) and preprocessed data for the same run (bottom row). (<bold>B</bold>) Amount of eye-tracking data removed during preprocessing in each session for each participant separately, sorted by proportion removed. On average we lost around 10% of the eye-tracking samples during preprocessing. (<bold>C</bold>) Gaze positions for all four participants. The large panel shows eye positions across sessions for each participant (downsampled to 100 Hz). To quantify fixations, we added rings to the gaze position plots corresponding to containing 25% (black) and 75% (white) of the data. In addition, we examined the proportion of data falling below different thresholds (small panel top right corner within the large panels). The vertical dashed lines indicate the 1 degree mark in all panels. (<bold>D</bold>) Mean time-resolved distance in gaze position relative to the baseline period in each trial. Shading represents standard error across all trials. (<bold>E</bold>) Time-resolved pupil size in volts. Larger numbers reflect a larger pupil area. Shading represents standard error across sessions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82580-app2-fig1-v3.tif"/></fig></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s12"><title>fMRI population receptive field mapping and category localizer</title><sec sec-type="appendix" id="s12-1"><title>fMRI population receptive field mapping and early visual regions of interest</title><p>The purpose of the pRF experiment was to estimate subject-specific retinotopy by stimulating different parts of the visual field. We adapted a paradigm used in previous studies (<xref ref-type="bibr" rid="bib113">Silson et al., 2015</xref>). Participants saw natural scene images through a bar aperture that swept across the screen. Stimuli were presented on a mid-grey background masked by a circular region (10.6° radius). Bars swept along 8 directions (horizontal, vertical, and diagonal axes, bidirectional). Each bar sweep was split into 18 positions, each lasting 3 s (54 s per sweep), and 10 scene stimuli were presented briefly (300ms) within the mask at each position. Each of the 90 scene images were presented twice in each sweep. A functional run (~8 min) entailed the bar mask sweeping along all 8 directions, plus an additional 15 s of fixation in the beginning and end. Participants carried out a task at fixation where they had to indicate a change in color of the white fixation dot. Participants performed 4–6 functional pRF runs during the localizer sessions.</p><p>We estimated each subject’s individual retinotopy based on a population receptive field model (<xref ref-type="bibr" rid="bib34">Dumoulin and Wandell, 2008</xref>) of the sweeping bar experiment. As additional preprocessing, we applied a temporal filter to each functional run (100 s high pass) and normalized the resulting voxel-wise time series to zero mean and unit variance before averaging them across functional runs. We estimated retinotopic parameters - eccentricity, polar angle, and receptive field size - in each voxel based on a circular population receptive field model as implemented in AFNI (<xref ref-type="bibr" rid="bib30">Cox, 1996</xref>). After projecting these results to the cortical surface, we used a Bayesian mapping approach that further refined these individual parameter estimates based on a predefined prior retinotopic group atlas that automatically delineates retinotopic visual regions accordingly (<xref ref-type="bibr" rid="bib9">Benson and Winawer, 2018</xref>). These retinotopic visual regions of interest include V1-V3, hV4, VO1/VO2, LO1/LO2, TO1/TO2, and V3a/V3b, which were also resampled from the individual subject’s cortical surface representation to functional volume space.</p></sec><sec sec-type="appendix" id="s12-2"><title>fMRI localizer of object category selective regions</title><p>The aim of the category localizer experiment was to identify brain regions responding selectively to specific object categories. To this end, we adapted a functional localizer paradigm used in previous studies (<xref ref-type="bibr" rid="bib47">Groen et al., 2021</xref>). Participants saw images of faces, body parts, scenes, words, objects, and scrambled object images in a block design. Each category block was presented twice per functional run with a duration of 15 s. Each functional run also contained fixation periods of 15 s in the beginning and 30 s in the end (4.5 min in total). The experiment included four functional runs, and the order of blocks within each run was randomized.</p><p>We aimed at identifying brain regions that are known to show increased activity to images of specific object categories. To this end, we fitted a general linear model (GLM) to the fMRI data of the object category localizer experiment (FSL version 5.0 <xref ref-type="bibr" rid="bib123">Woolrich et al., 2001</xref> as implemented in Nipype <xref ref-type="bibr" rid="bib41">Gorgolewski et al., 2011</xref>). Each functional run was spatially smoothed with a FWHM of 5 mm and entered in a GLM with regressors for body parts, faces, objects, scenes, words, and scrambled objects. We defined T-contrasts to estimate the selective response to object categories (body parts &gt;objects, faces &gt;objects, scenes &gt;objects, objects &gt;scrambled). The resulting statistical parametric maps were aggregated across functional runs within each subject with a fixed effects model (<xref ref-type="bibr" rid="bib124">Woolrich et al., 2004</xref>) and corrected for multiple comparisons (cluster p-threshold=0.0001, extent-threshold=3.7). The resulting subject-specific clusters were intersected with an existing group parcellation of category-selective brain areas (<xref ref-type="bibr" rid="bib68">Julian et al., 2012</xref>) to yield the final regions of interest: Fusiform face area (FFA), occipital face area (OFA), posterior superior temporal sulcus (pSTS), extrastriate body area (EBA), parahippocampal place area (PPA), medial place area / retrosplenial complex (MPA), occipital place area (OPA), and lateral occipital cortex (LOC).</p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82580.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.07.22.501123" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.22.501123"/></front-stub><body><p>Hebart et al., present a landmark, multimodal massive dataset to support the study of visual object representation, including data measured from functional magnetic resonance imaging, magnetoencephalography, and behavioral similarity judgments. The compelling, condition-rich design, conducted over a thoughtfully curated and sampled set of object concepts will be highly valuable to the cognitive/computational/neuroscience community, yielding data that will be amenable to many empirical questions beyond the field of visual object recognition. The dataset is accompanied by quality control evaluations, as well as examples of analyses that the community can re-run and further explore for building new hypotheses that can be tested with such a rich dataset.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82580.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Konkle</surname><given-names>Talia</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Glerean</surname><given-names>Enrico</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.22.501123">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.07.22.501123v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;THINGS-data: A multimodal collection of large-scale datasets for investigating object representations in brain and behavior&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Floris de Lange as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Talia Konkle (Reviewer #2); Enrico Glerean (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>In our consultation session, we were all in agreement that this work is excellent. Each reviewer offers a slightly different perspective, and you can see their full feedback in context below. In general, the revisions we view as essential are as follows:</p><p>1. Providing more clarity to the open code, within what is possible given your skillset (i.e., we are not asking you to become software engineers).</p><p>2. We thought that the paper might be more impactful if some of the potential applications were highlighted and described in more detail (per Reviewer 1). We wish to emphasize that no new analyses are required unless you wish to conduct them.</p><p>3. Providing additional clarity on some of the methods, as requested by the reviewers.</p><p>In addition, each reviewer offered some additional comments, which we hope you will consider as you revise the manuscript.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>My feedback below does not affect the authors' main conclusions and I see all these as addressable with a revision.</p><p>1. One important strength of this work is that it reveals fMRI datasets with few participants (n = 4) that can be replicable and reliable. Given recent discussions related to replication issues for certain types of studies (e.g., brain-wide associations), I wonder if the authors may wish to highlight the replicability of their work more directly. For example, it may be useful to define &quot;exemplary analysis&quot; (e.g., mentioned on pg 16) in the introduction and then describe that nearly a dozen neuroimaging results were replicated in the author's approach.</p><p>2. An additional paragraph in the discussion describing the strengths and weaknesses of THINGS relative to existing datasets may be needed. For example, the authors report analyses focused on ventral visual stream regions. Do the authors think that their dataset can be used to study other brain regions such as the prefrontal cortex or anterior temporal lobes that may be also involved in object recognition?</p><p>3. In general, I thought the authors could have been clearer in terms of the potential new insights this dataset can offer the field. For example, the authors mention &quot;…THINGS-data will serve as an important resource for the community, enabling novel analyses to provide significant insights into visual object processing…&quot; – pg. 3, and a series of research directions focused on methodology, e.g.: &quot;including information-based multivariate decoding at the image and category level, data-driven visualization of response patterns across space and time, large-scale hypothesis testing by evaluating the reproducibility of previous research findings, revealing the relevance of the neuroimaging datasets for learning about behavioral similarity judgments, and regression-based fusion of MEG and fMRI data for uncovering a spatiotemporally-resolved information flow in the human brain as validation and extension of existing findings..&quot; – pg. 15.</p><p>To clearly showcase the importance and impact of THINGS-data, I wonder if it may be needed to conduct an additional exploratory analysis that is not conditional upon a previous replication or alternatively, speculate in the Discussion the potential theoretical insights that may be gleaned from future work using THINGS.</p><p>4. Eye-tracking data was collected and reflects an additional source of behavioral data with which to relate to neural measures. Yet the authors mention this data only once in the introduction and once in the methods and do not report any results other than briefly in the Supplemental. Perhaps related to the above, I wished to see more elaboration, either through additional analysis or a discussion on how eye-tracking data can be used in a future study.</p><p>5. The authors suggest that correlating RDMs between MEG and fMRI is indirect and introduces additional assumptions (pg. 14). As these assumptions are never described, I am left unclear with how and what exactly about the authors approach can account for this potential problem.</p><p>6. Why does the number of dimensions capturing human similarity judgments increase with dataset size in Figure 3, especially related to the author's previous work (e.g., the original 49 dimensions from THINGS)?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>For the behavioral measures, I wasn't entirely sure I followed some of the statements you made-e.g. you estimated a saturation at 4.5M trials, but wouldn't you have to run more trials to confirm this saturation effect? Also, the idea that you might not need more data to get changes in embedding dimensionality seemed slightly at odds with the subsequent points that participant-specific modeling is more reliable and lead to new insights (are their participant-specific dimensions? Not entirely sure how these were related, and also to within-category triads). Importantly, for me, I think these points are not key to the value of the behavioral similarity measures for the THINGs dataset. Really for me the key results are about data reliability-and you have it, and that these images are the same as the neural data so can now be linked to brain/meg data… wide open frontiers here. For me, exactly how it relates to and extends your prior work is interesting but almost in the same way as the other demonstrative analyses you conducted… there is more work to do to dig deeper into this. (And that's exciting!).</p><p>Style comments related to the narrative arc of the results. TOTALLY OPTIONAL, feel free to do none of these. No need to even reply.</p><p>– For me, the level of the method detail early in the Results sections was a bit too deep (e.g. describing the ICA denoising procedures and ridge regression approach; talking about the &quot;edge fraction and high-frequency content&quot;). For me, this level of detail in the results detracted just a little bit from the flow, because I didn't quite understand it and I had an easier time when I read the extended methods, and saw Sup Figure 10. Similarly, you report head motion parameters for the brain datasets in the main text and figures, but I also think these graphs could also be relegated to the Supplement. I found results like Sup Figure 2--better reliability after denoising--to be the key data I was looking for regarding your choices.</p><p>The 66-dimensional embedding of the behavioral Results section comes in between fmri/meg data reliability and fmri/meg decoding. I think you were going for reliability/data quality measures for all 3 datasets, then results of richer analyses. But I'm wondering if maybe you can think of decoding quality as part of data quality, and keep those fMRI/EEG sections together more? Or, maybe signpost this Results section organization more?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I do not have major concerns to raise for this manuscript, however, I strongly believe that the work presented would be even more valuable if the code attached to the manuscript could be improved for clarity and follow &quot;good enough&quot; standards for research software (https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510). I know that software is not yet a valuable academic output compared to manuscripts, but I believe that the impact that will have on the community – along with the shared data – will be beyond the results and descriptions of this manuscript. In the list of comments below, I outline a minimal set of improvements to make the software more reusable. I am aware that researchers are not expected to be software engineers and I do understand if some of the requests might be too difficult, so please do as much as you can according to your skills.</p><p>List of improvements for the attached software</p><p>1. Licenses</p><p>Licenses are important because they let future users know if they can re-use your code. There are two zip files for the code in your manuscript. One contains licenses of the used packages and license for your code, the other did not mention any license. Please add at least a license for your code. If you are unsure, I recommend Apache 2.0 https://www.apache.org/licenses/LICENSE-2.0</p><p>2. README</p><p>Currently, there is a readme in PDF in one zip file describing the MEG scripts. There is no readme for the fmri part. There is no readme for the other zip file. Please include a REAME file in each of the two repositories (a text file is better than PDF, in general use PDF only for figures. Any text can be a text file). The readme file should list all files included in the repository and a short explanation of what they do</p><p>3. Folder structure</p><p>right now there is a mixture of scripts/results/other files to prepare containers/other files from other packages all in the same folder. Please consider separating the files into meaningful subfolders. There are no rules beyond the &quot;good enough practices&quot; paper linked above. At least separating results/derivative files from the code would be helpful.</p><p>4. Dependencies:</p><p>there are no dependencies listed. You need to specify which version of Matlab was used, you need to add the environment.yaml file for the conda environment that you activate in one script (if you are unsure please rune &quot;conda env export &gt; env.yml&quot; after activating that environment). You need to confirm that all python scripts were run with that environment (conda activate is only present in the reconall script). You need to include the version of docker you used and the version of neurodocker. Since docker and/or neurodocker might change in the future, it would be recommended to also add the docker image you obtained to the repository or push it to dockerhub for people to reuse your docker images without needing to build them.</p><p>5. Testing and how to run</p><p>You need to document how a user can run and test the scripts themselves. For example, I have noticed that the script assumes that the BIDS (?) data should be located at a few parent subfolders. You could reconsider simplifying the work of the users, create a subfolder &quot;BIDS&quot; and tell the user to put there the data from neurovault and make your scripts point to the BIDS subfolder.</p><p>6. Accepting contributions and improvements from the community + version control</p><p>This is totally optional: right now it is difficult for a member of the community to recommend changes to your code because your code is not stored in a version control system (github, gitlab, etc). Please consider storing the code on github (or gitlab) and engage with your community of users by encouraging them to improve your code, and add future analysis to the same repository. In the long run, your repository (along with the dataset) could be a very valuable resource for the community, especially if you start getting contributions and code from future reuses of the data. Furthermore, by using a git repository you will also have the added benefits of version control of your software. Unlike manuscripts, the software is dynamic and so are the data, you can keep on improving some of your analysis or functions or just make the code more reusable by other scientists and the changes made will be documented automatically by the git system.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82580.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>In our consultation session, we were all in agreement that this work is excellent. Each reviewer offers a slightly different perspective, and you can see their full feedback in context below. In general, the revisions we view as essential are as follows:</p><p>1. Providing more clarity to the open code, within what is possible given your skillset (i.e., we are not asking you to become software engineers).</p><p>2. We thought that the paper might be more impactful if some of the potential applications were highlighted and described in more detail (per Reviewer 1). We wish to emphasize that no new analyses are required unless you wish to conduct them.</p><p>3. Providing additional clarity on some of the methods, as requested by the reviewers.</p><p>In addition, each reviewer offered some additional comments, which we hope you will consider as you revise the manuscript.</p></disp-quote><p>We would like to thank the editors and reviewers for highlighting these key points. We have addressed each of them below by (1) clarifying the openly available code, the software versions used, and by publishing and improving all code on GitHub, (2) highlighting additional potential applications of these data – in contrast to existing data and in contrast to small-scale datasets, and (3) clarifying methodological details by moving non-critical sections to Appendices and adding more detail in the main text where necessary.</p><p>Responses to each of the specific points raised by the reviewers are below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>My feedback below does not affect the authors' main conclusions and I see all these as addressable with a revision.</p><p>1. One important strength of this work is that it reveals fMRI datasets with few participants (n = 4) that can be replicable and reliable. Given recent discussions related to replication issues for certain types of studies (e.g., brain-wide associations), I wonder if the authors may wish to highlight the replicability of their work more directly. For example, it may be useful to define &quot;exemplary analysis&quot; (e.g., mentioned on pg 16) in the introduction and then describe that nearly a dozen neuroimaging results were replicated in the author's approach.</p></disp-quote><p>We would like to thank the reviewer for this excellent suggestion. In the revised manuscript, we now better highlight both reliability and replicability: (1) the degree of reliability between participants, which is highly relevant given the small sample size; and (2) replicability of other research, highlighting the value of our dataset and demonstrating its usefulness for testing replicability of existing research more broadly. As suggested, we now also define “exemplary analysis” in the introduction and highlight the number of studies replicated using our approach in the Discussion.</p><disp-quote content-type="editor-comment"><p>2. An additional paragraph in the discussion describing the strengths and weaknesses of THINGS relative to existing datasets may be needed. For example, the authors report analyses focused on ventral visual stream regions. Do the authors think that their dataset can be used to study other brain regions such as the prefrontal cortex or anterior temporal lobes that may be also involved in object recognition?</p></disp-quote><p>We agree with the reviewer. We have expanded the section in the discussion comparing the neuroimaging datasets of THINGS-data to other datasets and – also in response to Reviewer #3 – highlight the strengths and weaknesses of THINGS more broadly. We additionally discuss the potential utility of these datasets beyond ventral visual cortex in a new limitations paragraph in the Discussion section.</p><disp-quote content-type="editor-comment"><p>3. In general, I thought the authors could have been clearer in terms of the potential new insights this dataset can offer the field. For example, the authors mention &quot;…THINGS-data will serve as an important resource for the community, enabling novel analyses to provide significant insights into visual object processing…&quot; – pg. 3, and a series of research directions focused on methodology, e.g.: &quot;including information-based multivariate decoding at the image and category level, data-driven visualization of response patterns across space and time, large-scale hypothesis testing by evaluating the reproducibility of previous research findings, revealing the relevance of the neuroimaging datasets for learning about behavioral similarity judgments, and regression-based fusion of MEG and fMRI data for uncovering a spatiotemporally-resolved information flow in the human brain as validation and extension of existing findings..&quot; – pg. 15.</p><p>To clearly showcase the importance and impact of THINGS-data, I wonder if it may be needed to conduct an additional exploratory analysis that is not conditional upon a previous replication or alternatively, speculate in the Discussion the potential theoretical insights that may be gleaned from future work using THINGS.</p></disp-quote><p>We would like to thank the reviewer. We believe this to be one of the most critical points raised by the reviewer. While we considered conducting additional exploratory analyses, we intentionally avoided overly novel and possibly controversial findings which might distract from the main message of releasing the three datasets for others to conduct new analyses. That said, we agree that it makes sense to better discuss the theoretical insights that these datasets may provide over and above small-scale datasets, and we now cover this more extensively in the Discussion section.</p><disp-quote content-type="editor-comment"><p>4. Eye-tracking data was collected and reflects an additional source of behavioral data with which to relate to neural measures. Yet the authors mention this data only once in the introduction and once in the methods and do not report any results other than briefly in the Supplemental. Perhaps related to the above, I wished to see more elaboration, either through additional analysis or a discussion on how eye-tracking data can be used in a future study.</p></disp-quote><p>We thank the reviewer for highlighting the eye-tracking data. Please note that participants were instructed to maintain central fixation throughout, which we confirmed empirically. While fixations were not perfect, this precludes an unbiased assessment of eye position. We now mention this restriction for the use of eye-tracking data more specifically. In addition, we mention a potential use case for the data in post-hoc cleaning of data (e.g. removal of time periods with eye blinks).</p><disp-quote content-type="editor-comment"><p>5. The authors suggest that correlating RDMs between MEG and fMRI is indirect and introduces additional assumptions (pg. 14). As these assumptions are never described, I am left unclear with how and what exactly about the authors approach can account for this potential problem.</p></disp-quote><p>We now better explain the assumptions introduced by RSA-based MEG-fMRI fusion in the main text. Hopefully, this makes the advantage of the approach we present clearer.</p><disp-quote content-type="editor-comment"><p>6. Why does the number of dimensions capturing human similarity judgments increase with dataset size in Figure 3, especially related to the author's previous work (e.g., the original 49 dimensions from THINGS)?</p></disp-quote><p>In our previous work<ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?s0VeCM">8</ext-link>, we highlighted that the accuracy of our embedding for predicting triplet odd-one out choices had already saturated (Extended Data Figure 4a). However, we also discussed the limitation that the resulting dimensionality was a function of dataset size (Extended Data Figure 4b) and that highly similar dimensions appeared to be merged (e.g. plant-related / green). By extrapolating the dimensionality of the original dataset, before we collected additional data, we predicted that the number of dimensions would saturate around 66 to 67, with a dataset size of ~5 million triplets. This is very much in line with what we now find empirically. This indicates that while the 49 dimensions were already providing an excellent description of the representational space, the 66 dimensions reported here offered a more fine-grained view for which collecting additional data would be expected to yield only a minor improvement, which may no longer justify the cost of acquiring additional triplets. We now better explain this line of reasoning in the main text.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>For the behavioral measures, I wasn't entirely sure I followed some of the statements you made-e.g. you estimated a saturation at 4.5M trials, but wouldn't you have to run more trials to confirm this saturation effect? Also, the idea that you might not need more data to get changes in embedding dimensionality seemed slightly at odds with the subsequent points that participant-specific modeling is more reliable and lead to new insights (are their participant-specific dimensions? Not entirely sure how these were related, and also to within-category triads). Importantly, for me, I think these points are not key to the value of the behavioral similarity measures for the THINGs dataset. Really for me the key results are about data reliability-and you have it, and that these images are the same as the neural data so can now be linked to brain/meg data… wide open frontiers here. For me, exactly how it relates to and extends your prior work is interesting but almost in the same way as the other demonstrative analyses you conducted… there is more work to do to dig deeper into this. (And that's exciting!).</p></disp-quote><p>We thank the reviewer for pointing this out, and we fully agree that these are not key results. Yet, we thought it was important to explain why we collected a larger set of triplets, why we stopped at 4.5 million, and what would be the value in this larger dataset. We believe we could have described the motivation for our analyses more clearly. Our intention was not to measure a saturation but to demonstrate that our prediction about a saturation effect prior to data collection led to a dimensionality that we observed empirically. Given that the prediction was fulfilled, we assume that adding more data would not have a strong effect on embedding dimensionality, or at least the benefit may not justify the cost. In addition, the statements about whether additional data collection may be justified only referred to <italic>global</italic> similarity at the <italic>population</italic> level. However, there definitely is room for improvement for <italic>within-category</italic> (e.g. animal, vehicle) similarity estimates and similarity measurements at the <italic>individual-participant</italic> level for getting at interindividual differences. The former would benefit from a more targeted sampling, while the latter would require a different sampling approach (many triplets within individuals are difficult to collect). We now better explain this motivation for the analyses in the revised manuscript and highlight the potential refinements that could be made in future work. We also clarify the nature of the within-subject noise ceilings in the updated text.</p><disp-quote content-type="editor-comment"><p>Style comments related to the narrative arc of the results.</p><p>– For me, the level of the method detail early in the Results sections was a bit too deep (e.g. describing the ICA denoising procedures and ridge regression approach; talking about the &quot;edge fraction and high-frequency content&quot;). For me, this level of detail in the results detracted just a little bit from the flow, because I didn't quite understand it and I had an easier time when I read the extended methods, and saw Sup Figure 10. Similarly, you report head motion parameters for the brain datasets in the main text and figures, but I also think these graphs could also be relegated to the Supplement. I found results like Sup Figure 2--better reliability after denoising--to be the key data I was looking for regarding your choices.</p></disp-quote><p>We thank the reviewer for highlighting these concerns. We agree that the level of detail detracts from the flow, and we have thus reduced the methodological details from the Results section and moved aspects to the Methods. Specifically, we have integrated the section on ICA denoising and single trial regressors with the Methods and have reduced their explanation in the Results section to one paragraph within the data quality section. However, since this is a dataset paper, we thought it would be important to keep in details such as head motion parameters.</p><disp-quote content-type="editor-comment"><p>The 66-dimensional embedding of the behavioral Results section comes in between fmri/meg data reliability and fmri/meg decoding. I think you were going for reliability/data quality measures for all 3 datasets, then results of richer analyses. But I'm wondering if maybe you can think of decoding quality as part of data quality, and keep those fMRI/EEG sections together more? Or, maybe signpost this Results section organization more?</p></disp-quote><p>The reviewer is correct that we were going for reliability and data quality for all 3 datasets, followed by results of example analyses for all three datasets. We had, indeed, tried a similar structure to the one proposed by the reviewer but then ended up deciding that the current structure is better suited, since readers may be wondering at what point we would be talking about data quality for behavior, or about the behavioral data set at all. However, we completely agree that it makes sense to signpost the Results section organization more, which we now do with a separate paragraph in the revised manuscript and with adapted subheadings.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I do not have major concerns to raise for this manuscript, however, I strongly believe that the work presented would be even more valuable if the code attached to the manuscript could be improved for clarity and follow &quot;good enough&quot; standards for research software (https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510). I know that software is not yet a valuable academic output compared to manuscripts, but I believe that the impact that will have on the community – along with the shared data – will be beyond the results and descriptions of this manuscript. In the list of comments below, I outline a minimal set of improvements to make the software more reusable. I am aware that researchers are not expected to be software engineers and I do understand if some of the requests might be too difficult, so please do as much as you can according to your skills.</p></disp-quote><p>We thank the reviewer for checking this critical aspect of our datasets. Indeed, the value depends strongly on the usability of the datasets, and the code is an important part that can facilitate this use a lot. In response to their comment, a major change we made was to move all code for fMRI and MEG data to GitHub, which will also make it easier to keep up with versions and to work on issues raised by the community. For simplicity, we decided to keep the code related to the behavioral dataset on OSF. We made sure to cross-reference between GitHub, Figshare, and OSF.</p><p>More generally, our aim is to address a large number of comments raised by the reviewer in this revision. Beyond the revision, we see this task of usability as an improvement that will (and should!) continue beyond this dataset paper. Even when we choose not to address a specific comment, we hope that the reviewer can see we truly care about these issues and that we intend on making THINGS-data a valuable resource for the research community.</p><p>In this context, we would like to mention that in the process of improving the MEG code, we found a minor error in our preprocessing script, and when checking the behavior code, we found another minor error in the embedding matrix. Specifically, for MEG the sensor exclusion had not been done as described in the methods section, and very few sensors had actually been excluded. We therefore decided to re-run all analyses and recreate all figures, this time without excluding any sensors. No results or conclusions changed as a consequence of skipping this preprocessing step. We have also adapted the methods section accordingly to reflect this description.</p><p>For behavior, when running the study on Mturk, there was a small mistake in the ordering of 21 out of the 1854 objects, which has to do with the different alphabetical sorting of different operating systems and programming environments. We were aware of this mistake and initially corrected it after creating the embedding. For the current dataset, since it is made publicly available, we decided to already correct this mistake at the level of the dataset, but then forgot that we did not need to correct this again after the embedding had been created. This led to a double correction, reintroducing the original mistake. Since this double correction was applied to all analyses, we had not noticed this mistake earlier during debugging, and as a consequence, it only affects a subset of the results. Specifically, the within-category analyses in Figure 3b were slightly improved for one of the conditions. We adapted the figure and the related statistics in the updated manuscript. The analyses involving RSA using behavioral similarity and fMRI/MEG led to almost identical results with no visible changes in the figures, so we kept the original figures.</p><p>In addition, we went through the 66 dimensions a second time and have slightly updated the labels, which is reflected in the updated Figure 3—figure supplement 1 (formerly Supplemental Figure 7).</p><disp-quote content-type="editor-comment"><p>List of improvements for the attached software</p><p>1. Licenses</p><p>Licenses are important because they let future users know if they can re-use your code. There are two zip files for the code in your manuscript. One contains licenses of the used packages and license for your code, the other did not mention any license. Please add at least a license for your code. If you are unsure, I recommend Apache 2.0 https://www.apache.org/licenses/LICENSE-2.0</p></disp-quote><p>We have added a CC0 license – the least restrictive kind – to all software and data. CC0 means no attribution is required when reusing materials.</p><disp-quote content-type="editor-comment"><p>2. README</p><p>Currently, there is a readme in PDF in one zip file describing the MEG scripts. There is no readme for the fmri part. There is no readme for the other zip file. Please include a REAME file in each of the two repositories (a text file is better than PDF, in general use PDF only for figures. Any text can be a text file). The readme file should list all files included in the repository and a short explanation of what they do</p></disp-quote><p>We have included / improved the README files in the GitHub repository. The README instructs about data download and contains MEG and fMRI-specific code. The fMRI section additionally contains notes and examples on how to use the data and run analyses.</p><disp-quote content-type="editor-comment"><p>3. Folder structure</p><p>right now there is a mixture of scripts/results/other files to prepare containers/other files from other packages all in the same folder. Please consider separating the files into meaningful subfolders. There are no rules beyond the &quot;good enough practices&quot; paper linked above. At least separating results/derivative files from the code would be helpful.</p></disp-quote><p>We agree with the reviewer. So far, we have addressed this point for the MEG analyses. All code is now command-line callable requiring the user to input the location of the BIDS-data directory. All analysis results and figures are then saved within the derivatives subfolder. For fMRI, this is currently a little challenging to implement. We are in the process of adjusting code to point to files in meaningful subfolders.</p><disp-quote content-type="editor-comment"><p>4. Dependencies:</p><p>there are no dependencies listed. You need to specify which version of Matlab was used, you need to add the environment.yaml file for the conda environment that you activate in one script (if you are unsure please rune &quot;conda env export &gt; env.yml&quot; after activating that environment). You need to confirm that all python scripts were run with that environment (conda activate is only present in the reconall script). You need to include the version of docker you used and the version of neurodocker. Since docker and/or neurodocker might change in the future, it would be recommended to also add the docker image you obtained to the repository or push it to dockerhub for people to reuse your docker images without needing to build them.</p></disp-quote><p>We have added dependencies and now specify versions more clearly. However, we decided not to push a docker image to dockerhub. We agree this is potentially useful but we believe that there will likely be many more changes to the code base in the future, which may make this step unnecessary for the purpose described by the reviewer, since the code will be kept up to date.</p><disp-quote content-type="editor-comment"><p>5. Testing and how to run</p><p>You need to document how a user can run and test the scripts themselves. For example, I have noticed that the script assumes that the BIDS (?) data should be located at a few parent subfolders. You could reconsider simplifying the work of the users, create a subfolder &quot;BIDS&quot; and tell the user to put there the data from neurovault and make your scripts point to the BIDS subfolder.</p></disp-quote><p>We agree with the reviewer and have adjusted the code accordingly. We made Python scripts command line executable and gave users the option to specify the path to the BIDS-formatted data. We will continuously develop the code and documentation and look forward to more helpful feedback from users.</p><disp-quote content-type="editor-comment"><p>6. Accepting contributions and improvements from the community + version control</p><p>This is totally optional: right now it is difficult for a member of the community to recommend changes to your code because your code is not stored in a version control system (github, gitlab, etc). Please consider storing the code on github (or gitlab) and engage with your community of users by encouraging them to improve your code, and add future analysis to the same repository. In the long run, your repository (along with the dataset) could be a very valuable resource for the community, especially if you start getting contributions and code from future reuses of the data. Furthermore, by using a git repository you will also have the added benefits of version control of your software. Unlike manuscripts, the software is dynamic and so are the data, you can keep on improving some of your analysis or functions or just make the code more reusable by other scientists and the changes made will be documented automatically by the git system.</p></disp-quote><p>We completely agree with the reviewer, and – as mentioned above – we have moved the code related to the fMRI and MEG datasets to GitHub. For simplicity, we kept the code for the behavioral data on OSF. Please note that the behavioral data is much simpler to analyze, and in its preprocessed form may not require much separate code. Thus, the existing code in relation to behavior is mostly useful for reproducing the basic analyses shown in the manuscript.</p></body></sub-article></article>