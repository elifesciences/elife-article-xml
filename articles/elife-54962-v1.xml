<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54962</article-id><article-id pub-id-type="doi">10.7554/eLife.54962</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Efficient sampling and noisy decisions</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-173055"><name><surname>Heng</surname><given-names>Joseph A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3643-4623</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-173056"><name><surname>Woodford</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-166948"><name><surname>Polania</surname><given-names>Rafael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6176-6806</contrib-id><email>rafael.polania@hest.ethz.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Health Sciences and Technology, Federal Institute of Technology (ETH)</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution>Department of Economics, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><role>Reviewing Editor</role><aff><institution>Harvard University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>09</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54962</elocation-id><history><date date-type="received" iso-8601-date="2020-01-29"><day>29</day><month>01</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-08-05"><day>05</day><month>08</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Heng et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Heng et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54962-v1.pdf"/><abstract><p>Human decisions are based on finite information, which makes them inherently imprecise. But what determines the degree of such imprecision? Here, we develop an efficient coding framework for higher-level cognitive processes in which information is represented by a finite number of discrete samples. We characterize the sampling process that maximizes perceptual accuracy or fitness under the often-adopted assumption that full adaptation to an environmental distribution is possible, and show how the optimal process differs when detailed information about the current contextual distribution is costly. We tested this theory on a numerosity discrimination task, and found that humans efficiently adapt to contextual distributions, but in the way predicted by the model in which people must economize on environmental information. Thus, understanding decision behavior requires that we account for biological restrictions on information coding, challenging the often-adopted assumption of precise prior knowledge in higher-level decision systems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision making</kwd><kwd>information theory</kwd><kwd>resource limitations</kwd><kwd>memory</kwd><kwd>sampling</kwd><kwd>reward</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>grant agreement No. 758604</award-id><principal-award-recipient><name><surname>Polania</surname><given-names>Rafael</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>ERC</institution></institution-wrap></funding-source><award-id>ENTRAINER</award-id><principal-award-recipient><name><surname>Polania</surname><given-names>Rafael</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Woddford</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An efficient coding theory for higher-level cognitive processes reveals that humans efficiently adapt to contextual distributions by economizing on environmental prior information.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p><italic>'We rarely know the statistics of the messages completely, and our knowledge may change … what is redundant today was not necessarily redundant yesterday.’</italic> <xref ref-type="bibr" rid="bib5">Barlow, 2001</xref>.</p><sec id="s1" sec-type="intro"><title>Introduction</title><p>It has been suggested that the rules guiding behavior are not arbitrary, but follow fundamental principles of acquiring information from environmental regularities in order to make the best decisions. Moreover, these principles should incorporate strategies of information coding in ways that minimize the costs of inaccurate decisions given biological constraints on information acquisition, an idea known as efficient coding (<xref ref-type="bibr" rid="bib3">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib4">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib40">Niven and Laughlin, 2008</xref>; <xref ref-type="bibr" rid="bib58">Sharpee et al., 2014</xref>). While early applications of efficient coding theory have primarily been to early stages of sensory processing (<xref ref-type="bibr" rid="bib30">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib18">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib72">Wei and Stocker, 2015</xref>), it is worth considering whether similar principles may also shape the structure of internal representations of higher-level concepts, such as the perceptions of value that underlie economic decision making (<xref ref-type="bibr" rid="bib31">Louie and Glimcher, 2012</xref>; <xref ref-type="bibr" rid="bib49">Polanía et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Rustichini et al., 2017</xref>). In this work, we contribute to the efficient coding framework applied to cognition and behavior in several respects.</p><p>A first aspect concerns the range of possible internal representation schemes that should be considered feasible, which determines the way in which greater precision of discrimination in one part of the stimulus space requires less precision of discrimination elsewhere. Implementational architectures proposed in previous work assume a population coding scheme in which different neurons have distinct 'preferred' stimuli (<xref ref-type="bibr" rid="bib18">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib72">Wei and Stocker, 2015</xref>). While this is clearly relevant for some kinds of low-level sensory features such as orientation, it is not obvious that this kind of internal representation is used in representing higher-level concepts such as economic values. We instead develop an efficient coding theory for a case in which an extensive magnitude (something that can be described by a larger or smaller number) is represented by a set of processing units that 'vote' in favor of the magnitude being larger rather than small. The internal representation therefore necessarily consists of a finite collection of binary signals.</p><p>Our restriction to representations made up of binary signals is in conformity with the observation that neural systems at many levels appear to transmit information via discrete stochastic events (<xref ref-type="bibr" rid="bib56">Schreiber et al., 2002</xref>; <xref ref-type="bibr" rid="bib59">Sharpee, 2017</xref>). Moreover, cognitive models with this general structure have been argued to be relevant for higher-order decision problems such as value-based choice. For example, it has been suggested that the perceived values of choice options are constructed by acquiring samples of evidence from memory regarding the emotions evoked by the presented items (<xref ref-type="bibr" rid="bib57">Shadlen and Shohamy, 2016</xref>). Related accounts suggest that when a choice must be made between alternative options, information is acquired via discrete samples of information that can be represented as binary responses (e.g., 'yes/no' responses to queries) (<xref ref-type="bibr" rid="bib41">Norman, 1968</xref>; <xref ref-type="bibr" rid="bib71">Weber and Johnson, 2009</xref>). The seminal <italic>decision by sampling</italic> (DbS) theory (<xref ref-type="bibr" rid="bib61">Stewart et al., 2006</xref>) similarly posits an internal representation of magnitudes relevant to a decision problem by tallies of the outcomes of a set of binary comparisons between the current magnitude and alternative values sampled from memory. The architecture that we assume for imprecise internal representations has the general structure of proposals of these kinds; but we go beyond the above-mentioned investigations, in analyzing what an efficient coding scheme consistent with our general architecture would be like.</p><p>A second aspect concerns the objective for which the encoding system is assumed to be optimized. Information maximization theories (<xref ref-type="bibr" rid="bib30">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib18">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib72">Wei and Stocker, 2015</xref>) assume that the objective should be maximal mutual information between the true stimulus magnitude and the internal representation. While this may be a reasonable assumption in the case of early sensory processing, it is less obvious in the case of circuits involved more directly in decision making, and in the latter case an obvious alternative is to ask what kind of encoding scheme will best serve to allow accurate decisions to be made. In the theory that we develop here, our primary concern is with encoding schemes that maximize a subject's probability of giving a correct response to a binary decision. However, we compare the coding rule that would be optimal from this standpoint to one that would maximize mutual information, or to one that would maximize the expected value of the chosen item.</p><p>Third, we extend our theory of efficient coding to consider not merely the nature of an efficient coding system for a single environmental frequency distribution assumed to be permanently relevant — so that there has been ample time for the encoding rule to be optimally adapted to that distribution of stimulus magnitudes — but also an efficient approach to adjusting the encoding as the environmental frequency distribution changes. Prior discussions of efficient coding have often considered the optimal choice of an encoding rule for a single environmental frequency distribution that is assumed to represent a permanent feature of the natural environment (<xref ref-type="bibr" rid="bib30">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib18">Ganguli and Simoncelli, 2014</xref>). Such an approach may make sense for a theory of neural coding in cortical regions involved in early-stage processing of sensory stimuli, but is less obviously appropriate for a theory of the processing of higher-level concepts such as economic value, where the idea that there is a single permanently relevant frequency distribution of magnitudes that may be encountered is doubtful.</p><p>A key goal of our work is to test the relevance of these different possible models of efficient coding in the case of numerosity discrimination. Judgments of the comparative numerosity of two visual displays provide a test case of particular interest given our objectives. On the one hand, a long literature has argued that imprecision in numerosity judgments has a similar structure to psychophysical phenomena in many low-level sensory domains (<xref ref-type="bibr" rid="bib37">Nieder and Dehaene, 2009</xref>; <xref ref-type="bibr" rid="bib38">Nieder and Miller, 2003</xref>). This makes it reasonable to ask whether efficient coding principles may also be relevant in this domain. At the same time, numerosity is plainly a more abstract feature of visual arrays than low-level properties such as local luminosity, contrast, or orientation, and therefore can be computed only at a later stage of processing. Moreover, processing of numerical magnitudes is a crucial element of many higher-level cognitive processes, such as economic decision making; and it is arguable that many rapid or intuitive judgments about numerical quantities, even when numbers are presented symbolically, are based on an 'approximate number system' of the same kind as is used in judgments of the numerosity of visual displays (<xref ref-type="bibr" rid="bib45">Piazza et al., 2007</xref>; <xref ref-type="bibr" rid="bib37">Nieder and Dehaene, 2009</xref>). It has further been argued that imprecision in the internal representation of numerical magnitudes may underly imprecision and biases in economic decisions (<xref ref-type="bibr" rid="bib27">Khaw et al., 2020</xref>; <xref ref-type="bibr" rid="bib74">Woodford, 2020</xref>).</p><p>It is well-known that the precision of discrimination between nearby numbers of items decreases in the case of larger numerosities, in approximately the way predicted by <italic>Weber's Law</italic>, and this is often argued to support a model of imprecise coding based on a logarithmic transformation of the true number (<xref ref-type="bibr" rid="bib37">Nieder and Dehaene, 2009</xref>; <xref ref-type="bibr" rid="bib38">Nieder and Miller, 2003</xref>). However, while the precision of internal representations of numerical magnitudes is arguably of great evolutionary relevance (<xref ref-type="bibr" rid="bib11">Butterworth et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Nieder, 2020</xref>), it is unclear why a specifically logarithmic transformation of number information should be of adaptive value, and also whether the same transformation is used independent of context (<xref ref-type="bibr" rid="bib43">Pardo-Vazquez et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Brus et al., 2019</xref>). Here, we report new experimental data on numerosity discrimination by human participants, where we find that our data are most consistent with an efficient coding theory for which the performance measure is the frequency of correct comparative judgments, and where people economize on the costs associated to learn about the statistics of the environment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A general efficient sampling framework</title><p>We consider a situation in which the objective magnitude of a stimulus with respect to some feature can be represented by a quantity <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. When the stimulus is presented to an observer, it gives rise to an imprecise representation <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the nervous system, on the basis of which the observer produces any required response. The internal representation <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be stochastic, with given values being produced with conditional probabilities <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that depend on the true magnitude. Here, we are more specifically concerned with discrimination experiments, in which two stimulus magnitudes <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are presented, and the subject must choose which of the two is greater. We suppose that each magnitude <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> has an internal representation <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, drawn independently from a distribution <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that depends only on the true magnitude of that individual stimulus. The observer’s choice must be based on a comparison of <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>One way in which the cognitive resources recruited to make accurate discriminations may be limited is in the variety of distinct internal representations that are possible. When the complexity of feasible internal representations is limited, there will necessarily be errors in the identification of the greater stimulus magnitude in some cases, even assuming an optimal decoding rule for choosing the larger stimulus on the basis of <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. One can then consider alternative encoding rules for mapping objective stimulus magnitudes to feasible internal representations. The answer to this efficient coding problem generally depends on the prior distribution <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from which the different stimulus magnitudes <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are drawn. The resources required for more precise internal representations of individual stimuli may be economized with respect to either or both of two distinct cognitive costs. The first goal of this work is to distinguish between these two types of efficiency concerns.</p><p>One question that we can ask is wheter the observed behavioral responses are consistent with the hypothesis that the conditional probabilities <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are well-adapted to the particular frequency distribution of stimuli used in the experiment, suggesting an efficient allocation of the limited encoding neural resources. The assumption of full adaptation is typically adopted in efficient coding formulations of early sensory systems (<xref ref-type="bibr" rid="bib30">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib73">Wei and Stocker, 2017</xref>), and also more recently in applications of efficient coding theories in value-based decisions (<xref ref-type="bibr" rid="bib31">Louie and Glimcher, 2012</xref>; <xref ref-type="bibr" rid="bib49">Polanía et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Rustichini et al., 2017</xref>).</p><p>There is also a second cost in which it may be important to economize on cognitive resources. An efficient coding scheme in the sense described above economizes on the resources used to represent each individual new stimulus that is encountered; however, the encoding and decoding rules are assumed to be precisely optimized for the specific distribution <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of stimuli that characterizes the experimental situation. In practice, it will be necessary for a decision maker to learn about this distribution in order to encode and decode individual stimuli in an efficient way, on the basis of experience with a given context. In this case, the relevant design problem should not be conceived as choosing conditional probabilities <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> once and for all, with knowledge of the prior distribution <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from which <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will be drawn. Instead, it should be to choose a rule that specifies how the probabilities <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> should adapt to the distribution of stimuli that have been encountered in a given context. It then becomes possible to consider how well a given learning rule economizes on the degree of information about the distribution of magnitudes associated with one’s current context that is required for a given level of average performance across contexts. This issue is important not only to reduce the cognitive resources required to implement the rule in a given context (by not having to store or access so detailed a description of the prior distribution), but in order to allow faster adaptation to a new context when the statistics of the environment can change unpredictably (<xref ref-type="bibr" rid="bib34">Młynarski and Hermundstad, 2019</xref>).</p></sec><sec id="s2-2"><title>Coding architecture</title><p>We now make the contrast between these two types of efficiency more concrete by considering a specific architecture for internal representations of sensory magnitudes. We suppose that the representation <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of a given stimulus will consist of the output of a finite collection of <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> processing units, each of which has only two possible output states ('high' or 'low' readings), as in the case of a simple perceptron. The probability that each of the units will be in one output state or the other can depend on the stimulus <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that is presented. We further restrict the complexity of feasible encoding rules by supposing that the probability of a given unit being in the 'high’ state must be given by some function <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that is the same for each of the individual units, rather than allowing the different units to coordinate in jointly representing the situation in some more complex way. We argue that the existence of multiple units operating in parallel effectively allows multiple repetitions of the same 'experiment’, but does not increase the complexity of the kind of test that can be performed. Note that we do not assume any unavoidable degree of stochasticity in the functioning of the individual units; it turns out that in our theory, it will be efficient for the units to be stochastic, but we do not assume that precise, deterministic functioning would be infeasible. Our resource limits are instead on the number of available units, the degree of differentiation of their output states, and the degree to which it is possible to differentiate the roles of distinct units.</p><p>Given such a mechanism, the internal representation <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the magnitude of an individual stimulus <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> will be given by the collection of output states of the <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> processing units. A specification of the function <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> then implies conditional probabilities for each of the <inline-formula><mml:math id="inf30"><mml:msup><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msup></mml:math></inline-formula> possible representations. Given our assumption of a symmetrical and parallel process, the number <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of units in the 'high' state will be a sufficient statistic, containing all of the information about the true magnitude <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> that can be extracted from the internal representation. An optimal decoding rule will therefore be a function only of <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and we can equivalently treat <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (an integer between 0 and <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) as the internal representation of the quantity <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. The conditional probabilities of different internal representations are then<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The efficient coding problem for a given environment, specified by a particular prior distribution <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> will be to choose the encoding rule <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> so as to allow an overall distribution of responses across trials that will be as accurate as possible (according to criteria that we will elaborate further below). We can further suppose that each of the individual processing units is a threshold unit, that produces a 'high’ reading if and only if the value <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> exceeds some threshold <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is a random term drawn independently on each trial from some distribution <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The encoding function <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can then be implemented by choosing an appropriate distribution <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula>. This implementation requires that <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be a non-decreasing function, as we shall assume.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Architecture of the sampling mechanism.</title><p>Each processing unit receives noisy versions of the input <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the noisy signals are i.i.d. additive random signals independent of <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The output of the neuron for each sample is 'high' (one) reading if <inline-formula><mml:math id="inf48"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula> and zero otherwise. The noisy percept of the input is simply the sum of the outputs of each sample given by <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig1-v1.tif"/></fig></sec><sec id="s2-3"><title>Limited cognitive resources</title><p>One measure of the cognitive resources required by such a system is the number <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of processing units that must produce an output each time an individual stimulus <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is evaluated. We can consider the optimal choice of <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula> in order to maximize, for instance, average accuracy of responses in a given environment <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, in the case of any bound <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on the number of units that can be used to represent each stimulus. But we can also consider the amount of information about the distribution <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that must be used in order to decide how to encode a given stimulus <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. If the system is to be able to adapt to changing environments, it must determine the value of <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (the probability of a 'high' reading) as a function of both the current <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and information about the distribution <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, in a way that must now be understood to apply across different potential contexts. This raises the issue of how precisely the distribution <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> associated with the current context is represented for purposes of such a calculation. A more precise representation of the prior (allowing greater sensitivity to fine differences in priors) will presumably entail a greater resource cost or very long adaptation periods.</p><p>We can quantify the precision with which the prior <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is represented by supposing that it is represented by a finite sample of <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> independent draws <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from the prior (or more precisely, from the set of previously experienced values, an empirical distribution that should after sufficient experience provide a good approximation to the true distribution). We further assume that an independent sample of <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> previously experienced values is used by each of the processing units (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Each of the <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> individual processing units is then in the 'high' state with probability <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The complete internal representation of the stimulus <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is then the collection of <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> independent realizations of this binary-valued random variable. We may suppose that the resource cost of an internal representation of this kind is an increasing function of both <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>This allows us to consider an efficient coding meta-problem in which for any given values <inline-formula><mml:math id="inf71"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the function <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is chosen so as to maximize some measure of average perceptual accuracy, where the average is now taken not only over the entire distribution of possible <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> occurring under a given prior <inline-formula><mml:math id="inf74"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> but over some range of different possible priors for which the adaptive coding scheme is to be optimized. We wish to consider how each of the two types of resource constraint (a finite bound on <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as opposed to a finite bound on <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) affects the nature of the predicted imprecision in internal representations, under the assumption of a coding scheme that is efficient in this generalized sense, and then ask whether we can tell in practice how tight each of the resource constraints appears to be.</p></sec><sec id="s2-4"><title>Efficient sampling for a known prior distribution</title><p>We first consider efficient coding in the case that there is no relevant constraint on the size of <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, while <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> instead is bounded. In this case, we can assume that each time an individual stimulus <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> must be encoded, a large enough sample of prior values is used to allow accurate recognition of the distribution <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and the problem reduces to a choice of a function <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that is optimal for each possible prior <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><sec id="s2-4-1"><title>Maximizing mutual information</title><p>The nature of the resource-constrained problem to be optimized depends on the performance measure that we use to determine the usefulness of a given encoding scheme. A common assumption in the literature on efficient coding has been that the encoding scheme maximizes the mutual information between the true stimulus magnitude and its internal representation (<xref ref-type="bibr" rid="bib18">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib49">Polanía et al., 2019</xref>; <xref ref-type="bibr" rid="bib72">Wei and Stocker, 2015</xref>). We start by characterizing the optimal <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for a given prior distribution <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, according to this criterion. It can be shown that for large <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the mutual information between <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (hence the mutual information between <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is maximized if the prior distribution <inline-formula><mml:math id="inf90"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> over <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is Jeffreys’ prior (<xref ref-type="bibr" rid="bib12">Clarke and Barron, 1994</xref>)<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>also known as the arcsine distribution. Hence, the mapping <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> induces a prior distribution <inline-formula><mml:math id="inf93"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> over <italic>θ</italic> given by the arcsine distribution (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, right panel). Based on this result, it can be shown that the optimal encoding rule <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that guarantees maximization of mutual information between the random variable <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the noisy encoded percept <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by (see Appendix 1)<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the CDF of the prior distribution <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Overview of our theory and differences in encoding rules.</title><p>(<bold>a</bold>) Schematic representation of our theory. Left: example prior distribution <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of values <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> encountered in the environment. Right: Prior distribution in the encoder space (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) due to optimal encoding (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). This optimal mapping determines the probability <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of generating a 'high' or 'low' reading. The ex-ante distribution over <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that guarantees maximization of mutual information is given by the arcsine distribution (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). (<bold>b</bold>) Encoding rules <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for different decision strategies under binary sampling coding: accuracy maximization (blue), reward maximization (red), DbS (green dashed). (<bold>c</bold>) Mutual information <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the different encoding rules as a function of the number of samples <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. As expected <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> increases with <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, however the rule that results in the highest loss of information is DbS. (<bold>d</bold>) Discriminability thresholds <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (log-scaled for better visualization) for the different encoding rules as a function of the input values <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for the prior <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> given in panel a. (<bold>e</bold>) Graphical representation of the perceptual accuracy optimization landscape. We plot the average probability of correct responses for the large-<inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limit using as benchmark a Beta distribution with parameters <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The blue star shows the average error probability assuming that <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the arcsine distribution (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), which is the optimal solution when the prior distribution <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in known. The blue open circle shows the average error probability based on the encoding rule assumed in DbS, which is located near the optimal solution. Please note that when formally solving this optimization problem, we did not assume a priori that the solution is related to the beta distribution. We use the beta distribution in this figure just as a benchmark for visualization. Detailed comparison of performance for finite <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> samples is presented in Appendix 7.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig2-v1.tif"/></fig></sec><sec id="s2-4-2"><title>Accuracy maximization for a known prior distribution</title><p>So far, we have derived the optimal encoding rule to maximize mutual information. However, one may ask what the implications are of such a theory for discrimination performance. This is important to investigate given that achieving channel capacity does not necessarily imply that the goals of the organism are also optimized (<xref ref-type="bibr" rid="bib44">Park and Pillow, 2017</xref>). Independent of information maximization assumptions, here, we start from scratch and investigate what are the necessary conditions for minimizing discrimination errors given the resource-constrained problem considered here. We solve this problem for the case of two alternative forced choice tasks, where the average probability of error is given by (see Appendix 2)<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow> <mml:mtext>E[error] </mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∬</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mtext>error </mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mtext>P</mml:mtext><mml:mtext>error</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability of erroneously choosing the alternative with the lowest value <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> given a noisy percept <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (assuming that the goal of the organism in any given trial is to choose the alternative with the highest value). Here, we want to find the density function <inline-formula><mml:math id="inf120"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that guarantees the smallest average error (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). The solution to this problem is (Appendix 2)<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which is exactly the same prior density function over <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that maximizes mutual information (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Crucially, please note that we have obtained this expression based on minimizing the frequency of erroneous choices and not the maximization of mutual information as a goal in itself. This provides a further (and normative) justification for why maximizing mutual information under this coding scheme is beneficial when the goal of the agent is to minimize discrimination errors (i.e., maximize accuracy).</p></sec><sec id="s2-4-3"><title>Optimal noise for a known prior distribution</title><p>Based on the coding architecture presented in <xref ref-type="fig" rid="fig1">Figure 1</xref>, the optimal encoding function <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can then be implemented by choice of an appropriate distribution <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula>. It can be shown that discrimination performance can be optimized by finding the optimal noise distribution <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula> (Appendix 3) (<xref ref-type="bibr" rid="bib33">McDonnell et al., 2007</xref>)<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Remarkably, this result is independent of the number of samples <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> available to encode the input variable, and generalizes to any prior distribution <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (recall that <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined as its cumulative density function).</p><p>This result reveals three important aspects of neural function and decision behavior: First, it makes explicit why a system that evolved to code information using a coding scheme of the kind assumed in our framework must be necessarily noisy. That is, we do not attribute the randomness of peoples’ responses to a particular set of stimuli or decision problem to unavoidable randomness of the hardware used to process the information. Instead, the relevant constraints are assumed to be the limited set of output states for each neuron, the limited number of neurons, and the requirement that the neurons operate in parallel (so that each one’s output state must be statistically independent of the others, conditional on the input stimulus). Given these constraints, we show that it is efficient for the operation of the neurons to be random. Second, it shows how the nervous system may take advantage of these noisy properties by reshaping its noise structure to optimize decision behavior. Third, it shows that the noise structure can remain unchanged irrespective of the amount of resources available to guide behavior (i.e., the noise distribution <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula> does not depend on <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). Please note however, that this minimalistic implementation does not directly imply that the samples in our algorithmic formulation are necessarily drawn in this way. We believe that this implementation provides a simple demonstration of the consequences of limited resources in systems that encode information based on discrete stochastic events (<xref ref-type="bibr" rid="bib59">Sharpee, 2017</xref>). Interestingly, it has been shown that this minimalistic formulation can be extended to more realistic population coding specifications (<xref ref-type="bibr" rid="bib39">Nikitin et al., 2009</xref>).</p></sec><sec id="s2-4-4"><title>Efficient coding and the relation between environmental priors and discrimination</title><p>The results presented above imply that this encoding framework imposes limitations on the ability of capacity-limited systems to discriminate between different values of the encoded variables. Moreover, we have shown that error minimization in discrimination tasks implies a particular shape of the prior distribution of the encoder (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) that is exactly the prior density that maximizes mutual information between the input <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the encoded noisy readings <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <xref ref-type="fig" rid="fig2">Figure 2a</xref> right panel). Does this imply a relation between prior and discriminability over the space of the encoded variable? Intuitively, following the efficient coding hypothesis, the relation should be that lower discrimination thresholds should occur for ranges of stimuli that occur more frequently in the environment or context.</p><p>Recently, it was shown that using an efficiency principle for encoding sensory variables (e.g., with a heterogeneous population of noisy neurons [<xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>]) it is possible to obtain an explicit relationship between the statistical properties of the environment and perceptual discriminability (<xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>). The theoretical relation states that discriminability thresholds <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should be inversely proportional to the density of the prior distribution <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, we investigated whether this particular relation also emerges in the efficient coding scheme that we propose in this study.</p><p>Remarkably, we obtain the following relation between discriminability thresholds, prior distribution of input variables, and the number of limited samples <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (Appendix 4):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>d</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∝</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Interestingly, this relationship between prior distribution and discriminability thresholds holds empirically across several sensory modalities (Appendix 4), thus once again demonstrating that the efficient coding framework that we propose here seems to incorporate the right kind of constraints to explain observed perceptual phenomena as consequences of optimal allocation of finite capacity for internal representations.</p></sec><sec id="s2-4-5"><title>Maximizing the expected size of the selected option (fitness maximization)</title><p>Until now, we have studied the case when the goal of the organism is to minimize the number of mistakes in discrimination tasks. However, it is important to consider the case when the goal of the organism is to maximize fitness or expected reward (<xref ref-type="bibr" rid="bib46">Pirrone et al., 2014</xref>). For example, when spending the day foraging fruit, one must make successive decisions about which tree has more fruits. Fitness depends on the number of fruit collected which is not a linear function of the number of accurate decisions, as each choice yields a different amount of fruit.</p><p>Therefore, in the case of reward maximization, we are interested in minimizing reward loss which is given by the following expression<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf135"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of choosing option <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> when the input values are <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. Thus, the goal is to find the encoding rule <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which guarantees that the amount of reward loss is as small as possible given our proposed coding framework.</p><p>Here we show that the optimal encoding rule <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that guarantees maximization of expected value is given by<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi>c</mml:mi><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a normalizing constant which guarantees that the expression within the integral is a probability density function (Appendix 5). The first observation based on this result is that the encoding rule for maximizing fitness is different from the encoding rule that maximizes accuracy (compare <xref ref-type="disp-formula" rid="equ3 equ9">Equations 3 and 9)</xref>, which leads to a slight loss of information transmission (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Additionally, one can also obtain discriminability threshold predictions for this new encoding rule. Assuming a right-skewed prior distribution, which is often the case for various natural priors in the environment (e.g., like the one shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>), we find that discriminability for small input values is lower for reward maximization compared to perceptual maximization, however this pattern inverts for higher values (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). In other words, when we intend to maximize reward (given the shape of our assumed prior, <xref ref-type="fig" rid="fig2">Figure 2a</xref>), the agent should allocate more resources to higher values (compared to the perceptual case), however without completely giving up sensitivity for lower values, as these values are still encountered more often.</p></sec></sec><sec id="s2-5"><title>Efficient sampling with costs on acquiring prior knowledge</title><p>In the previous section, we obtained analytical solutions that approximately characterize the optimal <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the limit as <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is made sufficiently large. Note however that we are always assuming that is finite, and that this constrains the accuracy of the decision maker’s judgments, while <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is instead unbounded and hence no constraint.</p><p>The nature of the optimal function <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is different, however, when <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small. We argue that this scenario is particularly relevant when full knowledge of the prior is not warranted given the costs vs benefits of learning, for instance, when the system expects contextual changes to occur often. In this case, as we will formally elaborate below, it ceases to be efficient for <italic>θ</italic> to vary only gradually as a function of <inline-formula><mml:math id="inf147"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, rather than moving abruptly from values near zero to values near one (Appendix 6). In the large-<inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limiting case, the distributions of sample values <inline-formula><mml:math id="inf149"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> used by the different processing units will be nearly the same for each unit (approximating the current true distribution <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). Then if <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were to take only the values zero and one for different values of its arguments, the <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> units would simply produce <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> copies of the same output (either zero or one) for any given stimulus <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and distribution <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Hence only a very coarse degree of differentiation among different stimulus magnitudes would be possible. Having <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> vary more gradually over the range of values of <inline-formula><mml:math id="inf157"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> in the support of <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> instead makes the representation more informative. But when <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small (e.g., because of costs vs benefits of accurately representing the prior <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), this kind of arbitrary randomization in the output of individual processing units is no longer essential. There will already be considerable variation in the outputs of the different units, even when the output of each unit is a deterministic function of <inline-formula><mml:math id="inf161"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, owing to the variability in the sample of prior observations that is used to assess the nature of the current environment. As we will show below, this variability will already serve to allow the collective output of the several units to differentiate between many gradations in the magnitude of <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, rather than only being able to classify it as 'small' or 'large' (because either all units are in the 'low’ or 'high’ states).</p><sec id="s2-5-1"><title>Robust optimality of decision by sampling</title><p>Because of the way in which sampling variability in the values <inline-formula><mml:math id="inf163"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> used to adapt each unit’s encoding rule to the current context can substitute for the arbitrary randomization represented by the noise term <inline-formula><mml:math id="inf164"><mml:msub><mml:mi>η</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), a sharp reduction in the value of <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> need not involve a great loss in performance relative to what would be possible (for the same limit on <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) if <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were allowed to be unboundedly large (Appendix 7). As an example, consider the case in which <inline-formula><mml:math id="inf168"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, so that each unit <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’s output state must depend only on the value of the current stimulus <inline-formula><mml:math id="inf170"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and one randomly selected draw <inline-formula><mml:math id="inf171"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> from the prior distribution <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A possible decision rule that is radically economical in this way is one that specifies that the unit will be in the 'high' state if and only if <inline-formula><mml:math id="inf173"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> In this case, the internal representation of a stimulus <inline-formula><mml:math id="inf174"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> will be given by the number <inline-formula><mml:math id="inf175"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> out of <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> independent draws from the contextual distribution <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the property that the contextual draw is smaller than <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, as in the model of <italic>decision by sampling</italic> (DbS) (<xref ref-type="bibr" rid="bib61">Stewart et al., 2006</xref>). However, it remains to be determined to what degree it might be beneficial for a system to adopt such coding strategy.</p><p>In any given environment (characterized by a particular contextual distribution <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), DbS will be equivalent to an encoding process with an architecture of the kind shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, but in which the distribution <inline-formula><mml:math id="inf180"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (compare to the optimal noise distribution <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula> for the full prior adaptation case in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). This makes <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> vary endogenously depending on the contextual distribution <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. And indeed, the way that <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> varies with the contextual distribution under DbS is fairly similar to the way in which it would be optimal for it to vary in the absence of any cost of precisely learning and representing the contextual distribution. This result implies that <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will be a monotonic transformation of a function that increases more steeply over those regions of the stimulus space where <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is higher, regardless of the nature of the contextual distribution. We consider its performance in a given environment, from the standpoint of each of the possible performance criteria considered for the case of full prior adaptation (i.e., maximize accuracy or fitness), and show that it differs from the optimal encoding rules under any of those criteria (<xref ref-type="fig" rid="fig2">Figure 2b–d</xref>). In particular, here, we show that using the encoding rule employed in DbS results in considerable loss of information compared to the full-prior adaptation solutions (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). An additional interesting observation is that for the strategy employed in DbS, the agent appears to be more sensitive for extreme input values, at least for a wide set of skewed distributions (e.g., for the prior distribution <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2a</xref>, the discriminability thresholds are lower at the extremes of the support of <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). In other words, agents appear to be more sensitive to salience in the DbS rule. Despite these differences, here it is important to emphasize that in general for all optimization objectives, the encoding rules will be steeper for regions of the prior with higher density. However, mild changes in the steepness of the curves will be represented in significant discriminability differences between the different encoding rules across the support of the prior distribution (<xref ref-type="fig" rid="fig2">Figure 2d</xref>).</p><p>While the predictions of DbS are not exactly the same as those of efficient coding in the case of unbounded <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, under any of the different objectives that we consider, our numerical results show that it can achieve performance nearly as high as that of the theoretically optimal encoding rule; hence radically reducing the value of <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> does not have a large cost in terms of the accuracy of the decisions that can be made using such an internal representation (Appendix 7 and <xref ref-type="fig" rid="fig2">Figure 2e</xref>). Under the assumption that reducing either <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> would serve to economize on scarce cognitive resources, we formally prove that it might well be most efficient to use an algorithm with a very low value of <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (even <inline-formula><mml:math id="inf194"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> as assumed by DbS), while allowing <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be much larger (Appendix 6, Appendix 7).</p><p>Crucially, here, it is essential to emphasize that the above-mentioned results are derived for the case of a particular finite number of processing units <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (and a corresponding finite total number of samples from the contextual distribution used to encode a given stimulus), and do not require that <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> must be large (Appendix 6, Appendix 7).</p></sec></sec><sec id="s2-6"><title>Testing theories of numerosity discrimination</title><p>Our goal now is to compare back-to-back the resource-limited coding frameworks elaborated above in a fundamental cognitive function for human behavior: numerosity perception. We designed a set of experiments that allowed us to test whether human participants would adapt their numerosity encoding system to maximize fitness or accuracy rates via full prior adaptation as usually assumed in optimal models, or whether humans employ a 'less optimal' but more efficient strategy such as DbS, or the more established logarithmic encoding model.</p><p>In Experiment 1, healthy volunteers (n = 7) took part in a two-alternative forced choice numerosity task in which each participant completed ∼2400 trials across four consecutive days (Materials and methods). On each trial, they were simultaneously presented with two clouds of dots and asked which one contained more dots, and were given feedback on their reward and opportunity losses on each trial (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Participants were either rewarded for their accuracy (perceptual condition, where maximizing the amount of correct responses is the optimal strategy) or the number of dots they selected (value condition, where maximizing reward is the optimal strategy). Each condition was tested for two consecutive days with the starting condition randomized across participants. Crucially, we imposed a prior distribution <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a right-skewed quadratic shape (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), whose parametrization allowed tractable analytical solutions of the encoding rules <inline-formula><mml:math id="inf199"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf200"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf201"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that correspond to the encoding rules for Accuracy maximization, Reward maximization, and DbS, respectively (<xref ref-type="fig" rid="fig3">Figure 3e</xref> and Materials and methods). Qualitative predictions of behavioral performance indicate that the accuracy-maximization model is the most accurate for trials with lower numerosities (the most frequent ones), whereas the reward-maximization model outperforms the others for trials with larger numerosities (trials where the difference in the number of dots in the clouds, and thus the potential reward, is the largest, <xref ref-type="fig" rid="fig2">Figure 2d</xref> and <xref ref-type="fig" rid="fig3">Figure 3f</xref>). In contrast, the DbS strategy presents markedly different performance predictions, in line with the discriminability predictions of our formal analyses (<xref ref-type="fig" rid="fig2">Figure 2c,d</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experimental design, model simulations and recovery.</title><p>(<bold>a</bold>) Schematic task design of Experiments 1 and 2. After a fixation period (1–2 s) participants were presented two clouds of dots (200 ms) and had to indicate which cloud contained the most dots. Participants were rewarded for being accurate (perceptual condition) or for the number of dots they selected (value condition) and were given feedback. In Experiment 2 participants collected on correctly answered trials a number of points equal to a fixed amount (perceptual condition) or a number equal to the dots in the cloud they selected (value condition) and had to reach a threshold of points on each run. (<bold>b</bold>) Empirical (grey bars) and theoretical (purple line) distribution of the number of dots in the clouds of dots presented across Experiments 1 and 2. (<bold>c</bold>) Distribution of the numerosity pairs selected per trial. (<bold>d</bold>) Synthetic data preserving the trial set statistics and number of trials per participant used in Experiment 1 was generated for each encoding rule (Accuracy (left), Reward (middle), and DbS (right)) and then the latent-mixture model was fitted to each generated dataset. The figures show that it is theoretically possible to recover each generated encoding rule. (<bold>e</bold>) Encoding function <inline-formula><mml:math id="inf202"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the different sampling strategies as a function of the input values <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e., the number of dots). (<bold>f</bold>) Qualitative predictions of the three models (blue: Accuracy, red: Reward, green: Decision by Sampling) on trials from Experiment 1 with <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>. Performance of each model as a function of the sum of the number of dots in both clouds (left), the absolute difference between the number of dots in both clouds (middle) and the ratio of the number of dots in the most numerous cloud over the less numerous cloud (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Model recovery for <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> fixed.</title><p>The latent-mixture model was fitted to synthetic data obtained by simulating 10 times each encoding rule on the trials from participants of Experiment 1. This also means that we used the same number of trials per condition that each participant experienced in our experiments. Each histogram shows the proportion (Prop) of the recovered encoding rule for synthetic data from (<bold>a</bold>) the accuracy maximizing encoding rule <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>θ</mml:mi><mml:mtext>A</mml:mtext></mml:msub></mml:math></inline-formula>, (<bold>b</bold>) the reward maximizing encoding rule <inline-formula><mml:math id="inf207"><mml:msub><mml:mi>θ</mml:mi><mml:mtext>R</mml:mtext></mml:msub></mml:math></inline-formula>, and (<bold>c</bold>) decision by sampling <inline-formula><mml:math id="inf208"><mml:msub><mml:mi>θ</mml:mi><mml:mtext>D</mml:mtext></mml:msub></mml:math></inline-formula>. The latent mixture model can accurately recover the underlying encoding rule. In this model the <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter was set to 2.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Model recovery with both <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as free parameters.</title><p>Synthetic data preserving the trial set statistics and number of trials per participant used in Experiment 1 was generated 100 times for each encoding rule with various values of <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. A model for each encoding rule was fitted to the data using maximum likelihood estimators with <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as free parameters. The histograms represent the proportion of best fitting models for data generated by (<bold>a</bold>) Accuracy, (<bold>b</bold>) Reward and (<bold>c</bold>) DbS models. Results are shown for different simulated values of <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (top: <inline-formula><mml:math id="inf217"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, middle: <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and bottom: <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>) and <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (left: <inline-formula><mml:math id="inf221"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>, middle: <inline-formula><mml:math id="inf222"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> and right: <inline-formula><mml:math id="inf223"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:math></inline-formula>). While DbS is always well recovered, the Accuracy and Reward models tend to be confounded with each other. (<bold>d</bold>) This same synthetic data were fitted with its generating model with <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as free parameters using maximum likelihood estimators. Results are shown for different simulated values of <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (first and second columns: <inline-formula><mml:math id="inf227"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, third and fourth columns: <inline-formula><mml:math id="inf228"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and fifth and sixth columns: <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>) and <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (top: <inline-formula><mml:math id="inf231"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:math></inline-formula>, middle: <inline-formula><mml:math id="inf232"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> and bottom: <inline-formula><mml:math id="inf233"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>). Error bars represent ± SD of the recovered parameter across simulations. The parameters are well recovered by the respective generating model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Discriminability differences between the different encoding rules.</title><p>This figure illustrates the discriminability differences between the different encoding rules considered in this study. Each dot represents the discriminability value for a pair of numerosity values <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> presented on a given trial to the participants in Experiment 1. For the sampling models, the discriminability rule is defined as<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the respective Accuracy maximizing (A), Reward maximizing (R) or Decision by Sampling (D) encoding rules. For the logarithmic model (L) the discriminability rule is defined as<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>log</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The color of each dot represents the log of the number of occurrences for the pairs of input values <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf238"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. Note that the encoding values of the presented numerosities are different depending on the encoding rule, which makes it possible to identify the participants’ encoding strategy. Also note that for our imposed prior distribution, the DbS encoding rule is similar to the logarithmic rule, which explains the smaller difference in the quantitative predictions between these two models. Nevertheless, DbS was always the model that provided the best quantitative and qualitative predictions irrespective of incentivized goals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig3-figsupp3-v1.tif"/></fig></fig-group><p>In our modelling specification, the choice structure is identical for the three different sampling models, differing only in the encoding rule <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Materials and methods). Therefore, answering the question of which encoding rule is the most favored for each participant can be parsimoniously addressed using a latent-mixture model, where each participant uses <inline-formula><mml:math id="inf240"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf241"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf242"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to guide their decisions (Materials and methods). Before fitting this model to the empirical data, we confirmed the validity of our model selection approach through a validation procedure using synthetic choice data (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, and Materials and methods).</p><p>After we confirmed that we can reliably differentiate between our competing encoding rules, the latent-mixture model was initially fitted to each condition (perceptual or value) using a hierarchical Bayesian approach (Materials and methods). Surprisingly, we found that participants did not follow the accuracy or reward optimization strategy in the respective experimental condition, but favored the DbS strategy (proportion that DbS was deemed best in the perceptual <inline-formula><mml:math id="inf243"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.86</mml:mn></mml:mrow></mml:math></inline-formula> and value <inline-formula><mml:math id="inf244"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:math></inline-formula> conditions, <xref ref-type="fig" rid="fig4">Figure 4</xref>). Importantly, this population-level result also holds at the individual level: DbS was strongly favored in 6 out of 7 participants in the perceptual condition, and seven out of seven in the value condition (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). These results are not likely to be affected by changes in performance over time, as performance was stable across the four consecutive days (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Additionally, we investigated whether biases induced by choice history effects may have influenced our results (<xref ref-type="bibr" rid="bib1">Abrahamyan et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Keung et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Talluri et al., 2018</xref>). Therefore, we incorporated both choice- and correctness-dependence history biases in our models and fitted the models once again (Materials and methods). We found similar results to the history-free models (<inline-formula><mml:math id="inf245"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow></mml:math></inline-formula> in perceptual and <inline-formula><mml:math id="inf246"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:math></inline-formula> in value conditions, <xref ref-type="fig" rid="fig4">Figure 4c</xref>). At the individual level, DbS was again strongly favored in 6 out of 7 participants in the perceptual condition, and 7 out of 7 in the value condition (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Behavioral results.</title><p>(<bold>a</bold>) Bars represent proportion of times an encoding rule (Accuracy [A, blue], Reward [R, red], DbS [D, green]) was selected by the Bayesian latent-mixture model based on the posterior estimates across participants. Each panel shows the data grouped for each and across experiments and experimental conditions (see titles on top of each panel). The results show that DbS was clearly the favored encoding rule. The latent vector <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> posterior estimates are presented in <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>. (<bold>b</bold>) Difference in LOO and WAIC between the best model (DbS (D) in all cases) and the competing models: Accuracy (A), Reward (R) and Logarithmic (L) models. Each panel shows the data grouped for each and across experimental conditions and experiments (see titles on top of each panel). (<bold>c</bold>) Behavioral data (black, error bars represent SEM across participants) and model predictions based on fits to the empirical data. Data and model predictions are presented for both the perceptual (left panels) or value (right panels) conditions, and excluding (top panels) or including (bottom panels) choice history effects. Performance of data model predictions is presented as function of the sum of the number of dots in both clouds (left), the absolute difference between the number of dots in both clouds (middle) and the ratio of the number of dots in the most numerous cloud over the less numerous cloud (right). Results reveal a remarkable overlap of the behavioral data and predictions by DbS, thus confirming the quantitative results presented in panels a and b.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Latent mixture model fits for each participant.</title><p>Individual level fit of the latent mixture model excluding (top) or including (bottom) choice history effects for (<bold>a</bold>) Experiment 1 and (<bold>b</bold>) Experiment 2. The panels on the far right shows the average fit for all the participants of the given experiment. DbS is strongly favored for nearly all participants and clearly favored across participants, irrespective of the experimental condition. Including choice and correctness information of previous trials has minimal influence in the results of these analyses, which rules out the influence of these effects on the decision rule used by the participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Performance across time.</title><p>Behavioral performance (mean ± SEM across participants) averaged over a moving window of 100 trials for (<bold>a</bold>) Experiment 1, (<bold>b</bold>) Experiment 2 and (<bold>c</bold>) Experiment 3. Each daily session took place between two dotted vertical lines. The performance of the participants is stable during and between daily sessions. Therefore, the quantitative and qualitative results presented in the main text are not likely to be influenced by changes in performance over time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Individual level fit of the latent mixture model combining data across experiments and experimental conditions.</title><p>(<bold>a</bold>) Individual level fit of the latent mixture model combining data across both experimental conditions for Experiment 1 (top) and Experiment 2 (bottom). (<bold>b</bold>) Individual level fit of the latent mixture model combining data across both experimental conditions and both experiments. Each panel shows the results excluding (top) or including (bottom) choice history effects. The panels labeled 'All participants' show the average fit for all the participants of the given experiment. DbS is strongly favored irrespective of incentivized goals. Including the previous trial effects has minimal influence on these results.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Model comparison based on leave-one-out cross-validation metrics.</title><p>Quantitative comparison of the models including choice and correctness effects of previous trials based on leave-one-out cross-validation metrics. Difference in LOO and WAIC between the best model (DbS (D) in all cases) and the competing models: Accuracy (A), Reward (R) and Logarithmic (L) models. Each panel shows the data grouped for each and across experiments and experimental conditions (see titles on top of each panel). Including the previous choice and correctness effects has only little influence on the results (compare with <xref ref-type="fig" rid="fig4">Figure 4b</xref> in main text). The DbS model provides the best fit to the behavioral data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Reaction times are similar in the perceptual and value conditions.</title><p>Mean reaction times of participants in Experiments 1 and 2 in the perceptual (red) and value (blue) condition. Error bars represent SEM across participants. Reaction times are presented as a function of the sum of the number of dots in both clouds (left), the absolute difference between the number of dots in both clouds (middle) and the ratio of the number of dots in the most numerous cloud over the less numerous cloud (right). Non-parametric ANOVA tests revealed no significant differences in any of these behavioral assessments (all tests p&gt;0.4).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp5-v1.tif"/></fig><fig id="fig4s6" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 6.</label><caption><title>Behavior and model predictions as a function of sum and difference in dots.</title><p>(<bold>a</bold>) Average behavior in both conditions of Experiments 1 and 2 as a function of the sum of the number of dots in both clouds (Sum Ndots) and the absolute difference between the number of dots in both clouds (Difference Ndots). The data are binned as in <xref ref-type="fig" rid="fig4">Figure 4</xref> but now expanded in two dimensions. (<bold>b</bold>) Predictions of each encoding rule model fit with only <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as a free parameter shown with the same scale as in a. (<bold>c</bold>) Linear regression between the behavior for each combination of Sum Ndots and Difference Ndots bins and the predictions of each model for the same bins. DbS captures best the changes in behavior across bins of sum and absolute difference of the number of dots in both clouds. This analysis should not be considered as a quantitative proof, but as a qualitative inspection of the results presented in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp6-v1.tif"/></fig><fig id="fig4s7" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 7.</label><caption><title>Model fit for the first experimental condition of each participant.</title><p>Similar as in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, bars represent proportion of times an encoding rule (Accuracy [A, blue], Reward [R, red], DbS [D, green]) was selected by the Bayesian latent-mixture model based on the posterior estimates across participants. Each panel shows the data grouped for each experiment and experimental conditions (see titles on top of each panel). The latent-mixture model was only fit to the first condition that was carried out by each participant. As the participants did not know of the second condition before carrying it out, they could not adopt compromise strategies between the two objectives. Therefore, the fact that DbS is favored in the results is not an artifact of carrying out two different conditions in the same participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp7-v1.tif"/></fig><fig id="fig4s8" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 8.</label><caption><title>Latent vector <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> posterior estimates.</title><p>Bars represent the posterior distribution of the latent vector <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with each bar representing an encoding rule (Accuracy (A, blue), Reward (R, red), DbS (D, green)). Results are presented for (<bold>a</bold>) all sessions and (<bold>b</bold>) only the first condition carried out by each participant. DbS is consistently the most likely encoding rule.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig4-figsupp8-v1.tif"/></fig></fig-group><p>In order to investigate further the robustness of this effect, we introduced a slight variation in the behavioral paradigm. In this new experiment (Experiment 2), participants were given points on each trial and had to reach a certain threshold in each run for it to be eligible for reward (<xref ref-type="fig" rid="fig3">Figure 3a</xref> and Materials and methods). This class of behavioral task is thought to be in some cases more ecologically valid than trial-independent choice paradigms (<xref ref-type="bibr" rid="bib28">Kolling et al., 2014</xref>). In this new experiment, either a fixed amount of points for a correct trial was given (perceptual condition) or an amount equal to the number of dots in the chosen cloud if the response was correct (value condition). We recruited a new set of participants (n = 6), who were tested on these two conditions, each for two consecutive days with the starting condition randomized across participants (each participant completed <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>560</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> trials). The quantitative results revealed once again that participants did not change their encoding strategy depending on the goals of the task, with DbS being strongly favored for both perceptual and value conditions (<inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf253"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.91</mml:mn></mml:mrow></mml:math></inline-formula>, respectively; <xref ref-type="fig" rid="fig4">Figure 4a</xref>), and these results were confirmed at the individual level where DbS was strongly favored in 6 out of 6 participants in both the perceptual and value conditions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Once again, we found that inclusion of choice history biases in this experiment did not significantly affect our results both at the population and individual levels. Population probability that DbS was deemed best in the perceptual (<inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math></inline-formula>) and value (<inline-formula><mml:math id="inf255"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>DbSfavored</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>) conditions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), and at the individual level DbS was strongly favored in 6 out of 6 participants in the perceptual condition and 5 of 6 in the value condition (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, Experiments 1 and 2 strongly suggest that our results are not driven by specific instructions or characteristics of the behavioral task.</p><p>As a further robustness check, for each participant we grouped the data in different ways across experiments (Experiments 1 and 2) and experimental conditions (perceptual or value) and investigated which sampling model was favored. We found that irrespective of how the data was grouped, DbS was the model that was clearly deemed best at the population (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and individual level (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). Additionally, we investigated whether these quantitative results specifically depended on our choice of using a latent-mixture model. Therefore, we also fitted each model independently and compared the quality of the model fits based on out-of-sample cross-validation metrics (Materials and methods). Once again, we found that the DbS model was favored independently of experiment and conditions (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>One possible reason why the two experimental conditions did not lead to differences could be that, after doing one condition for two days, the participants did not adapt as easily to the new incentive rule. However, note that as the participants did not know of the second condition before carrying it out, they could not adopt a compromise between the two behavioral objectives. Nevertheless, we fitted the latent-mixture model only to the first condition that was carried out by each participant. We found once again that DbS was the best model explaining the data, irrespective of condition and experimental paradigm (<xref ref-type="fig" rid="fig4s7">Figure 4—figure supplement 7</xref>). Therefore, the fact that DbS is favored in the results is not an artifact of carrying out two different conditions in the same participants.</p><p>We also investigated whether the DbS model makes more accurate predictions than the widely used logarithmic model of numerosity discrimination tasks (<xref ref-type="bibr" rid="bib14">Dehaene, 2003</xref>). We found that DbS still made better out-of-sample predictions than the log-model (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, <xref ref-type="fig" rid="fig5">Figure 5f,g</xref>). Moreover, these results continued to hold after taking into account possible choice history biases (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). In addition to these quantitative results, qualitatively we also found that behavior closely matched the predictions of the DbS model remarkably well (<xref ref-type="fig" rid="fig4">Figure 4c</xref>), based on virtually only one free parameter, namely, the number of samples (resources) <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Together, these results provide compelling evidence that DbS is the most likely resource-constrained sampling strategy used by participants in numerosity discrimination tasks.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Prior adaptation analyses.</title><p>(<bold>a</bold>) Estimation of the shape parameter <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for the DbS model by grouping the data for each and across experimental conditions and experiments. Error bars represent the 95% highest density interval of the posterior estimate of <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at the population level. The dashed line shows the theoretical value of <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>b</bold>) Theoretical prior distribution <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in Experiments 1 and 2 (<inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, purple) and 3 (<inline-formula><mml:math id="inf262"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, orange). The dashed line represents the value of <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of our prior parametrization that approximates the DbS and log discriminability models. (<bold>c</bold>) Posterior estimation of <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ29">Equation 18</xref>) as a function of the number of trials <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in each daily session for Experiments 1 and 2 (purple) and Experiment 3 (orange). The results reveal that, as expected, <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> reaches a lower asymptotic value <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Error bars represent ± SD of 3000 simulated <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values drawn from the posterior estimates of the HBM (see Materials and methods). (<bold>d</bold>) Model fit to the first 150 and last 350 trials of each daily session. The <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter was allowed to vary between the first and last sets of daily trials and between Experiments 1–2 and Experiment 3. In Experiment 3, <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is lower in the last set of trials compared to the first set of trials (<inline-formula><mml:math id="inf271"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>MCMC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.013</mml:mn></mml:mrow></mml:math></inline-formula>). In addition, <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for the last trials is lower for Experiment 3 than for Experiments 1–2 (<inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>MCMC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.006</mml:mn></mml:mrow></mml:math></inline-formula>). This confirms that the results presented in panel c are not artifacts of the adaptation parametrization assumed for <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Error bars represent ± SD of the posterior chains of the corresponding parameter. (*P&lt;0.05, **P&lt;0.01, and ***P&lt;0.001). (<bold>e</bold>) Behavioral data (black) and model fit predictions of the DbS (green) and Log (yellow) models. Performance of each model as a function of the sum of the number of dots in both clouds (left), the absolute difference between the number of dots in both clouds (middle) and the ratio of the number of dots in the most numerous cloud over the less numerous cloud (right). Error bars represent SEM (<bold>f</bold>) Difference in LOO and WAIC between the best fitting DbS (D) and logarithmic encoding (Log) model. (<bold>g</bold>) Population exceedance probabilities (xp, left) and protected exceedance probabilities (pxp, right) for DbS (green) vs Log (yellow) of a Bayesian model selection analysis (<xref ref-type="bibr" rid="bib60">Stephan et al., 2009</xref>): <inline-formula><mml:math id="inf275"><mml:mrow><mml:msub><mml:mi>xp</mml:mi><mml:mi>DbS</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf276"><mml:mrow><mml:msub><mml:mi>pxp</mml:mi><mml:mi>DbS</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow></mml:math></inline-formula>. These results provide a clear indication that the adaptive DbS explains the data better than the Log model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Performance across trial experience.</title><p>These plots represent the performance of the participants as a function of the number of trials they have experienced during the session. The performance of the participants (black, shaded area represents ± SEM across participants) was averaged over a moving window of 21 trials and is shown for Experiment 1 (<bold>a</bold>) Experiment 2 (<bold>b</bold>) and Experiment 3 (<bold>c</bold>). The blue line represents the performance predicted by the <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-adaptation model using the same moving window average. The model provides a good fit to average performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Quantitative and dynamical analysis of adaptation over time.</title><p>To further investigate the adaptation of the prior, we fit three models of varying complexity to the data of Experiments 1, 2 and 3. The Fixed-<inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model (blue) is defined with a fixed <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The Free-<inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model (red) allows the <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter to vary across participants but is kept constant across time. The Adaptative-<inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the model presented in <xref ref-type="fig" rid="fig5">Figure 5</xref> where the prior adapts as the participants gains experience with the experimental distribution of dots. To allow a fair comparison with the Free-<inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model, the <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter, corresponding to the asymptotic value of the prior, was free to vary across participants. The log-likelihood of each model on each trial were averaged over a moving window of 100 trials and the log-likelihood of the Adaptative-<inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model was subtracted for comparison. Vertical dashed lines represent 1, 2 and 3 times <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the rate of adaptation in the Adaptative-<inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model. The Adaptative-<inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model provides a better fit for the first trials (until around <inline-formula><mml:math id="inf290"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>), these trials correspond to the adaptation period where the <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter is changing in the Adaptative-<inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). After this point the Adaptative-<inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and Free-<inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> models provide a similar fit. This is to be expected as the function controlling the decay of <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> reaches its asymptotic value, leaving the two model virtually identical. The Fixed-<inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> provides overall a worse fit, except for the early trials.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Model fits for the beginning and end of each session without parametric assumptions.</title><p>A model was fitted to the first 150 and last 350 trials of each daily session. The prior parameter <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the number of neural resources <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were allowed to vary between the first and last sets of daily trials and between Experiments 1–2 (purple) and Experiment 3 (orange). (<bold>a</bold>) Each bar represents the mean value of the <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter for a combination of experiments and set of daily trials. In Experiment 3, <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is lower in the last set of trials compared to the first set of trials. In addition, the value of <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for Experiment 3 is lower than for Experiments 1–2 in the last set of daily trials. (<bold>b</bold>) Each bar represents the value of the neural resource parameter <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a combination of experiments and set of daily trials. The neural resources parameter <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in Experiment 3 is larger than in Experiments 1–2. However, there is no change in the neural resource parameter across the session. This suggests that the adaptation process is not an artifact of changes in the neural resource parameter, which could for example change with the engagement of the participants across the session. Significance between parameters was computed by subtracting the chain with the largest mean to the other one and measuring the proportion of values that fall below 0 (n.s. P &lt; 0.05, *P&lt;0.05, **P&lt;0.01, and ***P&lt;0.001). Error bars represent ± SD of the posterior chains of the corresponding parameter.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-fig5-figsupp3-v1.tif"/></fig></fig-group><p>Recent studies have also investigated behavior in tasks where perceptual and preferential decisions have been investigated in paradigms with identical visual stimuli (<xref ref-type="bibr" rid="bib15">Dutilh and Rieskamp, 2016</xref>; <xref ref-type="bibr" rid="bib47">Polanía et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Grueschow et al., 2015</xref>). In these tasks, investigators have reported differences in behavior, in particular in the reaction times of the responses, possibly reflecting differences in behavioral strategies between perceptual and value-based decisions. Therefore, we investigated whether this was the case also in our data. We found that reaction times did not differ between experimental conditions for any of the different performance assessments considered here (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). This further supports the idea that participants were in fact using the same sampling mechanism irrespective of behavioral goals.</p><p>Here it is important to emphasize that all sampling models and the logarithmic model of numerosity have the same degrees of freedom (performance is determined by <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the sampling models and Weber’s fraction <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the log model, Materials and methods). Therefore, qualitative and quantitative differences favoring the DbS model cannot be explained by differences in model complexity. It could also be argued that normal approximation of the binomial distributions in the sampling decision models only holds for large enough <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, we find evidence that the large-<inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> optimal solutions are also nearly optimal for low <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> values (Appendix 7). Estimates of <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in our data are in general <inline-formula><mml:math id="inf310"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≈</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="table" rid="table1">Table 1</xref>) and we find that the large-<inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> rule is nearly optimal already for <inline-formula><mml:math id="inf312"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> (Appendix 7). Therefore the asymptotic approximations should not greatly affect the conclusions of our work.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Resource parameter <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> fits.</title><p>Fits of the resource parameter for the Accuracy, Reward and Decision by Sampling (DbS) models including data across experiments and conditions (perceptual (P) or value (V)) either including or ignoring choice history effects. The values represent the mean ± SD of the posterior distributions at the population level for parameter <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that Reward and in particular the DbS encoding models require a higher number of resources than the Accuracy model, which is coherent with the fact that the Accuracy model allocates its resources to maximize efficiency, therefore reducing the number of resources needed to reach a given accuracy. DbS has the highest values of <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> because it is the most inefficient model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th/><th colspan="3">Model</th></tr><tr><th>Experiment</th><th>Condition</th><th>History effects</th><th><italic>n<sub>Accuracy</sub></italic></th><th><italic>n<sub>Reward</sub></italic></th><th><italic>n<sub>DbS</sub></italic></th></tr></thead><tbody><tr><td>1</td><td>V</td><td>not included</td><td>15.24 ± 3.09</td><td>17.54 ± 3.98</td><td>24.40 ± 5.16</td></tr><tr><td>2</td><td>V</td><td>not included</td><td>22.48 ± 2.43</td><td>27.58 ± 3.81</td><td>35.40 ± 3.44</td></tr><tr><td>1</td><td>P</td><td>not included</td><td>15.19 ± 3.99</td><td>17.84 ± 4.85</td><td>24.64 ± 6.59</td></tr><tr><td>2</td><td>P</td><td>not included</td><td>20.99 ± 1.59</td><td>24.22 ± 1.93</td><td>33.54 ± 2.45</td></tr><tr><td>1</td><td>P/V</td><td>not included</td><td>15.33 ± 3.41</td><td>17.25 ± 4.45</td><td>24.15 ± 5.75</td></tr><tr><td>2</td><td>P/V</td><td>not included</td><td>21.30 ± 0.96</td><td>25.27 ± 1.99</td><td>33.90 ± 1.51</td></tr><tr><td>1/2</td><td>V</td><td>not included</td><td>18.56 ± 2.04</td><td>22.05 ± 2.73</td><td>29.52 ± 3.25</td></tr><tr><td>1/2</td><td>P</td><td>not included</td><td>17.91 ± 2.09</td><td>20.66 ± 2.59</td><td>28.62 ± 3.51</td></tr><tr><td>1/2</td><td>P/V</td><td>not included</td><td>17.93 ± 1.87</td><td>21.03 ± 2.46</td><td>28.58 ± 3.04</td></tr><tr><td>1</td><td>V</td><td>included</td><td>15.50 ± 3.13</td><td>17.50 ± 3.91</td><td>24.68 ± 5.08</td></tr><tr><td>2</td><td>V</td><td>included</td><td>22.92 ± 2.37</td><td>28.07 ± 3.73</td><td>36.18 ± 2.91</td></tr><tr><td>1</td><td>P</td><td>included</td><td>15.41 ± 3.81</td><td>17.96 ± 4.88</td><td>24.70 ± 6.62</td></tr><tr><td>2</td><td>P</td><td>included</td><td>21.57 ± 1.71</td><td>24.88 ± 2.17</td><td>34.37 ± 2.93</td></tr><tr><td>1</td><td>P/V</td><td>included</td><td>15.16 ± 3.55</td><td>17.43 ± 4.39</td><td>24.30 ± 5.94</td></tr><tr><td>2</td><td>P/V</td><td>included</td><td>21.80 ± 0.92</td><td>25.81 ± 1.86</td><td>34.60 ± 1.40</td></tr><tr><td>1/2</td><td>V</td><td>included</td><td>18.86 ± 2.07</td><td>22.48 ± 2.75</td><td>29.85 ± 3.17</td></tr><tr><td>1/2</td><td>P</td><td>included</td><td>18.15 ± 2.17</td><td>21.11 ± 2.72</td><td>29.01 ± 3.47</td></tr><tr><td>1/2</td><td>P/V</td><td>included</td><td>18.22 ± 1.93</td><td>21.34 ± 2.50</td><td>29.12 ± 3.12</td></tr></tbody></table></table-wrap></sec><sec id="s2-7"><title>Dynamics of adaptation</title><p>Up to now, fits and comparison across models have been done under the assumption that the participants learned the prior distribution <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> imposed in our task. If participants are employing DbS, it is important to understand the dynamical nature of adaptation in our task. Note that the shape of the prior distribution is determined by the parameter <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, <xref ref-type="disp-formula" rid="equ12">Equation 10</xref> in Materials and methods). First, we made sure based on model recovery analyses that the DbS model could jointly and accurately recover both the shape parameter <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the resource parameter <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> based on synthetic data (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Then we fitted this model to the empirical data and found that the recovered value of the shape parameter <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> closely followed the value of the empirical prior with a slight underestimation (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Next, we investigated the dynamics of prior adaptation. To this end, we ran a new experiment (Experiment 3, n = 7 new participants) in which we set the shape parameter of the prior to a lower value compared to Experiments 1–2 (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, Materials and methods). We investigated the change of <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> over time by allowing this parameter to change with trial experience (<xref ref-type="disp-formula" rid="equ29">Equation 18</xref>, Materials and methods) and compared the evolution of <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for Experiments 1 and 2 (empirical <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>) with Experiment 3 (empirical <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig5">Figure 5b</xref>). If participants show prior adaptation in our numerosity discrimination task, we hypothesized that the asymptotic value of <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should be higher for Experiments 1–2 than for Experiment 3. First, we found that for Experiments 1–2, the value of <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> quickly reached an asymptotic value close to the target value (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). On the other hand, for Experiment 3 the value of <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> continued to decrease during the experimental session, but slowly approaching its target value. This seemingly slower adaptation to the shape of the prior in Experiment 3 might be explained by the following observation. The prior parametrized with <inline-formula><mml:math id="inf328"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in Experiment 3 is further away from an agent hypothesized to have a natural numerosity discrimination based on a log scale (<inline-formula><mml:math id="inf329"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2.58</mml:mn></mml:mrow></mml:math></inline-formula>, Materials and methods), which is closer in value to the shape of the prior in Experiments 1 and 2 (<inline-formula><mml:math id="inf330"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>). Irrespective of these considerations, the key result to confirm our adaptation hypothesis is that the asymptotic value of <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is lower for Experiment 3 compared to Experiments 1 and 2 (<inline-formula><mml:math id="inf332"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>MCMC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.006</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>In order to make sure that this result was not an artifact of the parametric form of adaptation assumed here (<xref ref-type="disp-formula" rid="equ29">Equation 18</xref>, Materials and methods), we fitted the DbS model to trials at the beginning and end of each experimental session allowing <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be a free but fixed parameter in each set of trials. The results of these new analyses are virtually identical to the results obtained with the parametric form, in which <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is smaller at the end of Experiment 3 sessions relative to beginning of Experiments 1 and 2 (<inline-formula><mml:math id="inf335"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>MCMC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0003</mml:mn></mml:mrow></mml:math></inline-formula>), beginning of Experiments 3 (<inline-formula><mml:math id="inf336"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>MCMC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.013</mml:mn></mml:mrow></mml:math></inline-formula>) and end of Experiments 1 and 2 (<inline-formula><mml:math id="inf337"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>MCMC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.006</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig5">Figure 5d</xref>). In this model, we did not allow <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to freely change for each condition, and therefore a concern might be that the results might be an artifact of changes in <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which could for example change with the engagement of the participants across the session. Given that we already demonstrated that both parameters <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are identifiable, we fitted the same model as in <xref ref-type="fig" rid="fig5">Figure 5d</xref>, however this time we allowed <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be free parameter alongside <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We found that the results obtained in <xref ref-type="fig" rid="fig5">Figure 5d</xref> remained virtually unchanged (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>), in addition to the result that the resource parameter <inline-formula><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> remained virtually identical across the session (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p><p>We further investigated evidence for adaptation using an alternative quantitative approach. First, we performed out-of-sample model comparisons based on the following models: (i) the adaptive-<inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model, (ii) free-<inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model with <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> free but non-adapting over time, and (iii) fixed-<inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model with <inline-formula><mml:math id="inf349"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The results of the out-of-sample predictions revealed that the best model was the free-<inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model, followed closely by the adaptive-<inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model (<inline-formula><mml:math id="inf352"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>LOO</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.8</mml:mn></mml:mrow></mml:math></inline-formula>) and then by fixed-<inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model (<inline-formula><mml:math id="inf354"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>LOO</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>32.6</mml:mn></mml:mrow></mml:math></inline-formula>). However, we did not interpret the apparent small difference between the adaptive-<inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the free-<inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> models as evidence for lack of adaptation, given that the more complex adaptive-<inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model will be strongly penalized after adaptation is stable. That is, if adaptation is occurring, then the adaptive-<inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> only provides a better fit for the trials corresponding to the adaptation period. After adaptation, the adaptive-<inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should provide a similar fit than the free-<inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model, however with a larger complexity that will be penalized by model comparison metrics. Therefore, to investigate the presence of adaptation, we took a closer quantitative look at the evolution of the fits across trial experience. We computed the average trial-wise predicted Log-Likelihood (by sampling from the hierarchical Bayesian model) and compared the differences of this metric between the competing models and the adaptive model. We hypothesized that if adaptation is taking place, the adaptive-<inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model would have an advantage relative to the free-<inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model at the beginning of the session, with these differences vanishing toward the end. On the other hand, the fixed-<inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should roughly match the adaptive-<inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> model at the beginning and then become worse over time, but these differences should stabilize after the end of the adaptation period. The results of these analyses support our hypotheses (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), thus providing further evidence of adaptation, highlighting the fact that the DbS model can parsimoniously capture adaptation to contextual changes in a continuous and dynamical manner. Furthermore, we found that the DbS model again provides more accurate qualitative and quantitative out-of-sample predictions than the log model (<xref ref-type="fig" rid="fig5">Figure 5e,f</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The brain is a metabolically expensive inference machine (<xref ref-type="bibr" rid="bib24">Hawkes et al., 1998</xref>; <xref ref-type="bibr" rid="bib35">Navarrete et al., 2011</xref>; <xref ref-type="bibr" rid="bib64">Stone, 2018</xref>). Therefore, it has been suggested that evolutionary pressure has driven it to make productive use of its limited resources by exploiting statistical regularities (<xref ref-type="bibr" rid="bib3">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib4">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib30">Laughlin, 1981</xref>). Here, we incorporate this important — often ignored — aspect in models of behavior by introducing a general framework of decision-making under the constraints that the system: (i) encodes information based on binary codes, (ii) has limited number of samples available to encode information, and (iii) considers the costs of contextual adaptation.</p><p>Under the assumption that the organism has fully adapted to the statistics in a given context, we show that the encoding rule that maximizes mutual information is the same rule that maximizes decision accuracy in two-alternative decision tasks. However, note that there is nothing privileged about maximizing mutual information, as it does not mean that the goals of the organism are necessarily achieved (<xref ref-type="bibr" rid="bib44">Park and Pillow, 2017</xref>; <xref ref-type="bibr" rid="bib54">Salinas, 2006</xref>). In fact, we show that if the goal of the organism is instead to maximize the expected value of the chosen options, the system should not rely on maximizing information transmission and must give up a small fraction of precision in information coding. Here, we derived analytical solution for each of these optimization objective criteria, emphasizing that these analytical solutions were derived for the large-<inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limiting case. However, we have provided evidence that these solutions continue to be more efficient relative to DbS for small values of <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and more importantly, they remain nearly optimal even at relatively low values of <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, in the range of values that might be relevant to explain human experimental data (Appendix 7).</p><p>Another key implication of our results is that we provide an alternative explanation to the usual conception of noise as the main cause of behavioral performance degradation, where noise is usually artificially added to models of decision behavior to generate the desired variability (<xref ref-type="bibr" rid="bib51">Ratcliff and Rouder, 1998</xref>; <xref ref-type="bibr" rid="bib70">Wang, 2002</xref>). On the contrary, our work makes it formally explicit why a system that evolved to encode information based on binary codes must be necessarily noisy, also revealing how the system could take advantage of its unavoidable noisy properties (<xref ref-type="bibr" rid="bib16">Faisal et al., 2008</xref>) to optimize decision behavior (<xref ref-type="bibr" rid="bib67">Tsetsos et al., 2016</xref>). Here, it is important to highlight that this conclusion is drawn from a purely homogeneous neural circuit, in other words, a circuit in which all neurons have the same properties (in our case, the same activation thresholds). This is not what is typically observed, as neural circuits are typically very heterogeneous. However, in the neural circuit that we consider here, it could mean that the firing thresholds can vary across neurons (<xref ref-type="bibr" rid="bib42">Orbán et al., 2016</xref>), which could be used by the system to optimize the required variability of binary neural codes. Interestingly, it has been shown in recent work that stochastic discrete events also serve to optimize information transmission in neural population coding (<xref ref-type="bibr" rid="bib2">Ashida and Kubo, 2010</xref>; <xref ref-type="bibr" rid="bib39">Nikitin et al., 2009</xref>; <xref ref-type="bibr" rid="bib55">Schmerl and McDonnell, 2013</xref>). Crucially, in our work we provide a direct link of the necessity of noise for systems that aim at optimizing decision behavior under our encoding and limited-capacity assumptions, which can be seen as algorithmic specifications of the more realistic population coding specifications mentioned above (<xref ref-type="bibr" rid="bib39">Nikitin et al., 2009</xref>). We argue that our results may provide a formal intuition for the apparent necessity of noise for improving training and learning performance in artificial neural networks (<xref ref-type="bibr" rid="bib13">Dapello et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Findling and Wyart, 2020</xref>), and we speculate that an implementation of 'the right' noise distribution for a given environmental statistic could be seen as a potential mechanism to improve performance in capacity-limited agents generally speaking (<xref ref-type="bibr" rid="bib21">Garrett et al., 2011</xref>). We acknowledge that based on the results of our work, we cannot confirm whether this is the case for higher order neural circuits, however, we leave it as an interesting theoretical formulation, which could be addressed in future work.</p><p>Interestingly, our results could provide an alternative explanation of the recent controversial finding that dynamics of a large proportion of LIP neurons likely reflect binary (discrete) coding states to guide decision behavior (<xref ref-type="bibr" rid="bib29">Latimer et al., 2015</xref>; <xref ref-type="bibr" rid="bib75">Zoltowski et al., 2019</xref>). Based on this potential link between their work and ours, our theoretical framework generates testable predictions that could be investigated in future neurophysiological work. For instance, noise distribution in neural circuits should dynamically adapt according to the prior distribution of inputs and goals of the organism. Consequently, the rate of 'step-like' coding in single neurons should also be dynamically adjusted (perhaps optimally) to statistical regularities and behavioral goals.</p><p>Our results are closely related to Decision by Sampling (DbS), which is an influential account of decision behavior derived from principles of retrieval and memory comparison by taking into account the regularities of the environment, and also encodes information based on binary codes (<xref ref-type="bibr" rid="bib61">Stewart et al., 2006</xref>). We show that DbS represents a special case of our more general efficient sampling framework, that uses a rule that is similar to (though not exactly like) the optimal encoding rule that assumes full (or costless) adaptation to the prior statistics of the environment. In particular, we show that DbS might well be the most efficient sampling algorithm, given that a reduction in the full representation of the prior distribution might not come at a great loss in performance. Interestingly, our experimental results (discussed in more detail below) also provide support for the hypothesis that numerosity perception is efficient in this particular way. Crucially, DbS automatically adjusts the encoding in response to changes in the frequency distribution from which exemplars are drawn in approximately the right way, while providing a simple answer to the question of how such adaptation of the encoding rule to a changing frequency distribution occurs, at a relatively low cost.</p><p>On a related line of work, <xref ref-type="bibr" rid="bib6">Bhui and Gershman, 2018</xref> develop a similar, but different specification of DbS, in which they also consider only a finite number of samples that can be drawn from the prior distribution to generate a percept, and ask what kind of algorithm would be required to improve coding efficiency. However, their implementation differs from ours in various important ways (see Appendix 8 for a detailed discussion). One of the main distinctions is that they consider the case in which only a finite number of samples can be drawn from the prior and show that a variant of DbS with kernel-smoothing is superior to its standard version. However, a key difference to our implementation is that they allow the kernel-smoothed quantity (computed by comparing the input <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with a sample <inline-formula><mml:math id="inf369"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> from the prior distribution) to vary continuously between 0 and 1, rather than having to be either 0 or 1 as in our implementation (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Thus, they show that coding efficiency can be improved by allowing a more flexible implementation of the coding scheme for the case when the agent is allowed to draw few samples from the prior distribution (Appendix 8). On the other hand, we restrict our framework to a coding scheme that is only allowed to encode information based on zeros or ones, where we show that coding efficiency can be improved relative to DbS only under a more complete knowledge of the prior distribution, where the optimal solutions can be formally derived in the large-<inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limit. Nevertheless, we have shown that even under the operation of few sampling units, the optimal rules will be still superior to the standard DbS (if the agent has fully adapted to the statistics of the environment in a given context), even when a few number of processing units are available to generate decision relevant percepts.</p><p>We tested these resource-limited coding frameworks in non-symbolic numerosity discrimination, a fundamental cognitive function for behavior in humans and other animals, which may have emerged during evolution to support fitness maximization (<xref ref-type="bibr" rid="bib36">Nieder, 2020</xref>). Here, we find that the way in which the precision of numerosity discrimination varies with the size of the numbers being compared is consistent with the hypothesis that the internal representations on the basis of which comparisons are made are sample-based. In particular, we find that the encoding rule varies depending on the frequency distribution of values encountered in a given environment, and that this adaptation occurs fairly quickly once the frequency distribution changes.</p><p>This adaptive character of the encoding rule differs, for example, from the common hypothesis of a logarithmic encoding rule (independent of context), which we show fits our data less well. Nonetheless, we can reject the hypothesis of full optimality of the encoding rule for each distribution of values used in our experiments, even after participants have had extensive experience with a given distribution. Thus, a possible explanation of why DbS is the favored model in our numerosity task is that accuracy and reward maximization requires optimal adaptation of the noise distribution based on our imposed prior, requiring complex neuroplastic changes to be implemented, which are in turn metabolically costly (<xref ref-type="bibr" rid="bib10">Buchanan et al., 2013</xref>). Relying on samples from memory might be less metabolically costly as these systems are plastic in short time scales, and therefore a relatively simpler heuristic to implement allowing more efficient adaptation. Here, it is important to emphasize, as it has been discussed in the past (<xref ref-type="bibr" rid="bib65">Tajima et al., 2016</xref>; <xref ref-type="bibr" rid="bib48">Polanía et al., 2015</xref>), that for decision-making systems beyond the perceptual domain, the identity of the samples is unclear. We hypothesize, that information samples derive from the interaction of memory on current sensory evidence depending on the retrieval of relevant samples to make predictions about the outcome of each option for a given behavioral goal (therefore also depending on the encoding rule that optimizes a given behavioral goal).</p><p>Interestingly, it was recently shown that in a reward learning task, a model that estimates values based on memory samples from recent past experiences can explain the data better than canonical incremental learning models (<xref ref-type="bibr" rid="bib7">Bornstein et al., 2017</xref>). Based on their and our findings, we conclude that sampling from memory is an efficient mechanism for guiding choice behavior, as it allows quick learning and generalization of environmental contexts based on recent experience without significantly sacrificing behavioral performance. However, it should be noted that relying on such mechanisms alone might be suboptimal from a performance- and goal-based point of view, where neural calibration of optimal strategies may require extensive experience, possibly via direct interactions between sensory, memory and reward systems (<xref ref-type="bibr" rid="bib22">Gluth et al., 2015</xref>; <xref ref-type="bibr" rid="bib53">Saleem et al., 2018</xref>).</p><p>Taken together, our findings emphasize the need of studying optimal models, which serve as anchors to understand the brain's computational goals without ignoring the fact that biological systems are limited in their capacity to process information. We addressed this by proposing a computational problem, elaborating an algorithmic solution, and proposing a minimalistic implementational architecture that solves the resource-constrained problem. This is essential, as it helps to establish frameworks that allow comparing behavior not only across different tasks and goals, but also across different levels of description, for instance, from single cell operation to observed behavior (<xref ref-type="bibr" rid="bib32">Marr, 1982</xref>). We argue that this approach is fundamental to provide benchmarks for human performance that can lead to the discovery of alternative heuristics (<xref ref-type="bibr" rid="bib50">Qamar et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Gardner, 2019</xref>) that could appear to be in principle suboptimal, but that might be in turn the optimal strategy to implement if one considers cognitive limitations and costs of optimal adaptation. We conclude that the understanding of brain function and behavior under a principled research agenda, which takes into account decision mechanisms that are biologically feasible, will be essential to accelerate the elucidation of the mechanisms underlying human cognition.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>The study tested young healthy volunteers with normal or corrected-to-normal vision (total n = 20, age 19–36 years, nine females: n = 7 in Experiment 1, two females; n = 6 new participants in Experiment 2, three females; n = 7 new participants in Experiment 3, four females). Participants were randomly assigned to each experiment and no participant was excluded from the analyses. Participants were instructed about all aspects of the experiment and gave written informed consent. None of the participants suffered from any neurological or psychological disorder or took medication that interfered with participation in our study. Participants received monetary compensation for their participation in the experiment partially related to behavioral performance (see below). The experiments conformed to the Declaration of Helsinki and the experimental protocol was approved by the Ethics Committee of the Canton of Zurich (BASEC: 2018–00659).</p></sec><sec id="s4-2"><title>Experiment 1</title><p>Participants (n = 7) carried out a numerosity discrimination task for four consecutive days for approximately one hour per day. Each daily session consisted of a training run followed by 8 runs of 75 trials each. Thus, each participant completed ∼2400 trials across the four days of experiment.</p><p>After a fixation period (1–1.5 s jittered), two clouds of dots (left and right) were presented on the screen for 200 ms. Participants were asked to indicate the side of the screen where they perceived more dots. Their response was kept on the screen for 1 s followed by feedback consisting of the symbolic number of dots in each cloud as well as the monetary gains and opportunity losses of the trial depending on the experimental condition. In the value condition, participants were explicitly informed that each dot in a cloud of dots corresponded to 1 Swiss Franc (CHF). Participants were informed that they would receive the amount in CHF corresponding to the total number of dots on the chosen side. At the end of the experiment a random trial was selected and they received the corresponding amount. In the accuracy condition, participants were explicitly informed that they could receive a fixed reward (15 Swiss Francs (CHF)) for each correct trial. This fixed amount was selected such that it approximately matched the expected reward received in the value condition (as tested in pilot experiments). At the end of the experiment, a random trial was selected and they would receive this fixed amount if they chose the cloud with more dots (i.e., the correct side). Each condition lasted for two consecutive days with the starting condition randomized across participants. Only after completing all four experiment days, participants were compensated for their time with 20 CHF per hour, in addition to the money obtained based on their decisions on each experimental day.</p></sec><sec id="s4-3"><title>Experiment 2</title><p>Participants (n = 6) carried out a numerosity discrimination task in which each of four daily sessions consisted of 16 runs of 40 trials each, thus each participant completed ∼2560 trials. A key difference with respect to Experiment 1 is that participants had to accumulate points based on their decisions and had to reach a predetermined threshold on each run. The rules of point accumulation depended on the experimental condition. In the perceptual condition, a fixed amount of points was awarded if the participants chose the cloud with more dots. In this condition, participants were instructed to accumulate a number of points and reach a threshold given a limited number of trials. Based on the results obtained in Experiment 1, the threshold corresponded to 85% of correct trials in a given run, however the participants were unaware of this. If the participants reached this threshold, they were eligible for a fixed reward (20 CHF) as described in Experiment 1. In the value condition, the number of points received was equal to the number of dots in the cloud, however, contrary to Experiment 1, points were only awarded if the participant chose the cloud with the most dots. Participants had to reach a threshold that was matched in the expected collection of points of the perceptual condition. As in Experiment 1, each condition lasted for two consecutive days with the starting condition randomized across participants. Only after completing all the four days of the experiment, participants were compensated for their time with 20 CHF per hour, in addition to the money obtained based on their decisions on each experimental day.</p></sec><sec id="s4-4"><title>Experiment 3</title><p>The design of Experiment 3 was similar to the value condition of Experiment 2 (n = 7 participants) and was carried out over three consecutive days. The key difference between Experiment 3 and Experiments 1–2 was the shape of the prior distribution <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that was used to draw the number of dots for each cloud in each trial (see below).</p></sec><sec id="s4-5"><title>Stimuli statistics and trial selection</title><p>For all experiments, we used the following parametric form of the prior distribution<disp-formula id="equ12"><label>(10)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>initially defined in the interval [0,1] for mathematical tractability in the analytical solution of the encoding rules <inline-formula><mml:math id="inf372"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see below), with <inline-formula><mml:math id="inf373"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> determining the shape of the distribution, and <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a normalizing constant. For Experiments 1 and 2 the shape parameter was set to <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, and for Experiment 3 was set to <inline-formula><mml:math id="inf376"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. i.i.d. samples drawn from this distribution were then multiplied by 50, added an offset of 5, and finally were rounded to the closest integer (i.e., the numerosity values in our experiment ranged from <inline-formula><mml:math id="inf377"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf378"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:math></inline-formula>). The pairs of dots on each trial were determined by sampling from a uniform density window in the CDF space (<xref ref-type="disp-formula" rid="equ12">Equation 10</xref> is its corresponding PDF). The pairs of dots in each trial were selected with the conditions that, first, their distance in the CDF space was less than a constant (0.25, 0.28 and 0.23 for Experiments 1, 2 and 3 respectively), and second, the number of dots in both clouds was different. <xref ref-type="fig" rid="fig3">Figure 3c</xref> illustrates the probability that a pair of choice alternatives was selected for a given trial in Experiments 1 and 2.</p></sec><sec id="s4-6"><title>Power analyses and model recovery</title><p>Given that adaptation dynamics in sensory systems often require long-term experience with novel prior distributions, we opted for maximizing the number of trials for a relatively small number of participants per experiment, as it is commonly done for this type of psychophysical experiments (<xref ref-type="bibr" rid="bib8">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Stocker and Simoncelli, 2006</xref>; <xref ref-type="bibr" rid="bib76">Zylberberg et al., 2018</xref>). Note that based on the power analyses described below, we collected in total ∼45,000 trials across the three Experiments, which is above the average number of trials typically collected in human studies.</p><p>In order to maximize statistical power in the differentiation of the competing encoding rules, we generated 10,000 sets of experimental trials for each encoding rule and selected the sets of trials with the highest discrimination power (i.e., largest differences in Log-Likelihood) between the encoding models. In these power analyses, we also investigated what was the minimum number of trials that would allow accurate generative model selection at the individual level. We found that ∼1000 trials per participant in each experimental condition would be sufficient to predict accurately (P&gt;0.95) the true generative model. Based on these analyses, we decided to collect at least 1200 trials per participant and condition (perceptual and value) in each of the three experiments. Model recovery analyses presented in <xref ref-type="fig" rid="fig3">Figure 3d</xref> illustrate the result of our power analyses (see also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec id="s4-7"><title>Apparatus</title><p>Eyetracking (EyeLink 1000 Plus) was used to check the participants' fixation during stimulus presentation. When participants blinked or moved their gaze (more than 2° of visual angle) away from the fixation cross during the stimulus presentation, the trial was canceled (only 212 out of 45,600 trials were canceled, that is, &lt;0.5% of the trials). Participants were informed when a trial was canceled and were encouraged not to do so as they would not receive any reward for this trial. A chinrest was used to keep the distance between the participants and the screen constant (55 cm). The task was run using Psychtoolbox Version 3.0.14 on Matlab 2018a. The diameter of the dots varied between 0.42° and 1.45° of visual angle. The center of each cloud was positioned 12.6° of visual angle horizontally from the fixation cross and had a maximum diameter of 19.6° of visual angle. Following previous numerosity experiments (<xref ref-type="bibr" rid="bib68">van den Berg et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Izard and Dehaene, 2008</xref>), either the average dot size or the total area covered by the dots was maintained constant in both clouds for each trial. The color of each dot (white or black) was randomly selected for each dot. Stimuli sets were different for each participant but identical between the two conditions.</p></sec><sec id="s4-8"><title>Encoding rules and model fits</title><p>The parametrization of the prior <inline-formula><mml:math id="inf379"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ12">Equation 10</xref>) allows tractable analytical solutions of the encoding rules <inline-formula><mml:math id="inf380"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf381"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf382"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that correspond to Accuracy maximization, Reward maximization, and DbS, respectively:<disp-formula id="equ13"><label>(11)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(12)</label><mml:math id="m14"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><label>(13)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Graphical representation of the respective encoding rules is shown in <xref ref-type="fig" rid="fig3">Figure 3e</xref> for Experiments 1 and 2. Given an encoding rule <inline-formula><mml:math id="inf383"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we now define the decision rule. The goal of the decision maker in our task is always to decide which of two input values <inline-formula><mml:math id="inf384"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf385"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is larger. Therefore, the agent choses <inline-formula><mml:math id="inf386"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> if and only if the internal readings <inline-formula><mml:math id="inf387"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. Following the definitions of expected value and variance of binomial variables, and approximating for large <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Appendix 2), the probability of choosing <inline-formula><mml:math id="inf389"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is given by<disp-formula id="equ16"><label>(14)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mtext>choose </mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf390"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard CDF, and <inline-formula><mml:math id="inf391"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf392"><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are the encoding rules for the input values <inline-formula><mml:math id="inf393"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf394"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, respectively. Thus, the choice structure is the same for all models, only differing in their encoding rule. The three models generate different qualitative performance predictions for a given number of samples <inline-formula><mml:math id="inf395"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3f</xref>).</p><p>Crucially, this probability decision rule (<xref ref-type="disp-formula" rid="equ16">Equation 14</xref>) can be parsimoniously extended to include potential side biases independent of the encoding process as follows<disp-formula id="equ17"><label>(15)</label><mml:math id="m17"><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mtext>choose </mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf396"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is the bias term. This is the base model used in our work. We were also interested in studying whether choice history effects (<xref ref-type="bibr" rid="bib1">Abrahamyan et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Talluri et al., 2018</xref>) may have influence in our task, thus possibly affecting the conclusions that can be drawn from the base model. Therefore, we extended this model to incorporate the effect of decision learning and choices from the previous trial<disp-formula id="equ18"><label>(16)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mtext>choose </mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi>Ch</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf397"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the choice made on the previous trial (+1 for left choice and −1 for right choice) and <inline-formula><mml:math id="inf398"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the 'outcome learning' on the previous trial (+1 for correct choice and −1 for incorrect choice). <inline-formula><mml:math id="inf399"><mml:msup><mml:mi>β</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf400"><mml:msup><mml:mi>β</mml:mi><mml:mi>Ch</mml:mi></mml:msup></mml:math></inline-formula> capture the effect of decision learning and choice in the previous trial, respectively.</p><p>Given that the choice structure is the same for all three sampling models considered here, we can naturally address the question of what decision rule the participants favor via a latent-mixture model. We implemented this model based on a hierarchical Bayesian modelling (HBM) approach. The base-rate probabilities for the three different encoding rules at the population level are represented by the vector <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf402"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">π</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the probability of selecting encoding rule model <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We initialize the model with an uninformative prior given by<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝝅</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Dirichlet</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This base-rate is updated based on the empirical data, where we allow each participant <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to draw from each model categorically based on the updated base-rate<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Categorical</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">𝝅</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the encoding rule <inline-formula><mml:math id="inf405"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for model <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The selected rule was then fed into <xref ref-type="disp-formula" rid="equ17 equ18">Equations 15 and 16</xref> to determine the probability of selecting a cloud of dots. The number of samples <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was also estimated within the same HBM with population mean <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>μ</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf410"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> initialized based on uninformative priors with plausible ranges<disp-formula id="equ22"><mml:math id="m22"><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>allowing each participant <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to draw from this population prior assuming that <inline-formula><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is normally distributed at the population level<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Normal</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Similarly, the latent variables <inline-formula><mml:math id="inf413"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in equations <xref ref-type="disp-formula" rid="equ17 equ18">Equations 15 and 16</xref> were estimated by setting population mean <inline-formula><mml:math id="inf414"><mml:msub><mml:mi>μ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf415"><mml:msub><mml:mi>σ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:math></inline-formula> initialized based on uninformative priors<disp-formula id="equ24"><mml:math id="m24"><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>allowing each participant <inline-formula><mml:math id="inf416"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to draw from this population prior assuming that <inline-formula><mml:math id="inf417"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is normally distributed at the population level<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Normal</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In all the results reported in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>, the value of the shape parameter of the prior was set to its true value <inline-formula><mml:math id="inf418"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The estimation of <inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5a</xref> was investigated with a similar hierarchical approach, allowing each participant to sample from the normal population distribution with uninformative priors over the population mean and standard deviation<disp-formula id="equ26"><mml:math id="m26"><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Uniform</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.0001</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The choice rule of the standard logarithmic model of numerosity discrimination is given by<disp-formula id="equ27"><label>(17)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mtext>choose </mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the internal noise in the logarithmic space. This model was extended to incorporate bias and choice history effects in the same way as implemented in the sampling models. Here, we emphasize that all sampling and log models have the same degrees of freedom, where performance is mainly determined by <inline-formula><mml:math id="inf421"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the sampling models and Weber’s fraction <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the log model, and biases are determined by parameters <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For all above-mentioned models, the trial-by-trial likelihood of the observed choice (i.e., the data) given probability of a decision was based on a Bernoulli process<disp-formula id="equ28"><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mtext>choose </mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf424"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the decision of each participant <inline-formula><mml:math id="inf425"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in each trial <inline-formula><mml:math id="inf426"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In order to allow for prior adaptation, the model fits presented in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref> were fit starting after a fourth of the daily trials (corresponding to 150 trials for Experiment 1 and 160 trials for Experiment 2) to allow for prior adaptation and fixing the shape parameter to its true generative value <inline-formula><mml:math id="inf427"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The dynamics of adaptation (<xref ref-type="fig" rid="fig5">Figure 5</xref>) were studied by allowing the shape parameter α to evolve through trial experience using all trials collected on each experiment day. This was studied using the following function<disp-formula id="equ29"><label>(18)</label><mml:math id="m29"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> represents a possible target adaptation value of <inline-formula><mml:math id="inf429"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the trial number, and <inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> determine the shape of the adaptation. Therefore, the encoding rule of the DbS model also changed trial-to-trial<disp-formula id="equ30"><label>(19)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Adaptation was tested based on the hypothesis that participants initially use a logarithmic discrimination rule (<xref ref-type="disp-formula" rid="equ27">Equation 17</xref>) (this strategy also allowed improving identification of the adaptation dynamics). Therefore, <xref ref-type="disp-formula" rid="equ29">Equation 18</xref> was parametrized such that the initial value of the shape parameter (<inline-formula><mml:math id="inf433"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) guaranteed that discriminability between the DbS and the logarithmic rule was as close as possible. This was achieved by finding the value of α in the DbS encoding rule (<inline-formula><mml:math id="inf434"><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></inline-formula>) that minimizes the following expression<disp-formula id="equ31"><label>(20)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf435"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf436"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the numerosity inputs for each trial <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This expression was minimized based on all trials generated in Experiments 1–3 (note that minimizing this expression does not require knowledge of the sensitivity levels <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for the log and DbS models, respectively). We found that the shape parameter value that minimizes <xref ref-type="disp-formula" rid="equ31">Equation 20</xref> is <inline-formula><mml:math id="inf440"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2.58</mml:mn></mml:mrow></mml:math></inline-formula>. Based on our prior <inline-formula><mml:math id="inf441"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> parametrization (<xref ref-type="disp-formula" rid="equ12">Equation 10</xref>), this suggests that the initial prior is more skewed than the priors used in Experiments 1–3 (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This is an expected result given that log-normal priors, typically assumed in numerosity tasks, are also highly skewed. We fitted the <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter independently for Experiments 1–2 and Experiment 3 but kept the <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter shared across all experiments. If adaptation is taking place, we hypothesized that the asymptotic value <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of the shape parameter <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should be larger for Experiments 1–2 compared to Experiment 3.</p><p>Posterior inference of the parameters in all the hierarchical models described above was performed via the Gibbs sampler using the Markov Chain Monte Carlo (MCMC) technique implemented in JAGS. For each model, a total of 50,000 samples were drawn from an initial burn-in step and subsequently a total of new 50,000 samples were drawn for each of three chains (samples for each chain were generated based on a different random number generator engine, and each with a different seed). We applied a thinning of 50 to this final sample, thus resulting in a final set of 1000 samples for each chain (for a total of 3000 pooling all three chains). We conducted Gelman–Rubin tests for each parameter to confirm convergence of the chains. All latent variables in our Bayesian models had <inline-formula><mml:math id="inf446"><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&lt;</mml:mo><mml:mn>1.05</mml:mn></mml:mrow></mml:math></inline-formula>, which suggests that all three chains converged to a target posterior distribution. We checked via visual inspection that the posterior population level distributions of the final MCMC chains converged to our assumed parametrizations. When evaluating different models, we are interested in the model's predictive accuracy for unobserved data, thus it is important to choose a metric for model comparison that considers this predictive aspect. Therefore, in order to perform model comparison, we used a method for approximating leave-one-out cross-validation (LOO) that uses samples from the full posterior (<xref ref-type="bibr" rid="bib69">Vehtari et al., 2017</xref>). These analyses were repeated using an alternative Bayesian metric: the WAIC (<xref ref-type="bibr" rid="bib69">Vehtari et al., 2017</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by an ERC starting grant (ENTRAINER) to RP and by a grant of the U.S. National Science Foundation to M.W. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 758604).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Investigation, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent, and consent to publish, was obtained. The experiments conformed to the Declaration of Helsinki and the experimental protocol was approved by the Ethics Committee of the Canton of Zurich (BASEC: 2018-00659).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-54962-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and essential code that support the findings of this study have been made available at the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/xgfu9/">https://osf.io/xgfu9/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Heng</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Efficient sampling and noisy decisions</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/xgfu9/">xgfu9</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrahamyan</surname> <given-names>A</given-names></name><name><surname>Silva</surname> <given-names>LL</given-names></name><name><surname>Dakin</surname> <given-names>SC</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Gardner</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adaptable history biases in human perceptual decisions</article-title><source>PNAS</source><volume>113</volume><fpage>E3548</fpage><lpage>E3557</lpage><pub-id pub-id-type="doi">10.1073/pnas.1518786113</pub-id><pub-id pub-id-type="pmid">27330086</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashida</surname> <given-names>G</given-names></name><name><surname>Kubo</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Suprathreshold stochastic resonance induced by ion channel fluctuation</article-title><source>Physica D: Nonlinear Phenomena</source><volume>239</volume><fpage>327</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1016/j.physd.2009.12.002</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><volume>61</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1037/h0054663</pub-id><pub-id pub-id-type="pmid">13167245</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Possible principles underlying the transformations of sensory messages</article-title><source>Sensory Communication</source><volume>1</volume><fpage>217</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Redundancy reduction revisited</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><fpage>241</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1080/net.12.3.241.253</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhui</surname> <given-names>R</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decision by sampling implements efficient coding of psychoeconomic functions</article-title><source>Psychological Review</source><volume>125</volume><fpage>985</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1037/rev0000123</pub-id><pub-id pub-id-type="pmid">30431303</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornstein</surname> <given-names>AM</given-names></name><name><surname>Khaw</surname> <given-names>MW</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reminders of past choices bias decisions for reward in humans</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15958</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15958</pub-id><pub-id pub-id-type="pmid">28653668</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname> <given-names>BW</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title><source>Science</source><volume>340</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1126/science.1233912</pub-id><pub-id pub-id-type="pmid">23559254</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brus</surname> <given-names>J</given-names></name><name><surname>Heng</surname> <given-names>JA</given-names></name><name><surname>Polanía</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Weber's Law: A mechanistic foundation after two centuries</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>906</fpage><lpage>908</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.09.001</pub-id><pub-id pub-id-type="pmid">31629634</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchanan</surname> <given-names>KL</given-names></name><name><surname>Grindstaff</surname> <given-names>JL</given-names></name><name><surname>Pravosudov</surname> <given-names>VV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Condition dependence, developmental plasticity, and cognition: implications for ecology and evolution</article-title><source>Trends in Ecology &amp; Evolution</source><volume>28</volume><fpage>290</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2013.02.004</pub-id><pub-id pub-id-type="pmid">23518414</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butterworth</surname> <given-names>B</given-names></name><name><surname>Gallistel</surname> <given-names>CR</given-names></name><name><surname>Vallortigara</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Introduction: the origins of numerical abilities</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>373</volume><elocation-id>20160507</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0507</pub-id><pub-id pub-id-type="pmid">29292355</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>BS</given-names></name><name><surname>Barron</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Jeffreys' prior is asymptotically least favorable under entropy risk</article-title><source>Journal of Statistical Planning and Inference</source><volume>41</volume><fpage>37</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/0378-3758(94)90153-8</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dapello</surname> <given-names>J</given-names></name><name><surname>Marques</surname> <given-names>T</given-names></name><name><surname>Schrimpf</surname> <given-names>M</given-names></name><name><surname>Geiger</surname> <given-names>F</given-names></name><name><surname>Cox</surname> <given-names>DD</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simulating a primary visual cortex at the front of CNNs improves robustness to image perturbations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.16.154542</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The neural basis of the Weber-Fechner law: a logarithmic mental number line</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>145</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00055-X</pub-id><pub-id pub-id-type="pmid">12691758</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dutilh</surname> <given-names>G</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparing perceptual and preferential decision making</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>723</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0941-1</pub-id><pub-id pub-id-type="pmid">26432714</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname> <given-names>AA</given-names></name><name><surname>Selen</surname> <given-names>LP</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Noise in the nervous system</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id><pub-id pub-id-type="pmid">18319728</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Findling</surname> <given-names>C</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computation noise promotes cognitive resilience to adverse conditions during decision-making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.10.145300</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname> <given-names>D</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Efficient sensory encoding and bayesian inference with heterogeneous neural populations</article-title><source>Neural Computation</source><volume>26</volume><fpage>2103</fpage><lpage>2134</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id><pub-id pub-id-type="pmid">25058702</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ganguli</surname> <given-names>D</given-names></name><name><surname>Simoncelli</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural and perceptual signatures of efficient sensory coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.00058">https://arxiv.org/abs/1603.00058</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimality and heuristics in perceptual neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>514</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0340-4</pub-id><pub-id pub-id-type="pmid">30804531</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname> <given-names>DD</given-names></name><name><surname>Kovacevic</surname> <given-names>N</given-names></name><name><surname>McIntosh</surname> <given-names>AR</given-names></name><name><surname>Grady</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The importance of being variable</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>4496</fpage><lpage>4503</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5641-10.2011</pub-id><pub-id pub-id-type="pmid">21430150</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Sommer</surname> <given-names>T</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Effective connectivity between Hippocampus and ventromedial prefrontal cortex controls preferential choices from memory</article-title><source>Neuron</source><volume>86</volume><fpage>1078</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.04.023</pub-id><pub-id pub-id-type="pmid">25996135</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grueschow</surname> <given-names>M</given-names></name><name><surname>Polania</surname> <given-names>R</given-names></name><name><surname>Hare</surname> <given-names>TA</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatic versus Choice-Dependent value representations in the human brain</article-title><source>Neuron</source><volume>85</volume><fpage>874</fpage><lpage>885</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.054</pub-id><pub-id pub-id-type="pmid">25640078</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkes</surname> <given-names>K</given-names></name><name><surname>O'Connell</surname> <given-names>JF</given-names></name><name><surname>Jones</surname> <given-names>NG</given-names></name><name><surname>Alvarez</surname> <given-names>H</given-names></name><name><surname>Charnov</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Grandmothering, menopause, and the evolution of human life histories</article-title><source>PNAS</source><volume>95</volume><fpage>1336</fpage><lpage>1339</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.1336</pub-id><pub-id pub-id-type="pmid">9448332</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izard</surname> <given-names>V</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Calibrating the mental number line</article-title><source>Cognition</source><volume>106</volume><fpage>1221</fpage><lpage>1247</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2007.06.004</pub-id><pub-id pub-id-type="pmid">17678639</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keung</surname> <given-names>W</given-names></name><name><surname>Hagen</surname> <given-names>TA</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Regulation of evidence accumulation by pupil-linked arousal processes</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>636</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0551-4</pub-id><pub-id pub-id-type="pmid">31190022</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaw</surname> <given-names>MW</given-names></name><name><surname>Li</surname> <given-names>Z</given-names></name><name><surname>Woodford</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cognitive imprecision and Small-Stakes risk aversion</article-title><source>The Review of Economic Studies</source><elocation-id>rdaa044</elocation-id><pub-id pub-id-type="doi">10.1093/restud/rdaa044</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolling</surname> <given-names>N</given-names></name><name><surname>Wittmann</surname> <given-names>M</given-names></name><name><surname>Rushworth</surname> <given-names>MFS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multiple neural mechanisms of decision making and their competition under changing risk pressure</article-title><source>Neuron</source><volume>81</volume><fpage>1190</fpage><lpage>1202</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.033</pub-id><pub-id pub-id-type="pmid">24607236</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latimer</surname> <given-names>KW</given-names></name><name><surname>Yates</surname> <given-names>JL</given-names></name><name><surname>Meister</surname> <given-names>ML</given-names></name><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>NEURONAL MODELING. Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title><source>Science</source><volume>349</volume><fpage>184</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1126/science.aaa4056</pub-id><pub-id pub-id-type="pmid">26160947</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A simple coding procedure enhances a neuron's Information Capacity</article-title><source>Zeitschrift Für Naturforschung C</source><volume>36</volume><fpage>910</fpage><lpage>912</lpage><pub-id pub-id-type="doi">10.1515/znc-1981-9-1040</pub-id><pub-id pub-id-type="pmid">7303823</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname> <given-names>K</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient coding and the neural representation of value</article-title><source>Annals of the New York Academy of Sciences</source><volume>1251</volume><fpage>13</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2012.06496.x</pub-id><pub-id pub-id-type="pmid">22694213</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation Into the Human Representation and Processing of Visual Information</source><publisher-name>WH Freeman and Company</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonnell</surname> <given-names>MD</given-names></name><name><surname>Stocks</surname> <given-names>NG</given-names></name><name><surname>Abbott</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Optimal stimulus and noise distributions for information transmission via suprathreshold stochastic resonance</article-title><source>Physical Review E</source><volume>75</volume><elocation-id>061105</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.75.061105</pub-id><pub-id pub-id-type="pmid">17677218</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Młynarski</surname> <given-names>W</given-names></name><name><surname>Hermundstad</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptability and efficiency in neural coding</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/669200</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarrete</surname> <given-names>A</given-names></name><name><surname>van Schaik</surname> <given-names>CP</given-names></name><name><surname>Isler</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Energetics and the evolution of human brain size</article-title><source>Nature</source><volume>480</volume><fpage>91</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1038/nature10629</pub-id><pub-id pub-id-type="pmid">22080949</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The adaptive value of numerical competence</article-title><source>Trends in Ecology &amp; Evolution</source><volume>35</volume><fpage>605</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2020.02.009</pub-id><pub-id pub-id-type="pmid">32521244</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname> <given-names>A</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of number in the brain</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>185</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135550</pub-id><pub-id pub-id-type="pmid">19400715</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname> <given-names>A</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Coding of cognitive magnitude: compressed scaling of numerical information in the primate prefrontal cortex</article-title><source>Neuron</source><volume>37</volume><fpage>149</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01144-3</pub-id><pub-id pub-id-type="pmid">12526780</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikitin</surname> <given-names>AP</given-names></name><name><surname>Stocks</surname> <given-names>NG</given-names></name><name><surname>Morse</surname> <given-names>RP</given-names></name><name><surname>McDonnell</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural population coding is optimized by discrete tuning curves</article-title><source>Physical Review Letters</source><volume>103</volume><elocation-id>138101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.103.138101</pub-id><pub-id pub-id-type="pmid">19905542</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niven</surname> <given-names>JE</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Energy limitation as a selective pressure on the evolution of sensory systems</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1792</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1242/jeb.017574</pub-id><pub-id pub-id-type="pmid">18490395</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Toward a theory of memory and attention</article-title><source>Psychological Review</source><volume>75</volume><fpage>522</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1037/h0026699</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orbán</surname> <given-names>G</given-names></name><name><surname>Berkes</surname> <given-names>P</given-names></name><name><surname>Fiser</surname> <given-names>J</given-names></name><name><surname>Lengyel</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural variability and Sampling-Based probabilistic representations in the visual cortex</article-title><source>Neuron</source><volume>92</volume><fpage>530</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.038</pub-id><pub-id pub-id-type="pmid">27764674</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pardo-Vazquez</surname> <given-names>JL</given-names></name><name><surname>Castiñeiras-de Saa</surname> <given-names>JR</given-names></name><name><surname>Valente</surname> <given-names>M</given-names></name><name><surname>Damião</surname> <given-names>I</given-names></name><name><surname>Costa</surname> <given-names>T</given-names></name><name><surname>Vicente</surname> <given-names>MI</given-names></name><name><surname>Mendonça</surname> <given-names>AG</given-names></name><name><surname>Mainen</surname> <given-names>ZF</given-names></name><name><surname>Renart</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The mechanistic foundation of Weber's law</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0439-7</pub-id><pub-id pub-id-type="pmid">31406366</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>I</given-names></name><name><surname>Pillow</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bayesian efficient coding</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/178418</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piazza</surname> <given-names>M</given-names></name><name><surname>Pinel</surname> <given-names>P</given-names></name><name><surname>Le Bihan</surname> <given-names>D</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A magnitude code common to numerosities and number symbols in human intraparietal cortex</article-title><source>Neuron</source><volume>53</volume><fpage>293</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.11.022</pub-id><pub-id pub-id-type="pmid">17224409</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pirrone</surname> <given-names>A</given-names></name><name><surname>Stafford</surname> <given-names>T</given-names></name><name><surname>Marshall</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>When natural selection should optimize speed-accuracy trade-offs</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>73</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00073</pub-id><pub-id pub-id-type="pmid">24782703</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polanía</surname> <given-names>R</given-names></name><name><surname>Krajbich</surname> <given-names>I</given-names></name><name><surname>Grueschow</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural oscillations and synchronization differentially support evidence accumulation in perceptual and value-based decision making</article-title><source>Neuron</source><volume>82</volume><fpage>709</fpage><lpage>720</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.03.014</pub-id><pub-id pub-id-type="pmid">24811387</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polanía</surname> <given-names>R</given-names></name><name><surname>Moisa</surname> <given-names>M</given-names></name><name><surname>Opitz</surname> <given-names>A</given-names></name><name><surname>Grueschow</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The precision of value-based choices depends causally on fronto-parietal phase coupling</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8090</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9090</pub-id><pub-id pub-id-type="pmid">26290482</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polanía</surname> <given-names>R</given-names></name><name><surname>Woodford</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Efficient coding of subjective value</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>134</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0292-0</pub-id><pub-id pub-id-type="pmid">30559477</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qamar</surname> <given-names>AT</given-names></name><name><surname>Cotton</surname> <given-names>RJ</given-names></name><name><surname>George</surname> <given-names>RG</given-names></name><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Prezhdo</surname> <given-names>E</given-names></name><name><surname>Laudano</surname> <given-names>A</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Trial-to-trial, uncertainty-based adjustment of decision boundaries in visual categorization</article-title><source>PNAS</source><volume>110</volume><fpage>20332</fpage><lpage>20337</lpage><pub-id pub-id-type="doi">10.1073/pnas.1219756110</pub-id><pub-id pub-id-type="pmid">24272938</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for Two-Choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rustichini</surname> <given-names>A</given-names></name><name><surname>Conen</surname> <given-names>KE</given-names></name><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal coding and neuronal adaptation in economic decisions</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1208</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01373-y</pub-id><pub-id pub-id-type="pmid">29084949</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Diamanti</surname> <given-names>EM</given-names></name><name><surname>Fournier</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Coherent encoding of subjective spatial position in visual cortex and Hippocampus</article-title><source>Nature</source><volume>562</volume><fpage>124</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0516-1</pub-id><pub-id pub-id-type="pmid">30202092</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>How behavioral constraints may determine optimal sensory representations</article-title><source>PLOS Biology</source><volume>4</volume><elocation-id>e387</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0040387</pub-id><pub-id pub-id-type="pmid">17132045</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmerl</surname> <given-names>BA</given-names></name><name><surname>McDonnell</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Channel-noise-induced stochastic facilitation in an auditory brainstem neuron model</article-title><source>Physical Review E</source><volume>88</volume><elocation-id>052722</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.88.052722</pub-id><pub-id pub-id-type="pmid">24329311</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiber</surname> <given-names>S</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Herz</surname> <given-names>AV</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Energy-efficient coding with discrete stochastic events</article-title><source>Neural Computation</source><volume>14</volume><fpage>1323</fpage><lpage>1346</lpage><pub-id pub-id-type="doi">10.1162/089976602753712963</pub-id><pub-id pub-id-type="pmid">12020449</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decision making and sequential sampling from memory</article-title><source>Neuron</source><volume>90</volume><fpage>927</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.036</pub-id><pub-id pub-id-type="pmid">27253447</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname> <given-names>TO</given-names></name><name><surname>Calhoun</surname> <given-names>AJ</given-names></name><name><surname>Chalasani</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information theory of adaptation in neurons, behavior, and mood</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>47</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.11.007</pub-id><pub-id pub-id-type="pmid">24709600</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname> <given-names>TO</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimizing neural information capacity through discretization</article-title><source>Neuron</source><volume>94</volume><fpage>954</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.044</pub-id><pub-id pub-id-type="pmid">28595051</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Penny</surname> <given-names>WD</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Moran</surname> <given-names>RJ</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian model selection for group studies</article-title><source>NeuroImage</source><volume>46</volume><fpage>1004</fpage><lpage>1017</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.03.025</pub-id><pub-id pub-id-type="pmid">19306932</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname> <given-names>N</given-names></name><name><surname>Chater</surname> <given-names>N</given-names></name><name><surname>Brown</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Decision by sampling</article-title><source>Cognitive Psychology</source><volume>53</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2005.10.003</pub-id><pub-id pub-id-type="pmid">16438947</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname> <given-names>AA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Noise characteristics and prior expectations in human visual speed perception</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>578</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/nn1669</pub-id><pub-id pub-id-type="pmid">16547513</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocks</surname> <given-names>NG</given-names></name><name><surname>Allingham</surname> <given-names>D</given-names></name><name><surname>Morse</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The application of suprathreshold stochastic resonance to cochlear implant coding</article-title><source>Fluctuation and Noise Letters</source><volume>02</volume><fpage>L169</fpage><lpage>L181</lpage><pub-id pub-id-type="doi">10.1142/S0219477502000774</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stone</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Principles of Neural Information Theory: Computational Neuroscience and Metabolic Efficiency</source><publisher-name>Sebtel Press</publisher-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname> <given-names>S</given-names></name><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimal policy for value-based decision-making</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12400</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12400</pub-id><pub-id pub-id-type="pmid">27535638</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talluri</surname> <given-names>BC</given-names></name><name><surname>Urai</surname> <given-names>AE</given-names></name><name><surname>Tsetsos</surname> <given-names>K</given-names></name><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>Donner</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Confirmation Bias through selective overweighting of Choice-Consistent evidence</article-title><source>Current Biology</source><volume>28</volume><fpage>3128</fpage><lpage>3135</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.052</pub-id><pub-id pub-id-type="pmid">30220502</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsetsos</surname> <given-names>K</given-names></name><name><surname>Moran</surname> <given-names>R</given-names></name><name><surname>Moreland</surname> <given-names>J</given-names></name><name><surname>Chater</surname> <given-names>N</given-names></name><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Economic irrationality is optimal during noisy decision making</article-title><source>PNAS</source><volume>113</volume><fpage>3102</fpage><lpage>3107</lpage><pub-id pub-id-type="doi">10.1073/pnas.1519157113</pub-id><pub-id pub-id-type="pmid">26929353</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Lindskog</surname> <given-names>M</given-names></name><name><surname>Poom</surname> <given-names>L</given-names></name><name><surname>Winman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recent is more: a negative time-order effect in nonsymbolic numerical judgment</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>43</volume><fpage>1084</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1037/xhp0000387</pub-id><pub-id pub-id-type="pmid">28263625</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Gabry</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title><source>Statistics and Computing</source><volume>27</volume><fpage>1413</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname> <given-names>EU</given-names></name><name><surname>Johnson</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mindful judgment and decision making</article-title><source>Annual Review of Psychology</source><volume>60</volume><fpage>53</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.60.110707.163633</pub-id><pub-id pub-id-type="pmid">18798706</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>XX</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A bayesian observer model constrained by efficient coding can explain 'anti-Bayesian' percepts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1038/nn.4105</pub-id><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>XX</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Lawful relation between perceptual Bias and discriminability</article-title><source>PNAS</source><volume>114</volume><fpage>10244</fpage><lpage>10249</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619153114</pub-id><pub-id pub-id-type="pmid">28874578</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodford</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling imprecision in perception, valuation, and choice</article-title><source>Annual Review of Economics</source><volume>12</volume><fpage>579</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1146/annurev-economics-102819-040518</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoltowski</surname> <given-names>DM</given-names></name><name><surname>Latimer</surname> <given-names>KW</given-names></name><name><surname>Yates</surname> <given-names>JL</given-names></name><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discrete stepping and nonlinear ramping dynamics underlie spiking responses of LIP neurons during Decision-Making</article-title><source>Neuron</source><volume>102</volume><fpage>1249</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.031</pub-id><pub-id pub-id-type="pmid">31130330</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Counterfactual reasoning underlies the learning of priors in decision making</article-title><source>Neuron</source><volume>99</volume><fpage>1083</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.035</pub-id><pub-id pub-id-type="pmid">30122376</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Infomax coding rule</title><p>We assume that the subjective perception of an environmental variable with value <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is determined by <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> independent <italic>samples</italic> of a binary random variable, that is, outcomes are either 'high’ (ones) or 'low’ (zeros) readings. Here, the probability <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of a 'high' reading is the same on each draw, but can depend on the input stimulus value, via the function <inline-formula><mml:math id="inf450"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Additionally, we assume that the input value <inline-formula><mml:math id="inf451"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on a given trial is an independent draw from some prior distribution <inline-formula><mml:math id="inf452"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in a given environment or context (with <inline-formula><mml:math id="inf453"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> being the corresponding cumulative distribution function). As we mentioned before, the choice of <inline-formula><mml:math id="inf454"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e., encoding of the input value) depends on <inline-formula><mml:math id="inf455"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Now suppose that the mapping <inline-formula><mml:math id="inf456"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (the encoding rule) is chosen so as to maximize the mutual information between the random variable <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the subjective value representation <inline-formula><mml:math id="inf458"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The mutual information is computed under the assumption that <inline-formula><mml:math id="inf459"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is drawn from a particular prior distribution <inline-formula><mml:math id="inf460"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf461"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is assumed to be optimized for this prior. The mutual information between <inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf463"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined as<disp-formula id="equ32"><label>(21)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the marginal entropy <inline-formula><mml:math id="inf464"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> quantifies the uncertainty of the marginal response distribution <inline-formula><mml:math id="inf465"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf466"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the average conditional entropy of <inline-formula><mml:math id="inf467"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> given <inline-formula><mml:math id="inf468"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The output distribution is given by<disp-formula id="equ33"><label>(22)</label><mml:math id="m33"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf469"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the input density function. For the encoding framework that we consider here, which is given by the binomial channel, the conditional probability mass function of the output given the input is<disp-formula id="equ34"><label>(23)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac linethickness="0pt"><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo rspace="22.5pt">,</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, we have all the ingredients to write the expression of the mutual information<disp-formula id="equ35"><label>(24)</label><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="1em"/><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We then seek to determine the encoding rule <inline-formula><mml:math id="inf470"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that solves the optimization problem<disp-formula id="equ36"><label>(25)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mrow><mml:mi>find</mml:mi><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It can be shown that for large <inline-formula><mml:math id="inf471"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the mutual information between <inline-formula><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (hence the mutual information between <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is maximized if the prior distribution over <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the Jeffreys prior (<xref ref-type="bibr" rid="bib12">Clarke and Barron, 1994</xref>)<disp-formula id="equ37"><label>(26)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Beta</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>also known as the arcsine distribution. Hence, the mapping <inline-formula><mml:math id="inf477"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> induces a prior distribution over <inline-formula><mml:math id="inf478"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> given by the arcsine distribution. This means that for each <inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the encoding function <inline-formula><mml:math id="inf480"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must be such that<disp-formula id="equ38"><label>(27)</label><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:msqrt><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Solving for <inline-formula><mml:math id="inf481"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> we finally obtain the optimal encoding rule<disp-formula id="equ39"><label>(28)</label><mml:math id="m39"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s9" sec-type="appendix"><title>Accuracy maximization for a known prior distribution</title><p>Here we derive the optimal encoding rule when the criterion to be maximized is the probability of a correct response in a binary comparison task, rather than mutual information as in Appendix 1. As in Appendix 1, we assume that the prior distribution <inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from which stimuli are drawn is known, and that the encoding rule is optimized for this particular distribution. (The case in which we wish the encoding rule to be robust to variations in the distribution from which stimuli are drawn is instead considered in Appendix 6.) Note that the objective assumed here corresponds to maximization of expected reward in the case of a perceptual experiment in which a subject must indicate which of two presented magnitudes is greater, and is rewarded for the number of correct responses. (In Appendix 5, we instead consider the encoding rule that would maximize expected reward if the subject’s reward is proportional to the magnitude selected by their response).</p><p>As above, we assume encoding by a binomial channel. The encoded value (number of ‘high’ readings) is given by <inline-formula><mml:math id="inf483"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is consequently an integer between 0 and <inline-formula><mml:math id="inf484"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is a random variable with a binomial distribution with expected value and variance given by<disp-formula id="equ40"><label>(29)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Suppose that the task of the decision maker is to decide which of two input values <inline-formula><mml:math id="inf485"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf486"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is larger. Assuming that <inline-formula><mml:math id="inf487"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf488"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are encoded independently, then the decision maker choses <inline-formula><mml:math id="inf489"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> if and only if the internal readings <inline-formula><mml:math id="inf490"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (here we may suppose that the probability of choosing stimulus 1 is 0.5 in the event that <inline-formula><mml:math id="inf491"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). Thus, the probability of choosing stimulus 1 is:<disp-formula id="equ41"><label>(30)</label><mml:math id="m41"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In the case of large <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can use a normal approximation to the binomial distribution to obtain<disp-formula id="equ42"><label>(31)</label><mml:math id="m42"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo>-</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and hence the probability of choosing <inline-formula><mml:math id="inf493"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is given by<disp-formula id="equ43"><label>(32)</label><mml:math id="m43"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi mathvariant="normal">P</mml:mi><mml:mi>choose</mml:mi><mml:none/><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:none/></mml:mmultiscripts><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard CDF. Thus the probability of an incorrect choice (i.e., choosing the item with the lower value) is approximately<disp-formula id="equ44"><label>(33)</label><mml:math id="m44"><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>error</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Now, suppose that the encoding rule, together with the prior distribution for <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (the same for both inputs that are independent draws from the prior distribution) results in an ex-ante distribution for <italic>θ</italic> (same for both goods) with density function <inline-formula><mml:math id="inf496"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Then the probability of error is given by<disp-formula id="equ45"><label>(34)</label><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>≈</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mstyle></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Our goal is to evaluate <xref ref-type="disp-formula" rid="equ45">Equation 34</xref> for any choice of the density <inline-formula><mml:math id="inf497"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. First, we fix the value of <inline-formula><mml:math id="inf498"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and integrate over <inline-formula><mml:math id="inf499"><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>:<disp-formula id="equ46"><label>(35)</label><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf500"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the expression above then becomes<disp-formula id="equ47"><label>(36)</label><mml:math id="m47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Then we can integrate over <inline-formula><mml:math id="inf501"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> to obtain:<disp-formula id="equ48"><label>(37)</label><mml:math id="m48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:msqrt><mml:mi>n</mml:mi><mml:mi>π</mml:mi></mml:msqrt></mml:mfrac><mml:mo>∫</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This problem can be solved using the method of Lagrange multipliers:<disp-formula id="equ49"><label>(38)</label><mml:math id="m49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:msqrt><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We now calculate the gradient<disp-formula id="equ50"><label>(39)</label><mml:math id="m50"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>and then find the optimum for <inline-formula><mml:math id="inf502"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> by setting<disp-formula id="equ51"><label>(40)</label><mml:math id="m51"><mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula>then solving for <inline-formula><mml:math id="inf503"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> to obtain<disp-formula id="equ52"><label>(41)</label><mml:math id="m52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msqrt><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Taken into consideration our optimization constraint, it can be shown that<disp-formula id="equ53"><mml:math id="m53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>π</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>and therefore this implies:<disp-formula id="equ54"><mml:math id="m54"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula>thus requiring:<disp-formula id="equ55"><mml:math id="m55"><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Replacing <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ52">Equation 41</xref> we finally obtain<disp-formula id="equ56"><mml:math id="m56"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Thus the optimal encoding rule is the same (at least in the large-<inline-formula><mml:math id="inf505"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limit) in this case as when we assume an objective of maximum mutual information (the case considered in Appendix 1), though here we assume that the objective is accurate performance of a specific discrimination task.</p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s10" sec-type="appendix"><title>Optimal noise for a known prior distribution</title><p>Interestingly, we found that the fundamental principles of the theory independently developed in our work are directly linked to the concept of suprathreshold stochastic resonance (SSR) discovered about two decades ago. Briefly, SSR occurs in an array of <inline-formula><mml:math id="inf506"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> identical threshold non-linearities, each of which is subject to independently sampled random additive noise (<xref ref-type="fig" rid="fig1">Figure 1</xref> in main text). SSR should not be confused with the standard stochastic resonance (SR) phenomenon. In SR, the amplitude of the input signal is restricted to values smaller than the threshold for SR to occur. On the other hand, in SSR, random draws from the distribution of input values can exist above threshold levels. Using the simplified implementational scheme proposed in our work, it can be shown that mutual information <inline-formula><mml:math id="inf507"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be also optimized by finding the optimal noise distribution. This is important as it provides a normative justification as for why sampling must be noisy in capacity-limited systems. Actually, SSR was initially motivated as a model of neural arrays such as those synapsing with hair cells in the inner ear, with the direct application of establishing the mechanisms by which information transmission can be optimized in the design of cochlear implants (<xref ref-type="bibr" rid="bib63">Stocks et al., 2002</xref>). Our goal in this subsection is to make evident the link between the novel theoretical implications of our work and the SSR phenomenon developed in previous work (<xref ref-type="bibr" rid="bib63">Stocks et al., 2002</xref>; <xref ref-type="bibr" rid="bib33">McDonnell et al., 2007</xref>), which should further justify our argument of efficient noisy sampling as a general framework for decision behavior, crucially, with a parsimonious implementational nature.</p><p>Following our notation, each threshold device (we will call it from now on a <italic>neuron</italic>) can be seen as the number of <inline-formula><mml:math id="inf508"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> resources available to encode an input stimulus <inline-formula><mml:math id="inf509"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, we assume that each neuron produces a 'high’ reading if and only if <inline-formula><mml:math id="inf510"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>, where <italic>η</italic> is i.i.d. random additive noise (independent of <inline-formula><mml:math id="inf511"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) following a distribution function <inline-formula><mml:math id="inf512"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf513"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the minimum threshold required to produce a 'high' reading. If we define the noise CDF as <inline-formula><mml:math id="inf514"><mml:msub><mml:mi>F</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula>, then the probability <inline-formula><mml:math id="inf515"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of the neuron giving a 'high' reading in response to the input signal <inline-formula><mml:math id="inf516"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by<disp-formula id="equ57"><label>(42)</label><mml:math id="m57"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It can be shown that the mutual information between the input <inline-formula><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the number of 'high’ readings <inline-formula><mml:math id="inf518"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for large <inline-formula><mml:math id="inf519"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by <xref ref-type="bibr" rid="bib33">McDonnell et al., 2007</xref>,<disp-formula id="equ58"><label>(43)</label><mml:math id="m58"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>KL</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>J</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf520"><mml:msub><mml:mi>f</mml:mi><mml:mi>J</mml:mi></mml:msub></mml:math></inline-formula> is the Jeffreys prior (<xref ref-type="disp-formula" rid="equ37">Equation 26</xref>). Therefore, Jeffreys’ prior can also be derived making it a function of the noise distribution <inline-formula><mml:math id="inf521"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula><disp-formula id="equ59"><label>(44)</label><mml:math id="m59"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>J</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Given that the first term in <xref ref-type="disp-formula" rid="equ58">Equation 43</xref> is always non-negative, a sufficient condition for achieving channel capacity is given by<disp-formula id="equ60"><label>(45)</label><mml:math id="m60"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>J</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Typically, the nervous system of any organism has little influence on the distribution of physical signals in the environment. However, it has the ability to shape its internal signals to optimize information transfer. Therefore, a parsimonious solution that the nervous system may adopt to adapt to statistical regularities of environmental signals in a given context is to find the optimal noise distribution <inline-formula><mml:math id="inf522"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>η</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> to achieve channel capacity. Note that this is different from classical problems in communication theory where the goal is usually to find the signal distribution that maximizes mutual information for a channel. Solving <xref ref-type="disp-formula" rid="equ59">Equation 44</xref> to find <inline-formula><mml:math id="inf523"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> one can find such optimal noise distribution<disp-formula id="equ61"><label>(46)</label><mml:math id="m61"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>η</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>A further interesting consequence of this set of results is that the ratio between the signal PDF <inline-formula><mml:math id="inf524"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the noise PDF <inline-formula><mml:math id="inf525"><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:math></inline-formula> is<disp-formula id="equ62"><label>(47)</label><mml:math id="m62"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>η</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Using the definition given in <xref ref-type="disp-formula" rid="equ57">Equation 42</xref> to make this expression a function of <inline-formula><mml:math id="inf526"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, one finds the optimal PDF of the encoder<disp-formula id="equ63"><label>(48)</label><mml:math id="m63"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which is once again the arcsine distribution (See <xref ref-type="disp-formula" rid="equ2 equ5">Equations 2 and 5</xref> in main text).</p></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s11" sec-type="appendix"><title>Efficient coding and the relation between environmental priors and discrimination</title><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Recently, it was shown that using an efficiency principle for encoding sensory variables, based on population of noisy neurons, it was possible to obtain an explicit relationship between the statistical properties of the environment (the prior) and perceptual discriminability (<xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>).</title><p>The theoretical relation states that discriminability should be inversely proportional to the density of the prior distribution. Interestingly, this relationship holds across several sensory modalities such as (<bold>a</bold>) acoustic frequency, (<bold>b</bold>) local orientation, (<bold>c</bold>) speed (figure adapted with permission from the authors <xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>). Here, we investigate whether this particular relation also emerges in our efficient sampling framework.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-app4-fig1-v1.tif"/></fig><p>We first show that we obtain a prediction of exactly the same kind from our model of encoding using a binary channel, in the case that (i) we assume that the encoding rule is optimized for a single environmental distribution, as in the theory of <xref ref-type="bibr" rid="bib18">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>, and (ii) the objective that is maximized is either mutual information (as in the theory of Ganguli and Simoncelli) or the probability of an accurate binary comparison (as considered in Appendix 2).</p><p>Note that the expected value and variance of a binomial random variable are given by<disp-formula id="equ64"><label>(49)</label><mml:math id="m64"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where we let here <inline-formula><mml:math id="inf527"><mml:mrow><mml:mi>r</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In Appendix 2, we show that if the objective is accuracy maximization, an efficient binomial channel requires that<disp-formula id="equ65"><mml:math id="m65"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, replacing <inline-formula><mml:math id="inf528"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ64">Equation 49</xref> implies the following relations<disp-formula id="equ66"><label>(50)</label><mml:math id="m66"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>sin</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="22.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>sin</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>cos</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where we let here <inline-formula><mml:math id="inf529"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Discrimination thresholds <inline-formula><mml:math id="inf530"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in sensory perception are defined as the ratio between the precision of the representation and the rate of change in the perceived stimulus<disp-formula id="equ67"><label>(51)</label><mml:math id="m67"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>≡</mml:mo><mml:mfrac><mml:msqrt><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Substituting the expressions for expected value and variance in <xref ref-type="disp-formula" rid="equ66">Equation 50</xref> results in<disp-formula id="equ68"><label>(52)</label><mml:math id="m68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>d</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:msup><mml:mi>ω</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus under our theory, this implies<disp-formula id="equ69"><label>(53)</label><mml:math id="m69"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>∝</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This is exactly the relationship derived and tested by <xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>.</p><p>Our model instead predicts a somewhat different relationship if the encoding rule is required to be robust to alternative possible environmental frequency distributions (the case further discussed in Appendix 6). In this case, the robustly optimal encoding rule is DbS, which corresponds to <inline-formula><mml:math id="inf531"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> rather than the relation <xref ref-type="disp-formula" rid="equ69">(53)</xref>. Substituting this into <xref ref-type="disp-formula" rid="equ64 equ67">Equations 49 and 51</xref> yields the prediction<disp-formula id="equ70"><label>(54)</label><mml:math id="m70"><mml:mrow><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>d</mml:mi></mml:mpadded><mml:mo rspace="5.3pt">=</mml:mo><mml:mrow><mml:mfrac><mml:msqrt><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>instead of <xref ref-type="disp-formula" rid="equ68">Equation 52</xref>.</p><p>One interpretation of the experimental support for relation <xref ref-type="disp-formula" rid="equ69">(53)</xref> reviewed by <xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref> could be that in the case of early sensory processing of the kind with which they are concerned, perceptual processing is optimized for a particular environmental frequency distribution (representing the long-run experience of an organism or even of the species), so that the assumptions used in Appendix 2 are the empirically relevant ones. Even so, it is arguable that robustness to changing contextual frequency distributions should be important in the case of higher forms of cognition, so that one might expect prediction of <xref ref-type="disp-formula" rid="equ70">Equation 54</xref> to be more relevant for these cases; and indeed, our experimental results for the case of numerosity discrimination are more consistent with <xref ref-type="disp-formula" rid="equ70">Equation 54</xref> than with <xref ref-type="disp-formula" rid="equ68">Equation 52</xref>.</p><p>One should also note that even in a case where <xref ref-type="disp-formula" rid="equ70">Equation 54</xref> holds, if one measures discrimination thresholds over a subset of the stimulus space, over which there is non-trivial variation in <inline-formula><mml:math id="inf532"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> but <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> does not change very much (because the prior distribution for which the encoding rule is optimized assigns a great deal of probability to magnitudes both higher and lower than those in the experimental data set), then relation (<xref ref-type="disp-formula" rid="equ70">54</xref>) restricted to this subset of the possible values for <inline-formula><mml:math id="inf534"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will imply that relation (<xref ref-type="disp-formula" rid="equ69">53</xref>) should approximately hold. This provides another possible interpretation of the fact that the relation (<xref ref-type="disp-formula" rid="equ69">53</xref>) holds fairly well in the data considered by <xref ref-type="bibr" rid="bib19">Ganguli and Simoncelli, 2016</xref>.</p></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s12" sec-type="appendix"><title>Maximizing expected size of the selected item (fitness maximization)</title><p>We now consider the optimal encoding rule under a different assumed objective, namely, maximizing the expected magnitude of the item selected by the subject’s response (that is, the stimulus judged to be larger by the subject), rather than maximizing the probability of a correct response as in Appendix 2. While in many perceptual experiments, maximizing the probability of a correct response would correspond to maximization of the subject’s expected reward (or at least maximization of a psychological reward to the subject, who is given feedback about the correctness of responses but not about true magnitudes), in many of the ecologically relevant cases in which accurate discrimination of numerosity is useful to an organism (<xref ref-type="bibr" rid="bib11">Butterworth et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Nieder, 2020</xref>), the decision maker’s reward depends on how much larger one number is than another, and not simply their ordinal ranking. This would also be true of typical cases in which internal representations of numerical magnitudes must be used in economic decision making: the reward from choosing an investment with a larger monetary payoff is proportional to the size of the payoff afforded by the option that is chosen. Hence it is of interest to consider the optimal encoding rule if we suppose that encoding is optimized to maximize performance in a decision task with this kind of reward structure.</p><p>As in Appendix 1 and Appendix 2, we again consider the problem of optimizing the encoding rule for a specific prior distribution <inline-formula><mml:math id="inf535"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the magnitudes that may be encountered, and we assume that it is only possible to encode information via ‘high’ or ‘low’ readings. The optimization problem that we need to solve is to find the optimal encoding function <inline-formula><mml:math id="inf536"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that guarantees a maximal expected value of the chosen outcome, for any given prior distribution <inline-formula><mml:math id="inf537"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus the quantity that we seek to maximize is given by<disp-formula id="equ71"><label>(55)</label><mml:math id="m71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf538"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of choosing option <inline-formula><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> when the encoded values of the two options are <inline-formula><mml:math id="inf540"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf541"><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> respectively.</p><p>We begin by noting that for any pair of input values <inline-formula><mml:math id="inf542"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the integrand in <xref ref-type="disp-formula" rid="equ71">Equation 55</xref> can be written as<disp-formula id="equ72"><label>(56)</label><mml:math id="m72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">[</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf543"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the indicator function (taking the value 1 if statement <italic>A</italic> is true, and the value 0 otherwise), and <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mtext>error</mml:mtext></mml:mpadded><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of choosing the lower-valued of the two options.</p><p>Substituting this last expression for the integrand in <xref ref-type="disp-formula" rid="equ71">Equation 55</xref>, we see that we can equivalently write<disp-formula id="equ73"><label>(57)</label><mml:math id="m73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where<disp-formula id="equ74"><label>(58)</label><mml:math id="m74"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt" stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≡</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>is a quantity which is independent of the encoding function <inline-formula><mml:math id="inf545"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Hence choosing <inline-formula><mml:math id="inf546"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to maximize <xref ref-type="disp-formula" rid="equ71">Equation 55</xref> is equivalent to choosing it to minimize<disp-formula id="equ75"><label>(59)</label><mml:math id="m75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As previously specified, the probability of error given two internal noisy readings <inline-formula><mml:math id="inf547"><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf548"><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is given by<disp-formula id="equ76"><mml:math id="m76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(60)</mml:mtext></mml:mtd><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(61)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where in this case we assume that <inline-formula><mml:math id="inf549"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is the lower-valued option and <inline-formula><mml:math id="inf550"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is the higher-valued option on any given trial. This implies that <inline-formula><mml:math id="inf551"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>error</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is very close to zero, except when <inline-formula><mml:math id="inf552"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In this case we have<disp-formula id="equ77"><label>(62)</label><mml:math id="m77"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>error</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:msqrt><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:msqrt><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mi>where</mml:mi></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As in the case of accuracy maximization, here we assume that <inline-formula><mml:math id="inf553"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are independent draws from the same distribution of possible values <inline-formula><mml:math id="inf554"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus <inline-formula><mml:math id="inf555"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Then fixing <inline-formula><mml:math id="inf556"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and integrating over all possible values of <inline-formula><mml:math id="inf557"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ75">Equation 59</xref>, the expected loss is approximately<disp-formula id="equ78"><mml:math id="m78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(63)</mml:mtext></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(64)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(65)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∫</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:msqrt><mml:mfrac><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(66)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(67)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>z</mml:mi><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:munder></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(68)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where in <xref ref-type="disp-formula" rid="equ78">Equation 66</xref> we have applied the change of variable<disp-formula id="equ79"><label>(69)</label><mml:math id="m79"><mml:mrow><mml:mi>z</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and in the integral of <xref ref-type="disp-formula" rid="equ78">Equation 67</xref> we have used<disp-formula id="equ80"><mml:math id="m80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(70)</mml:mtext></mml:mtd><mml:mtd><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>z</mml:mi><mml:mspace width="mediummathspace"/><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(71)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(72)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf558"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the standard normal PDF. Then integrating over <inline-formula><mml:math id="inf559"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, we have:<disp-formula id="equ81"><label>(73)</label><mml:math id="m81"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>loss</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.7pt" stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus we want to find the encoding rule <inline-formula><mml:math id="inf560"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to minimize this integral given the prior <inline-formula><mml:math id="inf561"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We now apply the change of variable <inline-formula><mml:math id="inf562"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:msup><mml:mi>sin</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf563"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an increasing function with a range <inline-formula><mml:math id="inf564"><mml:mrow><mml:mn>0</mml:mn><mml:mo>⩽</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⩽</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf565"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then we have<disp-formula id="equ82"><mml:math id="m82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(74)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(75)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>and therefore we have<disp-formula id="equ83"><label>(76)</label><mml:math id="m83"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This allows us to rewrite <xref ref-type="disp-formula" rid="equ81">Equation 73</xref> as follows<disp-formula id="equ84"><label>(77)</label><mml:math id="m84"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>loss</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Now the problem is to choose the function <inline-formula><mml:math id="inf566"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to minimize <inline-formula><mml:math id="inf567"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>loss</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> subject to <inline-formula><mml:math id="inf568"><mml:mrow><mml:mn>0</mml:mn><mml:mo>⩽</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⩽</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. Equivalently, we can choose the function <inline-formula><mml:math id="inf569"><mml:mrow><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to minimize <inline-formula><mml:math id="inf570"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>loss</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> subject to <inline-formula><mml:math id="inf571"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>⩽</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. Defining <inline-formula><mml:math id="inf572"><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the optimization problem to solve is to choose the function <inline-formula><mml:math id="inf573"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to<disp-formula id="equ85"><label>(78)</label><mml:math id="m85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:mo>∫</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mo>∫</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo>⩽</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Due to FOC, it can be shown that<disp-formula id="equ86"><label>(79)</label><mml:math id="m86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mi>v</mml:mi><mml:mspace width="1em"/><mml:mo stretchy="false">⇒</mml:mo><mml:mspace width="1em"/><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note also that the constraint <inline-formula><mml:math id="inf574"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⩽</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> must hold with equality, thus arriving at<disp-formula id="equ87"><label>(80)</label><mml:math id="m87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Therefore, we finally obtain the efficient encoding rule that maximizes the expected magnitude of the selected item<disp-formula id="equ88"><label>(81)</label><mml:math id="m88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec></boxed-text></app><app id="appendix-6"><title>Appendix 6</title><boxed-text><sec id="s13" sec-type="appendix"><title>Robust optimality of DbS among encoding rules with <inline-formula><mml:math id="inf575"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></title><p>Here we consider the nature of the optimal encoding function when the cost of increasing the size of the sample of values from prior experience that are used to adjust the encoding rule to the contextual distribution of stimulus values is great enough to make it optimal to base the encoding of a new stimulus magnitude <inline-formula><mml:math id="inf576"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on a single sampled value <inline-formula><mml:math id="inf577"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> from the contextual distribution. (The conditions required for this to be the case are discussed further in Appendix 7).</p><p>We assume that for each of the <inline-formula><mml:math id="inf578"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> independent processing units, the probability of a 'high' reading is given by <inline-formula><mml:math id="inf579"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf580"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the draw from the contextual distribution by processor <inline-formula><mml:math id="inf581"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf582"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the same function for each of the processing units. The <inline-formula><mml:math id="inf583"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf584"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent draws from the contextual distribution <inline-formula><mml:math id="inf585"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We further assume that the function <inline-formula><mml:math id="inf586"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> satisfies certain regularity conditions. First, we assume that <inline-formula><mml:math id="inf587"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a piecewise continuous function. That is, we assume that the <inline-formula><mml:math id="inf588"><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> plane can be divided into a countable number of connected regions, with the boundaries between regions defined by continuous curves; and that the function <inline-formula><mml:math id="inf589"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is continuous in the interior of any of these regions, though it may be discontinuous at the boundaries between regions. And second, we assume that <inline-formula><mml:math id="inf590"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is necessarily weakly increasing in <inline-formula><mml:math id="inf591"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and weakly decreasing in <inline-formula><mml:math id="inf592"><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The function is otherwise unrestricted.</p><p>For any prior distribution <inline-formula><mml:math id="inf593"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and any encoding function <inline-formula><mml:math id="inf594"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> we can compute the probability of an erroneous comparison when two stimulus magnitudes <inline-formula><mml:math id="inf595"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are independently drawn from the distribution <inline-formula><mml:math id="inf596"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and each of these stimuli is encoded using <inline-formula><mml:math id="inf597"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> additional independent draws <inline-formula><mml:math id="inf598"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> from the same distribution. Let this error probability be denoted <inline-formula><mml:math id="inf599"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We wish to find an encoding rule (for given <inline-formula><mml:math id="inf600"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) that will make this error probability as small as possible; however, the answer to this question will depend on the prior distribution <inline-formula><mml:math id="inf601"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Hence we wish to find an encoding rule that is robustly optimal, in the sense that it achieves the minimum possible value for the upper bound<disp-formula id="equ89"><mml:math id="m89"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≡</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">sup</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>for the probability of an erroneous comparison. Here, the class of possible priors <inline-formula><mml:math id="inf602"><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:math></inline-formula> to consider is the set of all possible probability distributions (over values of <inline-formula><mml:math id="inf603"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) that can be characterized by an integrable probability density function <inline-formula><mml:math id="inf604"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (We exclude from consideration priors in which there is an atom of probability mass at some single magnitude <inline-formula><mml:math id="inf605"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, since in that case there would be a positive probability of a situation in which it is not clear which response should be considered ‘correct’, so that <inline-formula><mml:math id="inf606"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is not well-defined.) Note that the criterion <inline-formula><mml:math id="inf607"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for ranking encoding rules is not without content, since there exist encoding rules (including DbS) for which the upper bound is less than 1/2 (the error probability in the case of a completely uninformative internal representation).</p><p>Let us consider first the case in which there is some part of the diagonal line along which <inline-formula><mml:math id="inf608"><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> which is not a boundary at which the function <inline-formula><mml:math id="inf609"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is discontinuous. Then we can choose an open interval <inline-formula><mml:math id="inf610"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> such that all values <inline-formula><mml:math id="inf611"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> with the property that both <inline-formula><mml:math id="inf612"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf613"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> lie within the interval <inline-formula><mml:math id="inf614"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are part of a single region on which <inline-formula><mml:math id="inf615"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a continuous function. Then let <inline-formula><mml:math id="inf616"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the greatest lower bound with the property that <inline-formula><mml:math id="inf617"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf618"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> lying within the specified interval, and similarly let <inline-formula><mml:math id="inf619"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the lowest upper bound such that <inline-formula><mml:math id="inf620"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for all values within the specified interval. Because of the continuity of <inline-formula><mml:math id="inf621"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on this region, as the values <inline-formula><mml:math id="inf622"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are chosen to be close enough to each other, the bounds <inline-formula><mml:math id="inf623"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be made arbitrarily close to one another.</p><p>Now for any probabilities <inline-formula><mml:math id="inf624"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>θ</mml:mi><mml:mo>≤</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> let <inline-formula><mml:math id="inf625"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the quantity defined in <xref ref-type="disp-formula" rid="equ41">Equation 30</xref>, when <inline-formula><mml:math id="inf626"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf627"><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:math></inline-formula> that is, for any <inline-formula><mml:math id="inf628"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that are not equal to one another, <inline-formula><mml:math id="inf629"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of an erroneous comparison if the units representing the smaller magnitude each give a 'high' reading with probability <inline-formula><mml:math id="inf630"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and those representing the larger magnitude each give a 'high' reading with probability <inline-formula><mml:math id="inf631"><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Then the probability of erroneous choice <inline-formula><mml:math id="inf632"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> when <inline-formula><mml:math id="inf633"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a distribution with support entirely within the interval <inline-formula><mml:math id="inf634"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is necessarily greater than or equal to the lower bound <inline-formula><mml:math id="inf635"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The reason is that for any <inline-formula><mml:math id="inf636"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> in the support of <inline-formula><mml:math id="inf637"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the probabilities<disp-formula id="equ90"><mml:math id="m90"><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mpadded><mml:mo rspace="5.3pt">=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>will necessarily lie within the bounds <inline-formula><mml:math id="inf638"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for both <inline-formula><mml:math id="inf639"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Given these bounds, the most favorable case for accurate discrimination between the two magnitudes will be to assign the largest possible probability <inline-formula><mml:math id="inf640"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to units being on in the representation of the larger magnitude, and the smallest possible probability <inline-formula><mml:math id="inf641"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to units being on in the representation of the smaller magnitude. Since the lower bound <inline-formula><mml:math id="inf642"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> applies in the case of any individual values <inline-formula><mml:math id="inf643"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> drawn from the support of <inline-formula><mml:math id="inf644"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, this same quantity is also a lower bound for the average error rate integrating over the prior distributions for <inline-formula><mml:math id="inf645"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf646"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>One can also show that as the two bounds <inline-formula><mml:math id="inf647"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> approach one another, the lower bound <inline-formula><mml:math id="inf648"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> approaches 1/2, regardless of the common value that <inline-formula><mml:math id="inf649"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf650"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> both approach. Hence it is possible to make <inline-formula><mml:math id="inf651"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> arbitrarily close to 1/2, by choosing values for <inline-formula><mml:math id="inf652"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that are close enough to one another. It follows that for any bound <inline-formula><mml:math id="inf653"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> less than 1/2 (including values arbitrarily close to 1/2), we can choose a prior distribution <inline-formula><mml:math id="inf654"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for which <inline-formula><mml:math id="inf655"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is necessarily equal to <inline-formula><mml:math id="inf656"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or larger. It follows that in the case of a function <inline-formula><mml:math id="inf657"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of this kind, the upper bound <inline-formula><mml:math id="inf658"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is equal to 1/2.</p><p>In order to achieve an upper bound lower than 1/2, then, we must choose a function <inline-formula><mml:math id="inf659"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that is discontinuous along the entire line <inline-formula><mml:math id="inf660"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. For any such function, let us consider a value <inline-formula><mml:math id="inf661"><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> with the property that all points <inline-formula><mml:math id="inf662"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> near <inline-formula><mml:math id="inf663"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf664"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&gt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> belong to one region on which <inline-formula><mml:math id="inf665"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is continuous, and all points near <inline-formula><mml:math id="inf666"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf667"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> belong to another region. Then under the assumption of piecewise continuity, <inline-formula><mml:math id="inf668"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must approach some value <inline-formula><mml:math id="inf669"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the values <inline-formula><mml:math id="inf670"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> converge to <inline-formula><mml:math id="inf671"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> from within the region where <inline-formula><mml:math id="inf672"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&gt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and similarly <inline-formula><mml:math id="inf673"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must approach some value <inline-formula><mml:math id="inf674"><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the values <inline-formula><mml:math id="inf675"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> converge to <inline-formula><mml:math id="inf676"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> from within the region where <inline-formula><mml:math id="inf677"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>It must also be possible to choose values <inline-formula><mml:math id="inf678"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> such that all points <inline-formula><mml:math id="inf679"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf680"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are points on the boundary between the two regions on which <inline-formula><mml:math id="inf681"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is continuous. Given such values, we can then define bounds <inline-formula><mml:math id="inf682"><mml:mrow><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf683"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> such that<disp-formula id="equ91"><mml:math id="m91"><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.3pt">≤</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≤</mml:mo><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>for all <inline-formula><mml:math id="inf684"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and<disp-formula id="equ92"><mml:math id="m92"><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.3pt">≤</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≤</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>for all <inline-formula><mml:math id="inf685"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>&lt;</mml:mo><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Moreover, piecewise continuity of the function <inline-formula><mml:math id="inf686"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> implies that by choosing both <inline-formula><mml:math id="inf687"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf688"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> close enough to <inline-formula><mml:math id="inf689"><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> we can make the bounds <inline-formula><mml:math id="inf690"><mml:mrow><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> arbitrarily close to <inline-formula><mml:math id="inf691"><mml:mrow><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and make the bounds <inline-formula><mml:math id="inf692"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> arbitrarily close to <inline-formula><mml:math id="inf693"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>Next, for any set of four probabilities <inline-formula><mml:math id="inf694"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>≤</mml:mo><mml:msup><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>′</mml:mo></mml:msup><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf695"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>≤</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> let us define<disp-formula id="equ93"><label>(82)</label><mml:math id="m93"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:msup><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>′</mml:mo></mml:msup><mml:mo rspace="4.2pt">;</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo rspace="5.3pt">≡</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where<disp-formula id="equ94"><label>(83)</label><mml:math id="m94"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≡</mml:mo><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mpadded></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≡</mml:mo><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mpadded></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and <inline-formula><mml:math id="inf696"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are two independent random variables, each distributed uniformly on [0, 1]. Then if <inline-formula><mml:math id="inf697"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> lies between the lower bound <inline-formula><mml:math id="inf698"><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder></mml:math></inline-formula> and upper bound <inline-formula><mml:math id="inf699"><mml:msup><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> whenever <inline-formula><mml:math id="inf700"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, and between the lower bound <inline-formula><mml:math id="inf701"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> and upper bound <inline-formula><mml:math id="inf702"><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> whenever <inline-formula><mml:math id="inf703"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&gt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, then the probability <inline-formula><mml:math id="inf704"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of a processing unit representing the magnitude <inline-formula><mml:math id="inf705"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> giving a 'high' reading will lie between the bounds <inline-formula><mml:math id="inf706"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mi>θ</mml:mi><mml:mo>≤</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf707"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the quantile of <inline-formula><mml:math id="inf708"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> within the prior distribution. It follows that in the case of any two magnitudes <inline-formula><mml:math id="inf709"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf710"><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the probability of an erroneous comparison will be bounded below by <inline-formula><mml:math id="inf711"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf712"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf713"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> since the probability of a correct discrimination will be maximized by making the units representing <inline-formula><mml:math id="inf714"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> give as few high readings as possible and the units representing <inline-formula><mml:math id="inf715"><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> give as many high readings as possible. Integrating over all possible draws of <inline-formula><mml:math id="inf716"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, one finds that the quantity <inline-formula><mml:math id="inf717"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:msup><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>′</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defined in <xref ref-type="disp-formula" rid="equ93">Equation 82</xref> is a lower bound for the overall probability of an erroneous comparison, given that regardless of the prior <inline-formula><mml:math id="inf718"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the quantiles <inline-formula><mml:math id="inf719"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> will be two independent draws from the uniform distribution on <inline-formula><mml:math id="inf720"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>Now consider again an encoding function <inline-formula><mml:math id="inf721"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the kind discussed two paragraphs above, and an interval of stimulus values <inline-formula><mml:math id="inf722"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of the kind discussed there. For any prior distribution <inline-formula><mml:math id="inf723"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with support entirely contained within the interval <inline-formula><mml:math id="inf724"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, the probability of an erroneous comparison is bounded below by<disp-formula id="equ95"><mml:math id="m95"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≥</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo rspace="4.2pt">;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the function <inline-formula><mml:math id="inf725"><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined in <xref ref-type="disp-formula" rid="equ93">Equation 82</xref>. Moreover, by choosing the values <inline-formula><mml:math id="inf726"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> close enough to <inline-formula><mml:math id="inf727"><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, we can make this lower bound arbitrarily close to <inline-formula><mml:math id="inf728"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where for any probabilities <inline-formula><mml:math id="inf729"><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> we define<disp-formula id="equ96"><label>(84)</label><mml:math id="m96"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≡</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo rspace="4.2pt">;</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Hence in the case of the encoding function considered, the upper bound <inline-formula><mml:math id="inf730"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must be at least as large as <inline-formula><mml:math id="inf731"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We further observe that the quantity <inline-formula><mml:math id="inf732"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defined in <xref ref-type="disp-formula" rid="equ96">Equation 84</xref> is just the probability of an erroneous comparison in the case of an encoding rule according to which<disp-formula id="equ97"><mml:math id="m97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:munder><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ98"><mml:math id="m98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that in the case of such an encoding rule, the probability of an erroneous comparison is the same for all prior distributions, since under this rule all that matters is the distribution of the quantile ranks of <inline-formula><mml:math id="inf733"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf734"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula>. It is moreover clear that <inline-formula><mml:math id="inf735"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an increasing function of <inline-formula><mml:math id="inf736"><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder></mml:math></inline-formula> and a decreasing function of <inline-formula><mml:math id="inf737"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> It thus achieves its minimum possible value if and only if <inline-formula><mml:math id="inf738"><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf739"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, in which case it takes the value <inline-formula><mml:math id="inf740"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the probability of erroneous comparison in the case of decision by sampling (again, independent of the prior distribution).</p><p>Thus in the case that there exists any magnitude <inline-formula><mml:math id="inf741"><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> for which <inline-formula><mml:math id="inf742"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover><mml:mi>θ</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> or both, there exist priors <inline-formula><mml:math id="inf743"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for which <inline-formula><mml:math id="inf744"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must exceed <inline-formula><mml:math id="inf745"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Hence in order to minimize the upper bound <inline-formula><mml:math id="inf746"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> it must be the case that <inline-formula><mml:math id="inf747"><mml:mrow><mml:mrow><mml:munder accentunder="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf748"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf749"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. But then our assumption that the encoding rule <inline-formula><mml:math id="inf750"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is at least weakly increasing in <inline-formula><mml:math id="inf751"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and at least weakly decreasing in <inline-formula><mml:math id="inf752"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> requires that<disp-formula id="equ99"><mml:math id="m99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:mn>0</mml:mn><mml:mspace width="2em"/><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mspace width="thickmathspace"/><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ100"><mml:math id="m100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thickmathspace"/><mml:mn>1</mml:mn><mml:mspace width="2em"/><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mspace width="thickmathspace"/><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus the encoding rule must be the DbS rule, the unique rule for which <inline-formula><mml:math id="inf753"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is no greater than <inline-formula><mml:math id="inf754"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p></sec></boxed-text></app><app id="appendix-7"><title>Appendix 7</title><boxed-text><sec id="s14" sec-type="appendix"><title>Sufficient conditions for the optimality of DbS</title><p>Here we consider the general problem of choosing a value of <inline-formula><mml:math id="inf755"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (the number of samples from the contextual distribution <inline-formula><mml:math id="inf756"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to use in encoding any individual stimulus) and an encoding rule <inline-formula><mml:math id="inf757"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be used by each of the <inline-formula><mml:math id="inf758"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> processing units that encode the magnitude of that single stimulus, so as to minimize the compound objective<disp-formula id="equ101"><mml:math id="m101"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf759"><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the upper bound on the probability of an erroneous comparison under the encoding rule <italic>θ</italic>, and <inline-formula><mml:math id="inf760"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the cost of using a sample of size <inline-formula><mml:math id="inf761"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> when encoding each stimulus magnitude. The value of <inline-formula><mml:math id="inf762"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is taken as fixed at some finite value. (This too can be optimized subject to some cost of additional processing units, but we omit formal analysis of this problem.) We assume that <inline-formula><mml:math id="inf763"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an increasing function of <inline-formula><mml:math id="inf764"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and can without loss of generality assume the normalization <inline-formula><mml:math id="inf765"><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> In this optimization problem, we assume that the only encoding functions <inline-formula><mml:math id="inf766"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be considered are ones that are piecewise continuous, at least weakly increasing in <inline-formula><mml:math id="inf767"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and weakly decreasing in each of the <inline-formula><mml:math id="inf768"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>For any value of <inline-formula><mml:math id="inf769"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, let <inline-formula><mml:math id="inf770"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the minimum achievable value for <inline-formula><mml:math id="inf771"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> (Appendix 6 illustrates how this kind of problem can be solved, for the case <inline-formula><mml:math id="inf772"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.) Then the optimal value of <inline-formula><mml:math id="inf773"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will be the one that minimizes <inline-formula><mml:math id="inf774"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>We can establish a lower bound for <inline-formula><mml:math id="inf775"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that holds for any <inline-formula><mml:math id="inf776"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ102"><label>(85)</label><mml:math id="m102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msup><mml:mi>P</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>≡</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix" movablelimits="true">inf</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:munder><mml:mo form="prefix" movablelimits="true">sup</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℱ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≥</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix" movablelimits="true">sup</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℱ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo form="prefix" movablelimits="true">inf</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix" movablelimits="true">sup</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℱ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo form="prefix" movablelimits="true">inf</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>≡</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:munder><mml:mi>P</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mi>n</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the second line, we allow the function <inline-formula><mml:math id="inf777"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be chosen after a particular prior <inline-formula><mml:math id="inf778"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has already been selected, which cannot increase the worst-case error probability. In the third line, we note that the only thing that matters about the encoding function chosen in the second line is the mean value of <inline-formula><mml:math id="inf779"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each possible magnitude <inline-formula><mml:math id="inf780"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, integrating over the possible samples of size <inline-formula><mml:math id="inf781"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that may be drawn from the specified prior; hence we can more simply write the problem on the second line as one involving a direct choice of a function <inline-formula><mml:math id="inf782"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> which may be different depending on the prior <inline-formula><mml:math id="inf783"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that has been chosen. The problem on the third line defines a bound <inline-formula><mml:math id="inf784"><mml:msub><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> that does not depend on <inline-formula><mml:math id="inf785"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>A set of sufficient conditions for <inline-formula><mml:math id="inf786"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to be optimal is then given by the assumptions that</p><list list-type="alpha-lower"><list-item><p><inline-formula><mml:math id="inf787"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and</p></list-item><list-item><p><inline-formula><mml:math id="inf788"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mi>P</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mo>&lt;</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>Condition (a) implies that <inline-formula><mml:math id="inf789"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> will be inferior to <inline-formula><mml:math id="inf790"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>: the cost of a single sample is not so large as to outweigh the reduction in <inline-formula><mml:math id="inf791"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that can be achieved using even one sample. Condition (b) implies that <inline-formula><mml:math id="inf792"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> will be superior to any <inline-formula><mml:math id="inf793"><mml:mrow><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The lower bound (<xref ref-type="disp-formula" rid="equ102">Equation 85</xref>), together with our monotonicity assumption regarding <inline-formula><mml:math id="inf794"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, implies that for any <inline-formula><mml:math id="inf795"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ103"><mml:math id="m103"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≤</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mpadded width="+2.8pt"><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder></mml:mpadded></mml:mrow><mml:mo rspace="5.3pt">&lt;</mml:mo><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≤</mml:mo><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and hence that<disp-formula id="equ104"><mml:math id="m104"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">&lt;</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>While condition (b) is stronger than is needed for this conclusion, the sufficient conditions stated in the previous paragraph have the advantage that we need only consider optimal encoding rules for the cases <inline-formula><mml:math id="inf796"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf797"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and the efficient coding problem stated in definition (<xref ref-type="disp-formula" rid="equ102">Equation 85</xref>), in order to verify that the conditions are both satisfied. The efficient coding problem for the case <inline-formula><mml:math id="inf798"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is treated in Appendix 6, where we show that <inline-formula><mml:math id="inf799"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Using the calculations explained in Appendix 2, we can provide an analytical approximation to this quantity in the limiting case of large <inline-formula><mml:math id="inf800"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><xref ref-type="disp-formula" rid="equ48">Equation 37</xref> states that for any encoding rule <inline-formula><mml:math id="inf801"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and any prior distribution <inline-formula><mml:math id="inf802"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the value of <inline-formula><mml:math id="inf803"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for any large enough value of <inline-formula><mml:math id="inf804"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will approximately equal<disp-formula id="equ105"><mml:math id="m105"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≈</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mfrac><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msqrt><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf805"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability density function of the distribution of values for <inline-formula><mml:math id="inf806"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> implied by the function <inline-formula><mml:math id="inf807"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the distribution <inline-formula><mml:math id="inf808"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of values for <inline-formula><mml:math id="inf809"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the case of DbS, the probability distribution over alternative internal representations <inline-formula><mml:math id="inf810"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (and hence the probability of error) is the same as in the case of an encoding rule <inline-formula><mml:math id="inf811"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> so that <xref ref-type="disp-formula" rid="equ48">Equation 37</xref> can be applied. Furthermore, for any prior distribution <inline-formula><mml:math id="inf812"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the probability distribution of values for the quantile <inline-formula><mml:math id="inf813"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> will be a uniform distribution over the interval <inline-formula><mml:math id="inf814"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf815"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all θ. It follows that<disp-formula id="equ106"><label>(86)</label><mml:math id="m106"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup><mml:mo>≈</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mfrac><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:msqrt><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mpadded width="+2.8pt"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msqrt><mml:mfrac><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In the case that <inline-formula><mml:math id="inf816"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> instead, the same function <inline-formula><mml:math id="inf817"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must be used regardless of the contextual distribution <inline-formula><mml:math id="inf818"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Under the assumption that <inline-formula><mml:math id="inf819"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is piecewise continuous, there must exist a magnitude <inline-formula><mml:math id="inf820"><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> such that <inline-formula><mml:math id="inf821"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is continuous over some interval <inline-formula><mml:math id="inf822"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> containing <inline-formula><mml:math id="inf823"><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> in its interior. Let <inline-formula><mml:math id="inf824"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> be the greatest lower bound and least upper bound respectively, such that<disp-formula id="equ107"><mml:math id="m107"><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.3pt">≤</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>for all <inline-formula><mml:math id="inf825"><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="4.2pt">&lt;</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>v</mml:mi></mml:mpadded><mml:mo rspace="4.2pt">&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The continuity of <inline-formula><mml:math id="inf826"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on this interval means that by choosing both <inline-formula><mml:math id="inf827"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf828"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> close enough to <inline-formula><mml:math id="inf829"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> we can make both <inline-formula><mml:math id="inf830"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf831"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> arbitrarily close to <inline-formula><mml:math id="inf832"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>By the same argument as in Appendix 6, for any prior distribution <inline-formula><mml:math id="inf833"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with support entirely contained in the interval <inline-formula><mml:math id="inf834"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, the pair of stimulus magnitudes <inline-formula><mml:math id="inf835"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> will have to imply <inline-formula><mml:math id="inf836"><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="4.2pt">≤</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with probability 1, and as a consequence the error probability <inline-formula><mml:math id="inf837"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will necessarily be greater than or equal to the lower bound <inline-formula><mml:math id="inf838"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> By choosing both <inline-formula><mml:math id="inf839"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf840"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> close enough to <inline-formula><mml:math id="inf841"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> we can make this lower bound arbitrarily close to <inline-formula><mml:math id="inf842"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn> 1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Hence for any encoding rule <inline-formula><mml:math id="inf843"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf844"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the upper bound <inline-formula><mml:math id="inf845"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> cannot be lower than 1/2. It follows that <inline-formula><mml:math id="inf846"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>Given this, condition (a) can alternatively be expressed as<disp-formula id="equ108"><mml:math id="m108"><mml:mrow><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mpadded><mml:mo rspace="4.2pt">+</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn> 1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that if <inline-formula><mml:math id="inf847"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> remains less than 1/2 no matter how large <inline-formula><mml:math id="inf848"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is, this condition will necessarily be satisfied for all large enough values of <inline-formula><mml:math id="inf849"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, since <xref ref-type="disp-formula" rid="equ106">Equation 86</xref> implies that <inline-formula><mml:math id="inf850"><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> eventually becomes arbitrarily small, in the case of large enough <inline-formula><mml:math id="inf851"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (On the other hand, the condition can easily be satisfied for some range of smaller values of <inline-formula><mml:math id="inf852"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, even if <inline-formula><mml:math id="inf853"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> once <inline-formula><mml:math id="inf854"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> becomes very large.)</p><p>In order to consider the conditions under which condition (b) will also be satisfied, it is necessary to further analyze the efficient coding problem stated in <xref ref-type="disp-formula" rid="equ102">Equation 85</xref>. We first observe that for any prior <inline-formula><mml:math id="inf855"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:mrow></mml:math></inline-formula> and encoding rule <inline-formula><mml:math id="inf856"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the encoding rule can always be expressed in the form <inline-formula><mml:math id="inf857"><mml:mrow><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf858"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a piecewise-continuous, weakly increasing function giving the probability of a 'high' reading as a function of the quantile <inline-formula><mml:math id="inf859"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of the stimulus magnitude in the prior distribution. We then note that when this representation is used for the encoding function in <xref ref-type="disp-formula" rid="equ102">Equation 85</xref>, the error probability <inline-formula><mml:math id="inf860"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depends only on the function <inline-formula><mml:math id="inf861"><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> in a way that is independent of the prior <inline-formula><mml:math id="inf862"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Hence the inner minimization problem in <xref ref-type="disp-formula" rid="equ102">Equation 85</xref> can equivalently be written as<disp-formula id="equ109"><label>(87)</label><mml:math id="m109"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">inf</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This problem has a solution for the optimal <inline-formula><mml:math id="inf863"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for any number of processing units <inline-formula><mml:math id="inf864"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and an associated value that is independent of the prior <inline-formula><mml:math id="inf865"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Hence we can write the bound defined in <xref ref-type="disp-formula" rid="equ102">Equation 85</xref> more simply as<disp-formula id="equ110"><label>(88)</label><mml:math id="m110"><mml:mrow><mml:mrow><mml:msub><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">inf</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Condition (b) will be satisfied as long as the bound defined in <xref ref-type="disp-formula" rid="equ110">Equation 88</xref> is not too much lower than <inline-formula><mml:math id="inf866"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> In fact, this bound can be a relatively large fraction of <inline-formula><mml:math id="inf867"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We consider the problem of the optimal choice of an encoding function <inline-formula><mml:math id="inf868"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for a known prior <inline-formula><mml:math id="inf869"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Appendix 2. In the limiting case of a sufficiently large <inline-formula><mml:math id="inf870"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, substitution of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> into 37 yields the approximate solution<disp-formula id="equ111"><label>(89)</label><mml:math id="m111"><mml:mrow><mml:mrow><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup><mml:mo>≈</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mfrac><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mpadded><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msqrt><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mpadded></mml:mrow><mml:mo rspace="5.3pt">=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus as <italic>n</italic> is made large, the ratio <inline-formula><mml:math id="inf871"><mml:mrow><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> converges to the value<disp-formula id="equ112"><label>(90)</label><mml:math id="m112"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>lim</mml:mi></mml:msup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>8</mml:mn><mml:mo>/</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.81</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This means that increases in the sample size <inline-formula><mml:math id="inf872"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> above one cannot reduce <inline-formula><mml:math id="inf873"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by even 20 percent relative to <inline-formula><mml:math id="inf874"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, no matter how large the sample may be, whereas <inline-formula><mml:math id="inf875"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> may be only a small fraction of <inline-formula><mml:math id="inf876"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (as is necessarily the case when <inline-formula><mml:math id="inf877"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is large). This makes it quite possible for <inline-formula><mml:math id="inf878"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to be larger than <inline-formula><mml:math id="inf879"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder></mml:mrow></mml:math></inline-formula> while at the same time <inline-formula><mml:math id="inf880"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is larger than <inline-formula><mml:math id="inf881"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. <bold>In this case, the optimal sample size will be</bold> <inline-formula><mml:math id="inf882"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo mathvariant="normal">,</mml:mo></mml:mrow></mml:math></inline-formula> <bold>and the optimal encoding rule will be DbS</bold>.</p><p>While these analytical results for the asymptotic (large-<inline-formula><mml:math id="inf883"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) case are useful, we can also numerically estimate the size of the terms <inline-formula><mml:math id="inf884"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf885"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:math></inline-formula> in the case of any finite value for <inline-formula><mml:math id="inf886"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We have derived an exact analytical value for <inline-formula><mml:math id="inf887"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> above. The quantity <inline-formula><mml:math id="inf888"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:math></inline-formula> can be computed through Monte Carlo simulation for any value of <inline-formula><mml:math id="inf889"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (Note that this calculation depends only on <inline-formula><mml:math id="inf890"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and is independent of the contextual distribution <inline-formula><mml:math id="inf891"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; we need only to calculate <inline-formula><mml:math id="inf892"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the function <inline-formula><mml:math id="inf893"><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>.) The calculation of <inline-formula><mml:math id="inf894"><mml:msub><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> for a given finite value of <inline-formula><mml:math id="inf895"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is instead more complex, since it requires us to optimize <inline-formula><mml:math id="inf896"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over the entire class of possible functions <inline-formula><mml:math id="inf897"><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p>Our approach is to estimate the minimum achievable value of <inline-formula><mml:math id="inf898"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by finding the minimum achievable value over a flexible parametric family of possible functions <inline-formula><mml:math id="inf899"><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We specify the function <inline-formula><mml:math id="inf900"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in terms of the implied <inline-formula><mml:math id="inf901"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the CDF for values of <inline-formula><mml:math id="inf902"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We let <inline-formula><mml:math id="inf903"><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be implicitly defined by<disp-formula id="equ113"><label>(91)</label><mml:math id="m113"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf904"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a function of <inline-formula><mml:math id="inf905"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with the properties that <inline-formula><mml:math id="inf906"><mml:mrow><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> as required for <inline-formula><mml:math id="inf907"><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be the CDF of a probability distribution. More specifically, we assume that <inline-formula><mml:math id="inf908"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a finite-order polynomial function consistent with these properties, which require that it can be written in the form<disp-formula id="equ114"><label>(92)</label><mml:math id="m114"><mml:mrow><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf909"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are a set of parameters over which we optimize. Note that for a large enough value of <inline-formula><mml:math id="inf910"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, any smooth function can be well approximated by a member of this family. At the same time, our choice of a parametric family of functions has the virtue that the CDF that corresponds to the optimal coding rule in the large-<inline-formula><mml:math id="inf911"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limit belongs to this family (regardless of the value of <inline-formula><mml:math id="inf912"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), since this coding rule (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) corresponds to the case <inline-formula><mml:math id="inf913"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> of <xref ref-type="disp-formula" rid="equ114">Equation 92</xref>.</p><p>We computed via numerical simulations the best encoder function assuming <inline-formula><mml:math id="inf914"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be of order 5 (<xref ref-type="disp-formula" rid="equ114">Equation 92</xref>) for various finite values of <inline-formula><mml:math id="inf915"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>35</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and we define the expected error of this optimal encoder for a given <inline-formula><mml:math id="inf916"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be <inline-formula><mml:math id="inf917"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:math></inline-formula> (i.e., a lower bound for <inline-formula><mml:math id="inf918"><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> within the family of functions defined by <inline-formula><mml:math id="inf919"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Our goal is to compare this quantity to the asymptotic approximation <inline-formula><mml:math id="inf920"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula>, in order to evaluate how accurate the asymptotic approximation is.</p><p>Additionally, we also compute the value <inline-formula><mml:math id="inf921"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:math></inline-formula> for each finite value of <inline-formula><mml:math id="inf922"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> through Monte Carlo simulation (please note that <inline-formula><mml:math id="inf923"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:math></inline-formula> is different from the quantity <inline-formula><mml:math id="inf924"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> defined in <xref ref-type="disp-formula" rid="equ106">Equation 86</xref>, that is only an asymptotic approximation for large <inline-formula><mml:math id="inf925"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Then, we can compare <inline-formula><mml:math id="inf926"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:math></inline-formula> to the value predicted by the asymptotic approximations <inline-formula><mml:math id="inf927"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf928"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula>.</p><p>Another quantity that is important to compute, in order to determine whether DbS can be optimal when <inline-formula><mml:math id="inf929"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is not too large, is the size of <inline-formula><mml:math id="inf930"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> relative to the quantities computed above. Since <inline-formula><mml:math id="inf931"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> does not shrink as <inline-formula><mml:math id="inf932"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> increases, it is obvious that <inline-formula><mml:math id="inf933"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is much larger than the other quantities in the large-<inline-formula><mml:math id="inf934"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> limit. But how much bigger is it when <inline-formula><mml:math id="inf935"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small? To investigate this, we compute the value of the ratio <inline-formula><mml:math id="inf936"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf937"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small. This quantity is given by<disp-formula id="equ115"><label>(93)</label><mml:math id="m115"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1</xref>, all error quantities discussed above are normalized relative to <inline-formula><mml:math id="inf938"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula>. The black dashed lines in both panels represent <inline-formula><mml:math id="inf939"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The ratio of the asymptotic approximation for <inline-formula><mml:math id="inf940"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> relative to <inline-formula><mml:math id="inf941"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula> is plotted with the red dashed lines, where <inline-formula><mml:math id="inf942"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1.23</mml:mn></mml:mrow></mml:math></inline-formula>. Note that the sufficient conditions for DbS to be optimal can be stated as</p><list list-type="alpha-lower"><list-item><p><inline-formula><mml:math id="inf943"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">&lt;</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">-</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and</p></list-item><list-item><p><inline-formula><mml:math id="inf944"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thickmathspace"/><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:munder><mml:mi>P</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>Therefore, <xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1</xref> shows the numerical magnitudes of the expressions on the right-hand side of both inequalities (normalized by the value of <inline-formula><mml:math id="inf945"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>). The most important result from the analyses presented in this figure is that even for small values of , the right-hand side of the first inequality (see right panel) will be a much larger quantity than the right-hand side of the second inequality (see left panel). Thus it can easily be the case that <inline-formula><mml:math id="inf946"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf947"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are such that both inequalities are satisfied: it is worth increasing <inline-formula><mml:math id="inf948"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from 0 to 1, but not worth increasing <inline-formula><mml:math id="inf949"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to any value higher than 1. In this case, the optimal sample size will be <inline-formula><mml:math id="inf950"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and the optimal encoding rule will be DbS.</p><fig id="app7fig1" position="float"><label>Appendix 7—figure 1.</label><caption><title>Performance of efficient coding rules.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54962-app7-fig1-v1.tif"/></fig><p>Additionally, we found that the computations of <inline-formula><mml:math id="inf951"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mi>DbS</mml:mi></mml:msubsup></mml:math></inline-formula> for each finite value of <inline-formula><mml:math id="inf952"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are slightly higher than <inline-formula><mml:math id="inf953"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula> even for small <inline-formula><mml:math id="inf954"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> values (blue line in the left panel), but quickly reach the asymptotic value <inline-formula><mml:math id="inf955"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>error</mml:mi><mml:mrow><mml:mi>DbS</mml:mi><mml:mo>,</mml:mo><mml:mi>lim</mml:mi></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf956"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> increases. Thus, even for small values of <inline-formula><mml:math id="inf957"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the asymptotic approximation of optimal performance for the case of complete prior knowledge is superior than DbS. We also found that the computations of <inline-formula><mml:math id="inf958"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msubsup></mml:math></inline-formula> for each finite value of <inline-formula><mml:math id="inf959"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> cannot reduce <inline-formula><mml:math id="inf960"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula> by even five percent for small <inline-formula><mml:math id="inf961"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> values (orange line in the left panel). Moreover, <inline-formula><mml:math id="inf962"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msubsup></mml:math></inline-formula> quickly reached the asymptotic value <inline-formula><mml:math id="inf963"><mml:msubsup><mml:munder accentunder="true"><mml:mi>P</mml:mi><mml:mo>¯</mml:mo></mml:munder><mml:mi>n</mml:mi><mml:mi>lim</mml:mi></mml:msubsup></mml:math></inline-formula>, thus suggesting that the asymptotic solution is virtually indistinguishable from the optimal solution (at least based on the flexible family of <inline-formula><mml:math id="inf964"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> functions) also for finite values of <inline-formula><mml:math id="inf965"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which crucially are in the range of the values found to explain the data in the numerosity discrimination experiment of our study. Thus, these results confirm that the asymptotic approximations used in our study are not likely to influence the conclusions of the experimental data in our work.</p></sec></boxed-text></app><app id="appendix-8"><title>Appendix 8</title><boxed-text><sec id="s15" sec-type="appendix"><title>Relation to <xref ref-type="bibr" rid="bib6">Bhui and Gershman, 2018</xref></title><p><xref ref-type="bibr" rid="bib6">Bhui and Gershman, 2018</xref> also argue that an efficient coding scheme can be implemented by a version of DbS. However, both the efficient coding problem that they consider, and the version of DbS that they consider, are different than in our analysis, so that our results are not implied by theirs.</p><p>Like us, Bhui and Gershman consider encoding schemes in which the internal representation <inline-formula><mml:math id="inf966"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> must take one of a finite number of values. However, their efficient coding problem considers the class of all encoding rules that assign one or another of <inline-formula><mml:math id="inf967"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> possible values of <inline-formula><mml:math id="inf968"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to a given stimulus <inline-formula><mml:math id="inf969"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In their discussion of the ideal efficient coding benchmark, they do not require <inline-formula><mml:math id="inf970"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be the ensemble of output states of a set of <inline-formula><mml:math id="inf971"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons, each of which must use the same rule as the other units, and therefore consider a more flexible family of possible encoding rules, as we explain in more detail below.</p><p>The encoding rule that solves our efficient coding problem is stochastic; even under the assumption that the prior <inline-formula><mml:math id="inf972"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is known with perfect precision (the case of unbounded <inline-formula><mml:math id="inf973"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the more general specification of our framework, so that sampling error in estimation of this distribution from prior experience is not an issue), we show that it is optimal for the probabilities <inline-formula><mml:math id="inf974"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> not to all equal either zero or one. The optimal rule within the more flexible class considered by Bhui and Gershman is instead deterministic: each stimulus magnitude <inline-formula><mml:math id="inf975"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is assigned to exactly one category <inline-formula><mml:math id="inf976"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with certainty. The boundaries between the set of <inline-formula><mml:math id="inf977"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> categories furthermore correspond to the quantiles <inline-formula><mml:math id="inf978"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of the prior distribution, so that each category is used with equal frequency. Thus the optimal encoding rule is given by a deterministic function <inline-formula><mml:math id="inf979"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> a non-decreasing step function that takes <inline-formula><mml:math id="inf980"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> discrete values.</p><p>Bhui and Gershman show that when there is no bound on <inline-formula><mml:math id="inf981"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the number of samples from prior experience that can be used to estimate the contextual distribution — their optimal encoding rule for a given number of categories <inline-formula><mml:math id="inf982"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> — can be implemented by a form of DbS. However, the DbS algorithm that they describe is different than in our discussion. Bhui and Gershman propose to implement the deterministic classification <inline-formula><mml:math id="inf983"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by computing the fraction of the sampled values <inline-formula><mml:math id="inf984"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> that are less than <inline-formula><mml:math id="inf985"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the limiting case of an infinite sample from the prior distribution, this fraction is equal to <inline-formula><mml:math id="inf986"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with probability one, and <inline-formula><mml:math id="inf987"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is then determined by which of the intervals <inline-formula><mml:math id="inf988"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> the quantile <inline-formula><mml:math id="inf989"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> falls within. Thus whereas in our discussion, DbS is an algorithm that allows each of our units to compute its state using only a single sampled value <inline-formula><mml:math id="inf990"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the DbS algorithm proposed by Bhui and Gershman to implement efficient coding is one in which a large number of sampled values are used to jointly compute the output states of all of the units in a coordinated way.</p><p>Bhui and Gershman also consider the case in which only a finite number of samples <inline-formula><mml:math id="inf991"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be used to compute the representation <inline-formula><mml:math id="inf992"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of a given stimulus magnitude <inline-formula><mml:math id="inf993"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and ask what kind of rule is efficient in that case. They show that in this case a variant of DbS with kernel-smoothing is superior to the version based on the empirical quantile of <inline-formula><mml:math id="inf994"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (which now involves sampling error). In this more general case, the variant DbS algorithms considered by Bhui and Gershman make the representation <inline-formula><mml:math id="inf995"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of a given stimulus probabilistic; but the class of probabilistic algorithms that they consider remains different from the one that we discuss. In particular, they continue to consider algorithms in which the category <inline-formula><mml:math id="inf996"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> can be an arbitrary function of <inline-formula><mml:math id="inf997"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and a single set of <inline-formula><mml:math id="inf998"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> sampled values that is used to compute the complete representation; they do not impose the restriction that <inline-formula><mml:math id="inf999"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> be the number of units giving a 'high' reading when the output state of each of <inline-formula><mml:math id="inf1000"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> individual processing units is computed independently using the same rule (but an independent sample of values from prior experience in the case of each unit).</p><p>The kernel-smoothing algorithms that they consider are based on a finite set of <inline-formula><mml:math id="inf1001"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> pairwise comparisons between the stimulus magnitude <inline-formula><mml:math id="inf1002"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and particular sampled values <inline-formula><mml:math id="inf1003"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, the outcomes of which are then aggregated to obtain the internal representation <inline-formula><mml:math id="inf1004"><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. However, they allow the quantity <inline-formula><mml:math id="inf1005"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> computed by comparing <inline-formula><mml:math id="inf1006"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to an individual sampled value to vary continuously between 0 and 1, rather than having to equal either 0 or 1, as in our case (where the state of an individual unit must be either 'high' or 'low'). The quantities <inline-formula><mml:math id="inf1007"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are able to be summed with perfect precision, before the resulting sum is then discretized to produce a final representation that takes one of only <inline-formula><mml:math id="inf1008"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> possible values. Thus an assumption that only finite-precision calculations are possible is made only at the stage where the final output of the joint computation of the processors must be ‘read out’; the results of the individual binary comparisons are assumed to be integrated with infinite precision. In this respect, the algorithms considered by Bhui and Gershman are not required to economize on processing resources in the same sense as the class that we consider; the efficient coding problem for which they present results is correspondingly different from the problem that we discuss for the case in which <inline-formula><mml:math id="inf1009"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is finite.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54962.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><role>Reviewing Editor</role><aff><institution>Harvard University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><role>Reviewer</role><aff><institution>Harvard University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Tsetsos</surname><given-names>Konstantinos</given-names> </name><role>Reviewer</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Gluth</surname><given-names>Sebastian</given-names> </name><role>Reviewer</role><aff><institution>University of Basel</institution><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>We believe that this paper makes fundamental contributions to our understanding of the perception-decision interface. In particular, it sheds light on how perceptual representation adapts to task demands. We foresee that this paper will stimulate future experimental work to test the mechanistic hypotheses postulated by the theory.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Efficient sampling and noisy decisions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Samuel J Gershman as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Konstantinos Tsetsos (Reviewer #2); Sebastian Gluth (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In their manuscript, Heng and colleagues derive general optimality principles and predictions of a decision-making system (or agent) that is restricted to encode information in terms of a finite number of binary samples. They find that in case of maximizing accuracy, the principle aligns with mutual information, but this changes if reward maximization is taken into account. Based on this, they conduct three numerosity experiments with different incentive rules ( &quot;perceptual&quot; vs. &quot;preferential&quot;). Surprisingly, they find that participants do not adjust their behavior to the incentive rule. Instead their behavior is best explained by the Decision by Sampling (DbS) framework, which assumes that behavior depends on the distribution of stimuli in the environment (in contrast to a simple log rule), but not in an optimal way. The reviewers agreed that this is an interesting and potentially important contribution to the literature on judgment and decision making, but requires substantive revisions in a number of respects detailed below.</p><p>Essential revisions:</p><p>1) Technical/conceptual issues.</p><p>a) The theoretical results depend on asymptotically taking n to be large. When n is finite, the proposed coding schemes may not necessarily be optimal, which should be recognized. Indeed this forms the basis of the Bhui and Gershman analysis of DbS.</p><p>b) The authors write that DbS continues to &quot;explain the shape of ubiquitous psycho-economic functions&quot;, but they also provide accuracy maximization results. Do the alternative optimization criteria affect these curves or not?</p><p>c) The reward maximizing scheme assumes that agents seek to minimize &quot;regret&quot; (v1-v2). This is a viable hypothesis, but how does optimal encoding change if instead agents care just about the obtained reward (say, v1)? More generally, it is not a given that relative and not absolute reward is the relevant quantity in value based tasks.</p><p>2) Experimental/analysis issues.</p><p>a) The model recovery results shown in Figure 3D as well as the fits in Figure 4 rely on models in which the shape of the prior distribution is fixed and equal to the shape of the prior distribution used in experiments 1-2. Is the encoding rule still identifiable if the parameter controlling the shape of the prior is free to vary? The authors show recovery of the α parameter within the DbS model (Figure 5) but not when the encoding rule is unknown. Crucially, the conclusion that DbS outperform the other two encoding rules can be undermined if letting the prior free to vary induces model mimicry. Please examine this possibility. If indeed the different encoding schemes are not falsifiable it should be clearly stated that the conclusions (e.g &quot;we found that humans employ a less optima strategy.&quot;, &quot;allowed us to test the hypothesis&quot; etc) hold under the specific assumption that the prior distribution is fixed.</p><p>b) Modeling of the adaptation to priors in experiments 1-3 assumes by definition that the prior parameter starts from a higher than the nominal value, and adapts across time with a time-scale that is shared across experiments. Is there indeed need/ evidence for adaptation? Observing the data in Figure 4—figure supplement 2 I can see that the accuracy a) is stable across time-rendering any adaptation process counterintuitive and, b) accuracy in Experiments 1-2 is higher than the accuracy in Experiment 3. Thus, the lower asymptotic α appears to serve the role of lowering overall accuracy. Please 1) superimpose the across time accuracy of the DbS model with prior adaptation on the traces shown in Figure 4—figure supplement 2, in order to see if the model systematically misfits the data by starting with α=2.84. 2) Please compare the fits of the adapting prior model with the a) the fits of a DbS model with just a free α parameter and b) a DbS model with α=2 and n as free parameter. Can these alternatives explain the data more parsimoniously?</p><p>c) One possible reason why the two conditions did not lead to differences could be that – after doing one condition for two days – it might have been impossible for the participants to adjust their &quot;habit-like&quot; behavior to a new incentive rule. This could be checked by analyzing the first half of the task in a between-subject manner.</p><p>3) Expository issues.</p><p>a) Provide more intuition for the equivalence between results under different optimization criteria. Do any of these results rely on the asymptotics?</p><p>b) The clarity of the Introduction can be improved. In the second paragraph, the authors suddenly jump to a discussion of differences between perceptual and preferential choice, but I think it would be more important to first make clear what the overall goal of the work is. The third paragraph is very confusing. Its first sentence is not even a full sentence (a verb is missing at &quot;where only a finite number.…&quot;) and pretty much incomprehensible. Then, the work of Simon Laughlin is discussed, but it is questionable whether this is really the best way to motivate the proposal of a binary encoding system (why not simply saying that neurons provide binary outcomes). The reference to Query Theory in the fourth paragraph remains vague. In a later paragraph, the idea of adaptation to a frequency distribution is introduced without explaining what it actually means (and one reason for this is that DbS is not well explained in the previous paragraph).</p><p>c) In the Abstract, the authors should make more clear what the task was about (though we understand that this isn't easy give the word limit). In addition, the word &quot;Here&quot; is used to start two consecutive sentences, and an &quot;a&quot; is missing at &quot;strategy that might be utilized…&quot;.</p><p>4) Links to related literature.</p><p>a) Do the results cast doubt on the argument made in the paper by Rustichini et al., 2017, which argues for a coding scheme based on expected utility maximization rather than mutual information?</p><p>b) Clarify that Equation 9 only corresponds to DbS in the asymptotic limit. The finite sample regime was emphasized in Bhui and Gershman, 2018, in order to explain certain phenomena (such as range effects) that do not follow directly from the CDF encoding function. Instead, that paper showed how these results could be obtained from a smoothed encoding function computed on a small set of samples. Relatedly, please clarify the links to the Bhui and Gershman paper. In particular, how does infomax in this paper (like in Supplementary Note 1) related to infomax in their paper? Also their work is described as adding noise after efficient coding, but this is not the case. The &quot;noise&quot; in that model comes purely from the fact that a finite number of samples are drawn, so that the sample-based CDF only approximates the true CDF.</p><p>c) The authors should discuss (and scrutinize) their empirical findings a bit more in the context of other studies that have compared perceptual and preferential decisions, in particular Dutilh and Rieskamp, 2015, who studied a quite similar task (choosing based on the number of dots with a perceptual vs. a preferential incentive rule). Here, it was found that decisions were slowest for the most difficult trials in the perceptual condition but not in the preferential condition. The question is whether there are any interesting, related response time differences between the two conditions in the current task (because if the number of dots on the left and right is very similar, one should not think too long about it if the reward depends on the number of dots [preferential], but one would need to think for a long time about it to decipher which side has more dots [perceptual]).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your article &quot;Efficient sampling and noisy decisions&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by three peer reviewers, including Samuel J Gershman as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Konstantinos Tsetsos (Reviewer #2); Sebastian Gluth (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. We are optimistic that the next revision will be acceptable for publication.</p><p>Summary:</p><p>In their revision the authors have successfully addressed most of the points we had raised. In particular the authors now discuss in detail the role of asymptotics and the relationship between their framework and the one proposed by Bhui and Gershman. This development has resulted in the extension of the framework in order to capture finite sampling from the prior distribution. Additionally, the authors have demonstrated that the encoding rule (as well as the shape of the prior and the number of samples) is identifiable when the shape of the prior is free to vary. All these developments have sufficiently improved the manuscript.</p><p>Revisions for this paper:</p><p>1) Having established the identifiability of the α parameter under the DbS model, it seems imperative to fit Experiment 3 using α as a free parameter and omitting the adaptation mechanism (this has been done in the revision but these results are used to examine whether there is adaptation or not, rather than to actually examine if the fitted prior differs between experiments 1-2 and 3). In other words, Figure 5A can be expanded to include the α fits from Experiment 3. This exercise can address whether there is indeed a change to the shape of the prior across experiments, which is a pivotal component of the proposed framework relative to alternative frameworks that assume complex representational non-linearities without sensitivity to the prior. The results in Figure 5C show that α in Experiment 3 converges to a lower asymptotic value. However, imposing an adaptation process, especially when there is no strong support for such process, can obscure the interpretation of the fits. Furthermore, I remain skeptical about the claim that there is dynamical adaptation within each experiment: i) if anything, the new analyses show that the &quot;free α&quot; model provides a better goodness of fit, and ii) Figures 4—figure supplement 2 and Figure 5—figure supplement 1 show no obvious dynamical trends in behavior. How does the adaptation manifest itself in the data? Figure 5—figure supplement 2 hints toward an early period in which the &quot;free-α&quot; fits worse than the dynamic α model (up to ~150 trials). Perhaps modeling this discrepancy explicitly (e.g. two α parameters for early and late trials, respectively) would suffice.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54962.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Technical/conceptual issues.</p><p>a) The theoretical results depend on asymptotically taking n to be large. When n is finite, the proposed coding schemes may not necessarily be optimal, which should be recognized. Indeed this forms the basis of the Bhui and Gershman analysis of DbS.</p></disp-quote><p>In the response to this point, we will also address other related comments: In point 4b, reviewers ask to clarify the links to relation to Bhui and Gershman and the asymptotic limit statements. In minor point a, the reviewers ask to provide more intuition for the equivalence between results under different optimization criteria, and whether any of these results rely on asymptotics.</p><p>We would like to highlight that your comments motivated us to improve the presentation and interpretation of the theoretical results. We have now redefined the elaboration of our theoretical framework which we hope will clarify various issues raised by the reviewers in this comment. This led us to re-structure the presentation of the theoretical results in the revised manuscript. Importantly, none of our conclusions are affected by these reformulations and analyses. Below, we provide a brief description of how our revision addresses the reviewers’ concerns.</p><p>1) In the re-elaboration of our framework, we now consider two distinct types of resource constraints and their implications for the nature of an efficient coding scheme. The first cognitive resource that we consider is the number of samples <italic>n</italic> that can be used to encode the magnitude of an individual stimulus, as formulated in the initial submission. However, our framework now formalizes a second resource constraint as well: the number of samples <italic>m</italic> from the prior that can be used to adapt the coding rule to a particular context. For the case of unbounded prior knowledge (i.e., perfect knowledge of the prior, as is typically assumed in the efficient coding frameworks in early sensory systems), we derive analytical solutions for the case when the system maximizes either accuracy or fitness (these results were presented in the initial submission). We now make clear at various points in the text that these results rely on asymptotic approximations. However, we now also analyze numerically how far these asymptotic approximations are from the actual optimal solutions for small values of <italic>n</italic> (crucially, including the range of values needed to fit the experimental data). We found via numerical approximations that the analytical solutions appear to be nearly optimal for the relevant range of <italic>n</italic> in our experiments (see Appendix 7—figure 1). Crucially, for a number <italic>n</italic> of samples in this range, both the numerical-optimal and large-<italic>n</italic> analytical approximation (again under the assumption of full prior knowledge) are found to be more efficient than the DbS model. Therefore, we argue that our conclusions in the initial submission regarding the way in which an optimal encoding rule (in the case of no limit on <italic>m</italic>) is different from DbS still hold for the range of values <italic>n</italic> that are relevant to explain our data.</p><p>2) As just discussed, we obtain analytical solutions that approximately characterize the optimal encoding rule <italic>θ</italic>(<italic>v</italic>) in the limit as <italic>n</italic> is made sufficiently large. It should be noted however that we are always assuming that <italic>n</italic> is finite, and that this constrains the accuracy of the decision maker’s judgments, while <italic>m</italic> (knowledge about the prior) is instead assumed to be unbounded in this part of the analysis and hence no constraint.</p><p>We then consider instead the case when it is costly for a system to have full knowledge of the prior distribution (because the system must dynamically adapt to different prior distributions in changing contexts). In this case, it can economize on processing resources to use only a few samples from past experiences rather than requiring the system to completely learn the prior distribution. Our general framework allows us to define formally an optimization problem for this case as well. Crucially, <italic>we have formally demonstrated</italic> that under the assumption that reducing either <italic>m</italic> or <italic>n</italic> allows one to economize on scarce cognitive resources, it is under certain circumstances (that we precisely define) most efficient to use an algorithm with a very low value of <italic>m</italic> (as assumed by DbS), while allowing <italic>n</italic> to be much larger. This allows us to state conditions under which DbS will be the optimal encoding rule. Moreover, we wish to emphasize that this result is derived for the case of a particular finite number of processing units <italic>n</italic> (and a corresponding finite total number of samples from the contextual distribution used to encode a given stimulus), and does not assume that n must be large (see revised Results section, and Appendixes 6 and 7). We believe that these results further extend the novel contributions of our work, as they provide a formal explanation of why DbS might better describe behavior in our experiments than the rules that would be optimal for the particular stimulus distributions that we use, and, more generally, show why DbS might be a good strategy to be used by higher-order level systems that need to flexibly adapt to dynamical contexts.</p><p>3) Regarding the relation of our work to Bhui and Gershman, we have now dedicated a paragraph to this topic in the Discussion section where we provide a high-level explanation of the similarities and differences between our studies. Moreover, we discuss in more detail the relation and differences between our studies in a new supplementary note (Appdendix 8, in the revised manuscript).</p><p>While Bhui and Gershman provide an argument for a version of DbS as the solution to an efficient coding problem, their definition of the efficient coding problem is quite different from ours, so that our result is quite distinct. Moreover, their conclusions differ from ours, given that different classes of feasible algorithms are considered. In Bhui and Gershman, the classic formulation of DbS is found to be optimal only in the limiting case in which a very large number of samples can be used for the encoding; in the case of a smaller sample, they find that a smoothed version of DbS improves upon the classic formulation. In our analysis, instead, the classic formulation of DbS is found to be optimal when the cost of using a greater number of samples is sufficiently important; instead, when it is economical to use a larger number of samples, we show that an alternative coding rule is superior to DbS. While the flavor of these results may seem directly opposed to each other, there is no inconsistency between them, as the classes of encoding rules considered are quite different in the two papers.</p><p>Briefly, Bhui and Gershman consider a case in which only a finite number of values can be sampled from the prior, and show that a variant of DbS with kernel-smoothing is superior to its standard version. However, a key difference from our analysis is that they allow the kernel-smoothed quantity (computed by comparing the input <italic>v</italic> with a sample ˜<italic>v</italic> from the prior distribution) to vary continuously between 0 and 1, rather than requiring the output of each processing unit to be either 0 or 1 as in our implementation (Figure 1 in the revised manuscript). When instead we consider only coding schemes that encode information based on zeros or ones, we show that coding efficiency can be improved relative to DbS, but only in the case that is sufficiently cheap to obtain a large sample from the prior distribution. In the case that a large sample from the prior distribution is available, we offer both an analytical solution for the optimal encoding rule in the large-<italic>n</italic> limit, and a numerical solution for it in the case of smaller (empirically realistic) values of <italic>n</italic>, and show that it is possible to improve upon DbS in both cases.</p><disp-quote content-type="editor-comment"><p>b) The authors write that DbS continues to &quot;explain the shape of ubiquitous psycho-economic functions&quot;, but they also provide accuracy maximization results. Do the alternative optimization criteria affect these curves or not?</p></disp-quote><p>We apologize for our lack of clarity in this statement. The alternative optimization criteria do affect these curves, as we show in Figure 2B (in the revised manuscript). For all of the optimization objectives that we consider, the encoding rules share the qualitative feature that they are steeper for regions of the colorred stimulus space with higher prior density. However, mild changes in the steepness of the curves will be represented in significant discriminability differences between the different encoding rules across the support of the prior distribution (Figure 2D). We have now clarified this point in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>c) The reward maximizing scheme assumes that agents seek to minimize &quot;regret&quot; (v1-v2). This is a viable hypothesis, but how does optimal encoding change if instead agents care just about the obtained reward (say, v1)? More generally, it is not a given that relative and not absolute reward is the relevant quantity in value based tasks.</p></disp-quote><p>It is not correct that in our discussion of the case of “expected reward maximization”, we assume that the decision maker cares about relative rather than the absolute reward achieved in a given outcome. We acknowledge that this misunderstanding might have been triggered by our description of the problem in the Results section of the initial submission. We have revised the corresponding appendix (Appendix 5 in the revised manuscript) and the Results section to clarify our calculations.</p><p>The objective that we maximize in this case is E[<italic>v</italic>(chosen)], the expected value (in absolute rather than relative terms) of the item that is chosen. In any given state (defined by the values (<italic>v</italic><sub>1</sub><italic>,v</italic><sub>2</sub>)), we assume that is only the value of the chosen item that matters for the decision maker’s payoff, not the value of the item that was available but not chosen; our theory is thus very different from a model of “regret.” We now show explicitly why maximization of the objective criterion stated in the initial submission is equivalent to minimizing expected loss (Equation 59, in Appendix 5 of the revised manuscript). The latter formulation is then useful in our subsequent calculations. But these calculations in no way assume a concern with relative valuations. Please see Appendix 5.</p><disp-quote content-type="editor-comment"><p>2) Experimental/analysis issues.</p><p>a) The model recovery results shown in Figure 3D as well as the fits in Figure 4 rely on models in which the shape of the prior distribution is fixed and equal to the shape of the prior distribution used in experiments 1-2. Is the encoding rule still identifiable if the parameter controlling the shape of the prior is free to vary? The authors show recovery of the α parameter within the DbS model (Figure 5) but not when the encoding rule is unknown. Crucially, the conclusion that DbS outperform the other two encoding rules can be undermined if letting the prior free to vary induces model mimicry. Please examine this possibility. If indeed the different encoding schemes are not falsifiable it should be clearly stated that the conclusions (e.g &quot;we found that humans employ a less optima strategy.&quot;, &quot;allowed us to test the hypothesis&quot; etc) hold under the specific assumption that the prior distribution is fixed.<disp-formula id="equ123"><mml:math id="m123"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>v</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ125"><mml:math id="m125"><mml:mrow><mml:mi>θ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>v</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>sin</mml:mo><mml:mrow><mml:mo form="prefix" stretchy="true">[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mover><mml:mi>v</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mspace width="0.333em"/> <mml:mtext mathvariant="normal">d</mml:mtext></mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ127"><mml:math id="m127"><mml:mrow><mml:mi>θ</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>v</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>sin</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo form="prefix" stretchy="true">[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mspace width="0.222em"/><mml:mo>•</mml:mo><mml:mi>c</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mrow><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mover><mml:mi>v</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>/</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mspace width="0.333em"/> <mml:mtext mathvariant="normal">d</mml:mtext></mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.222em"/></mml:mrow></mml:mrow></mml:math></disp-formula></p></disp-quote><p>Thanks for this observation, this is an excellent suggestion to test whether our competing models are falsifiable. Therefore, we carried out the recovery analyses suggested by the reviewers (i.e., leaving both <italic>α</italic> and <italic>n</italic> as free parameters). We found that it is possible to clearly distinguish DbS from the full-prior knowledge optimal models. We note that the distinction between the optimal models (accuracy and fitness) is more difficult for lower values of <italic>α</italic>, but still possible for the values of <italic>α</italic> = 2 assumed in experiments 1 and 2. This <italic>α</italic> ≈ 0 limiting-case behavior is expected based on our normative results. Recall that the shape of the prior distribution <italic>f</italic>(<italic>v</italic>) used in our experiments is given bywhich means that as <italic>α</italic> approaches 0, <italic>f</italic>(<italic>v</italic>) approaches a uniform distribution. Also recall that the encoding functions for accuracy and fitness maximization are given byandrespectively. This means that as the prior distribution approaches a uniform distribution, the encoding rules will be indistinguishable. Thus, this provides an explanation of why it is more difficult to distinguish the optimal rules from each other as <italic>α</italic> approaches 0 for our prior distribution. Nevertheless, we emphasize that DbS will always be distinguishable from the optimal encoding rules. In addition to correct model identification, the parameters <italic>α</italic> and <italic>n</italic> were successfully recovered for each generating model. These results thus provide clear evidence that, first, the sampling models are falsifiable, and second, DbS is the most likely strategy used by the participants among the tested models. We present the results of the model recovery analyses in Figure 3—figure supplement 3, (in panels a, b and c, the generating models are Accuracy (blue), Reward (red) and DbS (green), respectively).</p><disp-quote content-type="editor-comment"><p>b) Modeling of the adaptation to priors in experiments 1-3 assumes by definition that the prior parameter starts from a higher than the nominal value, and adapts across time with a time-scale that is shared across experiments. Is there indeed need/ evidence for adaptation? Observing the data in Figure 4—figure supplement 2 I can see that the accuracy a) is stable across time-rendering any adaptation process counterintuitive and, b) accuracy in Experiments 1-2 is higher than the accuracy in Experiment 3. Thus, the lower asymptotic α appears to serve the role of lowering overall accuracy. Please 1) superimpose the across time accuracy of the DbS model with prior adaptation on the traces shown in Figure 4—figure supplement 2, in order to see if the model systematically misfits the data by starting with α=2.84. 2) Please compare the fits of the adapting prior model with the a) the fits of a DbS model with just a free α parameter and b) a DbS model with α=2 and n as free parameter. Can these alternatives explain the data more parsimoniously?</p></disp-quote><p>Regarding the first questions in this point, here we would like to clarify that average performance across experiments should not be compared back to back across experiments given that the set of stimuli presented on each trial were selected to roughly match performance across experiments (based on model simulations) while taking into consideration the different encoding functions for each empirical prior distribution. In any case, we now provide the model predictions of the best model alongside the observed performance (new Figure 5—figure supplement 1). However, we agree with the reviewers that evidence for adaptation should be studied more in detail. As a first approach, we performed out-of-sample model comparisons based on the models proposed by the reviewers: (i) adaptive-<italic>α</italic> model, (ii) model with free <italic>α</italic> but non-adapting over time, and (iii) model with <italic>α</italic> = 2. The results of the out-of-sample predictions revealed that the best model was the free-<italic>α</italic> model, followed closely by the adaptive-<italic>α</italic> model (∆LOO = 1.8) and then by fixed-<italic>α</italic> model (∆LOO = 32.6). However, we did not interpret the apparent small difference between the adaptive-<italic>α</italic> and free-<italic>α</italic> model as evidence for lack of adaptation, given that the more complex adaptive model will be strongly penalized after adaptation is stable. That is, if adaptation is occurring, then the adaptive-<italic>α</italic> only provides a better fit for the trials corresponding to the adaptation period. After adaptation the adaptive-<italic>α</italic> model should provide a similar fit than the free-<italic>α</italic> model, however with a larger complexity that will be penalized by model comparison metrics. Therefore, to investigate the presence of adaptation, we took a closer quantitative look at evolution of the fits across trial experience. We computed the average trial-wise predicted Log-Likelihood (by sampling from the hierarchical Bayesian model) and compared the differences of this metric between the competing models and the adaptive model. We hypothesized that if adaptation is taking place, the adaptive-<italic>α</italic> model would have an advantage relative to the free-<italic>α</italic> model at the beginning of the session, with these differences vanishing toward the end. On the other hand, the fixed-<italic>α</italic> should roughly match the adaptive-<italic>α</italic> model at the beginning and then become worse over time, but these differences should stabilize after the end of the adaptation period. This is exactly what we found (Figure 5—figure supplement 2). These results thus provide evidence of adaptation and that the DbS model can parsimoniously capture these effects in a continuous and dynamical manner. These results were added in the Results section and presented in Figure 5—figure supplement 2 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>c) One possible reason why the two conditions did not lead to differences could be that – after doing one condition for two days – it might have been impossible for the participants to adjust their &quot;habit-like&quot; behavior to a new incentive rule. This could be checked by analyzing the first half of the task in a between-subject manner.</p></disp-quote><p>We agree with the reviewers that a possible reason why the two experimental conditions did not lead to differences could be that, after doing one condition for two days, the participants did not adapt as easily to the new incentive rule. However, note that as the participants did not know of the second condition before carrying it out, they could not adopt compromise strategies. Nevertheless, we fitted the latent-mixture model only to the first condition that was carried out by each participant. We found once again that DbS was the best model explaining the data, irrespective of condition and experimental paradigm. The results of these analyses are presented in Figure 4—figure supplement 7, and reported in the revised manuscript. Therefore, we conclude that the fact that DbS is favored in the results is not an artifact of carrying out two different conditions in the same participants.</p><disp-quote content-type="editor-comment"><p>3) Expository issues.</p><p>a) Provide more intuition for the equivalence between results under different optimization criteria. Do any of these results rely on the asymptotics?</p></disp-quote><p>The exact equivalence that we obtain between the optimal coding rules for mutual information maximization and accuracy maximization depends on our use of an asymptotic approximation. As noted in our response to point 1a, though, we verify that the characterization of the optimal coding rule for accuracy maximization using the asymptotic approximation is not too different from what we find using a flexible numerical solution method in the case of small values of <italic>n</italic> (see Appendix 7). We do not examine the question of how closely the optimal coding rules under these two objectives continue to be similar in the case of small <italic>n</italic>, however. This is because our primary focus is on the difference between the optimal rule for accuracy maximization (when <italic>m</italic> can be unbounded) and DbS. We compare the optimal coding rule for accuracy maximization to the one that would maximize mutual information primarily in order to compare our results to other discussions in the efficient coding literature, and those results are all based on asymptotic approximations. Thus we too only consider the implications of mutual information maximization in the case of an asymptotic approximation of the kind commonly used in the prior literature.</p><disp-quote content-type="editor-comment"><p>b) The clarity of the Introduction can be improved. In the second paragraph, the authors suddenly jump to a discussion of differences between perceptual and preferential choice, but I think it would be more important to first make clear what the overall goal of the work is. The third paragraph is very confusing. Its first sentence is not even a full sentence (a verb is missing at &quot;where only a finite number.…&quot;) and pretty much incomprehensible. Then, the work of Simon Laughlin is discussed, but it is questionable whether this is really the best way to motivate the proposal of a binary encoding system (why not simply saying that neurons provide binary outcomes). The reference to Query Theory in the fourth paragraph remains vague. In a later paragraph, the idea of adaptation to a frequency distribution is introduced without explaining what it actually means (and one reason for this is that DbS is not well explained in the previous paragraph).</p></disp-quote><p>We thank the reviewers for pointing this out. We agree that the Introduction in the initial submission was patchy and fragmented. We have now extensively rewritten the Introduction section, which we believe now better identifies the main focus of our work.</p><disp-quote content-type="editor-comment"><p>c) In the Abstract, the authors should make more clear what the task was about (though we understand that this isn't easy give the word limit). In addition, the word &quot;Here&quot; is used to start two consecutive sentences, and an &quot;a&quot; is missing at &quot;strategy that might be utilized…&quot;.</p></disp-quote><p>We have rewritten the Abstract and also tried to incorporate the reviewers’ suggestion. Given the 150 words limit, it is difficult to describe the details of the task, but we have now made clear that it was a numerosity discrimination task.</p><disp-quote content-type="editor-comment"><p>4) Links to related literature.</p><p>a) Do the results cast doubt on the argument made in the paper by Rustichini et al., 2017, which argues for a coding scheme based on expected utility maximization rather than mutual information?</p></disp-quote><p>Like Rustichini et al., we develop a theory of efficient coding that is based on optimization of the organism’s performance of a particular task (in our case, a discrimination task), rather than taking maximization of mutual information as the objective. (We do also provide some results about mutual information maximization, but this is for purposes of comparison of our theory to other discussions of efficient coding, which often assume that objective.) In this respect, our approach is fundamentally similar to theirs. There are however many differences between the particular problem that they address with their model and the situation that we analyze here. They consider a different decision task, a different class of possible encoding rules, and they also optimize their encoding and decoding rules for a particular prior distribution (that is, in terms of our formalism, they consider only the case in which <italic>m</italic> is unbounded); hence our results cannot be compared in detail to theirs. We find that our subjects do not behave much differently when we change the reward function used to incentivize their choices, and some may feel that this shows that the coding scheme that people use is not optimized to maximize reward. However, we only find that the coding scheme that would maximize expected reward is different in the case of the two incentive schemes when <italic>m</italic> is assumed to be unbounded; when it is instead important to economize on the number of samples from the prior that are used, we believe that DbS is equally efficient in either case, and this is the case that seems to be empirically relevant for our subjects. Thus we do not believe that our results contradict the Rustichini et al. position, though they cannot be said to prove that it must be correct, either.</p><disp-quote content-type="editor-comment"><p>b) Clarify that Equation 9 only corresponds to DbS in the asymptotic limit. The finite sample regime was emphasized in Bhui and Gershman, 2018, in order to explain certain phenomena (such as range effects) that do not follow directly from the CDF encoding function. Instead, that paper showed how these results could be obtained from a smoothed encoding function computed on a small set of samples. Relatedly, please clarify the links to the Bhui and Gershman paper. In particular, how does infomax in this paper (like in Supplementary Note 1) related to infomax in their paper? Also their work is described as adding noise after efficient coding, but this is not the case. The &quot;noise&quot; in that model comes purely from the fact that a finite number of samples are drawn, so that the sample-based CDF only approximates the true CDF.</p></disp-quote><p>This point is addressed in response to point 1a.</p><disp-quote content-type="editor-comment"><p>c) The authors should discuss (and scrutinize) their empirical findings a bit more in the context of other studies that have compared perceptual and preferential decisions, in particular Dutilh and Rieskamp, 2015, who studied a quite similar task (choosing based on the number of dots with a perceptual vs. a preferential incentive rule). Here, it was found that decisions were slowest for the most difficult trials in the perceptual condition but not in the preferential condition. The question is whether there are any interesting, related response time differences between the two conditions in the current task (because if the number of dots on the left and right is very similar, one should not think too long about it if the reward depends on the number of dots [preferential], but one would need to think for a long time about it to decipher which side has more dots [perceptual]).</p></disp-quote><p>The reviewers are right that recent studies have investigated behavior in tasks where perceptual and preferential decisions are also made in paradigms with identical visual stimuli. In these tasks, investigators have reported differences in behavior, in particular in the reaction times of the responses, possibly reflecting differences in behavioral strategies between perceptual and value-based decisions. Therefore, we investigated whether this was also the case in our data. We found that reaction times change as a function of the different performance metrics nearly as expected given the proportion of correct responses (Figure 4—figure supplement 5). However, we found that reaction times did not differ between experimental conditions for any of the different performance assessments considered here. While we agree that this result might appear surprising, it further supports the idea that subjects in our task were in fact using the DbS strategy irrespective of behavioral goals. These results are reported in the main text, and presented in Figure 4—figure supplement 5. We acknowledge that the study of efficiency in the context of free reaction time tasks is an interesting topic of research. However, we believe that it falls out of the scope of this study, but it is certainly a question that will be important to address in future work.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Revisions for this paper:</p><p>1) Having established the identifiability of the α parameter under the DbS model, it seems imperative to fit Experiment 3 using α as a free parameter and omitting the adaptation mechanism (this has been done in the revision but these results are used to examine whether there is adaptation or not, rather than to actually examine if the fitted prior differs between experiments 1-2 and 3). In other words, Figure 5A can be expanded to include the α fits from Experiment 3. This exercise can address whether there is indeed a change to the shape of the prior across experiments, which is a pivotal component of the proposed framework relative to alternative frameworks that assume complex representational non-linearities without sensitivity to the prior. The results in Figure 5C show that α in Experiment 3 converges to a lower asymptotic value. However, imposing an adaptation process, especially when there is no strong support for such process, can obscure the interpretation of the fits. Furthermore, I remain skeptical about the claim that there is dynamical adaptation within each experiment: i) if anything, the new analyses show that the &quot;free α&quot; model provides a better goodness of fit, and ii) Figures 4—figure supplement 2 and Figure 5—figure supplement 1 show no obvious dynamical trends in behavior. How does the adaptation manifest itself in the data? Figure 5—figure supplement 2 hints toward an early period in which the &quot;free-α&quot; fits worse than the dynamic α model (up to ~150 trials). Perhaps modeling this discrepancy explicitly (e.g. two α parameters for early and late trials, respectively) would suffice.</p></disp-quote><p>We followed the reviewer’s suggestion. In order to test whether the results obtained in Figure 5C were an artifact of the parametric assumption in the adaptation parameter, we ran a model using only the first 150 and last 350 trials of each daily session. The α parameter was allowed to vary between the first and last sets of daily trials and between Experiments 1–2 and Experiment 3. The results are shown (purple: Experiments 1-2, orange: Experiment 3). Each bar represents the mean value of the α parameter for a combination of experiments and set of daily trials. The α parameter is smaller at the end of Experiment 3 relative to the beginning of Experiments 1–2 and Experiment 3. In addition, the α parameter is lower in the last set of daily trials in Experiment 3 than in the last set of daily trials in Experiments 1–2. These results virtually reproduce the results obtained with the parametrization assumed in Figure 5C and therefore confirm that they are not artifacts of the parametric assumption. These results are now included in the new Figure 5D.</p><p>In order to allow a fair comparison with the results of the adaptation analyses presented in Figure 5C, in this model, we did not allow <italic>n</italic> to freely change for each condition. Therefore, a potential concern is that the results might be an artifact of changes in <italic>n</italic>, which could for example change with the engagement of the participants across the session. Given that we already demonstrated that both parameters <italic>n</italic> and <italic>α</italic> are identifiable, we fitted the same model as in Figure 5D, however this time we allowed <italic>n</italic> to be a free parameter alongside <italic>α</italic>. We found that the results obtained in Figure 5d remained virtually unchanged (see new Figure 5—figure supplement 3A), in addition to the result that the resource parameter <italic>n</italic> remained virtually identical across the whole session (see new Figure 5—figure supplement 3B). Altogether, these results provide further evidence of adaptation, highlighting the fact that the DbS model can parsimoniously capture adaptation to contextual changes in a continuous and dynamical manner. The results of these analyses are now presented in the revised manuscript</p></body></sub-article></article>