<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83382</article-id><article-id pub-id-type="doi">10.7554/eLife.83382</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Experience transforms crossmodal object representations in the anterior temporal lobes</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-290996"><name><surname>Li</surname><given-names>Aedan Yue</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0580-4676</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-292752"><name><surname>Ladyka-Wojcik</surname><given-names>Natalia</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1218-0080</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-292753"><name><surname>Qazilbash</surname><given-names>Heba</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-292754"><name><surname>Golestani</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-138026"><name><surname>Walther</surname><given-names>Dirk B</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8585-9858</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96981"><name><surname>Martin</surname><given-names>Chris B</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7014-4371</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-32294"><name><surname>Barense</surname><given-names>Morgan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution>, <institution>University of Toronto</institution>, <addr-line><named-content content-type="city">Toronto</named-content></addr-line>, <country>Canada</country></aff><aff id="aff2"><institution content-type="dept">Department of Physics and Astronomy</institution>, <institution>University of Calgary</institution>, <addr-line><named-content content-type="city">Toronto</named-content></addr-line>, <country>Canada</country></aff><aff id="aff3"><institution content-type="dept">Department of Psychology</institution>, <institution>Florida State University</institution>, <addr-line><named-content content-type="city">Tallahasse</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-76860"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing editor</role><aff><institution>Radboud University Nijmegen</institution>, <country>Netherlands</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>aedanyue.li@utoronto.ca</email> (AL);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>22</day><month>04</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e83382</elocation-id><history><date date-type="received"><day>09</day><month>09</month><year>2022</year></date><date date-type="accepted"><day>19</day><month>04</month><year>2024</year></date></history><permissions><copyright-statement>Â© 2024, Li et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Li et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83382-v2.pdf"/><abstract><p>Combining information from multiple senses is essential to object recognition, core to the ability to learn concepts, make new inferences, and generalize across distinct entities. Yet how the mind combines sensory input into coherent crossmodal representations - the <italic>crossmodal binding problem</italic> - remains poorly understood. Here, we applied multi-echo fMRI across a four-day paradigm, in which participants learned 3-dimensional crossmodal representations created from well-characterized unimodal visual shape and sound features. Our novel paradigm decoupled the learned crossmodal object representations from their baseline unimodal shapes and sounds, thus allowing us to track the emergence of crossmodal object representations as they were learned by healthy adults. Critically, we found that two anterior temporal lobe structures - temporal pole and perirhinal cortex - differentiated learned from non-learned crossmodal objects, even when controlling for the unimodal features that composed those objects. These results provide evidence for integrated crossmodal object representations in the anterior temporal lobes that were different from the representations for the unimodal features. Furthermore, we found that perirhinal cortex representations were by default biased towards visual shape, but this initial visual bias was attenuated by crossmodal learning. Thus, crossmodal learning transformed perirhinal representations such that they were no longer predominantly grounded in the visual modality, which may be a mechanism by which object concepts gain their abstraction.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Alexander Graham Bell Canada Graduate Scholarship-Doctoral</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Aedan Yue</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Discovery Grant (RGPIN-2020-05747)</award-id><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><award-id>Scholar Award</award-id><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001804</institution-id><institution>Canada Research Chairs</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100015668</institution-id><institution>Ontario Ministry of Research and Innovation</institution></institution-wrap></funding-source><award-id>Early Researcher Award</award-id><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf2"><p>Morgan Barense, Reviewing editor, <italic>eLife</italic>.</p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experiments described in this study were approved by the University of Toronto Ethics Review Board: 37590. Informed consent was obtained for all participants in the study.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Anonymized data are available on the Open Science Framework: https://osf.io/vq4wj/.Univariate maps are available on NeuroVault: https://neurovault.org/collections/LFDCGMAY/</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Li AY</collab><collab>Ladyka-Wojcik N</collab><collab>Qazilbash H</collab><collab>Golestani A</collab><collab>Bernhardt-Walther D</collab><collab>Martin CB</collab><collab>Barense</collab><collab>MD</collab></person-group><year iso-8601-date="2022">2022</year><source>Crossmodal Object Representations Rely on Integrative Coding</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/vq4wj/">https://osf.io/vq4wj/</ext-link><comment>Open Science Framework, https://doi.org/10.17605/OSF.IO/VQ4WJ</comment></element-citation><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Li AY</collab><collab>Ladyka-Wojcik N</collab><collab>Qazilbash H</collab><collab>Golestani A</collab><collab>Bernhardt-Walther D</collab><collab>Martin CB</collab><collab>Barense</collab><collab>MD</collab></person-group><year iso-8601-date="2022">2022</year><source>https://neurovault.org/collections/LFDCGMAY/</source><ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/LFDCGMAY/">https://neurovault.org/collections/LFDCGMAY/</ext-link><comment>NeuroVault, https://identifiers.org/neurovault.collection:12807</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-83382-supp-v2.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>