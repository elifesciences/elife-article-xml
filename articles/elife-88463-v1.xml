<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">88463</article-id><article-id pub-id-type="doi">10.7554/eLife.88463</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88463.4</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Microbiology and Infectious Disease</subject></subj-group></article-categories><title-group><article-title>Tools and methods for high-throughput single-cell imaging with the mother machine</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-312526"><name><surname>Thiermann</surname><given-names>Ryan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0348-4181</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-314854"><name><surname>Sandler</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-314855"><name><surname>Ahir</surname><given-names>Gursharan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-314856"><name><surname>Sauls</surname><given-names>John T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-314857"><name><surname>Schroeder</surname><given-names>Jeremy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8829-8494</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-314858"><name><surname>Brown</surname><given-names>Steven</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-188040"><name><surname>Le Treut</surname><given-names>Guillaume</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-87453"><name><surname>Si</surname><given-names>Fangwei</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-188041"><name><surname>Li</surname><given-names>Dongyang</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-7911"><name><surname>Wang</surname><given-names>Jue D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1503-170X</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-29935"><name><surname>Jun</surname><given-names>Suckjoon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0139-4297</contrib-id><email>s2jun@ucsd.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>Department of Physics, University of California, San Diego</institution></institution-wrap><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jmfr291</institution-id><institution>Department of Biological Chemistry, University of Michigan Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Ann Arbor</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00knt4f32</institution-id><institution>Chan Zuckerberg Biohub</institution></institution-wrap><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Department of Physics, Carnegie Mellon University</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Division of Biology and Biological Engineering, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y2jtd41</institution-id><institution>Department of Bacteriology, University of Wisconsin–Madison</institution></institution-wrap><addr-line><named-content content-type="city">Madison</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Salman</surname><given-names>Hanna</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>University of Pittsburgh</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>18</day><month>04</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP88463</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-05-11"><day>11</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-04-07"><day>07</day><month>04</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.27.534286"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-07-26"><day>26</day><month>07</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88463.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-06"><day>06</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88463.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-18"><day>18</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88463.3"/></event></pub-history><permissions><copyright-statement>© 2023, Thiermann, Sandler, Ahir et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Thiermann, Sandler, Ahir et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-88463-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-88463-figures-v1.pdf"/><abstract><p>Despite much progress, image processing remains a significant bottleneck for high-throughput analysis of microscopy data. One popular platform for single-cell time-lapse imaging is the mother machine, which enables long-term tracking of microbial cells under precisely controlled growth conditions. While several mother machine image analysis pipelines have been developed in the past several years, adoption by a non-expert audience remains a challenge. To fill this gap, we implemented our own software, MM3, as a plugin for the multidimensional image viewer napari. napari-MM3 is a complete and modular image analysis pipeline for mother machine data, which takes advantage of the high-level interactivity of napari. Here, we give an overview of napari-MM3 and test it against several well-designed and widely used image analysis pipelines, including BACMMAN and DeLTA. Researchers often analyze mother machine data with custom scripts using varied image analysis methods, but a quantitative comparison of the output of different pipelines has been lacking. To this end, we show that key single-cell physiological parameter correlations and distributions are robust to the choice of analysis method. However, we also find that small changes in thresholding parameters can systematically alter parameters extracted from single-cell imaging experiments. Moreover, we explicitly show that in deep learning-based segmentation, ‘what you put is what you get’ (WYPIWYG) – that is, pixel-level variation in training data for cell segmentation can propagate to the model output and bias spatial and temporal measurements. Finally, while the primary purpose of this work is to introduce the image analysis software that we have developed over the last decade in our lab, we also provide information for those who want to implement mother machine-based high-throughput imaging and analysis methods in their research.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>microfluidics</kwd><kwd>mother machine</kwd><kwd>image analysis</kwd><kwd>microbiology</kwd><kwd>bacterial physiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>E. coli</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014989</institution-id><institution>Chan Zuckerberg Initiative</institution></institution-wrap></funding-source><award-id>NPA-0000000033</award-id><principal-award-recipient><name><surname>Jun</surname><given-names>Suckjoon</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000057</institution-id><institution>National Institute of General Medical Sciences</institution></institution-wrap></funding-source><award-id>R35GM139622</award-id><principal-award-recipient><name><surname>Jun</surname><given-names>Suckjoon</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>MCB-2016090</award-id><principal-award-recipient><name><surname>Jun</surname><given-names>Suckjoon</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014989</institution-id><institution>Chan Zuckerberg Initiative</institution></institution-wrap></funding-source><award-id>DAF2021-239849</award-id><principal-award-recipient><name><surname>Jun</surname><given-names>Suckjoon</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014989</institution-id><institution>Chan Zuckerberg Initiative</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.37921/244803gjbgup</award-id><principal-award-recipient><name><surname>Jun</surname><given-names>Suckjoon</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A user-friendly pipeline for analysis of time-lapse imaging data from the microfluidic mother machine.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The mother machine (<xref ref-type="bibr" rid="bib64">Wang et al., 2010</xref>) is a popular microfluidic platform for long-term, high-throughput imaging of single cells. It has been widely adopted as a standard for long-term imaging of bacteria such as <italic>Escherichia coli</italic> and <italic>Bacillus subtilis</italic> (<xref ref-type="bibr" rid="bib44">Sauls et al., 2019a</xref>), as well as the eukaryote <italic>Schizosaccharomyces pombe</italic> (<xref ref-type="bibr" rid="bib28">Nakaoka and Wakamoto, 2017</xref>; <xref ref-type="bibr" rid="bib51">Spivey et al., 2017</xref>). In the mother machine, thousands of single cells are trapped in one-ended growth channels that open into a central trench (<xref ref-type="fig" rid="fig1">Figure 1-1.1</xref>). The cells at the end of the growth channels (mother cells) grow and divide over hundreds of generations, while their progeny are successively flushed out of the device (<xref ref-type="fig" rid="fig1">Figure 1-1.2, 1.3</xref>). Data gathered from the mother machine has brought critical insight into diverse domains such as aging (<xref ref-type="bibr" rid="bib64">Wang et al., 2010</xref>), single-cell physiology (<xref ref-type="bibr" rid="bib18">Jun et al., 2018</xref>), starvation adaptation (<xref ref-type="bibr" rid="bib6">Bakshi et al., 2021</xref>), antibiotic persistence (<xref ref-type="bibr" rid="bib20">Kaplan et al., 2021</xref>), cell differentiation (<xref ref-type="bibr" rid="bib42">Russell et al., 2017</xref>), and the mechanics of cell wall growth (<xref ref-type="bibr" rid="bib2">Amir et al., 2014</xref>; <xref ref-type="fig" rid="fig1">Figure 1-1.4</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Mother machine workflow, schematic, and applications.</title><p>(<bold>1.1</bold>) Mother machine schematic. Growth channels flank a central flow cell that supplies fresh media and whisks away daughter cells. In a typical experiment, numerous fields of view (FOVs) are imaged for several hours. (<bold>1.2</bold>) Fluorescence images of <italic>E. coli</italic> strains expressing cytoplasmic YFP (<xref ref-type="bibr" rid="bib64">Wang et al., 2010</xref>) (left) and markers for the replisome protein DnaN and division protein FtsZ (right) (<xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>). (<bold>1.3</bold>) The mother machine setup allows long-term monitoring of the old-pole mother cell lineage (<xref ref-type="bibr" rid="bib64">Wang et al., 2010</xref>) and has other versatile applications, including (<bold>1.4</bold>) the study of the mechanical properties of bacterial cells by applying controlled Stokes forces (<xref ref-type="bibr" rid="bib2">Amir et al., 2014</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Inexpensive fabrication of cell loader with 3D printing.</title><p>An inexpensive device for loading cells into the mother machine. The construction involves 3D printing a custom holder/rotor for a 50-mm WillCo dish, on which a mother machine is attached. The holder is printed in three parts (two blades and a central base) to account for 3D printers with small printing areas. This piece is then assembled and secured to a Honeywell fan from which the original blade has been removed. CAD files and details of the fan centrifuge construction are available at <xref ref-type="bibr" rid="bib58">Thiermann, 2023</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig1-figsupp1-v1.tif"/></fig></fig-group><p>Despite the progress in imaging techniques and microfluidics, image processing remains a major bottleneck in the analysis pipelines. The unique structure of the mother machine device enables precise control of growth conditions and long-term tracking of cells, to the degree that cannot be achieved by traditional tracking of cells in microcolonies (<xref ref-type="bibr" rid="bib52">Stewart et al., 2005</xref>). However, automated image processing is essential to process the large amounts of data generated by these high-throughput experiments. In addition, the unique structure of the mother machine device requires a specialized workflow to select and track individual growth channels. As experimentalists often need to extract precise statistics over multiple generations or observe rare events, the analysis workflow must be modular to allow inspection and curation of intermediate results. To meet these needs, numerous mother machine-specific image analysis packages have been introduced in the last few years (<xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Smith et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>), in addition to general image analysis packages adaptable to the mother machine workflow (<xref ref-type="bibr" rid="bib54">Stylianidou et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Panigrahi et al., 2021</xref>; <xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Spahn et al., 2022</xref>; <xref ref-type="bibr" rid="bib46">Schwartz et al., 2019</xref>). Much recent work has been catalyzed by advances in biomedical image analysis with deep convolutional neural networks (CNNs), particularly the U-Net architecture (<xref ref-type="bibr" rid="bib41">Ronneberger et al., 2015</xref>). Many of these tools (<xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>) have been designed with ease-of-use and accessibility in mind. However, they can still present a steep learning curve for first-time users. In addition, as the outputs of these pipelines are often used by researchers to derive biological principles based on correlations, it is important to understand the limitations of and differences between different image analysis methods.</p><p>This article consists of three parts. First, for first-time users, we provide a brief walkthrough on implementing the mother machine in research (<xref ref-type="box" rid="box1">Box 1</xref>), including how to duplicate microfluidic devices at no cost using epoxy replicas and troubleshoot common image analysis problems. Next, we introduce MM3 (<xref ref-type="bibr" rid="bib30">Napari hub, 2023</xref>), a fast and interactive image analysis pipeline for mother machine experiments that we have developed and used internally for over a decade. Our latest version is a Python plugin for the multidimensional image viewer napari (<xref ref-type="bibr" rid="bib29">napari contributors, 2023</xref>). Finally, we compare the accessibility, performance, and robustness of various current image analysis platforms. In order to trust analysis results, researchers should understand the limitations of their chosen method. With this in mind, we show that ‘what you put is what you get’: both classical and deep learning-based segmentation methods are highly sensitive to user-determined threshold values. As exact cell boundaries may be difficult to distinguish by eye, these values are difficult to set definitively, and can systematically alter the output of the analysis. Fortunately, we find that key single-cell physiological parameter correlations and distributions are robust to the choice of analysis method. However, interpreting and comparing the results of different analyses require care.</p><boxed-text id="box1"><label>Box 1.</label><caption><title>Mother machine experimental workflow.</title></caption><p>Despite the well-appreciated power of single-cell time-lapse imaging approaches, the potential user base remains much greater than the number of researchers directly benefiting from the technology. A primary reason for this discrepancy between demand and actual adoption is the perceived cost in time and resources of investment in the required core technology: microfluidics and high-throughput image analysis. Until a few years ago, setting up a typical microfluidic system for the first time took several years of training and trial-and-error, along with significant resources, for most individual labs.</p><p>Running a mother machine experiment requires the following steps: (1) fabricating a mold for the device, (2) assembling the device, (3) performing time-lapse microscopy, and (4) analyzing the images to extract time traces and statistics. To our knowledge, steps (1) and (4) have been the primary bottlenecks for most groups. Here, we give a brief overview of the experimental workflow. We refer interested readers to our previous review article on single-cell physiology (<xref ref-type="bibr" rid="bib57">Taheri-Araghi et al., 2015b</xref>), along with other recent reviews (<xref ref-type="bibr" rid="bib1">Allard et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Potvin-Trottier et al., 2018</xref>) and published protocols (<xref ref-type="bibr" rid="bib9">Cabeen and Losick, 2018</xref>), for a more extensive guide to single-cell imaging techniques.<bold>Device design and fabrication</bold>. In the original mother machine design (<xref ref-type="bibr" rid="bib64">Wang et al., 2010</xref>), narrow channels trap bacterial cells perpendicular to a larger main trench through which fresh medium flows (<xref ref-type="fig" rid="fig1">Figure 1-1.4</xref>). Several constraints apply to the design of the device. The height and width of the channels should match the dimensions of the organism under study. The channels must be large enough to facilitate the loading of the cells and allow for fast diffusion of nutrients to mother cells at the channel ends. If the channels are too deep, cells may move out of focus and potentially overlap in the <italic>z</italic>-direction, both of which impede accurate segmentation. Similarly, if channels are too wide, cells may not grow in a single file, complicating segmentation and tracking. Longer trenches will retain cells longer and allow more cells to be tracked per channel. The prohibitive cost of mold fabrication in clean room facilities has been a bottleneck to distributing microfluidic devices. We resolved this problem using an epoxy-based fabrication technique (<xref ref-type="bibr" rid="bib19">Kamande et al., 2015</xref>), allowing us to easily and cheaply create replicative molds (<xref ref-type="fig" rid="box1fig1">Box 1—figure 1</xref>). Once the first microfluidic device is fabricated in the clean room, the epoxy duplication method allows us to reliably create and distribute high-fidelity device molds at a fraction of the cost of the initial fabrication. Undergraduate students in our lab routinely perform this procedure. To assist new users of the mother machine, we include a detailed procedure for the duplication method at <xref ref-type="bibr" rid="bib58">Thiermann, 2023</xref>.</p><p><bold>Experiment setup</bold>. The first step of making the mother machine device is to pour PDMS (polydimethylsiloxane) onto a master mold, cure it, and remove it from the mold. Holes are punched in the cut devices at the inlet and outlet of the central channel to connect tubing for fresh medium (inlet) and waste removal (outlet) before plasma treatment (<xref ref-type="fig" rid="fig1">Figure 1-1.1</xref>). Plasma treatment covalently bonds the PDMS device to a glass cover slide or dish to be mounted on the microscope. BSA (bovine serum albumin) passed through the device passivates the surface. In our setup, we load cells to the growth channels in the device via a custom centrifuge (<xref ref-type="bibr" rid="bib58">Thiermann, 2023</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Growth medium is passed through the device using a syringe pump. The medium flow should be fast enough to clear dead cells or biofilms in the device, but slow enough that the device does not delaminate. Mounting the device on an inverted microscope requires a custom stage insert for long-term imaging. The microscope temperature must be controlled tightly.</p><p><bold>Data analysis</bold>. Most mother machine image analysis workflows share the following steps: preprocessing the acquired images, including identification and cropping of cell traps, cell segmentation, and cell tracking. Cell segmentation is the most difficult and crucial step, as adjacent cells must be separated from each other and from device features. After accurate segmentation, the one-dimensional structure of the mother machine – which constrains the cells to move only in one direction along the length of the trap without bypassing each other – makes cell tracking relatively simple.</p><fig position="float" id="box1fig1"><label>Box 1—figure 1.</label><caption><title>Duplication and distribution of mother machine devices with epoxy molds.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-box1-fig1-v1.tif"/></fig></boxed-text></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Mother machine image analysis with napari-MM3</title><p>Analysis of time-lapse imaging experiments requires dedicated software due to the sheer volume of data produced. For instance, an experiment tracking aging might require imaging 50 fields of view (FOVs; <xref ref-type="fig" rid="fig1">Figure 1-1.1</xref>) every 2 min for a week, producing a quarter of a million images comprising hundreds of gigabytes of data. While the experimental methods for mother machine experiments have become increasingly accessible, image analysis tools have lagged behind. Typically, labs using the mother machine have developed their own customized analysis pipelines. Many available tools require programming experience, familiarity with command line tools, and extensive knowledge of image analysis methods. They are also often fine-tuned for specific experimental setups and difficult for the average user to adapt. Finally, existing workflows frequently require users to move between multiple interfaces such as ImageJ, MATLAB, the command line, Python scripting, and Jupyter notebooks. Newer deep learning approaches are more versatile than traditional computer vision methods. Still, they bring new issues for novices: users may need to construct their own training data and train a model, requiring a new set of tools and technical expertise, and manual annotation of training data is susceptible to human error and bias.</p><p>These considerations guided us in the development of our in-house analysis tool. In building MM3, we sought to provide modularity and extensive interactivity while minimizing unnecessary user intervention. MM3 aims to be a complete and flexible solution for mother machine image analysis, taking raw images, and producing readily graphable cell data, while accommodating both machine learning-based and traditional computer vision techniques. It supports phase contrast and fluorescence images, and has been tested with different species (bacteria <italic>E. coli</italic> and <italic>B. subtilis</italic>, yeast <italic>S. pombe</italic>), mother machine designs, and optical configurations. The modular pipeline architecture allows flexible use of mid-stream outputs and straightforward troubleshooting (for instance, while <italic>M. mycoides</italic> is too small to segment with traditional microscopy methods (<xref ref-type="bibr" rid="bib40">Rideau et al., 2022</xref>), we were able to obtain growth rate measurements by running the first half of the pipeline).</p><p>MM3 reflects the culmination of several iterations of our in-house mother machine analysis software developed over the past decade. Before MM3, we developed our image analysis pipeline in C++ (<xref ref-type="bibr" rid="bib64">Wang et al., 2010</xref>) and MATLAB (<xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>). Eventually, Python became enormously popular, and we began MM3 as a set of Python scripts run from the command line (<xref ref-type="bibr" rid="bib45">Sauls et al., 2019b</xref>). However, the command-line-based interface had several drawbacks. The interface was more difficult for users unfamiliar with the command line or programming. It also had limited interactivity. As a result, troubleshooting was difficult and required modifying the source code to display image output at intermediate steps or manually inspecting output files in ImageJ. This made the user repeatedly move back and forth between different windows and applications, slowing the analysis.</p><p>These drawbacks motivated us to convert MM3 into a plug-in for the Python-based interactive image viewer napari (<xref ref-type="bibr" rid="bib29">napari contributors, 2023</xref>). napari provides an <italic>N</italic>-dimensional display ideal for visualizing multichannel time-lapse data. It offers built-in annotation tools and label layers to compare and annotate segmentation masks and tracking labels. It also provides a Python interpreter, allowing users to move easily between the viewer interface and the underlying data objects. For the best usability, we designed the napari-MM3 plug-in to allow the user to run the entire pipeline without leaving the napari interface.</p><p>Image analysis via napari-MM3 consists of four steps (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><list list-type="order"><list-item><p>Crop raw images and compile them into stacks corresponding to individual growth channels.</p></list-item><list-item><p>Choose channel stacks to be (1) analyzed, (2) used as templates for background subtraction, or (3) ignored.</p></list-item><list-item><p>Segment cells.</p></list-item><list-item><p>Construct cell lineages. napari-MM3 treats individual cells in the lineages as objects that can be plotted directly or converted to another data format.</p></list-item></list><p>We elaborate on these steps as follows.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>MM3 workflow and example images.</title><p>(<bold>2.1</bold>) The MM3 image analysis pipeline takes raw mother machine images and produces cell objects. Processes (rounded rectangles) are modular; multiple methods are provided for each. (<bold>2.2</bold>) Example images from the processing of one growth channel in a single field of view (FOV). The growth channel is first identified, cropped, and compiled in time. All cells are segmented (colored regions). Lineages are tracked by linking segments in time to determine growth and division (solid and dashed lines, respectively), creating cell objects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig2-v1.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>napari-MM3 interface.</title><p>The napari viewer enables interactive analysis of mother machine data with real-time feedback and fast debugging. Raw data shown is from MG1655 background <italic>E coli</italic> expressing the fluorescence protein YPet fused to the replisome protein DnaN (<xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig3-v1.tif"/></fig><sec id="s2-1-1"><title>Channel detection and curation</title><p>The first section of the napari-MM3 pipeline takes in raw micrographs and returns image stacks corresponding to one growth channel through time. napari-MM3 detects channels using a wavelet transform and then aligns them over time to correct for stage drift and vibration. The aligned growth channels are saved as unique image stacks with all time points for a given growth channel and color channel. As not all growth channels contain cells, napari-MM3 auto-detects channels as full or empty based on the time correlation of the y-profile of the growth channel. The auto-detected growth channels and their classifications are then displayed in the napari viewer for the user to inspect and modify as needed.</p></sec><sec id="s2-1-2"><title>Cell segmentation</title><p>napari-MM3 offers two methods for cell segmentation, one using traditional computer vision techniques and the other using deep learning. The non-learning method utilizes Otsu’s method to apply a binary threshold to separate cell objects from the background. It then labels the isolated cells and uses a random walker algorithm (<xref ref-type="bibr" rid="bib13">Grady, 2006</xref>) to fill out the cell boundaries. This method is fast but optimized for specific mother machine designs and phase contrast imaging of bacteria. It also requires accurate background subtraction of phase contrast images (<xref ref-type="box" rid="box2">Box 2</xref>), to ensure that the presence of the channel border does not interfere with cell detection. The supervised learning method uses a CNN with the U-Net architecture (<xref ref-type="bibr" rid="bib41">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>; <xref ref-type="bibr" rid="bib11">Falk et al., 2019</xref>). The napari viewer can be used to construct training data, with the option to import existing Otsu or U-Net segmentation output as a template. The neural net can then be trained directly from napari, with the option to check the performance of the model in the napari viewer after successive rounds of training.</p><boxed-text id="box2"><label>Box 2.</label><caption><title>Segmentation via Otsu’s method.</title></caption><p>The Otsu segmentation method first aligns the growth channel of interest with an empty background channel by computing the orientation that maximizes the pixel-wise cross-correlation (<xref ref-type="fig" rid="box2fig1">Box 2—figure 1</xref>). The empty channel is then subtracted from the full channel, and the image is inverted. This background subtraction step is essential, as it removes the dark image of the PDMS device, which will otherwise interfere with segmenting the (dark) cells. Otsu’s method (<xref ref-type="bibr" rid="bib36">Otsu, 1979</xref>) is applied to find the binary threshold value that maximizes the inter-region variance. We then apply a Euclidean distance transform, wherein each pixel is labeled with its distance to the dark region. The image is thresholded again, and a morphological opening is applied to erode links between regions. Small objects and objects touching the image border are removed. Each region is labeled, and the labels are used to seed a random walker algorithm (<xref ref-type="bibr" rid="bib13">Grady, 2006</xref>) on the original image.</p><fig position="float" id="box2fig1"><label>Box 2—figure 1.</label><caption><title>Background subtraction and segmentation via Otsu's method and random walker algorithm.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-box2-fig1-v1.tif"/></fig></boxed-text></sec><sec id="s2-1-3"><title>Cell tracking and lineage reconstruction</title><p>Finally, napari-MM3 links segmented cells in time to define a lineage of cell objects, using a simple decision tree based on a priori knowledge of binary fission and the mother machine. Tracking produces a dictionary of cell objects containing relevant information derived from the cell segments, including the cell lengths and volumes over time, cell elongation rate, and generation time. Plotting and additional analysis can then be done with the user’s tool of choice. Statistics can be directly extracted from the cell objects, or the cell objects can be converted into a.csv file, a pandas DataFrame, or a MATLAB structure. We provide a <ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/napari-mm3">Jupyter notebook</ext-link> demonstrating this analysis at <xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref> (copy archived at <xref ref-type="bibr" rid="bib62">Thiermann et al., 2024b</xref>).</p></sec></sec><sec id="s2-2"><title>Additional features and future extensions</title><p>napari-MM3 offers several additional modules supplemental to the main processing pipeline, including methods for fluorescence image analysis and U-Net training data construction and model training. Integrated fluorescence signal and fluorescence per cell area and volume for each time point can be extracted using the ‘Colors’ module. napari-MM3 also includes a module for the detection and tracking of fluorescent spots or ‘foci’. For example, we have used it to track fluorescently labeled replisome machinery in bacteria in order to measure the timing and synchrony of DNA replication initiation (<xref ref-type="bibr" rid="bib44">Sauls et al., 2019a</xref>; <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>). Lastly, U-Net segmentation training data can be constructed by manual annotation of raw images in the napari viewer. napari-MM3 offers the option to construct training data with existing Otsu or U-Net segmentation data as a template. This allows the user to iteratively train a model, correct mistakes in its output, and use the modified output as input for the next round of training. We also provide a Jupyter notebook covering training data construction and model training at <xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref>.</p><p>Going forward, we plan to add support for additional segmentation and tracking modalities (<xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Ollion and Ollion, 2020</xref>). We will also incorporate support for additional organisms such as the budding yeast <italic>S. cerevisiae</italic>. Finally, we plan to take advantage of napari’s interactive display to add interactive data visualization and plotting.</p></sec><sec id="s2-3"><title>Performance test of napari-MM3</title><p>To evaluate the speed of napari-MM3, we timed the processing of a typical dataset (<xref ref-type="table" rid="table1">Table 1</xref>). Using consumer-grade hardware, a single-channel stack consisting of several hundred time frames can be processed in less than 5 s, and a typical experiment consisting of 25 GB of imaging data can be processed in under an hour. These metrics are on par with those reported by other recently published mother machine software (<xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Banerjee et al., 2020</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Performance metrics for napari-MM3.</title><p>Processing times were measured on an iMac with a 3.6-GHz 10-Core Intel Core i9 processor with 64 GB of RAM and an AMD Radeon Pro 5500 XT 8 GB GPU. Tensorflow was configured to use the AMD GPU according to <xref ref-type="bibr" rid="bib3">Apple Inc, 2023</xref>. The GPU was used in U-Net training and segmentation steps. The dataset analyzed is from <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref> and consists of 26 GB of raw image data (12 hr, 262 time frames, 2 imaging planes, 34 fields of view [FOVs], and ~35 growth channels per FOV). Note that while the Otsu segmentation method is slightly faster than the U-Net, it also requires a background subtraction step, such that the total runtimes of the two methods are comparable.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Channel detection</th><th align="left" valign="bottom">Background subtraction</th><th align="left" valign="bottom">Segmentation (Otsu)</th><th align="left" valign="bottom">Segmentation (U-Net)</th><th align="left" valign="bottom">Tracking</th><th align="left" valign="bottom">Total (Otsu)</th><th align="left" valign="bottom">Total (U-Net)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Frame processing time</td><td align="left" valign="bottom">N/A</td><td align="char" char="." valign="bottom">2 ms</td><td align="char" char="." valign="bottom">4 ms</td><td align="char" char="." valign="bottom">5.3 ms</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">N/A</td></tr><tr><td align="left" valign="bottom">Channel stack processing time (262 time frames)</td><td align="left" valign="bottom">N/A</td><td align="char" char="." valign="bottom">0.54 s</td><td align="char" char="." valign="bottom">1.14 s</td><td align="char" char="." valign="bottom">1.4 s</td><td align="char" char="." valign="bottom">0.7 s</td><td align="char" char="." valign="bottom">3.1 s</td><td align="char" char="." valign="bottom">2.1 s</td></tr><tr><td align="left" valign="bottom">FOV processing time (35 channels)</td><td align="char" char="." valign="bottom">14.1 s</td><td align="char" char="." valign="bottom">17.5 s</td><td align="char" char="." valign="bottom">36.5 s</td><td align="char" char="." valign="bottom">46 s</td><td align="char" char="." valign="bottom">46.7 s</td><td align="char" char="." valign="bottom">2 min</td><td align="char" char="." valign="bottom">1.7 min</td></tr><tr><td align="left" valign="bottom">Exp. processing time (26 GB, 34 FOVs, ~20,000 cells)</td><td align="char" char="." valign="bottom">3.2 min</td><td align="char" char="." valign="bottom">9.9 min</td><td align="char" char="." valign="bottom">20.6 min</td><td align="char" char="." valign="bottom">26 min</td><td align="char" char="." valign="bottom">26.4 min</td><td align="char" char="." valign="bottom">60 min</td><td align="char" char="." valign="bottom">55 min</td></tr></tbody></table></table-wrap></sec><sec id="s2-4"><title>Testing napari-MM3 on other published datasets</title><p>We tested napari-MM3 on several publicly available mother machine datasets: three from experiments on <italic>E. coli</italic> provided with the mother machine image analysis tools DeLTA, MoMA, and BACMMAN (<xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Jug et al., 2014</xref>) and one from <italic>C. glutamicum</italic> provided with the software molyso (<xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref>). We were able to process all four datasets with minimal adjustments to the default parameter values (Methods). We quantified the performance of MM3 on each dataset by comparing the output of the MM3 segmentation to manually determined ground truth masks from a subset of each dataset (<xref ref-type="table" rid="table2">Table 2</xref>). To evaluate the segmentation quality, we computed the Jaccard index (JI) (<xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>; <xref ref-type="bibr" rid="bib55">Taha and Hanbury, 2015</xref>) at an intersection-over-union (IoU) threshold of 0.6 (Methods). The software performed well on the Ollion et al., Sachs et al., and Jug et al. datasets with JI of 0.98, 0.98, and 1, respectively. Segmentation was notably worse on the Lugagne et al. dataset, with JI of 0.92. However, we observed that most segmentation errors in the Lugagne et al. dataset arose from misclassification of cells near the channel opening, where determining cell boundaries is often more difficult.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Testing napari-MM3 on external datasets.</title><p>Quality of segmentation masks produced by running napari-MM3 on a subset of published datasets from other groups (<xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Jug et al., 2014</xref>). As exact boundaries are difficult to determine by eye, we considered a cell to be correctly segmented if the Intersection over Union of the predicted mask and ground truth mask was greater than 0.6 (Methods). To evaluate the quality of the segmentation, we report the Jaccard index (<xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>; <xref ref-type="bibr" rid="bib55">Taha and Hanbury, 2015</xref>).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Correctly segmented cells</th><th align="left" valign="bottom">False positives</th><th align="left" valign="bottom">False negatives</th><th align="left" valign="bottom">Jaccard index</th></tr></thead><tbody><tr><td align="left" valign="bottom">Ollion et al. (BACMMAN) (<xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>)</td><td align="char" char="." valign="bottom">228</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.98</td></tr><tr><td align="left" valign="bottom">Lugagne et al. (DeLTA) (<xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>)</td><td align="char" char="." valign="bottom">247</td><td align="char" char="." valign="bottom">22</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.92</td></tr><tr><td align="left" valign="bottom">Sachs et al. (molyso) (<xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref>)</td><td align="char" char="." valign="bottom">247</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.98</td></tr><tr><td align="left" valign="bottom">Jug et al. (MoMA) (<xref ref-type="bibr" rid="bib17">Jug et al., 2014</xref>)</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">1</td></tr></tbody></table></table-wrap></sec><sec id="s2-5"><title>Comparison with other image analysis software</title><p>We also tested napari-MM3’s usability and performance against other popular software. We began by surveying a range of existing mother machine image analysis tools (<xref ref-type="table" rid="table3">Table 3</xref>). Some early analysis pipelines used one-dimensional segmentation methods (<xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Jug et al., 2014</xref>), which perform adequately when cells are tightly confined in the growth channels. In recent years, many excellent general-purpose CNN-based cell segmentation tools have also been developed (<xref ref-type="bibr" rid="bib54">Stylianidou et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Panigrahi et al., 2021</xref>; <xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Spahn et al., 2022</xref>; <xref ref-type="bibr" rid="bib53">Stringer et al., 2021</xref>), which may be extended to process mother machine data.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Overview of mother machine image analysis tools.</title><p>A comparison of several published imaging methods. ‘2D’ or ‘1D’ segmentation indicates whether the cells are labeled in an image and analyzed in two dimensions, or projected onto a vertical axis and analyzed in one dimension. Several tools support the use of deep learning (in place of or in addition to classical computer vision techniques).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Software</th><th align="left" valign="bottom">Implementation</th><th align="left" valign="bottom">Segmentation</th><th align="left" valign="bottom">Deep learning support</th></tr></thead><tbody><tr><td align="left" valign="bottom">BACMMAN <xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>/DistNet <xref ref-type="bibr" rid="bib35">Ollion and Ollion, 2020</xref></td><td align="left" valign="bottom">ImageJ plugin</td><td align="char" char="." valign="bottom">2D</td><td align="left" valign="bottom">✓</td></tr><tr><td align="left" valign="bottom">DeLTA <xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref></td><td align="left" valign="bottom">Python package</td><td align="char" char="." valign="bottom">2D</td><td align="left" valign="bottom">✓</td></tr><tr><td align="left" valign="bottom">napari-MM3 <xref ref-type="bibr" rid="bib45">Sauls et al., 2019b</xref>, this work</td><td align="left" valign="bottom">napari plug-in</td><td align="char" char="." valign="bottom">2D</td><td align="left" valign="bottom">✓</td></tr><tr><td align="left" valign="bottom">SAM <xref ref-type="bibr" rid="bib7">Banerjee et al., 2020</xref></td><td align="left" valign="bottom">MATLAB</td><td align="char" char="." valign="bottom">2D</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">MMHelper <xref ref-type="bibr" rid="bib49">Smith et al., 2019</xref></td><td align="left" valign="bottom">ImageJ plugin</td><td align="char" char="." valign="bottom">2D</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">molyso <xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref></td><td align="left" valign="bottom">Python package</td><td align="char" char="." valign="bottom">1D</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">MoMA <xref ref-type="bibr" rid="bib17">Jug et al., 2014</xref></td><td align="left" valign="bottom">ImageJ plugin</td><td align="char" char="." valign="bottom">1D</td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><p>In this work, we only tested mother machine-specific pipelines. We constrained our analysis to DeLTA and BACMMAN, two excellent open-source mother machine-specific pipelines offering 2D segmentation and cell tracking, which are also well documented and actively maintained. BACMMAN (<xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>) performs 2D segmentation via traditional computer vision methods similar to those implemented in napari-MM3 and has recently added support for CNN-based segmentation as well (<xref ref-type="bibr" rid="bib33">Ollion et al., 2013</xref>). DeLTA (<xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>) uses the U-Net architecture for channel detection, cell segmentation, and cell tracking, with a mother machine-specific and general agar pad mode. We used BACMMAN, DeLTA, and napari-MM3 to analyze the same published dataset (<xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>; <xref ref-type="bibr" rid="bib63">Thiermann et al., 2024c</xref>) consisting of <italic>E. coli</italic> MG1655 grown in minimal growth medium (MOPS modified buffer + 0.4% glycerol + 11 amino acids with ~60 min doubling time). Data processed in napari-MM3 was separately segmented with U-Net and traditional computer vision methods. We found that the pre-trained mother machine model provided with DeLTA did not generalize well to our data. However, after training a new model with representative data, we achieved accurate segmentation.</p><p>We compared the distributions and correlations of key physiological parameters generated by each analysis tool, motivated by our standard approach to single-cell physiology (<xref ref-type="bibr" rid="bib18">Jun et al., 2018</xref>; <xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>; <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Le Treut et al., 2021</xref>). First, we confirmed that all four analysis methods yield essentially identical correlations between cell length at birth (<italic>S</italic><sub>B</sub>) vs. (1) generation time (<italic>τ</italic>), (2) elongation rate (<italic>λ</italic>), and (3) the length added between birth and division (Δ) (<xref ref-type="fig" rid="fig4">Figure 4-4.3</xref>). Next, we compared the distributions of various physiological parameters. The CV (coefficient of variation) of a physiological parameter distribution is often taken to reflect the tightness of the underlying biological control. We have previously found (<xref ref-type="bibr" rid="bib44">Sauls et al., 2019a</xref>; <xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>) that the CVs of a set of physiological parameters (birth length, division length, length added between divisions, growth rate, generation time, and septum position) are invariant across growth conditions in <italic>E. coli</italic> and <italic>B. subtilis</italic>, and that the hierarchy of CVs is preserved across the two evolutionarily divergent species (<xref ref-type="bibr" rid="bib44">Sauls et al., 2019a</xref>; <xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>). Here, we confirmed that the distributions of these physiological parameters are independent of the analysis methods (<xref ref-type="fig" rid="fig4">Figure 4-4.4</xref>). In particular, the hierarchy of CVs is preserved by all three methods tested. Last, while in this dataset the old-pole ‘mother’ cells showed signs of aging (in particular, a reduced elongation rate), this aging phenotype is strain- and condition-dependent (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Comparison of various image analysis approaches.</title><p>(<bold>4.1</bold>) A time series of a typical cell growing in a nutrient-rich medium. The birth size, division size, and added size are indicated. (<bold>4.2</bold>) The adder principle ensures cell size homeostasis via passive convergence of cell size to the population mean. (<bold>4.3</bold>) We analyzed multiple datasets from our lab using MM3, DeLTA, and BACMMAN, and obtained robust correlations between birth length, doubling time, elongation rate, and added length. Representative results from one dataset (<xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>) for MG1655 background <italic>E. coli</italic> grown in MOPS glycerol + 11 amino acids are shown, with 9000–13,000 cells analyzed depending on the method. Error bars indicate standard error of the mean (note the standard error is smaller than marker size in most cases). (<bold>4.4</bold>) Distributions of key physiological parameters are independent of the analysis methods. The data and code used to generate this figure are available at <xref ref-type="bibr" rid="bib59">Thiermann, 2024a</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Old-pole aging phenotype is strain specific.</title><p>Cells imaged with fluorescence often show signs of aging in the old-pole ‘mother’ cell. For instance, in the dataset analyzed in <xref ref-type="fig" rid="fig4">Figure 4</xref> (<italic>E. coli</italic> MG1655 with the fluorescent protein YPet fused to DnaN), we observed systematic differences in cell elongation rate and size between the old-pole cell at the end of the growth channel and its sisters, which inherit the new pole (top center). However, this asymmetry is not universal. Using napari-MM3’s Otsu segmentation method, we re-analyzed previously published data obtained without fluorescence illumination (<xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>), and found that the old- and new-pole cell elongation rates varied only on the order of 1% (lower center), while in the dataset obtained under fluorescence imaging, the old-pole mother cells grow 7–10% slower than the new-pole cells. Cells born third or fourth from the closed end of the channel also grow slower than the old-pole mother (right). The asymmetry in growth rate between old- and new-pole cells persists across time (right, inset). These results are consistent with a previous survey (<xref ref-type="bibr" rid="bib39">Rang et al., 2012</xref>), which found that most evidence for aging in <italic>E. coli</italic> comes from studies utilizing fluorescent proteins for visualization.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig4-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Systematic discrepancies in cell segmentation outputs</title><p>While we found that the correlations between physiological parameters were preserved across the different analysis methods (<xref ref-type="fig" rid="fig4">Figure 4-4.3</xref>), we also observed systematic discrepancies in the results obtained by different methods, including cell length at birth (<italic>S</italic><sub>b</sub>), length at division (<italic>S</italic><sub>d</sub>), and length added between birth and division (Δ) (<xref ref-type="fig" rid="fig4">Figure 4-4.4</xref>). In particular, napari-MM3’s classical segmentation method systematically generated larger cell masks than napari-MM3 U-Net, DeLTA, and BACMMAN (<xref ref-type="fig" rid="fig4">Figure 4-4.4</xref>). We focused on the discrepancies between the two MM3 outputs. Although the deviation between the two masks may not appear significant when individual masks are inspected by eye (<xref ref-type="fig" rid="fig5">Figure 5-5.1</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), the classical method yields cells that are 5–10% larger at each time point than those returned by the U-Net method when averaging over an entire experiment with tens of thousands of cells tracked (<xref ref-type="fig" rid="fig5">Figure 5-5.2</xref>). Cell birth and division times are also systematically shifted in the classical method, as the expanded cell boundaries lead the algorithm to split cells one to two time frames later on average.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Effect of systematic deviation in segmentation output from different methods.</title><p>(<bold>5.1</bold>) Otsu/random walker and U-Net segmentation masks. The classical method systematically yields masks that are 5–10% larger than the other methods. (<bold>5.2</bold>) We confirmed that this discrepancy occurs consistently across the cell cycle. (<bold>5.3</bold>) We trained the Omnipose model on masks generated by either napari-MM3-Otsu or napari-MM3-U-Net separately. (<bold>5.4</bold>) The systematic discrepancy in the training data masks propagated to the output of the trained models.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Evaluating segmentation output of napari-MM3 Otsu and U-Net methods.</title><p>To evaluate the quality of the segmentation masks generated by MM3’s Otsu and U-Net segmentation methods, we computed the Jaccard index (<xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Jeckel and Drescher, 2021</xref>) as a function of the intersection-over-union (IoU) threshold.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88463-fig5-figsupp1-v1.tif"/></fig></fig-group><p>The root of this discrepancy is as follows. Exact cell boundaries are difficult to distinguish by eye, and the classical methods tested here require the user to set threshold values that can systematically alter the measured cell size. Indeed, both MM3 and BACMMAN’s non-learning method (which also uses Otsu thresholding and a watershed/diffusion algorithm) – output different cell masks with their ‘default’ parameter settings. On the other hand, binary U-Net segmentation methods, such as those implemented in napari-MM3 and DeLTA, tend to output smaller cell sizes because the model must leave a gap between cells so that they are not stitched together (note this is not a fundamental limitation of U-Net, but a consequence of our implementation: see, for example <xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref> or <xref ref-type="bibr" rid="bib35">Ollion and Ollion, 2020</xref> for more complex approaches which avoid this issue).</p></sec><sec id="s2-7"><title>WYPIWYG (What You Put Is What You Get) in deep learning-based image analysis</title><p>Given that classical methods are clearly sensitive to this threshold tuning, we predicted that deep learning approaches would also be impacted (<xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Geiger et al., 2021</xref>). We chose the recent cutting-edge segmentation model Omnipose and separately trained it on masks derived from the Otsu segmentation output and masks from the napari-MM3 U-Net segmentation output. We chose Omnipose as it assigns different labels to different cells, and can thus segment cells with contiguous boundaries, in contrast to MM3 or DeLTA’s U-Net implementations. Indeed, we found that the systematic discrepancy in the training masks propagated to the output of the trained models: the Omnipose model trained on larger Otsu masks generated larger masks upon evaluation with the same data, while the Omnipose model trained on smaller U-Net masks output smaller masks (<xref ref-type="fig" rid="fig5">Figure 5-5.3</xref>). In computer science, the phrase ‘Garbage in, garbage out’ denotes the concept that undesirable attributes in the input to a program will propagate to the output (<xref ref-type="bibr" rid="bib27">Mellin, 2024</xref>; <xref ref-type="bibr" rid="bib5">Babbage, 1864</xref>). Here, we propose a related notion WYPIWYG, or ‘what you put is what you get’. That is, at least for our setup, systematic differences in training data masks lead the model to learn different threshold intensity values and thus to systematically output larger or smaller masks. We emphasize this result does not reflect a flaw in Omnipose – whose performance we found impressive – but rather a well-studied feature of machine learning methods in general (<xref ref-type="bibr" rid="bib12">Geiger et al., 2021</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we introduced a modular and interactive image analysis pipeline for mother machine experiments and compared its effectiveness to other existing tools. Unlike its predecessors, napari-MM3 is equipped with an intuitive and modular interface, making it highly accessible to new users. Our main goal is to lower the barrier to entry in image analysis, which has been a primary obstacle in adopting the mother machine, and ultimately increase its user base.</p><p>Finally, we discuss common challenges faced by users new to high-throughput image analysis and give our prescriptions for overcoming them.</p><sec id="s3-1"><title>Validating results</title><p>We showed that distributions and correlations in key cell cycle parameters are invariant to the choice of analysis pipeline, provided that care is taken in parameter adjustment and postprocessing. However, this parallel processing of data is not feasible for every experiment. Instead, we suggest users can validate their results in the following ways:</p><list list-type="order"><list-item><p>A qualitative ‘eye test’ is an important first step: one should always visually inspect one’s data. Often, this may be sufficient to establish whether the analysis is operating as expected.</p></list-item><list-item><p>When a more quantitative and systematic approach is needed, the user can compare the output of their analysis to a subset of manually annotated ‘ground truth’ images. Quantitative measures such as the JI, <italic>F</italic><sub>1</sub> score or dice coefficient may be used (<xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>; <xref ref-type="bibr" rid="bib55">Taha and Hanbury, 2015</xref>). These metrics are particularly useful for comparing the results of different parameter choices in a given method, allowing the user to determine the combination that yields the most accurate segmentation or tracking results.</p></list-item><list-item><p>Verify that the averages calculated from single-cell measurements match the results of population-level control experiments.</p></list-item><list-item><p>When possible, filter for subsets of the data that are likely to reflect accurate segmentation and continuous tracking, such as cell lineages that are continuously tracked for the duration of the experiment.</p></list-item></list></sec><sec id="s3-2"><title>Choosing an image analysis tool</title><p>For many years, published and well-documented pipelines for mother machine image analysis were scarce, and existing software required extensive parameter reconfiguration, knowledge of image-processing techniques, and programming experience to use effectively. In recent years, advances in deep learning have contributed to a rapidly growing set of image analysis tools that perform cell segmentation and tracking.</p><p>Inspired by previous reviews (<xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Jeckel and Drescher, 2021</xref>), we make the following suggestions for new users selecting a tool:</p><list list-type="order"><list-item><p>Tools that are actively maintained, with an easy way to contact the developer, will be more likely to work well and will be easier to troubleshoot than others.</p></list-item><list-item><p>Detailed documentation and tutorials are valuable and will allow the user to troubleshoot the software without direct guidance from the developers.</p></list-item><list-item><p>Depending on the user’s level of comfort with coding, it may be beneficial to choose a tool that is implemented through a graphical user interface and does not require additional programming. Moreover, even for programmers, we found within our lab that introducing interactivity when necessary dramatically expedited the data analysis process.</p></list-item><list-item><p>Full stack (vertically integrated) tools that cover the entire analysis pipeline may save time and work, relative to those which only perform a portion of the needed analysis.</p></list-item><list-item><p>It is worthwhile to engage with the online community around the tool. We have found the image.sc forum (<xref ref-type="bibr" rid="bib15">Image.Sc, 2023</xref>) valuable in the past for help with napari.</p></list-item><list-item><p>Consider whether the tool is open source or requires a license. We encourage tool developers to avoid proprietary software such as MATLAB, which may not be accessible to all users. The open-source Java-based image-processing program ImageJ (<xref ref-type="bibr" rid="bib8">Bourne, 2010</xref>) has been a dominant tool in biological image analysis for many years. The recent growth of image analysis and machine learning tools in Python makes napari (<xref ref-type="bibr" rid="bib29">napari contributors, 2023</xref>) an attractive alternative to ImageJ.</p></list-item></list></sec><sec id="s3-3"><title>Traditional computer vision vs. deep learning methods</title><p>A key choice many users will face is whether to use deep learning-based or traditional methods for image analysis. The field has increasingly shifted toward deep learning methods, and this shift will likely accelerate. While traditional computer vision methods remain useful, deep learning-based methods have a clear advantage in their ability to generalize quickly to new datasets.</p><p>In our lab, we have found that traditional computer vision techniques perform excellently on cell segmentation and tracking in the mother machine, subject to constraints on the experimental setup. However, such methods often require extensive reconfiguration or fail entirely when applied to data obtained under new biological conditions (different organisms, different cell morphology) and imaging conditions (varied illumination, microscope setup). Our own non-learning segmentation method performs well, provided that cells are tightly confined in the mother machine channels and do not move substantially. Prior to the adoption of deep learning methods, this requirement necessitated the design of different devices for cells grown in different growth conditions, as the cell width in some <italic>E. coli</italic> strain backgrounds varies with the population growth rate.</p><p>In contrast, the key strength of deep learning approaches is their ability to generalize to new conditions – whether to different illumination conditions, different types of input images (phase contrast, brightfield, fluorescence) or different organisms and cell types entirely. The main barrier to adoption of learning-based methods remains the construction of training data, which can be tedious and time consuming. A training data set of 50–100 images comprising several hundred cells can be constructed in a few hours and will achieve passable segmentation on representative data. However, larger training sets on the order of thousands of images are preferable and will yield improved model accuracy and generalizability. The time needed for annotation can be reduced by seeding the data with masks generated by classical methods – or iteratively seeding with U-Net output – and then refining the masks further by hand. Model performance and generalizability can often be significantly improved by augmenting training data via manipulations such as rotating or shearing, distorting the intensity profile, and adding noise. Nonetheless, we have found that even with extensive data augmentation, applying the U-Net segmentation to new experimental configurations or imaging conditions often requires retraining the model on an expanded dataset with more representative data. Ultimately, deep learning methods are only as good as the data they are trained on and are most likely to fail when training data is insufficient, mislabeled, or not representative. Going forward, sharing of training sets and models (<xref ref-type="bibr" rid="bib4">Assets, 2023</xref>) between different groups can facilitate progress and aid reproducibility.</p><p>In addition to deep learning-based segmentation, learning-based cell tracking in the mother machine has been implemented recently by multiple groups (<xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Ollion and Ollion, 2020</xref>). For cells growing unconstrained on 2D surfaces such as agar pads, U-Net tracking dramatically outperforms traditional methods (<xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>). On the other hand, for steady-state growth in the mother machine where cells are confined and constrained to move in one dimension, we have not found a significant difference between the performance of deep learning-based tracking and the non-learning tracking method implemented in MM3. In both cases, errors in tracking nearly always arise from errors in segmentation. Nonetheless, deep learning-based tracking may offer an advantage in cases where cells may move substantially along the length of the channel or undergo dramatic morphological changes such as filamentation.</p><p>Ultimately, for groups with existing analysis pipelines fine-tuned for specific organisms under specific imaging conditions to perform simple tasks such as segmentation and 1D tracking, there may be little incentive to switch to deep learning methods. However, for users looking to develop a new pipeline or analyze more complex data, the power and generality of deep learning tools will make them the method of choice.</p></sec><sec id="s3-4"><title>Should users worry about the systematic discrepancy in segmentation results between different methods?</title><p>Given the 5–10% variance in the segmented bacterial cell size is comparable to the CVs of several physiological parameters (<xref ref-type="fig" rid="fig4">Figure 4</xref>), should researchers be concerned about the robustness of their results? The answer depends on the purpose of the image analysis.</p><p>If the research critically relies on the absolute cell size, such as cell-size control (<xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>; <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>), the researcher must be aware of inherent limitations to the accuracy of spatial measurements from cell segmentation. These arise in part from the difficulty of consistently distinguishing cell boundaries by eye. Once a threshold is chosen, the choice will affect all analyzed cells systematically. This limitation applies to both deep learning (through the construction of training data) and traditional computer vision methods (through the manual input of a threshold value). For cell segmentation, the uncertainties are typically comparable to the pixel size of the images, rather than optical resolutions. For example, the pixel size in the images in <xref ref-type="fig" rid="fig5">Figure 5</xref> is 0.065 µm (for the camera pixel size 6.5 µm and ×100 magnification), which is non-negligible for many commonly cultured bacterial cells with submicron cell widths – for example, <italic>Enterobacterales</italic>, <italic>Pseudomonas</italic>, <italic>B. subtilis</italic>, and <italic>Caulobacter crescentus</italic>. For most commercially available cameras and objective lenses used in quantitative bacterial cell biology, 10% should be taken as a conservative lower bound for uncertainty when comparing absolute spatial measurements of bacterial cell size.</p><p>Indeed, researchers should be particularly careful when comparing absolute measurements of cell size, for example, at division or initiation of chromosome replication obtained by different groups using different image analysis methods. While absolute temporal measurements are more robust than spatial measurements (<xref ref-type="fig" rid="fig4">Figure 4-4.4</xref>), the differences in spatial measurements can propagate to the measured timing of, for example, cell division. For instance, we observed that the classical method stitched cells together for slightly longer than the U-Net method did (<xref ref-type="fig" rid="fig5">Figure 5-5.2</xref>), but as this shift applied equally to birth and division, it did not affect the average cell generation time (<xref ref-type="fig" rid="fig4">Figure 4-4.4</xref>).</p><p>Fortunately, the examples mentioned above are extreme cases. For instance, the pixel-size uncertainties will reflect a smaller proportion of the cell size when imaging larger cells such as yeast or mammalian cells. Even in our research on single-cell bacterial physiology (<xref ref-type="bibr" rid="bib44">Sauls et al., 2019a</xref>; <xref ref-type="bibr" rid="bib56">Taheri-Araghi et al., 2015a</xref>; <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>), we find that correlations and relative changes are more likely to be robust than absolute spatial measurements to the choice of analysis method (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Furthermore, different applications of deep learning-based image analysis, such as high-throughput phenotypic classification (<xref ref-type="bibr" rid="bib47">Shiaelis et al., 2023</xref>) will be much more robust to the pixel-size uncertainties in image segmentation results.</p></sec><sec id="s3-5"><title>Generating robust and unbiased segmentation results</title><p>We have shown that both traditional computer vision and deep learning methods are susceptible to biases introduced by imprecise thresholding and human error. How, then, can more precise cell boundaries be determined? For non-learning methods, thresholds could be calibrated against data from alternate imaging methods such as fluorescence or brightfield. For learning methods, one promising technique is the generation of synthetic training data (<xref ref-type="bibr" rid="bib14">Hardo et al., 2022</xref>). This method also has the advantage that new training datasets can be instantaneously for different imaging conditions or cell types once the appropriate parameters have been determined. For deep learning methods, metrics which lead the model to recognize cell interiors or centers (<xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Ollion and Ollion, 2020</xref>; <xref ref-type="bibr" rid="bib31">Naylor et al., 2019</xref>) may yield more robust results than binary pixel-level classification. Once cell centers are known, boundaries can be determined relatively easily via classical watershed or random walker diffusion algorithms.</p></sec><sec id="s3-6"><title>Conclusion and recommendations</title><p>Here, we presented a guide to first-time users of the mother machine, introduced our updated image analysis software, and validated it against existing tools. napari-MM3 provides a simple and modular user-friendly interface, which we believe makes it uniquely accessible and valuable to novice users. By lowering the barrier to entry in image analysis – the key bottleneck in mother machine adoption – we aim to increase the user base of this powerful tool dramatically.</p><p>After testing two other well-constructed mother machine image analysis pipelines, we concluded that all four methods (BACMMAN, DeLTA, MM3 Otsu, and MM3 U-Net) yielded consistent and reproducible results, up to previously discussed limitations of segmentation algorithms. Thus, for users already comfortable with a given pipeline, there is no strong incentive to switch to a new one. However, the different pipelines do have markedly different user interfaces. DeLTA is set up to provide a simple ‘one-shot’ analysis, in which image preprocessing, channel detection, segmentation, and tracking are performed in sequence with minimal user input. This arrangement simplifies the analysis process, especially for first-time users. In particular, it can be helpful for users who want to quickly verify that the software will serve their purpose, before investing more time in setting up and running the analysis. On the other hand, the intermediate steps in the pipeline are less accessible, which may make debugging and troubleshooting more involved. BACMMAN, like napari-MM3, is more modular than DeLTA. This modularity can aid troubleshooting and improves versatility, but configuration can be time consuming. With napari-MM3, we attempted to strike a balance between these two well-designed and well-performing tools, while taking advantage of the fast-growing next-generation image analysis platform napari. napari-MM3 attempts to infer or pre-set as many parameters as possible, while the napari interface makes midstream output easily accessible. We have been using MM3, and more recently napari-MM3, for over a decade since our introduction of the mother machine in 2010, and we will continue to actively maintain and improve it in the coming years.</p><p>The mother machine setup has become increasingly accessible to researchers in recent years, through the distribution of molds and the publication of in-depth protocols and open-source image analysis software. At the same time, new variations of the device have found diverse applications, including bacterial starvation (<xref ref-type="bibr" rid="bib6">Bakshi et al., 2021</xref>) and genetic screening (<xref ref-type="bibr" rid="bib22">Lawson et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Luro et al., 2020</xref>). Clearly, the combination of microfluidics with high-resolution time-lapse imaging remains powerful among single-cell techniques. We hope that this article will prove useful to mother machine veterans and first-time users alike.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Resources</title><list list-type="bullet"><list-item><p><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/napari-mm3">napari-MM3 Github repository</ext-link> (<xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref>, copy archived at <xref ref-type="bibr" rid="bib62">Thiermann et al., 2024b</xref>).</p><p>○ Contains installation instructions and video tutorial.</p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/napari-mm3/blob/main/notebooks/napari_mm3_analysis_template.ipynb">Jupyter notebook</ext-link> demonstrating analysis of MM3 output data (<xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref>).</p><p>○ A notebook providing functions for postprocessing and plotting of the napari-MM3 output.</p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/mother-machine-protocols">Protocols</ext-link> for device fabrication and loading (<xref ref-type="bibr" rid="bib58">Thiermann, 2023</xref>).</p></list-item><list-item><p>Raw data analyzed in this manuscript (<xref ref-type="bibr" rid="bib63">Thiermann et al., 2024c</xref>).</p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/mother-machine-data">Processed data</ext-link> analyzed in this manuscript (<xref ref-type="bibr" rid="bib59">Thiermann, 2024a</xref>, copy archived at <xref ref-type="bibr" rid="bib61">Thiermann, 2024b</xref>).</p></list-item></list></sec><sec id="s4-2"><title>Getting started with napari-MM3</title><p>napari-MM3 is implemented entirely in Python and can be accessed on Github (<xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref>), along with documentation covering installation and usage. It will run on a standard Mac, PC, or Linux machine. We recommend using the Anaconda Python distribution to simplify installation.</p></sec><sec id="s4-3"><title>Imaging conditions</title><p>The data analyzed in <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref> (originally published in <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>) was obtained on an inverted microscope (Nikon Ti-E) with Perfect Focus 3 (PFS3), ×100 oil immersion objective (PH3, numerical aperture = 1.45), and Obis laser 488 LX (Coherent Inc, CA) as a fluorescence light source, and an Andor NEO sCMOS (Andor Technology) camera. The laser power was 18 mW. The exposure time was 200 ms for phase contrast imaging and 50 ms for fluorescence.</p></sec><sec id="s4-4"><title>Image analysis for software comparison</title><p>For the software comparison in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we analyzed a dataset from <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref> consisting of <italic>E. coli</italic> MG1655 expressing a fluorescent protein YPet fused to the replisome protein DnaN. The cells were grown in MOPS minimal medium + glycerol and 11 amino acids. The dataset was analyzed end-to-end starting from the raw.nd2 file with BACMMAN, DeLTA, and MM3. For analysis with DeLTA, we used the provided channel detection and tracking models but trained a new model on our own data for segmentation. For segmentation with BACMMAN, we used the standard non-learning phase contrast segmentation method ‘MicrochannelPhase2D’. Postprocessing of the output of each pipeline was done in Python. For each pipeline, we filtered for cells whose mothers and daughters were also tracked.</p><p>The code and data to reproduce the plots in <xref ref-type="fig" rid="fig4">Figure 4</xref> are available at <xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref> and <xref ref-type="bibr" rid="bib59">Thiermann, 2024a</xref>, respectively. The raw image data is available at <xref ref-type="bibr" rid="bib63">Thiermann et al., 2024c</xref>.</p><p>For the comparison of Otsu and U-Net outputs from Omnipose in <xref ref-type="fig" rid="fig5">Figure 5</xref>, we trained Omnipose with a learning rate of.01 without a pre-trained model. We used the same set of 1000 randomly selected images for both Otsu and U-Net, the only difference coming from the labeled masks themselves. Both models were trained until the loss dipped below 0.9 (390 epochs for U-Net, 210 epochs for Otsu). In some cases, the model ‘hallucinated’ cells along the channel features. We excluded these images from the final analysis.</p></sec><sec id="s4-5"><title>Analysis of external datasets</title><p>The external datasets were preprocessed as follows: Ollion et al., Jug et al., and Sachs et al. datasets were rotated 1–2 degrees to align the channels vertically. Ollion et al., Sachs et al., and Lugagne et al. datasets were cropped to remove imaging artifacts from the main trench.</p><p>The parameter values used for analysis of each dataset are shown in <xref ref-type="table" rid="table4">Table 4</xref>. In general, the optimal parameter values for the compilation and subtraction steps depend on the size of device features as well as the optical resolution and camera pixel size, while the optimal segmentation parameters depend on cell size as well as pixel size and optical resolution. Finally, the tracking parameters are either sensitive to the imaging frequency and the single-cell elongation rate (growth ratios and lost cell time), or the spatial position of the cells in the frame (y cutoff). The output cell size is sensitive to the ‘Otsu threshold scale’ parameter, so care should be taken when adjusting this value. In addition, the growth length and growth area ratio parameters may filter out fast- or slow-growing cells if they are set too close to 1. The remaining parameters will not impact the output statistics.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>MM3 parameter values for processed external datasets.</title><p>Parameters which were changed from the default values are shaded in yellow. Ollion et al., Jug et al., and Sachs et al. datasets were segmented with the non-learning method, while the Lugagne et al. dataset was segmented using the U-Net method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom">Default value</th><th align="left" valign="bottom">Ollion et al.</th><th align="left" valign="bottom">Lugagne et al.</th><th align="left" valign="bottom">Jug et al.</th><th align="left" valign="bottom">Sachs et al.</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="2"><bold>Compile</bold></td><td align="left" valign="bottom">Channel width (px)</td><td align="char" char="." valign="bottom">10</td><td style="background-color: #FFF176;"><bold>20</bold></td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom">Channel separation (px)</td><td align="char" char="." valign="bottom">45</td><td style="background-color: #FFF176;"><bold>90</bold></td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">45</td><td align="char" char="." valign="bottom">45</td></tr><tr><td align="left" valign="bottom"><bold>Subtract</bold></td><td align="left" valign="bottom">Align pad (px)</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td></tr><tr><td align="left" valign="bottom" rowspan="5"><bold>Segment</bold></td><td align="left" valign="bottom">1st opening (px)</td><td align="char" char="." valign="bottom">2</td><td style="background-color: #FFF176;"><bold>3</bold></td><td align="left" valign="bottom">N/A</td><td style="background-color: #FFF176;"><bold>3</bold></td><td style="background-color: #FFF176;"><bold>3</bold></td></tr><tr><td align="left" valign="bottom">Distance threshold (px)</td><td align="char" char="." valign="bottom">2</td><td style="background-color: #FFF176;"><bold>3</bold></td><td align="left" valign="bottom">N/A</td><td style="background-color: #FFF176;"><bold>3</bold></td><td style="background-color: #FFF176;"><bold>3</bold></td></tr><tr><td align="left" valign="bottom">2nd opening (px)</td><td align="char" char="." valign="bottom">1</td><td style="background-color: #FFF176;"><bold>2</bold></td><td align="left" valign="bottom">N/A</td><td align="char" char="." valign="bottom">1</td><td style="background-color: #FFF176;"><bold>2</bold></td></tr><tr><td align="left" valign="bottom">Otsu threshold scale</td><td align="char" char="." valign="bottom">1</td><td style="background-color: #FFF176;"><bold>1.2</bold></td><td align="left" valign="bottom">N/A</td><td align="char" char="." valign="bottom">1.0</td><td align="char" char="." valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Min object size (px<sup>2</sup>)</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td></tr><tr><td align="left" valign="bottom" rowspan="4"><bold>Track</bold></td><td align="left" valign="bottom">Growth length ratio (min, max)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td><td style="background-color: #FFF176;">(<bold>0.9, 1.5</bold>)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td></tr><tr><td align="left" valign="bottom">Growth area ratio (min, max)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td><td style="background-color: #FFF176;">(<bold>0.9, 1.5</bold>)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td><td align="char" char="." valign="bottom">(0.8, 1.3)</td></tr><tr><td align="left" valign="bottom">Lost cell time (frames)</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="left" valign="bottom">New cell y cutoff (px)</td><td align="char" char="." valign="bottom">150</td><td style="background-color: #FFF176;"><bold>300</bold></td><td align="char" char="." valign="bottom">150</td><td align="char" char="." valign="bottom">150</td><td align="char" char="." valign="bottom">150</td></tr></tbody></table></table-wrap><p>Each dataset was processed in its entirety with napari-MM3. To evaluate the segmentation quality, we selected one to two representative traps (comprising 50–100 time steps) and constructed ground truth masks for these images. On this subset, we computed the JI (<xref ref-type="bibr" rid="bib55">Taha and Hanbury, 2015</xref>) as the ratio of true positives (correctly identified cells) to the sum of true positives, false positives (identified cells which were not present in the ground truth data), and false negatives (ground truth cells which were not identified by the segmentation). The segmentation and ground truth masks were determined to be matching if their IoU value was at least 0.6. Note that two masks become indistinguishable to the human eye at IoU 0.8 and higher (<xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref>; <xref ref-type="bibr" rid="bib21">Laine et al., 2021</xref>).</p><p>The output JSON file and kymographs showing reconstructed cell lineages from each sample datasets are available at <xref ref-type="bibr" rid="bib59">Thiermann, 2024a</xref>, along with JSON files containing the parameter values used for each step of the analyses.</p></sec><sec id="s4-6"><title>U-Net model training</title><p>Training data was augmented as described below to aid the generalizability of the model. We trained the U-Net model using a binary cross-entropy loss function, with pixel-wise weighting to force the model to learn border pixels (<xref ref-type="bibr" rid="bib41">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>). The model was trained using the Adam optimizer with a learning rate of 10<sup>−4</sup>, a dropout rate of 50%, a batch size of eight samples, a patience (early stopping value) of 50 epochs and a train-test split of 90–10.</p></sec><sec id="s4-7"><title>Overview of the MM3 pipeline</title><sec id="s4-7-1"><title>Channel compilation and designation</title><p>The first section of the MM3 pipeline takes in raw micrographs and returns image stacks corresponding to one growth channel over time. Further pipeline operations are then applied to these stacks.</p><p>A standard mother machine experiment consists of thousands of images across multiple FOVs and many time points. Images are first collated based on the available metadata. MM3 expects TIFF files and looks for metadata in the TIFF header and from the file name.</p><p>All images from a particular FOV are analyzed for the location of channels using the phase contrast plane. Channel detection is performed using a wavelet transform, in which a mask is made which is applied across all time points. Channels are cropped through time using the masks and saved as unique image stacks that include all time points for a given channel and imaging plane. MM3 saves channel stacks in TIFF format.</p><p>MM3 attempts to compile all channels. However, not all channels contain cells, and some channels may have undesirable artifacts from the device preparation. It is, therefore, desirable to only process certain channels for analysis. Consequently, MM3 auto-detects empty and full channels based on the time correlation of the y-profile of the channel (empty channels are highly correlated in time, while channels containing cells are not). The auto-detected channels and their classifications are then displayed in the napari viewer for the user to inspect and modify as needed. The user may also manually select empty channels free of artifacts to be used as templates for phase or fluorescence background subtraction.</p></sec><sec id="s4-7-2"><title>Background subtraction</title><p>MM3’s Otsu segmentation method requires background subtraction of phase contrast images. The subtraction ensures that the presence of the channel border does not interfere with detection of cells. To this end, we overlay the previously identified empty channels on the full channels to be subtracted. The two channels are aligned such that the cross-correlation of overlaid pixels is maximized. After the inversion of the image, this leaves the cells as the only bright objects on a dark background. Good alignment of the device features in the empty and full channel is essential here. Imperfect alignment will leave artifacts in the subtracted image, which interfere with later steps, and is a common failure point for this method. Note that the subtraction step necessitates the presence of some empty channels in each experiment. The U-Net segmentation does not require background subtraction.</p></sec><sec id="s4-7-3"><title>Cell segmentation</title><p>Cell segmentation is the first of the two major tasks in the image analysis pipeline. Segmentation receives channel stacks and produces 8-bit segmented image stacks. Typically, segmentation is done using the phase contrast time-collated stack.</p><p>MM3 has two methods for segmentation: a ‘standard’ method and a supervised learning method. The standard method uses traditional image analysis techniques, specifically background subtraction, Otsu thresholding, morphological operations, and watershed algorithms. As the standard method may require fine-tuning of parameters, the napari plugin allows the user to quickly preview the effect of tuning morphological parameters and threshold value on the segmentation output, without having to process the entire dataset. The Otsu segmentation method first aligns the channel of interest with an empty background channel by computing the orientation, which maximizes the pixel-wise cross-correlation. The empty channel is then subtracted from the full channel, and the image is inverted. Otsu’s method is then applied to find the binary threshold value which maximizes the inter-region variance (or equivalently, minimizes the intra-region variance). We then apply a Euclidean distance transform, in which each pixel is labeled with its distance to the dark region. The image is thresholded again, and a morphological opening is applied to erode links between regions. Small objects and objects touching the image border are removed. Each region is labeled, and the labels are used to seed a random walker algorithm (<xref ref-type="bibr" rid="bib13">Grady, 2006</xref>) on the original image. As implemented in MM3, this ‘standard’ method has three adjustable parameters: the first opening pixel size, second opening pixel size, distance threshold (i.e., threshold which is applied to the distance transformed image, in pixels), and a dimensionless parameter to rescale the Otsu-determined threshold, if needed.</p><p>The supervised learning method uses a standard U-Net architecture with five levels (<xref ref-type="bibr" rid="bib41">Ronneberger et al., 2015</xref>). The model outputs a cell class probability between 0 and 1 for each pixel, which is thresholded at 0.5 to obtain a binary segmentation. The napari viewer can be used to construct training data, with the option to import existing Otsu or U-Net segmentation output as a template. The neural net can then be trained using a separate widget, with the option to check the performance of the model in the napari viewer after successive rounds of training. We found that applying a weighted loss depending on pixel location – as suggested in the original U-Net paper (<xref ref-type="bibr" rid="bib41">Ronneberger et al., 2015</xref>) and implemented for instance in DeLTA (<xref ref-type="bibr" rid="bib25">Lugagne et al., 2020</xref>) – sped up model training and improved segmentation and tracking. Since the accurate separation of adjacent cells is vital for cell tracking, the cost of misidentifying pixels between bordering cells is high. We initially implemented a simple binary weight map where pixels between cells were weighted highly and all other pixels relatively lower. We later added a more complex mapping, drawing directly from the one implemented in DeLTA (<xref ref-type="bibr" rid="bib32">O’Connor et al., 2022</xref>), where weights are maximized on the skeletons (<xref ref-type="bibr" rid="bib23">Lee et al., 1994</xref>) of the cells and borders. Intuitively, this weighting tells the model that pixels in the center of the cell, in regions far from cells, and on the borders between cells are most important to predict accurately.</p><p>Illumination conditions can vary across laboratories, microbial species, and with device design. To aid the generalizability of the U-Net model, on specific conditions, we augmented the training data with various morphological techniques, including changing magnification, zooming, and rotating, and Gaussian noise and blur. We also adapted several non-standard operations from DeLTA, one which performs elastic deformation and two others that distort image contrast to simulate changes in illumination within the FOV and between experiments.</p></sec><sec id="s4-7-4"><title>Cell tracking</title><p>Tracking segmented cells is the second major task in the pipeline. Tracking involves linking cell segments in time to define a lineage of cell objects. The default tracking method is a simple decision tree based on a priori knowledge of binary fission and the mother machine. For example, cells normally grow by a small amount between time intervals, divide into two similarly sized daughter cells, and cannot pass each other in the channel. The tracking method accounts for the absolute positions and relative ordering of cells in each channel over time. Specifically, at each time point we iterate over all detected regions (potential cells). Based on their relative y positions in the channel and sizes, each is linked to a set of potential descendants/ancestors. When two cells are best matched to the same region, the event is classified as a division, subject to constraints on the size of the regions. This tracking implementation is like that employed by BACMMAN (<xref ref-type="bibr" rid="bib34">Ollion et al., 2019</xref>) although it does not explicitly consider relative ordering of cells in the channel. It contrasts with more complex optimization-based methods used by other mother machine software (<xref ref-type="bibr" rid="bib43">Sachs et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Jug et al., 2014</xref>).</p><p>The lineage tree obtained by tracking is displayed in the napari viewer in the form of a kymograph, in which the <italic>x</italic>-axis represents time, and cell linkages and divisions are indicated by forking lines.</p></sec><sec id="s4-7-5"><title>Data output and analysis</title><p>Tracking produces a dictionary of cell objects which contains relevant information derived from the cell segments. This includes, but is not limited to, birth and division size, growth rate, and generation time. Each object is identified by a key that represents the FOV and channel of the cell, the time point of its birth, and its position in the channel. Since each cell object has the requisite information to find its corresponding position in the channel stacks, the objects can be modified and extended by additional analysis. For example, the corresponding location of a cell in a fluorescent image stack can be retrieved, focus detection performed, and that information can be added to the cell object. This minimizes the burden of rerunning previous sections of the pipeline for new subanalyses.</p><p>Plotting can be done from this cell object dictionary directly, or it can first be converted to a.csv, a pandas DataFrame, or a MATLAB structure. We provide a Jupyter notebook (<xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref>) to illustrate how the data can be extracted and plotted.</p></sec><sec id="s4-7-6"><title>Fluorescence analysis</title><p>Integrated fluorescence signal and fluorescence per cell area and volume for each time point can be extracted using the Colors module.</p></sec><sec id="s4-7-7"><title>Focus tracking</title><p>The focus tracking module enables the identification and tracking of fluorescent spots or ‘foci’. This module has been used in our lab for tracking fluorescently labeled replisome machinery in bacteria to measure the timing and synchrony of DNA replication initiation. However, it may be applied to any use case requiring localization and tracking of intracellular spots. The module uses a Laplacian convolution to identify fluorescent spots. Foci are linked to the cell objects in which they appear.</p></sec><sec id="s4-7-8"><title>U-Net training data annotation</title><p>Training data can be constructed by manual annotation of raw images in the napari viewer. MM3 offers the option to construct training data with existing (Otsu or U-Net) segmentation data as a template. This allows the user to iteratively train a model, correct mistakes in its output, and use the modified output as input for the next round of training.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Software, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Software, Visualization, Methodology</p></fn><fn fn-type="con" id="con7"><p>Data curation, Software, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Data curation, Software, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Data curation, Software, Writing - review and editing</p></fn><fn fn-type="con" id="con10"><p>Supervision, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Supervision, Funding acquisition, Visualization, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-88463-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>No new raw data was generated for this manuscript. Raw imaging data (previously analyzed in <xref ref-type="bibr" rid="bib48">Si et al., 2019</xref>) has been stored in <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.2fqz612xd">Dryad</ext-link>. Processed data is available on <ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/mother-machine-data">GitHub</ext-link>, (<xref ref-type="bibr" rid="bib59">Thiermann, 2024a</xref>, copy archived at <xref ref-type="bibr" rid="bib61">Thiermann, 2024b</xref>). Source code used to generate Figure 4 is also available on <ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/napari-mm3/blob/main/notebooks/fig4_plots.ipynb">GitHub</ext-link> (<xref ref-type="bibr" rid="bib60">Thiermann et al., 2024a</xref>, copy archived at <xref ref-type="bibr" rid="bib62">Thiermann et al., 2024b</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Ahir</surname><given-names>G</given-names></name><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>Schroeder</surname><given-names>JW</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Le Treut</surname><given-names>G</given-names></name><name><surname>Si</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>JD</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data for: Tools and methods for high-throughput single-cell imaging with the mother machine</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.2fqz612xd</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work has been made possible in part by CZI grant DAF2021-239849 and grant DOI (10.37921/244803gjbgup) from the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundation. The work was also supported by NIH grant R35GM139622 and NSF grant MCB-2016090. We thank Mara Casebeer, Thias Boesen, and the members of the Jun Lab for testing and debugging napari-MM3. We also thank Kevin Cutler for helping us to install Omnipose and run it on our data.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allard</surname><given-names>P</given-names></name><name><surname>Papazotos</surname><given-names>F</given-names></name><name><surname>Potvin-Trottier</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Microfluidics for long-term single-cell time-lapse microscopy: Advances and applications</article-title><source>Frontiers in Bioengineering and Biotechnology</source><volume>10</volume><elocation-id>968342</elocation-id><pub-id pub-id-type="doi">10.3389/fbioe.2022.968342</pub-id><pub-id pub-id-type="pmid">36312536</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amir</surname><given-names>A</given-names></name><name><surname>Babaeipour</surname><given-names>F</given-names></name><name><surname>McIntosh</surname><given-names>DB</given-names></name><name><surname>Nelson</surname><given-names>DR</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bending forces plastically deform growing bacterial cell walls</article-title><source>PNAS</source><volume>111</volume><fpage>5778</fpage><lpage>5783</lpage><pub-id pub-id-type="doi">10.1073/pnas.1317497111</pub-id><pub-id pub-id-type="pmid">24711421</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Apple Inc</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Tensorflow plugin - metal</article-title><ext-link ext-link-type="uri" xlink:href="https://developer.apple.com/metal/tensorflow-plugin/">https://developer.apple.com/metal/tensorflow-plugin/</ext-link><date-in-citation iso-8601-date="2023-07-14">July 14, 2023</date-in-citation></element-citation></ref><ref id="bib4"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Assets</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Assets — DeLTA 2.0-gamma documentation</article-title><ext-link ext-link-type="uri" xlink:href="https://delta.readthedocs.io/en/latest/usage/assets_desc.html">https://delta.readthedocs.io/en/latest/usage/assets_desc.html</ext-link><date-in-citation iso-8601-date="2023-02-23">February 23, 2023</date-in-citation></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Babbage</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1864">1864</year><source>Passages from the Life of a Philosopher</source><publisher-name>Longman, Green, Longman, Roberts, &amp; Green</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakshi</surname><given-names>S</given-names></name><name><surname>Leoncini</surname><given-names>E</given-names></name><name><surname>Baker</surname><given-names>C</given-names></name><name><surname>Cañas-Duarte</surname><given-names>SJ</given-names></name><name><surname>Okumus</surname><given-names>B</given-names></name><name><surname>Paulsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Tracking bacterial lineages in complex and dynamic environments with applications for growth control and persistence</article-title><source>Nature Microbiology</source><volume>6</volume><fpage>783</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1038/s41564-021-00900-4</pub-id><pub-id pub-id-type="pmid">34017106</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Banerjee</surname><given-names>DS</given-names></name><name><surname>Stephenson</surname><given-names>G</given-names></name><name><surname>Das</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Segmentation and Analysis of Mother Machine Data: SAM</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.10.01.322685</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bourne</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>ImageJ</chapter-title><person-group person-group-type="editor"><name><surname>Bourne</surname><given-names>R</given-names></name></person-group><source>Fundamentals of Digital Imaging in Medicine</source><publisher-name>Springer</publisher-name><fpage>185</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1007/978-1-84882-087-6</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabeen</surname><given-names>MT</given-names></name><name><surname>Losick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Single-cell Microfluidic Analysis of <italic>Bacillus subtilis</italic></article-title><source>Journal of Visualized Experiments</source><elocation-id>56901</elocation-id><pub-id pub-id-type="doi">10.3791/56901</pub-id><pub-id pub-id-type="pmid">29443042</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutler</surname><given-names>KJ</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Lo</surname><given-names>TW</given-names></name><name><surname>Rappez</surname><given-names>L</given-names></name><name><surname>Stroustrup</surname><given-names>N</given-names></name><name><surname>Brook Peterson</surname><given-names>S</given-names></name><name><surname>Wiggins</surname><given-names>PA</given-names></name><name><surname>Mougous</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation</article-title><source>Nature Methods</source><volume>19</volume><fpage>1438</fpage><lpage>1448</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01639-4</pub-id><pub-id pub-id-type="pmid">36253643</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname><given-names>T</given-names></name><name><surname>Mai</surname><given-names>D</given-names></name><name><surname>Bensch</surname><given-names>R</given-names></name><name><surname>Çiçek</surname><given-names>Ö</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Marrakchi</surname><given-names>Y</given-names></name><name><surname>Böhm</surname><given-names>A</given-names></name><name><surname>Deubner</surname><given-names>J</given-names></name><name><surname>Jäckel</surname><given-names>Z</given-names></name><name><surname>Seiwald</surname><given-names>K</given-names></name><name><surname>Dovzhenko</surname><given-names>A</given-names></name><name><surname>Tietz</surname><given-names>O</given-names></name><name><surname>Dal Bosco</surname><given-names>C</given-names></name><name><surname>Walsh</surname><given-names>S</given-names></name><name><surname>Saltukoglu</surname><given-names>D</given-names></name><name><surname>Tay</surname><given-names>TL</given-names></name><name><surname>Prinz</surname><given-names>M</given-names></name><name><surname>Palme</surname><given-names>K</given-names></name><name><surname>Simons</surname><given-names>M</given-names></name><name><surname>Diester</surname><given-names>I</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title><source>Nature Methods</source><volume>16</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id><pub-id pub-id-type="pmid">30559429</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiger</surname><given-names>RS</given-names></name><name><surname>Cope</surname><given-names>D</given-names></name><name><surname>Ip</surname><given-names>J</given-names></name><name><surname>Lotosh</surname><given-names>M</given-names></name><name><surname>Shah</surname><given-names>A</given-names></name><name><surname>Weng</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>“Garbage in, garbage out” revisited: What do machine learning application papers report about human-labeled training data?</article-title><source>Quantitative Science Studies</source><volume>2</volume><fpage>795</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1162/qss_a_00144</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grady</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Random walks for image segmentation</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>28</volume><fpage>1768</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2006.233</pub-id><pub-id pub-id-type="pmid">17063682</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardo</surname><given-names>G</given-names></name><name><surname>Noka</surname><given-names>M</given-names></name><name><surname>Bakshi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Synthetic Micrographs of Bacteria (SyMBac) allows accurate segmentation of bacterial cells using deep neural networks</article-title><source>BMC Biology</source><volume>20</volume><elocation-id>263</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-022-01453-6</pub-id><pub-id pub-id-type="pmid">36447211</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Image.Sc</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Image.sc Forum</article-title><ext-link ext-link-type="uri" xlink:href="https://forum.image.sc/">https://forum.image.sc/</ext-link><date-in-citation iso-8601-date="2023-01-24">January 24, 2023</date-in-citation></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeckel</surname><given-names>H</given-names></name><name><surname>Drescher</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Advances and opportunities in image analysis of bacterial cells and communities</article-title><source>FEMS Microbiology Reviews</source><volume>45</volume><elocation-id>fuaa062</elocation-id><pub-id pub-id-type="doi">10.1093/femsre/fuaa062</pub-id><pub-id pub-id-type="pmid">33242074</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jug</surname><given-names>F</given-names></name><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Kainmüller</surname><given-names>D</given-names></name><name><surname>Funke</surname><given-names>J</given-names></name><name><surname>Kaiser</surname><given-names>M</given-names></name><name><surname>Nimwegen</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Optimal joint Segmentation and tracking of <italic>Escherichia coli</italic> in the mother machine</chapter-title><person-group person-group-type="editor"><name><surname>Jug</surname><given-names>F</given-names></name></person-group><source>Bayesian and Graphical Models for Biomedical Imaging</source><publisher-name>Springer International Publishing</publisher-name><fpage>25</fpage><lpage>36</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>S</given-names></name><name><surname>Si</surname><given-names>F</given-names></name><name><surname>Pugatch</surname><given-names>R</given-names></name><name><surname>Scott</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fundamental principles in bacterial physiology-history, recent progress, and the future with focus on cell size control: a review</article-title><source>Reports on Progress in Physics. Physical Society</source><volume>81</volume><elocation-id>056601</elocation-id><pub-id pub-id-type="doi">10.1088/1361-6633/aaa628</pub-id><pub-id pub-id-type="pmid">29313526</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamande</surname><given-names>JW</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cloning SU8 silicon masters using epoxy resins to increase feature replicability and production for cell culture devices</article-title><source>Biomicrofluidics</source><volume>9</volume><elocation-id>036502</elocation-id><pub-id pub-id-type="doi">10.1063/1.4922962</pub-id><pub-id pub-id-type="pmid">26180572</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>Y</given-names></name><name><surname>Reich</surname><given-names>S</given-names></name><name><surname>Oster</surname><given-names>E</given-names></name><name><surname>Maoz</surname><given-names>S</given-names></name><name><surname>Levin-Reisman</surname><given-names>I</given-names></name><name><surname>Ronin</surname><given-names>I</given-names></name><name><surname>Gefen</surname><given-names>O</given-names></name><name><surname>Agam</surname><given-names>O</given-names></name><name><surname>Balaban</surname><given-names>NQ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Observation of universal ageing dynamics in antibiotic persistence</article-title><source>Nature</source><volume>600</volume><fpage>290</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04114-w</pub-id><pub-id pub-id-type="pmid">34789881</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laine</surname><given-names>RF</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Henriques</surname><given-names>R</given-names></name><name><surname>Jacquemet</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Avoiding a replication crisis in deep-learning-based bioimage analysis</article-title><source>Nature Methods</source><volume>18</volume><fpage>1136</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01284-3</pub-id><pub-id pub-id-type="pmid">34608322</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname><given-names>MJ</given-names></name><name><surname>Camsund</surname><given-names>D</given-names></name><name><surname>Larsson</surname><given-names>J</given-names></name><name><surname>Baltekin</surname><given-names>Ö</given-names></name><name><surname>Fange</surname><given-names>D</given-names></name><name><surname>Elf</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>In situ genotyping of a pooled strain library after characterizing complex phenotypes</article-title><source>Molecular Systems Biology</source><volume>13</volume><elocation-id>947</elocation-id><pub-id pub-id-type="doi">10.15252/msb.20177951</pub-id><pub-id pub-id-type="pmid">29042431</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>TC</given-names></name><name><surname>Kashyap</surname><given-names>RL</given-names></name><name><surname>Chu</surname><given-names>CN</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Building Skeleton Models via 3-D Medial Surface Axis Thinning Algorithms</article-title><source>CVGIP</source><volume>56</volume><fpage>462</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1006/cgip.1994.1042</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Treut</surname><given-names>G</given-names></name><name><surname>Si</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Quantitative Examination of Five Stochastic Cell-Cycle and Cell-Size Control Models for <italic>Escherichia coli</italic> and <italic>Bacillus subtilis</italic></article-title><source>Frontiers in Microbiology</source><volume>12</volume><elocation-id>721899</elocation-id><pub-id pub-id-type="doi">10.3389/fmicb.2021.721899</pub-id><pub-id pub-id-type="pmid">34795646</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lugagne</surname><given-names>JB</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Dunlop</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>DeLTA: Automated cell segmentation, tracking, and lineage reconstruction using deep learning</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007673</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007673</pub-id><pub-id pub-id-type="pmid">32282792</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luro</surname><given-names>S</given-names></name><name><surname>Potvin-Trottier</surname><given-names>L</given-names></name><name><surname>Okumus</surname><given-names>B</given-names></name><name><surname>Paulsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Isolating live cells after high-throughput, long-term, time-lapse microscopy</article-title><source>Nature Methods</source><volume>17</volume><fpage>93</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0620-7</pub-id><pub-id pub-id-type="pmid">31768062</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mellin</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="2024">2024</year><source>Work with New Electronic “Brains” Opens Field for Army Math Experts</source><publisher-name>Hammond Times</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakaoka</surname><given-names>H</given-names></name><name><surname>Wakamoto</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Aging, mortality, and the fast growth trade-off of <italic>Schizosaccharomyces pombe</italic></article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e2001109</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2001109</pub-id><pub-id pub-id-type="pmid">28632741</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="web"><person-group person-group-type="author"><collab>napari contributors</collab></person-group><year iso-8601-date="2023">2023</year><article-title>napari: a fast, interactive viewer for multi-dimensional images in Python</article-title><ext-link ext-link-type="uri" xlink:href="https://napari.org/stable/">https://napari.org/stable/</ext-link><date-in-citation iso-8601-date="2023-01-23">January 23, 2023</date-in-citation></element-citation></ref><ref id="bib30"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Napari hub</collab></person-group><year iso-8601-date="2023">2023</year><article-title>napari-mm3</article-title><ext-link ext-link-type="uri" xlink:href="https://www.napari-hub.org/plugins/napari-mm3">https://www.napari-hub.org/plugins/napari-mm3</ext-link><date-in-citation iso-8601-date="2023-01-17">January 17, 2023</date-in-citation></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naylor</surname><given-names>P</given-names></name><name><surname>Lae</surname><given-names>M</given-names></name><name><surname>Reyal</surname><given-names>F</given-names></name><name><surname>Walter</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Segmentation of nuclei in histopathology images by deep regression of the distance map</article-title><source>IEEE Transactions on Medical Imaging</source><volume>38</volume><fpage>448</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2865709</pub-id><pub-id pub-id-type="pmid">30716022</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connor</surname><given-names>OM</given-names></name><name><surname>Alnahhas</surname><given-names>RN</given-names></name><name><surname>Lugagne</surname><given-names>JB</given-names></name><name><surname>Dunlop</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>DeLTA 2.0: A deep learning pipeline for quantifying single-cell spatial and temporal dynamics</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009797</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009797</pub-id><pub-id pub-id-type="pmid">35041653</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollion</surname><given-names>J</given-names></name><name><surname>Cochennec</surname><given-names>J</given-names></name><name><surname>Loll</surname><given-names>F</given-names></name><name><surname>Escudé</surname><given-names>C</given-names></name><name><surname>Boudier</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>TANGO: a generic tool for high-throughput 3D image analysis for studying nuclear organization</article-title><source>Bioinformatics</source><volume>29</volume><fpage>1840</fpage><lpage>1841</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt276</pub-id><pub-id pub-id-type="pmid">23681123</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollion</surname><given-names>J</given-names></name><name><surname>Elez</surname><given-names>M</given-names></name><name><surname>Robert</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-throughput detection and tracking of cells and intracellular spots in mother machine experiments</article-title><source>Nature Protocols</source><volume>14</volume><fpage>3144</fpage><lpage>3161</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0216-9</pub-id><pub-id pub-id-type="pmid">31554957</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ollion</surname><given-names>J</given-names></name><name><surname>Ollion</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Distnet: deep tracking by displacement regression: application to bacteria growing in the mother machine</chapter-title><person-group person-group-type="editor"><name><surname>Ollion</surname><given-names>J</given-names></name></person-group><source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</source><publisher-name>Springer International Publishing</publisher-name><fpage>215</fpage><lpage>225</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otsu</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>A threshold selection method from gray-level histograms</article-title><source>IEEE Transactions on Systems, Man, and Cybernetics</source><volume>9</volume><fpage>62</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panigrahi</surname><given-names>S</given-names></name><name><surname>Murat</surname><given-names>D</given-names></name><name><surname>Le Gall</surname><given-names>A</given-names></name><name><surname>Martineau</surname><given-names>E</given-names></name><name><surname>Goldlust</surname><given-names>K</given-names></name><name><surname>Fiche</surname><given-names>J-B</given-names></name><name><surname>Rombouts</surname><given-names>S</given-names></name><name><surname>Nöllmann</surname><given-names>M</given-names></name><name><surname>Espinosa</surname><given-names>L</given-names></name><name><surname>Mignot</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities</article-title><source>eLife</source><volume>10</volume><elocation-id>e65151</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65151</pub-id><pub-id pub-id-type="pmid">34498586</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potvin-Trottier</surname><given-names>L</given-names></name><name><surname>Luro</surname><given-names>S</given-names></name><name><surname>Paulsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Microfluidics and single-cell microscopy to study stochastic processes in bacteria</article-title><source>Current Opinion in Microbiology</source><volume>43</volume><fpage>186</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.mib.2017.12.004</pub-id><pub-id pub-id-type="pmid">29494845</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rang</surname><given-names>CU</given-names></name><name><surname>Peng</surname><given-names>AY</given-names></name><name><surname>Poon</surname><given-names>AF</given-names></name><name><surname>Chao</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Ageing in <italic>Escherichia coli</italic> requires damage by an extrinsic agent</article-title><source>Microbiology</source><volume>158</volume><fpage>1553</fpage><lpage>1559</lpage><pub-id pub-id-type="doi">10.1099/mic.0.057240-0</pub-id><pub-id pub-id-type="pmid">22422756</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rideau</surname><given-names>F</given-names></name><name><surname>Villa</surname><given-names>A</given-names></name><name><surname>Belzanne</surname><given-names>P</given-names></name><name><surname>Verdier</surname><given-names>E</given-names></name><name><surname>Hosy</surname><given-names>E</given-names></name><name><surname>Arfi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Imaging minimal bacteria at the nanoscale: A reliable and versatile process to perform single-molecule localization microscopy in mycoplasmas</article-title><source>Microbiology Spectrum</source><volume>10</volume><elocation-id>e0064522</elocation-id><pub-id pub-id-type="doi">10.1128/spectrum.00645-22</pub-id><pub-id pub-id-type="pmid">35638916</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Lect Notes Comput Sci</source><volume>9351</volume><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>JR</given-names></name><name><surname>Cabeen</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>PA</given-names></name><name><surname>Paulsson</surname><given-names>J</given-names></name><name><surname>Losick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Noise in a phosphorelay drives stochastic entry into sporulation in <italic>Bacillus subtilis</italic></article-title><source>The EMBO Journal</source><volume>36</volume><fpage>2856</fpage><lpage>2869</lpage><pub-id pub-id-type="doi">10.15252/embj.201796988</pub-id><pub-id pub-id-type="pmid">28838935</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sachs</surname><given-names>CC</given-names></name><name><surname>Grünberger</surname><given-names>A</given-names></name><name><surname>Helfrich</surname><given-names>S</given-names></name><name><surname>Probst</surname><given-names>C</given-names></name><name><surname>Wiechert</surname><given-names>W</given-names></name><name><surname>Kohlheyer</surname><given-names>D</given-names></name><name><surname>Nöh</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Image-based single cell profiling: High-throughput processing of mother machine experiments</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0163453</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0163453</pub-id><pub-id pub-id-type="pmid">27661996</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>Cox</surname><given-names>SE</given-names></name><name><surname>Do</surname><given-names>Q</given-names></name><name><surname>Castillo</surname><given-names>V</given-names></name><name><surname>Ghulam-Jelani</surname><given-names>Z</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Control of <italic>Bacillus subtilis</italic> Replication Initiation during Physiological Transitions and Perturbations</article-title><source>mBio</source><volume>10</volume><elocation-id>e02205-19</elocation-id><pub-id pub-id-type="doi">10.1128/mBio.02205-19</pub-id><pub-id pub-id-type="pmid">31848269</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>Schroeder</surname><given-names>JW</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Treut</surname><given-names>GL</given-names></name><name><surname>Si</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>JD</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Mother Machine Image Analysis with MM3</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/810036</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>MS</given-names></name><name><surname>Moen</surname><given-names>E</given-names></name><name><surname>Miller</surname><given-names>G</given-names></name><name><surname>Dougherty</surname><given-names>T</given-names></name><name><surname>Borba</surname><given-names>E</given-names></name><name><surname>Ding</surname><given-names>R</given-names></name><name><surname>Graf</surname><given-names>W</given-names></name><name><surname>Pao</surname><given-names>E</given-names></name><name><surname>Van Valen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Caliban: Accurate Cell Tracking and Lineage Construction in Live-Cell Imaging Experiments with Deep Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/803205</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiaelis</surname><given-names>N</given-names></name><name><surname>Tometzki</surname><given-names>A</given-names></name><name><surname>Peto</surname><given-names>L</given-names></name><name><surname>McMahon</surname><given-names>A</given-names></name><name><surname>Hepp</surname><given-names>C</given-names></name><name><surname>Bickerton</surname><given-names>E</given-names></name><name><surname>Favard</surname><given-names>C</given-names></name><name><surname>Muriaux</surname><given-names>D</given-names></name><name><surname>Andersson</surname><given-names>M</given-names></name><name><surname>Oakley</surname><given-names>S</given-names></name><name><surname>Vaughan</surname><given-names>A</given-names></name><name><surname>Matthews</surname><given-names>PC</given-names></name><name><surname>Stoesser</surname><given-names>N</given-names></name><name><surname>Crook</surname><given-names>DW</given-names></name><name><surname>Kapanidis</surname><given-names>AN</given-names></name><name><surname>Robb</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Virus detection and identification in minutes using single-particle imaging and deep learning</article-title><source>ACS Nano</source><volume>17</volume><fpage>697</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1021/acsnano.2c10159</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Si</surname><given-names>F</given-names></name><name><surname>Le Treut</surname><given-names>G</given-names></name><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>Vadia</surname><given-names>S</given-names></name><name><surname>Levin</surname><given-names>PA</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mechanistic origin of cell-size control and homeostasis in bacteria</article-title><source>Current Biology</source><volume>29</volume><fpage>1760</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.062</pub-id><pub-id pub-id-type="pmid">31104932</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>A</given-names></name><name><surname>Metz</surname><given-names>J</given-names></name><name><surname>Pagliara</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MMHelper: An automated framework for the analysis of microscopy images acquired with the mother machine</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>10123</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-46567-0</pub-id><pub-id pub-id-type="pmid">31300741</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spahn</surname><given-names>C</given-names></name><name><surname>Gómez-de-Mariscal</surname><given-names>E</given-names></name><name><surname>Laine</surname><given-names>RF</given-names></name><name><surname>Pereira</surname><given-names>PM</given-names></name><name><surname>von Chamier</surname><given-names>L</given-names></name><name><surname>Conduit</surname><given-names>M</given-names></name><name><surname>Pinho</surname><given-names>MG</given-names></name><name><surname>Jacquemet</surname><given-names>G</given-names></name><name><surname>Holden</surname><given-names>S</given-names></name><name><surname>Heilemann</surname><given-names>M</given-names></name><name><surname>Henriques</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>DeepBacs for multi-task bacterial image analysis using open-source deep learning approaches</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>688</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-03634-z</pub-id><pub-id pub-id-type="pmid">35810255</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spivey</surname><given-names>EC</given-names></name><name><surname>Jones</surname><given-names>SK</given-names></name><name><surname>Rybarski</surname><given-names>JR</given-names></name><name><surname>Saifuddin</surname><given-names>FA</given-names></name><name><surname>Finkelstein</surname><given-names>IJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An aging-independent replicative lifespan in a symmetrically dividing eukaryote</article-title><source>eLife</source><volume>6</volume><elocation-id>e20340</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.20340</pub-id><pub-id pub-id-type="pmid">28139976</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>EJ</given-names></name><name><surname>Madden</surname><given-names>R</given-names></name><name><surname>Paul</surname><given-names>G</given-names></name><name><surname>Taddei</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Aging and death in an organism that reproduces by morphologically symmetric division</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e45</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030045</pub-id><pub-id pub-id-type="pmid">15685293</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title><source>Nature Methods</source><volume>18</volume><fpage>100</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id><pub-id pub-id-type="pmid">33318659</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stylianidou</surname><given-names>S</given-names></name><name><surname>Brennan</surname><given-names>C</given-names></name><name><surname>Nissen</surname><given-names>SB</given-names></name><name><surname>Kuwada</surname><given-names>NJ</given-names></name><name><surname>Wiggins</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>SuperSegger: robust image segmentation, analysis and lineage tracking of bacterial cells</article-title><source>Molecular Microbiology</source><volume>102</volume><fpage>690</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1111/mmi.13486</pub-id><pub-id pub-id-type="pmid">27569113</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taha</surname><given-names>AA</given-names></name><name><surname>Hanbury</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title><source>BMC Medical Imaging</source><volume>15</volume><elocation-id>29</elocation-id><pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id><pub-id pub-id-type="pmid">26263899</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taheri-Araghi</surname><given-names>S</given-names></name><name><surname>Bradde</surname><given-names>S</given-names></name><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>Hill</surname><given-names>NS</given-names></name><name><surname>Levin</surname><given-names>PA</given-names></name><name><surname>Paulsson</surname><given-names>J</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Cell-size control and homeostasis in bacteria</article-title><source>Current Biology</source><volume>25</volume><fpage>385</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.12.009</pub-id><pub-id pub-id-type="pmid">25544609</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taheri-Araghi</surname><given-names>S</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>McIntosh</surname><given-names>DB</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Single-cell physiology</article-title><source>Annual Review of Biophysics</source><volume>44</volume><fpage>123</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1146/annurev-biophys-060414-034236</pub-id><pub-id pub-id-type="pmid">25747591</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Mother-machine-protocols: procedures for Duplicating, constructing and using the Microfluidic mother machine device</data-title><version designator="4e7a875">4e7a875</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/mother-machine-protocols">https://github.com/junlabucsd/mother-machine-protocols</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024a</year><data-title>Mother-machine-data: A repository for processed mother machine data from the Jun lab</data-title><version designator="4c1c689">4c1c689</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/mother-machine-data">https://github.com/junlabucsd/mother-machine-data</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Ahir</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024a</year><data-title>napari-mm3: A Plugin for mother machine image analysis by the Jun lab</data-title><version designator="03b4ce9">03b4ce9</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/junlabucsd/napari-mm3">https://github.com/junlabucsd/napari-mm3</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024b</year><data-title>Mother-machine-data</data-title><version designator="swh:1:rev:4c1c689192c51e2a523d2b768863b56c46bc802f">swh:1:rev:4c1c689192c51e2a523d2b768863b56c46bc802f</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9e3fa47464e43a4982b694828413c365183d704f;origin=https://github.com/junlabucsd/mother-machine-data;visit=swh:1:snp:d688eb5e841ebac61c4083e4519efe87b0699380;anchor=swh:1:rev:4c1c689192c51e2a523d2b768863b56c46bc802f">https://archive.softwareheritage.org/swh:1:dir:9e3fa47464e43a4982b694828413c365183d704f;origin=https://github.com/junlabucsd/mother-machine-data;visit=swh:1:snp:d688eb5e841ebac61c4083e4519efe87b0699380;anchor=swh:1:rev:4c1c689192c51e2a523d2b768863b56c46bc802f</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Ahir</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024b</year><data-title>Napari-Mm3: A Plugin for mother machine image analysis by the Jun lab</data-title><version designator="swh:1:rev:03b4ce9bf6c94b3b11639013acd53d0fb66d6287">swh:1:rev:03b4ce9bf6c94b3b11639013acd53d0fb66d6287</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0349778e501d224f0a2fdb88400d50dbab1504cd;origin=https://github.com/junlabucsd/napari-mm3;visit=swh:1:snp:0f7edeb588973b7619e1943ebf4d8f3707ef87d6;anchor=swh:1:rev:03b4ce9bf6c94b3b11639013acd53d0fb66d6287">https://archive.softwareheritage.org/swh:1:dir:0349778e501d224f0a2fdb88400d50dbab1504cd;origin=https://github.com/junlabucsd/napari-mm3;visit=swh:1:snp:0f7edeb588973b7619e1943ebf4d8f3707ef87d6;anchor=swh:1:rev:03b4ce9bf6c94b3b11639013acd53d0fb66d6287</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thiermann</surname><given-names>R</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Ahir</surname><given-names>G</given-names></name><name><surname>Sauls</surname><given-names>JT</given-names></name><name><surname>Schroeder</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Le Treut</surname><given-names>G</given-names></name><name><surname>Si</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>JD</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024c</year><data-title>Data for: tools and methods for high-throughput single-cell imaging with the mother machine</data-title><source>Dryad</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.2fqz612xd">https://doi.org/10.5061/dryad.2fqz612xd</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Robert</surname><given-names>L</given-names></name><name><surname>Pelletier</surname><given-names>J</given-names></name><name><surname>Dang</surname><given-names>WL</given-names></name><name><surname>Taddei</surname><given-names>F</given-names></name><name><surname>Wright</surname><given-names>A</given-names></name><name><surname>Jun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Robust growth of <italic>Escherichia coli</italic></article-title><source>Current Biology</source><volume>20</volume><fpage>1099</fpage><lpage>1103</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.04.045</pub-id><pub-id pub-id-type="pmid">20537537</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88463.4.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Salman</surname><given-names>Hanna</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Pittsburgh</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This article provides a review and test of image-analysis methods for bacteria growing in the 'mother-machine' microfluidic device, introducing also a new graphical user interface for the computational analysis of mother-machine movies based on the 'Napari' environment. The tool allows users to segment cells based on two previously published methods (classical image transformation and thresholding as well as UNet-based analysis), with <bold>solid</bold> evidence for their robust performance based on comparison with other methods and use of datasets from other labs. While it was difficult to assess the user-friendliness of the new GUI, it appears to be <bold>valuable</bold> and promising for the field.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88463.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors aim to develop an easy-to-use image analysis tool for the mother machine that is used for single-cell time-lapse imaging. Compared with related software, they tried to make this software more user-friendly for non-experts with a design of &quot;What You Put Is What You Get&quot;. This software is implemented as a plugin of Napari, which is an emerging microscopy image analysis platform. The users can interactively adjust the parameters in the pipeline with good visualization and interaction interface.</p><p>Strengths:</p><p>- Updated platform with great 2D/3D visualization and annotation support.</p><p>- Integrated one-stop pipeline for mather machine image processing.</p><p>- Interactive user-friendly interface.</p><p>- The users can have a visualization of intermediate results and adjust the parameters.</p><p>Weaknesses:</p><p>- Based on the presentation of the manuscript, it is not clear that the goals are fully achieved.</p><p>- Although there is great potential, there is little evidence that this tool has been adopted by other labs.</p><p>- the diversity of datasets used in this study is limited.</p><p>- Some paragraphs in the Discussion section are like blogs with general recommendations. Although the suggestions look pretty useful, it is not the focus of this manuscript. It might be more appropriate to put it in the GitHub repo or a documentation page. The discussion should still focus on the software, such as features, software maintenance, software development roadmap, and community adoption.</p><p>A discussion of the likely impact of the work on the field, and the utility of the methods and data to the community.</p><p>- The impact of this work depends on the adoption of the software MM3. Napari is a promising platform with an expanding community. With good software user experience and long-term support, there is a good chance that this tool could be widely adopted in the mother machine image analysis community.</p><p>- The data analysis in this manuscript is used as a demo of MM3 features, rather than scientific research.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88463.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors present an image-analysis pipeline for mother-machine data, i.e., for time-lapses of single bacterial cells growing for many generations in one-dimensional microfluidic channels. The pipeline is available as a plugin of the python-based image-analysis platform Napari. The tool comes with two different previously published methods to segment cells (classical image transformation and thresholding as well as UNet-based analysis), which compare qualitatively and quantitatively well with the results of widely accessible tools developed by others (BACNET, DelTA, Omnipose). The tool comes with a graphical user interface and example scripts, which should make it valuable for other mother-machine users, even if this has not been demonstrated yet.</p><p>The authors also add a practical overview of how to prepare and conduct mother-machine experiments, citing their previous work, referring to detailed instructions on their github page, and giving more advice on how to load cells using centrifugation.</p><p>Finally, the authors emphasize that machine-learning methods for image segmentation reproduce average quantities of training datasets, such as the length at birth or division. Therefore, differences in training can propagate to differences in measured average quantities. This result is not surprising but good to remember before interpreting absolute measurements of cell shape.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88463.4.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jun</surname><given-names>Suckjoon</given-names></name><role specific-use="author">Author</role><aff><institution>University of California, San Diego</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Thiermann</surname><given-names>Ryan</given-names></name><role specific-use="author">Author</role><aff><institution>University of California, San Diego</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Sandler</surname><given-names>Michael</given-names></name><role specific-use="author">Author</role><aff><institution>UC San Diego</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Ahir</surname><given-names>Gursharan</given-names></name><role specific-use="author">Author</role><aff><institution>UC San Diego</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Sauls</surname><given-names>John T</given-names></name><role specific-use="author">Author</role><aff><institution>UC San Diego</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Schroeder</surname><given-names>Jeremy</given-names></name><role specific-use="author">Author</role><aff><institution>University of Michigan Medical School</institution><addr-line><named-content content-type="city">Ann Arbor</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Brown</surname><given-names>Steven</given-names></name><role specific-use="author">Author</role><aff><institution>UC San Diego</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Le Treut</surname><given-names>Guillaume</given-names></name><role specific-use="author">Author</role><aff><institution>UCSD</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Si</surname><given-names>Fangwei</given-names></name><role specific-use="author">Author</role><aff><institution>UC San Diego</institution><addr-line><named-content content-type="city">San Diego</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Dongyang</given-names></name><role specific-use="author">Author</role><aff><institution>UCSD</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Jue D</given-names></name><role specific-use="author">Author</role><aff><institution>University of Wisconsin-Madison</institution><addr-line><named-content content-type="city">Madison</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p>After revision, I only have a few remaining remarks:</p><p>l. 180 The authors write: We were able to process all 4 datasets with minimal adjustments to the default parameter values (Methods).</p><p>But they still don't indicate how they vary parameters and how important this is for success or how this affects absolute measurements such as average cell length. Could they give a table of parameter values and some sense of sensitivity for any future user?</p></disp-quote><p>We thank the reviewer for the suggestion. We see how this info is valuable for the user. We’ve added a table with the parameter values used for processing each dataset in the supplemental information, along with the default parameters for reference (lines 476 - 496). In that section we also discuss which parameters may affect the output measurements of cell size, etc.</p><disp-quote content-type="editor-comment"><p>l. 192-193 They write 'The software performed well on BACMMAN, molyso and MoMa datasets.' Naming the datasets after the analysis methods used in the original papers could be confusing, as they analyse data with MM3. Not sure how best to resolve this, maybe using first author names instead.</p></disp-quote><p>We thank the reviewer for pointing this out. We now refer to them with the first author names.</p><disp-quote content-type="editor-comment"><p>Related to the request of ref. #1 for a video tutorial, the video currently displayed under the github readme.md section 'Usage guide' is not functional. And the video at the top of the same page is very short with minimal information.</p></disp-quote><p>We thank the reviewer for letting us know the tutorial video was not functional. We’ve tested it on Linux, Mac and Windows machines on both Firefox and Chrome. We were not able to reproduce any problems for the video - could they let us know what browser / OS was used and any other specifics? If it’s easier, we can be reached through the Github page as well.</p></body></sub-article></article>