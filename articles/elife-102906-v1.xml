<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">102906</article-id><article-id pub-id-type="doi">10.7554/eLife.102906</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102906.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Q-learning with temporal memory to navigate turbulence</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Rando</surname><given-names>Marco</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0008-3839-1429</contrib-id><email>marco.rando@edu.unige.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>James</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Verri</surname><given-names>Alessandro</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Seminara</surname><given-names>Agnese</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5633-8180</contrib-id><email>agnese.seminara@unige.it</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0107c5v14</institution-id><institution>MaLGa, Department of Computer Science, Bioengineering, Robotics and Systems Engineering, University of Genova</institution></institution-wrap><addr-line><named-content content-type="city">Genoa</named-content></addr-line><country>Italy</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0107c5v14</institution-id><institution>MalGa, Department of Civil, Chemical and Environmental Engineering, University of Genoa</institution></institution-wrap><addr-line><named-content content-type="city">Genova</named-content></addr-line><country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution>CNRS</institution><country>France</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>07</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP102906</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-09-12"><day>12</day><month>09</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-04-26"><day>26</day><month>04</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2404.17495"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-16"><day>16</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102906.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-03-05"><day>05</day><month>03</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102906.2"/></event></pub-history><permissions><copyright-statement>Â© 2024, Rando et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Rando et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-102906-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-102906-figures-v1.pdf"/><abstract><p>We consider the problem of olfactory searches in a turbulent environment. We focus on agents that respond solely to odor stimuli, with no access to spatial perception nor prior information about the odor. We ask whether navigation to a target can be learned robustly within a sequential decision making framework. We develop a reinforcement learning algorithm using a small set of interpretable olfactory states and train it with realistic turbulent odor cues. By introducing a temporal memory, we demonstrate that two salient features of odor traces, discretized in a few olfactory states, are sufficient to learn navigation in a realistic odor plume. Performance is dictated by the sparse nature of turbulent odors. An optimal memory exists which ignores blanks within the plume and activates a recovery strategy outside the plume. We obtain the best performance by letting agents learn their recovery strategy and show that it is mostly casting cross wind, similar to behavior observed in flying insects. The optimal strategy is robust to substantial changes in the odor plumes, suggesting minor parameter tuning may be sufficient to adapt to different environments.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Many animals use odors to locate mates, food, and to avoid danger. Unlike light, which travels in straight lines, odors are carried by turbulent air or water, leading to intermittent whiffs separated by long gaps with no detectable scent. These patchy odor landscapes can make it difficult for animals to decide which direction to move in.</p><p>Despite these challenges, animals are remarkably good at using odors to navigate. While previous studies have modelled this behavior computationally, the most principled models often relied on complex concepts of memory, that were not directly interpretable. In particular, what must be remembered about past odor detections and for how long remained unclear.</p><p>To investigate this, Rando et al. developed an algorithm that enables agents to learn to navigate by trial and error, responding only to a short excerpt of past odor detections. Agents had no prior knowledge about the odor nor access to spatial information, other than their ability to orient relative to the wind. The simulated environment mimicked realistic odor plumes in turbulent air and the algorithm was given a short-term memory to track changes in a limited set of specific odor-related signals over time.</p><p>Analysis showed that there is an optimal length of memory that helps the agent ignore temporary gaps in the odor signal while still recognizing when it has fully exited the plume. This allowed the agent to activate a strategy to return to the scent plume only when truly necessary. When it was allowed to learn behavior both within and outside the plume, it performed better than when using fixed strategies based on animal behavior. Interestingly, the learned strategy often resembled the casting behavior, seen in flying insects, which involves a side-to-side search in the crosswind direction to relocate odor plumes.</p><p>Overall, the work of Rando et al. shows that simple odor signals and a basic form of temporal memory are enough to learn effective navigation in turbulent environments with no prior knowledge of the odor environment. The algorithm performed reliably, reaching the odor source in 90% to 100% of trials. These findings help explain how animals might use short-memory of odor to navigate in space, even in unknown or variable environments and could be used to develop search algorithms for robots in complex real-world settings like disaster zones or polluted areas.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>olfactory navigation</kwd><kwd>turbulence</kwd><kwd>reinforcement learning</kwd><kwd>time series</kwd><kwd>memory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/101002724</award-id><principal-award-recipient><name><surname>James</surname><given-names>Martin</given-names></name><name><surname>Seminara</surname><given-names>Agnese</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/819789</award-id><principal-award-recipient><name><surname>Rando</surname><given-names>Marco</given-names></name><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01DC018789</award-id><principal-award-recipient><name><surname>James</surname><given-names>Martin</given-names></name><name><surname>Verri</surname><given-names>Alessandro</given-names></name><name><surname>Seminara</surname><given-names>Agnese</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000181</institution-id><institution>Air Force Office of Scientific Research</institution></institution-wrap></funding-source><award-id>FA8655-20-1-7028</award-id><principal-award-recipient><name><surname>Rando</surname><given-names>Marco</given-names></name><name><surname>James</surname><given-names>Martin</given-names></name><name><surname>Verri</surname><given-names>Alessandro</given-names></name><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name><name><surname>Seminara</surname><given-names>Agnese</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000181</institution-id><institution>Air Force Office of Scientific Research</institution></institution-wrap></funding-source><award-id>FA8655-22-1-7034</award-id><principal-award-recipient><name><surname>Rando</surname><given-names>Marco</given-names></name><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Ministero dell'Istruzione, dell'UniversitÃ  e della Ricerca</institution></institution-wrap></funding-source><award-id>ML4IP R205T7J2KP</award-id><principal-award-recipient><name><surname>Rando</surname><given-names>Marco</given-names></name><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>CCF-1231216, Center for Brain Minds and Machines</award-id><principal-award-recipient><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100018693</institution-id><institution>HORIZON EUROPE Framework Programme</institution></institution-wrap></funding-source><award-id>101120237 ELIAS</award-id><principal-award-recipient><name><surname>Rando</surname><given-names>Marco</given-names></name><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution>NextGenerationEU</institution></institution-wrap></funding-source><award-id>BAC FAIR PE00000013</award-id><principal-award-recipient><name><surname>Rando</surname><given-names>Marco</given-names></name><name><surname>Verri</surname><given-names>Alessandro</given-names></name><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Animals may learn to locate preys effectively by memorizing short excerpts of their scent trace, which duration is dictated by the sparse nature of turbulent odor plumes.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Bacterial cells localize the source of an attractive chemical even if they hold no spatial perception. They respond solely to temporal changes in chemical concentration, and the result of their response is that they move toward attractive stimuli by climbing concentration gradients (<xref ref-type="bibr" rid="bib11">Berg, 1975</xref>). Larger organisms also routinely sense chemicals in their environment to localize or escape targets, but cannot follow chemical gradients since turbulence breaks odors into sparse pockets and gradients lose information (<xref ref-type="bibr" rid="bib45">Murlis et al., 1992</xref>; <xref ref-type="bibr" rid="bib68">Vergassola et al., 2007</xref>; <xref ref-type="bibr" rid="bib59">Shraiman and Siggia, 2000</xref>; <xref ref-type="bibr" rid="bib8">Balkovsky and Shraiman, 2002</xref>; <xref ref-type="bibr" rid="bib51">Reddy et al., 2022</xref>). The question of which features of turbulent odor traces are used by animals for navigation is natural, but not well understood. Beyond olfaction, some animals could also use prior spatial information to navigate (<xref ref-type="bibr" rid="bib14">CardÃ©, 2021</xref>; <xref ref-type="bibr" rid="bib57">Schal, 1982</xref>; <xref ref-type="bibr" rid="bib27">Gire et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Baker et al., 2018</xref>), but if and how chemosensation and spatial perception are coupled is still not clear.</p><p>An algorithmic perspective to olfactory navigation in turbulence can shed light on some of these questions. Without aiming at an exhaustive taxonomy, see for example <xref ref-type="bibr" rid="bib18">Celani and Panizon, 2024</xref> for a recent review, we recall some approaches relevant to put our contribution in context. One class of methods is biomimetic algorithms, where explicit navigation rules are crafted taking inspiration from animal behavior. An advantage of these methods is interpretability, in the sense that they provide insights into features that effectively achieve turbulent navigation, for example: odor presence/absence (<xref ref-type="bibr" rid="bib6">Baker, 1990</xref>; <xref ref-type="bibr" rid="bib35">Kramer, 1997</xref>; <xref ref-type="bibr" rid="bib9">Belanger and Willis, 1988</xref>; <xref ref-type="bibr" rid="bib8">Balkovsky and Shraiman, 2002</xref>); odor slope at onset of detection (<xref ref-type="bibr" rid="bib5">Atema, 1996</xref>; number of detections in a given interval of time (<xref ref-type="bibr" rid="bib43">Michaelis et al., 2020</xref>) and the time of odor onset (<xref ref-type="bibr" rid="bib20">Demir et al., 2020</xref>). On the flip side, in biomimetic algorithms, behaviors are hardwired and typically reactive, not relying on any optimality criterion.</p><p>A way to tackle this shortcoming is to cast olfactory navigation within a sequential decision-making framework (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 1998</xref>). In this context, navigation is formalized as a task with a reward for success; by maximizing reward, optimal strategies can be sought to efficiently reach the target. A byproduct is that most algorithmic choices can often be done in a principled way. Within this framework, some approaches make explicit use of spatial information. Bayesian algorithms use a spatial map to guess the target location and use odor to refine this guess or âbeliefâ. A prominent algorithm for olfactory navigation based on the concept of belief is the information-seeking algorithm (<xref ref-type="bibr" rid="bib68">Vergassola et al., 2007</xref>) akin to exploration heuristics widely used in robotics (<xref ref-type="bibr" rid="bib16">Cassandra et al., 1996</xref>; <xref ref-type="bibr" rid="bib38">LaValle, 2006</xref>; see e.g. <xref ref-type="bibr" rid="bib40">Loisy and Eloy, 2022</xref>; <xref ref-type="bibr" rid="bib33">Ishida et al., 2012</xref>). Using Bayesian sequential decision making and the notion of beliefs, navigation can be formalized as a Partially Observable Markov Decision Process (POMDP; <xref ref-type="bibr" rid="bib36">Krishnamurthy, 2016</xref>; <xref ref-type="bibr" rid="bib29">Hauskrecht, 2000</xref>; <xref ref-type="bibr" rid="bib58">Shani et al., 2013</xref>), that can be approximatively solved (<xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref>; <xref ref-type="bibr" rid="bib30">Heinonen et al., 2023</xref>; <xref ref-type="bibr" rid="bib41">Loisy and Heinonen, 2023</xref>). POMDP approaches are appealing since beliefs are a sufficient statistic for the entire history of odor detections. However, they are computationally cumbersome. Further, they leave the question open of whether navigation as sequential decision making can be performed using solely olfactory information.</p><p>Recently, two algorithms studied navigation as a response to olfactory input alone (<xref ref-type="bibr" rid="bib60">Singh et al., 2023</xref>; <xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>). In <xref ref-type="bibr" rid="bib60">Singh et al., 2023</xref>, artificial neural networks were shown to learn near-optimal strategies as a response to odor and instantaneous flow direction, although they were trained on odor cues with limited sparsity, and training with sparse odor cues typical of turbulence remains to be tested. In <xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>, an approach based on finite state controllers was proposed. Here, optimization was done assuming fixed known mean flow direction and using a model-based technique, relying on prior knowledge of the likelihood to detect the odor in space, hence still using spatial information. A different model-free optimization could also be considered, avoiding spatial information, but this latter approach also remains to be tested. More generally, all the above approaches manipulate internally the previous history (memory) of odor detections. In this sense, they are less interpretable, since the features of odor traces that drive navigation do not emerge explicitly.</p><p>In this paper, we propose a reinforcement learning (RL) approach to navigation in turbulence based on a set of interpretable olfactory features, with no spatial information other than the ability to orient relative to the mean flow, and highlight the role played by memory within this context. More precisely, we learn optimal strategies from data by training tabular Q learning (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 1998</xref>) with realistic odor cues obtained from state-of-the-art Direct Numerical Simulations of turbulence. From the odor cues, we define features as moving averages of odor intensity and sparsity: the moving window is the temporal memory and naturally connects to the physics of turbulent odors. States are then obtained by discretizing such features. Due to sparsity, agents may detect no odor within the moving window. We show there is an optimal memory minimizing the occurrence of this âvoid stateâ. The optimal memory scales with the blank time dictated by turbulence as it emerges from a trade-off requiring that: (<italic>i</italic>) short blanks â typical of turbulent plumes â are ignored by responding to detections further in the past and (<italic>ii</italic>) long blanks promptly trigger a recovery strategy to make contact with the plume again. We leverage these observations to tune the memory adaptively, by setting it equal to the previous blank experienced along an agentâs path. With this choice, the algorithm tests successfully in distinct environments, suggesting that tuning can be made robustly to enable generalization. The agent learns to surge upwind in most non-void states and to recover by casting crosswind in the absence of detections. Optimal agents limit encounters with the void state to a narrow band right at the edge of the plume. This suggests that the temporal odor features we considered effectively predict when the agent is exiting the plume and point to an intimate connection between temporal predictions and spatial navigation.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Background</title><p>Given a source of odor placed in an unknown position of a two-dimensional space, we consider the problem of learning to reach the source, <xref ref-type="fig" rid="fig1">Figure 1A</xref>. We formulate the problem as a discrete Markov Decision Process by discretizing space in tiles, also called âgridworldâ in the reinforcement learning literature (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 1998</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Learning a stimulus-response strategy for turbulent navigation.</title><p>(<bold>A</bold>) Representation of the search problem with turbulent odor cues obtained from Direct Numerical Simulations of fluid turbulence (gray scale, odor snapshot from the simulations). The discrete position <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$ s$\end{document}</tex-math></alternatives></inline-formula> is hidden; the odor concentration <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>â¤</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$ z_{T}=z(s(t^{\prime}),t^{\prime})|t-T\leq t^{\prime}\leq t$\end{document}</tex-math></alternatives></inline-formula> is observed along the trajectory <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$ s(t^{\prime})$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> is the sensing memory. (<bold>B</bold>) Odor traces from direct numerical simulations at different (fixed) points within the plume. Odor is noisy and sparse, information about the source is hidden in the temporal dynamics. (<bold>C</bold>) Contour maps of olfactory states with nearly infinite memory (<inline-formula><alternatives><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>2598</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$ T=2598$\end{document}</tex-math></alternatives></inline-formula>): on average, olfactory states map to different locations within the plume, and the void state is outside the plume. Intermittency is discretized in three bins defined by two thresholds: 66% (red line) and 33% (blue line). Intensity is discretized in 5 bins (dark red shade to white shade) defined by four thresholds (percentiles 99%, 80%, 50%, 25%). (<bold>D</bold>) Performance of stimulus-response strategies obtained during training, averaged over 500 episodes. We train using realistic turbulent data with memory <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$ T=20$\end{document}</tex-math></alternatives></inline-formula> and backtracking recovery.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig1-v1.tif"/></fig><p>In this problem, an agent is in a given state <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> which is one of a discrete set of <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> tiles: <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>â</mml:mo><mml:mi>S</mml:mi><mml:mo>:=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$s\in S:=\{s_{1},...,s_{n}\}$\end{document}</tex-math></alternatives></inline-formula>. At each time step, it chooses an action <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> which is a step in any of the coordinate directions <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$a\in$\end{document}</tex-math></alternatives></inline-formula> {upwind, downwind, crosswind-left, crosswind-right}. Directions are labeled relative to the mean wind, which is assumed known. In our figures, the flow always goes from left to right, hence the actions upwind, downwind, crosswind-right, and crosswind-left correspond in the figures to a step left, right, up, and down, respectively. The goal is to find sequences of actions that lead to the source as fast as possible and is formalized with the notion of policy and reward, which we will introduce later. If agents have perfect knowledge of their own location and of the location of the source in space, the problem reduces to finding the shortest path.</p></sec><sec id="s2-2"><title>Using time <italic>vs</italic> space to address partial observability</title><p>In our problem, however, the agent does not know where the source is; hence, its position <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$ s$\end{document}</tex-math></alternatives></inline-formula> relative to the source is unknown or âpartially observedâ. Instead, it can sense odor released by the target. In the language of RL, odor is an âobservationâ â but does it hold information about the position <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>? The answer is yes: several properties of odor stimuli depend on the distance from the source (<xref ref-type="bibr" rid="bib13">Boie et al., 2018</xref>; <xref ref-type="bibr" rid="bib2">Ackels et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Nag and van Breugel, 2024</xref>). However, in the presence of turbulence, information lies in the statistics of the odor stimulus. Indeed, when odor is carried by a turbulent flow, it develops into a dramatically stochastic plume, that is a complex and convoluted region of space where the fluid is rich in odor molecules. Turbulent plumes break into structures that distort and expand while they travel away from their source and become more and more diluted (<xref ref-type="bibr" rid="bib24">Falkovich et al., 2001</xref>; <xref ref-type="bibr" rid="bib59">Shraiman and Siggia, 2000</xref>; <xref ref-type="bibr" rid="bib17">Celani et al., 2014</xref>; <xref ref-type="bibr" rid="bib51">Reddy et al., 2022</xref>), see <xref ref-type="fig" rid="fig1">Figure 1A</xref>. As a consequence, an agent within the plume experiences intermittent odor traces that endlessly switch on (whiff) and off (blank) <xref ref-type="fig" rid="fig1">Figure 1B</xref>. The intensity of odor whiffs and how they are interleaved with blanks depends on distance from release, as dictated by physics (<xref ref-type="bibr" rid="bib17">Celani et al., 2014</xref>). Thus, the upshot of turbulent transport is that the statistical properties of odor traces depend intricately on the position of the agent relative to the source. In other words, information about the state <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> is hidden within the observed odor traces.</p><p>This positional information can be leveraged with a Bayesian approach that relies on guessing <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, that is defining the probability distribution of the position, also called belief. This is the approach that has been more commonly adopted in the literature until now (<xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref>; <xref ref-type="bibr" rid="bib30">Heinonen et al., 2023</xref>; <xref ref-type="bibr" rid="bib41">Loisy and Heinonen, 2023</xref>). Note that because of the complexity of these algorithms, only relatively simple measures of the odor are computationally feasible, for example instantaneous presence/absence. Here, we take a different model-free and map-free approach. Instead of guessing the current state <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$ s$\end{document}</tex-math></alternatives></inline-formula>, we ignore the spatial position and respond directly to the temporal traces of the odor cues. Two other algorithms have been proposed to solve partial observability by responding solely to odor traces with recurrent neural networks (<xref ref-type="bibr" rid="bib60">Singh et al., 2023</xref>) and finite state controllers (<xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>) that manipulate implicitly the odor traces. Here, instead, we manipulate odor traces explicitly, by defining memory as a moving window and by crafting a small number of features of odor traces.</p></sec><sec id="s2-3"><title>Features of odor cues: definition of discrete olfactory states and sensing memory</title><p>To learn a response to odor traces, we set out to craft a finite set of <italic>olfactory states</italic>, <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi><mml:mo>â</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$o\in O$\end{document}</tex-math></alternatives></inline-formula>, so that they bear information about the location <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. Defining the olfactory states is a challenge due to the dramatic fluctuations and irregularity of turbulent odor traces. To construct a fully interpretable low-dimensional state space, we aim at a small number of olfactory states that bear robust information about <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, that is for all values of <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$ s$\end{document}</tex-math></alternatives></inline-formula>. We previously found that pairing features of sparsity as well as intensity of turbulent odor traces predicts robustly the location of the source for all <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="bibr" rid="bib53">Rigolli et al., 2022a</xref>). Guided by these results, we use these two features extracted from the temporal history of odor detections to define a small set of olfactory states.</p><p>We proceed to define a function that takes as input the history of odor detections along an agentâs path and returns its current olfactory state. We indicate with <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$ s(t)$\end{document}</tex-math></alternatives></inline-formula> the (unknown) path of an agent, and with <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$z(s(t),t)$\end{document}</tex-math></alternatives></inline-formula> the observations that is odor concentration along its path. First, we define a sensing memory <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> and we consider a short excerpt of the history of odor detections <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$z_{T}$\end{document}</tex-math></alternatives></inline-formula> of duration <inline-formula><alternatives><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> prior to the current time <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>. Formally, <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>â¤</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$ z_{T}(t):=\{z(s(t^{\prime}),t^{\prime})\,|\,t-T\leq t^{\prime}\leq t\}$\end{document}</tex-math></alternatives></inline-formula>. Second, we measure the average intensity <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> (moving average of odor intensity over the time window <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>, conditioned to times when odor is above threshold), and intermittency <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$ i$\end{document}</tex-math></alternatives></inline-formula> (the fraction of time the odor is above threshold during the sensing window <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>). Both features <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> are described by continuous, positive real numbers. Third, we define 15 olfactory states by discretizing <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> in 3 and 5 bins, respectively. Intermittency <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> is bounded between 0 and 1, and we discretize it in 3 bins by defining two thresholds (33% and 66%). The average concentration, <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula>, is bounded between 0 and the odor concentration at the source, hence prior information on the source is needed to discretize <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> using set thresholds. To avoid relying on prior information, we define thresholds of intensity as percentiles, based on a histogram that is populated online, along each agentâs path (see Materials and methods). The special case where no odor is detected over <inline-formula><alternatives><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula> deserves attention, hence we include it as an additional state named âvoid stateâ and indicate it with <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi><mml:mo>â¡</mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$o\equiv\emptyset$\end{document}</tex-math></alternatives></inline-formula>. When <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> is sufficiently long, the resulting olfactory states map to different spatial locations (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, with <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula> equal to the simulation time). Hence, this definition of olfactory states can potentially mitigate the problem of partial observability using temporal traces, rather than spatial maps. But will these olfactory states with finite memory <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> guide agents to the source?</p></sec><sec id="s2-4"><title>Q learning: a map-less and model-free navigation to odor sources</title><p>To answer this question, we trained tabular episodic Q learning (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 1998</xref>). In a nutshell, we use a simulator to place an agent at a random location in space at the beginning of each episode. The agent is not aware of its location in space, but it senses odor provided by the fluid dynamics simulator and thus can compute its olfactory state <inline-formula><alternatives><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft45">\begin{document}$ o$\end{document}</tex-math></alternatives></inline-formula>, based on odor detected along its path in the previous <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> sensing window. It then makes a move according to a set policy of actions <inline-formula><alternatives><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¼</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$ a\sim\pi_{0}(a|o)$\end{document}</tex-math></alternatives></inline-formula>. After the move, the simulator displaces the agent to its new location and relays the agent a penalty <inline-formula><alternatives><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$ R=-\sigma$\end{document}</tex-math></alternatives></inline-formula> with <inline-formula><alternatives><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$\sigma=0.001$\end{document}</tex-math></alternatives></inline-formula> if it is not at the source and a reward <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$ R=1$\end{document}</tex-math></alternatives></inline-formula> if it reaches the source. The goal of RL is to find a policy of actions that maximizes the expected cumulative future reward <inline-formula><alternatives><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>Î³</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$G=E_{\pi}(\sum_{t=0}^{\infty}\gamma^{t}R_{t+1})$\end{document}</tex-math></alternatives></inline-formula> where the expectation is over the ensemble of trajectories and rewards generated by the policy from any initial condition. Because reward is only positive at the source, the optimal policy is the one that reaches the source as fast as possible. To further encourage the agent to reach the source quickly, we introduce a discount factor <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$\gamma \lt 1$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Episodes where the agent does not reach the source are ended after <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$H_{\text{max}}=5000$\end{document}</tex-math></alternatives></inline-formula> with no positive reward. As it tries actions and receives rewards, the agent learns how good the actions are. This is accomplished by estimating the quality matrix <inline-formula><alternatives><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$Q(o,a)$\end{document}</tex-math></alternatives></inline-formula>, that is the maximum expected cumulative reward conditioned to being in <inline-formula><alternatives><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$o$\end{document}</tex-math></alternatives></inline-formula> and choosing action <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> at the present time: <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>Î³</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}$Q(o,a)=\max_{\pi}E_{\pi}(\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}|o_{t}=o,a_{t}=a)$\end{document}</tex-math></alternatives></inline-formula>. At each step, the agent improves its policy by choosing more frequently putatively good actions. Once the agent has a good approximation of the quality matrix, the optimal policy corresponds to the simple readout: <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Î´</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$\pi^{*}(a|o)=\delta(a-a^{*}(o))$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$a^{*}(o)=\arg\max_{a}Q(o,a)$\end{document}</tex-math></alternatives></inline-formula>, for non-void states <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi><mml:mo>â </mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$ o\neq\emptyset$\end{document}</tex-math></alternatives></inline-formula>.</p><sec id="s2-4-1"><title>Recovery strategy</title><p>To fully describe the behavior of our Q-learning agents, we have to prescribe their policy from the void state <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi><mml:mo>â¡</mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$o\equiv\emptyset$\end{document}</tex-math></alternatives></inline-formula>. This is problematic because turbulent plumes are full of holes, thus the void state can occur anywhere both within and outside the plume, <xref ref-type="fig" rid="fig1">Figure 1A</xref>. As a consequence, the optimal action <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">â</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$ a^{*}(\emptyset)$\end{document}</tex-math></alternatives></inline-formula> from the void state is ill-defined. We address this issue by using a separate policy called ârecovery strategyâ. Inspired by path integration as defined in biology (<xref ref-type="bibr" rid="bib23">Etienne and Jeffery, 2004</xref>; <xref ref-type="bibr" rid="bib22">Etienne et al., 1996</xref>; <xref ref-type="bibr" rid="bib31">Heinze et al., 2018</xref>), we propose the backtracking strategy consisting of retracing the last <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$T_{a}$\end{document}</tex-math></alternatives></inline-formula> steps after the agent lost track of the odor. If at the end of backtracking the agent is still in the void state, it activates Brownian motion. Backtracking requires that we introduce memory of the past <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$ T_{a}$\end{document}</tex-math></alternatives></inline-formula> actions. This timescale <inline-formula><alternatives><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$T_{a}$\end{document}</tex-math></alternatives></inline-formula> for activating recovery is conceptually distinct from the duration of the sensing memory â however, here we set <inline-formula><alternatives><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft66">\begin{document}$ T_{a}=T$\end{document}</tex-math></alternatives></inline-formula> for simplicity. Backtracking was observed in ants displaced in unfamiliar environments (<xref ref-type="bibr" rid="bib76">Wystrach et al., 2013</xref>), tsetse flies executing reverse turns bringing them back towards the location where they last detected odor (<xref ref-type="bibr" rid="bib65">Torr, 1988</xref>; <xref ref-type="bibr" rid="bib26">Gibson and Brady, 1985</xref>) and cockroaches retracing their steps downwind, sometimes walking all the way back to the release point upon plume loss <xref ref-type="bibr" rid="bib74">Willis et al., 2008</xref>; it was also previously used in computational models (<xref ref-type="bibr" rid="bib48">Park et al., 2016</xref>).</p><p>We find that Q-learning agents successfully learn to navigate to the odor source by responding solely to their olfactory state, with no sense of space nor models of the odor cues. Learning can be quantified by monitoring the cumulative reward which continuously improves with further training episodes (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, left). Improved reward corresponds to agents learning how to reach the source more quickly and reliably with training. Indeed, it is easy to show that the expected cumulative reward <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>Ï</mml:mi></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>Ï</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î³</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$G=\langle e^{-\lambda\tau}-\sigma(1-e^{-\lambda\tau})/(1-\gamma)\rangle$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$\tau$\end{document}</tex-math></alternatives></inline-formula> is a random variable corresponding to time to reach the source and <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$\gamma=e^{-\lambda\Delta t}$\end{document}</tex-math></alternatives></inline-formula> is the discount factor, with the time step <inline-formula><alternatives><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft70">\begin{document}$\Delta t=1$\end{document}</tex-math></alternatives></inline-formula> (see Materials and methods). Large rewards arise when (<italic>i</italic>) a large fraction <inline-formula><alternatives><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft71">\begin{document}$ f^{+}$\end{document}</tex-math></alternatives></inline-formula> of agents successfully reaches the source and (<italic>ii</italic>) the agents reach the source quickly, which maximizes <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>Ï</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>success</mml:mtext><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$g^{+}=\langle e^{-\lambda\tau}|\text{success}\rangle$\end{document}</tex-math></alternatives></inline-formula>. Indeed <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$G=f^{+}G^{+}+(1-f^{+})G^{-}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$G^{+}=g^{+}-\sigma(1-g^{+})/(1-\gamma)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft75">\begin{document}$G^{-}=-\sigma(1-e^{-\lambda H_{\text{max}}})/(1-\gamma)$\end{document}</tex-math></alternatives></inline-formula> is the horizon of the agent that is the maximum time the agent is allowed to search, and after which the search is considered failed. Note that agents starting closer to the target receive larger rewards purely because of their initial position. To monitor performance independently on the starting location, we introduce the inverse time to reach the source relative to the shortest-path time from the same initial location, which goes from 0 for failing agents to 1 for ideal agents <inline-formula><alternatives><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft76">\begin{document}$ \langle\tau_{\text{min}}/\tau\rangle$\end{document}</tex-math></alternatives></inline-formula>, independently on their starting location. Note that this is not the quantity that is optimized for. One may specifically target this performance metrics, which is agnostic on the duration of an agentâs path, by discounting proportionally to <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$t/\tau_{\text{min}}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>All four measures of performance plateau to a maximum, suggesting learning has achieved a nearly optimal policy (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Once training is completed, we simulate the trajectory of test agents starting from any of the about 43,000 admissible locations within the plume and moving according to the optimal policy. Admissible locations are defined as any location where the odor is non-zero at least once within the entire simulation. We will recapitulate performance with the cumulative reward <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$ G$\end{document}</tex-math></alternatives></inline-formula> averaged over the test agents and dissect it into speed <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}$g^{+}$\end{document}</tex-math></alternatives></inline-formula>, convergence <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$f^{+}$\end{document}</tex-math></alternatives></inline-formula> and relative time <inline-formula><alternatives><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft81">\begin{document}$\langle\tau_{\text{min}}/\tau\rangle$\end{document}</tex-math></alternatives></inline-formula>.</p></sec></sec><sec id="s2-5"><title>Optimal memory</title><p>By repeating training using different values of <inline-formula><alternatives><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft82">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>, we find that performance depends on memory and an optimal memory <inline-formula><alternatives><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft83">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula> exists (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Why is there an optimal memory? The shortest memory <inline-formula><alternatives><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft84">\begin{document}$ T=1$\end{document}</tex-math></alternatives></inline-formula> corresponds to instantaneous olfactory states: the instantaneous contour maps of the olfactory states are convoluted, and the void state is pervasive (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, top). As a consequence, agents often activate recovery even when they are within the plume. The policy almost always leads to the source (<inline-formula><alternatives><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>79</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>Â±</mml:mo><mml:mn>13</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft85">\begin{document}$f^{+}=79\%\pm 13\%$\end{document}</tex-math></alternatives></inline-formula>) but follows lengthy convoluted paths (<inline-formula><alternatives><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>0.14</mml:mn><mml:mo>Â±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft86">\begin{document}$\tau_{\text{min}}/\tau=0.14\pm 0.05$\end{document}</tex-math></alternatives></inline-formula>, <xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom). As memory increases, the olfactory states become smoother and agents encounter fewer voids (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, center), perform straighter trajectories (<inline-formula><alternatives><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>Â±</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft87">\begin{document}$\tau_{\text{min}}/\tau=0.5\pm 0.3$\end{document}</tex-math></alternatives></inline-formula>), and reach the source reliably (<inline-formula><alternatives><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>Â±</mml:mo><mml:mn>8</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft88">\begin{document}$ f^{+}=95\%\pm 8\%$\end{document}</tex-math></alternatives></inline-formula>), <xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom. Further increasing memory leads to even less voids within the plume and even smoother olfactory states. However â perhaps surprisingly â performance does not further improve but slightly decreases (at <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$ T=50$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>94</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>Â±</mml:mo><mml:mn>8</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft90">\begin{document}$ f^{+}=94\%\pm 8\%$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>0.38</mml:mn><mml:mo>Â±</mml:mo><mml:mn>0.36</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}$\tau_{\text{min}}/\tau=0.38\pm 0.36$\end{document}</tex-math></alternatives></inline-formula>). A long memory is deleterious because it delays recovery from accidentally exiting the plume, thus increasing the number of voids <italic>outside</italic> of the plume (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom). Indeed, agents often leave the plume accidentally as they measure their olfactory state <italic>while they move</italic>. They receive no warning, but realize their mistake after <inline-formula><alternatives><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft92">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula> steps, when they enter the void state and activate recovery to re-enter the plume. The delay is linear with memory when agents recover by backtracking, but it depends on the recovery strategy (see Materials and methods and <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The optimal memory <inline-formula><alternatives><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft93">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula>.</title><p>(<bold>A</bold>) Four measures of performance as a function of memory with backtracking recovery (solid line) show that the optimal memory <inline-formula><alternatives><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft94">\begin{document}$ T^{*}=20$\end{document}</tex-math></alternatives></inline-formula> maximizes average performance and minimizes standard deviation, except for the normalized time. Top: Averages computed over 10 realizations of test trajectories starting from 43,000 initial positions (dash: results with adaptive memory). Bottom: standard deviation of the mean performance metrics for each initial condition (see Materials and methods). (<bold>B</bold>) Average number of times agents encounter the void state along their path, <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$ \langle N_{\emptyset}\rangle$\end{document}</tex-math></alternatives></inline-formula>, as a function of memory (top); cumulative average reward <inline-formula><alternatives><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mi>G</mml:mi><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft96">\begin{document}$\langle G\rangle$\end{document}</tex-math></alternatives></inline-formula> is inversely correlated to <inline-formula><alternatives><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft97">\begin{document}$\langle N_{\emptyset}\rangle$\end{document}</tex-math></alternatives></inline-formula> (bottom), hence the optimal memory minimizes encounters with the void. (<bold>C</bold>) Colormaps: Probability that agents at different spatial locations are in the void state at any point in time, starting the search from anywhere in the plume and representative trajectory of a successful searcher (green solid line) with memory <inline-formula><alternatives><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft98">\begin{document}$ T=1$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft99">\begin{document}$ T=20$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft100">\begin{document}$ T=50$\end{document}</tex-math></alternatives></inline-formula> (left to right). At the optimal memory, agents in the void state are concentrated near the edge of the plume. Agents with shorter memories encounter voids throughout the plume; agents with longer memories encounter more voids outside of the plume as they delay recovery. In all panels, shades are Â± standard deviation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>The role of temporal memory with Brownian recovery strategy (same as main <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</title><p>(<bold>A</bold>) Total cumulative reward (top left) and standard deviation (top right) as a function of memory showing an optimal memory <inline-formula><alternatives><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft101">\begin{document}$ T^{*}=3$\end{document}</tex-math></alternatives></inline-formula> for the Brownian agent. Other measures of performance with their standard deviations show the same optimal memory (bottom). The trade-off between long and short memories discussed in the main text holds, but here exiting the plume is much more detrimental because regaining position within the plume by Brownian motion is much lengthier. (<bold>B</bold>) As for the Backtracking agent, the optimal memory corresponds to minimizing the number of times the agent encounters the void state.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig2-figsupp1-v1.tif"/></fig></fig-group><p>Thus, short memories increase time in void <italic>within</italic> the plume, whereas long memories increase time in void <italic>outside</italic> the plume: the optimal memory minimizes the overall chances to experience the void (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Intuitively, <inline-formula><alternatives><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft102">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula> should match the typical duration <inline-formula><alternatives><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft103">\begin{document}$\tau_{b}$\end{document}</tex-math></alternatives></inline-formula> of blanks encountered within the plume, so that voids within the plume are effectively ignored without delaying recovery unnecessarily. Consistently, <inline-formula><alternatives><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft104">\begin{document}$\langle\tau_{b}\rangle$\end{document}</tex-math></alternatives></inline-formula> averaged across all locations and times within the plume is <inline-formula><alternatives><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo><mml:mo>=</mml:mo><mml:mn>9.97</mml:mn><mml:mo>Â±</mml:mo><mml:mn>41.16</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft105">\begin{document}$\langle\tau_{b}\rangle=9.97\pm 41.16$\end{document}</tex-math></alternatives></inline-formula>, comparable with the optimal memory <inline-formula><alternatives><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft106">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p></sec><sec id="s2-6"><title>Adaptive memory</title><p>There is no way to select the optimal memory <inline-formula><alternatives><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft107">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula> without comparing several agents or relying on prior information on the blank durations. In order to avoid prior information, we venture to define memory adaptively along each agentâs path, using the intuition outlined above. We define a buffer memory <inline-formula><alternatives><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft108">\begin{document}$ T_{b}$\end{document}</tex-math></alternatives></inline-formula>, and let the agent respond to a sensing window <inline-formula><alternatives><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft109">\begin{document}$T \lt T_{b}$\end{document}</tex-math></alternatives></inline-formula>. Ideally, we would like to set <inline-formula><alternatives><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>â¼</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft110">\begin{document}$T\sim\langle\tau_{b}\rangle$\end{document}</tex-math></alternatives></inline-formula>. With this choice, blanks shorter than the average blank are ignored, as they are expected within the plume, whereas blanks longer than average initiate recovery, as they signal that the agent exited the plume. However, agents do not have access to <inline-formula><alternatives><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft111">\begin{document}$\langle\tau_{b}\rangle$\end{document}</tex-math></alternatives></inline-formula> hence we set <inline-formula><alternatives><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft112">\begin{document}$T=\tau_{b}^{-}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft113">\begin{document}$\tau_{b}^{-}$\end{document}</tex-math></alternatives></inline-formula> is the most recent blank experienced by the agent. With this choice, the sensing memory <inline-formula><alternatives><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft114">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula> fluctuates considerably along an agentâs path, due to turbulence (<xref ref-type="bibr" rid="bib17">Celani et al., 2014</xref> and <xref ref-type="fig" rid="fig3">Figure 3AâB</xref>). Note that blanks are estimated along paths, thus the statistics of <inline-formula><alternatives><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft115">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> only qualitatively matches the Eulerian statistics of <inline-formula><alternatives><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft116">\begin{document}$ \tau_{b}$\end{document}</tex-math></alternatives></inline-formula>. Despite the fluctuations, performance using the adaptive memory nears performance with the optimal memory (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This result confirms our intuition that memory should match the blank time. The advantage of adaptive memory is that it relies solely on experience, with no prior information whatsoever. This is unlike <inline-formula><alternatives><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft117">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula> which can only be selected using prior information, with no guarantee of generalization to other plumes.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The adaptive memory approximates the duration of the blank dictated by physics, and it is an efficient heuristic, especially when coupled with a learned recovery strategy.</title><p>(<bold>A</bold>) Top to bottom: Colormaps of the Eulerian average blank time <inline-formula><alternatives><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft118">\begin{document}$ \tau_{b}$\end{document}</tex-math></alternatives></inline-formula>; average sensing memory <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula>; standard deviation of Eulerian blank time and of sensing memory. The sensing memory statistics are computed over all agents that are located at each discrete cell, at any point in time. (<bold>B</bold>) Probability distribution of <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$ \tau_{b}$\end{document}</tex-math></alternatives></inline-formula> across all spatial locations and times (black) and of <inline-formula><alternatives><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft121">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> across all agents at all times (gray). (<bold>C</bold>) Performance with the adaptive memory nears performance of the optimal fixed memory, here shown for backtracking; similar results apply to the Brownian recovery (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>). (<bold>D</bold>) Comparison of five recovery strategies with adaptive memory: The learned recovery with adaptive memory outperforms all fixed and adaptive memory agents. In (<bold>C</bold>) and (<bold>D</bold>), dark squares mark the mean, and light rectangles mark Â± standard deviation. <inline-formula><alternatives><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft122">\begin{document}$ f^{+}$\end{document}</tex-math></alternatives></inline-formula> is defined as the fraction of agents that reach the target at test, hence has no standard deviation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>All four measures of performance across agents with fixed memory and Backtracking vs Brownian recovery (green and red respectively, unframed boxes) and with adaptive memory for Backtracking, Brownian, and Learned recovery (green, red, and blue respectively, framed boxes).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig3-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-7"><title>Learning to recover</title><p>So far, our agents combine a learned policy from non-void states to a heuristic from the void state, which we called the recovery strategy. We have considered biologically inspired heuristics where searchers make it back to locations within the plume by retracing their path backward. To further strip the algorithm of heuristics, we ask whether the recovery strategy may be learned, rather than fixed a priori. To this end, we split the void state in many states, labeled with the time elapsed since first entering the void. We pick 50 void states, as less than 50 void states results in no convergence, and states above 50 are useless because they are rarely visited. The counter is reset to 0 whenever the searcher detects the odor. The definition of the 15 non-void states <inline-formula><alternatives><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft123">\begin{document}$ o_{1},\dots,o_{15}$\end{document}</tex-math></alternatives></inline-formula> remains unaltered. Interestingly, with this added degree of freedom, the agent learns an even better recovery strategy as reflected by all our measures of performance (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Note that the learned recovery strategy resembles the casting behavior observed in flying insects (<xref ref-type="bibr" rid="bib19">David et al., 1983</xref>), as discussed below. In fact, insects deploy a range of recovery strategies depending on locomotor mode and environment. To corroborate these results, we compare performance using two additional biologically-inspired recovery strategies, i.e. circling (observed in windless environments <xref ref-type="bibr" rid="bib61">Stupski and van Breugel, 2024a</xref>), and cast &amp; surge (<xref ref-type="bibr" rid="bib19">David et al., 1983</xref>) as well as a Brownian recovery which does not have a direct biological relevance but represents a simple computational benchmark. The learned recovery outperforms all heuristic recoveries, as seen by the cumulative reward <inline-formula><alternatives><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft124">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Circling is the second-best recovery and shortly follows the learned recovery. Circling achieves nearly optimal performance by further decreasing failures (metrics <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$f^{+}$\end{document}</tex-math></alternatives></inline-formula>), but slowing down (metrics <inline-formula><alternatives><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft126">\begin{document}$g^{+}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft127">\begin{document}$\tau_{min}/\tau$\end{document}</tex-math></alternatives></inline-formula>).</p><sec id="s2-7-1"><title>Characterization of the optimal policies</title><p>To understand how different recoveries affect the agentâs behavior, we characterize the optimal policies obtained using the three recovery strategies. We visualize the probability of encountering each of the 16 olfactory states, or occupancy (circles in <xref ref-type="fig" rid="fig4">Figure 4</xref>), and the spatial distribution of the olfactory states.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Optimal policies with adaptive memory for different recovery strategies: backtracking (green), Brownian (red), and learned (blue).</title><p>For each recovery, we show the spatial distribution of the olfactory states (top); the policy (center) and the state occupancy (bottom) for non-void states (left) <italic>vs</italic> the void state <inline-formula><alternatives><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft128">\begin{document}$ \pi^{*}(a|\emptyset)$\end{document}</tex-math></alternatives></inline-formula>(right). Spatial distribution: probability that an agent at a given position is in any non-void olfactory state (left) or in the void state (right), color-coded from yellow to blue. Policy: actions learned in the non-void states <inline-formula><alternatives><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mo>â </mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft129">\begin{document}$ \sum_{o\neq\emptyset}n_{o}\pi^{*}(a|o)$\end{document}</tex-math></alternatives></inline-formula>, weighted on their occupancy <inline-formula><alternatives><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft130">\begin{document}$ n_{o}$\end{document}</tex-math></alternatives></inline-formula> (left, arrows proportional to the frequency of the corresponding action) and schematic view of recovery policy in the void state (right). State occupancy: fraction of agents that is in any of the 15 non-void states (left) or in the void state (right) at any point in space and time. Occupancy is proportional to the radius of the corresponding circle. The position of the circle identifies the olfactory state (rows and columns indicate the discrete intensity and intermittency respectively). All statistics are computed over 43,000 trajectories, starting from any location within the plume.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Optimal policies for different recovery strategies and adaptive memory.</title><p>From left to right: results for backtracking (green), Brownian (red), and learned (blue) recovery strategies. Top: probability that an agent in a given olfactory state is at a specific spatial location color-coded from yellow to blue. Rows and columns indicate the olfactory state; the void state is in the lower right corner. Arrows indicate the optimal action from that state. Bottom: Circles represent occupancy of each state, olfactory states are arranged as in the top panel. All statistics are computed over 43,000 trajectories, starting from any location within the plume.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>The learned recovery resembles the cast and surge observed in animals, with an initial surge of 5Â±2 steps and a subsequent motion crosswind starting from either side of the centerline and overshooting to the other side.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig4-figsupp2-v1.tif"/></fig></fig-group><p>In the void state, the agent activates the recovery strategy. Recovery from the void state affects non-void olfactory states as well: their occupancy, their spatial distribution, and the action they elicit (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). This is because the agent computes its olfactory state online, according to its prior history which is affected by encounters with the void state. However, for all recoveries, non-void states are mostly encountered within the plume and largely elicit upwind motion (<xref ref-type="fig" rid="fig4">Figure 4</xref>, top, center). Thus macroscopically, all agents learn to surge upwind when they detect any odor within their memory, and to recover when their memory is empty. This suggests a considerable level of redundancy which may be leveraged to reduce the number of olfactory states, thus the computational cost. Reducing the number of non-empty olfactory states drastically to just 1 does indeed show degraded performance (see <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). A systematic optimization of odor representation requires a considerable reformulation of the algorithm, which is beyond the scope of the current work. Note that, exclusively for the learned recovery, the optimal policy is enriched in actions downwind to avoid overshooting the source. Indeed, from positions beyond the source, the learned strategy is unable to recover the plume as it mostly casts sideways, with little to no downwind action. Intuitively, the precise locations where agents move downwind may be crucial to efficiently avoid overshooting. Thus, the policy may depend on specific details of the odor plume, consistent with poorer generalization of the learned recovery (discussed next). We expect that in conditions where overshooting the source is more prominent, downwind motion may emerge as an effective component of the recovery strategy, similar to observations in insects (e.g. <xref ref-type="bibr" rid="bib75">Wolf and Wehner, 2000</xref>; <xref ref-type="bibr" rid="bib4">Ãlvarez-Salvado et al., 2018</xref>).</p><p>The void state shows the most relevant differences: for both heuristic recoveries, 40% or more of the agents are in the void state and they are spatially spread out. In contrast, in the case of learned recovery, the optimal policy limits the occurrence of the void state to 26% of the agents, confined to a narrow band near the edge of the plume. From these locations, the agents quickly recover the plume, explaining the boost in performance discussed above. In all distinct trainings of the agents with learned recovery, we observed that the trajectories in the void start with an initial surge of 6Â±2 steps; continue with either crosswind direction and then switch to the other side, with little to no downwind actions (see <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref> and <xref ref-type="table" rid="table1">Table 1</xref>). This recovery thus mixes aspects of exploitation (surge) to aspects of exploration (cast): we defer a more in-depth analysis that disentangles these two aspects elsewhere.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameters of the learned recovery, statistics over 20 independent trainings.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Initial surge upwind</th><th align="left" valign="bottom">6 Â± 2</th></tr></thead><tbody><tr><td align="left" valign="bottom">Total steps upwind</td><td align="left" valign="bottom">15 Â± 2</td></tr><tr><td align="left" valign="bottom">Total steps downwind</td><td align="left" valign="bottom">1.3 Â± 1.4</td></tr><tr><td align="left" valign="bottom">Total steps to the right</td><td align="left" valign="bottom">15 Â± 3</td></tr><tr><td align="left" valign="bottom">Total steps to the left</td><td align="left" valign="bottom">18 Â± 6</td></tr></tbody></table></table-wrap></sec></sec><sec id="s2-8"><title>Tuning for adaptation to different environments</title><p>Finally, we test the performance of the trained agents on six environments, characterized by distinct fluid flows and odor plumes (<xref ref-type="fig" rid="fig5">Figure 5</xref> and Materials and methods). Environment 1 (âbulk nativeâ) is the environment where the agents were originally trained; Environment 2 (âbulk sparserâ) is obtained by increasing the threshold of detection, which makes the signals considerably more sparse with longer blanks. Environments 3 (âsurfaceâ) and 4 (âsurface sparserâ) are closer to the lower surface of the simulated domain, where the plume is smaller and fluctuates less. Environment 5 (âbulk lower Reâ) is a similar geometry, but obtained for a smaller Reynolds number and a different way to generate turbulence. Finally, Environment 6 (âbulk higher Reâ) has an even larger Reynolds number, a longer domain, and a smaller source, which creates an even more dramatically sparse signal. All bulk environments (1, 2, 5, and 6) are representative of conditions encountered far from a substrate, for example by flying or swimming organisms. The two surface environments (3 and 4) represent odor near surfaces relevant to terrestrial or benthic navigation (but not directly applicable to trail tracking, where odor traces <italic>on</italic> the substrate are tracked). Note that we consider a Schmidt number Sc = 1 appropriate for odors in air but not in water. However, we expect a weak dependence on the Schmidt number as the Batchelor and Kolmogorov scales are below the size of the source and we are interested in the large scale statistics (<xref ref-type="bibr" rid="bib24">Falkovich et al., 2001</xref>; <xref ref-type="bibr" rid="bib17">Celani et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Duplat et al., 2010</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Generalization to statistically different environments.</title><p>(<bold>A</bold>) Snapshots of odor concentration normalized with concentration at the source, color-coded from blue (0) to yellow (1) for environment 1â6 as labeled. Environment 1 is the native environment where all agents are trained. (<bold>B</bold>) Performance for the five recovery strategies backtracking (green), learned (blue), circling (orange), zigzag (purple) and brownian (red), with adaptive memory, trained on the native environment and tested across all environments 1â6. Four measures of performance defined in the main text are shown. Dark squares mark the mean, and empty rectangles Â± standard deviation. For definition of the metrics used, see Materials and Methods, Agents Evaluation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>The learned recovery with adaptive memory and a single non-empty olfactory state (empty circles) displays degraded performance with respect to the full model (full circles).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102906-fig5-figsupp1-v1.tif"/></fig></fig-group><p>We consider agents with adaptive memory and compare the five recovery strategies discussed above â backtracking, learned, circling, cast and surge, and Brownian, see <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Comparing performance across environments, we find that: (<italic>i</italic>) although performance is degraded when testing in non-native environments, backtracking, learned and circling recoveries with adaptive memory are still extremely likely to find the source. The upshot of generalization is that agents may navigate distinct turbulent plumes using a baseline strategy learned in a specific plume. Importantly, as most of these agents still do reach the source, fine-tuning may enable efficient adaptation to different environments. Further work is needed to establish how much fine-tuning is needed to fully adapt to different environments. (<italic>ii</italic>) Brownian and cast and surge recoveries have the lowest performance and generalization across all environments. Cast and surge is often used as a comparison (<xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>) and can be extremely effective: our results do not contradict the literature, but simply showcase that cast width and surge length need to be carefully defined. (<italic>iii</italic>) The cumulative reward <inline-formula><alternatives><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft131">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> shows that the learned recovery is the best at generalizing to environments 2, 5, and 6. Particularly, in the most intermittent Environment 6 a striking 91% of agents succeed in finding the source, with trajectories less than twice as long as the shortest path to the source. (<italic>iv</italic>) Circling is the best at generalizing to environments 3 and 4, representative of less intermittent regions near the substrate. As observed for the native environment, circling favors success rate (metrics <inline-formula><alternatives><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft132">\begin{document}$ f^{+}$\end{document}</tex-math></alternatives></inline-formula>) against speed (metrics <inline-formula><alternatives><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$g^{+}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft134">\begin{document}$\tau_{\text{min}}/\tau$\end{document}</tex-math></alternatives></inline-formula>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we showed that agents exposed to a turbulent plume learn to associate salient features of the odor time trace â the olfactory state â to an optimal move that guides them to the odor source. The upshot of responding solely to odor is that the agent does not navigate based on <italic>where</italic> it believes the target is and thus needs no map of space nor prior information about the odor plume, which avoids considerable computational burden. The only spatial awareness needed to implement this algorithm is the ability to orient motion relative to the mean flow, which is assumed known. In reality, animals cannot measure the mean flow but rely on local measures of flow speed, using for example antennas for insects (<xref ref-type="bibr" rid="bib52">Reynolds et al., 2010</xref>; <xref ref-type="bibr" rid="bib10">Bell and Kramer, 1979</xref>; <xref ref-type="bibr" rid="bib64">Suver et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Okubo et al., 2020</xref>), whiskers for rodents (<xref ref-type="bibr" rid="bib77">Yu et al., 2016</xref>) or the lateral line for marine organisms (<xref ref-type="bibr" rid="bib39">Liao, 2006</xref>). Further work is needed to bridge the gap with our simplified setting. On the flip side, in our stimulus-response algorithm, agents need to start from within the plume, however sparse and fragmented. Indeed, far enough from the source, Q-learning agents are mostly in the void state and they can only recover the plume if they have previously detected the odor or are right outside the plume. In contrast, agents using a map of space can navigate from larger distances than are reachable by responding directly to odor cues. Indeed, in the map-based POMDP setting, absence of odor detection is still informative and it enables agents to first find the plume and then refine the search to localize the target within the plume (<xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref>; <xref ref-type="bibr" rid="bib41">Loisy and Heinonen, 2023</xref>).</p><p>We show that because the odor signal within a turbulent plume constantly switches on and off, navigation must handle both absence and presence of odor stimuli. We address this fundamental issue by alternating between two distinct strategies: (<italic>i</italic>) Prolonged absence of odor prompts entry in the void state and triggers a recovery strategy to make contact with the plume again. We explored four heuristic recoveries and found that spiraling around the location where the agent last detected odor is the most efficient heuristic, which privileges reliable success rather than speed. An even more efficient recovery can be learned that resembles cross-wind casting and limits the void state to a narrow region right outside of the plume. Casting is a well-studied computational strategy (<xref ref-type="bibr" rid="bib6">Baker, 1990</xref>; <xref ref-type="bibr" rid="bib8">Balkovsky and Shraiman, 2002</xref>) also observed in animal behavior, most famously in flying insects (<xref ref-type="bibr" rid="bib19">David et al., 1983</xref>). Intriguingly, cast and surge also emerges in algorithms making use of a model of the odor, whether for Bayesian updates or for policy optimization (<xref ref-type="bibr" rid="bib68">Vergassola et al., 2007</xref>; <xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref>; <xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>). Whether natural casting behavior is learned, as in Q-learning, or is hard-wired in a model of the odor plume remains a fascinating question for further research. Clearly, the width of the casts and length of the surges are of crucial importance: a hard-wired cast and surge recovery with arbitrary parameters shows poor performance. (<italic>ii</italic>) Odor detections prompt entry in non-void olfactory states, which predominantly elicit upwind surge. Blanks shorter than the sensing memory are ignored, that is agents do not enact recovery but respond to stimuli experienced prior to the short blank. The non-void olfactory states are crafted based on biologically plausible features which have been shown to harness positional information. Further work may optimize these non-void olfactory states by feature engineering, for example testing different discretizations to reduce redundancy. A drastic reduction to a single non-void olfactory state degrades performance, suggesting screening a large library of features using supervised learning as in <xref ref-type="bibr" rid="bib53">Rigolli et al., 2022a</xref> may be used to potentially improve performance. This approach will provide a systematic ranking of the most efficient temporal features of odor time traces for navigation; however, it will have to test different memories, discretizations, and regression algorithms as well, making it cumbersome. Alternatively, feature engineering may be bypassed altogether by the use of recurrent neural networks (RNNs) (<xref ref-type="bibr" rid="bib60">Singh et al., 2023</xref>) or finite state controllers (<xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>). These algorithms are appealing in that they bypass entirely the need to hand-craft explicit features of odor time traces. On the flip side, they provide no explicit handle on the odor features that drive behavior nor on the specific duration of the temporal memory and how it is related to the physics of the odor cues. Thus, to extract these information, extra work is needed to interrogate these algorithms. For example, principal component analysis in <xref ref-type="bibr" rid="bib60">Singh et al., 2023</xref> suggests the hidden state of trained agents correlates with biologically relevant variables, including head direction, odor concentration, and time since last detection. Finally, a systematic comparison using a common dataset is needed to elucidate how other heuristic and normative model-free algorithms handle odor presence <italic>vs</italic> odor absence.</p><p>To switch between the odor-driven strategy and the recovery strategy, we introduce a timescale <inline-formula><alternatives><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft135">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula>, which is an explicit form of temporal memory. <inline-formula><alternatives><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft136">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> delimits a sensing window extending in the recent past, prior to the present time. All odor stimuli experienced within the sensing window affect the current response. By using fixed memories of different durations, we demonstrate that an optimal memory exists and that the optimal memory minimizes the occurrence of the void state. On the one hand, long memories are detrimental because they delay recovery from accidentally exiting the plume. On the other hand, short memories are detrimental because they trigger recovery unnecessarily, i.e. even for blanks typically experienced within the turbulent plume. The optimal memory thus matches the typical duration of the blanks. To avoid using prior information on the statistics of the odor, we propose a simple heuristic setting memory adaptively equal to the most recent blank experienced along the path. The adaptive memory nears optimal performance despite dramatic fluctuations dictated by turbulence. Success of the heuristics suggests that a more accurate estimate of the future blank time may enable an even better adaptive memory; further work is needed to corroborate this idea.</p><p>Thus, in Q-learning, memory is a temporal window matching odor blanks and distinguishing whether agents are in or out of the plume. The role of memory for olfactory search has been recently discussed in <xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>. In POMDPs, memory is stored in a detailed belief of agent position relative to the source. In finite state controllers, memory denotes an internal state of the agent and was linked to a coarse-grained belief of the searcher being within or outside of the plume, similar to our findings. In recurrent neural networks, memory is stored in the learned weights. A quantitative relationship between these different forms of memory and their connection to spatial perception remains to be understood.</p><p>We conclude by listing a series of experiments to test these ideas in living systems. First, olfactory search in living systems displays memory (<xref ref-type="bibr" rid="bib67">Verano et al., 2023</xref>; <xref ref-type="bibr" rid="bib7">Baker et al., 2018</xref> and references therein). In insects, temporal scales can be measured associated with memory. Indeed, for flying insects, loss of contact with a pheromone plume triggers crosswind casting and sometimes even downwind displacement (<xref ref-type="bibr" rid="bib14">CardÃ©, 2021</xref>; <xref ref-type="bibr" rid="bib37">Kuenen and CardÃ©, 1994</xref>). Interestingly, the onset of casting is delayed with respect to loss of contact with the plume (<xref ref-type="bibr" rid="bib37">Kuenen and CardÃ©, 1994</xref>; <xref ref-type="bibr" rid="bib66">van Breugel and Dickinson, 2014</xref>), but this delay is not understood. Similarly, an odor detection elicits upwind surges that can outlast the odor by several seconds (<xref ref-type="bibr" rid="bib34">Kathman et al., 2024</xref>; <xref ref-type="bibr" rid="bib4">Ãlvarez-Salvado et al., 2018</xref>). In walking flies, the timing of previous odor encounters biases navigation (<xref ref-type="bibr" rid="bib20">Demir et al., 2020</xref>). (How) do these temporal timescales depend on the waiting times between previous detections? Using optogenetics (<xref ref-type="bibr" rid="bib25">Gepner et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Hernandez-Nunez et al., 2015</xref>; <xref ref-type="bibr" rid="bib42">Matheson et al., 2022</xref>; <xref ref-type="bibr" rid="bib62">Stupski and van Breugel, 2024b</xref>) or olfactory virtual reality with controlled odor delivery (<xref ref-type="bibr" rid="bib49">Radvansky and Dombeck, 2018</xref>), experiments may measure memory as a function of the full history of odor traces. For insects, one may monitor memory by tracking the onset of crosswind casting with respect to the loss of the plume. More in general, a temporal memory may be defined by monitoring how far back in the past two odor traces should be identical in order to elicit the same repertoire of motor controls.</p><p>Second, our algorithm learns a stimulus-response strategy that relies solely on odor cues. The price to pay is that the agent must follow the ups and downs of the odor trace in order to compute averages and recognize blanks. A systematic study may use our algorithm to test the requirements of fidelity of this temporal representation and how it depends on turbulence. How does turbulence affect the fidelity of odor temporal representation in living systems? Crustaceans provide an excellent model system to ask this question, as they are known to use bursting olfactory receptor neurons to encode temporal information from olfactory scenes (<xref ref-type="bibr" rid="bib12">Bobkov and Ache, 2007</xref>; <xref ref-type="bibr" rid="bib1">Ache et al., 2016</xref>). Temporal information is also encoded in the olfactory bulb of mammals (<xref ref-type="bibr" rid="bib15">Carey et al., 2009</xref>; <xref ref-type="bibr" rid="bib2">Ackels et al., 2021</xref>). Organisms with chemo-tactile systems like the octopus (<xref ref-type="bibr" rid="bib3">Allard et al., 2023</xref>) may serve as a comparative model, to ask whether touch-chemosensation displays a sloppier temporal response, reflecting that surface-bound stimuli are not intermittent.</p><p>Third, our Q-learning algorithm requires the agent to receive olfactory information, thus start near or within the odor plume. In contrast, algorithms making use of a spatial map and prior information on the odor plume may first search for the plume (in conditions of near zero information) and then search the target within the plume (<xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref>; <xref ref-type="bibr" rid="bib41">Loisy and Heinonen, 2023</xref>; <xref ref-type="bibr" rid="bib68">Vergassola et al., 2007</xref>). Animals are known to use prior information to home into regions of space where the target is more likely to be found; but they can switch to navigation in response to odor (see e.g. <xref ref-type="bibr" rid="bib14">CardÃ©, 2021</xref>; <xref ref-type="bibr" rid="bib57">Schal, 1982</xref>; <xref ref-type="bibr" rid="bib27">Gire et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Baker et al., 2018</xref>). What triggers the switch from spatial navigation driven by prior information to sensory driven navigation using odor? For mice, the need for spatial perception may be tested indirectly by comparing paths in light <italic>vs</italic> dark, noting that neuronal place fields, that mediate spatial perception, are better stabilized by vision than olfaction (<xref ref-type="bibr" rid="bib56">Save et al., 2000</xref>; <xref ref-type="bibr" rid="bib78">Zhang and Manahan-Vaughan, 2015</xref>). Thus in the light, animals have the ability to implement both map-less and map-based algorithms, whereas in the dark they are expected to more heavily rely on map-less algorithms. To make sure animals start searching for the odor target even before sensing odor, operant conditioning can be deployed so that animals associate an external cue (e.g. a sound) to the beginning of the task. Note that distinct species control locomotion differently, and as a result, trajectories are usually far more complex than a sequence of discrete steps on a checkerboard. Thus, to compare algorithms to animal behavior, a more detailed model of the specific motor controls is to be developed.</p><p>The reinforcement learning view of olfactory navigation offers an exciting opportunity to probe how living systems interact with the environment to accomplish complex real-world tasks affected by uncertainty. Coupling time-varying odor stimuli with spatial perception is an instance of the broader question asking how animals combine prior knowledge regarding the environment with reaction to sensory stimuli. We hope that our work will spark further progress into connecting these broader questions to the physics of fluids.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Computational fluid dynamics source code</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib73">Viola et al., 2023</xref>; <xref ref-type="bibr" rid="bib71">Viola et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Viola et al., 2022</xref>; <xref ref-type="bibr" rid="bib70">Verzicco et al., 2025</xref>. Courtesy of F. Viola.</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://gitlab.com/vdv9265847/IBbookVdV/">https://gitlab.com/vdv9265847/IBbookVdV/</ext-link></td><td align="left" valign="bottom">Reused Computational Fluid Dynamics software used to run simulations of odor transport. An earlier version of the code is publicly available at the website indicated in the âidentifiersâ entry, described in <xref ref-type="bibr" rid="bib70">Verzicco et al., 2025</xref>. Our simulations were conducted using a GPU-accelerated version of the code that was developed by F. Viola and colleagues in the Refs indicated in the âSource or referenceâ entry. This version will be shared in the near future. All requests may be directed to Viola and colleagues</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Datasets of odor field obtained with computational fluid dynamics â Environments 5 and 6</td><td align="left" valign="bottom">This paper</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14655991">https://doi.org/10.5281/zenodo.14655991</ext-link></td><td align="left" valign="bottom">Newly developed datasets of turbulent odor fields, obtained through computational fluid dynamics.</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Tabular Q-learning</td><td align="left" valign="bottom">This paper, <xref ref-type="bibr" rid="bib50">Rando, 2025</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/Akatsuki96/qlearning_for_navigation">https://github.com/Akatsuki96/qlearning_for_navigation</ext-link></td><td align="left" valign="bottom">Newly developed Model-free Algorithm for training olfactory search agents, with settings described in materials and methods. Shared on github address mentioned as âIdentifierâ</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Datasets of odor field obtained with computational fluid dynamics â Environments 1â4</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6538177">https://doi.org/10.5281/zenodo.6538177</ext-link></td><td align="left" valign="bottom">Reused datasets of turbulent odor fields, obtained through computational fluid dynamics in <xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref></td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Data description</title><p>The data we used to train the agents (Environment 1) is a set of 2598 matrices <inline-formula><alternatives><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2598</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft137">\begin{document}$ \{D_{t}\}_{t=1}^{2598}$\end{document}</tex-math></alternatives></inline-formula>. Every matrix <inline-formula><alternatives><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1225</mml:mn><mml:mo>Ã</mml:mo><mml:mn>280</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft138">\begin{document}$D_{t}\in\mathbb{R}^{1225\times 280}$\end{document}</tex-math></alternatives></inline-formula> contains the odor intensity in every position <inline-formula><alternatives><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft139">\begin{document}$ (i,j)$\end{document}</tex-math></alternatives></inline-formula> i.e. <inline-formula><alternatives><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft140">\begin{document}$ (D_{t})_{i,j}$\end{document}</tex-math></alternatives></inline-formula> represents the odor intensity in position <inline-formula><alternatives><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft141">\begin{document}$ (i,j)$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft142">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula>. The source of odor is in position (20, 142), and in order to simplify the training, we considered as terminal states every position in a circle centered in the source position and with radius 10 called the <italic>source region</italic>. Data are obtained from a direct numerical simulation of the Navier-Stokes equations and the equations of transport of the odor. Environments 1â4 are derived from Simulation 1, a direct numerical simulation of a channel flow described in <xref ref-type="bibr" rid="bib53">Rigolli et al., 2022a</xref> and used to develop a POMDP algorithm in <xref ref-type="bibr" rid="bib54">Rigolli et al., 2022b</xref>, dataset available from <xref ref-type="bibr" rid="bib55">Rigolli et al., 2022c</xref>. The simulation represents a boundary layer whose dimensions match the lowest â¼1 m of the atmosphere and with horizontal dimensions â¼1.9 Ã 9.5 m and Reynolds number <inline-formula><alternatives><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mn>1400</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft143">\begin{document}$ Re_{\lambda}\sim 1400$\end{document}</tex-math></alternatives></inline-formula> (on the low side of atmospheric Reynolds typically ranging from 1000â10,000 <xref ref-type="bibr" rid="bib28">Gulitski et al., 2007</xref>). Odor snapshots are extracted at a height <inline-formula><alternatives><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>â¼</mml:mo><mml:mn>.5</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft144">\begin{document}$\sim.5\,\rm m$\end{document}</tex-math></alternatives></inline-formula> from the ground for Environments 1 and 2 and <inline-formula><alternatives><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>â¼</mml:mo><mml:mn>.01</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft145">\begin{document}$ \sim.01\,\rm m$\end{document}</tex-math></alternatives></inline-formula> for Environments 3 and 4 respectively. We preprocess the data to zero every entry of these matrices when they are smaller than a <italic>noise level</italic> <inline-formula><alternatives><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>lvl</mml:mtext></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mn>3</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft146">\begin{document}$ n_{\text{lvl}}:=3\times 10^{-6}$\end{document}</tex-math></alternatives></inline-formula> (or 0.13% relative to concentration at the source). The noise level is increased to 0.22% in Environments 2 and 4. Data information are summarized in <xref ref-type="table" rid="table2 table3">Tables 2 and 3</xref>. Levels of intermittency in <xref ref-type="fig" rid="fig1">Figure 1</xref> show that only a thin core region has intermittency larger than 66%, whereas the most challenging regions at the edge of the plume have intermittency under 33%. For reference, experimental values of 25% to 20% were reported for a surrogate odor in the atmospheric boundary layer, along the centerline at 2â15 m from the source (<xref ref-type="bibr" rid="bib44">Murlis and Jones, 1981</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Gridworld geometry.</title><p>From Top: 2D size of the simulation, agents that leave the simulation box continue to receive negative reward and no odor; number of time stamps in the simulation, beyond which simulations are looped; number of actions per time stamp; speed of the agent; noise level below which odor is not detected; location of the source on the grid. See <xref ref-type="table" rid="table3">Table 3</xref> for the values of the grid size <inline-formula><alternatives><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft147">\begin{document}$\Delta x$\end{document}</tex-math></alternatives></inline-formula> and time stamps at which odor snapshots are saved.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Simulation 1</th><th align="left" valign="bottom">Simulation 2</th><th align="left" valign="bottom">Simulation 3</th></tr></thead><tbody><tr><td align="left" valign="bottom">2D simulation grid</td><td align="left" valign="bottom">1225 Ã 280</td><td align="left" valign="bottom">1024 Ã 256</td><td align="left" valign="bottom">2000 Ã 500</td></tr><tr><td align="left" valign="bottom"># time stamps</td><td align="left" valign="bottom">2598</td><td align="left" valign="bottom">5000</td><td align="left" valign="bottom">5000</td></tr><tr><td align="left" valign="bottom"># decisions per time stamp</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Speed (grid points / time stamp)</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">10</td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>lvl</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft148">\begin{document}$ n_{\text{lvl}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft149">\begin{document}$3\times 10^{-6}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft150">\begin{document}$ 3\times 10^{-6}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft151">\begin{document}$ 10^{-4}$\end{document}</tex-math></alternatives></inline-formula></td></tr><tr><td align="left" valign="bottom">Source location</td><td align="left" valign="bottom">(20, 142)</td><td align="left" valign="bottom">(128, 128)</td><td align="left" valign="bottom">(150, 250)</td></tr></tbody></table></table-wrap><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Parameters of the simulations.</title><p>From Left to Right: Simulation ID (1, 2, 3); Length <inline-formula><alternatives><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft152">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula>, width <inline-formula><alternatives><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft153">\begin{document}$ W$\end{document}</tex-math></alternatives></inline-formula>, height <inline-formula><alternatives><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft154">\begin{document}$ H$\end{document}</tex-math></alternatives></inline-formula> of the computational domain; mean horizontal speed <inline-formula><alternatives><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mi>u</mml:mi><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft155">\begin{document}$ U_{b}=\langle u\rangle$\end{document}</tex-math></alternatives></inline-formula>; Kolmogorov length scale <inline-formula><alternatives><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î·</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Î½</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Ïµ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft156">\begin{document}$ \eta=(\nu^{3}/\epsilon)^{1/4}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î½</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft157">\begin{document}$\nu$\end{document}</tex-math></alternatives></inline-formula> is the kinematic viscosity and <inline-formula><alternatives><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft158">\begin{document}$\epsilon$\end{document}</tex-math></alternatives></inline-formula> is the energy dissipation rate; mean size of gridcell <inline-formula><alternatives><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft159">\begin{document}$ \Delta x$\end{document}</tex-math></alternatives></inline-formula>; Kolmogorov timescale <inline-formula><alternatives><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>Î·</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Î½</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft160">\begin{document}$\tau_{\eta}=\eta^{2}/\nu$\end{document}</tex-math></alternatives></inline-formula>; energy dissipation rate <inline-formula><alternatives><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ïµ</mml:mi><mml:mo>=</mml:mo><mml:mi>Î½</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft161">\begin{document}$ \epsilon=\nu/2\langle(\partial u_{i}/\partial x_{j}+\partial u_{j}/\partial x_ {i})^{2}\rangle$\end{document}</tex-math></alternatives></inline-formula>; wall unit <inline-formula><alternatives><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Î½</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft162">\begin{document}$y^{+}=\nu/u_{\tau}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft163">\begin{document}$ u_{\tau}$\end{document}</tex-math></alternatives></inline-formula> is the friction velocity; bulk Reynolds number <inline-formula><alternatives><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>Î½</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft164">\begin{document}$Re_{b}=U_{b}(H/2)/\nu$\end{document}</tex-math></alternatives></inline-formula> based on the bulk speed <inline-formula><alternatives><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft165">\begin{document}$ U_{b}$\end{document}</tex-math></alternatives></inline-formula> and half height; magnitude of velocity fluctuations <inline-formula><alternatives><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft166">\begin{document}$ u^{\prime}$\end{document}</tex-math></alternatives></inline-formula> relative to the bulk speed; large eddy turnover time <inline-formula><alternatives><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft167">\begin{document}$ T=H/2u^{\prime}$\end{document}</tex-math></alternatives></inline-formula> frequency at which odor snapshots are saved <inline-formula><alternatives><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>Ã</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft168">\begin{document}$\times\tau_{\eta}$\end{document}</tex-math></alternatives></inline-formula>. For each simulation, the first row reports results in non-dimensional units. Second and third rows provide an idea of how non-dimensional parameters match dimensional parameters in real flows in air and water, assuming the Kolmogorov length is 1.5 mm in air and 0.4 mm in water.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Sim ID</th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft169">\begin{document}$ L$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft170">\begin{document}$ W$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft171">\begin{document}$ H$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft172">\begin{document}$ U_{b}$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft173">\begin{document}$ \eta$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft174">\begin{document}$\Delta x$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft175">\begin{document}$ \tau_{\eta}$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft176">\begin{document}$ \epsilon$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft177">\begin{document}$ y^{+}$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft178">\begin{document}$ {Re}_{b}$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft179">\begin{document}$ \frac{u^{\prime}}{U_{b}}$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft180">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula></th><th align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>save</mml:mtext></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft181">\begin{document}$\omega_{\text{save}}\times\tau_{\eta}$\end{document}</tex-math></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom">1</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">23</td><td align="left" valign="bottom">0.006</td><td align="left" valign="bottom">0.025</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">39</td><td align="left" valign="bottom">0.0035</td><td align="left" valign="bottom">11500</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>64</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft182">\begin{document}$64\tau_{\eta}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">air</td><td align="left" valign="bottom">9.50 m</td><td align="left" valign="bottom">1.90 m</td><td align="left" valign="bottom">0.96 m</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>36</mml:mn><mml:mfrac><mml:mtext>cm</mml:mtext><mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft183">\begin{document}$ 36 \frac{\text{cm}}{\text{s}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.15 cm</td><td align="left" valign="bottom">0.6 cm</td><td align="left" valign="bottom">0.15 s</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>6.3</mml:mn><mml:mi>e</mml:mi><mml:mo>â</mml:mo><mml:mn>4</mml:mn><mml:mfrac><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>s</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft184">\begin{document}$6.3e-4 \frac{\text{m}^{2}}{\text{s}^{3}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.09 cm</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">water</td><td align="left" valign="bottom">2.66 m</td><td align="left" valign="bottom">0.53 m</td><td align="left" valign="bottom">0.27 m</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>8.6</mml:mn><mml:mfrac><mml:mtext>cm</mml:mtext><mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft185">\begin{document}$8.6 \frac{\text{cm}}{\text{s}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.04 cm</td><td align="left" valign="bottom">0.2 cm</td><td align="left" valign="bottom">0.18 s</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mi>e</mml:mi><mml:mo>â</mml:mo><mml:mn>5</mml:mn><mml:mfrac><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>s</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft186">\begin{document}$3e-5\frac{\text{m}^{2}}{\text{s}^{3}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.02 cm</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">2</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">0.004</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">0.005</td><td align="left" valign="bottom">163</td><td align="left" valign="bottom">0.0038</td><td align="left" valign="bottom">7830</td><td align="left" valign="bottom">15%</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>95</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft187">\begin{document}$95\tau_{\eta}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">5</td></tr><tr><td align="left" valign="bottom">air</td><td align="left" valign="bottom">7.50 m</td><td align="left" valign="bottom">1.875 m</td><td align="left" valign="bottom">0.75 m</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>17.5</mml:mn><mml:mfrac><mml:mtext>cm</mml:mtext><mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft188">\begin{document}$17.5 \frac{\text{cm}}{\text{s}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.15 cm</td><td align="left" valign="bottom">0.75 cm</td><td align="left" valign="bottom">0.15 s</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>6.3</mml:mn><mml:mi>e</mml:mi><mml:mo>â</mml:mo><mml:mn>4</mml:mn><mml:mfrac><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>s</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft189">\begin{document}$6.3e-4\frac{\text{m}^{2}}{\text{s}^{3}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.142 cm</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">water</td><td align="left" valign="bottom">2.00 m</td><td align="left" valign="bottom">0.50 m</td><td align="left" valign="bottom">0.20 m</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3.9</mml:mn><mml:mfrac><mml:mtext>cm</mml:mtext><mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft190">\begin{document}$ 3.9 \frac{\text{cm}}{\text{s}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.04 cm</td><td align="left" valign="bottom">0.2 cm</td><td align="left" valign="bottom">0.18 s</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mi>e</mml:mi><mml:mo>â</mml:mo><mml:mn>5</mml:mn><mml:mfrac><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>s</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft191">\begin{document}$3e-5 \frac{\text{m}^{2}}{\text{s}^{3}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.038 cm</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">3</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">22</td><td align="left" valign="bottom">0.0018</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">0.0025</td><td align="left" valign="bottom">204</td><td align="left" valign="bottom">0.0012</td><td align="left" valign="bottom">17500</td><td align="left" valign="bottom">13%</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>141</mml:mn><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft192">\begin{document}$141\tau_{\eta}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">2.5</td></tr><tr><td align="left" valign="bottom">air</td><td align="left" valign="bottom">16.7 m</td><td align="left" valign="bottom">4.18 m</td><td align="left" valign="bottom">1.67 m</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>30.6</mml:mn><mml:mfrac><mml:mtext>cm</mml:mtext><mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft193">\begin{document}$30.6 \frac{\text{cm}}{\text{s}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.15 cm</td><td align="left" valign="bottom">0.83 cm</td><td align="left" valign="bottom">0.15 s</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>6.3</mml:mn><mml:mi>e</mml:mi><mml:mo>â</mml:mo><mml:mn>4</mml:mn><mml:mfrac><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>s</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft194">\begin{document}$6.3e-4 \frac{\text{m}^{2}}{\text{s}^{3}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.1 cm</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">water</td><td align="left" valign="bottom">4.44 m</td><td align="left" valign="bottom">1.11 m</td><td align="left" valign="bottom">0.44 m</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>6.8</mml:mn><mml:mfrac><mml:mtext>cm</mml:mtext><mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft195">\begin{document}$6.8 \frac{\text{cm}}{\text{s}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.04 cm</td><td align="left" valign="bottom">0.22 cm</td><td align="left" valign="bottom">0.18 s</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mi>e</mml:mi><mml:mo>â</mml:mo><mml:mn>5</mml:mn><mml:mfrac><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>s</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="inft196">\begin{document}$3e-5 \frac{\text{m}^{2}}{\text{s}^{3}}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.03 cm</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><p>Environments 5 and 6 correspond to horizontal slices at mid height extracted from two additional simulations we performed to corroborate the results (Simulation 2 and 3). In Simulation 2, the odor is advected by a turbulent open channel flow, with three hemispherical obstacles placed on the ground close to the inlet to generate turbulence. The Navier-Stokes <xref ref-type="disp-formula" rid="equ1">equations (1)</xref> and advection-diffusion equation for odor transport (3) are solved using a central second-order finite difference scheme. The convective terms are discretized in time using an explicit AdamsâBashforth method, and the viscous and diffusion terms using an implicit Crank-Nicolson method (<xref ref-type="bibr" rid="bib73">Viola et al., 2023</xref>; <xref ref-type="bibr" rid="bib71">Viola et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Viola et al., 2022</xref>). The code is written in Fortran and is GPU parallelized. The channel is divided into 1024 Ã 256 Ã 128 grid points along streamwise, spanwise, and wall-normal directions respectively. The corresponding average spatial resolutions are <inline-formula><alternatives><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mi>Î·</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mi>Î·</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft197">\begin{document}$\Delta x=5\eta,\Delta y=5\eta,\Delta z=4\eta$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft198">\begin{document}$ \eta$\end{document}</tex-math></alternatives></inline-formula> is the Kolmogorov length scale. Three hemispheres of radius <inline-formula><alternatives><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>100</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft199">\begin{document}$ 100\eta$\end{document}</tex-math></alternatives></inline-formula> are placed at a distance of <inline-formula><alternatives><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>250</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft200">\begin{document}$ 250\eta$\end{document}</tex-math></alternatives></inline-formula> from the inlet on the ground, equally spaced along the spanwise direction. The obstacles are implemented using the immersed boundary method (<xref ref-type="bibr" rid="bib70">Verzicco et al., 2025</xref>). The channel is forced using a constant pressure gradient. For the velocity field, we impose a no-slip boundary condition at the ground and on the obstacles (<inline-formula><alternatives><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft201">\begin{document}$ {\boldsymbol{u}}=0$\end{document}</tex-math></alternatives></inline-formula>) and a free-slip boundary on top (<inline-formula><alternatives><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft202">\begin{document}$ u_{z}=0,\partial_{z}u_{x}=\partial_{z}u_{y}=0$\end{document}</tex-math></alternatives></inline-formula>). The velocity field is periodic along the streamwise and spanwise directions. The bulk Reynolds number is 7800. For the odor field, we impose Dirichlet condition (<inline-formula><alternatives><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft203">\begin{document}$ c=0$\end{document}</tex-math></alternatives></inline-formula>) at the ground, on the obstacles and inlet, no-flux (<inline-formula><alternatives><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft204">\begin{document}$\partial_{z}c=0$\end{document}</tex-math></alternatives></inline-formula>) on top, and outflow along other directions. Similar to the native environment, we choose the Schmidt number to be 1. The odor source is located downstream of the obstacle and centered at [<inline-formula><alternatives><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>640</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft205">\begin{document}$640\eta$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>640</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft206">\begin{document}$ 640\eta$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>256</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft207">\begin{document}$ 256\eta$\end{document}</tex-math></alternatives></inline-formula>] along streamwise, spanwise, and wall-normal directions. respectively. The odor source has a Gaussian profile with a standard deviation of <inline-formula><alternatives><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>8</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft208">\begin{document}$ 8\eta$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Simulation 3 is similar to Simulation 2, albeit with a higher bulk Reynolds number of 17,500. Here, the channel is divided into 2000 Ã 500 Ã 200 grid points and has an average spatial resolution of <inline-formula><alternatives><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>5.5</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft209">\begin{document}$ \Delta x=\Delta y=\Delta z=5.5\eta$\end{document}</tex-math></alternatives></inline-formula>. The odor source has a Gaussian profile centered at [<inline-formula><alternatives><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>825</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft210">\begin{document}$ 825\eta$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1375</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft211">\begin{document}$ 1375\eta$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>550</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft212">\begin{document}$550\eta$\end{document}</tex-math></alternatives></inline-formula>] with a standard deviation of <inline-formula><alternatives><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mi>Î·</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft213">\begin{document}$ 3\eta$\end{document}</tex-math></alternatives></inline-formula>. For Environments 5 and 6, the noise level is 0.01% relative to concentration at the source. See Table for a summary of parameters and how they match the physical dimensions of the domain.<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mi mathvariant="bold">â</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>Î¼</mml:mi><mml:msup><mml:mi mathvariant="bold">â</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="bold">â</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  \begin{array}{ll}\rho \left(\frac{\partial u}{\partial t} + \boldsymbol{u}.\boldsymbol{\nabla u} \right) =- {\nabla \boldsymbol P}+\mu \boldsymbol\nabla^2 u + \boldsymbol f;\\\quad \quad \quad \quad \quad \boldsymbol{\nabla u}=0. \end{array}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>.</mml:mo><mml:mi mathvariant="normal">â</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mi mathvariant="normal">â</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  \frac{\partial z}{\partial t} +\mathbf{u} .\nabla z=D\nabla^2 z+s.$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-2"><title>Olfactory states, features, and discretization</title><p>Each agent stores the odor concentrations detected in the previous <inline-formula><alternatives><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft214">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> time steps in a vector <inline-formula><alternatives><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft215">\begin{document}$ \mathbf{M}=(z(s(t-T),t-T),...,z(s(t),t))$\end{document}</tex-math></alternatives></inline-formula>. We introduce an adaptive sensitivity threshold function <inline-formula><alternatives><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft216">\begin{document}$s_{\text{thr}}(\cdot)$\end{document}</tex-math></alternatives></inline-formula> defined as<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mtext>thr</mml:mtext></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">{</mml:mo></mml:mrow><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mtext>thr</mml:mtext></mml:msub><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mtext>thr</mml:mtext></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  s_\text{thr}(T) := \max \Big\{ \frac{C_\text{thr}}{T} \sum\limits_{i = 1}^{T} M_i , n_\text{thr} \Big\}, $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft217">\begin{document}$ M_{i}$\end{document}</tex-math></alternatives></inline-formula> denotes the <italic>i</italic>-th element of <inline-formula><alternatives><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft218">\begin{document}$ M$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft219">\begin{document}$C_{\text{thr}} \gt 0$\end{document}</tex-math></alternatives></inline-formula> is a scaling constant (in our experiments we set it as 0.5). <inline-formula><alternatives><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft220">\begin{document}$ T$\end{document}</tex-math></alternatives></inline-formula> denotes the cardinality of <inline-formula><alternatives><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft221">\begin{document}$ M$\end{document}</tex-math></alternatives></inline-formula>. Given a memory <inline-formula><alternatives><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft222">\begin{document}$ M$\end{document}</tex-math></alternatives></inline-formula>, we can define the filtered memory <inline-formula><alternatives><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft223">\begin{document}$\Delta^{M}$\end{document}</tex-math></alternatives></inline-formula> as the set which contains every element of the memory <inline-formula><alternatives><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft224">\begin{document}$ M$\end{document}</tex-math></alternatives></inline-formula> that is higher than the sensitivity threshold <inline-formula><alternatives><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft225">\begin{document}$ s_{\text{thr}}(M)$\end{document}</tex-math></alternatives></inline-formula>, that is<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>M</mml:mi></mml:msup><mml:mo>:=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>z</mml:mi><mml:mo>â</mml:mo><mml:mi>M</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>z</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mtext>thr</mml:mtext></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle  \Delta^M := \{z \in M \, | \, z \gt s_\text{thr}(M)\}. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Then at time step <inline-formula><alternatives><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft226">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula>, given the agent memory <inline-formula><alternatives><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft227">\begin{document}$ M_{t}$\end{document}</tex-math></alternatives></inline-formula>, we define the average intensity <inline-formula><alternatives><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft228">\begin{document}$ c(M_{t})$\end{document}</tex-math></alternatives></inline-formula> and the intermittency <inline-formula><alternatives><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft229">\begin{document}$ i(M_{t})$\end{document}</tex-math></alternatives></inline-formula> as:<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mstyle><mml:mspace linebreak="newline"/><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  c(M_t) := \left\{\begin{array}{ll} \frac{1}{|\Delta^{M_t}|} \sum\limits_{i = 1}^{|\Delta^{M_t}|} \Big(\Delta^{M_t} \Big)_i, &amp; |\Delta^{M_t}| \gt 0\\ &amp; \\ 0 \end{array} \right.,\\ i(M_t) := \frac{| \Delta^{M_t} |}{|M_t|}. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Note that the average intensity is defined on the filtered memory <inline-formula><alternatives><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft230">\begin{document}$ \Delta^{M}$\end{document}</tex-math></alternatives></inline-formula>, that is conditioned to detecting odors above threshold. Since the features defined in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> return real numbers, in order to use (tabular) q-learning, we need to discretize them. We denote with <inline-formula><alternatives><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>i</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft231">\begin{document}$ \bar{i}(M_{t})$\end{document}</tex-math></alternatives></inline-formula> the discretized intermittency. This is defined as follow<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>i</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mn>0.33</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mn>0.33</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mn>0.66</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.66</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  \bar{i} (M_t) := \left\{ \begin{array}{ll} 0 ,&amp; \text{if} \quad i(M_t) \leq 0.33\\ 1 ,&amp; \text{if} \quad 0.33 \lt i(M_t) \leq 0.66\\ 2 ,&amp; \text{if} \quad i(M_t) \gt 0.66 \end{array} \right.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The average intensity is bounded between zero and the maximum concentration of odor at the source. To avoid prior information on the source, we use a more structured procedure to discretize the average intensity online, based on the agentâs experience only. At every time step <inline-formula><alternatives><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft232">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>, the average intensity <inline-formula><alternatives><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft233">\begin{document}$ c(M_{t})$\end{document}</tex-math></alternatives></inline-formula> is computed and collected in a dataset <inline-formula><alternatives><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft234">\begin{document}$ X_{t}$\end{document}</tex-math></alternatives></inline-formula>, that is<disp-formula id="equ7"><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  X_t := \{c(M_0), \cdots, c(M_t)\}. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Then, its discretized value is obtained by the following rule:<disp-formula id="equ8"><label>(7)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>25</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>25</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>99</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>4</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>99</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  \bar{c}(M_t, X_t) := \left\{ \begin{array}{ll} 0 ,&amp; c(M_t) \leq \text{p}(X_t, 25)\\ 1 ,&amp; \text{p}(X_t, 25) \lt c(M_t) \leq \text{p}(X_t, 50)\\ 2 ,&amp; \text{p}(X_t, 50) \lt c(M_t) \leq \text{p}(X_t, 80)\\ 3 ,&amp; \text{p}(X_t, 80) \lt c(M_t) \leq \text{p}(X_t, 99)\\ 4 ,&amp; c(M_t) \gt \text{p}(X_t, 99) \end{array} \right.,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft235">\begin{document}$p(X_{t},n)$\end{document}</tex-math></alternatives></inline-formula> denotes the <italic>n</italic>-th percentile of <inline-formula><alternatives><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft236">\begin{document}$ X_{t}$\end{document}</tex-math></alternatives></inline-formula>. Finally, we can define the feature map <inline-formula><alternatives><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft237">\begin{document}$ \phi_{t}$\end{document}</tex-math></alternatives></inline-formula> as a function of the memory <inline-formula><alternatives><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft238">\begin{document}$ M_{t}$\end{document}</tex-math></alternatives></inline-formula> and the dataset of average intensities <inline-formula><alternatives><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft239">\begin{document}$ X_{t}$\end{document}</tex-math></alternatives></inline-formula> at time step <inline-formula><alternatives><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft240">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula><disp-formula id="equ9"><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mi>i</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  \phi_t(M_t, X_t) := [\bar{i}(M_t), \bar{c}(M_t, X_t)]. $$\end{document}</tex-math></alternatives></disp-formula></p><p>This defines the current olfactory state <inline-formula><alternatives><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft241">\begin{document}$ s_{t}$\end{document}</tex-math></alternatives></inline-formula> that is at time step <inline-formula><alternatives><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft242">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula>, the agent is in the olfactory state <inline-formula><alternatives><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft243">\begin{document}$o_{t}:=\phi_{t}(M_{t},X_{t})$\end{document}</tex-math></alternatives></inline-formula>. The case where the agent has no odor detections above threshold in its current memory, that is <inline-formula><alternatives><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft244">\begin{document}$|\Delta(M_{t})|=0$\end{document}</tex-math></alternatives></inline-formula> corresponds to an additional state called void state (<inline-formula><alternatives><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft245">\begin{document}$ \emptyset$\end{document}</tex-math></alternatives></inline-formula>) in the main text.</p></sec><sec id="s4-3"><title>Agent behavior and policies</title><p>Now, we describe how the agent interacts with the environment to solve the navigation problem. At every time step <inline-formula><alternatives><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft246">\begin{document}$ t\in\mathbb{N}$\end{document}</tex-math></alternatives></inline-formula>, the agent observes an odor point <inline-formula><alternatives><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft247">\begin{document}$ z_{t}$\end{document}</tex-math></alternatives></inline-formula> and updates its memory, including the new observation and removing the oldest that is it defines a memory <inline-formula><alternatives><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft248">\begin{document}$ M_{t}$\end{document}</tex-math></alternatives></inline-formula> with the following rule<disp-formula id="equ10"><label>(8)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle  M_t := \Bigg[\Big(M_{t - 1} \Big)_{2}, \cdots, \Big(M_{t - 1} \Big)_{|M_{t - 1}|}, o_t \Bigg]. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Then, it updates the dataset of average intensities that is <inline-formula><alternatives><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>âª</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft249">\begin{document}$ X_{t}:=X_{t-1}\cup\{c(M_{t})\}$\end{document}</tex-math></alternatives></inline-formula> and it computes the olfactory state <inline-formula><alternatives><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft250">\begin{document}$o_{t}$\end{document}</tex-math></alternatives></inline-formula>. According to <inline-formula><alternatives><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft251">\begin{document}$ o_{t}$\end{document}</tex-math></alternatives></inline-formula>, the agent chooses an action <inline-formula><alternatives><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft252">\begin{document}$ a_{t}$\end{document}</tex-math></alternatives></inline-formula> using a policy. As indicated in the main text, actions are the coordinate directions that is we define an action set <inline-formula><alternatives><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft253">\begin{document}$ \mathcal{A}$\end{document}</tex-math></alternatives></inline-formula> as follow<disp-formula id="equ11"><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle  \mathcal{A} := \{e_1, e_2, -e_1, -e_2\}, $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft254">\begin{document}$ e_{i}$\end{document}</tex-math></alternatives></inline-formula> denotes the <italic>i</italic>-th canonical base. Actions are steps in any of the four directions, labeled relative to the mean flow which is assumed fixed and known. The gridworld is infinite, in that agents can leave indefinitely. If they exit the simulation box, they continue to receive zero signal and negative reward â0.001. As explained in the main text, actions are selected using one of two policies according to the current olfactory state <inline-formula><alternatives><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft255">\begin{document}$o_{t}$\end{document}</tex-math></alternatives></inline-formula>. More precisely, if the olfactory state <inline-formula><alternatives><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft256">\begin{document}$ o_{t}$\end{document}</tex-math></alternatives></inline-formula> is not the void state, then the (<inline-formula><alternatives><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft257">\begin{document}$ \epsilon$\end{document}</tex-math></alternatives></inline-formula>-greedy) Q-learning policy is used. Formally, let <inline-formula><alternatives><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft258">\begin{document}$ Q$\end{document}</tex-math></alternatives></inline-formula> be the Q matrix of the agent and let <inline-formula><alternatives><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â </mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft259">\begin{document}$ o_{t}\neq\emptyset$\end{document}</tex-math></alternatives></inline-formula>, then the agent plays the action <inline-formula><alternatives><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft260">\begin{document}$ a_{t}$\end{document}</tex-math></alternatives></inline-formula> such that<disp-formula id="equ12"><label>(9)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mtext>with probabilityÂ </mml:mtext><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ïµ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mtext>with probabilityÂ </mml:mtext><mml:mi>Ïµ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle  a_t = \left\{ \begin{array}{ll} a \in \arg\max\limits_{a \in \mathcal{A}} Q(o_t, a) &amp; \text{with probability } 1 - \epsilon\\ a \sim \mathcal{U}(\mathcal{A}) &amp; \text{with probability } \epsilon \end{array} \right., $$\end{document}</tex-math></alternatives></disp-formula></p><p>where, with <inline-formula><alternatives><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft261">\begin{document}$ a\sim\mathcal{U}(\mathcal{A})$\end{document}</tex-math></alternatives></inline-formula>, we indicate an action <italic>a</italic> uniformly sampled from <inline-formula><alternatives><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft262">\begin{document}$ \mathcal{A}$\end{document}</tex-math></alternatives></inline-formula>. At the test phase, the exploration-exploitation parameter <inline-formula><alternatives><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ïµ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft263">\begin{document}$ \epsilon$\end{document}</tex-math></alternatives></inline-formula> is set to 0, and thus, in an olfactory state <inline-formula><alternatives><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â </mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft264">\begin{document}$ o_{t}\neq\emptyset$\end{document}</tex-math></alternatives></inline-formula> the policy is deterministic. While training phase behavior is described in the next paragraphs. In the void state <inline-formula><alternatives><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft265">\begin{document}$o_{t}=\emptyset$\end{document}</tex-math></alternatives></inline-formula>, the agent chooses the action <inline-formula><alternatives><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft266">\begin{document}$ a_{t}\in\mathcal{A}$\end{document}</tex-math></alternatives></inline-formula> according to a separated policy called <italic>recovery strategy</italic>. In our experiments, we defined and compared three different recovery strategies: Brownian, Backtracking, and Learned.</p></sec><sec id="s4-4"><title>Brownian recovery</title><p>It is the simplest strategy we consider, consisting of playing random actions in the void state. Suppose that at time step <inline-formula><alternatives><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft267">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula>, the agent is in the void olfactory state, that is <inline-formula><alternatives><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft268">\begin{document}$ o_{t}=\emptyset$\end{document}</tex-math></alternatives></inline-formula>, then <inline-formula><alternatives><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft269">\begin{document}$ a_{t}$\end{document}</tex-math></alternatives></inline-formula> is sampled uniformly from the action set <inline-formula><alternatives><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft270">\begin{document}$ \mathcal{A}$\end{document}</tex-math></alternatives></inline-formula>. However, it is important to note that long-memory agents start to recover when they are already far from the plume, and hitting the plume by random walk is prohibitively long. To avoid wandering away from the plume, the memory is constrained to be shorter, consistent with the observation that the optimal memory is <inline-formula><alternatives><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft271">\begin{document}$T^{*}=3$\end{document}</tex-math></alternatives></inline-formula> to 5, much shorter than for backtracking. At this memory, several blanks within the plume will cause the agent to recover, hence the lower performance of the Brownian recovery.</p></sec><sec id="s4-5"><title>Backtracking recovery</title><p>In order to accelerate recovery from accidentally exiting the plume, we let the agents backtrack to the position where they last detected the odor. To this end, we first enumerate the actions with numbers from one to four. Then, we introduce a new memory called <italic>action memory</italic> <inline-formula><alternatives><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft272">\begin{document}$ A$\end{document}</tex-math></alternatives></inline-formula>. For simplicity, we consider the setting in which <inline-formula><alternatives><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft273">\begin{document}$|A|=|M|$\end{document}</tex-math></alternatives></inline-formula>. At time step <inline-formula><alternatives><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft274">\begin{document}$ t=0$\end{document}</tex-math></alternatives></inline-formula>, this memory is initialized as a vector of zeros indicating that the action memory is empty that is we define <inline-formula><alternatives><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft275">\begin{document}$ A_{0}\in\mathbb{N}^{|M|}$\end{document}</tex-math></alternatives></inline-formula> such that for every <inline-formula><alternatives><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft276">\begin{document}$ i=1,\cdots,|A|$\end{document}</tex-math></alternatives></inline-formula><disp-formula id="equ13"><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle  A_i = 0. $$\end{document}</tex-math></alternatives></disp-formula></p><p>For every time step <inline-formula><alternatives><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft277">\begin{document}$ t \gt 0$\end{document}</tex-math></alternatives></inline-formula>, the agent observes an odor point <inline-formula><alternatives><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft278">\begin{document}$ z_{t}$\end{document}</tex-math></alternatives></inline-formula> and updates the memory through (<xref ref-type="disp-formula" rid="equ10">Equation 8</xref>). Moreover, the action memory is updated according to the status of the memory. If the last observation is smaller than the sensitivity threshold, that is <inline-formula><alternatives><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft279">\begin{document}$ z_{t} \lt s_{\text{thr}}(M_{t})$\end{document}</tex-math></alternatives></inline-formula>, the action previously played <inline-formula><alternatives><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft280">\begin{document}$ a_{t-1}$\end{document}</tex-math></alternatives></inline-formula> (represented by a natural number in <inline-formula><alternatives><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft281">\begin{document}$ [1,4]$\end{document}</tex-math></alternatives></inline-formula>) is stored in the action memory, that is for some <inline-formula><alternatives><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft282">\begin{document}$ \Delta \gt 0$\end{document}</tex-math></alternatives></inline-formula>, let<disp-formula id="equ14"><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle  A_{t - 1} = [a_{t - \Delta}, \cdots, a_{t - 2}, 0, \cdots, 0]. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Then<disp-formula id="equ15"><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle  A_{t} = [a_{t - \Delta}, \cdots, a_{t - 2}, a_{t - 1}, \cdots, 0]. $$\end{document}</tex-math></alternatives></disp-formula></p><p>If at time step <inline-formula><alternatives><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft283">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula>, the observation <inline-formula><alternatives><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft284">\begin{document}$ z_{t}$\end{document}</tex-math></alternatives></inline-formula> is larger than the sensitivity threshold then the action memory is reset, that is <inline-formula><alternatives><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft285">\begin{document}$ A_{t}\in\mathbb{N}^{|M|}$\end{document}</tex-math></alternatives></inline-formula> with <inline-formula><alternatives><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft286">\begin{document}$ (A_{t})_{i}=0$\end{document}</tex-math></alternatives></inline-formula> for every <italic>i</italic>. If at time step <inline-formula><alternatives><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft287">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula>, the memory is empty, that is <inline-formula><alternatives><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft288">\begin{document}$ c(M_{t})=0$\end{document}</tex-math></alternatives></inline-formula>, then the backtracking procedure is executed: the last non-zero element of the action memory is extracted, and the inverse action is played that is for some <inline-formula><alternatives><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft289">\begin{document}$ \Delta \gt 0$\end{document}</tex-math></alternatives></inline-formula>, let<disp-formula id="equ16"><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle  A_{t-1} = [a_{t - \Delta}, \cdots, a_{t - 2}]. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Then, it plays the action <inline-formula><alternatives><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft290">\begin{document}$ a_{t-2}$\end{document}</tex-math></alternatives></inline-formula> and updates the action memory as follow<disp-formula id="equ17"><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">Î</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle  A_{t} = [a_{t - \Delta}, \cdots, a_{t - 3}, 0]. $$\end{document}</tex-math></alternatives></disp-formula></p><p>This procedure is repeated until either an observation larger than the sensitivity threshold is obtained or the action memory becomes empty. In the former case, the action memory is cleared, and the action is chosen according to the Q-learning policy (<xref ref-type="disp-formula" rid="equ12">Equation 9</xref>). In the latter case, a random action is played.</p><p>Note that this strategy only provides exploration after the backtracking fails to recover detections. Also, if agents start with no detection at time 0, the procedure is equivalent to Brownian motion.</p></sec><sec id="s4-6"><title>Circling recovery</title><p>In this case, the recovery strategy consists of adopting a circling behavior (<xref ref-type="bibr" rid="bib61">Stupski and van Breugel, 2024a</xref>) when the agent is in a void state. The agent keeps in memory two counters, <inline-formula><alternatives><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft291">\begin{document}$ t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft292">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula>, as well as an action <inline-formula><alternatives><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft293">\begin{document}$ a_{\text{void}}\in\mathcal{A}$\end{document}</tex-math></alternatives></inline-formula>, initialized to 0, 1, and <inline-formula><alternatives><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft294">\begin{document}$ e_{1}$\end{document}</tex-math></alternatives></inline-formula>, respectively. The first counter represents the consecutive number of void observations, <inline-formula><alternatives><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft295">\begin{document}$a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> is the action to play when the void state is reached, and <inline-formula><alternatives><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft296">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula> is a time threshold that indicates when to switch the action <inline-formula><alternatives><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft297">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula>. When the agent reaches a void state, it plays the action <inline-formula><alternatives><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft298">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> and increments the counter <inline-formula><alternatives><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft299">\begin{document}$ t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> by one. If <inline-formula><alternatives><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft300">\begin{document}$t_{\text{void}}=T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula> then the agent resets <inline-formula><alternatives><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft301">\begin{document}$ t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> to zero, increases <inline-formula><alternatives><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft302">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula> by one, and updates the action <inline-formula><alternatives><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft303">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> as follows<disp-formula id="equ18"><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mtext>void</mml:mtext></mml:msub><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mtext>void</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mtext>void</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mtext>void</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mtext>void</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle  a_\text{void} \leftarrow \left\{\begin{array}{cc} e_2 &amp; \text{if } a_\text{void} = e_1 \\ -e_1 &amp; \text{if } a_\text{void} = e_2 \\ -e_2 &amp; \text{if } a_\text{void} = -e_1 \\ e_1 &amp; \text{if } a_\text{void} = -e_2 \\ \end{array} \right. $$\end{document}</tex-math></alternatives></disp-formula></p><p>When the agent receives a non-void observation, it resets <inline-formula><alternatives><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft304">\begin{document}$ t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> to 0, <inline-formula><alternatives><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft305">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula> to 1, and <inline-formula><alternatives><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft306">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft307">\begin{document}$ e_{1}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-7"><title>Cast-surge recovery</title><p>In this case, the agent plays a cast-surge behavior (<xref ref-type="bibr" rid="bib6">Baker, 1990</xref>) when reaching the void state. As in the circling recovery strategy, the agent keeps two counters, <inline-formula><alternatives><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft308">\begin{document}$ t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft309">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula>, as well as an action <inline-formula><alternatives><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft310">\begin{document}$ a_{\text{void}}\in\mathcal{A}$\end{document}</tex-math></alternatives></inline-formula>, initialized to 0, 1, and <inline-formula><alternatives><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft311">\begin{document}$e_{2}$\end{document}</tex-math></alternatives></inline-formula>, respectively. At every step in the void state, the agent plays the action <inline-formula><alternatives><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft312">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> and increments the void counter <inline-formula><alternatives><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft313">\begin{document}$t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> by one. If <inline-formula><alternatives><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft314">\begin{document}$ t_{\text{void}}=T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula>, the agent takes an upwind step, doubles the value of <inline-formula><alternatives><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft315">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula>, updates <inline-formula><alternatives><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft316">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> by setting it to <inline-formula><alternatives><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>â</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft317">\begin{document}$ -a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula>, and resets <inline-formula><alternatives><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft318">\begin{document}$ t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> to zero. When the agent receives a non-void observation, it resets <inline-formula><alternatives><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft319">\begin{document}$t_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> to 0, <inline-formula><alternatives><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>change</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft320">\begin{document}$ T_{\text{change}}$\end{document}</tex-math></alternatives></inline-formula> to 1, and <inline-formula><alternatives><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>void</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft321">\begin{document}$ a_{\text{void}}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft322">\begin{document}$ e_{2}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-8"><title>Learned recovery</title><p>In this case, the recovery policy is learned by splitting the void state into several states labeled by the time since entry in the void state. In our experiments, we split the void state into 50 states. Actions are then learned as in all other non-void states, and the optimal action is always chosen with (<xref ref-type="disp-formula" rid="equ12">Equation 9</xref>).</p></sec><sec id="s4-9"><title>Training</title><p>An agent starts at a random location within the odor plume at time 0. Its memory is initialized with the prior <inline-formula><alternatives><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft323">\begin{document}$ |M_{0}|$\end{document}</tex-math></alternatives></inline-formula> odor detections at its initial location <inline-formula><alternatives><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft324">\begin{document}$M_{0}=[z_{-|M_{0}|},\cdots,z_{0}]$\end{document}</tex-math></alternatives></inline-formula>, obtained from the fluid dynamics simulation. The Q-function <inline-formula><alternatives><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft325">\begin{document}$ Q_{0}$\end{document}</tex-math></alternatives></inline-formula> is initialized with 0.6 for all actions and olfactory states. The first dataset of average intensities contains the first value <inline-formula><alternatives><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft326">\begin{document}$ X_{0}=\{c(M_{0})\}$\end{document}</tex-math></alternatives></inline-formula>. At every time step <inline-formula><alternatives><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft327">\begin{document}$ t \gt 0$\end{document}</tex-math></alternatives></inline-formula>, the agent gets an odor observation <inline-formula><alternatives><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft328">\begin{document}$z_{t}$\end{document}</tex-math></alternatives></inline-formula> from its new position and updates its memory, including the new observation and removing the oldest; the olfactory state <inline-formula><alternatives><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft329">\begin{document}$o_{t}$\end{document}</tex-math></alternatives></inline-formula> is computed (as described in previous paragraphs). The dataset of average intensities is updated: <inline-formula><alternatives><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>âª</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft330">\begin{document}$ X_{t}=X_{t-1}\cup\{c(M_{t})\}$\end{document}</tex-math></alternatives></inline-formula>. Exploration-exploitation parameter <inline-formula><alternatives><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ïµ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft331">\begin{document}$ \epsilon_{k}$\end{document}</tex-math></alternatives></inline-formula> is scheduled as follow<disp-formula id="equ19"><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Î·</mml:mi><mml:mtext>init</mml:mtext></mml:msub><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î·</mml:mi><mml:mtext>decay</mml:mtext></mml:msub><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle  \epsilon_k = \eta_\text{init} \exp(- \eta_\text{decay} k), $$\end{document}</tex-math></alternatives></disp-formula></p><p>where, in our experiments, <inline-formula><alternatives><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î·</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft332">\begin{document}$ \eta_{\text{init}}=0.99$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î·</mml:mi><mml:mrow><mml:mtext>decay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft333">\begin{document}$ \eta_{\text{decay}}=0.0001$\end{document}</tex-math></alternatives></inline-formula>. At every episode <inline-formula><alternatives><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft334">\begin{document}$ k$\end{document}</tex-math></alternatives></inline-formula>, the Q-function is updated at every time step <inline-formula><alternatives><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft335">\begin{document}$ t$\end{document}</tex-math></alternatives></inline-formula> as<disp-formula id="equ20"><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>Î³</mml:mi><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle  Q_{k + 1}(s_t, a_t) := (1 - \alpha_k) Q_k(s_t, a_t) + \alpha_k (r_t + \gamma \max\limits_{a'} Q_k(s_{t + 1}, a')), $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft336">\begin{document}$ R_{t}$\end{document}</tex-math></alternatives></inline-formula> is the immediate reward received playing the action <inline-formula><alternatives><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft337">\begin{document}$ a_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft338">\begin{document}$ o_{t+1}$\end{document}</tex-math></alternatives></inline-formula> are the current and the next olfactory states and <inline-formula><alternatives><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft339">\begin{document}$\alpha_{k}$\end{document}</tex-math></alternatives></inline-formula> is the learning rate at episode <inline-formula><alternatives><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft340">\begin{document}$ k$\end{document}</tex-math></alternatives></inline-formula>. This is scheduled as<disp-formula id="equ21"><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Î±</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mtext>init</mml:mtext></mml:msub><mml:mi>exp</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mtext>decay</mml:mtext></mml:msub><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle  \alpha_k = \alpha_\text{init} \exp(- \alpha_\text{decay} k), $$\end{document}</tex-math></alternatives></disp-formula></p><p>where, in our experiments, <inline-formula><alternatives><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft341">\begin{document}$ \alpha_{\text{init}}=0.25$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mtext>decay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft342">\begin{document}$ \alpha_{\text{decay}}=0.001$\end{document}</tex-math></alternatives></inline-formula>. For the experiments, agents are trained in 100,000 episodes and a horizon of 5000 steps. The agent velocity is set to 10, and the discount factor is <inline-formula><alternatives><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Î³</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9999</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft343">\begin{document}$ \gamma=0.9999$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-10"><title>Agents Evaluation</title><p>To evaluate the performance of the different agents, we consider four metrics: the cumulative reward <inline-formula><alternatives><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft344">\begin{document}$ G$\end{document}</tex-math></alternatives></inline-formula> (which is the actual quantity that the algorithm optimizes for); normalized time (defined below); the fraction of success <inline-formula><alternatives><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft345">\begin{document}$f^{+}$\end{document}</tex-math></alternatives></inline-formula> and the value conditioned on success <inline-formula><alternatives><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft346">\begin{document}$ g^{+}$\end{document}</tex-math></alternatives></inline-formula>. For a fixed position <inline-formula><alternatives><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft347">\begin{document}$ (i,j)$\end{document}</tex-math></alternatives></inline-formula>, we denote with <inline-formula><alternatives><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft348">\begin{document}$ \tau_{\text{min}}(i,j)$\end{document}</tex-math></alternatives></inline-formula> the minimum number of steps required to reach the source region from <inline-formula><alternatives><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft349">\begin{document}$(i,j)$\end{document}</tex-math></alternatives></inline-formula> that is the length of the shortest path.</p><p>We define <inline-formula><alternatives><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft350">\begin{document}$D_{\text{init}}$\end{document}</tex-math></alternatives></inline-formula> the set of points in which the first observation is above the sensitivity threshold (valid points). For each initial position <inline-formula><alternatives><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft351">\begin{document}$ (i,j)\in D_{\text{init}}$\end{document}</tex-math></alternatives></inline-formula>, let <inline-formula><alternatives><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft352">\begin{document}$\tau(i,j)$\end{document}</tex-math></alternatives></inline-formula> be the duration of the path obtained by an agent to reach the source. Note that <inline-formula><alternatives><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft353">\begin{document}$\tau(i,j)$\end{document}</tex-math></alternatives></inline-formula> is a random variable for the stochastic backtracking and Brownian recoveries, but it is deterministic for the learned strategy that has no random components. For each admissible location <inline-formula><alternatives><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft354">\begin{document}$ (i,j)$\end{document}</tex-math></alternatives></inline-formula>, we define four performance metrics:<disp-formula id="equ22"><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:mfrac><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î³</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle  \begin{array}{ll}G(i,j) =&amp;\langle e^{-\lambda \tau(i,j)}-\frac{\sigma}{1-\gamma}(1-e^{-\lambda \tau(i,j)})\rangle\end{array}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ23"><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>success</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>reps</mml:mtext></mml:mrow></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle  f^+(i,j) = \frac{n_{\text{success}}(i,j)}{n_{\text{reps}}}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ24"><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>g</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mi>Î»</mml:mi><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mtext>success</mml:mtext><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle  g^+(i,j)=\langle e^{-\lambda \tau(i,j)}\,|\, \text{success}\rangle$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ25"><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mi>Ï</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle  \frac{\tau_{\text{min}}}{\tau}(i,j)= \langle \frac{\tau_{\text{min}}(i, j)}{\tau(i, j)}\rangle$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>reps</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft355">\begin{document}$ n_{\text{reps}}$\end{document}</tex-math></alternatives></inline-formula> is the number of test trajectories from each admissible location, and we use <inline-formula><alternatives><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>reps</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft356">\begin{document}$ n_{\text{reps}}=10$\end{document}</tex-math></alternatives></inline-formula>. We then compute statistics of the performance metrics over the <inline-formula><alternatives><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft357">\begin{document}$ D_{\text{init}}$\end{document}</tex-math></alternatives></inline-formula> initial positions and report the average (<inline-formula><alternatives><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">â¨</mml:mo><mml:mo>â</mml:mo><mml:mo fence="false" stretchy="false">â©</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft358">\begin{document}$ \langle\cdot\rangle$\end{document}</tex-math></alternatives></inline-formula>) and standard deviation (std). Note that both the backtracking and Brownian strategies have stochastic steps; for these strategies, <inline-formula><alternatives><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft359">\begin{document}$f^{+}(i,j)$\end{document}</tex-math></alternatives></inline-formula> denotes the average success fraction computed at each position over 10 repetitions.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Investigation, Visualization, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Visualization, Writing â original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing â original draft, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Visualization, Methodology, Writing â original draft, Project administration, Writing â review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-102906-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The datasets and code used to perform the experiments are available at the following links: (1) Newly created source code for Q-learning training and test: <ext-link ext-link-type="uri" xlink:href="https://github.com/Akatsuki96/qlearning_for_navigation">https://github.com/Akatsuki96/qlearning_for_navigation</ext-link> (copy archived at <xref ref-type="bibr" rid="bib50">Rando, 2025</xref>) (2) Newly created datasets of odor snapshots: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14655992">https://doi.org/10.5281/zenodo.14655992</ext-link>. Additionally, the work re-uses previously developed data and codes: Reused datasets of odor snapshots from <xref ref-type="bibr" rid="bib55">Rigolli et al., 2022c</xref>. Reused source code to run full computational fluid dynamics simulations: the code is described in <xref ref-type="bibr" rid="bib70">Verzicco et al., 2025</xref> and an earlier version of the code is available at <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/vdv9265847/IBbookVdV/">https://gitlab.com/vdv9265847/IBbookVdV/</ext-link> (<xref ref-type="bibr" rid="bib69">Verzicco et al., 2024</xref>). The GPU-accelerated version of the code was used here; it was developed in <xref ref-type="bibr" rid="bib73">Viola et al., 2023</xref>; <xref ref-type="bibr" rid="bib71">Viola et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Viola et al., 2022</xref> and <xref ref-type="bibr" rid="bib70">Verzicco et al., 2025</xref>, and was obtained as a courtesy of F. Viola. The full source code will be made available by the authors of the cited work in the near future.This is a computational study: no experimental data have been generated for this manuscript.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Marco</surname><given-names>R</given-names></name><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Alessandro</surname><given-names>V</given-names></name><name><surname>Lorenzo</surname><given-names>R</given-names></name><name><surname>Agnese</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Q-learning with temporal memory to navigate turbulence - Datasets</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.14655992</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Rigolli</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>G</given-names></name><name><surname>Seminara</surname><given-names>A</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Alternation emerges as a multi-modal strategy for turbulent odor navigation - Dataset</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.6538177</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by grants to AS from the European Research Council (ERC) under the European Unionâs Horizon 2020 research and innovation programme (grant agreement No 101002724 RIDING), the Air Force Office of Scientific Research under award number FA8655-20-1-7028, and the National Institutes of Health (NIH) under award number R01DC018789. LR and MR acknowledge the financial support of the European Research Council (grant SLING 819789), the European Commission (Horizon Europe grant ELIAS 101120237), the US Air Force Office of Scientific Research (FA8655-22-1-7034), the Ministry of Education, University and Research (FARE grant ML4IP R205T7J2KP; grant BAC FAIR PE00000013 funded by the EU - NGEU) and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. MR is a member of the Gruppo Nazionale per lâAnalisi Matematica, la Probabilitâa e le loro Applicazioni (GNAMPA) of the Istituto Nazionale di Alta Matematica (INdAM). This work represents only the view of the authors. The European Commission and the other organizations are not responsible for any use that may be made of the information it contains. We thank Francesco Viola for sharing a GPU accelerated version of the CFD code as well as support and discussions regarding computational fluid dynamics; Antonio Celani, Venkatesh Murthy, Yujia Qi, Francesco Boccardo, Luca Gagliardi, Francesco Marcolli and Arnaud Ruymaekers for comments on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ache</surname><given-names>BW</given-names></name><name><surname>Hein</surname><given-names>AM</given-names></name><name><surname>Bobkov</surname><given-names>YV</given-names></name><name><surname>Principe</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Smelling time: a neural basis for olfactory scene analysis</article-title><source>Trends in Neurosciences</source><volume>39</volume><fpage>649</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2016.08.002</pub-id><pub-id pub-id-type="pmid">27594700</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackels</surname><given-names>T</given-names></name><name><surname>Erskine</surname><given-names>A</given-names></name><name><surname>Dasgupta</surname><given-names>D</given-names></name><name><surname>Marin</surname><given-names>AC</given-names></name><name><surname>Warner</surname><given-names>TPA</given-names></name><name><surname>Tootoonian</surname><given-names>S</given-names></name><name><surname>Fukunaga</surname><given-names>I</given-names></name><name><surname>Harris</surname><given-names>JJ</given-names></name><name><surname>Schaefer</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast odour dynamics are encoded in the olfactory system and guide behaviour</article-title><source>Nature</source><volume>593</volume><fpage>558</fpage><lpage>563</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03514-2</pub-id><pub-id pub-id-type="pmid">33953395</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allard</surname><given-names>CAH</given-names></name><name><surname>Kang</surname><given-names>G</given-names></name><name><surname>Kim</surname><given-names>JJ</given-names></name><name><surname>Valencia-Montoya</surname><given-names>WA</given-names></name><name><surname>Hibbs</surname><given-names>RE</given-names></name><name><surname>Bellono</surname><given-names>NW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Structural basis of sensory receptor evolution in octopus</article-title><source>Nature</source><volume>616</volume><fpage>373</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-05822-1</pub-id><pub-id pub-id-type="pmid">37045920</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ãlvarez-Salvado</surname><given-names>E</given-names></name><name><surname>Licata</surname><given-names>AM</given-names></name><name><surname>Connor</surname><given-names>EG</given-names></name><name><surname>McHugh</surname><given-names>MK</given-names></name><name><surname>King</surname><given-names>BM</given-names></name><name><surname>Stavropoulos</surname><given-names>N</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Crimaldi</surname><given-names>JP</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Elementary sensory-motor transformations underlying olfactory navigation in walking fruit-flies</article-title><source>eLife</source><volume>7</volume><elocation-id>e37815</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37815</pub-id><pub-id pub-id-type="pmid">30129438</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atema</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Eddy chemotaxis and odor landscapes: exploration of nature with animal sensors</article-title><source>The Biological Bulletin</source><volume>191</volume><fpage>129</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.2307/1543074</pub-id><pub-id pub-id-type="pmid">29220222</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Upwind flight and casting flight: complementary and tonic systems used for location of sex pheromone sources by male moths</article-title><conf-name>Proc 10 Intl Symposium on Olfaction and Taste</conf-name><fpage>13</fpage><lpage>18</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>KL</given-names></name><name><surname>Dickinson</surname><given-names>M</given-names></name><name><surname>Findley</surname><given-names>TM</given-names></name><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Louis</surname><given-names>M</given-names></name><name><surname>Suver</surname><given-names>MP</given-names></name><name><surname>Verhagen</surname><given-names>JV</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Algorithms for olfactory search across species</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9383</fpage><lpage>9389</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1668-18.2018</pub-id><pub-id pub-id-type="pmid">30381430</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balkovsky</surname><given-names>E</given-names></name><name><surname>Shraiman</surname><given-names>BI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Olfactory search at high Reynolds number</article-title><source>PNAS</source><volume>99</volume><fpage>12589</fpage><lpage>12593</lpage><pub-id pub-id-type="doi">10.1073/pnas.192393499</pub-id><pub-id pub-id-type="pmid">12228727</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Belanger</surname><given-names>JH</given-names></name><name><surname>Willis</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Biologically-inspired search algorithms for locating unseen odor sources</article-title><conf-name>Proc IEEE Symp Intell Control (ISIC â98) and IEEE Symp Comp Intell Robot Autom</conf-name><pub-id pub-id-type="doi">10.1109/ISIC.1998.713672</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>WJ</given-names></name><name><surname>Kramer</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Search and anemotaxis in insects</article-title><source>Journal of Insect Physiology</source><volume>25</volume><fpage>631</fpage><lpage>640</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Chemotaxis in bacteria</article-title><source>Annual Review of Biophysics and Bioengineering</source><volume>4</volume><fpage>119</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1146/annurev.bb.04.060175.001003</pub-id><pub-id pub-id-type="pmid">1098551</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bobkov</surname><given-names>YV</given-names></name><name><surname>Ache</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Intrinsically bursting olfactory receptor neurons</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>1052</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.1152/jn.01111.2006</pub-id><pub-id pub-id-type="pmid">17135465</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boie</surname><given-names>SD</given-names></name><name><surname>Connor</surname><given-names>EG</given-names></name><name><surname>McHugh</surname><given-names>M</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name><name><surname>Ermentrout</surname><given-names>GB</given-names></name><name><surname>Crimaldi</surname><given-names>JP</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Information-theoretic analysis of realistic odor plumes: What cues are useful for determining location?</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006275</pub-id><pub-id pub-id-type="pmid">29990365</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>CardÃ©</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Navigation along windborne plumes of pheromone and resource-linked odors</article-title><source>Annual Review of Entomology</source><volume>66</volume><fpage>317</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1146/annurev-ento-011019-024932</pub-id><pub-id pub-id-type="pmid">32926790</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carey</surname><given-names>RM</given-names></name><name><surname>Verhagen</surname><given-names>JV</given-names></name><name><surname>Wesson</surname><given-names>DW</given-names></name><name><surname>PÃ­rez</surname><given-names>N</given-names></name><name><surname>Wachowiak</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal structure of receptor neuron input to the olfactory bulb imaged in behaving rats</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>1073</fpage><lpage>1088</lpage><pub-id pub-id-type="doi">10.1152/jn.90902.2008</pub-id><pub-id pub-id-type="pmid">19091924</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cassandra</surname><given-names>AR</given-names></name><name><surname>Kaelbling</surname><given-names>LP</given-names></name><name><surname>Kurien</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Acting under uncertainty: discrete Bayesian models for mobile-robot navigation</article-title><conf-name>IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS â96</conf-name><conf-loc>Osaka, Japan</conf-loc><pub-id pub-id-type="doi">10.1109/IROS.1996.571080</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celani</surname><given-names>A</given-names></name><name><surname>Villermaux</surname><given-names>E</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Odor landscapes in turbulent environments</article-title><source>Physical Review X</source><volume>4</volume><elocation-id>41015</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.4.041015</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Celani</surname><given-names>A</given-names></name><name><surname>Panizon</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Olfactory search</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2405.03374</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>CT</given-names></name><name><surname>Kennedy</surname><given-names>JS</given-names></name><name><surname>Ludlow</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Finding of a sex pheromone source by gypsy moths released in the field</article-title><source>Nature</source><volume>303</volume><fpage>804</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/303804a0</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demir</surname><given-names>M</given-names></name><name><surname>Kadakia</surname><given-names>N</given-names></name><name><surname>Anderson</surname><given-names>HD</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Emonet</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Walking <italic>Drosophila</italic> navigate complex plumes using stochastic decisions biased by the timing of odor encounters</article-title><source>eLife</source><volume>9</volume><elocation-id>e57524</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57524</pub-id><pub-id pub-id-type="pmid">33140723</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duplat</surname><given-names>J</given-names></name><name><surname>Jouary</surname><given-names>A</given-names></name><name><surname>Villermaux</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Entanglement rules for random mixtures</article-title><source>Physical Review Letters</source><volume>105</volume><elocation-id>034504</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.105.034504</pub-id><pub-id pub-id-type="pmid">20867769</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etienne</surname><given-names>AS</given-names></name><name><surname>Maurer</surname><given-names>R</given-names></name><name><surname>SÃ©guinot</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Path integration in mammals and its interaction with visual landmarks</article-title><source>The Journal of Experimental Biology</source><volume>199</volume><fpage>201</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1242/jeb.199.1.201</pub-id><pub-id pub-id-type="pmid">8576691</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etienne</surname><given-names>AS</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Path integration in mammals</article-title><source>Hippocampus</source><volume>14</volume><fpage>180</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1002/hipo.10173</pub-id><pub-id pub-id-type="pmid">15098724</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falkovich</surname><given-names>G</given-names></name><name><surname>GawÈ©dzki</surname><given-names>K</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Particles and fields in fluid turbulence</article-title><source>Reviews of Modern Physics</source><volume>73</volume><fpage>913</fpage><lpage>975</lpage><pub-id pub-id-type="doi">10.1103/RevModPhys.73.913</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gepner</surname><given-names>R</given-names></name><name><surname>Mihovilovic</surname><given-names>SM</given-names></name><name><surname>Bernat</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computations underlying <italic>Drosophila</italic> photo-taxis</article-title><source>eLife</source><volume>13</volume><elocation-id>6229</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06229</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>G</given-names></name><name><surname>Brady</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Anemotacticâ flight paths of tsetse flies in relation to host odour: a preliminary video study in nature of the response to loss of odour</article-title><source>PhysiologicalEntomology</source><volume>10</volume><fpage>395</fpage><lpage>406</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Kapoor</surname><given-names>V</given-names></name><name><surname>Arrighi-Allisan</surname><given-names>A</given-names></name><name><surname>Seminara</surname><given-names>A</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mice develop efficient strategies for foraging and navigation using complex natural stimuli</article-title><source>Current Biology</source><volume>26</volume><fpage>1261</fpage><lpage>1273</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.03.040</pub-id><pub-id pub-id-type="pmid">27112299</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulitski</surname><given-names>G</given-names></name><name><surname>Kholmyansky</surname><given-names>M</given-names></name><name><surname>Kinzelbach</surname><given-names>W</given-names></name><name><surname>LÃ¼thi</surname><given-names>B</given-names></name><name><surname>Tsinober</surname><given-names>A</given-names></name><name><surname>Yorish</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Velocity and temperature derivatives in high-Reynolds-number turbulent flows in the atmospheric surface layer: Part 1. Facilities, methods and some general results</article-title><source>Journal of Fluid Mechanics</source><volume>589</volume><fpage>57</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1017/S0022112007007495</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauskrecht</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Value-function approximations for partially observable markov decision processes</article-title><source>Journal of Artificial Intelligence Research</source><volume>13</volume><fpage>33</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1613/jair.678</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinonen</surname><given-names>RA</given-names></name><name><surname>Biferale</surname><given-names>L</given-names></name><name><surname>Celani</surname><given-names>A</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Optimal policies for Bayesian olfactory search in turbulent flows</article-title><source>Physical Review. E</source><volume>107</volume><elocation-id>55105</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.107.055105</pub-id><pub-id pub-id-type="pmid">37329026</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname><given-names>S</given-names></name><name><surname>Narendra</surname><given-names>A</given-names></name><name><surname>Cheung</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Principles of insect path integration</article-title><source>Current Biology</source><volume>28</volume><fpage>R1043</fpage><lpage>R1058</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.058</pub-id><pub-id pub-id-type="pmid">30205054</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernandez-Nunez</surname><given-names>L</given-names></name><name><surname>Belina</surname><given-names>J</given-names></name><name><surname>Klein</surname><given-names>M</given-names></name><name><surname>Si</surname><given-names>G</given-names></name><name><surname>Claus</surname><given-names>L</given-names></name><name><surname>Carlson</surname><given-names>JR</given-names></name><name><surname>Samuel</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reverse-correlation analysis of navigation dynamics in <italic>Drosophila</italic> larva using optogenetics</article-title><source>eLife</source><volume>4</volume><elocation-id>e06225</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06225</pub-id><pub-id pub-id-type="pmid">25942453</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ishida</surname><given-names>H</given-names></name><name><surname>Wada</surname><given-names>Y</given-names></name><name><surname>Matsukura</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Chemical sensing in robotic applications: a review</article-title><conf-name>IEEE Sensors Journal</conf-name><pub-id pub-id-type="doi">10.1109/JSEN.2012.2208740</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kathman</surname><given-names>ND</given-names></name><name><surname>Lanz</surname><given-names>AJ</given-names></name><name><surname>Freed</surname><given-names>JD</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Neural dynamics for working memory and evidence integration during olfactory navigation in <italic>Drosophila</italic></article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.10.05.616803</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1997">1997</year><chapter-title>A tentative intercausal nexus and its computer model on insect orientation in windborne pheromone plumes</chapter-title><source>Insect Pher Res</source><publisher-name>Springer</publisher-name><fpage>232</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1007/978-1-4615-6371-6_22</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krishnamurthy</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Partially Observed Markov Decision Processes</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781316471104</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuenen</surname><given-names>LPS</given-names></name><name><surname>CardÃ©</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Strategies for recontacting a los pheromone plume: casting and upwind flight in the male gypsy moth</article-title><source>Physiological Entomology</source><volume>15</volume><elocation-id>317</elocation-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LaValle</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Planning Algorithms</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511546877</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of the lateral line and vision on body kinematics and hydrodynamic preference of rainbow trout in turbulent flow</article-title><source>The Journal of Experimental Biology</source><volume>209</volume><fpage>4077</fpage><lpage>4090</lpage><pub-id pub-id-type="doi">10.1242/jeb.02487</pub-id><pub-id pub-id-type="pmid">17023602</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loisy</surname><given-names>A</given-names></name><name><surname>Eloy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Searching for a source without gradients: how good is infotaxis and how to beat it</article-title><source>Proceedings of the Royal Society A</source><volume>478</volume><elocation-id>20220118</elocation-id><pub-id pub-id-type="doi">10.1098/rspa.2022.0118</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loisy</surname><given-names>A</given-names></name><name><surname>Heinonen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Deep reinforcement learning for the olfactory search POMDP: a quantitative benchmark</article-title><source>The European Physical Journal E</source><volume>46</volume><elocation-id>277</elocation-id><pub-id pub-id-type="doi">10.1140/epje/s10189-023-00277-8</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matheson</surname><given-names>AMM</given-names></name><name><surname>Lanz</surname><given-names>AJ</given-names></name><name><surname>Medina</surname><given-names>AM</given-names></name><name><surname>Licata</surname><given-names>AM</given-names></name><name><surname>Currier</surname><given-names>TA</given-names></name><name><surname>Syed</surname><given-names>MH</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A neural circuit for wind-guided olfactory navigation</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>4613</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-32247-7</pub-id><pub-id pub-id-type="pmid">35941114</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaelis</surname><given-names>BT</given-names></name><name><surname>Leathers</surname><given-names>KW</given-names></name><name><surname>Bobkov</surname><given-names>YV</given-names></name><name><surname>Ache</surname><given-names>BW</given-names></name><name><surname>Principe</surname><given-names>JC</given-names></name><name><surname>Baharloo</surname><given-names>R</given-names></name><name><surname>Park</surname><given-names>IM</given-names></name><name><surname>Reidenbach</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Odor tracking in aquatic organisms: the importance of temporal and spatial intermittency of the turbulent plume</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>7961</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-64766-y</pub-id><pub-id pub-id-type="pmid">32409665</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murlis</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Fineâscale structure of odour plumes in relation to insect orientation to distant pheromone and other attractant sources</article-title><source>Physiological Entomology</source><volume>6</volume><fpage>71</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1111/j.1365-3032.1981.tb00262.x</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murlis</surname><given-names>J</given-names></name><name><surname>Elkinton</surname><given-names>JS</given-names></name><name><surname>CardÃ©</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Odor plumes and how insects use them</article-title><source>Annual Review of Entomology</source><volume>37</volume><fpage>505</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1146/annurev.en.37.010192.002445</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nag</surname><given-names>A</given-names></name><name><surname>van Breugel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Odour source distance is predictable from a time history of odour statistics for large scale outdoor plumes</article-title><source>Journal of the Royal Society, Interface</source><volume>21</volume><elocation-id>20240169</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2024.0169</pub-id><pub-id pub-id-type="pmid">39079675</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okubo</surname><given-names>TS</given-names></name><name><surname>Patella</surname><given-names>P</given-names></name><name><surname>DâAlessandro</surname><given-names>I</given-names></name><name><surname>Wilson</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A neural network for wind-guided compass navigation</article-title><source>Neuron</source><volume>107</volume><fpage>924</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.022</pub-id><pub-id pub-id-type="pmid">32681825</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>IJ</given-names></name><name><surname>Hein</surname><given-names>AM</given-names></name><name><surname>Bobkov</surname><given-names>YV</given-names></name><name><surname>Reidenbach</surname><given-names>MA</given-names></name><name><surname>Ache</surname><given-names>BW</given-names></name><name><surname>Principe</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurally encoding time for olfactory navigation</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004682</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004682</pub-id><pub-id pub-id-type="pmid">26730727</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radvansky</surname><given-names>BA</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An olfactory virtual reality system for mice</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>839</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-03262-4</pub-id><pub-id pub-id-type="pmid">29483530</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rando</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Qlearning_for_navigation</data-title><version designator="swh:1:rev:3419b2652441ddb0fcee335bd41d8a8e167f3bdf">swh:1:rev:3419b2652441ddb0fcee335bd41d8a8e167f3bdf</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ed030683fa1c46b455e87f12c495c07a2beddb15;origin=https://github.com/Akatsuki96/qlearning_for_navigation;visit=swh:1:snp:2fd37818b442da0c62807938cf03c7787ad3cb1c;anchor=swh:1:rev:3419b2652441ddb0fcee335bd41d8a8e167f3bdf">https://archive.softwareheritage.org/swh:1:dir:ed030683fa1c46b455e87f12c495c07a2beddb15;origin=https://github.com/Akatsuki96/qlearning_for_navigation;visit=swh:1:snp:2fd37818b442da0c62807938cf03c7787ad3cb1c;anchor=swh:1:rev:3419b2652441ddb0fcee335bd41d8a8e167f3bdf</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>G</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Olfactory sensing and navigation in turbulent environments</article-title><source>Annual Review of Condensed Matter Physics</source><volume>13</volume><fpage>191</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1146/annurev-conmatphys-031720-032754</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>AM</given-names></name><name><surname>Reynolds</surname><given-names>DR</given-names></name><name><surname>Smith</surname><given-names>AD</given-names></name><name><surname>Chapman</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Orientation cues for high-flying nocturnal insect migrants: do turbulence-induced temperature and velocity fluctuations indicate the mean wind flow?</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e15758</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0015758</pub-id><pub-id pub-id-type="pmid">21209956</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigolli</surname><given-names>N</given-names></name><name><surname>Magnoli</surname><given-names>N</given-names></name><name><surname>Rosasco</surname><given-names>L</given-names></name><name><surname>Seminara</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Learning to predict target location with turbulent odor plumes</article-title><source>eLife</source><volume>11</volume><elocation-id>e72196</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.72196</pub-id><pub-id pub-id-type="pmid">35959726</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigolli</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>G</given-names></name><name><surname>Seminara</surname><given-names>A</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Alternation emerges as a multi-modal strategy for turbulent odor navigation</article-title><source>eLife</source><volume>11</volume><elocation-id>e76989</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.76989</pub-id><pub-id pub-id-type="pmid">35996954</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rigolli</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>G</given-names></name><name><surname>Seminara</surname><given-names>A</given-names></name><name><surname>Vergassola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022c</year><data-title>Alternation emerges as a multi-modal strategy for turbulent odor navigation - dataset</data-title><version designator="V1">V1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6538177">https://doi.org/10.5281/zenodo.6538177</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Save</surname><given-names>E</given-names></name><name><surname>Nerad</surname><given-names>L</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Contribution of multiple sensory information to place field stability in hippocampal place cells</article-title><source>Hippocampus</source><volume>10</volume><fpage>64</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(2000)10:1&lt;64::AID-HIPO7&gt;3.0.CO;2-Y</pub-id><pub-id pub-id-type="pmid">10706218</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schal</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Intraspecific vertical stratification as a mate-finding mechanism in tropical cockroaches</article-title><source>Science</source><volume>215</volume><fpage>1405</fpage><lpage>1407</lpage><pub-id pub-id-type="doi">10.1126/science.215.4538.1405</pub-id><pub-id pub-id-type="pmid">17753019</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shani</surname><given-names>G</given-names></name><name><surname>Pineau</surname><given-names>J</given-names></name><name><surname>Kaplow</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A survey of point-based POMDP solvers</article-title><source>Autonomous Agents and Multi-Agent Systems</source><volume>27</volume><fpage>1</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1007/s10458-012-9200-2</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shraiman</surname><given-names>BI</given-names></name><name><surname>Siggia</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Scalar turbulence</article-title><source>Nature</source><volume>405</volume><fpage>639</fpage><lpage>646</lpage><pub-id pub-id-type="doi">10.1038/35015000</pub-id><pub-id pub-id-type="pmid">10864314</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>SH</given-names></name><name><surname>van Breugel</surname><given-names>F</given-names></name><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Emergent behaviour and neural dynamics in artificial agents tracking odour plumes</article-title><source>Nature Machine Intelligence</source><volume>5</volume><fpage>58</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00599-w</pub-id><pub-id pub-id-type="pmid">37886259</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stupski</surname><given-names>SD</given-names></name><name><surname>van Breugel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024a</year><article-title>Wind gates olfaction-driven search states in free flight</article-title><source>Current Biology</source><volume>34</volume><fpage>4397</fpage><lpage>4411</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2024.07.009</pub-id><pub-id pub-id-type="pmid">39067453</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stupski</surname><given-names>D</given-names></name><name><surname>van Breugel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024b</year><article-title>Wind gates search states in free flight</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.11.30.569086</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>R</given-names></name><name><surname>Barto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suver</surname><given-names>MP</given-names></name><name><surname>Matheson</surname><given-names>AMM</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name><name><surname>Damiata</surname><given-names>M</given-names></name><name><surname>Schoppik</surname><given-names>D</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Encoding of wind direction by central neurons in <italic>Drosophila</italic></article-title><source>Neuron</source><volume>102</volume><fpage>828</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.03.012</pub-id><pub-id pub-id-type="pmid">30948249</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torr</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Behaviour of tsetse flies (Glossina) in host odour plumes in the field</article-title><source>Physiological Entomology</source><volume>13</volume><fpage>467</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1111/j.1365-3032.1988.tb01131.x</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Breugel</surname><given-names>F</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Plume-tracking behavior of flying <italic>Drosophila</italic> emerges from a set of distinct sensory-motor reflexes</article-title><source>Current Biology</source><volume>24</volume><fpage>274</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.12.023</pub-id><pub-id pub-id-type="pmid">24440395</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verano</surname><given-names>KVB</given-names></name><name><surname>Panizon</surname><given-names>E</given-names></name><name><surname>Celani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Olfactory search with finite-state controllers</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2304230120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2304230120</pub-id><pub-id pub-id-type="pmid">37579168</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergassola</surname><given-names>M</given-names></name><name><surname>Villermaux</surname><given-names>E</given-names></name><name><surname>Shraiman</surname><given-names>BI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>âInfotaxisâ as a strategy for searching without gradients</article-title><source>Nature</source><volume>445</volume><fpage>406</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1038/nature05464</pub-id><pub-id pub-id-type="pmid">17251974</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Verzicco</surname><given-names>R</given-names></name><name><surname>de Tullio</surname><given-names>M</given-names></name><name><surname>Viola</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>IBbookVdV</data-title><source>GitLab</source><ext-link ext-link-type="uri" xlink:href="https://gitlab.com/vdv9265847/IBbookVdV/">https://gitlab.com/vdv9265847/IBbookVdV/</ext-link></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Verzicco</surname><given-names>R</given-names></name><name><surname>de Tullio</surname><given-names>MD</given-names></name><name><surname>Viola</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2025">2025</year><source>An Introduction to Immersed Boundary Methods</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>F</given-names></name><name><surname>Meschini</surname><given-names>V</given-names></name><name><surname>Verzicco</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>FluidâStructure-Electrophysiology interaction (FSEI) in the left-heart: A multi-way coupled computational model</article-title><source>European Journal of Mechanics - B/Fluids</source><volume>79</volume><fpage>212</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.euromechflu.2019.09.006</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>F</given-names></name><name><surname>Spandan</surname><given-names>V</given-names></name><name><surname>Meschini</surname><given-names>V</given-names></name><name><surname>Romero</surname><given-names>J</given-names></name><name><surname>Fatica</surname><given-names>M</given-names></name><name><surname>de Tullio</surname><given-names>MD</given-names></name><name><surname>Verzicco</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>FSEI-GPU: GPU accelerated simulations of the fluidâstructureâelectrophysiology interaction in the left heart</article-title><source>Computer Physics Communications</source><volume>273</volume><elocation-id>108248</elocation-id><pub-id pub-id-type="doi">10.1016/j.cpc.2021.108248</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>F</given-names></name><name><surname>Del Corso</surname><given-names>G</given-names></name><name><surname>Verzicco</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>High-fidelity model of the human heart: An immersed boundary implementation</article-title><source>Physical Review Fluids</source><volume>8</volume><elocation-id>100502</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevFluids.8.100502</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willis</surname><given-names>MA</given-names></name><name><surname>Avondet</surname><given-names>JL</given-names></name><name><surname>Finnell</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of altering flow and odor information on plume tracking behavior in walking cockroaches, Periplaneta americana (L.)</article-title><source>The Journal of Experimental Biology</source><volume>211</volume><fpage>2317</fpage><lpage>2326</lpage><pub-id pub-id-type="doi">10.1242/jeb.016006</pub-id><pub-id pub-id-type="pmid">18587126</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>H</given-names></name><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Pinpointing food sources: olfactory and anemotactic orientation in desert ants, Cataglyphis fortis</article-title><source>The Journal of Experimental Biology</source><volume>203</volume><fpage>857</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1242/jeb.203.5.857</pub-id><pub-id pub-id-type="pmid">10667968</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname><given-names>A</given-names></name><name><surname>Schwarz</surname><given-names>S</given-names></name><name><surname>Baniel</surname><given-names>A</given-names></name><name><surname>Cheng</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Backtracking behaviour in lost ants: an additional strategy in their navigational toolkit</article-title><source>Proceedings. Biological Sciences</source><volume>280</volume><elocation-id>20131677</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2013.1677</pub-id><pub-id pub-id-type="pmid">23966644</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>YSW</given-names></name><name><surname>Graff</surname><given-names>MM</given-names></name><name><surname>Bresee</surname><given-names>CS</given-names></name><name><surname>Man</surname><given-names>YB</given-names></name><name><surname>Hartmann</surname><given-names>MJZ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whiskers aid anemotaxis in rats</article-title><source>Science Advances</source><volume>2</volume><elocation-id>e1600716</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.1600716</pub-id><pub-id pub-id-type="pmid">27574705</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Manahan-Vaughan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spatial olfactory learning contributes to place field formation in the hippocampus</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>423</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht239</pub-id><pub-id pub-id-type="pmid">24008582</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102906.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study uses reinforcement learning to study how turbulent odor stimuli should be processed to yield successful navigation. The authors find that there is an optimal memory length over which an agent should ignore blanks in the odor to discriminate whether the agent is still inside the plume or outside of it, complementing recent studies using recurrent neural networks and finite state controllers to identify optimal strategies for navigating a turbulent plume. The strength of evidence is <bold>compelling</bold>, presenting a novel approach to understanding optimal representations for navigation in stochastic sensory environments.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102906.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Overall I found the approach taken by the authors to be clear and convincing. It is striking that the conclusions are similar to those obtained in a recent study using a different computational approach (finite state controllers), and lends confidence to the conclusions about the existence of an optimal memory duration. There are a few questions that could be expanded on in future studies:</p><p>(1) Spatial encoding requirements</p><p>The manuscript contrasts the approach taken here (reinforcement learning in a gridworld) with strategies that involve a &quot;spatial map&quot; such as infotaxis. However, the gridworld navigation algorithm has an implicit allocentric representation, since movement can be in one of four allocentric directions (up, down, left, right), and wind direction is defined in these coordinates. Future studies might ask if an agent can learn the strategy without a known wind direction if it can only go left/right/forward/back/turn (in egocentric coordinates). In discussing possible algorithms, and the features of this one, it might be helpful to distinguish (1) those that rely only on egocentric computations (run and tumble), (2) those that rely on a single direction cue such as wind direction, (3) those that rely on allocentric representations of direction, and (4) those that rely on a full spatial map of the environment.</p><p>(2) Recovery strategy on losing the plume</p><p>The authors explore several recovery strategies upon losing the plume, including backtracking, circling, and learned strategies, finding that a learned strategy is optimal. As insects show a variety of recovery strategies that can depend on the model of locomotion, it would be interesting in the future to explore under which conditions various recovery strategies are optimal and whether they can predict the strategies of real animals in different environments.</p><p>(3) Is there a minimal representation of odor for efficient navigation?</p><p>The authors suggest that the number of olfactory states could potentially be reduced to reduce computational cost. They show that reducing the number of olfactory states to 1 dramatically reduces performance. In the future it would be interesting to identify optimal internal representations of odor for navigation and to compare these to those found in real olfactory systems. Does the optimal number of odor and void states depend on the spatial structure of the turbulence as explored in Figure 5?</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102906.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors investigate the problem of olfactory search in turbulent environments using artificial agents trained using tabular Q-learning, a simple and interpretable reinforcement learning (RL) algorithm. The agents are trained solely on odor stimuli, without access to spatial information or prior knowledge about the odor plume's shape. This approach makes the emergent control strategy more biologically plausible for animals navigating exclusively using olfactory signals. The learned strategies show parallels to observed animal behaviors, such as upwind surging and crosswind casting. The approach generalizes well to different environments and effectively handles the intermittency of turbulent odors.</p><p>Strengths:</p><p>* The use of numerical simulations to generate realistic turbulent fluid dynamics sets this paper apart from studies that rely on idealized or static plumes.</p><p>* A key innovation is the introduction of a small set of interpretable olfactory states based on moving averages of odor intensity and sparsity, coupled with an adaptive temporal memory.</p><p>* The paper provides a thorough analysis of different recovery strategies when an agent loses the odor trail, offering insights into the trade-offs between various approaches.</p><p>* The authors provide a comprehensive performance analysis of their algorithm across a range of environments and recovery strategies, demonstrating the versatility of the approach.</p><p>* Finally, the authors list an interesting set of real-world experiments based on their findings, that might invite interest from experimentalists across multiple species.</p><p>Weaknesses:</p><p>* Using tabular Q-learning is both a strength and a limitation. It's simple and interpretable, making it easier to analyze the learned strategies, but the discrete action space seems somewhat unnatural. In real-world biological systems, actions (like movement) are continuous rather than discrete. Additionally, the ground-frame actions may not map naturally to how animals navigate odor plumes (e.g. insects often navigate based on their own egocentric frame).</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102906.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rando</surname><given-names>Marco</given-names></name><role specific-use="author">Author</role><aff><institution>University of Genoa</institution><addr-line><named-content content-type="city">Genoa</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>James</surname><given-names>Martin</given-names></name><role specific-use="author">Author</role><aff><institution>University of Genoa</institution><addr-line><named-content content-type="city">Genoa</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>Verri</surname><given-names>Alessandro</given-names></name><role specific-use="author">Author</role><aff><institution>University of Genoa</institution><addr-line><named-content content-type="city">Genoa</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>Rosasco</surname><given-names>Lorenzo</given-names></name><role specific-use="author">Author</role><aff><institution>University of Genova</institution><addr-line><named-content content-type="city">Genova</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>Seminara</surname><given-names>Agnese</given-names></name><role specific-use="author">Author</role><aff><institution>University of Genoa</institution><addr-line><named-content content-type="city">Genoa</named-content></addr-line><country>Italy</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the current reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public review):</bold></p><p>Overall I found the approach taken by the authors to be clear and convincing. It is striking that the conclusions are similar to those obtained in a recent study using a different computational approach (finite state controllers), and lends confidence to the conclusions about the existence of an optimal memory duration. There are a few questions that could be expanded on in future studies:</p><p>(1) Spatial encoding requirements</p><p>The manuscript contrasts the approach taken here (reinforcement learning in a gridworld) with strategies that involve a &quot;spatial map&quot; such as infotaxis. However, the gridworld navigation algorithm has an implicit allocentric representation, since movement can be in one of four allocentric directions (up, down, left, right), and wind direction is defined in these coordinates. Future studies might ask if an agent can learn the strategy without a known wind direction if it can only go left/right/forward/back/turn (in egocentric coordinates). In discussing possible algorithms, and the features of this one, it might be helpful to distinguish (1) those that rely only on egocentric computations (run and tumble), (2) those that rely on a single direction cue such as wind direction, (3) those that rely on allocentric representations of direction, and (4) those that rely on a full spatial map of the environment.</p></disp-quote><p>We agree that the question of what orientation skills are needed to implement an algorithm is interesting. We remark that our agents do not use allocentric directions in the sense of north, east, west and east relative to e.g. fixed landmarks in the environment. Instead, directions are defined <italic>relative to the mean wind</italic>, which is assumed fixed and known. (In our first answer to reviewers we used ânorth east south west relative to mean windâ, which may have caused confusion â but in the manuscript we only use upwind downwind and crosswind).</p><disp-quote content-type="editor-comment"><p>(2) Recovery strategy on losing the plume</p><p>The authors explore several recovery strategies upon losing the plume, including backtracking, circling, and learned strategies, finding that a learned strategy is optimal. As insects show a variety of recovery strategies that can depend on the model of locomotion, it would be interesting in the future to explore under which conditions various recovery strategies are optimal and whether they can predict the strategies of real animals in different environments.</p></disp-quote><p>Agreed, it will be interesting to study systematically the emergence of distinct recovery strategies and compare to living organisms.</p><disp-quote content-type="editor-comment"><p>(3) Is there a minimal representation of odor for efficient navigation?</p><p>The authors suggest that the number of olfactory states could potentially be reduced to reduce computational cost. They show that reducing the number of olfactory states to 1 dramatically reduces performance. In the future it would be interesting to identify optimal internal representations of odor for navigation and to compare these to those found in real olfactory systems. Does the optimal number of odor and void states depend on the spatial structure of the turbulence as explored in Figure 5?</p></disp-quote><p>We agree that minimal odor representations are an intriguing question. While tabular Q learning cannot derive optimal odor representations systematically, one could expand on the approach we have taken here and provide more comparisons. It will be interesting to follow this approach in a future study.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The authors investigate the problem of olfactory search in turbulent environments using artificial agents trained using tabular Q-learning, a simple and interpretable reinforcement learning (RL) algorithm. The agents are trained solely on odor stimuli, without access to spatial information or prior knowledge about the odor plume's shape. This approach makes the emergent control strategy more biologically plausible for animals navigating exclusively using olfactory signals. The learned strategies show parallels to observed animal behaviors, such as upwind surging and crosswind casting. The approach generalizes well to different environments and effectively handles the intermittency of turbulent odors.</p><p>Strengths:</p><p>* The use of numerical simulations to generate realistic turbulent fluid dynamics sets this paper apart from studies that rely on idealized or static plumes.</p><p>* A key innovation is the introduction of a small set of interpretable olfactory states based on moving averages of odor intensity and sparsity, coupled with an adaptive temporal memory.</p><p>* The paper provides a thorough analysis of different recovery strategies when an agent loses the odor trail, offering insights into the trade-offs between various approaches.</p><p>* The authors provide a comprehensive performance analysis of their algorithm across a range of environments and recovery strategies, demonstrating the versatility of the approach.</p><p>* Finally, the authors list an interesting set of real-world experiments based on their findings, that might invite interest from experimentalists across multiple species.</p><p>Weaknesses:</p><p>* Using tabular Q-learning is both a strength and a limitation. It's simple and interpretable, making it easier to analyze the learned strategies, but the discrete action space seems somewhat unnatural. In real-world biological systems, actions (like movement) are continuous rather than discrete. Additionally, the ground-frame actions may not map naturally to how animals navigate odor plumes (e.g. insects often navigate based on their own egocentric frame).</p></disp-quote><p>We agree with the reviewer, and will look forward to study this problem further to make it suitable for meaningful comparisons with animal behavior.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>The authors have addressed my major concerns and I support publication of this interesting manuscript. A couple of small suggestions:</p><p>(1) In discussing performance in different environments (line 328-362) it might be easier to read if you referred to the environments by descriptive names rather than numbers.</p></disp-quote><p>Thank you for the suggestion, which we implemented</p><disp-quote content-type="editor-comment"><p>(2) Line 371: measurements of flow speed depend on antennae in insects. Insects can measure local speed and direct of flow using antennae, e.g. Bell and Kramer, 1979, Suver et al. 2019. Okubo et al. 2020,</p></disp-quote><p>Thank you for the references</p><disp-quote content-type="editor-comment"><p>(3) line 448: &quot;Similarly, an odor detection elicits upwind surges that can last several seconds&quot; maybe &quot;Similarly, an odor detection elicits upwind surges that can outlast the odor by several seconds&quot;?</p></disp-quote><p>Thank you for the suggestion</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>I commend the authors for their revisions in response to reviewer feedback.</p><p>While I appreciate that the manuscript is now accompanied by code and data, I must note that the accompanying code-repository lacks proper instructions for use and is likely incomplete (e.g. where is the main function one should run to run your simulations? How should one train? How should one recreate the results? Which data files go where?).</p><p>For examples of high-quality code-release, please see the documentation for these RL-for-neuroscience code repositories (from previously published papers):</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/ryzhang1/Inductive_bias">https://github.com/ryzhang1/Inductive_bias</ext-link></p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/BruntonUWBio/plumetracknets">https://github.com/BruntonUWBio/plumetracknets</ext-link></p><p>The accompanying data does provide snapshots from their turbulent plume simulations, which should be valuable for future research.</p></disp-quote><p>Thank you for the suggestions for how to improve clarity of the code. The way we designed the repository is to serve both the purpose of developing the code as well as sharing. This is because we are going to build up on this work to proceed further. Nothing is missing in the repository (we know it because it is what we actually use).</p><p>We do plan to create a more user-friendly version of the code, hopefully this will be ready in the next few months, but it wont be immediate as we are aiming to also integrate other aspects of the work we are currently doing in the Lab. The Brunton repository is very well organized, thanks for the pointer.</p><p>The following is the authorsâ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Overall I found the approach taken by the authors to be clear and convincing. It is striking that the conclusions are similar to those obtained in a recent study using a different computational approach (finite state controllers), and lend confidence to the conclusions about the existence of an optimal memory duration. There are a few points or questions that could be addressed in greater detail in a revision:</p><p>(1) Discussion of spatial encoding</p><p>The manuscript contrasts the approach taken here (reinforcement learning in a grid world) with strategies that involve a &quot;spatial map&quot; such as infotaxis. The authors note that their algorithm contains &quot;no spatial information.&quot; However, I wonder if further degrees of spatial encoding might be delineated to better facilitate comparisons with biological navigation algorithms. For example, the gridworld navigation algorithm seems to have an implicit allocentric representation, since movement can be in one of four allocentric directions (up, down, left, right). I assume this is how the agent learns to move upwind in the absence of an explicit wind direction signal. However, not all biological organisms likely have this allocentric representation. Can the agent learn the strategy without wind direction if it can only go left/right/forward/back/turn (in egocentric coordinates)? In discussing possible algorithms, and the features of this one, it might be helpful to distinguish</p><p>(1) those that rely only on egocentric computations (run and tumble),</p><p>(2) those that rely on a single direction cue such as wind direction,</p><p>(3) those that rely on allocentric representations of direction, and</p><p>(4) those that rely on a full spatial map of the environment.</p></disp-quote><p>As Referee 1 points out, even if the algorithm does not require a map of space, the agent is still required to tell apart directions relative to the wind direction which is assumed known. Indeed, although in the manuscript we labeled actions allocentrically as â up down left and rightâ, the source is always placed in the same location, hence âleftâ corresponds to upwind; ârightâ to downwind and âupâ and âdownâ to crosswind right and left. Thus in fact directions are relative to the mean wind, which is therefore assumed known. We have better clarified the spatial encoding required to implement these strategies, and re-labeled the directions as upwind, downwind, crosswind-right and crosswind-left.</p><p>In reality, animals cannot measure the mean flow, but rather the local flow speed e.g. with antennas for insects, with whiskers for rodents and with the lateral line for marine organisms. Further work is needed to address how local flow measures enable navigation using Q learning.</p><disp-quote content-type="editor-comment"><p>(2) Recovery strategy on losing the plume</p><p>While the approach to encoding odor dynamics seems highly principled and reaches appealingly intuitive conclusions, the approach to modeling the recovery strategy seems to be more ad hoc. Early in the paper, the recovery strategy is defined to be path integration back to the point at which odor was lost, while later in the paper, the authors explore Brownian motion and a learned recovery based on multiple &quot;void&quot; states. Since the learned strategy works best, why not first consider learned strategies, and explore how lack of odor must be encoded or whether there is an optimal division of void states that leads to the best recovery strategies? Also, although the authors state that the learned recovery strategies resemble casting, only minimal data are shown to support this. A deeper statistical analysis of the learned recovery strategies would facilitate comparison to those observed in biology.</p></disp-quote><p>We thank Referee 1 for their remarks and suggestion to give the learned recovery a more prominent role and better characterize it. We agree that what is done in the void state is definitely key to turbulent navigation. In the revised manuscript, we have further substantiated the statistics of the learned recovery by repeating training 20 times and comparing the trajectories in the void (Figure 3 figure supplement 3, new Table 1). We believe however that starting with the heuristic recovery is clearer because it allows to introduce the concept of recovery more clearly. Indeed, the learned ârecoveryâ is so flexible that it ends up mixing recovery (crosswind motion) to aspects of exploitation (surge): we defer a more in-depth analysis that disentangles these two aspects elsewhere. Also, we added a whole new comparison with other biologically inspired recoveries both in the native environment and for generalization (Figure 3 and 5).</p><disp-quote content-type="editor-comment"><p>(3) Is there a minimal representation of odor for efficient navigation?</p><p>The authors suggest (line 280) that the number of olfactory states could potentially be reduced to reduce computational cost. This raises the question of whether there is a maximally efficient representation of odors and blanks sufficient for effective navigation. The authors choose to represent odor by 15 states that allow the agent to discriminate different spatial regimes of the stimulus, and later introduce additional void states that allow the agent to learn a recovery strategy. Can the number of states be reduced or does this lead to loss of performance? Does the optimal number of odor and void states depend on the spatial structure of the turbulence as explored in Figure 5?</p></disp-quote><p>We thank the referee for their comment. Q learning defines the olfactory states prior to training and does not allow a systematic optimization of odor representation for the task. We can however compare different definitions of the olfactory states, for example based on the same features but different discretizations. We added a comparison with a drastically reduced number of non-empty olfactory states to just 1, i.e. if the odor is above threshold at any time within the memory, the agent is in the non-void olfactory state, otherwise it is in the void state. This drastic reduction in the number of olfactory states results in less positional information and degrades performance (Figure 5 figure supplement 5).</p><p>The number of void states is already minimal: we chose 50 void states because this matches the time agents typically remain in the void (less than 50 void states results in no convergence and more than 50 introduces states that are rarely visited).</p><p>One may instead resort to deep Q-learning or to recurrent neural networks, which however do not provide answers as for what are the features or olfactory states that drive behavior (see discussion in manuscript and questions below).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The authors investigate the problem of olfactory search in turbulent environments using artificial agents trained using tabular Q-learning, a simple and interpretable reinforcement learning (RL) algorithm. The agents are trained solely on odor stimuli, without access to spatial information or prior knowledge about the odor plume's shape. This approach makes the emergent control strategy more biologically plausible for animals navigating exclusively using olfactory signals. The learned strategies show parallels to observed animal behaviors, such as upwind surging and crosswind casting. The approach generalizes well to different environments and effectively handles the intermittency of turbulent odors.</p><p>Strengths:</p><p>(1) The use of numerical simulations to generate realistic turbulent fluid dynamics sets this paper apart from studies that rely on idealized or static plumes.</p><p>(2) A key innovation is the introduction of a small set of interpretable olfactory states based on moving averages of odor intensity and sparsity, coupled with an adaptive temporal memory.</p><p>(3) The paper provides a thorough analysis of different recovery strategies when an agent loses the odor trail, offering insights into the trade-offs between various approaches.</p><p>(4) The authors provide a comprehensive performance analysis of their algorithm across a range of environments and recovery strategies, demonstrating the versatility of the approach.</p><p>(5) Finally, the authors list an interesting set of real-world experiments based on their findings, that might invite interest from experimentalists across multiple species.</p><p>Weaknesses:</p><p>(1) The inclusion of Brownian motion as a recovery strategy, seems odd since it doesn't closely match natural animal behavior, where circling (e.g. flies) or zigzagging (ants' &quot;sector search&quot;) could have been more realistic.</p></disp-quote><p>We agree that Brownian motion may not be biologically plausible -- we used it as a simple benchmark. We clarified this point, and re-trained our algorithm with adaptive memory using circling and zigzaging (cast and surge) recoveries. The learned recovery outperforms all heuristic recoveries (Figure 3D, metrics G). Circling ranks second, and achieves these good results by further decreasing the probability of failure and paying slightly in speed. When tested in the non-native environments 2 to 6, the learned recovery performs best in environments 2, 5 and 6 i.e. from long range more relevant to flying insects; whereas circling generalizes best in odor rich environments 3 and 4, representative of closer range and close to the substrate (Figure 5B, metrics G). In the new environments, similar to the native environment, circling favors convergence (Figure 5B, metrics f<sup>+</sup>) over speed (Figure 5B, metrics g<sup>+</sup> and Ï<sub>min</sub>/Ï), which is particularly deleterious at large distance.</p><disp-quote content-type="editor-comment"><p>(2) Using tabular Q-learning is both a strength and a limitation. It's simple and interpretable, making it easier to analyze the learned strategies, but the discrete action space seems somewhat unnatural. In real-world biological systems, actions (like movement) are continuous rather than discrete. Additionally, the ground-frame actions may not map naturally to how animals navigate odor plumes (e.g. insects often navigate based on their own egocentric frame).</p></disp-quote><p>We agree with the reviewer that animal locomotion does not look like a series of discrete displacements on a checkerboard. However, to overcome this limitation, one has to first focus on a specific system to define actions in a way that best adheres to a speciesâ motor controls. Moreover, these actions are likely continuous, which makes reinforcement learning notoriously more complex. While we agree that more realistic models are definitely needed for a comparison with real systems, this remains outside the scope of the current work. We have added a remark to clarify this limitation.</p><disp-quote content-type="editor-comment"><p>(3) The lack of accompanying code is a major drawback since nowadays open access to data and code is becoming a standard in computational research. Given that the turbulent fluid simulation is a key element that differentiates this paper, the absence of simulation and analysis code limits the study's reproducibility.</p></disp-quote><p>We have published the code and the datasets at</p><p>- code: <ext-link ext-link-type="uri" xlink:href="https://github.com/Akatsuki96/qNav">https://github.com/Akatsuki96/qNav</ext-link></p><p>- datasets: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/14655992">https://zenodo.org/records/14655992</ext-link></p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) Line 59-69: In comparing the results here to other approaches (especially the Verano and Singh papers), it would also be helpful to clarify which of these include an explicit representation of the wind direction. My understanding is that both the Singh and Verano approaches include an explicit representation of wind direction. In Singh wind direction is one of the observations that inputs to the agent, while in Verano, the actions are defined relative to the wind direction. In the current paper, my understanding is that there is no explicitly defined wind direction, but because movement directions are encoded allocentrically, the agent is able to learn the upwind direction from the structure of the plume- is this correct? I think this information would be helpful to spell out and also to address whether an agent without any allocentric direction sense can learn the task.</p></disp-quote><p>Thank you for the comment. In our algorithm the directions are defined relative to the mean wind, which is assumed known, as in Verano et al. As far as we understand, Singh et al provide the instantaneous, egocentric wind velocities as part of the input.</p><disp-quote content-type="editor-comment"><p>(1) Line 105: &quot;several properties of odor stimuli depend on the distance from the source&quot; might cite Boie...Victor 2018, Ackles...Schaefer, 2021, Nag...van Breugel 2024.</p></disp-quote><p>Thank you for the suggestions - we have added these references</p><disp-quote content-type="editor-comment"><p>(2) Line 130: &quot;we first define a finite set of olfactory states&quot; might be helpful to the reader to state what you chose in this paragraph rather than further down.</p></disp-quote><p>We have slightly modified the incipit of the paragraph. We first declare we are setting out to craft the olfactory states, then define the challenges, finally we define the olfactory states.</p><disp-quote content-type="editor-comment"><p>(3) Line 267: &quot;Note that the learned recovery strategy resembles casting behavior observed in flying insects&quot; Might note that insects seem to deploy a range of recovery strategies depending on locomotor mode and environment. For example, flying flies circle and sink when odor is lost in windless environments (Stupski and van Breugel 2024).</p></disp-quote><p>Thank you for your comment. We have included the reference and we now added comparisons to results using circling and cast &amp; surge recovery strategies.</p><disp-quote content-type="editor-comment"><p>(4) Line 289: &quot;from positions beyond the source, the learned strategy is unable to recover the plume as it mostly casts sideways, with little to no downwind action&quot; This is curious as many insects show a downwind bias in the absence of odor that helps them locate the plumes in the first place (e.g. Wolf and Wehner, 2000, Alvarez-Salvado et al. 2018). Is it possible that the agent could learn a downwind bias in the absence of odor if given larger environments or a longer time to learn?</p></disp-quote><p>The reviewer is absolutely correct â Downwind motion is not observed in the recovery simply because the agent rarely overshoots the source. Hence overall optimization for that condition is washed out by the statistics. We believe downwind motion will emerge if an agent needs to avoid overshooting the source â we do not have conclusive results yet but are planning to introduce such flexibility in a further work. We added this remark and refs.</p><disp-quote content-type="editor-comment"><p>(5) Line 377-391: testing these ideas in living systems. Interestingly, Kathman..Nagel 2024 (bioRxiv) shows exactly the property predicted here and in Verano in fruit flies- an odor memory that outlasts the stimulus by a duration of several seconds, appropriate for filling in &quot;blanks.&quot; Relatedly, Alvarez-Salvado et al. 2018 showed that fly upwind running reflected a temporal integration of odor information over ~10s, sufficient to avoid responding to blanks as loss of odor.</p></disp-quote><p>Indeed, we believe this is the most direct connection between algorithms and experiments. We are excited to discuss with our colleagues and pursue a more direct comparison with animal behavior. We were aware of the references and forgot to cite them, thank you for your careful reading of our work !</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>Suggestions</p><p>(1) The paper does not clearly specify which type of animals (e.g., flying insects, terrestrial mammals) the model is meant to approximate or not approximate. The authors should consider clarifying how these simulations are suited to be a general model across varied olfactory navigators. Further, it isn't clear how low/high the intermittency studied in this model is compared to what different animals actually encounter. (Minor: The Figure 4 occupancy circles visualization could be simplified).</p></disp-quote><p>Environment 1 represents the lower layers of a moderately turbulent boundary layer. Search occurs on a horizontal plane ~half meter from the ground. The agent is trained at distances of about 10 meters and also tested on longer distances ~ 17 meters (environment 6), lower heights ~1cm from the ground (environments 3-4), lower Reynolds number (environment 5) and higher threshold of detection (environment 2 and 4). Thus Environments 1,2,5 and 6 are representative of conditions encountered by flying organisms (or pelagic in water), and Environments 3 and 4 of searches near the substrate, potentially involved in terrestrial navigation (benthic in water). Even near the substrate, we use odor dispersed in the fluid, and not odor attached to the substrate (relevant to trail tracking).</p><p>Also note that we pick Schmidt number Sc = 1 and this is appropriate for odors in air but not in water. However, we expect a weak dependence on the Schmidt number as the Batchelor and Kolmogorov scales are below the size of the source and we are interested in the large scale statistics Falkovich et al., 2001; Celani et al., 2014; Duplat et al., 2010.</p><p>Intermittency contours are shown in Fig 1C, they are highest along the centerline, and decay away from the centerline, so that even within the plume detecting odor is relatively rare. Only a thin region near the centerline has intermittency larger than 66%; the outer and most critical bin of the plume has intermittency under 33%; in the furthest point on the centerline intermittency is &lt;10%. For reference, experimental values in the atmospheric boundary layer report intermittency 25% to 20% at 2 to 15m from the source along the centerline (Murlis and Jones, 1981).</p><p>We have more clearly labeled the contours in Fig 1C and added these remarks.</p><p>We included these remarks and added a whole table with matching to real conditions within the different environments.</p><disp-quote content-type="editor-comment"><p>(2) Could some biological examples and references be added to support that backtracking is a biologically plausible mechanism?</p></disp-quote><p>Backtracking was observed e.g. in ants displaced in unfamiliar environments (Wystrach et al, P Roy Soc B, 280, 2013), in tsetse flies executing reverse turns uncorrelated to wind, which bring them back towards the location where they last detected odor (Torr, Phys Entom, 13, 1988, Gibson &amp; Brady Phys Entom 10, 1985) and in coackroaches upon loss of contact with the plume (Willis et al, J. Exp. Biol. 211, 2008). It is also used in computational models of olfactory navigation (Park et al, Plos Comput Biol, 12:e1004682, 2016).</p><disp-quote content-type="editor-comment"><p>(3) Hand-crafted features can be both a strength and a limitation. On the one hand, they offer interpretability, which is crucial when trying to model biological systems. On the other hand, they may limit the generality of the model. A more thorough discussion of this paper's limitations should address this.</p><p>(4) The authors mention the possibility of feature engineering or using recurrent neural networks, but a more concrete discussion of these alternatives and their potential advantages/disadvantages would be beneficial. It should be noted that the hand-engineered features in this manuscript are quite similar to what the model of Singh et al suggests emerges in their trained RNNs.</p></disp-quote><p>Merged answer to points 3 and 4.</p><p>We agree with the reviewer that hand-crafted features are both a strength and a limitation in terms of performance and generality. This was a deliberate choice aimed at stripping the algorithm bare of implicit components, both in terms of features and in terms of memory. Even with these simple features, our model performs well in navigating across different signals, consistent with our previous results showing that these features are a âgoodâ surrogate for positional information.</p><p>To search for the most effective temporal features, one may consider a more systematic hand crafting, scaling up our approach. In this case one would first define many features of the odor trace; rank groups of features for their accuracy in regression against distance; train Q learning with the most promising group of features and rank again. Note however that this approach will be cumbersome because multiple factors will have to be systematically varied: the regression algorithm; the discretization of the features and the memory.</p><p>Alternatively, to eliminate hand crafting altogether and seek better performance or generalization, one may consider replacing these hand-crafted features and the tabular Q-learning approach with recurrent neural networks or with finite state controllers. On the flip side, neither of these algorithms will directly provide the most effective features or the best memory, because these properties are hidden within the parameters that are optimized for. So extra work is needed to interrogate the algorithms and extract these information. For example, in Singh et al, the principal components of the hidden states in trained agents correlate with head direction, odor concentration and time since last odor encounter. More work is needed to move beyond correlations and establish more systematically what are the features that drive behavior in the RNN.</p><p>We have added these points to the discussion.</p><disp-quote content-type="editor-comment"><p>(5) Minor: the title of the paper doesn't immediately signal its focus on recovery strategies and their interplay with memory in the context of olfactory navigation. Given the many other papers using a similar RL approach, this might help the authors position this paper better.</p></disp-quote><p>We agree with the referee and have modified the title to reflect this.</p><disp-quote content-type="editor-comment"><p>(6) Minor: L 331: &quot;because turbulent odor plumes constantly switch on and off&quot; -- the signal received rather than the plume itself is switching on and off.</p></disp-quote><p>Thank you for the suggestion, we implemented it.</p></body></sub-article></article>