<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74943</article-id><article-id pub-id-type="doi">10.7554/eLife.74943</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Perception of an object’s global shape is best described by a model of skeletal structure in human infants</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-259230"><name><surname>Ayzenberg</surname><given-names>Vladislav</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2739-3935</contrib-id><email>vayzenb@cmu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-259228"><name><surname>Lourenco</surname><given-names>Stella</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3070-7122</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Neuroscience Institute, Carnegie Mellon University</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Department of Psychology, Emory University</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>25</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e74943</elocation-id><history><date date-type="received" iso-8601-date="2021-10-22"><day>22</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-09"><day>09</day><month>05</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Ayzenberg and Lourenco</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Ayzenberg and Lourenco</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-74943-v1.pdf"/><abstract><p>Categorization of everyday objects requires that humans form representations of shape that are tolerant to variations among exemplars. Yet, how such invariant shape representations develop remains poorly understood. By comparing human infants (6–12 months; N=82) to computational models of vision using comparable procedures, we shed light on the origins and mechanisms underlying object perception. Following habituation to a never-before-seen object, infants classified other novel objects across variations in their component parts. Comparisons to several computational models of vision, including models of high-level and low-level vision, revealed that infants’ performance was best described by a model of shape based on the skeletal structure. Interestingly, infants outperformed a range of artificial neural network models, selected for their massive object experience and biological plausibility, under the same conditions. Altogether, these findings suggest that robust representations of shape can be formed with little language or object experience by relying on the perceptually invariant skeletal structure.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>infant development</kwd><kwd>categorization</kwd><kwd>object recognition</kwd><kwd>medial axis</kwd><kwd>one-shot learning</kwd><kwd>shape perception</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32 HD071845</award-id><principal-award-recipient><name><surname>Ayzenberg</surname><given-names>Vladislav</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Six- to twelve-month old infants, who have little linguistic or object experience, classify objects by relying on a invariant representation of global shape known as the shape skeleton.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The appearance of objects within a single category can vary greatly. For instance, despite the shared category of dog, the exemplars (i.e. different dogs) may have different snouts, tails, and/or torsos. Despite this variability, humans readily categorize never-before-seen dog breeds as members of the same basic-level category. How do we do this? Although there is widespread agreement that shape information is crucial for object categorization (<xref ref-type="bibr" rid="bib12">Biederman, 1995</xref>; <xref ref-type="bibr" rid="bib56">Mervis and Rosch, 1981</xref>), it remains unclear how humans come to form global representations of shape that are tolerant to variations among exemplars.</p><p>It has been suggested that global shape information becomes crucial for object categorization in early childhood because linguistic experience, particularly the learning of object labels (e.g. ‘dog’), draws children’s attention to global shape as a diagnostic cue—inducing a so-called ‘shape bias’ (<xref ref-type="bibr" rid="bib49">Landau et al., 1998</xref>; <xref ref-type="bibr" rid="bib81">Smith et al., 1996</xref>; <xref ref-type="bibr" rid="bib82">Smith et al., 2002</xref>). Indeed, labels may bootstrap object recognition abilities more generally, such that even prelinguistic children are better at individuating and categorizing objects when verbal labels are provided (<xref ref-type="bibr" rid="bib27">Ferry et al., 2010</xref>; <xref ref-type="bibr" rid="bib96">Xu et al., 2005</xref>). The advantage of labeled object experience is particularly evident in supervised artificial neural networks (ANNs), which have begun to match the object recognition abilities and neural representations of human adults (<xref ref-type="bibr" rid="bib43">Krizhevsky et al., 2017</xref>; <xref ref-type="bibr" rid="bib66">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Schrimpf et al., 2018</xref>). These models learn the diagnostic properties of objects following training with millions of labeled naturalistic images. With appropriate experience, these models may even develop a shape bias that supports object categorization, at least when generalizing across variations in color or texture (<xref ref-type="bibr" rid="bib69">Ritter et al., 2017</xref>; <xref ref-type="bibr" rid="bib87">Tartaglini et al., 2022</xref>). Thus, global representations of shape may develop with labeled object experience that highlights the diagnostic properties of objects.</p><p>An alternative possibility, however, is that rather than labeled experience, humans develop global shape representations by relying on (non-linguistic) invariant perceptual properties inherent to the object (<xref ref-type="bibr" rid="bib10">Biederman, 1987</xref>; <xref ref-type="bibr" rid="bib24">Feldman, 1997</xref>; <xref ref-type="bibr" rid="bib67">Rakison and Butterworth, 1998</xref>; <xref ref-type="bibr" rid="bib80">Sloutsky, 2003</xref>). One such property is known as the shape skeleton—a quantitative model that describes an object’s global shape via a series of internal symmetry axes (<xref ref-type="bibr" rid="bib14">Blum, 1967</xref>; <xref ref-type="bibr" rid="bib25">Feldman and Singh, 2006</xref>). These axes define the topological arrangement of object parts, making models of skeletal structure tolerant to local variations in shape typical of basic-level exemplars (<xref ref-type="bibr" rid="bib4">Ayzenberg et al., 2019a</xref>; <xref ref-type="bibr" rid="bib92">Wilder et al., 2011</xref>). From this perspective, extensive experience with objects and linguistic labels may not be necessary to form global shape representations, and, instead, one might rely on the shape skeleton (<xref ref-type="bibr" rid="bib24">Feldman, 1997</xref>). However, the contributions of labeled experience and the shape skeleton are difficult to examine because, by adulthood, humans have had massive amounts of labeled object experience, making the source of their shape representations ambiguous. Here we tested whether human infants (who have little linguistic or object experience) represent global object shape according to a shape skeleton.</p><p>Object representations in infancy have most often been tested using visual attention procedures. In these experiments, infants are typically habituated to stimuli that share a common visual dimension (e.g. shape), but vary according to other dimensions (e.g. color). Infants are then tested with objects that are either familiar (e.g. similar in shape to the habituated object) or novel (i.e. different in shape to the habituated object). If infants learn the relevant dimension during the habituation phase, then their looking times are longer for the novel object compared to the familiar one. Habituation paradigms provide an informative window into infants’ object representations because they reveal what properties infants learned during the habituation phase and subsequently generalized to the test phase. Using this approach, researchers have shown that newborns can already discriminate between simple 2D shapes (<xref ref-type="bibr" rid="bib77">Slater et al., 1983</xref>) and display shape constancy, such that they recognize a shape from a novel orientation (<xref ref-type="bibr" rid="bib78">Slater and Morison, 1985</xref>). By 6 months of age, infants’ shape representations are also robust to variations among category exemplars, such that they can categorize objects using only the stimulus’ shape silhouette (<xref ref-type="bibr" rid="bib62">Quinn et al., 1993</xref>; <xref ref-type="bibr" rid="bib63">Quinn et al., 2001a</xref>), as well as extend category membership to objects with varying local contours, but the same global shape (<xref ref-type="bibr" rid="bib65">Quinn et al., 2002</xref>; <xref ref-type="bibr" rid="bib64">Quinn et al., 2001b</xref>; <xref ref-type="bibr" rid="bib88">Turati et al., 2003</xref>). However, the mechanisms underlying global shape representation remain unclear. Indeed, because infants in these studies were habituated to multiple (often familiar) objects, it is unclear whether shape representations in these studies were learned from the statistics of the habituation period (<xref ref-type="bibr" rid="bib58">Oakes and Spalding, 1997</xref>; <xref ref-type="bibr" rid="bib99">Younger, 1990</xref>) or, rather, are an invariant perceptual property infants extract from objects more generally.</p><p>In the current study, we used a habituation paradigm to examine how 6–12 months old infants represent the global shape of objects. In particular, we tested whether infants classified never-before-seen objects by comparing the similarity between their shape skeletons. Importantly, we habituated infants to only a single object so as to measure their pre-existing shape representations, rather than ones they may have learned over the course of habituation. We chose to test 6–12-month-olds because the visual experience at this age is dominated by just a few common objects (~10 objects; <xref ref-type="bibr" rid="bib17">Clerkin et al., 2017</xref>) and they have relatively little linguistic understanding (~7 words; <xref ref-type="bibr" rid="bib9">Bergelson and Swingley, 2012</xref>). Moreover, we elucidate the mechanisms that support object perception in infancy by comparing infants to a range of computational models. Of particular interest was a flux-based medial axis algorithm that computes a shape skeleton from images (<xref ref-type="bibr" rid="bib68">Rezanejad and Siddiqi, 2013</xref>). If infants represent objects using the shape skeleton, then their classification judgments should best match that of a Skeletal model.</p><p>In addition, we included four ANNs, known for their success on object recognition tasks but which do not represent the shape skeleton. These ANNs included two ResNet models, one trained on ImageNet (ResNet-IN), a convolutional ANN frequently used in object recognition tasks, and a variation of ResNet trained on Stylized-ImageNet (ResNet-SIN), an image set that leads models to develop a shape bias, at least in relation to color and textures cues (<xref ref-type="bibr" rid="bib30">Geirhos et al., 2018</xref>). Two other ANNs were CorNet-S, a top-performing model of object recognition behavior and neural processing in primates, as measured by the brain-score benchmark (<xref ref-type="bibr" rid="bib75">Schrimpf et al., 2018</xref>), and ResNext-SAY, a model trained with an unsupervised learning algorithm on first-person videos from infants (<xref ref-type="bibr" rid="bib60">Orhan et al., 2020</xref>). All of these ANNs were included because they exhibit varying degrees of biological plausibility in terms of neural organization or visual experience (<xref ref-type="bibr" rid="bib74">Russakovsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib75">Schrimpf et al., 2018</xref>). If infant performance is best matched by ANNs, then this would suggest that global shape representations might be learned as a diagnostic cue following extensive object experience. Importantly, because none of these models represent the shape skeleton, they make for an excellent contrast to the Skeletal model. Finally, we also included a model of pixel similarity, and FlowNet, a model of optic flow (<xref ref-type="bibr" rid="bib34">Ilg et al., 2017</xref>) in order to assess the extent to which shape representations may be supported by lower-level visual properties like image similarity (<xref ref-type="bibr" rid="bib42">Kiat et al., 2022</xref>; <xref ref-type="bibr" rid="bib95">Xie et al., 2021</xref>) or motion trajectory (<xref ref-type="bibr" rid="bib38">Kellman, 1984</xref>; <xref ref-type="bibr" rid="bib39">Kellman and Short, 1987</xref>; <xref ref-type="bibr" rid="bib94">Wood and Wood, 2018</xref>). Altogether, these comparisons provided a novel approach to understanding object perception in human infants.</p><p>Because the strength of any object classification task depends on the degree of dissimilarity between training (i.e. habituation) and test objects, infants were tested with objects that mimicked the variability of basic-level category exemplars (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Within-category objects comprised objects with the same skeletons, but visually distinct component parts. Variation in the component parts was generated by manipulating the objects’ surface forms, thereby changing both the image-level features and the non-accidental properties (NAPs; <xref ref-type="bibr" rid="bib10">Biederman, 1987</xref>), without altering the skeleton. Between-category objects comprised different skeletons and surface forms. If infants are capable of classifying objects vis-à-vis a shape skeleton, then they should look longer at objects with different skeletons compared to those with the same skeleton, even though both objects differ from the habituated object in surface form. Importantly, we tested whether within-category and between-category objects were equally discriminable by infants to ensure that any differences in looking time were not related to infants’ ability to differentiate surface forms. Moreover, if infants rely on objects’ shape skeletons to determine similarity, then their classification performance should be best matched by the Skeletal model, rather than ANNs, or other models of vision. Thus, the current study provides critical insight regarding the nature of shape representations at an age when experience with labeled objects is minimal, and, crucially, provides a novel benchmark by which to evaluate the biological plausibility of computational models of vision.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Screen shots of the stimuli used in Experiment 1 (left) and Experiment 2 (right).</title><p>Objects were presented as rotating videos during habituation and test phases.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74943-fig1-v1.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><p>Infants’ looking times were analyzed using two-sided paired sample <italic>t-</italic>tests (<italic>α</italic>=0.05) and standard measures of effect size (Cohen’s <italic>d</italic>). Furthermore, to ensure that sample size decisions did not unduly influence the results, we also conducted non-parametric (Binomial tests) and Bayesian analyses. A Bayes factor (BF<sub>10</sub>) was computed for each analysis using a standard Cauchy prior (<italic>d</italic>=0.707). A BF<sub>10</sub> greater than 1 is evidence that two distributions are different from one another, whereas a BF<sub>10</sub> less than 1 is evidence that two distributions are similar to one another (<xref ref-type="bibr" rid="bib71">Rouder et al., 2009</xref>).</p><sec id="s2-1"><title>How do infants represent shape?</title><p>A comparison of the two types of test trials in Experiment 1 (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) revealed that, across the test phase, infants looked longer at the object with the different skeleton compared to the one with the matching skeleton (<italic>t</italic>[33]=3.04, p=0.005, <italic>d</italic>=0.52, 95% CI [0.16, 0.88], BF<sub>10</sub>=8.42; <xref ref-type="fig" rid="fig2">Figure 2B</xref>), with the majority of infants showing this effect (25/34 infants, p=0.009). In addition, a comparison between the end of habituation (mean of last 4 trials) and looking times across the test phase revealed that dishabituation only occurred for the object with the different skeleton (<italic>t</italic>[33]=3.36, p=0.002, <italic>d</italic>=0.58, 95% CI [0.21, 0.94], BF<sub>10</sub>=17.47; 25/34 infants, p=0.009), not the object with the matching skeleton (<italic>t</italic>[33]=1.00, p=0.325, <italic>d</italic>=0.17, 95% CI [–0.16, 0.51], BF<sub>10</sub>=0.29; <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Thus, infants treated objects with matching skeletons as more similar to one another than objects with different skeletons.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experimental design and results for (top) Experiment 1 and (bottom) Experiment 2.</title><p>(<bold>A, D</bold>) Illustration of the experimental procedure administered to infants and the computational models in (<bold>A</bold>) Experiment 1 and (<bold>D</bold>) Experiment 2. Infants and models were habituated to one object and then tested with objects that consisted of either the same or different shape skeleton. Both types of test objects (counterbalanced order) differed in their surface forms from the habituation object. (<bold>B, E</bold>) Mean looking times for (<bold>B</bold>) Experiment 1 and (<bold>E</bold>) Experiment 2. For the habituation phase, results are shown for the first four and last four trials. For the test phase, results are shown for the two types of test objects (i.e. same and different skeletons; 3 test trials each). Error bars represent SE. (<bold>C, F</bold>) Classification performance for infants and models for (<bold>C</bold>) Experiment 1 and (<bold>F</bold>) Experiment 2. Error bars represent bootstrapped confidence intervals, and the dashed line represents chance performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74943-fig2-v1.tif"/></fig><p>However, one might ask whether infants were simply unable to differentiate between surface forms, leading to greater looking at the object with a different skeleton. To test this possibility, we compared infants’ looking times on the first test trial following habituation to the last trial during habituation. Because the first test trial immediately follows habituation, this comparison allows for a direct measure of perceptual discriminability between habituation and test objects when the memory demands are minimal and in the absence of carry-over effects between test objects. This analysis revealed that infants were indeed capable of discriminating objects on the basis of surface form alone. That is, infants dishabituated to the object with the same skeleton but different surface form, <italic>t</italic>(15) = 3.76, <italic>p</italic>=0.002, <italic>d</italic>=0.94, 95% CI [0.34, 1.52], BF<sub>10</sub>=22.23, with the majority of infants showing this effect (14/16, <italic>p</italic>=0.004). Moreover, and crucially, infants' looking times on the first test trial did not differ for either test object (8.31 s vs. 9.18 s; <italic>t</italic>[32]=0.50, <italic>p</italic>=0.624, <italic>d</italic>=.17, 95% CI [–0.51, 0.84], BF<sub>10</sub>=0.36). These findings demonstrate that not only could infants differentiate between surface forms, but also, that the two types of test objects were matched for discriminability relative to the habituation object. These findings argue against a pure discrimination account and, instead, support the interpretation that infants classified objects on the basis of skeletal similarity.</p><p>But did infants actually rely on the object’s shape skeleton to judge similarity or some other visual representation? To explore this possibility, we compared infants’ classification behavior to a flux-based Skeletal model, a range of ANNs (ResNet-IN, ResNet-SIN, and CorNet-S, ResNext-Say), and models of image similarity (Pixel) and motion (FlowNet). If infants relied on the shape skeleton to classify objects, then their performance would be best matched by the Skeletal model, rather than the others.</p><p>Models were tested with the same stimuli presented to infants using a procedure comparable to the habituation paradigm. More specifically, because habituation/dishabituation can be conceived as a measure of alignment between the stimulus and the infant’s internal representation (<xref ref-type="bibr" rid="bib53">Mareschal et al., 2000</xref>; <xref ref-type="bibr" rid="bib90">Westermann and Mareschal, 2004</xref>), we tested models by feeding their outputs into an autoencoder and measuring the error signal across habituation and test phases (see Methods). Like habituation paradigms, the error signal of an autoencoder reflects the degree of alignment between the internal representation of the model and the input stimulus (for review, see <xref ref-type="bibr" rid="bib98">Yermolayeva and Rakison, 2014</xref>). Unlike conventional classifiers, which often require multiple labeled contrasting examples (e.g. Support Vector Machines), autoencoders allow for measuring a model’s performance following exposure to just one exemplar, as with infants in the current study. Moreover, like infant learning during habituation, the learned representation of an autoencoder reflects the entire habituation video, rather than the representation of individual frames. Most importantly, unlike other techniques, autoencoders can be tested using the same habituation and test criteria as infants (see Methods). For comparison, performance for both infants and models was converted into a classification score (see Methods) and significance was assessed using bootstrapped confidence intervals (5000 iterations).</p><p>These analyses revealed that all models performed above chance (0.50; see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). However, and importantly, infant performance was best matched to the Skeletal model. Both infants and the Skeletal model performed significantly better than all other models, except FlowNet. That infants outperformed the ANNs suggests that extensive object experience may not be necessary to develop robust shape representations. Likewise, that infants outperformed the Pixel model suggests that infant performance is not explained by the low-level visual similarity between objects. However, the success of FlowNet does suggest that motion information may contribute to representations of shape. Nevertheless, FlowNet’s performance did not differ from that of any ANN or the Pixel model, leaving its exact role in object classification unclear. Altogether, these results suggest that infants’ performance is most closely aligned with the Skeletal model and, thus, that infants classified objects, at least in part, by relying on the similarity between objects’ shape skeletons.</p></sec><sec id="s2-2"><title>Can infant performance be explained by another representation of global shape?</title><p>An alternative explanation for the findings from the first experiment is that, rather than the shape skeleton, infants classified objects using another representation of global shape—namely, the coarse spatial relations among object parts (<xref ref-type="bibr" rid="bib11">Biederman and Gerhardstein, 1993</xref>; <xref ref-type="bibr" rid="bib33">Hummel and Stankiewicz, 1996</xref>). Whereas a Skeletal model provides a continuous, quantitative description of part relations, a model based on coarse spatial relations describes part relations in qualitative terms (e.g. two parts below a third vs. two parts on either side of a third). In Experiment 1, test objects with different skeletons also consisted of part relations that could be considered qualitatively different from that of the habituated object, making it unclear whether infants relied on coarse spatial relations instead of the shape skeletons.</p><p>To address this possibility, in Experiment 2, coarse spatial relations were held constant between habituation and test phases (i.e. two parts on either side of a third; <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2D</xref>). Thus, if infants relied on coarse spatial relations for object classification, then they would fail to dishabituate to the test object that differed in the shape skeleton (but not coarse spatial relations). However, we found that infants continued to look longer at the object with the different skeleton compared to the one with the matching skeleton (<italic>t</italic>[47]=2.60, <italic>p</italic>=0.012, <italic>d</italic>=0.38, 95% CI [0.08, 0.67], BF<sub>10</sub>=3.18; <xref ref-type="fig" rid="fig2">Figure 2E</xref>), with the majority of infants showing this effect (33/48 infants, <italic>p</italic>=0.013). Moreover, infants only dishabituated during the test phase to the object with the different skeleton (<italic>t</italic>[47]=3.63, p&lt;0.001, <italic>d</italic>=0.52, 95% CI [0.22, 0.82], BF<sub>10</sub>=39.77; 36/48 infants, <italic>p</italic>&lt;0.001), not the one with the matching skeleton (<italic>t</italic>[47]=1.42, <italic>p</italic>=0.163, <italic>d</italic>=0.16, 95% CI [–0.08, 0.49], BF<sub>10</sub>=0.40), as in Experiment 1. The results from this experiment rule out the possibility that infants classified objects on the basis of their coarse spatial relations, rather than their shape skeletons.</p><p>Importantly, as in the previous experiment, we also compared infants’ looking times on the first test trial to the last trial during habituation to ensure that infants could distinguish surface forms and that the two test objects were equally discriminable. We found that infants dishabituated to the object with the same shape skeleton but different surface form, <italic>t</italic>(22)=3.51, <italic>p</italic>=0.002, <italic>d</italic>=0.73, 95% CI [0.26, 1.19], BF<sub>10</sub>=19.71, with the majority of infants showing this effect (17/23, p=0.035), suggesting that discrimination was possible on the basis of surface form alone. Moreover, infants' looking times on the first test trial did not differ for the two types of test objects (6.87 s vs. 10.83 s; <italic>t</italic>[45]=1.47, p=0.149, <italic>d</italic>=0.43, 95% CI [–0.15, 1.00], BF<sub>10</sub>=0.69), confirming comparable discriminability. Thus, as in Experiment 1, these findings suggest that although infants were capable of discriminating both types of test objects, they nevertheless treated objects with the same skeletons as more similar to one another.</p><p>To ensure that the effects in Experiment 2 were not unduly influenced by the larger sample size, we computed bootstrapped CIs on a smaller sample. For each bootstrap procedure (10,000 iterations), we calculated Cohen’s <italic>d</italic> on data that were resampled (without replacement) to match the sample size of Experiment 1 (<italic>n</italic>=34). We found that infants dishabituated to the test object with a different skeleton, 95% CI [0.22, 0.87], but not the same skeleton, 95%CI [–0.13, 0.61]. Infants also looked longer at the test object with a different skeleton than the test object with the same skeleton, 95% CI [0.20, 0.61], confirming the robustness of these effects regardless of sample size. Finally, analyses of the first test trial confirmed that infants discriminated between surface forms when objects had the same skeletons, 95% CI [0.22, 0.81,] and that looking times did not significantly differ between the two types of test trials, 95% CI [–0.15, 0.49].</p><p>To examine whether infants’ performance was best described by a Skeletal model we, again, compared infants to a model that represents the shape skeleton, as well as to models that do not. These analyses revealed that the Skeletal, Pixel, and FlowNet models all performed above chance (<xref ref-type="fig" rid="fig2">Figure 2F</xref>), but infant performance was most closely matched to the Skeletal model. None of the ANNs performed above chance. As in Experiment 1, both infants and the Skeletal model significantly outperformed the ANNs and the Pixel model, but not FlowNet. These findings again suggest that infants’ judgments reflect the use of skeletal structure and that there may be a role for motion information in this ability.</p></sec><sec id="s2-3"><title>Model classification using local shape properties</title><p>Given the competitive performance of ANNs on other object recognition tasks, and their match to adult performance in other studies (<xref ref-type="bibr" rid="bib75">Schrimpf et al., 2018</xref>), one might wonder why ANNs performed so poorly on our task. One possibility is that ANNs rely on local shape properties (e.g. surface form), which were irrelevant to our task (<xref ref-type="bibr" rid="bib7">Baker et al., 2018</xref>; <xref ref-type="bibr" rid="bib8">Baker et al., 2020</xref>). To test this possibility, we examined whether ANNs were capable of classifying objects using local shape properties, namely, the surface forms of the objects. For comparison, we also tested the other models’ performance on this task (Skeleton, Pixel, and FlowNet).</p><p>Using a comparable procedure to the previous experiments, all models were tested with objects in which the surface forms either matched or differed from the habituated object; both test objects differed in their shape skeletons (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For objects from Experiment 1 (<xref ref-type="fig" rid="fig1">Figure 1</xref>), the analyses revealed that ResNet-IN, CorNet-S, and ResNext-SAY performed above chance (Ms = 0.62–0.66, 95% CIs [0.53–0.59, 0.70–0.76]; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). By contrast, none of the other models (Skeleton, ResNet-SIN, Pixel, FlowNet) performed differently from chance (Ms = 0.52–0.61, 95% CIs [0.40–0.48, 0.60–0.72]; <xref ref-type="fig" rid="fig3">Figure 3B</xref>) when classifying objects by surface form (i.e. across shape skeletons). For objects from Experiment 2 (<xref ref-type="fig" rid="fig1">Figure 1</xref>), all models performed above chance (Ms = 0.67–0.74, 95% CIs [0.51–0.67, 0.72–0.80]), except Skeletal and Pixel models (Ms = 0.54–0.59, 95% CIs [0.40–0.41, 0.68–0.73]; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). Altogether, these findings demonstrate that most ANNs are capable of classifying objects using local shape properties. These findings are consistent with other research showing sensitivity to local shape properties in ANNs (<xref ref-type="bibr" rid="bib7">Baker et al., 2018</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experimental design and results for the surface form classification task used with the computational models.</title><p>(<bold>A</bold>) Illustration of the experimental procedure administered to models. (<bold>B–C</bold>) Classification performance of models on stimuli from (<bold>B</bold>) Experiment 1 and (<bold>C</bold>) Experiment 2. Error bars represent bootstrapped confidence intervals and dashed lines represent chance performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74943-fig3-v1.tif"/></fig></sec><sec id="s2-4"><title>General discussion</title><p>Although it is well known that shape is crucial for object categorization, much speculation remains about how representations of global shape are formed in development. Here, we demonstrate that infants represent global shape by extracting a skeletal structure. With only one exemplar as a reference, infants classified objects by their shape skeletons across variations in local properties that altered component parts of the objects, including when coarse spatial relations could not be used as a diagnostic cue. Moreover, a Skeletal model provided the closest match to infants’ performance, further suggesting that infants relied on the shape skeleton to determine object similarity. Based on these findings, we would argue that the formation of robust shape representations in humans is largely rooted in sensitivity to the shape skeleton, an invariant representation of global shape.</p></sec><sec id="s2-5"><title>The role of other visual properties in object perception</title><p>It is important to acknowledge that our results also suggest that object classification can be accomplished on the basis of image-level similarity. In particular, when generalizing across the surface form, a Pixel model performed above chance. Does this mean that infants’ performance can be explained by low-level image similarity between habituation and test objects rather than the shape skeleton? We would suggest not. First, we found that infants discriminated both test objects from the habituation object equally, which argues against the possibility that infants’ performance was strictly based on differences in image-level similarity. Second, both infants and the Skeletal model outperformed the Pixel model in both experiments, further arguing against an account based on image-level similarity alone. Nevertheless, we would not argue that image similarity plays no role in object perception, particularly in infants. Indeed, recent studies comparing the visual representations of infants and computational models reveal that low-level visual similarity explains more variance in infants’ behavioral and neural responses than the upper layers of ANNs (<xref ref-type="bibr" rid="bib42">Kiat et al., 2022</xref>; <xref ref-type="bibr" rid="bib95">Xie et al., 2021</xref>). Moreover, recent studies suggest that object categorization in infancy may be supported by the representations of the early visual cortex (V1-V3), rather than the higher-level ventral cortex, as in adults (<xref ref-type="bibr" rid="bib83">Spriet et al., 2022</xref>).</p><p>Like image-level representations, shape skeletons, themselves might be an emergent property of early visual regions. Specifically, several studies have suggested that skeletal representations may emerge in area V3 as a consequence of between-layer recurrent interactions among border-ownership cells in area V2 and grouping cells in area V3 (<xref ref-type="bibr" rid="bib3">Ardila et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Craft et al., 2007</xref>). Interestingly, representations of skeletal structures in V3 are invariant across changes in component parts, which lends support to their role in categorization (<xref ref-type="bibr" rid="bib6">Ayzenberg et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Lescroart and Biederman, 2013</xref>). Other research on the anatomical and functional organization of V3 in primates shows evidence of functional maturity shortly after birth (<xref ref-type="bibr" rid="bib2">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib23">Ellis et al., 2020</xref>; <xref ref-type="bibr" rid="bib91">Wiesel and Hubel, 1974</xref>), further raising the possibility that skeletal structure is represented in this area with little object experience or any language.</p><p>In the present study, we also found that FlowNet, a model of optic flow, appeared comparable to infants and the Skeletal model at classifying objects when they differed in surface form. This finding highlights the role of motion in the formation of invariant object representations (<xref ref-type="bibr" rid="bib50">Lee et al., 2021</xref>; <xref ref-type="bibr" rid="bib61">Ostrovsky et al., 2009</xref>). It is known that infants are better at recognizing objects from novel viewpoints when they are first familiarized with moving objects rather than static images (<xref ref-type="bibr" rid="bib38">Kellman, 1984</xref>; <xref ref-type="bibr" rid="bib40">Kellman and Shipley, 1991</xref>; <xref ref-type="bibr" rid="bib39">Kellman and Short, 1987</xref>). Moreover, controlled-rearing studies with chicks have found that viewpoint-invariant object recognition occurs only if the chicks are raised in environments in which objects exhibit smooth, continuous motion (<xref ref-type="bibr" rid="bib94">Wood and Wood, 2018</xref>). Indeed, several studies suggest that motion information may initially bootstrap infants’ ability to extract 3D shape structure (<xref ref-type="bibr" rid="bib41">Kellman and Arterberry, 2006</xref>; <xref ref-type="bibr" rid="bib61">Ostrovsky et al., 2009</xref>; <xref ref-type="bibr" rid="bib94">Wood and Wood, 2018</xref>). Thus, motion information may work in concert with shape skeletons to support object perception in infancy. Yet, it is important to note that classifying objects on our task was also possible without relying on motion cues (<xref ref-type="bibr" rid="bib63">Quinn et al., 2001a</xref>; <xref ref-type="bibr" rid="bib64">Quinn et al., 2001b</xref>). Indeed, the Skeletal model, which does not incorporate any motion information, was more closely aligned to the performance of human infants than was FlowNet.</p><p>In contrast to infants and the other models, ANNs were not capable of classifying objects across variations in surface form. However, they were capable of classifying objects by their surface form (across variation in shape skeleton), ruling out alternative explanations for their poor performance in the first task based on idiosyncrasies of the stimuli or testing procedures. Moreover, this finding suggests that, regardless of the specific architecture or training type, conventional ANNs rely on qualitatively different mechanisms than do humans to represent objects. This class of models may be especially sensitive to local object properties, making them susceptible to spurious changes in the objects’ contours (<xref ref-type="bibr" rid="bib7">Baker et al., 2018</xref>; <xref ref-type="bibr" rid="bib8">Baker et al., 2020</xref>). By contrast, representations of skeletal structure in humans are particularly robust to perturbations (<xref ref-type="bibr" rid="bib4">Ayzenberg et al., 2019a</xref>; <xref ref-type="bibr" rid="bib25">Feldman and Singh, 2006</xref>) and, thus, may be especially well-suited to basic-level categorization. Although conventional ANNs (e.g. ResNet-SIN) can generalize across variations in color and texture (<xref ref-type="bibr" rid="bib30">Geirhos et al., 2018</xref>; <xref ref-type="bibr" rid="bib87">Tartaglini et al., 2022</xref>), fundamental changes to ANNs’ architectures and/or training may be needed before they can achieve human-like categorization on the basis of global shape.</p><p>Despite our claim that infants relied on skeletal similarity to classify objects, we would not suggest that it is the only information represented by humans. Indeed, by adulthood, humans also use visual properties such as texture (<xref ref-type="bibr" rid="bib35">Jagadeesh and Gardner, 2022</xref>; <xref ref-type="bibr" rid="bib52">Long et al., 2018</xref>) and local contours (<xref ref-type="bibr" rid="bib20">Davitt et al., 2014</xref>), as well as inferential processes such as abstract rules (<xref ref-type="bibr" rid="bib59">Ons and Wagemans, 2012</xref>; <xref ref-type="bibr" rid="bib72">Rouder and Ratcliff, 2016</xref>) and the object’s generative history (<xref ref-type="bibr" rid="bib28">Fleming and Schmidt, 2019</xref>; <xref ref-type="bibr" rid="bib84">Spröte et al., 2016</xref>) to reason about objects. Properties such as texture and local features may even override shape skeletons in certain contexts, such as when identifying objects in the periphery (<xref ref-type="bibr" rid="bib29">Gant et al., 2021</xref>) or during subordinate-level categorization (<xref ref-type="bibr" rid="bib20">Davitt et al., 2014</xref>; <xref ref-type="bibr" rid="bib86">Tarr and Bülthoff, 1998</xref>). We suggest that the shape skeleton may be uniquely suited to the basic level of categorization, wherein objects have similar global shapes but vary in their component parts (<xref ref-type="bibr" rid="bib12">Biederman, 1995</xref>; <xref ref-type="bibr" rid="bib70">Rosch et al., 1976</xref>). However, more research will be needed to understand the extent to which children may also make use of other properties when categorizing objects.</p></sec><sec id="s2-6"><title>Implications for one-shot categorization</title><p>A key aspect of our design was that infants were habituated to a single, novel object. We did this to better understand the pre-existing visual representations infants rely on for object perception, rather than the ones they may learn over the course of habituation. Using this approach, we found that infants reliably classified objects on the basis of the shape skeleton. Might this result also suggest that infants are capable of one-shot categorization using the shape skeleton? One-shot categorization is the process of learning novel categories following exposure to just one exemplar (<xref ref-type="bibr" rid="bib47">Lake and Piantadosi, 2019</xref><xref ref-type="bibr" rid="bib45">Lake et al., 2011</xref>; <xref ref-type="bibr" rid="bib57">Morgenstern et al., 2019</xref>; <xref ref-type="bibr" rid="bib76">Shepard, 1987</xref>). We cannot answer this question definitively, given that visual attention paradigms make it difficult to distinguish between category learning per se and judgments of visual similarity. However, it is intriguing to consider whether the shape skeleton may support rapid object learning at an age when linguistic and object experience is minimal.</p><p>How well do our results align with the one-shot categorization literature from older children and adults? On the one hand, our results are consistent with studies showing that one-shot categorization of objects by older children and adults involves identifying invariant visual properties of the objects, namely shape (<xref ref-type="bibr" rid="bib13">Biederman and Bar, 1999</xref>; <xref ref-type="bibr" rid="bib24">Feldman, 1997</xref>; <xref ref-type="bibr" rid="bib48">Landau et al., 1988</xref>). Moreover, research using simple visual objects (e.g. handwritten characters) suggests that a skeleton-like compositional structure is central to one-shot categorization (<xref ref-type="bibr" rid="bib45">Lake et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Lake et al., 2015</xref>). On the other hand, our results would suggest that such categorization is possible at a much earlier age than has previously been suggested. Indeed, the extant research with both children (<xref ref-type="bibr" rid="bib49">Landau et al., 1998</xref>; <xref ref-type="bibr" rid="bib82">Smith et al., 2002</xref>; <xref ref-type="bibr" rid="bib97">Xu and Kushnir, 2013</xref>) and adults (<xref ref-type="bibr" rid="bib18">Coutanche and Thompson-Schill, 2014</xref>; <xref ref-type="bibr" rid="bib73">Rule and Riesenhuber, 2020</xref>) has been taken as evidence that such categorization abilities are accomplished by leveraging extensive category and linguistic knowledge. Thus, our results are consistent with the hypothesis that one-shot category learning may be possible early in development by relying on a perceptually invariant skeletal structure.</p><p>However, we would not suggest that object experience is irrelevant in infancy. As mentioned previously, experience with moving objects may play a role in supporting the extraction of a 3D shape structure. Moreover, although infants’ visual experience is dominated by a small number of objects, this experience includes a large volume of viewpoints for each object (<xref ref-type="bibr" rid="bib17">Clerkin et al., 2017</xref>). Interestingly, infants’ chosen views of objects are biased toward planar orientations (<xref ref-type="bibr" rid="bib36">James et al., 2014</xref>; <xref ref-type="bibr" rid="bib79">Slone et al., 2019</xref>), which may serve to highlight the shape skeleton, though it is unclear whether such a bias is a consequence or a cause of skeletal extraction. Moreover, certain visual experiences, such as low visual acuity at birth, may be particularly important for highlighting global shape information (<xref ref-type="bibr" rid="bib16">Cassia et al., 2002</xref>; <xref ref-type="bibr" rid="bib61">Ostrovsky et al., 2009</xref>). Indeed, ANNs trained with a blurry-to-clear visual regimen showed improved performance for categories where global information is important, such as faces (<xref ref-type="bibr" rid="bib37">Jang and Tong, 2021</xref>; <xref ref-type="bibr" rid="bib89">Vogelsang et al., 2018</xref>).</p><p>Altogether, our work suggests that infants form robust global representations of shape by extracting the object’s shape skeleton. With limited language and object experience, infants generalized across variations in local shape properties to classify objects—a feat not matched by conventional ANNs. Nevertheless, by comparing infants’ performance to existing computational models of vision, the present study provides unique insight into humans’ representations of shape and their capacity for categorization, and may serve to inform future computational models of human vision.</p></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Participants</title><p>A total of 92 full-term infants participated in this study. Ten infants were excluded (Experiment 1: 5 for fussiness and 1 because of equipment failure; Experiment 2: 3 for fussiness and 1 because of equipment failure). The final sample included 34 infants in Experiment 1 (<italic>M</italic>=9.53 months, range = 6.47–12.20 months; 18 females) and 48 infants in Experiment 2 (<italic>M</italic>=9.12 months, range = 6.17–12.00 months; 20 females). Each infant was tested only once. All families gave informed consent according to a protocol approved by the Emory University Institutional Review Board (IRB) under the project ‘Spatial Origins’ (Study Number IRB0003452).</p><p>The sample size for Experiment 1 was determined using a priori power analysis with a hypothesized medium effect size (<italic>d</italic>=0.50; 1 - <italic>β</italic>&gt;.8). For Experiment 2, we hypothesized that objects with the same coarse spatial relations would be more difficult to discriminate because the shape skeletons were more similar to one another (compared to Experiment 1), leading to an attenuated effect. Accordingly, to retain adequate power, we tested 14 more infants than in Experiment 1, the exact number of which was determined according to a fully counterbalanced design. Importantly, we nevertheless find the same results in Experiment 2 when the sample size is subsampled (n=34) to match that of Experiment 1.</p></sec><sec id="s3-2"><title>Stimuli</title><p>For Experiment 1, six videos of 3D novel objects were rendered from the stimulus set created by <xref ref-type="bibr" rid="bib5">Ayzenberg and Lourenco, 2019b</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>. The objects were comprised of three distinct skeletons selected (from a set of 30) on the basis of their skeletal similarity. The skeletal similarity was calculated in 3D, object-centered, space as the mean Euclidean distance between each point on one skeleton and the closest point on the second skeleton following maximal alignment. A <italic>k</italic>-means cluster analysis (<italic>k</italic>=3) was used to select three distinct skeletons, one from each cluster (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We ensured that the three skeletons were matched for discriminability by analyzing participants’ discrimination judgments using data from <xref ref-type="bibr" rid="bib5">Ayzenberg and Lourenco, 2019b</xref>. Adult participants (<italic>n</italic>=42) were shown images of two objects (side-by-side) with either the same or different skeletons (same surface form). Participants were instructed to decide whether the two images showed the same or different objects. A repeated-measures ANOVA, with skeleton pair as the within-subject factor, revealed that the three skeletons used in Experiment 1 did not significantly differ in their discriminability, <italic>F</italic>(2, 64)=0.11, <italic>p</italic>=0.898.</p><p>For Experiment 2, we selected one object from Experiment 1 whose skeleton could be altered without changing the coarse spatial relations. We altered the object’s skeleton by moving one segment 50% down the length of the central segment (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Each object was also rendered with two different surface forms, which changed the component parts and image-level properties of the object without altering its skeleton (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The selection of these surface forms was based on adult participants’ data from the study of <xref ref-type="bibr" rid="bib5">Ayzenberg and Lourenco, 2019b</xref>. In a match-to-sample task, participants (<italic>n</italic>=39) were shown one object (sample) placed centrally above two choice objects. One of the choice objects matched the sample’s skeleton, but not the surface form, and the other choice object matched the sample’s surface form, but not the skeleton. Participants were instructed to decide which of the two choice objects was most likely to be from the same category as the sample object. Participants performed worst at categorizing objects by their skeleton when surface form 1 was paired with surface form 2, <italic>M</italic>=0.58, compared to the other surface forms (<italic>Ms =</italic> 0.61–0.78). Thus, by choosing the surface forms that presented adult participants with the greatest conflict, we provided infants with an especially strong test of generalization on the basis of the skeletal structure.</p><p>In a separate set of analyses, we tested whether the surface forms were comprised of qualitatively different component parts by having participants rate each surface form on the degree to which it exhibited a specific non-accidental property (NAP). During a training phase, adult participants (<italic>n</italic>=34) were taught four NAPs (drawn from <xref ref-type="bibr" rid="bib1">Amir et al., 2012</xref>). They then rated the degree to which each surface form exhibited a particular NAP. The four NAPs were: (1) <italic>taper</italic>, defined as whether the thickness of an object part was reduced towards the end; (2) <italic>positive curvature</italic>, defined as whether an object part bulged outwards; (3) <italic>negative curvature</italic>, defined as the degree to which an object part caved inwards; and (4) <italic>convergence to vertex</italic>, defined as whether an object part ended in a point. Prior to the statistical analyses, we ensured that all participants in the sample exhibited reliable performance (αs &gt;0.7). A repeated-measures ANOVA, with NAP as the within-subject factor and surface form as the between-subject factor, revealed a significant main effect of surface form, <italic>F</italic>(1, 66)=64.00, <italic>p</italic>&lt;0.001, such that surface forms corresponded to different NAPs. Thus, because objects between habituation and test phases consisted of different NAPs, it could not be used as a diagnostic cue for categorizing the different test objects.</p><p>We also tested whether objects with different surface forms, but the same skeleton, had significantly different image-level properties, in order to ensure that both objects presented to infants during the test phase differed from the habituation object in these properties. Each object video was converted into a sequence of images (30 frames/s; 300 frames total), which were analyzed with the Gabor-jet model (<xref ref-type="bibr" rid="bib54">Margalit et al., 2016</xref>). This model overlays a 12 × 12 grid of Gabor filters (5 scales × 8 orientations) on each image. The image is convolved with each filter, and the magnitude and phase of the filtered image is stored as a feature vector. Paired <italic>t</italic>-tests were used to compare the feature vectors from each frame of one video to the corresponding frames of the second video. To provide an estimate of the image-level difference across the entire video, the resulting p-values from each <italic>t</italic>-test were then averaged across frames. This analysis revealed that objects with different surface forms (but same skeleton) had significantly different image-level properties (<italic>p</italic>=0.002), whereas objects with different skeletons (but same surface forms) did not (<italic>p</italic>=0.090). In other words, the surface forms used in the present study were actually more variable than the shape skeletons with respect to image-level properties.</p><p>Finally, we ensured that surface forms were matched in discriminability to the selected skeletons. Adult participants (<italic>n</italic>=41) conducted a surface form discrimination task, wherein they were shown images of two objects (side-by-side) which consisted of either the same or different surface forms (same skeleton). Participants were instructed to decide whether the two images showed the same or different objects. Participants discriminated between surface forms 1 and 2 significantly better than would be predicted by chance, <italic>M</italic>=0.86, <italic>t</italic>(40) = 8.95, <italic>p</italic>&lt;.001, and importantly, discrimination accuracy between surface forms did not differ from discrimination accuracy between skeletons, <italic>t</italic>(80) = 0.02, <italic>p</italic>=.981.</p></sec><sec id="s3-3"><title>Procedure for testing infants</title><p>Each infant sat on their caregiver’s lap approximately 60 cm from a 22-inch computer monitor (1920×1080 px). Caregivers were instructed to keep their eyes closed and to refrain from interacting with the infant during the study session. The experiment was controlled by a custom program written in Visual Basic (Microsoft) and gaze data were recorded with an EyeLink 1000 plus eye tracker recording at 500 Hz (SR-Research). Prior to the start of the experiment, the eye tracker was calibrated for each infant using a 5-point calibration routine. Looking times were coded as any fixation falling within the screen for at least 500 ms. Any trial failing to meet this criterion was not analyzed (2.03% of trials in Experiment 1; 2.04% of trials in Experiment 2).</p><p>The experiment consisted of a habituation phase, in which infants were presented with one object, followed by a test phase where classification was tested using objects with matching and different skeletons. Both test objects differed from the habituated object in their surface form (see <xref ref-type="fig" rid="fig2">Figure 2A and D</xref>). Each trial began with an attention-getting stimulus, which remained onscreen until infants fixated on it for 2 s. On each trial, infants were then shown a video of a single object rotating back-and-forth across 60° (12° per second). Each video remained on screen for 60 s or until infants looked away for 2 s. Videos were used instead of static images to maintain infants’ attention.</p><p>Each infant was habituated to an object with one of three possible skeletons in Experiment 1 and with one of two possible skeletons in Experiment 2 (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), with half of the infants habituated to each surface form in each experiment. Infants met the habituation criterion when the average looking time in the preceding four trials was less than 50% of the average looking time in the first four trials. Test trials were presented after infants had habituated or following 24 habituation trials, whichever came first. All infants were presented with a total of six test trials, alternating between objects with the same or different skeletons (3 test trials of each type). The type of first test trial (same or different skeleton) was counterbalanced across infants.</p></sec><sec id="s3-4"><title>Model descriptions</title><p>For our Skeletal model, we used a flux-based medial axis algorithm (<xref ref-type="bibr" rid="bib21">Dimitrov et al., 2003</xref>; <xref ref-type="bibr" rid="bib68">Rezanejad and Siddiqi, 2013</xref>) which computes a ‘pruned’ skeletal structure tolerant to local contour variations (<xref ref-type="bibr" rid="bib25">Feldman and Singh, 2006</xref>). A pruned Skeletal model was selected for its biological plausibility in describing human shape judgments (<xref ref-type="bibr" rid="bib4">Ayzenberg et al., 2019a</xref>; <xref ref-type="bibr" rid="bib26">Feldman et al., 2013</xref>; <xref ref-type="bibr" rid="bib92">Wilder et al., 2011</xref>; <xref ref-type="bibr" rid="bib93">Wilder et al., 2019</xref>).</p><p>The four ANNs tested in the present study were: ResNet-IN, ResNet-SIN, CorNet-S, and ResNext-SAY. ResNet-IN and ResNet-SIN are 50-layer residual networks (<xref ref-type="bibr" rid="bib31">He et al., 2016</xref>) chosen for their strong performance on object recognition tasks. ResNet-IN was trained on ImageNet (<xref ref-type="bibr" rid="bib74">Russakovsky et al., 2015</xref>), a dataset consisting of high-quality naturalistic photographs, whereas ResNet-SIN was trained on Stylized-ImageNet, a variation on the conventional ImageNet dataset that decorrelates color and texture information from object images using style transfer techniques (<xref ref-type="bibr" rid="bib30">Geirhos et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Huang and Belongie, 2017</xref>). The third model, CorNet-S, is a 5-layer recurrent network explicitly designed to mimic the organization and functional profile of the primate ventral stream (<xref ref-type="bibr" rid="bib44">Kubilius et al., 2019</xref>). It was chosen because it is a biologically plausible model of primate object recognition behavior and neural processing, as measured by the brain-score benchmark (<xref ref-type="bibr" rid="bib75">Schrimpf et al., 2018</xref>). Finally, ResNext-SAY uses an updated version of the ResNet architecture and was designed to approximate the visual processing abilities of an infant (<xref ref-type="bibr" rid="bib60">Orhan et al., 2020</xref>). It was trained using a self-supervised temporal classification method on the SAYCam dataset (<xref ref-type="bibr" rid="bib85">Sullivan et al., 2020</xref>), a large, longitudinal dataset recorded from three infants’ first-person perspectives.</p><p>To test whether infant-looking behaviors could be accounted for by low-level image properties, we also tested a model of pixel similarity and FlowNet, a model of motion flow (<xref ref-type="bibr" rid="bib22">Dosovitskiy et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Ilg et al., 2017</xref>). For the Pixel model, the raw image frame was passed into the evaluation pipeline and classification was based on this information alone. FlowNet estimates the motion energy between adjacent video frames and is able to successfully use motion information to segment objects from the background as well as support action recognition in videos. Here we used an iteration of FlowNet known as FlowNet2-S, which was pre-trained on the MPI-Sintel and ‘flying chairs’ datasets (<xref ref-type="bibr" rid="bib15">Butler et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Mayer et al., 2016</xref>).</p></sec><sec id="s3-5"><title>Model analyses</title><p>Models were evaluated on the same stimulus sets presented to infants and tested using methods similar to the habituation/dishabituation procedure. One way to conceive of this procedure is as a measure of alignment between the stimulus and the infant’s internal representation of the stimulus. Infants will continue looking at a display for as long as they perceive a mismatch, or ‘error’, between the stimulus and their representation. To approximate this process, we converted each model into an autoencoder, which was ‘habituated’ and tested using the same criteria as infants (<xref ref-type="bibr" rid="bib53">Mareschal et al., 2000</xref>; <xref ref-type="bibr" rid="bib90">Westermann and Mareschal, 2004</xref>). An autoencoder is an unsupervised learning model that attempts to recreate the input stimulus using a lower-dimensional set of features than the input. Like infants, the error signal from the output layer of an autoencoder remains high when there is a mismatch between the input stimulus and the internal representation. Each model was converted into an autoencoder by passing the outputs of the model to a single transposed convolutional decoding layer. For ANNs, outputs were extracted from the penultimate layer of the model, ‘AvgPool’ (with ReLu) for each frame of the video. For the Skeletal model, each frame of the video was first binarized and the skeleton extracted. The resulting skeletal image was blurred using a 3-pixel Gaussian kernel and passed into the decoder. For FlowNet, image representations of the flow fields were generated for each pair of adjacent frames before being passed into the decoder. No image preprocessing was conducted for the Pixel model. To match the output dimensions of each ANN, each image from the Skeletal, FlowNet, or Pixel model was passed through a single convolutional feature extraction layer with Max and Average pooling, before being sent to the decoding layer.</p><p>During the habituation phase, models were shown repeated presentations (epochs) of an object. Habituation was accomplished by training the models on each video using the Adam optimizer and a mean squared error loss function. For the ANNs, the weights of the decoding layer were updated during habituation. The weights of the pretrained models were frozen. For the Skeletal, FlowNet, and Pixel models, the weights of both the output and decoding layers were updated to support efficient feature extraction. The Skeletal, FlowNet, or Pixel model backbone was not altered. Models were said to have habituated once the average error signal in the last four epochs was below 50% of the average error in the first four epochs. The Skeletal model and ANNs met the habituation criteria within 8 epochs. Pixel and FlowNet models met the habituation criteria within 22 and 43 trials, respectively. The Skeletal, ANN, and Pixel models showed good reconstruction of the habituated objects (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). Reconstructions using FlowNet were not possible because multiple frames were used as input. At test, models were presented with objects that had the same/different skeletons or the same/different surface forms as the habituated object, and the error signal was recorded. See <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref> for the error signal between all object pairs.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Examples of autoencoder reconstructions using objects from Experiment 1 (top) and Experiment 2 (bottom) for all models except FlowNet.</title><p>FlowNet reconstructions are not possible because it requires multiple frames as input. For the Skeletal model, the inset displays the original input image. Each reconstruction was created by feeding a random frame from the habituation object video to each model immediately following its habituation to said video.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74943-fig4-v1.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Dissimilarity matrices for each computational model in Experiment 1.</title><p>Dissimilarity for each object pair was calculated as the error from an autoencoder following habituation to one object and testing on a second object. Internal values of each cell in the matrix indicate the error between habituation and test objects. Error-values are normalized to the end of habituation. Dissimilarity matrices are asymmetrical because the error value changes depending on which object the model was habituated to. The object adjacent to each row is the habituation object, and the object adjacent to each column is the test object.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74943-fig5-v1.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Dissimilarity matrices for each computational model in Experiment 2.</title><p>Dissimilarity for each object pair was calculated as the error from an autoencoder following habituation to one object, and testing on a second object. Internal values of each cell in the matrix indicate the error between habituation and test objects. Error-values are normalized to the end of habituation. Dissimilarity matrices are asymmetrical because the error value changes depending on which object the model was habituated to. The object adjacent to each row is the habituation object, and the object adjacent to each column is the test object.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74943-fig6-v1.tif"/></fig></sec><sec id="s3-6"><title>Classification score</title><p>For comparison, the performance levels of infants and computational models were converted to a classification score. Organisms’ responses (i.e. looking time/error signal) in the test phase were first normalized to the end of the habituation phase (last 4 trials/epochs) by taking the difference between the two. The response to the novel object was then converted into a proportion by dividing it by the combined response to the novel and familiar test object. For both the models and infants, a classification score of 0.50 reflects chance performance.</p></sec></sec></body><back><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Validation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology, Project administration, Resources, Supervision, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All families gave informed consent according to a protocol approved by the Emory University Institutional Review Board (IRB) under the project 'Spatial Origins' (Study Number IRB0003452).</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-74943-transrepform1-v1.pdf"/></supplementary-material></sec><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>All stimuli and data are available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/4vswu/">https://osf.io/4vswu/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Perception of an object's global shape is best described by a model of skeletal structure in human infants</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/4VSWU</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amir</surname><given-names>O</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Hayworth</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sensitivity to nonaccidental properties across various shape dimensions</article-title><source>Vision Research</source><volume>62</volume><fpage>35</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.03.020</pub-id><pub-id pub-id-type="pmid">22491056</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title><source>eLife</source><volume>6</volume><elocation-id>e26196</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26196</pub-id><pub-id pub-id-type="pmid">28671063</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ardila</surname><given-names>D</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>46th Annual Conference on Information Sciences and Systems (CISS</article-title><source>Princeton</source><volume>1</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/CISS.2012.6310946</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Yousif</surname><given-names>SR</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Skeletal representations of shape in human vision: Evidence for a pruned medial axis model</article-title><source>Journal of Vision</source><volume>19</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1167/19.6.6</pub-id><pub-id pub-id-type="pmid">31173631</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Skeletal descriptions of shape provide unique perceptual information for object recognition</article-title><source>Scientific Reports</source><volume>9</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-45268-y</pub-id><pub-id pub-id-type="pmid">31249321</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Kamps</surname><given-names>FS</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Skeletal representations of shape in the human visual cortex</article-title><source>Neuropsychologia</source><volume>164</volume><elocation-id>108092</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2021.108092</pub-id><pub-id pub-id-type="pmid">34801519</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>N</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Erlikhman</surname><given-names>G</given-names></name><name><surname>Kellman</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep convolutional networks do not classify based on global object shape</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006613</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006613</pub-id><pub-id pub-id-type="pmid">30532273</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>N</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Erlikhman</surname><given-names>G</given-names></name><name><surname>Kellman</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Local features and global shape information in object classification by deep convolutional neural networks</article-title><source>Vision Research</source><volume>172</volume><fpage>46</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2020.04.003</pub-id><pub-id pub-id-type="pmid">32413803</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergelson</surname><given-names>E</given-names></name><name><surname>Swingley</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>At 6-9 months, human infants know the meanings of many common nouns</article-title><source>PNAS</source><volume>109</volume><fpage>3253</fpage><lpage>3258</lpage><pub-id pub-id-type="doi">10.1073/pnas.1113380109</pub-id><pub-id pub-id-type="pmid">22331874</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><pub-id pub-id-type="pmid">3575582</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Gerhardstein</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Recognizing depth-rotated objects: evidence and conditions for three-dimensional viewpoint invariance</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>19</volume><fpage>1162</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.19.6.1162</pub-id><pub-id pub-id-type="pmid">8294886</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Visual object recognition</source><volume>2</volume><publisher-name>MIT press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>One-shot viewpoint invariance in matching novel objects</article-title><source>Vision Research</source><volume>39</volume><fpage>2885</fpage><lpage>2899</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(98)00309-5</pub-id><pub-id pub-id-type="pmid">10492817</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1967">1967</year><chapter-title>A transformation for extracting descriptors of shape</chapter-title><person-group person-group-type="editor"><name><surname>Wathen-Dunn</surname><given-names>W</given-names></name></person-group><source>Models for the Perception of Speech and Visual Form</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>362</fpage><lpage>380</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>DJ</given-names></name><name><surname>Wulff</surname><given-names>J</given-names></name><name><surname>Stanley</surname><given-names>GB</given-names></name><name><surname>Black</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A Naturalistic Open Source Movie for Optical Flow Evaluation</article-title><source>Berlin, Heidelberg</source><volume>7577</volume><fpage>611</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-33783-3_44</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cassia</surname><given-names>VM</given-names></name><name><surname>Simion</surname><given-names>F</given-names></name><name><surname>Milani</surname><given-names>I</given-names></name><name><surname>Umiltà</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dominance of global visual properties at birth</article-title><source>Journal of Experimental Psychology. General</source><volume>131</volume><fpage>398</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.131.3.398</pub-id><pub-id pub-id-type="pmid">12214754</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clerkin</surname><given-names>EM</given-names></name><name><surname>Hart</surname><given-names>E</given-names></name><name><surname>Rehg</surname><given-names>JM</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Real-world visual statistics and infants’ first-learned object names</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160055</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0055</pub-id><pub-id pub-id-type="pmid">27872373</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fast mapping rapidly integrates information into existing memory networks</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>2296</fpage><lpage>2303</lpage><pub-id pub-id-type="doi">10.1037/xge0000020</pub-id><pub-id pub-id-type="pmid">25222265</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craft</surname><given-names>E</given-names></name><name><surname>Schütze</surname><given-names>H</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A neural model of figure-ground organization</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4310</fpage><lpage>4326</lpage><pub-id pub-id-type="doi">10.1152/jn.00203.2007</pub-id><pub-id pub-id-type="pmid">17442769</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davitt</surname><given-names>LI</given-names></name><name><surname>Cristino</surname><given-names>F</given-names></name><name><surname>Wong</surname><given-names>ACN</given-names></name><name><surname>Leek</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Shape information mediating basic- and subordinate-level object recognition revealed by analyses of eye movements</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>40</volume><fpage>451</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1037/a0034983</pub-id><pub-id pub-id-type="pmid">24364701</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dimitrov</surname><given-names>P</given-names></name><name><surname>Damon</surname><given-names>JN</given-names></name><name><surname>Siddiqi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Flux invariants for shape</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003</conf-name></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Ilg</surname><given-names>E</given-names></name><name><surname>Hausser</surname><given-names>P</given-names></name><name><surname>Hazirbas</surname><given-names>C</given-names></name><name><surname>Golkov</surname><given-names>V</given-names></name><name><surname>Smagt</surname><given-names>PVD</given-names></name><name><surname>Cremers</surname><given-names>D</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>FlowNet: Learning Optical Flow with Convolutional Networks</article-title><conf-name>IEEE International Conference on Computer Vision</conf-name><conf-loc>Santiago</conf-loc><pub-id pub-id-type="doi">10.1109/ICCV.2015.316</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Retinotopic Organization of Visual Cortex in Human Infants</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>437</elocation-id><pub-id pub-id-type="doi">10.1101/2020.12.01.407437</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Structure of Perceptual Categories</article-title><source>Journal of Mathematical Psychology</source><volume>41</volume><fpage>145</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1006/jmps.1997.1154</pub-id><pub-id pub-id-type="pmid">9237918</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian estimation of the shape skeleton</article-title><source>PNAS</source><volume>103</volume><fpage>18014</fpage><lpage>18019</lpage><pub-id pub-id-type="doi">10.1073/pnas.0608811103</pub-id><pub-id pub-id-type="pmid">17101989</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name><name><surname>Briscoe</surname><given-names>E</given-names></name><name><surname>Froyen</surname><given-names>V</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Wilder</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>An Integrated Bayesian Approach to Shape Representation and Perceptual Organization</chapter-title><person-group person-group-type="editor"><name><surname>Dickinson</surname><given-names>SJ</given-names></name><name><surname>Pizlo</surname><given-names>Z</given-names></name></person-group><source>Shape Perception in Human and Computer Vision: An Interdisciplinary Perspective</source><publisher-loc>London</publisher-loc><publisher-name>Springer</publisher-name><fpage>55</fpage><lpage>70</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferry</surname><given-names>AL</given-names></name><name><surname>Hespos</surname><given-names>SJ</given-names></name><name><surname>Waxman</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Categorization in 3- and 4-month-old infants: an advantage of words over tones</article-title><source>Child Development</source><volume>81</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8624.2009.01408.x</pub-id><pub-id pub-id-type="pmid">20438453</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Getting “fumpered”: Classifying objects by what has been done to them</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1167/19.4.15</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gant</surname><given-names>JM</given-names></name><name><surname>Banburski</surname><given-names>A</given-names></name><name><surname>Deza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Evaluating the Adversarial Robustness of a Foveated Texture Transform Module in a CNN</article-title><conf-name>NeurIPS 2021 Workshop SVRHM</conf-name><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><name><surname>Rubisch</surname><given-names>P</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>ImageNet-Trained CNNs Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1811.12231</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV</conf-name><conf-loc>Venice</conf-loc><pub-id pub-id-type="doi">10.1109/ICCV.2017.167</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>JE</given-names></name><name><surname>Stankiewicz</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Categorical relations in shape perception</article-title><source>Spatial Vision</source><volume>10</volume><fpage>201</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1163/156856896x00141</pub-id><pub-id pub-id-type="pmid">9061832</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ilg</surname><given-names>E</given-names></name><name><surname>Mayer</surname><given-names>N</given-names></name><name><surname>Saikia</surname><given-names>T</given-names></name><name><surname>Keuper</surname><given-names>M</given-names></name><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Honolulu, HI</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2017.179</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jagadeesh</surname><given-names>AV</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Texture-like Representation of Objects in Human Visual Cortex</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>849</elocation-id><pub-id pub-id-type="doi">10.1101/2022.01.04.474849</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>James</surname><given-names>KH</given-names></name><name><surname>Jones</surname><given-names>SS</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Swain</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Young Children’s Self-Generated Object Views and Object Recognition</article-title><source>Journal of Cognition and Development</source><volume>15</volume><fpage>393</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1080/15248372.2012.749481</pub-id><pub-id pub-id-type="pmid">25368545</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>H</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Convolutional neural networks trained with a developmental sequence of blurry to clear images reveal core differences between face and object processing</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>e6</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.12.6</pub-id><pub-id pub-id-type="pmid">34767621</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kellman</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Perception of three-dimensional form by human infants</article-title><source>Perception &amp; Psychophysics</source><volume>36</volume><fpage>353</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.3758/BF03202789</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kellman</surname><given-names>PJ</given-names></name><name><surname>Short</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Development of three-dimensional form perception</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>13</volume><fpage>545</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.13.4.545</pub-id><pub-id pub-id-type="pmid">2965746</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kellman</surname><given-names>PJ</given-names></name><name><surname>Shipley</surname><given-names>TF</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A theory of visual interpolation in object perception</article-title><source>Cognitive Psychology</source><volume>23</volume><fpage>141</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(91)90009-d</pub-id><pub-id pub-id-type="pmid">2055000</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kellman</surname><given-names>PJ</given-names></name><name><surname>Arterberry</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>Infant Visual Perception</chapter-title><person-group person-group-type="editor"><name><surname>Kuhn</surname><given-names>D</given-names></name><name><surname>Siegler</surname><given-names>RS</given-names></name><name><surname>Damon</surname><given-names>W</given-names></name><name><surname>Lerner</surname><given-names>RM</given-names></name></person-group><source>Handbook of Child Psychology: Cognition, Perception, and Language</source><publisher-name>John Wiley &amp; Sons Inc</publisher-name><fpage>109</fpage><lpage>160</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiat</surname><given-names>JE</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Beckner</surname><given-names>AG</given-names></name><name><surname>Hayes</surname><given-names>TR</given-names></name><name><surname>Pomaranski</surname><given-names>KI</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Oakes</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Linking patterns of infant eye movements to a neural network model of the ventral stream using representational similarity analysis</article-title><source>Developmental Science</source><volume>25</volume><elocation-id>e13155</elocation-id><pub-id pub-id-type="doi">10.1111/desc.13155</pub-id><pub-id pub-id-type="pmid">34240787</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Communications of the ACM</source><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>N</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs</article-title><conf-name>Advances in Neural Information Processing Systems 32 (NeurIPS 2019</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>B</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Tenenbaum</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>One shot learning of simple visual concepts</article-title><conf-name>Proceedings of the annual meeting of the cognitive science society</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level concept learning through probabilistic program induction</article-title><source>Science (New York, N.Y.)</source><volume>350</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1126/science.aab3050</pub-id><pub-id pub-id-type="pmid">26659050</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Piantadosi</surname><given-names>ST</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>People Infer Recursive Visual Concepts from Just a Few Examples</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>54</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1007/s42113-019-00053-y</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Jones</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The importance of shape in early lexical learning</article-title><source>Cognitive Development</source><volume>3</volume><fpage>299</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1016/0885-2014(88)90014-7</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>L</given-names></name><name><surname>Jones</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Object perception and object naming in early development</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(97)01111-x</pub-id><pub-id pub-id-type="pmid">21244958</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Gujarathi</surname><given-names>P</given-names></name><name><surname>Wood</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Controlled-Rearing Studies of Newborn Chicks and Deep Neural Networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2112.06106</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical representation of medial axis structure</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>23</volume><fpage>629</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs046</pub-id><pub-id pub-id-type="pmid">22387761</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>CP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>PNAS</source><volume>115</volume><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id><pub-id pub-id-type="pmid">30171168</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mareschal</surname><given-names>D</given-names></name><name><surname>French</surname><given-names>RM</given-names></name><name><surname>Quinn</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A connectionist account of asymmetric category learning in early infancy</article-title><source>Developmental Psychology</source><volume>36</volume><fpage>635</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1037/0012-1649.36.5.635</pub-id><pub-id pub-id-type="pmid">10976603</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margalit</surname><given-names>E</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Herald</surname><given-names>SB</given-names></name><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>von der Malsburg</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An applet for the Gabor similarity scaling of the differences between complex stimuli</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>78</volume><fpage>2298</fpage><lpage>2306</lpage><pub-id pub-id-type="doi">10.3758/s13414-016-1191-7</pub-id><pub-id pub-id-type="pmid">27557818</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mayer</surname><given-names>N</given-names></name><name><surname>Ilg</surname><given-names>E</given-names></name><name><surname>Hausser</surname><given-names>P</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Cremers</surname><given-names>D</given-names></name><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.438</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mervis</surname><given-names>CB</given-names></name><name><surname>Rosch</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Categorization of Natural Objects</article-title><source>Annual Review of Psychology</source><volume>32</volume><fpage>89</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1146/annurev.ps.32.020181.000513</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgenstern</surname><given-names>Y</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>One-shot categorization of novel object classes in humans</article-title><source>Vision Research</source><volume>165</volume><fpage>98</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2019.09.005</pub-id><pub-id pub-id-type="pmid">31707254</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oakes</surname><given-names>LM</given-names></name><name><surname>Spalding</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The role of exemplar distribution in infants’ differentiation of categories</article-title><source>Infant Behavior and Development</source><volume>20</volume><fpage>457</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1016/S0163-6383(97)90036-9</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ons</surname><given-names>B</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Generalization of Visual Shapes by Flexible and Simple Rules</article-title><source>Seeing and Perceiving</source><volume>25</volume><fpage>237</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1163/187847511X571519</pub-id><pub-id pub-id-type="pmid">21771394</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Orhan</surname><given-names>EA</given-names></name><name><surname>Gupta</surname><given-names>PV</given-names></name><name><surname>Lake</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Self-Supervised Learning through the Eyes of a Child</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2007.16189</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostrovsky</surname><given-names>Y</given-names></name><name><surname>Meyers</surname><given-names>E</given-names></name><name><surname>Ganesh</surname><given-names>S</given-names></name><name><surname>Mathur</surname><given-names>U</given-names></name><name><surname>Sinha</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual parsing after recovery from blindness</article-title><source>Psychological Science</source><volume>20</volume><fpage>1484</fpage><lpage>1491</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02471.x</pub-id><pub-id pub-id-type="pmid">19891751</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>PC</given-names></name><name><surname>Eimas</surname><given-names>PD</given-names></name><name><surname>Rosenkrantz</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Evidence for representations of perceptually similar natural categories by 3-month-old and 4-month-old infants</article-title><source>Perception</source><volume>22</volume><fpage>463</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1068/p220463</pub-id><pub-id pub-id-type="pmid">8378134</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>PC</given-names></name><name><surname>Eimas</surname><given-names>PD</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2001">2001a</year><article-title>Perceptual categorization of cat and dog silhouettes by 3- to 4-month-old infants</article-title><source>Journal of Experimental Child Psychology</source><volume>79</volume><fpage>78</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1006/jecp.2000.2609</pub-id><pub-id pub-id-type="pmid">11292312</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>PC</given-names></name><name><surname>Slater</surname><given-names>AM</given-names></name><name><surname>Brown</surname><given-names>E</given-names></name><name><surname>Hayes</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2001">2001b</year><article-title>Developmental change in form categorization in early infancy</article-title><source>British Journal of Developmental Psychology</source><volume>19</volume><fpage>207</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1348/026151001166038</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>PC</given-names></name><name><surname>Bhatt</surname><given-names>RS</given-names></name><name><surname>Brush</surname><given-names>D</given-names></name><name><surname>Grimes</surname><given-names>A</given-names></name><name><surname>Sharpnack</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Development of form similarity as a Gestalt grouping principle in infancy</article-title><source>Psychological Science</source><volume>13</volume><fpage>320</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00459</pub-id><pub-id pub-id-type="pmid">12137134</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Large-Scale, High-Resolution Comparison of the Core Visual Object Recognition Behavior of Humans, Monkeys, and State-of-the-Art Deep Artificial Neural Networks</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7255</fpage><lpage>7269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0388-18.2018</pub-id><pub-id pub-id-type="pmid">30006365</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rakison</surname><given-names>DH</given-names></name><name><surname>Butterworth</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Infants’ attention to object structure in early categorization</article-title><source>Developmental Psychology</source><volume>34</volume><fpage>1310</fpage><lpage>1325</lpage><pub-id pub-id-type="doi">10.1037//0012-1649.34.6.1310</pub-id><pub-id pub-id-type="pmid">9823514</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rezanejad</surname><given-names>M</given-names></name><name><surname>Siddiqi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Flux graphs for 2D shape analysis</chapter-title><source>Advances in Computer Vision and Pattern Recognition</source><publisher-name>Springer</publisher-name><fpage>41</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1007/978-1-4471-5195-1_3</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ritter</surname><given-names>S</given-names></name><name><surname>Barrett</surname><given-names>DG</given-names></name><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cognitive psychology for deep neural networks: A shape bias case study</article-title><conf-name>Proceedings of the 34 th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosch</surname><given-names>E</given-names></name><name><surname>Mervis</surname><given-names>CB</given-names></name><name><surname>Gray</surname><given-names>WD</given-names></name><name><surname>Johnson</surname><given-names>DM</given-names></name><name><surname>Boyes-Braem</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Basic objects in natural categories</article-title><source>Cognitive Psychology</source><volume>8</volume><fpage>382</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(76)90013-X</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname><given-names>JN</given-names></name><name><surname>Speckman</surname><given-names>PL</given-names></name><name><surname>Sun</surname><given-names>D</given-names></name><name><surname>Morey</surname><given-names>RD</given-names></name><name><surname>Iverson</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian t tests for accepting and rejecting the null hypothesis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>225</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.2.225</pub-id><pub-id pub-id-type="pmid">19293088</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname><given-names>JN</given-names></name><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparing Exemplar- and Rule-Based Theories of Categorization</article-title><source>Current Directions in Psychological Science</source><volume>15</volume><fpage>9</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1111/j.0963-7214.2006.00397.x</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname><given-names>JS</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Leveraging Prior Concept Learning Improves Generalization From Few Examples in Computational Models of Human Object Recognition</article-title><source>Frontiers in Computational Neuroscience</source><volume>14</volume><elocation-id>586671</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2020.586671</pub-id><pub-id pub-id-type="pmid">33510629</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?</article-title><source>Neuroscience</source><volume>1</volume><elocation-id>e7</elocation-id><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Toward a universal law of generalization for psychological science</article-title><source>Science (New York, N.Y.)</source><volume>237</volume><fpage>1317</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1126/science.3629243</pub-id><pub-id pub-id-type="pmid">3629243</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slater</surname><given-names>A</given-names></name><name><surname>Morison</surname><given-names>V</given-names></name><name><surname>Rose</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Perception of shape by the new-born baby</article-title><source>British Journal of Developmental Psychology</source><volume>1</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1111/j.2044-835X.1983.tb00551.x</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slater</surname><given-names>A</given-names></name><name><surname>Morison</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Shape constancy and slant perception at birth</article-title><source>Perception</source><volume>14</volume><fpage>337</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1068/p140337</pub-id><pub-id pub-id-type="pmid">4088795</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slone</surname><given-names>LK</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Self-generated variability in object images predicts vocabulary growth</article-title><source>Developmental Science</source><volume>22</volume><elocation-id>e12816</elocation-id><pub-id pub-id-type="doi">10.1111/desc.12816</pub-id><pub-id pub-id-type="pmid">30770597</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sloutsky</surname><given-names>VM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The role of similarity in the development of categorization</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>246</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00109-8</pub-id><pub-id pub-id-type="pmid">12804690</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Jones</surname><given-names>SS</given-names></name><name><surname>Landau</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Naming in young children: A dumb attentional mechanism?</article-title><source>Cognition</source><volume>60</volume><fpage>143</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(96)00709-3</pub-id><pub-id pub-id-type="pmid">8811743</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Jones</surname><given-names>SS</given-names></name><name><surname>Landau</surname><given-names>B</given-names></name><name><surname>Gershkoff-Stowe</surname><given-names>L</given-names></name><name><surname>Samuelson</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Object name learning provides on-the-job training for attention</article-title><source>Psychological Science</source><volume>13</volume><fpage>13</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00403</pub-id><pub-id pub-id-type="pmid">11892773</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spriet</surname><given-names>C</given-names></name><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Hochmann</surname><given-names>JR</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual object categorization in infancy</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2105866119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2105866119</pub-id><pub-id pub-id-type="pmid">35169072</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spröte</surname><given-names>P</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual perception of shape altered by inferred causal history</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/srep36245</pub-id><pub-id pub-id-type="pmid">27824094</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sullivan</surname><given-names>J</given-names></name><name><surname>Mei</surname><given-names>M</given-names></name><name><surname>Perfors</surname><given-names>A</given-names></name><name><surname>Wojcik</surname><given-names>EH</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded from the Infant’s Perspective</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/fy8zx</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Image-based object recognition in man, monkey and machine</article-title><source>Cognition</source><volume>67</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(98)00026-2</pub-id><pub-id pub-id-type="pmid">9735534</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tartaglini</surname><given-names>AR</given-names></name><name><surname>Vong</surname><given-names>WK</given-names></name><name><surname>Lake</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2202.08340</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turati</surname><given-names>C</given-names></name><name><surname>Simion</surname><given-names>F</given-names></name><name><surname>Zanon</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Newborns’ Perceptual Categorization for Closed and Open Geometric Forms</article-title><source>Infancy</source><volume>4</volume><fpage>309</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1207/S15327078IN0403_01</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogelsang</surname><given-names>L</given-names></name><name><surname>Gilad-Gutnick</surname><given-names>S</given-names></name><name><surname>Ehrenberg</surname><given-names>E</given-names></name><name><surname>Yonas</surname><given-names>A</given-names></name><name><surname>Diamond</surname><given-names>S</given-names></name><name><surname>Held</surname><given-names>R</given-names></name><name><surname>Sinha</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Potential downside of high initial visual acuity</article-title><source>PNAS</source><volume>115</volume><fpage>11333</fpage><lpage>11338</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800901115</pub-id><pub-id pub-id-type="pmid">30322940</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westermann</surname><given-names>G</given-names></name><name><surname>Mareschal</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>From Parts to Wholes: Mechanisms of Development in Infant Visual Object Processing</article-title><source>Infancy</source><volume>5</volume><fpage>131</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1207/s15327078in0502_2</pub-id><pub-id pub-id-type="pmid">33401785</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesel</surname><given-names>TN</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Ordered arrangement of orientation columns in monkeys lacking visual experience</article-title><source>The Journal of Comparative Neurology</source><volume>158</volume><fpage>307</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1002/cne.901580306</pub-id><pub-id pub-id-type="pmid">4215829</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Superordinate shape classification using natural shape statistics</article-title><source>Cognition</source><volume>119</volume><fpage>325</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2011.01.009</pub-id><pub-id pub-id-type="pmid">21440250</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Rezanejad</surname><given-names>M</given-names></name><name><surname>Dickinson</surname><given-names>S</given-names></name><name><surname>Siddiqi</surname><given-names>K</given-names></name><name><surname>Jepson</surname><given-names>A</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local contour symmetry facilitates scene categorization</article-title><source>Cognition</source><volume>182</volume><fpage>307</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.09.014</pub-id><pub-id pub-id-type="pmid">30415132</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>JN</given-names></name><name><surname>Wood</surname><given-names>SMW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Development of Invariant Object Recognition Requires Visual Experience With Temporally Smooth Objects</article-title><source>Cognitive Science</source><volume>42</volume><fpage>1391</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1111/cogs.12595</pub-id><pub-id pub-id-type="pmid">29537108</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Moeskops</surname><given-names>M</given-names></name><name><surname>Kayhan</surname><given-names>E</given-names></name><name><surname>Kliesch</surname><given-names>C</given-names></name><name><surname>Turtleton</surname><given-names>B</given-names></name><name><surname>Köster</surname><given-names>M</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual Category Representations in the Infant Brain</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.03.466293</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>F</given-names></name><name><surname>Cote</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Labeling guides object individuation in 12-month-old infants</article-title><source>Psychological Science</source><volume>16</volume><fpage>372</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.01543.x</pub-id><pub-id pub-id-type="pmid">15869696</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>F</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Infants Are Rational Constructivist Learners</article-title><source>Current Directions in Psychological Science</source><volume>22</volume><fpage>28</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1177/0963721412469396</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yermolayeva</surname><given-names>Y</given-names></name><name><surname>Rakison</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Connectionist modeling of developmental changes in infancy: approaches, challenges, and contributions</article-title><source>Psychological Bulletin</source><volume>140</volume><fpage>224</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1037/a0032150</pub-id><pub-id pub-id-type="pmid">23477448</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Younger</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Infants’ detection of correlations among feature categories</article-title><source>Child Development</source><volume>61</volume><fpage>614</fpage><lpage>620</lpage><pub-id pub-id-type="pmid">2364738</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74943.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><p>This well-conducted study uses relatively large sample sizes, comprehensive statistical testing, and state-of-the-art modeling to provide novel evidence that human infants generalize shape from single examples on the basis of the &quot;shape skeleton&quot;, a structural description of the part structure of the shape. It will be of interest to researchers working on object shape processing and on the development of visual perception.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74943.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting the paper &quot;The shape skeleton supports one-shot categorization in human infants: Behavioral and computational evidence&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by a Senior Editor. The reviewers have opted to remain anonymous.</p><p>We are sorry to say that, after consultation with the reviewers, we have decided that this work will not be considered further for publication by <italic>eLife</italic>.</p><p>Specifically, the reviewers thought that results may be alternatively explained by motion similarity and/or low-level visual similarity between habituation and test stimuli, as explained in more detail below.</p><p><italic>Reviewer #1:</italic></p><p>This is a very lucidly written article on a fascinating and important topic: how humans are able to learn novel visual categories based on few or even just one single example. This ability can be contrasted with conventional machine learning models which typically require massive data sets with thousands of training examples to learn the ability to categorize novel examples accurately. How humans generalize so well from such sparse training data remains poorly understood and working out how we achieve this is important not only for the psychological sciences, but also has implications for artificial intelligence and machine learning too.</p><p>The authors start with a strong premise, which is also likely to be true: a crucial aspect of human one-shot visual learning likely involves the perceptual processes that segment observed shapes into parts and represent them as a hierarchy of limbs, in a representation known in the field as 'shape skeletons'. There is a long history of evidence suggesting the human visual system analyses shape in this way, and such representations naturally lend themselves to abstractions and generalizations that are robust against significant variations in appearance, such as the surface structure and even pose of objects. While this is an excellent starting point for investigating one-shot learning, the idea that such representations might play a role in human visual categorization more generally is rather well accepted in the field, and thus perhaps not so original on its own. The potential novelty and importance of the contribution therefore rests on demonstrating that such representations are central to one-shot learning in particular.</p><p>Unfortunately, due to their choice of stimuli, I believe that the experiments the authors perform do not yet provide compelling evidence that the infants' looking times are driven by skeletal representations, or indeed whether one-shot learning is necessarily playing a role in their habituation.</p><p>The problem is that the different Surface Forms of each Skeleton are more similar to one another than those of the other Skeletons, in terms of the raw pixel similarity. I took screenshots of the stimuli from the MS and compared them in MATLAB. For 7 out of 10 possible comparisons, the corresponding Surface Forms from the same 'category' are closer in raw pixel terms than to any of the rivals. This leads me to believe that the pattern of results is based on the raw physical (and thus perceptual) similarity between the habituation and test stimuli, rather than based on one-shot generalization per se, or on skeletal representations of the shapes in particular. In my opinion, the fact that sophisticated artificial neural network models don't predict this is not in itself strong evidence against my suggested explanation of the findings.</p><p>In future studies, this issue could be addressed by using stimuli for which this confound is not a problem. This could be achieved, for example, by using additional transformations of the objects, such as rotations, that preserve part structure but radically alter the projected image of the objects. This way skeletal structure could be decoupled from straightforward image similarity, and a stronger case for generalization beyond the 'one-shot' training exemplar would be provided.</p><p><italic>Reviewer #2:</italic></p><p>This is an important paper. The issue of &quot;one-shot&quot; learning---how people can learn categories and concepts from a single example or set of examples---lies at the heart of the current controversies over &quot;deep learning&quot; models. These models are ubiquitous these days and are often promoted as descriptive models of human learning, but they inherently require many trials to learn. But human learners can often learn from single trials, presenting a fundamental challenge to the entire deep learning paradigm. This has been pointed out in broad terms before, but to really advance this debate, a study needs to establish that humans can indeed learn from single examples, simultaneously demonstrate that appropriate deep learning models cannot, and explain something about exactly what why---all of which this paper accomplishes with impressive rigor.</p><p>In that context, the paper specifically studies shape category learning by human infants, a particularly interesting case, and establishes a specific pattern: that infants generalize shape categories based on the &quot;shape skeleton,&quot; a structural description of the part structure of the shape. That is, infants exposed to a single shape with a particular shape skeleton tend to be &quot;unsurprised&quot; by other shapes with the same skeleton, but more surprised by shapes with different skeletons, indicating that the single example was enough to establish in their minds an apparent shape category.</p><p>This generalization is in a sense unsurprising, because the shape skeleton is supposed to define characteristic &quot;invariants&quot; within shape categories, but it is nevertheless novel. Although the tendency for infants to generalize lexical categories based on shape is well established, the precise nature of one-shot shape generalizations has not previously been studied. For researchers in shape this is a very important result, and for anybody interested in how humans learn categories I think is is both fundamental and thought-provoking.</p><p>I found the methodology and statistical analysis in the paper comprehensive and rigorous, and the writing clear, so and have only relatively minor comments on the manuscript, which follow. I don't see page numbers, so give quotations to indicate the relevant location.</p><p>– &quot;a phenomenon known as 'one-shot categorization'&quot; – The issue of one-shot category learning, and the computational question of what makes it possible for human learners to instantly generalize from some examples but not others, has been studied somewhat more extensively than this very brief introduction lets on. I would point to for example Feldman (1997, J. Math Psych) which explicitly takes it up and argues for a mechanism related to the current paper (namely, that one-shot categorization is possible when the one example implies a highly specific structural model).</p><p>– &quot;However, one might ask whether these findings are truly indicative of categorization, or simply better discrimination of objects with the different skeletons.&quot; I'm not sure these are really different explanations. Learners are better at discriminating objects that appear to be from different categories (categorical perception, etc.).</p><p>– &quot;infants' looking times on the first test trial did not differ for within- and between-category test objects&quot; – This claim is followed by a non-significant NHST test, which does not allow an affirmative conclusion of no difference here, and a Bayes Factor, which does. The NHST test really doesn't add anything. I personally think these tests could be omitted throughout the paper in favor of the more informative BFs – but particularly when null results are discussed.</p><p>– members from different -&gt; members of different.</p><p>– &quot;we tested models by feeding their outputs into an autoencoder and measuring the error signal across habituation and test phases (see Methods).&quot; I didn't quite get this. To evaluate similarity within the network models, can't one use a Euclidean norm or a cosine? Please clarify.</p><p>– &quot;but did not differ from one another&quot; – Meaning what? Low BF? If so, please give BF.</p><p>– &quot;it has remained unknown whether one-shot categorization is possible within the first year of human life.&quot; Has this really never been established, even for simple categories? Anecdotally, infants seem to do one-shot learning all the time.</p><p>– &quot;Moreover, V2 and V3 are evolutionarily preserved in primate and non-primate animals&quot; I think the entire discussion of neural analogs here is misleading. I am not a bird expert but I didn't think birds have homologous visual cortical areas to primates. But that doesn't matter, because functional organization is analogous when computational problems are analogous. In other words, birds don't generalize the same way we do because they have V2 areas like us, but because they are solving problems like us.</p><p>– &quot;set was comprised of two…&quot; -&gt; &quot;set comprised two…&quot; Use of &quot;comprised of&quot; to mean &quot;composed of&quot; is colloquial. The US comprises states, not the other way around.</p><p><italic>Reviewer #3:</italic></p><p>This study relates habituation in infants to categorization in neural networks, showing that infants learn (as evidenced by looking times) shape skeletons across surface form, while neural networks that lack an explicit skeletal representation do not show this generalization. A key aspect of the study is that infants are only exposed to one exemplar, suggesting that infants learn shape skeletons using &quot;one-shot categorization&quot;. The study is well-conducted, using relatively large sample sizes, comprehensive statistical testing, and careful modelling. The manuscript is well-written and easy to follow. However, results may be alternatively explained by motion similarity and/or low-level visual similarity between habituation and test stimuli.</p><p>– The shapes are shown as videos during both habituation and test phases. While I understand that this was preferred for drawing the infants' attention, it complicates the interpretation of the results. First, it becomes hard to disentangle the learning of the shape skeleton from the learning of the motion trajectory. Second, the comparison with neural networks becomes more difficult as these neural networks are not sensitive to motion.</p><p>– Because surface form differed for both test objects, the same-skeleton object will be visually more similar to the habituated object than the different-skeleton object. Therefore, it cannot be ruled out that results reflect habituation to lower-level stimulus properties rather than shape skeleton. This is also suggested by recent fMRI results using these stimuli (Ayzenberg et al., Neuropsychologia 2022), showing that the skeletal model correlates with activity patterns throughout the visual system, including V1 (though the cross-surface form results are only shown for V3 and LO, as far as I could tell).</p><p>I would recommend the authors to include a discussion of the possible contributions of motion and non-skeletal visual properties to the habituation results.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for choosing to send your work entitled &quot;The shape skeleton supports one-shot categorization in human infants: Behavioral and computational evidence&quot; for consideration at <italic>eLife</italic>. Your letter of appeal has been considered by a Senior Editor and a Reviewing Editor, and we are prepared to consider a revised submission with no guarantees of acceptance.</p><p>In addition to the comments of the previous reviews, during the consultation of the appeal, the following questions came up:</p><p>1. What new insight do we gain from the infant study above and beyond what we already know from adults?</p><p>2. Why do you think that (dis)habitation is evidence of learning per se, rather than comparisons of perceptual similarity between items?</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;The shape skeleton supports one-shot categorization in human infants&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Floris de Lange (Senior Editor) and a Reviewing Editor, in consultation with reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The reviewers were not convinced that the experiments convincingly demonstrate “one-shot” categorization. This concern can be addressed by adjusting the headline claim throughout (title, abstract, intro, results and discussion would need some modifications). A suggested alternative title could be: &quot;The shape skeleton supports shape similarity judgments in human infants&quot;.</p><p>The design involves making a judgment about which objects appear to be more similar. This requires comparing the distances between presented items (in some feature space describing the stimuli). Making such a comparison doesn't involve learning anything on the basis of the experience. It doesn't involve generalization and there is no sense in which it is a 'one-shot' task, except that a given trial presents only a few items. But this is true of practically ANY experiment involving comparing a small number of items.</p><p>Here is an analogy: suppose the infants were shown three different (i.e., easily discriminable) patches of gray shades: two patches are relatively light shades of gray and one is a significantly darker shade. A habituation experiment like the one the authors performed would reveal that the infants see the two light grays as more similar than the darker one. But in what sense is this 'one shot categorization'? The infants wouldn't have learned a new category. There is no meaningful generalization. The experiment simply reflects the fact that similar grays look more similar than more different ones.</p><p>The same is true in the authors' experiments. The experiments demonstrate that objects with more similar skeletons appear more similar to infants. This is not a trivial result, and is worthy of publication in its own right (although similar findings have already been shown in adults). However not under the title of 'one-shot categorization'. Instead, it should be pitched (correctly) as the shape skeleton contributing substantially to judgments of shape similarity.</p><p>To reiterate: I think the authors have performed an elegant study with some interesting findings. I think their interpretation can certainly be discussed in the MS. However, the headline claim about one-shot categorization should be toned down. Otherwise, the term would 'one-shot' would not mean anything anymore.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74943.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: The authors appealed the original decision. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This is a very lucidly written article on a fascinating and important topic: how humans are able to learn novel visual categories based on few or even just one single example. This ability can be contrasted with conventional machine learning models which typically require massive data sets with thousands of training examples to learn the ability to categorize novel examples accurately. How humans generalize so well from such sparse training data remains poorly understood and working out how we achieve this is important not only for the psychological sciences, but also has implications for artificial intelligence and machine learning too.</p><p>The authors start with a strong premise, which is also likely to be true: a crucial aspect of human one-shot visual learning likely involves the perceptual processes that segment observed shapes into parts and represent them as a hierarchy of limbs, in a representation known in the field as 'shape skeletons'. There is a long history of evidence suggesting the human visual system analyses shape in this way, and such representations naturally lend themselves to abstractions and generalizations that are robust against significant variations in appearance, such as the surface structure and even pose of objects. While this is an excellent starting point for investigating one-shot learning, the idea that such representations might play a role in human visual categorization more generally is rather well accepted in the field, and thus perhaps not so original on its own. The potential novelty and importance of the contribution therefore rests on demonstrating that such representations are central to one-shot learning in particular.</p><p>Unfortunately, due to their choice of stimuli, I believe that the experiments the authors perform do not yet provide compelling evidence that the infants' looking times are driven by skeletal representations, or indeed whether one-shot learning is necessarily playing a role in their habituation.</p><p>The problem is that the different Surface Forms of each Skeleton are more similar to one another than those of the other Skeletons, in terms of the raw pixel similarity. I took screenshots of the stimuli from the MS and compared them in MATLAB. For 7 out of 10 possible comparisons, the corresponding Surface Forms from the same 'category' are closer in raw pixel terms than to any of the rivals. This leads me to believe that the pattern of results is based on the raw physical (and thus perceptual) similarity between the habituation and test stimuli, rather than based on one-shot generalization per se, or on skeletal representations of the shapes in particular. In my opinion, the fact that sophisticated artificial neural network models don't predict this is not in itself strong evidence against my suggested explanation of the findings.</p></disp-quote><p>We thank the reviewer for raising this concern. It is indeed the case that the test objects with the familiar skeleton exhibited greater image-level similarity to the habituation object than the test objects with the novel skeleton. Importantly, however, greater numerical similarity does not guarantee greater perceptual similarity, as raw pixel differences could be indiscriminable to humans. Thus, as in adult psychophysics, we ensured that both test objects were equally discriminable from the habituation object by infants. As discussed on pages 6 and 8, infants discriminated both test objects from the habituation object. To address this concern further, we also compared infants to a model based on pixel similarity (see pages 6-9, 18-20). Although the Pixel model performed above chance in both experiments, we found that infants (and the Skeletal model) outperformed the pixel model in both cases (see pages 7-9). Thus, despite some evidence that pixel similarity may allow for categorization, it is unlikely that this alone accounts for infants’ performance. Nevertheless, we now discuss the potential role of image-level similarity (and motion trajectory) to object categorization in the General Discussion (see page 11-12).</p><disp-quote content-type="editor-comment"><p>In future studies, this issue could be addressed by using stimuli for which this confound is not a problem. This could be achieved, for example, by using additional transformations of the objects, such as rotations, that preserve part structure but radically alter the projected image of the objects. This way skeletal structure could be decoupled from straightforward image similarity, and a stronger case for generalization beyond the 'one-shot' training exemplar would be provided.</p></disp-quote><p>We have provided greater justification for the claim that infants’ performance was based on the shape skeleton, rather than image similarity alone. Nevertheless, we agree with the reviewer that future research would benefit from the suggested manipulations.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>This is an important paper. The issue of &quot;one-shot&quot; learning---how people can learn categories and concepts from a single example or set of examples---lies at the heart of the current controversies over &quot;deep learning&quot; models. These models are ubiquitous these days and are often promoted as descriptive models of human learning, but they inherently require many trials to learn. But human learners can often learn from single trials, presenting a fundamental challenge to the entire deep learning paradigm. This has been pointed out in broad terms before, but to really advance this debate, a study needs to establish that humans can indeed learn from single examples, simultaneously demonstrate that appropriate deep learning models cannot, and explain something about exactly what why---all of which this paper accomplishes with impressive rigor.</p><p>In that context, the paper specifically studies shape category learning by human infants, a particularly interesting case, and establishes a specific pattern: that infants generalize shape categories based on the &quot;shape skeleton,&quot; a structural description of the part structure of the shape. That is, infants exposed to a single shape with a particular shape skeleton tend to be &quot;unsurprised&quot; by other shapes with the same skeleton, but more surprised by shapes with different skeletons, indicating that the single example was enough to establish in their minds an apparent shape category.</p><p>This generalization is in a sense unsurprising, because the shape skeleton is supposed to define characteristic &quot;invariants&quot; within shape categories, but it is nevertheless novel. Although the tendency for infants to generalize lexical categories based on shape is well established, the precise nature of one-shot shape generalizations has not previously been studied. For researchers in shape this is a very important result, and for anybody interested in how humans learn categories I think is is both fundamental and thought-provoking.</p><p>I found the methodology and statistical analysis in the paper comprehensive and rigorous, and the writing clear, so and have only relatively minor comments on the manuscript, which follow. I don't see page numbers, so give quotations to indicate the relevant location.</p><p>– &quot;a phenomenon known as 'one-shot categorization'&quot; – The issue of one-shot category learning, and the computational question of what makes it possible for human learners to instantly generalize from some examples but not others, has been studied somewhat more extensively than this very brief introduction lets on. I would point to for example Feldman (1997, J. Math Psych) which explicitly takes it up and argues for a mechanism related to the current paper (namely, that one-shot categorization is possible when the one example implies a highly specific structural model).</p></disp-quote><p>We thank the reviewer for this suggestion. In the general discussion, we now situate our results within the broader one-shot categorization literature and have included the suggested paper (see pages 12-13).</p><disp-quote content-type="editor-comment"><p>– &quot;However, one might ask whether these findings are truly indicative of categorization, or simply better discrimination of objects with the different skeletons.&quot; I'm not sure these are really different explanations. Learners are better at discriminating objects that appear to be from different categories (categorical perception, etc.).</p></disp-quote><p>We agree with the reviewer that visual similarity plays an important role in categorization. However, an important prerequisite of categorization is that exemplars within a category are distinct from one another, yet grouped into the same category. Thus, and in response to concerns about image similarity, we provide evidence that the test objects were comparably discriminable from the habituation object (see pages 6 and 8).</p><disp-quote content-type="editor-comment"><p>– &quot;infants' looking times on the first test trial did not differ for within- and between-category test objects&quot; – This claim is followed by a non-significant NHST test, which does not allow an affirmative conclusion of no difference here, and a Bayes Factor, which does. The NHST test really doesn't add anything. I personally think these tests could be omitted throughout the paper in favor of the more informative BFs – but particularly when null results are discussed.</p></disp-quote><p>Though we appreciate the reviewer’s suggestion, we have opted to continue to include NHST to ensure accessibility to a larger community, who may be less familiar with Bayes factors. Importantly, though, all of the statistics converge on the same results.</p><disp-quote content-type="editor-comment"><p>– members from different -&gt; members of different.</p></disp-quote><p>Thank you. We have made this correction (see pages 6 and 8).</p><disp-quote content-type="editor-comment"><p>– “we tested models by feeding their outputs into an autoencoder and measuring the error signal across habituation and test phases (see Methods).” I didn’t quite get this. To evaluate similarity within the network models, can’t one use a Euclidean norm or a cosine? Please clarify.</p></disp-quote><p>In the current study, we chose to use an autoencoder approach because it allowed us to better match evaluation of the models to infants (for review, see Yermolayeva &amp; Rakison, 2014 – <italic>Psychological Bulletin</italic>). Specifically, unlike conventional measures of classification, autoencoders allow for measuring model categorization following exposure to just one exemplar, rather than abelled contrasting examples (e.g., supervised classifiers). Moreover, like infant learning during habituation, the learned representation of an autoencoder reflects the entire habituation video, rather than the similarity between individual frames, as might be measured by Euclidean or cosine measures. Most importantly, unlike other techniques, autoencoders can be tested using the same criteria as infants. We have provided further justification for the use of autoencoders on pages 6-7.</p><disp-quote content-type="editor-comment"><p>– “but did not differ from one another” – Meaning what? Low BF? If so, please give BF.</p></disp-quote><p>The lack of a difference is based on overlapping confidence intervals (see page 7).</p><disp-quote content-type="editor-comment"><p>– “it has remained unknown whether one-shot categorization is possible within the first year of human life.” Has this really never been established, even for simple categories? Anecdotally, infants seem to do one-shot learning all the time.</p></disp-quote><p>To the best of our knowledge, this is the first study to test one-shot categorization in human infants. A predominant perspective in developmental psychology is that one-shot categorization on the basis of shape is possible at around 4 years of age following more extensive linguistic and object experience (see page 3 and 12).</p><disp-quote content-type="editor-comment"><p>– “Moreover, V2 and V3 are evolutionarily preserved in primate and non-primate animals” I think the entire discussion of neural analogs here is misleading. I am not a bird expert, but I didn't think birds have homologous visual cortical areas to primates. But that doesn't matter, because functional organization is analogous when computational problems are analogous. In other words, birds don't generalize the same way we do because they have V2 areas like us, but because they are solving problems like us.</p></disp-quote><p>We have removed the aforementioned section from the General Discussion.</p><disp-quote content-type="editor-comment"><p>– &quot;set was comprised of two…&quot; -&gt; &quot;set comprised two…&quot; Use of &quot;comprised of&quot; to mean &quot;composed of&quot; is colloquial. The US comprises states, not the other way around.</p></disp-quote><p>We have made this correction (see page 13).</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This study relates habituation in infants to categorization in neural networks, showing that infants learn (as evidenced by looking times) shape skeletons across surface form, while neural networks that lack an explicit skeletal representation do not show this generalization. A key aspect of the study is that infants are only exposed to one exemplar, suggesting that infants learn shape skeletons using &quot;one-shot categorization&quot;. The study is well-conducted, using relatively large sample sizes, comprehensive statistical testing, and careful modelling. The manuscript is well-written and easy to follow. However, results may be alternatively explained by motion similarity and/or low-level visual similarity between habituation and test stimuli.</p><p>– The shapes are shown as videos during both habituation and test phases. While I understand that this was preferred for drawing the infants' attention, it complicates the interpretation of the results. First, it becomes hard to disentangle the learning of the shape skeleton from the learning of the motion trajectory. Second, the comparison with neural networks becomes more difficult as these neural networks are not sensitive to motion.</p></disp-quote><p>The reviewer raises a valid concern. However, we would point out that the skeletal model, like the neural networks, is not sensitive to motion, yet it performed comparably to infants, suggesting that motion information is not needed for models to succeed on one-shot categorization. We would also note that the neural networks were successful at categorizing objects by their surface forms from the same videos (see pages 9-10), suggesting that their deficit was related to the type of information used for categorization, not to the use of videos <italic>per se</italic>. Finally, we now include a model of motion flow (FlowNet), which we compare to infants. Although FlowNet performed above chance in both experiments, our data nevertheless suggest that the skeletal model was a better match to infants’ performance (see pages 6-9). Nevertheless, we now explicitly discuss the potential contribution of motion trajectory in forming object representations (see pages 11-12).</p><disp-quote content-type="editor-comment"><p>– Because surface form differed for both test objects, the same-skeleton object will be visually more similar to the habituated object than the different-skeleton object. Therefore, it cannot be ruled out that results reflect habituation to lower-level stimulus properties rather than shape skeleton. This is also suggested by recent fMRI results using these stimuli (Ayzenberg et al., Neuropsychologia 2022), showing that the skeletal model correlates with activity patterns throughout the visual system, including V1 (though the cross-surface form results are only shown for V3 and LO, as far as I could tell).</p></disp-quote><p>Please see our previous response (#3). Although it is true that the same-skeleton object was more similar than the different-skeleton object, in raw pixels, to the habituated object, infants’ looking times suggest that both test objects (same-skeleton and different-skeleton) were perceptually discriminable from the habituated object (see pages 6 and 8). Moreover, we also demonstrate that infants (and the Skeletal model) outperformed a model based on image-level similarity (Pixel model; see pages 6-9). Nevertheless, we now elaborate on the potential contribution of image-level similarity to categorization (see page 11).</p><p>Finally, we would note that although Ayzenberg et al. (2022) found that the Skeletal model was correlated with the response profile of V1, it did not explain unique variance in this region after controlling for low-level visual similarity. By contrast, the Skeletal model explained unique variance in areas V3 and LO, even when controlling for several other models of visual similarity, and even when surface forms varied. These findings suggest that there can be some covariation between the Skeletal model and other visual properties but, importantly, different models explain unique variance in human behavioral judgments and neural responses. These findings further highlight the importance of the current approach in which we compared infants to several models of vision, not only the Skeletal model.</p><disp-quote content-type="editor-comment"><p>I would recommend the authors to include a discussion of the possible contributions of motion and non-skeletal visual properties to the habituation results.</p></disp-quote><p>We thank the reviewer for their recommendation. We now include discussion of the possible contributions of motion and non-skeletal visual properties (see pages 11-12).</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>In addition to the comments of the previous reviews, during the consultation of the appeal, the following questions came up:</p><p>1. What new insight do we gain from the infant study above and beyond what we already know from adults?</p></disp-quote><p>Although adults are capable of one-shot categorization, this says nothing of its developmental origins. In particular, because of adults’ extensive experiences, it is difficult to disambiguate the contributions of early-developing perceptual mechanisms, supervised object experience, and language. With infants, we side-step these issues. By testing children who are largely non-verbal and who have had minimal experience with different objects, we provide a unique test of the perceptual mechanisms that support one-shot categorization. Moreover, few studies have compared the categorization abilities of infants to different computational models. This approach allows us to better assess the mechanism(s) underlying one-shot categorization and provides a novel avenue by which to evaluate the biological plausibility of existing models. Indeed, a common perspective in the machine learning and neuroscience literatures is that ANNs may simply need more extensive training to match human performance. Our study provides empirical evidence against this perspective by showing that one-shot categorization can be achieved early in human development, when language and object experiences are limited. Thus, by exploring object categorization at this developmental timepoint, we elucidate the initial processes that may support object categorization throughout the lifespan and provide a strong constraint on the development of object recognition models. In the revised manuscript, we have further elaborated on our motivation to test infants (see page 3).</p><disp-quote content-type="editor-comment"><p>2. Why do you think that (dis)habitation is evidence of learning per se, rather than comparisons of perceptual similarity between items?</p></disp-quote><p>We would argue that category learning and comparisons of visual similarity are not mutually exclusive. In particular, there is strong evidence that both children and adults make category decisions on the basis of visual similarity and that basic-level categories are grouped on the basis of visual similarity (e.g., Sloutsky, 2003 – Trends in Cognitive Sciences). The key question of our work is: what kind of visual properties are infants using to determine similarity, and, consequently, group membership? Habituation paradigms provide an excellent method to address this question because they reveal what properties, from the habituation phase, infants generalize to the test phase. Because there was no point at which habituation and test stimuli were presented simultaneously, infants’ looking behaviors were based on what they retained (i.e., learned) from the habituation phase, not direct comparisons of visual information.</p><p>We would also note that extensive research with infants has shown that their looking times in the test phase reflect the learned regularities of the habituation phase, such that, like adults, their category boundaries change depending on the variability of the stimuli during habituation (e.g., Bomba &amp; Siqueland, 1983 – Journal of Experimental Child Psychology). This work suggests that infants learn the relevant properties of a category from the habituation phase. We have now provided additional review of the habituation and infant categorization literatures (see page 4).</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The reviewers were not convinced that the experiments convincingly demonstrate “one-shot” categorization. This concern can be addressed by adjusting the headline claim throughout (title, abstract, intro, results and discussion would need some modifications). A suggested alternative title could be: &quot;The shape skeleton supports shape similarity judgments in human infants&quot;.</p><p>The design involves making a judgment about which objects appear to be more similar. This requires comparing the distances between presented items (in some feature space describing the stimuli). Making such a comparison doesn't involve learning anything on the basis of the experience. It doesn't involve generalization and there is no sense in which it is a 'one-shot' task, except that a given trial presents only a few items. But this is true of practically ANY experiment involving comparing a small number of items.</p><p>Here is an analogy: suppose the infants were shown three different (i.e., easily discriminable) patches of gray shades: two patches are relatively light shades of gray and one is a significantly darker shade. A habituation experiment like the one the authors performed would reveal that the infants see the two light grays as more similar than the darker one. But in what sense is this 'one shot categorization'? The infants wouldn't have learned a new category. There is no meaningful generalization. The experiment simply reflects the fact that similar grays look more similar than more different ones.</p><p>The same is true in the authors' experiments. The experiments demonstrate that objects with more similar skeletons appear more similar to infants. This is not a trivial result, and is worthy of publication in its own right (although similar findings have already been shown in adults). However not under the title of 'one-shot categorization'. Instead, it should be pitched (correctly) as the shape skeleton contributing substantially to judgments of shape similarity.</p><p>To reiterate: I think the authors have performed an elegant study with some interesting findings. I think their interpretation can certainly be discussed in the MS. However, the headline claim about one-shot categorization should be toned down. Otherwise, the term would 'one-shot' would not mean anything anymore.</p></disp-quote><p>Thank you for the additional opportunity to revise our manuscript. As suggested, we have changed the title of our manuscript to reflect the focus on shape similarity as opposed to one-shot categorization. The new title is: &quot; Perception of an object’s global shape is best described by a model of skeletal structure in human infants.&quot; And we have revised the manuscript accordingly to reflect the new focus. The potential implications for one-shot categorization are now exclusively discussed in the general discussion.</p></body></sub-article></article>