<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103425</article-id><article-id pub-id-type="doi">10.7554/eLife.103425</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103425.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dual-format attentional template during preparation in human visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yilin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5695-7093</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Taosheng</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Jia</surname><given-names>Ke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0005-5498-8952</contrib-id><email>kjia@zju.edu.cn</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Theeuwes</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Gong</surname><given-names>Mengyuan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0333-4957</contrib-id><email>gongmy426@zju.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Department of Psychology and Behavioral Sciences, Zhejiang University</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05hs6h993</institution-id><institution>Department of Psychology, Michigan State University</institution></institution-wrap><addr-line><named-content content-type="city">East Lansing</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Liangzhu Laboratory, MOE Frontier Science Center for Brain Science and Brain-machine Integration, State Key Laboratory of Brain-machine Intelligence, Zhejiang University</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0310dsa24</institution-id><institution>Department of Neurobiology, Affiliated Mental Health Center &amp; Hangzhou Seventh People's Hospital, Zhejiang University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>NHC and CAMS Key Laboratory of Medical Neurobiology, Zhejiang University</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>29</day><month>10</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP103425</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-10-03"><day>03</day><month>10</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-10-03"><day>03</day><month>10</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.12.602176"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-03"><day>03</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103425.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-25"><day>25</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103425.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-09-12"><day>12</day><month>09</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103425.3"/></event></pub-history><permissions><copyright-statement>© 2024, Chen et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Chen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103425-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-103425-figures-v1.pdf"/><abstract><p>Goal-directed attention relies on forming internal templates of key information relevant for guiding behavior, particularly when preparing for upcoming sensory inputs. However, evidence on how these attentional templates are represented during preparation remains controversial. Here, we combine functional magnetic resonance imaging with an orientation cueing task to isolate preparatory activity from stimulus-evoked responses. Using multivariate pattern analysis, we found decodable information about the to-be-attended orientation during preparation; yet preparatory activity patterns were different from those evoked when actual orientations were perceived. When perturbing the neural activity by means of a visual impulse (‘pinging’ technique), the preparatory activity patterns in visual cortex resembled those associated with perceiving these orientations. The observed differential patterns with and without the impulse perturbation suggest a predominantly non-sensory format and a latent, sensory-like format of representation during preparation. Furthermore, the emergence of the sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. By engaging this dual-format mechanism during preparation, the brain is able to encode both abstract, non-sensory information and more detailed, sensory information, potentially providing advantages for adaptive attentional control. For example, consistent with recent theories of visual search, a predominantly non-sensory template can support the initial guidance and a latent sensory-like format can support prospective stimulus processing.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>attentional template</kwd><kwd>neural representation</kwd><kwd>fMRI</kwd><kwd>decoding</kwd><kwd>visual cortex</kwd><kwd>impulse perturbation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>National Science and Technology Innovation 2030</institution></institution-wrap></funding-source><award-id>Major Project 2021ZD0200409</award-id><principal-award-recipient><name><surname>Gong</surname><given-names>Mengyuan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32371087</award-id><principal-award-recipient><name><surname>Gong</surname><given-names>Mengyuan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32300855</award-id><principal-award-recipient><name><surname>Jia</surname><given-names>Ke</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>3200784</award-id><principal-award-recipient><name><surname>Gong</surname><given-names>Mengyuan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Fundamental Research Funds for the Central University</institution></institution-wrap></funding-source><award-id>226-2024-00118</award-id><principal-award-recipient><name><surname>Gong</surname><given-names>Mengyuan</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>MOE Frontiers Science Center for Brain Science &amp; Brain-Machine Integration</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gong</surname><given-names>Mengyuan</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02drdmm93</institution-id><institution>Chinese Academy of Medical Sciences</institution></institution-wrap></funding-source><award-id>Non-profit Central Research Institute Fund 2023-PT310-01</award-id><principal-award-recipient><name><surname>Jia</surname><given-names>Ke</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Preparatory attentional templates exist in two formats with distinct functional states, a default non-sensory format and a latent sensory-like format.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To address the challenge of processing the overwhelming amounts of sensory inputs from the external environment, the brain must allocate attentional resources to prioritize the processing of task-relevant information. Importantly, humans can proactively prepare for stimulus selection before the arrival of sensory inputs (<xref ref-type="bibr" rid="bib61">Summerfield and de Lange, 2014</xref>). For example, when preparing to hail a taxi on the road, we tend to form a mental representation of the defining features of a taxi (e.g., yellow with a car-like shape). This ability relies on the formation of attentional templates – mental representations of the target – to accelerate stimulus selection and resolve perceptual competition by enhancing task-relevant information and suppressing irrelevant information (<xref ref-type="bibr" rid="bib9">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib30">Kastner et al., 1999</xref>). While most attentional models posit that attentional templates during stimulus processing reflect veridical representations of the target (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Malcolm and Henderson, 2009</xref>), the nature of the template during preparation remains less understood.</p><p>A classical view suggests that attentional template during preparation may reflect veridical target features, analogous to the representational format during stimulus selection. However, evidence supporting this account has been mixed. For example, while some previous functional magnetic resonance imaging (fMRI) studies have demonstrated that preparatory activity contains target information similar to the sensory responses to the corresponding targets (<xref ref-type="bibr" rid="bib33">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Lewis-Peacock et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Stokes et al., 2009</xref>), more recent electrophysiological studies suggest that, if anything, this template is engaged only shortly before the expected arrival of sensory input rather than being continuously active (<xref ref-type="bibr" rid="bib19">Grubert and Eimer, 2018</xref>; <xref ref-type="bibr" rid="bib42">Myers et al., 2015</xref>). Notably, in some cases, the template is even largely undetectable during preparation (<xref ref-type="bibr" rid="bib66">Wen et al., 2019</xref>). Alternatively, an emerging view suggests a non-veridical template suffices for guiding attention during preparation, where precise processing of stimuli may be unnecessary at this stage. Support for this notion comes from the identification of attentional signals during preparation that differ from neural signals observed during perceptual target processing (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>). Recent theories of visual search also propose a non-veridical, ‘good-enough’ template for early attentional guidance (<xref ref-type="bibr" rid="bib68">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="bib73">Yu et al., 2023</xref>). However, it remains unclear whether a ‘good-enough’ template for search also applies to preparatory attention.</p><p>The notion that there may be a sensory and non-sensory attentional template might not be as far-fetched as it seems. Indeed, it is feasible that during preparation, following stimulus presentation, attentional signals undergo a transformation from a non-sensory to a sensory-like template. Previous behavioral (<xref ref-type="bibr" rid="bib21">Hamblin-Frohman and Becker, 2021</xref>; <xref ref-type="bibr" rid="bib72">Yu et al., 2022</xref>) and neural studies (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Wen et al., 2019</xref>) are generally consistent with this idea of coarse-to-fine transitions, suggesting that during preparation, a sensory-like template may not be initially necessary but only becomes relevant when the stimulus needs to be identified. However, if and in what way the brain coordinates these non-sensory and sensory-like templates remains unclear. Here, we propose that during preparation, a sensory-like template may be stored in a latent (e.g., activity–silent) state concurrently with a non-sensory template. This idea parallels recent findings from working memory studies, which suggest that information intended for proactive use is kept in activity–silent traces to support future behavior (<xref ref-type="bibr" rid="bib59">Stokes, 2015</xref>; <xref ref-type="bibr" rid="bib69">Wolff et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Wolff et al., 2017</xref>). The present study seeks to determine the possibility of the latent, sensory-like template during the preparation for discriminating an upcoming stimulus.</p><p>To test these hypotheses, participants engaged in a cueing task in which they prepared during an extended period of time for the presentation of a compound stimulus grating containing the cued orientation and a distractor orientation. In addition, in order to be able to construct the sensory-format representations (leftward and rightward orientation), single orientations were presented during the perception task. Critically, we used a ‘pinging’ technique combined with multivariate decoding methods, which has been shown to be effective in retrieving information from latent brain states (<xref ref-type="bibr" rid="bib11">Duncan et al., 2023</xref>; <xref ref-type="bibr" rid="bib69">Wolff et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Wolff et al., 2017</xref>; <xref ref-type="bibr" rid="bib74">Zhang and Luo, 2023</xref>). In the standard condition (<italic>No-Ping session</italic>), the preparation period was devoid of visual impulses. During preparation, the neural activity patterns in visual and frontoparietal areas could discriminate between the orientations that participants were preparing for. Yet, neural activity patterns evoked by the preparation for upcoming orientations were distinct from those evoked by perception of orientations, suggesting a predominantly non-sensory template during preparation. By contrast, when we presented a high-contrast, task-irrelevant impulse stimulus during preparation (<italic>Ping session</italic>), neural activity patterns activated by the preparation for orientation in the visual cortex were similar to those evoked by the perception of orientations, suggesting the existence of a latent, sensory-like format of representation during preparatory attention. Furthermore, the emergence of sensory-like templates coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. Our findings provide evidence for the co-existence of two formats of attentional templates (non-sensory vs. sensory-like) during preparation, as well as a novel neural mechanism for their maintenance in different functional states (active vs. latent). We propose that this dual-format representation may serve to increase flexibility of attentional control.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral performance during the attention tasks</title><p>In an orientation cueing task, participants were shown a color cue indicating the reference orientation (45° or 135°) to attend to during preparation period (a delay of 5.5 or 7.5 s) with or without the impulse perturbation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This was followed by the presentation of a compound stimulus consisting of two oriented gratings. During the stimulus selection period (after the gratings appeared), participants were tasked with discriminating a small angular offset of the cued grating from the cued reference orientation. The angular offset was individually thresholded before the scanning sessions (mean offset = 2.50° in the No-Ping session and 2.52° in the Ping session) without significant difference between the two sessions (independent <italic>t</italic>-test: <italic>t</italic>(38) = 0.085, p = 0.932, Cohen’s <italic>d</italic> = –0.027). Participants’ discrimination performance showed no significant difference between two attended orientations in either the No-Ping (paired <italic>t</italic>-test: <italic>t</italic>(19) = 1.439, p = 0.166, Cohen’s <italic>d</italic> = –0.321) or the Ping session (paired <italic>t</italic>-test: <italic>t</italic>(19) = 0.494, p = 0.627, Cohen’s <italic>d</italic> = 0.122; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). A two-way mixed ANOVA (attended orientation × session) revealed neither significant main effects (attended orientation: <italic>F</italic>(1,38) = 0.392, p = 0.535, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.01; session: <italic>F</italic>(1,38) = 0.001, p = 0.970, <italic>η</italic><sub>p</sub><sup>2</sup> &lt; 0.001) nor interaction effect (<italic>F</italic>(1,38) = 1.811, p = 0.186, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.045). Bayesian analyses provided moderate evidence to support the null hypothesis (BF<sub>excl</sub> &gt; 3.633), suggesting comparable performance levels between two sessions and two attended orientations.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experiment procedure and behavioral performance.</title><p>(<bold>A</bold>) Attention tasks in the No-Ping and Ping sessions. Note that only long-delay trials are shown. A small proportion of short-delay trials (20%, with a delay of 1.5 or 3.5 s) were included to create temporal uncertainty and encourage consistent active preparation during the delay. Both component gratings were flickering at 10 Hz between white and black, so that luminance could not confound either the task strategy (e.g., attending to luminance) or neural measures. The inset shows two sets of color-orientation mapping, which were reversed halfway through the experiment to minimize the impact of cue-induced sensory difference on neural activity. A high-contrast impulse was presented during the preparation period in the Ping session. (<bold>B</bold>) Perception task. Similar to the attention task, the single-orientation grating also flickered at 10 Hz between white and black. (<bold>C</bold>) Behavioral accuracy in the attention tasks in the No-Ping and Ping sessions. Each dot represents one subject’s data. Error bars denote standard error of the means (SEM).</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Behavioral accuracy in the attention tasks.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig1-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>A default, non-sensory representation of attentional template during preparation</title><p>The first aim of this study was to determine whether attentional signals during preparation are encoded in a sensory-like or non-sensory format. To address this, we first examined whether, in the attention task during the No-Ping condition, the distributed neural pattern contained feature-specific information. We trained and tested separate classifiers to predict the attended orientation during the preparation and stimulus selection periods (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, ‘Attention decoding’; see Materials and methods for details). This analysis was performed for each of the four regions along the visual hierarchy, including primary visual cortex (V1), extrastriate visual cortex (EVC), intraparietal sulcus (IPS), and prefrontal cortex (PFC). The average decoding accuracies for both preparation and stimulus selection periods were significantly above-chance level in each region (permutation analyses: ps &lt; 0.004 across regions, <xref ref-type="fig" rid="fig2">Figure 2B</xref>), indicating that the brain maintained reliable information about the attended feature both before and after the onset of the compound grating. Next, we examined whether the preparatory activity reflected a sensory-like format of attentional template (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, ‘Cross-task generalization’, see Materials and methods). We trained a classifier using data from the perception task (leftward vs. rightward orientation; <xref ref-type="fig" rid="fig1">Figure 1B</xref>) and tested its performance on data from the preparation period in the attention task (attend leftward vs. attend rightward). However, this cross-task generalization analysis yielded no significant effects (ps &gt; 0.132 across the regions). In contrast, we observed above-chance generalization from the perception task to the stimulus selection period (ps &lt; 0.001 across regions, <xref ref-type="fig" rid="fig2">Figure 2C</xref>), confirming previous findings of the sensory-like attentional template following stimulus presentation (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Wen et al., 2019</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Multivoxel pattern analysis (MVPA) for the No-Ping session and Ping session.</title><p>(<bold>A</bold>) Schematic illustration of the decoding of attended orientation (attend leftward vs. attend rightward) in the attention task (left panel) and the cross-task generalization analysis from perception task to the attention task (right panel). The four regions are shown on a representative right hemisphere as colored areas: V1 is marked in red, extrastriate visual cortex (EVC) in yellow, intraparietal sulcus (IPS) in cyan, and prefrontal cortex (PFC) in purple. (<bold>B</bold>) Decoding accuracy during preparation and stimulus selection periods across regions in the No-Ping and (<bold>D</bold>) Ping session. (<bold>C</bold>) Cross-task generalization performance from the perception task to the preparatory periods and the stimulus selection periods across regions in the No-Ping and (<bold>E</bold>) Ping session. The dashed lines represent the theoretical chance level (0.5). Each dot represents one subject’s data. Error bars denote SEM.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Decoding accuracy across brain regions in the attention tasks.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig2-data1-v1.csv"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Cross-task decoding generalization across brain regions.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig2-data2-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Sensory decoding in the perception task.</title><p>(<bold>A</bold>) Decoding of orientations in No-Ping and (<bold>B</bold>) Ping sessions. The dashed lines represent theoretical chance level (0.5). The decoding accuracy was significantly above-chance across regions in both sessions (ps &lt; 0.004). A two-way mixed ANOVA (sessions × region) revealed a main effect of region (<italic>F</italic>(3,114) = 84.274, p <italic>&lt;</italic> 0.001, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.689), Neither the main effect of session (<italic>F</italic>(1,38) = 1.103, p = 0.300, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.028; BF<sub>excl</sub> = 2.605) nor an interaction effect was significant (<italic>F</italic>(3,114) = 0.592, p = 0.621, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.015; BF<sub>excl</sub> = 5.809). Each dot represents one participant. Error bars denote standard error of the means.</p><p><supplementary-material id="fig2s1sdata1"><label>Figure 2—figure supplement 1—source data 1.</label><caption><title>Decoding accuracy across brain regions in the perception task.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig2-figsupp1-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig2-figsupp1-v1.tif"/></fig></fig-group><p>Before drawing conclusions based on the lack of generalization from the perception task to preparatory attention, we considered two alternative explanations to rule out potential confounds. First, the robust attention decoding during preparation ruled out the possibility that participants were not actively engaged in the task during preparation (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, unfilled bars). Second, the generalizable effect from the perception task to the stimulus selection period across regions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, filled bars) argues against the possibility of low statistical power. Overall, these findings suggest that the preparatory attention and sensory processing of features have distinct formats, presumably reflecting a non-sensory format of representation during the preparation. These results replicate those of a previous fMRI study using motion stimuli with a similar design (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>). Furthermore, consistent with previous studies (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>), univariate analysis did not reveal any reliable difference in overall BOLD responses between attention orientations (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>).</p></sec><sec id="s2-3"><title>A latent, sensory-like attentional template during preparation revealed by visual impulse</title><p>The second aim of our study was to examine whether a latent, sensory-like template exists during preparation. While this precise template may not be necessary for preparation, it is relevant for subsequent target selection and discrimination (i.e., select the cued grating from the compound stimulus and discriminate a small angular offset between the cued grating and the reference orientation). To test this hypothesis, we perturbed the neural activity by means of a visual impulse during the preparation period in the Ping session (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right panel). Using the same analyses as those performed in the No-Ping session, robust attentional signals were observed during both preparation and stimulus selection periods (permutation analyses: ps &lt; 0.001 across regions; <xref ref-type="fig" rid="fig2">Figure 2D</xref>). Importantly, the cross-task generalization analyses indicated that the visual impulse led to above-chance generalization from the perception task to preparation period (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, unfilled bars) in V1 and EVC (ps &lt; 0.001), but not in IPS and PFC (ps &gt; 0.584), along with generalizable effects from the perception task to the stimulus selection periods (ps &lt; 0.036 across regions; <xref ref-type="fig" rid="fig2">Figure 2E</xref>; filled bars). These results suggest that different brain areas are involved in coding for sensory-like templates. To further evaluate whether the cross-task generalization from the perception task to the preparation period was statistically different with and without visual impulse, we conducted a two-way mixed ANOVA (session × region) on the generalization performance. The analysis revealed main effects of region (<italic>F</italic>(3,114) = 5.220, p = 0.002, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.121), session (<italic>F</italic>(1,38) = 7.321, p = 0.010, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.162), and importantly, a significant interaction effect (<italic>F</italic>(3,114) = 3.964, p = 0.010, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.094) that the visual impulse led to significantly increased decoding accuracy in V1 (independent <italic>t</italic>-test: <italic>t</italic>(38) = 3.145, p = 0.003, Cohen’s <italic>d</italic> = 0.995) and EVC (independent <italic>t</italic>-test: <italic>t</italic>(38) = 2.153, p = 0.038, Cohen’s <italic>d</italic> = 0.681), but not in the frontoparietal regions (ps &gt; 0.374). This dissociable result between the two sessions further supports the activation of a latent, sensory-like template by the visual impulse during preparatory attention.</p><p>To further solidify this conclusion, the following analyses were used to examine several alternative possibilities. First, we examined whether the impulse-driven generalization resulted from stronger feature information in the Ping compared to No-Ping session during the perception task. This was not the case, as evidenced by comparable levels of decodable orientation information between the Ping and No-Ping sessions (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Next, we asked whether the increased generalization was due to generally stronger attentional signals in the Ping session during the attention tasks – for example, if visual impulses simply refocused attention during long delays. This was not the case, as the two-way mixed ANOVAs (session × region) on attention decoding accuracy revealed neither a significant main effect of session nor an interaction effect during both the preparation (ps &gt; 0.519; BF<sub>excl</sub> &gt; 3.247) and stimulus selection periods (ps &gt; 0.336; BF<sub>excl</sub> &gt; 3.297), suggesting comparable amount of attentional information between the two sessions. Therefore, the findings of impulse-driven sensory-like template in the visual cortex during preparation cannot be explained by general differences between two sessions.</p></sec><sec id="s2-4"><title>Matching preparatory attention to sensory template: impact on neural representation and behavior</title><p>The reported decoding accuracy from the cross-task generalization analysis quantifies the degree to which differences in neural activity pattern between two conditions are shared across attention and perception tasks. However, it does not directly measure how similar the neural patterns are when attending to an orientation compared to perceiving that orientation. Unlike decoding accuracies, Mahalanobis distance provides a continuous measure for characterizing representational geometries between different conditions (<xref ref-type="bibr" rid="bib39">Mahalanobis, 1936</xref>). To further corroborate our findings of the impulse-driven sensory-like template, we calculated the Mahalanobis distance between each attention condition during preparation and each perception condition (see Materials and methods). If the patterns of activity reflect a sensory-like template, we would expect greater pattern similarity (smaller distance) between ‘attend leftward’ and ‘perceive leftward’ than between ‘attend leftward’ and ‘perceive rightward’, and vice versa for the ‘attend rightward’ conditions (see <xref ref-type="fig" rid="fig3">Figure 3A</xref> for a schematic of the four pair-wise distance measures), leading to an interaction between attended and perceived orientation conditions.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Orientation-selective attentional modulations on neural pattern distances during preparation.</title><p>(<bold>A</bold>) Schematic illustration of the representational distance (mean Mahalanobis distances) between each of the attention conditions and each of the perception conditions. Colored arrows indicate measures of the pair-wise Mahalanobis distance. The right panel shows two attention trials (red indicates attend-to-leftward and green indicates attend-to-rightward) to the distribution of each perception condition (shown in a cloud of light-colored dots). (<bold>B</bold>) Mahalanobis distance between preparatory attention condition and perceived orientation condition in the No-Ping and (<bold>C</bold>) Ping sessions. Error bars denote SEM. (<bold>D</bold>) Correlations between attentional modulation index (AMI) and reaction time (RT) in the No-Ping and (<bold>E</bold>) Ping sessions. Each dot represents one subject’s data. The shaded area represents the confidence intervals of the regressed lines. **p &lt; 0.01.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Mahalanobis distances between preparatory attention conditions and the perception conditions.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig3-data1-v1.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>Attentional modulation index and reaction time.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig3-data2-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Mahalanobis distance during stimulus selection period.</title><p>(<bold>A</bold>) Attentional modulation on Mahalanobis distance in the No-Ping and (<bold>B</bold>) Ping sessions. Two-way repeated-measures ANOVAs (attended orientation × orientation) were performed on the Mahalanobis distance during stimulus selection period. Significant interaction effects were observed in both the No-Ping session (V1: <italic>F</italic>(1,19) = 12.470, p = 0.002, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.396; extrastriate visual cortex [EVC]: <italic>F</italic>(1,19) = 26.538, p &lt; 0.001, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.583; intraparietal sulcus [IPS]: <italic>F</italic>(1,19) = 4.250, p = 0.053, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.183; prefrontal cortex [PFC]: <italic>F</italic>(1,19) = 1.557, p = 0.227, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.076) and Ping session (V1: <italic>F</italic>(1,19) = 11.698, p = 0.003, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.381; EVC: <italic>F</italic>(1,19) = 12.742, p = 0.002, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.401; IPS: <italic>F</italic>(1,19) = 5.227, p = 0.034, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.216; PFC: <italic>F</italic>(1,19) = 0.087, p = 0.771, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.005). These results support the predicted role of attention in selectively enhancing the representation of task-relevant features while filtering out task-irrelevant ones, leading to neural representations that closely resembled the perception of single features. Each dot represents one participant. Error bars denote standard error of the means. <sup>†</sup>p &lt; 0.06, *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001.</p><p><supplementary-material id="fig3s1sdata1"><label>Figure 3—figure supplement 1—source data 1.</label><caption><title>Mahalanobis distances between stimulus-based attention conditions and the perception conditions.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig3-figsupp1-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Relationship between attentional modulation on Mahalanobis distance and RT.</title><p>Using the attentional modulation index (AMI) calculated based on trial-wise Mahalanobis distance in V1 preparatory activity, we sorted the AMI values in a descending order and selected the top-ranked 25% of trials (i.e., 36 trials) and bottom-ranked 25% of trials to represent ‘strong modulation’ and ‘weak modulation’ trials, respectively. We then extracted behavioral responses on these selected trials and calculated RT for each trial type in the (<bold>A</bold>) No-Ping and (<bold>B</bold>) Ping sessions. The analysis revealed faster responses in the ‘strong modulation’ than ‘weak modulation’ trials in the Ping session (paired <italic>t</italic>-test: <italic>t</italic>(19) = –2.746, p = 0.013, Cohen’s <italic>d</italic> = –0.614, right panel), but not in the No-Ping session (paired <italic>t</italic>-test: <italic>t</italic>(19) = –1.487, p = 0.154, Cohen’s <italic>d</italic> = –0.332, left panel). Each dot represents one subject’s data. Error bars denote SEM. *p &lt; 0.05.</p><p><supplementary-material id="fig3s2sdata1"><label>Figure 3—figure supplement 2—source data 1.</label><caption><title>Reaction time in strong and weak attentional modulation trials.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig3-figsupp2-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig3-figsupp2-v1.tif"/></fig></fig-group><p>We used a two-way repeated-measures ANOVA (attended orientation × perceived orientation) on the Mahalanobis distance, separately for each session and each region. During the preparatory period in the No-Ping session, no significant interaction effects were observed across regions (ps &gt; 0.443; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). In contrast, the same analyses applied to the Ping session revealed significant interaction effects in visual areas (V1: <italic>F</italic>(1,19) = 9.335, p = 0.007, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.329; EVC: <italic>F</italic>(1,19) = 8.563, p = 0.009, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.311; <xref ref-type="fig" rid="fig3">Figure 3C</xref>), but not for frontoparietal regions (ps &gt; 0.213). This cross-region difference is consistent with the function of sensory areas in encoding precise neural representations for basic visual features. Next, we directly compared whether attentional modulation of Mahalanobis distance was statistically different with and without the visual impulse. We defined a new condition label based on orientation consistency between attended and perceived orientations: (1) same orientation: averaging ‘attend leftward/perceive leftward’ and ‘attend rightward/perceive rightward’; and (2) different orientation: averaging ‘attend leftward/perceive rightward’ and ‘attend rightward/perceive leftward’. A two-way mixed ANOVA (session × orientation consistency) on Mahalanobis distance revealed a main effect of orientation consistency in V1 (<italic>F</italic>(1,38) = 4.21, p = 0.047, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.100), indicating that activity patterns were more similar when attended and perceived orientations matched. No significant main effect of session was found (p = 0.923). Importantly, a significant interaction was found in V1 (<italic>F</italic>(1,38) = 5.00, p = 0.031, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.116), suggesting that visual impulse enhanced the similarity between preparatory attentional template and the perception of corresponding orientation. In EVC, the same analysis revealed only a main effect of orientation consistency (<italic>F</italic>(1,38) = 5.87, p = 0.020, <italic>η</italic><sub>p</sub><sup>2</sup> = 0.134), with no other significant effects (ps &gt;0.240). We also calculated the Mahalanobis distance between neural patterns evoked by superimposed gratings during the stimulus selection period and each condition in the perception task, finding similar results (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This result was expected, as feature-based attention is known to selectively enhance task-relevant features while filtering out task-irrelevant ones.</p><p>The continuous nature of the Mahalanobis distance also made it possible to further investigate potential neural–behavioral correlations. We examined whether activating a sensory-like template during preparation would benefit subsequent orientation processing. In particular, we calculated attentional modulation indices (AMIs) based on trial-wise Mahalanobis distance in V1. The index was calculated as follows: AMI = (<italic>D</italic><sub>different</sub> – <italic>D</italic><sub>same</sub>)/(<italic>D</italic><sub>different</sub> + <italic>D</italic><sub>same</sub>), where <italic>D</italic><sub>same</sub> and <italic>D</italic><sub>different</sub> are the measured distance (<italic>D</italic>) in the Same (e.g., attend and perceive the same orientation) and Different (e.g., attend and perceive different orientations) orientation condition, respectively (see Methods and materials). Then, we calculated the correlation between AMI and both reaction time (RT) and accuracy across participants, separately for each session. In the No-Ping session, we observed no significant correlation between AMI in V1 and RT (<italic>r</italic> = –0.366, p = 0.113; <xref ref-type="fig" rid="fig3">Figure 3D</xref>). By contrast, the same analysis in the Ping condition revealed a significantly negative correlation (<italic>r</italic> = –0.518, p = 0.019; <xref ref-type="fig" rid="fig3">Figure 3E</xref>). These results indicate that the attentional modulations evoked by visual impulse were associated with faster RTs. These effects were not observed for accuracy (ps &gt; 0.550). Furthermore, we also performed within-subject analysis by sorting trials as ‘strong modulation’ and ‘weak modulation’ trials based on each individual’s AMI values, facilitated RTs were observed in ‘strong modulation’ trials during the Ping session (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). These results suggest that the impulse-driven sensory-like template in primary visual cortex is functionally relevant to subsequent attentional selection, providing evidence for the prospective use of sensory-like template in this task. In addition, we did not observe such behavioral differences in analogous analyses using data from the stimulus selection period in either session (ps &gt; 0.230), which might be due to the potential dilution by strong stimulus-evoked responses during the stimulus selection period.</p></sec><sec id="s2-5"><title>Activating sensory-like template strengthens the informational connectivity between sensory and frontoparietal areas</title><p>Selective attention is generally believed to rely on coordinated network activity (<xref ref-type="bibr" rid="bib8">Corbetta and Shulman, 2002</xref>). In particular, studies have shown that functional connectivity between sensory and frontoparietal areas was modulated by attentional control (<xref ref-type="bibr" rid="bib7">Bressler et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Rosenberg et al., 2020</xref>). Given that the impulse-driven sensory-like template facilitated behavior, we reasoned that it may also enhance network communication. Thus, we examined informational connectivity (IC) measures to explore how the impulse altered network function during the attention task.</p><p>We used a method that allows inference based on multivoxel pattern information rather than univariate BOLD response (<xref ref-type="bibr" rid="bib26">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="bib43">Ng et al., 2021</xref>). For each region of interest (ROI), we calculated the cross-validated Mahalanobis distance from each attention trial (from one left-out run) to the distribution of each attended orientation (all trials from remaining runs) during preparation (see Methods and materials). To quantify the degree of attentional modulation during preparation, we calculated the AMI based on trial-wise Mahalanobis distance and generated a time course of AMI values across trials (see <xref ref-type="fig" rid="fig4">Figure 4A</xref> for the schematic). Pearson correlation was used to estimate the covariation between each pair of ROIs, and the resulting correlation coefficients were transformed using Fisher’s <italic>z</italic>-transform for statistical inference (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The analysis revealed numerically higher levels of connectivity in Ping than in No-Ping session. This impulse-driven increase in connectivity reached statistical significance in two pairs (<xref ref-type="fig" rid="fig4">Figure 4C</xref>): V1–IPS (independent <italic>t</italic>-test: <italic>t</italic>(38) = 2.566<italic>,</italic> p = 0.014; Cohen’s <italic>d</italic> = 0.812) and V1–PFC (independent <italic>t</italic>-tests: <italic>t</italic>(38) = 3.158<italic>,</italic> p = 0.003; Cohen’s <italic>d</italic>=0.999). The enhanced functional connectivity between V1 and frontoparietal areas driven by the impulse may potentially facilitate information flow among areas to improve attentional control, as implicated by a trend of ping-enhanced correlations between V1-PFC and RTs (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Additionally, the same analysis of AMI based on cross-validated Mahalanobis distance during the stimulus selection period showed no significant differences in information connectivity between No-Ping and Ping sessions (ps &gt;0.224; <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The lack of changes in long-range connectivity during the stimulus selection period may be attributed to a general rise in connectivity caused by strong sensory inputs in this period, which could have attenuated any potential impacts of visual impulses. Furthermore, connectivity analysis based on mean BOLD response over time did not reveal any significant changes in inter-cortical connections between the two sessions (ps &gt; 0.136; <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>), suggesting that the impulse-driven increased information connectivity between V1 and higher-order areas was unlikely contributed by the overall changes of BOLD response.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Information connectivity analysis.</title><p>(<bold>A</bold>) Schematic illustration of the procedure for the information connectivity analysis in the space of two hypothetical voxels. For each region, we calculated the Mahalanobis distance of the attention trial (from one left-out run) from two attention distributions (all trials from remaining runs). Red and green dots indicate activity patterns from two trials (right panel). The brain image shows an example pair of intercortical information connectivity between V1 and prefrontal cortex (PFC). The time series (lower-left panel) consisted of attentional modulation index (AMI) based on the Mahalanobis distance. (<bold>B</bold>) Between-region information connectivity in the No-Ping and Ping sessions. (<bold>C</bold>) The differences in connectivity between the Ping and No-Ping sessions. *p &lt; 0.05, **p &lt; 0.01.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Information connectivity between brain regions during the preparation period.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig4-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Relationships between information connectivity and reaction time.</title><p>(<bold>A</bold>) Correlations between prefrontal cortex (V1–PFC) connectivity and RTs in the No-Ping and (<bold>B</bold>) Ping sessions. We observed a trend toward significance in the correlation between V1–PFC connectivity and RTs in the Ping session (<italic>r</italic> = –0.394, p = 0.086), but not in the No-Ping session (<italic>r</italic> = –0.046, p = 0.846). Each dot represents one subject’s data. The shaded area represents the confidence intervals of the regressed lines.</p><p><supplementary-material id="fig4s1sdata1"><label>Figure 4—figure supplement 1—source data 1.</label><caption><title>Information connectivity in prefrontal cortex (V1–PFC) during preparation and reaction time.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig4-figsupp1-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Functional connectivity analysis based on multivoxel activity patterns during the stimulus selection period.</title><p>The panels show between-region connectivity in the No-Ping session and Ping session based on cross-validated Mahalanobis distance (left panel), and the differences in connectivity between the two sessions (right panel). No significant differences in functional connectivity were observed between sessions (ps &gt;0.224).</p><p><supplementary-material id="fig4s2sdata1"><label>Figure 4—figure supplement 2—source data 1.</label><caption><title>Information connectivity between brain regions during the stimulus selection period.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig4-figsupp2-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Functional connectivity analysis based on mean BOLD activity during the preparation period.</title><p>To examine whether impulse-driven connectivity changes during preparation were influenced by overall changes in BOLD response, we conducted a standard functional connectivity analysis based on mean BOLD response over time. The panels show between-region connectivity in the No-Ping session and Ping session based on raw time series (left panel), and the connectivity differences between sessions (right panel). This analysis also revealed no significant changes in inter-cortical connections between sessions (ps &gt; 0.136).</p><p><supplementary-material id="fig4s3sdata1"><label>Figure 4—figure supplement 3—source data 1.</label><caption><title>Information connectivity between brain regions during the preparatory period based on BOLD activity.</title></caption><media mimetype="application" mime-subtype="octet-stream" xlink:href="elife-103425-fig4-figsupp3-data1-v1.csv"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-fig4-figsupp3-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While there is ample evidence that the brain can maintain an attentional template of an upcoming target before sensory information is presented (<xref ref-type="bibr" rid="bib9">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib30">Kastner et al., 1999</xref>; <xref ref-type="bibr" rid="bib61">Summerfield and de Lange, 2014</xref>), its representational format remains unclear. To address this, we used an orientation cueing paradigm with separated preparation and stimulus selection periods and applied multivoxel pattern analysis (MVPA) to decode neural activity patterns associated with feature-specific attentional information of the upcoming target. The analyses showed robust attentional information both before and after the presentation of the compound grating, indicating a sustained maintenance of attentional templates throughout a trial. Importantly, while the decoders trained on the perception of single orientations could not generalize to preparation until the stimulus selection period (<italic>No-Ping session</italic>), perturbing the brain with a visual impulse resulted in generalizable activity patterns during preparation in V1 and EVC (<italic>Ping session</italic>). These results suggest a predominantly non-sensory format of representation, with a sensory-like template in a latent state during feature-based preparation in the visual cortices. Furthermore, impulse-driven sensory-like template was accompanied by enhanced information connectivity between V1 and frontoparietal areas, as well as enhanced orientation-specific neural modulations of neural distances in the visual areas that predicted levels of behavioral performance. We observed similar response profiles in V1 and EVC, with V1 exhibiting more robust ping-evoked changes compared to EVC, consistent with its primary role in orientation processing (<xref ref-type="bibr" rid="bib45">Priebe, 2016</xref>). The differences between the Ping and No-Ping sessions could not be attributed to differences in sensory information from the perception task, overall strength of preparatory attention, or differences in eye position (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Therefore, our findings suggest a dual-format neural representation scheme (non-sensory vs. sensory-like) operating in different functional states (active vs. latent). This mechanism may give rise to flexible attentional control, allowing effective transition from coarse to fine attentional templates at various processing stages (initial guidance vs. precise stimulus discrimination).</p><p>Recent advances in theories of visual search differentiated between the ‘guiding template’ and ‘target template’ based on measures of behavioral performance and eye movements (<xref ref-type="bibr" rid="bib68">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="bib73">Yu et al., 2023</xref>). According to these theories, early attentional guidance typically depends on non-veridical codes that represent only the most diagnostic information, whereas later, target-match processes utilize more precise codes to optimize decision accuracy (<xref ref-type="bibr" rid="bib31">Kerzel, 2019</xref>; <xref ref-type="bibr" rid="bib53">Scolari et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Yu et al., 2022</xref>). Our study reveals a parallel coding mechanism in the context of feature-based attention, expanding upon these theoretical notions in three key aspects.</p><p>First, we provide neural evidence for a default, predominantly non-sensory template during preparation, indicating that the concept of a ‘guiding template’, as proposed by visual search theories (<xref ref-type="bibr" rid="bib68">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="bib73">Yu et al., 2023</xref>), also applies to preparatory attention in a non-search context. This highlights a shared functional role of a non-veridical attentional template in early guidance across different scenarios. Second, despite the theoretical notion that the brain maintains a more veridical template with detailed target information than is typically utilized to form the guiding template (<xref ref-type="bibr" rid="bib68">Wolfe, 2021</xref>; <xref ref-type="bibr" rid="bib73">Yu et al., 2023</xref>), neural evidence supporting this hypothesis is currently lacking. We provide evidence for this notion and propose a plausible neural implementation for preserving a more veridical, sensory-like template in the latent state. A natural question is why the sensory-like template remains latent during preparation. We note that our task requires both coarse and fine featural information. During preparation, a coarse, non-veridical guiding template suffices for target-distractor discrimination, while during stimulus selection, a precise template is needed for the fine discrimination task (reporting the tilted direction of a small angular offset). Maintaining a latent sensory-like template during preparation is thus efficient, as it facilitates future sensory processing while conserving resources. Finally, information connectivity between visual and higher-order frontoparietal regions was enhanced by visual impulse during preparation, which correlated with improved behavioral performance in feature selection. This result suggests that improved information flow across the relevant areas leads to enhanced attentional control, which in turn contributes to refined sensory representations of target in early visual cortex, facilitating transitions from a non-sensory to sensory-like template. Future studies may adopt layer-specific fMRI to infer the direction of this improved information flow (<xref ref-type="bibr" rid="bib27">Jia et al., 2023</xref>; <xref ref-type="bibr" rid="bib28">Jia et al., 2024</xref>) and explore the relationship between long-range connections and the utilization of different formats of target templates.</p><p>It could be argued that preparatory attention relies on the same mechanisms as working memory maintenance (<xref ref-type="bibr" rid="bib5">Bettencourt and Xu, 2016</xref>; <xref ref-type="bibr" rid="bib57">Sheremata et al., 2018</xref>). While these functions are intuitively similar and likely overlap, there is also evidence indicating that they can be dissociated (<xref ref-type="bibr" rid="bib4">Battistoni et al., 2017</xref>). In particular, we note that in our task, attention is guided by symbolic cues (color-orientation associations), while working memory tasks typically present the actual visual stimulus as the memorandum. A central finding in working memory studies is that neural signals during WM maintenance are sensory in nature, as demonstrated by generalizable neural activity patterns from stimulus encoding to maintenance in visual cortex (<xref ref-type="bibr" rid="bib22">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib54">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Rademaker et al., 2019</xref>). However, in our task, neural signals during preparation were non-sensory, as demonstrated by a lack of such generalization in the No-Ping condition (see also <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>). We believe that the differences in cue format and task demand in these studies may account for such differences. In addition to the difference in the sensory nature of the preparatory versus delay-period activity, our ping-related results also exhibited divergence from working memory studies (<xref ref-type="bibr" rid="bib69">Wolff et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Wolff et al., 2017</xref>). While these studies used the visual impulse to differentiate active and latent representations of <italic>different items</italic> (e.g., attended vs. unattended memory item), our study demonstrated the active and latent representations of <italic>a single item in different formats</italic> (i.e., non-sensory vs. sensory-like). Moreover, unlike our study, the impulse did not evoke sensory-like neural patterns during memory retention (<xref ref-type="bibr" rid="bib70">Wolff et al., 2017</xref>). These observations suggest that the cognitive and neural processes underlying preparatory attention and working memory maintenance could very well diverge. Future studies are necessary to delineate the relationship between these functions both at the behavioral and neural level.</p><p>While we found that the ping allowed us to detect a sensory-like template during preparation, the underlying neural mechanism of such effects remains unclear. One possibility, as informed by theoretical studies of working memory, is that the sensory-like template could be maintained via an ‘activity-silent’ mechanism through short-term changes in synaptic weights (<xref ref-type="bibr" rid="bib41">Mongillo et al., 2008</xref>). In this framework, a visual impulse may function as nonspecific inputs that momentarily convert latent traces into detectable activity patterns (<xref ref-type="bibr" rid="bib47">Rademaker and Serences, 2017</xref>). Related to our findings, it is unlikely that the orientation-specific templates observed during the Ping session emerged de novo from purely non-sensory representations and were entirely induced by an exogenous ping, which was devoid of any orientation signal. Instead, the more parsimonious explanation is that visual impulse reactivated pre-existing latent sensory signals, consistent with the models of ‘activity-silent’ working memory. However, the detailed circuit-level mechanism of such reactivation is still unclear, as well as whether this effect is modality specific. Prior work shows that only visual, but not auditory, impulses reactivate latent visual working memory (<xref ref-type="bibr" rid="bib71">Wolff et al., 2020</xref>), suggesting some degree of modality specificity. However, this finding warrants direct investigation in future studies. Furthermore, we acknowledge that whether pinging identifies an activity-silent mechanism is currently debated (<xref ref-type="bibr" rid="bib3">Barbosa et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Schneegans and Bays, 2017</xref>). An alternative possibility is that the visual impulse amplified a subtle but active representation of the sensory template during preparation. Distinguishing between these alternatives likely requires future studies with more detailed neurophysiological measurements. Regardless of the precise neural mechanism for the observed latent, sensory representation, our results suggest that both sensory and non-sensory templates likely co-exist.</p><p>The non-generalizable activity patterns from perception to preparatory attention, in the absence of visual impulse, suggest a default, predominantly non-sensory template during preparation. This finding is largely consistent with electrophysiological studies (<xref ref-type="bibr" rid="bib42">Myers et al., 2015</xref>; <xref ref-type="bibr" rid="bib66">Wen et al., 2019</xref>) and our prior fMRI work on preparatory attention to motion directions (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), but differs from some previous neuroimaging studies that demonstrated sensory-like templates during preparation (<xref ref-type="bibr" rid="bib33">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Peelen and Kastner, 2011</xref>; <xref ref-type="bibr" rid="bib58">Stokes et al., 2009</xref>). One potential account for these discrepancies is that those studies used cue-only trials where the target was expected but not actually presented, in contrast to our task where the target was shown on every trial with temporally separated preparation and stimulus selection periods. This seemingly subtle difference may significantly impact the formats of the neural representations. Because cue-only trials increased the likelihood of target appearance at the subsequent time point, sensory template may be activated due to modulations of temporal expectations (<xref ref-type="bibr" rid="bib19">Grubert and Eimer, 2018</xref>). This explanation is consistent with theories suggesting differential influences of expectation and attention on neural activity: expectation reflects visual interpretations of stimuli due to sensory uncertainty, whereas attention is guided based on the task relevance of sensory information (<xref ref-type="bibr" rid="bib50">Rungratsameetaweemana and Serences, 2019</xref>; <xref ref-type="bibr" rid="bib60">Summerfield and Egner, 2009</xref>; <xref ref-type="bibr" rid="bib62">Summerfield and Egner, 2016</xref>). Our finding of a predominantly non-sensory format may indicate an optimized coding strategy employed by the brain to effectively and robustly represent information for future use. This aligns with the proposed role of attention in modulating sensory representations to encode only currently relevant information at a minimal cost (<xref ref-type="bibr" rid="bib73">Yu et al., 2023</xref>).</p><p>While our findings cannot pinpoint the exact format of this non-sensory template, we consider categorical coding a plausible candidate based on previous findings. For instance, visual search studies demonstrate that categorical attributes (e.g., steep vs. shallow; left-tilted vs. right-tilted) efficiently guide attention for simple features, such as an orientation or a color (<xref ref-type="bibr" rid="bib35">Kong et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Wolfe et al., 1992</xref>), particularly when features are consistent and predictable (<xref ref-type="bibr" rid="bib24">Hout et al., 2017</xref>). In our task, the angular relations between the target and distractor orientation were defined by categorical attributes (e.g., left-tilted vs. right-tilted) and remained consistent across trials, making a categorical template feasible during preparatory attention. Furthermore, the categorical template allows for greater tolerance of stimulus variability, which is also useful given the trial-by-trial variations in target orientation around the reference orientation in our task. Future studies are needed to address the nature of the non-sensory template during preparation as well as task parameters that might modulate them.</p><p>In summary, the current study suggests that there are two formats of attentional templates, each having a distinct functional state: a default, non-sensory format and a latent, sensory-like format. This dual-format representation aligns with theories on the dual-function of attentional template for different task goals (<xref ref-type="bibr" rid="bib23">Hout and Goldinger, 2015</xref>; <xref ref-type="bibr" rid="bib73">Yu et al., 2023</xref>). The current findings provide a plausible neural implementation for these theories by demonstrating different formats in different functional states. This mechanism likely reflects an optimized coding scheme that effectively balances processing efforts and demands, particularly well suited for flexible control and transitions from coarse to fine task demands in visually guided behavior.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twenty individuals participated in the No-Ping session (11 females, mean age = 22.9) and twenty individuals participated in the Ping session (14 females, mean age = 23.7). Among them, 14 participants took part in both sessions, while 12 of them took part in only one session. The sample size was comparable to previous studies using similar attention tasks (<xref ref-type="bibr" rid="bib2">Baldauf and Desimone, 2014</xref>; <xref ref-type="bibr" rid="bib15">Gong and Liu, 2020a</xref>; <xref ref-type="bibr" rid="bib16">Gong and Liu, 2020b</xref>; <xref ref-type="bibr" rid="bib20">Guo et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Liu and Hou, 2013</xref>). Because our primary interest is the generalization from the perception task to the attention task, we used the minimal effect size of decoding accuracy across regions (one-sample <italic>t</italic>-tests: <italic>d</italic>=0.868) from our previous study with a similar design (<xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), and used G*Power (Version 3.1) (<xref ref-type="bibr" rid="bib13">Faul et al., 2007</xref>) to confirm that this sample size is sufficient to detect a cross-task generalization effect with a power greater than 95% (<italic>a</italic> = 0.05). All participants were right-handed and had a normal or corrected-to-normal vision. Participants provided written informed consent according to the study protocol approved by the Institutional Review Board at Zhejiang University (2020-06-001). They were paid ¥200 (~$27.4) for their participation in each session.</p></sec><sec id="s4-2"><title>Stimuli and apparatus</title><p>Stimuli were generated using Psychtoolbox (<xref ref-type="bibr" rid="bib6">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib32">Kleiner et al., 2007</xref>) implemented in MATLAB. The stimuli were presented on an LCD monitor (resolution: 1920 × 1080,, refresh rate: 60 Hz) during behavioral training, at a viewing distance of 90 cm in a dark room. During the fMRI scans, stimuli were projected to a screen via a MR-compatible LCD projector (PT-011, Jiexin Technology Co, Ltd, Shenzhen, China) with the same resolution and refresh rate as the LCD monitor during behavioral training. Participants viewed the screen via an angled mirror attached to the head coil at a viewing distance of 115 cm. Angular stimulus size was the same across behavioral and fMRI sessions.</p><p>The orientation stimuli were square-wave gratings (1.3 cycles per deg, duty cycle: 10%) in a circular aperture (inner radius: 1.5°; outer radius: 6°). The gratings flashed on a gray background at 10 Hz, alternating between black and white. There were two types of stimuli: two overlapping gratings oriented leftward (~135°) and rightward (~45°), or a single grating with one of the two orientations (~135° or ~45°). Here, we refer to the 45° and 135° orientations as the reference orientations. The impulse stimulus was a high-contrast, white (at the maximum projector output level) circular disk that covered the same area as the orientation stimulus (radius: 6°).</p></sec><sec id="s4-3"><title>Experimental procedures and tasks</title><p>Each participant completed at least two fMRI sessions on different days. One session was used for defining ROIs (see Definition of ROIs), while the remaining sessions were used for the main experiment (see Attention task and Perception task). Before the scanning sessions, participants were trained to familiarize themselves with the tasks in a separate behavioral session. The procedures and tasks were similar to our previous work (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>).</p><sec id="s4-3-1"><title>Attention task</title><p>We used a cueing paradigm (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each trial began with a color cue (red or green) for 0.5 s to indicate the reference orientation of the upcoming target (leftward vs. rightward orientation). In the No-Ping session, the cue was followed by a blank display during the preparation period; in the Ping session, a task-irrelevant, high-luminance visual impulse (‘ping’, 0.1 s) occurred at either 0.5 s (for short delays of 1.5 and 3.5 s) or 2.5 s (for long delays of 5.5 and 7.5 s) after the onset of the cue display. The orders of these sessions were counterbalanced across participants who completed both. Following the preparatory period, two superimposed gratings were then shown for 1 s. The target grating was shown with a small angular offset with respect to the cued reference orientation, whereas the distractor grating was shown in the uncued reference orientation (e.g., if rightward orientation was cued, the rightward grating was shown in 45° ± d and the leftward grating was shown in 135°). Note that the angular offset was determined individually based on the threshold obtained during the training session (at least 3 blocks, 30 trials/block), using a staircase procedure (Best Parameter Estimation by Sequential Testing, Best PEST), as implemented in the Palamedes Toolbox (<xref ref-type="bibr" rid="bib46">Prins and Kingdom, 2009</xref>). Participants used a keypad to report whether the attended orientation was more leftward or rightward relative to the reference orientation. Each trial was separated by an inter-trial interval of 3–7 s (2 s per step). Trial-by-trial feedback (‘correct’ or ‘incorrect’) was provided in the training session but not during scanning. Instead, the percentage of correct responses was provided at the end of each run in the scanning session to avoid the impact of trial-level feedback on neural activity.</p><p>Given the need to maximize the number of trials for fMRI-based MVPA, we could not accommodate additional conditions (e.g., neutral cue) to measure the behavioral effects of attention. However, our prior work using similar feature cueing paradigms (<xref ref-type="bibr" rid="bib37">Liu et al., 2007</xref>; <xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>) found that attentional cueing improved behavioral performance relative to a neutral condition. Thus, it is highly likely that our well-trained participants used the cue to direct their attention in the fMRI experiment. Furthermore, our neural measures of attentional signals revealed feature-specific attentional modulations, further validating our approach. To prevent the cue-related sensory difference from contributing to neural activity, we reversed the mapping between colors and orientations halfway through the experiment (e.g., red indicated ‘attend leftward orientation’ and green indicated ‘attend rightward orientation’ in the first half of the runs, and vice versa for the second half of the runs), with the order counterbalanced across subjects. The mapping of colors and orientations was reversed only once in the middle of the experiment to prevent misremembering of the color-orientation associations. To reduce temporal expectancy over a fixed period, the preparatory period (i.e., cue-to-stimulus interval) varied from 1.5 to 7.5 s with different probabilities (10% for 1.5 or 3.5 s each, 40% for 5.5 or 7.5 s each). The long-delay trials (5.5 or 7.5 s) were selected for subsequent analyses, as they allow the separation of the preparatory activity from the grating-evoked response during fMRI scanning. The short-delay trials were included to encourage a sustained maintenance of attention throughout the entire preparation period.</p></sec><sec id="s4-3-2"><title>Perception task</title><p>On each trial of the perception task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), a single grating was shown for 1 s, followed by an inter-trial interval between 3 and 7 s. To equate the sensory inputs between attention and perception tasks, the orientation was shifted away from the reference orientation by the same angular offset as that used in the attention task with each individual participant’s own threshold. Participants performed the same orientation discrimination task by comparing the single orientation to the reference orientation. We provided the percentage of correct responses at the end of each run as feedback.</p></sec></sec><sec id="s4-4"><title>Eye tracking and analysis</title><p>To evaluate the stability of visual fixation, we used Eyelink Portable Duo system (SR Research, Ontario, Canada) to monitor each observer’s eye position during the training session at a sampling rate of 500 Hz. One participant’s data was not used due to the unstable recording of the eye. The data were then analyzed using custom Matlab code.</p><p>To examine whether participants adopted a space-based strategy during the preparatory period in the attention task, such as directing their gaze leftward in attend leftward trials, and vice versa for the attend rightward trials, we analyzed the participants’ eye positions recorded during the training session. Horizontal and vertical eye positions were analyzed separately. Paired <italic>t</italic>-tests were performed to compare horizontal and vertical eye positions between two attention conditions. A two-way mixed ANOVA (2 sessions × 2 attended orientations) was applied to the horizontal and vertical positions, respectively.</p></sec><sec id="s4-5"><title>fMRI data acquisition</title><p>Imaging was performed on a Siemens 3T scanner (MAGNETOM Prisma, Siemens Healthcare, Erlangen, Germany) using a 20-channel coil at Zhejiang University (Hangzhou, China). For each participant, we acquired high-resolution T1-weighted anatomical images (field of view, 240 × 240 mm, 208 sagittal slices; 0.9 mm<sup>3</sup> resolution), T2*-weighted echo-planar functional images consisting of 46 slices (TR, 2 s; TE, 34 ms; flip angle, 50°; matrix size, 80 × 80; in-plane resolution, 3 × 3 mm; slice thickness, 3 mm, interleaved, no gap) and a 2D T1-weighted anatomical image (0.8 × 0.8 × 3 mm) for aligning functional data to high-resolution anatomical data.</p></sec><sec id="s4-6"><title>fMRI data preprocessing</title><p>Data analyses were performed using mrTools (<xref ref-type="bibr" rid="bib14">Gardner et al., 2018</xref>) and custom code in Matlab. For each run, functional data were preprocessed with head motion correction, linear detrending, and temporal high-pass filtering at 0.01 Hz. Data were converted to percentage signal change by dividing the time course of each voxel by its mean signals in each run. We concatenated the six runs of the attention task and the three runs of the perception task separately for further analysis. One of the attention runs in one subject was excluded due to low accurate performance (&lt;50%).</p></sec><sec id="s4-7"><title>Definition of ROIs</title><sec id="s4-7-1"><title>Visual and parietal ROIs</title><p>Following previous work (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Gong and Liu, 2020b</xref>; <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), for each observer, we ran a separate retinotopic mapping session to obtain ROIs in occipital and parietal areas. Observers viewed four runs of rotating wedges (i.e., clockwise and counterclockwise) and two runs of rings (i.e., expanding and contracting) to map the polar angle and radial components, respectively (<xref ref-type="bibr" rid="bib10">DeYoe et al., 1996</xref>; <xref ref-type="bibr" rid="bib12">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="bib55">Sereno et al., 1995</xref>). Borders between areas were defined as the phase reversals in a polar angle map of the visual field. Phase maps were visualized on computationally flattened representations of the cortical surface, which were generated from the high-resolution anatomical image using FreeSurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</ext-link>) and custom Matlab code.</p><p>To help identify the topographic areas in parietal areas, we ran two runs of memory-guided saccade task modeled after previous studies (<xref ref-type="bibr" rid="bib34">Konen and Kastner, 2008</xref>; <xref ref-type="bibr" rid="bib51">Schluppeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib56">Sereno et al., 2001</xref>). Observers fixated at the screen center while a peripheral (~10° radius) target dot was flashed for 500 ms. The flashed target was quickly masked by a ring of 100 distractor dots randomly positioned within an annulus (8.5° – 10.5°). The mask remained on screen for 3 s, after which participants were instructed to make a saccade to the memorized target position, then immediately saccade back to the central fixation. The position of the peripheral target shifted around the annulus from trial to trial in either a clockwise or counterclockwise order. Data from the memory-guided saccade task were analyzed using the same phase encoding method as the wedge and ring data. Therefore, the following ROIs in each hemisphere were identified after the completion of this session: V1, V2, V3, V3A/B, V4, V7/IPS0, and IPS1–IPS4.</p></sec><sec id="s4-7-2"><title>Frontal ROIs</title><p>Following previous work (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Gong and Liu, 2020b</xref>; <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), we used a deconvolution approach by fitting each voxel’s time series from the attention task with a general linear model (GLM) to determine the event-related activations in the brain (see Supplementary materials: Deconvolution). For each voxel, we computed the goodness of fit measure (<italic>r</italic><sup>2</sup> value), which indicates the amount of variance explained by the deconvolution model (<xref ref-type="bibr" rid="bib14">Gardner et al., 2018</xref>). The <italic>r</italic><sup>2</sup> value represents the degree to which the voxel’s time series is correlated with the task events, regardless of any differential responses among conditions. Based on the task-related activation (as indexed by <italic>r</italic><sup>2</sup> value) and anatomical criteria, we defined two frontal areas in each hemisphere that were active during the attention task: one is located superior to the precentral sulcus and near the superior frontal sulcus (FEF) and the other is located toward the inferior precentral sulcus, close to the junction with the inferior frontal sulcus (IFJ).</p></sec><sec id="s4-7-3"><title>Groups of region</title><p>To characterize the patterns of neural response across cortical hierarchy and streamline data presentation, we grouped results from the nine areas into four groups based on functional and anatomical considerations: primary visual cortex (V1); EVC, consisting of V2, V3, V3ab, and V4; IPS, consisting of IPS0–IPS4; PFC, consisting of FEF and IFJ. Individual areas within each group exhibited qualitatively similar results.</p><p>Note that we analyzed V1 separately for two reasons. First, previous studies consistently identify V1 as the main locus of sensory-like templates during feature-specific preparatory attention (<xref ref-type="bibr" rid="bib33">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). Second, V1 shows the strongest orientation selectivity within the visual hierarchy (<xref ref-type="bibr" rid="bib45">Priebe, 2016</xref>). In contrast, the EVC (comprising V2, V2, V3AB, and V4) demonstrates broader selectivity for complex features (<xref ref-type="bibr" rid="bib18">Grill-Spector and Malach, 2004</xref>). Therefore, it would be particularly informative to analyze V1 separately for our orientation-based attention paradigm.</p></sec></sec><sec id="s4-8"><title>Multivoxel pattern analysis</title><sec id="s4-8-1"><title>Decoding of attended orientation</title><p>To test if multivariate patterns of activity represent information of the attended orientation, separate MVPA analyses were applied on the activity patterns for the preparation and stimulus selection periods. Following previous work (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Gong and Liu, 2020b</xref>; <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), for this analysis, we extracted fMRI signals from raw time series in the long delay trials with correct behavioral responses (~72 trials per attention condition); short-delay trials were excluded as they could not provide enough data points to measure preparatory activity. We then obtained averaged BOLD response in a 2-s window for each voxel and each trial in a given ROI, separately for preparatory activity (4–6 s after the onset of the cue) and stimulus-evoked activity (4–6 s after the onset of the gratings). The response amplitudes across two attention conditions in each ROI were further z-normalized, separately for the preparation and stimulus-related activity. These normalized single-trial BOLD responses were used for the MVPA. We trained a classifier using the Fisher linear discriminant (FLD) analysis to discriminate between two attended orientations (leftward vs. rightward) and tested its performance with a leave-one-run-out cross-validation scheme. This process was repeated until each run was tested once and the decoding accuracy (i.e., the proportion of correctly classified trials) was averaged across the cross-validation folds. The statistical significance of decoding accuracy was evaluated by comparing it to the chance level obtained from a permutation test (see Permutation test). To assess if the decoding accuracy differed between No-Ping and Ping experiments, we performed two-way mixed ANOVAs (2 sessions × 4 regions) on the decoding accuracy.</p></sec><sec id="s4-8-2"><title>Cross-task generalization from the perception task to attention task</title><p>Following previous work (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), to test whether the neural patterns in the preparatory and stimulus selection periods from the attention task reflected sensory processing of isolated features, we trained an FLD classifier using the normalized BOLD responses from the perception task (4–6 s after the trial onset) to discriminate leftward versus rightward orientation. Then, we tested this classifier on the normalized response from the independent runs of the attention task to discriminate between attend leftward versus attend rightward orientations, separately for preparation and stimulus selection periods. The significance of decoding accuracy was compared to the chance level obtained from a permutation test (see Permutation test). To assess if the generalization performance differed between No-Ping and Ping sessions, we performed two-way mixed ANOVAs (2 sessions × 4 regions) on the decoding accuracy.</p></sec><sec id="s4-8-3"><title>Neural distance between attended and perceived orientations</title><p>The decoding accuracy from the cross-task generalization test reflects a discretized readout of the pattern similarity between different conditions. However, employing continuous similarity measures, such as Mahalanobis distance (<xref ref-type="bibr" rid="bib39">Mahalanobis, 1936</xref>), could be more reliable compared to decoding accuracy (<xref ref-type="bibr" rid="bib65">Walther et al., 2016</xref>). Therefore, we calculated the Mahalanobis distance to quantify the pattern similarity between two attended orientations and two perceived orientations. For each participant and each ROI, we have M points (i.e., M trials for each attended orientation) in the <italic>N</italic>-dimensional space (<italic>N</italic> = 100, number of voxels). For each data point in the attended orientation condition, we computed its distance to each of the orientation distributions (from the perception task). Averaged distance values were then calculated for each combination of attended orientation and perceived orientation pairs. A sensory-like hypothesis would predict smaller distance between the distribution of the attended orientation (e.g., attend leftward) and the distribution of corresponding orientation (e.g., perceive leftward) compared to the alternative orientation (e.g., perceive rightward). Two-way repeated-measures ANOVA (2 attended orientations × 2 perceived orientations) was applied on the Mahalanobis distance, separately for each region and each session.</p></sec><sec id="s4-8-4"><title>Neural–behavioral relationships</title><p>We tested if the representation format during preparatory attention was associated with subsequent behavior. For each trial, we calculated the Mahalanobis distance between the attention conditions (attend leftward and attend rightward) and the perceived orientations (leftward and rightward orientation). We estimated the AMI based on these distance values. This index measures how much attention modulated the pattern similarity for the Same orientation condition (e.g., attend and perceive the same orientation) relative to the Different orientation condition (e.g., attend and perceive different orientations). The index was calculated as follows: AMI = (<italic>D</italic><sub>different</sub> – <italic>D</italic><sub>same</sub>)/(<italic>D</italic><sub>different</sub> + <italic>D</italic><sub>same</sub>), where <italic>D</italic><sub>same</sub> and <italic>D</italic><sub>different</sub> are the measured distance (<italic>D</italic>) in the Same and Different orientation condition, respectively. Next, we tested the behavioral relevance of AMI in two ways: (1) Inter-subject analysis: correlated AMI with RT and accuracy across participants, separately for each session; (2) Within-subject analysis: for each participant, we sorted the single-trial AMI values in descending order and selected top-ranked 25% trials and bottom-ranked 25% trials to represent ‘strong modulation’ and ‘weak modulation’ trials, respectively. We then extracted behavioral responses on these selected trials and calculated RT and accuracy for each trial type. Paired <italic>t</italic>-tests were used to compare between ‘strong modulation’ and ‘weak modulation’ trials in each session.</p></sec><sec id="s4-8-5"><title>IC analysis</title><p>We used IC to examine shared changes in pattern discriminability over time, a method that allows inference based on multivoxel pattern information rather than overall BOLD response (<xref ref-type="bibr" rid="bib26">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="bib43">Ng et al., 2021</xref>). To track the flow of multivariate information across time (i.e., across trials), we measured the fluctuations (covariance) in pattern-based discriminability by calculating the Mahalanobis distance of each trial to the two attended orientations, using a leave-one-run-out cross-validation scheme. For each ROI, we calculated the Mahalanobis distance between the pattern of activity for each attention trial from one left-out run and the distribution of each attended orientation of the remaining runs. To quantify the degree of attentional modulation, we calculated the AMI using the same formula as mentioned above, where <italic>D</italic><sub>same</sub> and <italic>D</italic><sub>different</sub> are the measured distance (<italic>D</italic>) in the Same and Different condition. This index measures how much the pattern similarity increased for the same attention condition (e.g., attend leftward to attend leftward) relative to the different attention condition (e.g., attend leftward to attend rightward). A positive AMI indicates relative proximity to the same attention condition, whereas a negative AMI indicates relative proximity to the different attention condition. A time course of AMI values was generated across runs and pairwise correlated between ROIs using Pearson correlation analysis and Fisher <italic>z</italic>-transformed. Independent <italic>t</italic>-tests were used to compare the connectivity between No-Ping and Ping sessions. To assess the relationship between ICs and behavior, we correlated ICs with RT and accuracy across participants, separately for each session.</p></sec></sec><sec id="s4-9"><title>Permutation test to evaluate classifier performance</title><p>Following previous work (<xref ref-type="bibr" rid="bib29">Jigo et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Gong et al., 2022</xref>), for each brain area, we evaluated the statistical significance of the observed decoding accuracy using a permutation test scheme. We first shuffled the trial labels in the training data and trained the same FLD classifier on the shuffled data. We then tested the classifier on the (unshuffled) test data to obtain decoding accuracy. For each ROI and each participant, we repeated this procedure 1000 times to compute a null distribution of decoding accuracy. To compute the group-level significance, we averaged the 20 null distributions to obtain a single null distribution of 1000 values for each ROI. To determine if the observed decoding accuracy significantly exceeds the chance level, we compared the observed value to the 95 percentiles of this group-level distribution (corresponding to p = 0.05). Note that these ROIs were pre-defined with strong priors as their activation in attention tasks has been consistently reported in the literature. Nevertheless, for those analyses where multiple comparisons were performed across regions, we applied a Bonferroni correction to adjust the p-values.</p></sec><sec id="s4-10"><title>Bayesian analysis</title><p>To evaluate the strength of evidence for the null hypothesis, we conducted Bayesian analyses (<xref ref-type="bibr" rid="bib64">Wagenmakers, 2007</xref>) using standard priors as implemented in JASP Version 0.17.1 (<xref ref-type="bibr" rid="bib25">JASP Team, 2023</xref>). We performed Bayesian <italic>t</italic>-tests and computed Bayes factor (BF<sub>01</sub>) to compare between two attention conditions (attend leftward vs. attend rightward). Additionally, we used Bayesian repeated-measures ANOVA and computed the exclusion Bayes factors (BF<sub>excl</sub>) to assess the evidence for excluding specific effects across all models. A Bayes factor (BF) greater than 1 provides support for the null hypothesis. Specifically, a BF between 1 and 3 indicates weak evidence, a BF between 3 and 10 indicates moderate evidence, and a BF greater than 10 indicates strong evidence (<xref ref-type="bibr" rid="bib63">van Doorn et al., 2021</xref>).</p></sec><sec id="s4-11"><title>Approach to handle partially overlapped samples</title><p>Our study used partially overlapping samples, with 14 out of 20 participants completing both No-Ping and Ping sessions, while the remainder completed one of the two sessions. The most important analyses entailed assessing whether decoding accuracy was above chance, for which we used the permutation-based method (see above) within each session. Thus, these analyses were unaffected by the partially overlapping samples. In a few analyses where we compared across sessions, we used statistical tests treating ‘session’ as a between-subject factor. We believe this is a reasonable approach, as a between-subject test is more conservative than a within-subject test, such that any significant effect emerged should be a genuine effect. To be certain, we also conducted additional analyses with ‘session’ as a within-subject factor on the subset of data from the 14 participants who completed both sessions in a counterbalanced order. The results were highly similar to those reported in the main text.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con2"><p>Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Participants provided written informed consent according to the study protocol approved by the Institutional Review Board at Zhejiang University (2020-06-001).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103425-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data, analyses, and task codes have been made publicly available via the Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://osf.io/ghaxv/">https://osf.io/ghaxv/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Data from dual-format attentional template during preparation</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/RDQFS</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by National Science and Technology Innovation 2030—Major Project 2021ZD0200409, National Natural Science Foundation of China (32371087, 32300855, 3200784), Fundamental Research Funds for the Central University (226-2024-00118), a grant from the MOE Frontiers Science Center for Brain Science &amp; Brain-Machine Integration at Zhejiang University and Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences 2023-PT310-01.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>F</given-names></name><name><surname>Menelaou</surname><given-names>G</given-names></name><name><surname>Warrington</surname><given-names>O</given-names></name><name><surname>Koolschijn</surname><given-names>RS</given-names></name><name><surname>Corbin</surname><given-names>N</given-names></name><name><surname>Callaghan</surname><given-names>MF</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prior expectations evoke stimulus-specific activity in the deep layers of the primary visual cortex</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3001023</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001023</pub-id><pub-id pub-id-type="pmid">33284791</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>D</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Lozano-Soldevilla</surname><given-names>D</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pinging the brain with visual impulses reveals electrically active, not activity-silent, working memories</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001436</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001436</pub-id><pub-id pub-id-type="pmid">34673775</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battistoni</surname><given-names>E</given-names></name><name><surname>Stein</surname><given-names>T</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Preparatory attention in visual cortex</article-title><source>Annals of the New York Academy of Sciences</source><volume>1396</volume><fpage>92</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1111/nyas.13320</pub-id><pub-id pub-id-type="pmid">28253445</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bettencourt</surname><given-names>KC</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/nn.4174</pub-id><pub-id pub-id-type="pmid">26595654</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bressler</surname><given-names>SL</given-names></name><name><surname>Tang</surname><given-names>W</given-names></name><name><surname>Sylvester</surname><given-names>CM</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Top-down control of human visual cortex by frontal and parietal cortex in anticipatory visual spatial attention</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>10056</fpage><lpage>10061</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1776-08.2008</pub-id><pub-id pub-id-type="pmid">18829963</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>3</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id><pub-id pub-id-type="pmid">11994752</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeYoe</surname><given-names>EA</given-names></name><name><surname>Carman</surname><given-names>GJ</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name><name><surname>Glickman</surname><given-names>S</given-names></name><name><surname>Wieser</surname><given-names>J</given-names></name><name><surname>Cox</surname><given-names>R</given-names></name><name><surname>Miller</surname><given-names>D</given-names></name><name><surname>Neitz</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Mapping striate and extrastriate visual areas in human cerebral cortex</article-title><source>PNAS</source><volume>93</volume><fpage>2382</fpage><lpage>2386</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.6.2382</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>DH</given-names></name><name><surname>van Moorselaar</surname><given-names>D</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Pinging the brain to reveal the hidden attentional priority map using encephalography</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>4749</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-40405-8</pub-id><pub-id pub-id-type="pmid">37550310</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>SA</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id><pub-id pub-id-type="pmid">9087826</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Lang</surname><given-names>AG</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>G*Power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behavior Research Methods</source><volume>39</volume><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3758/bf03193146</pub-id><pub-id pub-id-type="pmid">17695343</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Besle</surname><given-names>J</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>MrTools: analysis and visualization package for functional magnetic resonance imaging data</data-title><version designator="4.7">4.7</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1299483">https://doi.org/10.5281/zenodo.1299483</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Biased neural representation of feature-based attention in the human frontoparietal network</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>8386</fpage><lpage>8395</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0690-20.2020</pub-id><pub-id pub-id-type="pmid">33004380</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Continuous and discrete representations of feature-based attentional priority in human frontoparietal network</article-title><source>Cognitive Neuroscience</source><volume>11</volume><fpage>47</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1080/17588928.2019.1601074</pub-id><pub-id pub-id-type="pmid">30922203</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Preparatory attention to visual features primarily relies on non-sensory representation</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>21726</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-26104-2</pub-id><pub-id pub-id-type="pmid">36526653</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The human visual cortex</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>649</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144220</pub-id><pub-id pub-id-type="pmid">15217346</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grubert</surname><given-names>A</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The time course of target template activation processes during preparation for visual search</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9527</fpage><lpage>9538</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0409-18.2018</pub-id><pub-id pub-id-type="pmid">30242053</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>F</given-names></name><name><surname>Preston</surname><given-names>TJ</given-names></name><name><surname>Das</surname><given-names>K</given-names></name><name><surname>Giesbrecht</surname><given-names>B</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Feature-independent neural coding of target detection during search of natural scenes</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>9499</fpage><lpage>9510</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5876-11.2012</pub-id><pub-id pub-id-type="pmid">22787035</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamblin-Frohman</surname><given-names>Z</given-names></name><name><surname>Becker</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The attentional template in high and low similarity search: Optimal tuning or tuning to relations?</article-title><source>Cognition</source><volume>212</volume><elocation-id>104732</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2021.104732</pub-id><pub-id pub-id-type="pmid">33862440</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hout</surname><given-names>MC</given-names></name><name><surname>Goldinger</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Target templates: the precision of mental representations affects attentional guidance and decision-making in visual search</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>77</volume><fpage>128</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.3758/s13414-014-0764-6</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hout</surname><given-names>MC</given-names></name><name><surname>Robbins</surname><given-names>A</given-names></name><name><surname>Godwin</surname><given-names>HJ</given-names></name><name><surname>Fitzsimmons</surname><given-names>G</given-names></name><name><surname>Scarince</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Categorical templates are more useful when features are consistent: Evidence from eye movements during search for societally important vehicles</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>79</volume><fpage>1578</fpage><lpage>1592</lpage><pub-id pub-id-type="doi">10.3758/s13414-017-1354-1</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><collab>JASP Team</collab></person-group><year iso-8601-date="2023">2023</year><data-title>JASP</data-title><source>Computer Software</source><ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/">https://jasp-stats.org/</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>K</given-names></name><name><surname>Zamboni</surname><given-names>E</given-names></name><name><surname>Kemper</surname><given-names>V</given-names></name><name><surname>Rua</surname><given-names>C</given-names></name><name><surname>Goncalves</surname><given-names>NR</given-names></name><name><surname>Ng</surname><given-names>AKT</given-names></name><name><surname>Rodgers</surname><given-names>CT</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent processing drives perceptual plasticity</article-title><source>Current Biology</source><volume>30</volume><fpage>4177</fpage><lpage>4187</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.08.016</pub-id><pub-id pub-id-type="pmid">32888488</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>K</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Ultra-high field imaging of human visual cognition</article-title><source>Annual Review of Vision Science</source><volume>9</volume><fpage>479</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-111022-123830</pub-id><pub-id pub-id-type="pmid">37137282</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Steinwurzel</surname><given-names>C</given-names></name><name><surname>Ziminski</surname><given-names>JJ</given-names></name><name><surname>Xi</surname><given-names>Y</given-names></name><name><surname>Emir</surname><given-names>U</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Recurrent inhibition refines mental templates to optimize perceptual decisions</article-title><source>Science Advances</source><volume>10</volume><elocation-id>eado7378</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.ado7378</pub-id><pub-id pub-id-type="pmid">39083601</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jigo</surname><given-names>M</given-names></name><name><surname>Gong</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural determinants of task performance during feature-based attention in human cortex</article-title><source>eNeuro</source><volume>5</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1523/ENEURO.0375-17.2018</pub-id><pub-id pub-id-type="pmid">29497703</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Increased activity in human visual cortex during directed attention in the absence of visual stimulation</article-title><source>Neuron</source><volume>22</volume><fpage>751</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80734-5</pub-id><pub-id pub-id-type="pmid">10230795</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The precision of attentional selection is far worse than the precision of the underlying memory representation</article-title><source>Cognition</source><volume>186</volume><fpage>20</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2019.02.001</pub-id><pub-id pub-id-type="pmid">30739056</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Failing</surname><given-names>MF</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Prior expectations evoke stimulus templates in the primary visual cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>1546</fpage><lpage>1554</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00562</pub-id><pub-id pub-id-type="pmid">24392894</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konen</surname><given-names>CS</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representation of eye movements and stimulus motion in topographically organized areas of human posterior parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>8361</fpage><lpage>8375</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1930-08.2008</pub-id><pub-id pub-id-type="pmid">18701699</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>G</given-names></name><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Van der Burg</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Orientation categories used in guidance of attention in visual search can differ in strength</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>79</volume><fpage>2246</fpage><lpage>2256</lpage><pub-id pub-id-type="doi">10.3758/s13414-017-1387-5</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name><name><surname>Drysdale</surname><given-names>AT</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural evidence for the flexible control of mental representations</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3303</fpage><lpage>3313</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu130</pub-id><pub-id pub-id-type="pmid">24935778</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Larsson</surname><given-names>J</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Feature-based attention modulates orientation-selective responses in human visual cortex</article-title><source>Neuron</source><volume>55</volume><fpage>313</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.030</pub-id><pub-id pub-id-type="pmid">17640531</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Hou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A hierarchy of attentional priority signals in human frontoparietal cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>16606</fpage><lpage>16616</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1780-13.2013</pub-id><pub-id pub-id-type="pmid">24133264</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mahalanobis</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="1936">1936</year><article-title>On the generalised distance in statistics</article-title><conf-name>Proceedings of the National Institute of Sciences of India</conf-name><fpage>49</fpage><lpage>55</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malcolm</surname><given-names>GL</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The effects of target template specificity on visual search in real-world scenes: evidence from eye movements</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/9.11.8</pub-id><pub-id pub-id-type="pmid">20053071</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Walther</surname><given-names>L</given-names></name><name><surname>Wallis</surname><given-names>G</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Temporal dynamics of attention during encoding versus maintenance of working memory: complementary views from event-related potentials and alpha-band oscillations</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>492</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00727</pub-id><pub-id pub-id-type="pmid">25244118</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>AKT</given-names></name><name><surname>Jia</surname><given-names>K</given-names></name><name><surname>Goncalves</surname><given-names>NR</given-names></name><name><surname>Zamboni</surname><given-names>E</given-names></name><name><surname>Kemper</surname><given-names>VG</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Welchman</surname><given-names>AE</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Ultra-high-field neuroimaging reveals fine-scale processing for 3D perception</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>8362</fpage><lpage>8374</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0065-21.2021</pub-id><pub-id pub-id-type="pmid">34413206</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>12125</fpage><lpage>12130</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id><pub-id pub-id-type="pmid">21730192</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priebe</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mechanisms of orientation selectivity in the primary visual cortex</article-title><source>Annual Review of Vision Science</source><volume>2</volume><fpage>85</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114456</pub-id><pub-id pub-id-type="pmid">28532362</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Prins</surname><given-names>N</given-names></name><name><surname>Kingdom</surname><given-names>FAA</given-names></name></person-group><year iso-8601-date="2009">2009</year><data-title>Palamedes: matlab routines for analyzing psychophysical data</data-title><version designator="3">3</version><source>Psychtoolbox</source><ext-link ext-link-type="uri" xlink:href="http://www.palamedestoolbox.org">http://www.palamedestoolbox.org</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pinging the brain to reveal hidden memories</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>767</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1038/nn.4560</pub-id><pub-id pub-id-type="pmid">28542151</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id><pub-id pub-id-type="pmid">31263205</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>MD</given-names></name><name><surname>Scheinost</surname><given-names>D</given-names></name><name><surname>Greene</surname><given-names>AS</given-names></name><name><surname>Avery</surname><given-names>EW</given-names></name><name><surname>Kwon</surname><given-names>YH</given-names></name><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Ramani</surname><given-names>R</given-names></name><name><surname>Qiu</surname><given-names>M</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Functional connectivity predicts changes in attention observed across minutes, days, and months</article-title><source>PNAS</source><volume>117</volume><fpage>3797</fpage><lpage>3807</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912226117</pub-id><pub-id pub-id-type="pmid">32019892</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rungratsameetaweemana</surname><given-names>N</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissociating the impact of attention and expectation on early sensory processing</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.014</pub-id><pub-id pub-id-type="pmid">31022561</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Sustained activity in topographic areas of human posterior parietal cortex during memory-guided saccades</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>5098</fpage><lpage>5108</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5330-05.2006</pub-id><pub-id pub-id-type="pmid">16687501</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneegans</surname><given-names>S</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Restoration of fMRI decodability does not imply latent working memory states</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>1977</fpage><lpage>1994</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01180</pub-id><pub-id pub-id-type="pmid">28820674</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scolari</surname><given-names>M</given-names></name><name><surname>Byers</surname><given-names>A</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Optimal deployment of attentional gain during fine discriminations</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>7723</fpage><lpage>7733</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5558-11.2012</pub-id><pub-id pub-id-type="pmid">22649250</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus-specific delay activity in human primary visual cortex</article-title><source>Psychological Science</source><volume>20</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02276.x</pub-id><pub-id pub-id-type="pmid">19170936</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science</source><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1126/science.7754376</pub-id><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Pitzalis</surname><given-names>S</given-names></name><name><surname>Martinez</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans</article-title><source>Science</source><volume>294</volume><fpage>1350</fpage><lpage>1354</lpage><pub-id pub-id-type="doi">10.1126/science.1063695</pub-id><pub-id pub-id-type="pmid">11701930</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheremata</surname><given-names>SL</given-names></name><name><surname>Somers</surname><given-names>DC</given-names></name><name><surname>Shomstein</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual short-term memory activity in parietal lobe reflects cognitive processes beyond attentional selection</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1511</fpage><lpage>1519</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1716-17.2017</pub-id><pub-id pub-id-type="pmid">29311140</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>M</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Shape-specific preparatory activity mediates attention to targets in human visual cortex</article-title><source>PNAS</source><volume>106</volume><fpage>19569</fpage><lpage>19574</lpage><pub-id pub-id-type="doi">10.1073/pnas.0905306106</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>‘Activity-silent’ working memory in prefrontal cortex: a dynamic coding framework</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>394</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Expectation (and attention) in visual cognition</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>403</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>745</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1038/nrn3838</pub-id><pub-id pub-id-type="pmid">25315388</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Feature-based attention and feature-based expectation</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>401</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.03.008</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Doorn</surname><given-names>J</given-names></name><name><surname>van den Bergh</surname><given-names>D</given-names></name><name><surname>Böhm</surname><given-names>U</given-names></name><name><surname>Dablander</surname><given-names>F</given-names></name><name><surname>Derks</surname><given-names>K</given-names></name><name><surname>Draws</surname><given-names>T</given-names></name><name><surname>Etz</surname><given-names>A</given-names></name><name><surname>Evans</surname><given-names>NJ</given-names></name><name><surname>Gronau</surname><given-names>QF</given-names></name><name><surname>Haaf</surname><given-names>JM</given-names></name><name><surname>Hinne</surname><given-names>M</given-names></name><name><surname>Kucharský</surname><given-names>Š</given-names></name><name><surname>Ly</surname><given-names>A</given-names></name><name><surname>Marsman</surname><given-names>M</given-names></name><name><surname>Matzke</surname><given-names>D</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Sarafoglou</surname><given-names>A</given-names></name><name><surname>Stefan</surname><given-names>A</given-names></name><name><surname>Voelkel</surname><given-names>JG</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The JASP guidelines for conducting and reporting a Bayesian analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>28</volume><fpage>813</fpage><lpage>826</lpage><pub-id pub-id-type="doi">10.3758/s13423-020-01798-5</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A practical solution to the pervasive problems ofp values</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>14</volume><fpage>779</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.3758/BF03194105</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>T</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Mitchell</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The time-course of component processes of selective attention</article-title><source>NeuroImage</source><volume>199</volume><fpage>396</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.067</pub-id><pub-id pub-id-type="pmid">31150787</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name><name><surname>Friedman-Hill</surname><given-names>SR</given-names></name><name><surname>Stewart</surname><given-names>MI</given-names></name><name><surname>O’Connell</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The role of categorization in visual search for orientation</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>18</volume><fpage>34</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.18.1.34</pub-id><pub-id pub-id-type="pmid">1532193</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Guided Search 6.0:an updated model of visual search</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>28</volume><fpage>1060</fpage><lpage>1092</lpage><pub-id pub-id-type="doi">10.3758/s13423-020-01859-9</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Revealing hidden states in visual working memory using electroencephalography</article-title><source>Frontiers in Systems Neuroscience</source><volume>9</volume><elocation-id>123</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00123</pub-id><pub-id pub-id-type="pmid">26388748</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Jochim</surname><given-names>J</given-names></name><name><surname>Akyürek</surname><given-names>EG</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic hidden states underlying working-memory-guided behavior</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>864</fpage><lpage>871</lpage><pub-id pub-id-type="doi">10.1038/nn.4546</pub-id><pub-id pub-id-type="pmid">28414333</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Kandemir</surname><given-names>G</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Akyürek</surname><given-names>EG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unimodal and bimodal access to sensory working memories by auditory and visual impulses</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>671</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1194-19.2019</pub-id><pub-id pub-id-type="pmid">31754009</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Attentional guidance and match decisions rely on different template information during visual search</article-title><source>Psychological Science</source><volume>33</volume><fpage>105</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1177/09567976211032225</pub-id><pub-id pub-id-type="pmid">34878949</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Becker</surname><given-names>SI</given-names></name><name><surname>Boettcher</surname><given-names>SEP</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Good-enough attentional guidance</article-title><source>Trends in Cognitive Sciences</source><volume>27</volume><fpage>391</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2023.01.007</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Feature-specific reactivations of past information shift current neural encoding thereby mediating serial bias behaviors</article-title><source>PLOS Biology</source><volume>21</volume><elocation-id>e3002056</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002056</pub-id><pub-id pub-id-type="pmid">36961821</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Univariate analysis</title><sec sec-type="appendix" id="s8-1"><title>Deconvolution</title><p>For the data in the attention task, we used a deconvolution approach by fitting each voxel’s time series with a general linear model (GLM) with seven regressors, four corresponding to the long delay (5.5 and 7.5 s) trials with correct responses (attended orientation × delay), two corresponding to the short delay (1.5 and 3.5 s) trials with correct responses, and the remaining four regressors corresponding to incorrect trials for each length of the delay. Each trial was modeled by a set of 12 finite impulse response functions (FIR) after the trial onset. For the perception task data, we used a GLM with two regressors, corresponding to two features (leftward vs. rightward). Each trial was modeled by a set of eight FIR after the trial onset. The design matrix was pseudo-inversed and multiplied by the time series to obtain an estimate of the hemodynamic response evoked by each condition. Correct trials from the attention task and all trials from the perception task were entered into further univariate and multivariate analysis.</p></sec><sec sec-type="appendix" id="s8-2"><title>Attentional modulations on BOLD response</title><p>To select active voxels during the task, we discarded noisy voxels with mean responses larger than 10% signal change. Because the stimuli were presented at the center of the display, areas across two hemispheres were combined. Using the <italic>r</italic><sup>2</sup> values from the deconvolution analysis to index the relevance of each voxel to the task, voxels in each brain area were sorted in descending order of their <italic>r</italic><sup>2</sup> values. The top-ranked 100 voxels in each combined ROI were then selected for further analyses. The results remained highly similar when using different numbers of voxels (e.g., 80 or 120).</p><p>To assess whether feature-based attention modulated overall BOLD response during the preparatory and stimulus selection periods, we averaged the fMRI response across voxels for long-delay trials of the attention task, separately for each ROI, each attention condition, and each time point. Then, we averaged a time window of 4–6 s after the onset of the cue to index the preparatory activity, and averaged a time window of 4–6 s after the onset of superimposed gratings to index the stimulus-evoked response. To test the influence of attention in different brain regions, we applied two-way repeated-measures ANOVAs (attended orientation × region) on BOLD response, separately for activity during preparatory and stimulus selection periods.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Univariate results</title><p>During the No-Ping session, when examining the group-level fMRI time courses, we observed slightly elevated activity during the preparation period, particularly in higher-order frontoparietal areas, followed by a robust response to the superimposed gratings (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). To examine whether feature-based attention modulated overall BOLD in the attention task across regions, we conducted two-way repeated-measures ANOVAs (attended orientation × region) on the BOLD response during preparation and stimulus periods, separately. The results showed a main effect of region in the preparation period (<italic>F</italic>(3,57) = 6.748; p = 0.001; <italic>η</italic><sub>p</sub><sup>2</sup> = 0.262) and stimulus periods (<italic>F</italic>(3,57) = 38.230; p &lt; 0.001; <italic>η</italic><sub>p</sub><sup>2</sup> = 0.668). No other effects were significant in both periods (preparation: ps &gt; 0.526; stimulus: ps &gt; 0.135). These results demonstrate stronger response in higher-order frontoparietal areas than in sensory areas. The lack of attention modulation on BOLD response suggests that attending to specific features did not influence mean neural activity during the preparation and stimulus selection periods. These results are consistent with our previous studies using similar paradigms.</p><p>The presence of visual impulse during preparation in the Ping session yielded a similar time course of neural activity (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Consistent with the observation in No-Ping session, two-way repeated-measures ANOVAs revealed a main effect of region in both the preparation (<italic>F</italic>(3,57) = 3.445; p = 0.023; <italic>η</italic><sub>p</sub><sup>2</sup> = 0.153) and stimulus periods (<italic>F</italic>(3,57) = 29.911; p &lt; 0.001; <italic>η</italic><sub>p</sub><sup>2</sup> = 0.612). No other effects were significant in both periods (preparation: ps &gt; 0.462; stimulus: ps &gt; 0.063). These results suggest that a briefly presented impulse stimulus caused little changes to the neural response amplitude and promoted us to further investigate multivariate neural activity patterns.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Univariate BOLD results.</title><p>(<bold>A</bold>). Mean fMRI time course from four regions (V1, EVC, IPS, and PFC) for No-Ping session and (<bold>B</bold>) Ping session. The gray triangle indicates the onset of superimposed gratings. The gray area indicates a slightly elevated response in the presence of visual impulse. Error bars denote standard error of the means.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-app1-fig1-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10"><title>Analysis of eye movement</title><p>Although participants were required to maintain stable fixation in our experiment, they might have shifted their spatial attention differently when preparing to attend different features (e.g., eyes moved toward leftward or rightward). To assess the potential influence of overt spatial attention in our results, we analyzed the participants’ eye position recorded during the training session and observed no significant difference between attended features during preparation in the No-Ping (p = 0.328, BF<sub>01</sub> = 2.760 for horizontal; p = 0.231, BF<sub>01</sub> = 2.210 for vertical; <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>) and Ping sessions (p = 0.450, BF<sub>01</sub> = 3.229 for horizontal; p = 0.588, BF<sub>01</sub> = 3.675 for vertical). To examine whether there were any differences in eye movement between two sessions, a two-way mixed ANOVA (sessions × attended orientation) was applied to the eye position data. The analyses revealed neither main effects nor an interaction effect for the horizontal (ps &gt; 0.222, BF<sub>excl</sub> &gt; 1.593) and vertical eye movements (ps &gt; 0.158, BF<sub>excl</sub> &gt; 1.185). We note that we only collected eye-tracking data during behavioral training and not during scanning due to technical limitations. However, it seems unlikely that the participants would adopt a different eye fixation strategy after training, especially given the centrally presented stimulus configuration that made eye movement an ineffective strategy.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Analysis of eye positions.</title><p>(<bold>A</bold>) Group-level horizontal eye position and (<bold>B</bold>) vertical eye position for the preparation period during behavioral training in No-Ping and Ping sessions. Each dot represents one participant. Error bars denote standard error of the means.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103425-app1-fig2-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103425.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>By combining the 'pinging' technique with fMRI-based multivariate pattern analysis, this <bold>important</bold> study provides <bold>compelling</bold> evidence for a dual-format representation of attention during the preparatory period. The findings help reconcile the debate between sensory-like and non-sensory accounts of attentional templates and shed light on how the brain flexibly deploys different forms of templates to guide attention. This work will be of broad interest to researchers in psychology, vision science, and cognitive neuroscience.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103425.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The aim of the experiment reported in this paper is to examine the nature of the representation of a template of an upcoming target. To this end, participants were presented with compound gratings (consisting of tilted to the right and tilted to the left lines) and were cued to a particular orientation - red left tilt or blue right tilt (counterbalanced across participants). There two directly compared conditions: (i) no ping: where there was a cue, that was followed by a 5.5-7.5s delay, then followed by a target grating in which the cued orientation deviated from the standard 45 degrees; and (ii) ping condition in which all aspects were the same with the only difference that a ping (visual impulse presented for 100ms) was presented after the 2.5 seconds following the cue. There was also a perception task in which only the 45 degrees to the right or to the left lines were presented. It was observed that during the delay, only in the ping condition, were the authors able to decode orientation of the to be reported target using the cross-task generalization. Attention decoding, on the other hand, was decoded in both ping and non-ping conditions. It is concluded that the visual system has two different functional states associated with a template during preparation: a predominantly non-sensory representation for guidance and a latent sensory-like for prospective stimulus processing.</p><p>Strengths:</p><p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative -the cross-task decoding, the use of Mahalanobis distance as a function of representational similarity, the fact that the question is theoretically interesting, the excellent figures.</p><p>Comments on revisions:</p><p>I have no further comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103425.4.sa2</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This paper discusses how non-sensory and latent, sensory-like attentional templates are represented during attentional preparation. Using multivariate pattern analysis, they found that visual impulses can enhance the decoding generalization from perception to attention tasks in the preparatory stage in the visual cortex. Furthermore, the emergence of the sensory-like template coincided with enhanced information connectivity between V1 and frontoparietal areas and was associated with improved behavioral performance. It is an interesting paper with supporting evidence for the latent, sensory-like attentional template.</p><p>Comments on revisions:</p><p>I appreciate the authors' thoughtful revisions, which have addressed my earlier concerns. I have no further comments.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103425.4.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yilin</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Taosheng</given-names></name><role specific-use="author">Author</role><aff><institution>Michigan State University</institution><addr-line><named-content content-type="city">Michigan</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Jia</surname><given-names>Ke</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Theeuwes</surname><given-names>Jan</given-names></name><role specific-use="author">Author</role><aff><institution>Vrije Universiteit Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Gong</surname><given-names>Mengyuan</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>I am impressed with the thoroughness with which the authors addressed my concerns. I don't have any further concerns and think that this paper makes an interesting and significant contribution to our understanding of VWM. I would only suggest adding citations to the newly added paragraph where the authors state &quot;It could be argued that preparatory attention relies on the same mechanisms as working memory maintenance.&quot; They could cite work by Bettencourt and Xu, 2016; and Sheremata, Somers, and Shomstein (2018).</p></disp-quote><p>We thank the reviewer for the positive feedback. We have now cited the referenced work in the manuscript (Page. 19, Line 371).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Overall, I think that the authors' revision has addressed most, if not all, of my major concerns noted in my previous comments. The results appear convincing and I do not have additional comments.</p></disp-quote><p>We thank the reviewer for the positive feedback and are pleased that the revision addressed the major concerns.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>(1) The authors addressed most of my previous concerns and provided additional data analysis. They conducted further analyses to demonstrate that the observed changes in network communication are associated with behavioral RTs, supporting the idea that the impulse-driven sensory-like template enhances informational connectivity between sensory and frontoparietal areas, and relates to behavior.</p></disp-quote><p>We are pleased that the revision addressed the major concerns.</p><disp-quote content-type="editor-comment"><p>(2) I would like to further clarify my previous points regarding the definition of the two types of templates and the evidence for their coexistence. The authors stated that the sensory-like template likely existed in a latent state and was reactivated by visual pings, proposing that sensory and non-sensory templates coexist. However, it remains unclear whether this reflects a dynamic switch between formats or true coexistence. If the templates are non-sensory in nature, what exactly do they represent? Are they meant to be abstract or conceptual representations, or, put simply, just &quot;top-down attentional information&quot;? If so, why did the generalization analysestraining classifiers on activity during the stimulus selection period and testing on preparatory activity-fail to yield significant results? While the stimulus selection period necessarily encodes both target and distractor information, it should still contain attentional information. I would appreciate more discussion from this perspective.</p></disp-quote><p>We thank the reviewer for the helpful clarification of previous comments. Since we addressed similar comments from Reviewer 2 (Point 2) in the previous round, our response below may appear somewhat repetitive. First, regarding whether our findings reflect a dynamic switch between non-sensory and sensory-like template, or the ‘coexistence’ of two template formats, we acknowledge that the temporal limitations of fMRI prevent us from directly testing dynamic representations. However, several aspects of our data favor the latter interpretation: (1) our key findings remained consistent in the subset of participants (N=14) who completed both No-Ping and Ping sessions in counterbalanced order. This makes it unlikely that participants systematically switched cognitive strategies (e.g., using non-sensory templates in the No-Ping session versus sensory-like templates in the Ping session) in response to the taskirrelevant, uninformative visual impulse; (2) while we agree that the temporal dynamics between the two templates remain unclear, it is difficult to imagine that orientation-specific templates observed in the Ping session emerged de novo from purely non-sensory templates and an exogenous ping. In other words, if there is no orientation information at all to begin with, how does it come into being from an orientation-less external ping? A more parsimonious explanation is that orientation information was already present in a latent format and was activated by the ping, in line with the models of “activity-silent” working memory. However, since the detailed circuit-level mechanism underlying such reactivation remain unclear, we acknowledge that this interpretation warrants direct investigation in future studies. This point is discussed in the main texts (Page 19-20, Line 389-402).</p><p>Second, while our data cannot definitively determine the nature of the non-sensory template, we consider categorical coding a plausible candidate based on prior visual search studies. For instance, categorical attributes (e.g., left-tilted vs. right-tilted) have been shown to effectively guide attention in orientation search tasks (Wolfe et al., 1992), similar to our paradigm. Further, categorical templates are more tolerant of stimulus variability, making them well-suited to our task, which involved trial-by-trial variations in target orientation around a reference (see Page 21, Line 427- 437 for more detailed discussions).</p><p>Third, the lack of generalization from stimulus selection to preparatory attention in the Ping session may relate to the limited overlap in shared information between these two periods. Neural activity during stimulus selection encodes sensory information about both orientations, along with sensory-like attentional signals (as indicated by the attention decoding and crosstask generalization from perception task to the stimulus-selection period). In contrast, preparatory activity likely involves a dominant non-sensory template, a latent sensory-like template, and residual sensory effects from the impulse stimulus. The limited overlap in sensory-like attentional signals may therefore be insufficient to support generalization across the two periods.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors)</bold></p><p>I think the central prediction of greater pattern similarity between 'attend leftward' and 'perceived leftward' in the ping session in comparison to the no-ping session the same also holds for 'attend rightward' and 'perceived rightward' could be directly examined by a two-way ANOVA (session × the attend orientation is the same/different from the perceived orientation) for each ROI (V1 and EVC). A three-way ANOVA might complicate readers' intuitive understanding of the implications of the statistical results.</p></disp-quote><p>We thank the reviewer for the suggestion. Following the reviewer’s suggestion, we defined a new condition label based on orientation consistency between attended and perceived orientations: (1) same orientation: averaging “attend leftward/perceive leftward” and “attend rightward/perceive rightward”; and (2) different orientation: averaging “attend leftward/perceive rightward” and “attend rightward/perceive leftward”. A two-way mixed ANOVA (session × orientation consistency) on Mahalanobis distance revealed a main effect of orientation consistency in V1 (F(1,38) = 4.21, p = 0.047, η<sub>p</sub><sup>2</sup> = 0.100), indicating that activity patterns were more similar when attended and perceived orientations matched. No significant main effect of session was found (p = 0.923). Importantly, a significant interaction was found in V1 (F(1,38) = 5.00, p = 0.031, η<sub>p</sub><sup>2</sup> = 0.116), suggesting that visual impulse enhanced the similarity between preparatory attentional template and the perception of corresponding orientation. In EVC, the same analysis revealed only a main effect of orientation consistency (F(1,38) = 5.87, p = 0.020, η<sub>p</sub><sup>2</sup> = 0.134), with no significant other effects (ps &gt; 0.240). The interaction results were consistent with those reported in the original three-way ANOVA. We have now replaced the previous analysis with the new one in the main texts (Page 11-12, Line 231-242).</p></body></sub-article></article>