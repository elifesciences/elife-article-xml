<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">92495</article-id><article-id pub-id-type="doi">10.7554/eLife.92495</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92495.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Multi-day neuron tracking in high-density electrophysiology recordings using earth mover’s distance</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-334701"><name><surname>Yuan</surname><given-names>Augustine Xiaoran</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-102576"><name><surname>Colonell</surname><given-names>Jennifer</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-3940-0689</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-334702"><name><surname>Lebedeva</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-253937"><name><surname>Okun</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-170151"><name><surname>Charles</surname><given-names>Adam S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9045-3489</contrib-id><email>adamsc@jhu.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-82966"><name><surname>Harris</surname><given-names>Timothy D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6289-4439</contrib-id><email>harrist@janelia.hhmi.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013sk6x84</institution-id><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution></institution-wrap><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Department of Biomedical Engineering, Center for Imaging Science Institute, Kavli Neuroscience Discovery Institute, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Sainsbury Wellcome Centre, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Department of Psychology and Neuroscience Institute, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peyrache</surname><given-names>Adrien</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>07</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP92495</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-18"><day>18</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-03"><day>03</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.03.551724"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-12"><day>12</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92495.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-27"><day>27</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92495.2"/></event></pub-history><permissions><copyright-statement>© 2023, Yuan et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Yuan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-92495-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-92495-figures-v1.pdf"/><abstract><p>Accurate tracking of the same neurons across multiple days is crucial for studying changes in neuronal activity during learning and adaptation. Advances in high-density extracellular electrophysiology recording probes, such as Neuropixels, provide a promising avenue to accomplish this goal. Identifying the same neurons in multiple recordings is, however, complicated by non-rigid movement of the tissue relative to the recording sites (drift) and loss of signal from some neurons. Here, we propose a neuron tracking method that can identify the same cells independent of firing statistics, that are used by most existing methods. Our method is based on between-day non-rigid alignment of spike-sorted clusters. We verified the same cell identity in mice using measured visual receptive fields. This method succeeds on datasets separated from 1 to 47 days, with an 84% average recovery rate.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>electrophysiology</kwd><kwd>Neuropixels</kwd><kwd>tracking</kwd><kwd>single unit</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>BRAIN Initiative</institution></institution-wrap></funding-source><award-id>U01 NS115587</award-id><principal-award-recipient><name><surname>Yuan</surname><given-names>Augustine Xiaoran</given-names></name><name><surname>Harris</surname><given-names>Timothy D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurons can be tracked in mice implanted with Neuropixels 2.0 probes for up to 8 weeks using visual receptive fields to score tracking in the absence of reference data.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ability to longitudinally track neural activity is crucial to understanding central capabilities and changes of neural circuits that operate on long time-scales, such as learning and plasticity (<xref ref-type="bibr" rid="bib8">Carmena et al., 2005</xref>; <xref ref-type="bibr" rid="bib12">Clopath et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Huber et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Liberti et al., 2016</xref>), motor stability (<xref ref-type="bibr" rid="bib8">Carmena et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Dhawale et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Jensen et al., 2022</xref>), etc. We seek to develop a method capable of tracking single units regardless of changes in functional responses for the duration of an experiment spanning 1–2 months.</p><p>High-density multi-channel extracellular electrophysiology (ephys) recording devices enable chronic recordings over large areas for days to months (<xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>). Such chronic recordings make possible experiments targeted at improving our understanding of neural computation and underlying mechanisms. Examples include perceptual decision-making, exploration, and navigation (<xref ref-type="bibr" rid="bib5">Brown et al., 2004</xref>; <xref ref-type="bibr" rid="bib6">Buzsáki, 2004</xref>; <xref ref-type="bibr" rid="bib17">Harris, 2005</xref>; <xref ref-type="bibr" rid="bib18">Harris et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Luo et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Quian Quiroga and Panzeri, 2009</xref>). Electrode arrays with hundreds to thousands of sites, e.g., Neuropixels, are now used extensively to record the neural activity of large populations stably and with high spatio-temporal resolution, capturing hundreds of neurons with single neuron resolution (<xref ref-type="bibr" rid="bib6">Buzsáki, 2004</xref>; <xref ref-type="bibr" rid="bib18">Harris et al., 2016</xref>). Moreover, ephys retains the higher time resolution needed for single spike identification, as compared with calcium imaging that provides more spatial cues with which to track neurons over days.</p><p>The first step in analyzing ephys data is to extract single neuron signals from the recorded voltage traces, i.e., spike sorting. Spike sorting identifies individual neurons by grouping detected action potentials using waveform profiles and amplitudes. Specific algorithms include principal components-based methods (<xref ref-type="bibr" rid="bib29">Quiroga et al., 2004</xref>; <xref ref-type="bibr" rid="bib9">Chah et al., 2011</xref>), and template-matching methods, e.g., Kilosort (<xref ref-type="bibr" rid="bib5">Brown et al., 2004</xref>; <xref ref-type="bibr" rid="bib7">Carlson and Carin, 2019</xref>; <xref ref-type="bibr" rid="bib18">Harris et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Pachitariu et al., 2016</xref>). Due to the high-dimensional nature of the data, spike sorting is often computationally intensive on large datasets (tens to hundreds of GB) and optimized to run on single sessions. Thus processing multiple sessions has received minimal attention, and the challenges therein remain largely unaddressed.</p><p>One major challenge in reliably tracking neurons is the potential for changes in the neuron population recorded (<xref ref-type="fig" rid="fig1">Figure 1a</xref> and <xref ref-type="fig" rid="fig1">Figure 1b</xref>). In particular, since the probe is attached to the skull, brain tissue can move relative to the probe, e.g., during licking, and drift can accumulate over time (<xref ref-type="bibr" rid="bib22">Jun et al., 2017</xref>). Kilosort 2.5 corrects drift within a single recording by inferring tissue motion from continuous changes in spiking activity and interpolating the data to account for that motion (<xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>). Larger between-recording drift occurs for sessions on different days, and can (1) change the size and location of spike waveforms along the probe (<xref ref-type="bibr" rid="bib16">Hall et al., 2021</xref>), (2) lose neurons that move out of range, and (3) gain new neurons that move into recording range. Thus clusters can change firing pattern characteristics or completely appear/disappear. As a result the specific firing patterns classified as unit clusters may appear and disappear in different recordings (<xref ref-type="bibr" rid="bib1">Bar-Hillel et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Harris et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Swindale and Spacek, 2014</xref>; <xref ref-type="bibr" rid="bib35">Tolias et al., 2007</xref>). Another challenge is that popular template-matching-based spike-sorting methods usually involve some randomness in template initialization (<xref ref-type="bibr" rid="bib10">Chung et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Lee et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Pachitariu et al., 2016</xref>). As a result, action potentials can be assigned into clusters differently, and clusters can be merged or separated differently across runs.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic depiction of drift.</title><p>(<bold>a</bold>) Mice were implanted with a four-shank Neuropixels 2.0 probe in visual cortex area V1. (<bold>b</bold>) Each colored star represents the location of a unit recorded on the probe. In this hypothetical case, the same color indicates unit correspondence across days. The black unit is missing on day 48, while the turquoise star is an example of a new unit. Tracking aims to correctly match the red and blue units across all datasets and determine that the black unit is undetected on day 48. (<bold>c</bold>) Two example spatial-temporal waveforms of units recorded in two datasets that likely represent the same neuron, based on similar visual responses. Each trace is the average waveform on one channel across 2.7 ms. The blue traces are waveforms on the peak channel and nine nearby channels (two rows above, two rows below, and one in the same row) from the first dataset (day 1). The red traces, similarly selected, are from the second dataset. Waveforms are aligned at the electrodes with peak amplitude, different on the 2 days.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig1-v1.tif"/></fig><p>Previous neuron tracking methods are frequently based on waveform and firing statistics, e.g., firing rate similarity (<xref ref-type="bibr" rid="bib11">Chung et al., 2019</xref>), action potential shape correlation, and inter-spike interval (ISI) histogram shape (<xref ref-type="bibr" rid="bib36">Vasil’eva et al., 2016</xref>). When neuronal representations change, e.g., during learning (<xref ref-type="bibr" rid="bib8">Carmena et al., 2005</xref>; <xref ref-type="bibr" rid="bib19">Huber et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Liberti et al., 2016</xref>) or representational drift (<xref ref-type="bibr" rid="bib30">Rokni et al., 2007</xref>), neural activity statistics became less reliable. In this work, we take advantage of the rich spatial-temporal information in the multi-channel recordings, matching units based on the estimated neuron locations, and unit waveforms (<xref ref-type="bibr" rid="bib24">Lewicki, 1998</xref>), instead of firing patterns.</p><p>As an alternative method, <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, concatenated pairs of datasets after low-resolution alignment, awkward for more than two datasets. We report here a more flexible, expandable, and robust tracking method that can track neurons effectively and efficiently across any number of sessions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Procedure</title><p>Our datasets consist of multiple recordings taken from three mice (Figure 7a) over 2 months. The time gap between two recordings ranges from 2 to 25 days. Each dataset is spike-sorted individually with a standard Kilosort 2.5 pipeline. The sorting results, including unit assignment, spike times, etc., are used as input for our method (post-processed using ecephys spike sorting pipeline; <xref ref-type="bibr" rid="bib14">Colonell, 2018</xref>) (section Dataset). To ensure the sorting results are unbiased, we performed no manual curation. As the clusters returned by Kilosort can vary in quality, we only considered the subset of units labeled as ‘good’ by Kilosort, here referred to as KSgood units (section Reference set). KSgood units are mainly determined by the amount of ISI violations and are believed to represent a single unit (<xref ref-type="bibr" rid="bib27">Pachitariu et al., 2016</xref>).</p><p>Our overall strategy is to run spike sorting once per session, and then to generate a unit-by-unit assignment between pairs of datasets. When tracking units across more than two sessions, two strategies are possible: match all ensuing sessions to a single session (e.g. the first session) (section Measuring rigid drift using the EMD and section Determining <italic>z</italic>-distance threshold), or match consecutive pairs of sessions and then trace matched units through all sessions (section Units can be tracked in discontinuous recordings for 48 days).</p><p>We refer to the subset of KSgood units with strong and distinguishable visual responses in both datasets of a comparison as reference units (see section Reference set for details). Similar to <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, we validated our unit matching of reference units using visual receptive field similarity. Finally, we showed that trackable units with strong visual responses are qualitatively similar to those without (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> to Figure 5).</p><p>To provide registration between pairs of recordings, we used the earth mover’s distance (EMD) (<xref ref-type="bibr" rid="bib2">Bertrand et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Cohen, 1999</xref>). We use a feature space consisting of a geometric distance space and a waveform similarity space, to address both rigid and non-rigid neuron motion. The EMD finds matches between objects in the two distributions by minimizing the overall distances between the established matches (section Earth mover’s distance).</p><p>We use EMD in two stages: rigid drift correction and unit assignment. Importantly, the EMD incorporates two parameters crucial for matching units: location-based physical distance and a waveform distance metric that characterizes similarity of waveforms (section Calculating the EMD metric). The EMD matrix is constructed with a weighted combination of the two (details in section Methods), i.e., a distance between two units <inline-formula><mml:math id="inf1"><mml:semantics><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ω</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). The first EMD stage estimates the homogeneous vertical movement of the entire population of KSgood units (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). This movement estimate is used to correct the between-session rigid drift in unit locations. The rigid drift estimation procedure is illustrated in <xref ref-type="fig" rid="fig2">Figure 2b</xref>. Post drift correction, a unit’s true match will be close in both physical distance and waveform distance. Drift-corrected units were then matched at the second EMD stage. The EMD between assigned units can be thought of as the local non-rigid drift combined with the waveform distortion resulting from drift. We test the accuracy of the matching by comparing with reference unit assignments based on visual receptive fields (section Reference set).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The earth mover’s distance (EMD) can detect the displacement of single units.</title><p>(<bold>a</bold>) Schematic of EMD unit matching. Each blue unit in day 1 is matched to a red unit in day 2. Dashed lines indicate the matches to be found by minimizing the weighted sum of physical and waveform distances. (<bold>b</bold>) Open and filled circles show positions of units in days 1 and 2, respectively. Arrows indicate matching using EMD. The arrow color represents the match direction; upward matches found with the EMD are in red and downward in black. Solid lines indicate a <italic>z</italic>-match distance within 15 μm, while a dashed line indicates a <italic>z</italic>-distance <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>15</mml:mn><mml:mtext>µm</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>. Expanded view shows probe area from 3120 to 3220 μm. (<bold>c</bold>) Histogram of <italic>z</italic>-distances of matches (black and red bars) and kernel fit (light blue solid curve). The light blue dashed line shows the mode (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>15.65</mml:mn><mml:mtext>µm</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>). The dark blue dashed line shows the imposed drift (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>12</mml:mn><mml:mtext>µm</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>). The red region shows the matches within 15 μm of the mode. The EMD needs to detect the homogeneous movement against the background, i.e., units in the black region that are unlikely to be the real matches due to biological constraints.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>The effect of drift correction on reference units yield for all three animals.</title><p>Note that drift correction improves the recovery rate for most cases; the degree of improvement is a function of the magnitude of the drift.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Earth mover’s distance (EMD) cost can be used to detect discontinuities in the data.</title><p>In animal AL036, we noted a large decrease in the number of reference units (units with matched visual responses, see section Reference set) after the second dataset. This likely indicates a large physical shift in the tissue relative to the probe. It is important to be able to detect such discontinuities to eliminate datasets from consideration. We find that the discontinuity can be detected in the EMD mean cost, location mean cost, and waveform mean cost. The four heatmaps on the left show reference counts and pairwise costs for units matched on one shank in animal AL036. Note that the days with few reference units also have higher EMD cost. To show that days 1–2 (first two rows) are significantly different from days 3 to 9, we use the Mann-Whitney U test. All three cost values show signiﬁcant differences between the groups (EMD mean cost, reject <italic>H</italic>0, p = <inline-formula><mml:math id="inf6"><mml:semantics><mml:mrow><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>; location mean cost, reject <italic>H</italic>0, p = <inline-formula><mml:math id="inf7"><mml:semantics><mml:mrow><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>; waveform mean cost, reject <italic>H</italic>0, p = <inline-formula><mml:math id="inf8"><mml:semantics><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>). To show that days 3–9 come from the same distribution, we compare odd and even rows using the same test. All three cost values show no significant difference between odd and even days (accept <italic>H</italic>0, p=0.92). Based on this significant difference between days 1 and 2 and later days (datasets in the red rectangles), we infer that the first two datasets sampled a different population of units than the later recordings. These first two datasets were eliminated from our analysis. Matrices on the right show similar information for animal AL032 for reference. To estimate the relative magnitude of EMD cost in related datasets versus unrelated datasets, we calculated the cost between unrelated datasets with similar number of units (AL032 shank 1 and AL036 shank 1, EMD cost = 78, location cost = 67, and waveform cost = 32). The EMD cost is between 70 and 80, much larger than observed for related datasets (between 20 and 30).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>The normalized earth mover’s distance (EMD) cost (unitless), <italic>z-</italic>distance (μm), physical distance (μm), and waveform distance (unitless) and the corresponding recovery rate of reference unit (units with matched visual responses) in pairwise matches of all to all pairs of recordings, on each shank.</title><p>Each triangle represents the recovery rate in a pair of datasets. Animal AL031 has 6 sets of matching, with one outlier removed. Animal AL032 has 24 sets of matching. Animal AL036 has 60 sets of matched units. Overall, most of the datasets with high recovery rates have per-unit EMD in the range 20–30, but datasets with lower recovery are in the same range. Therefore, while very high EMD cost reveals discontinuous data, EMD cost in the normal range is not predictive of reference unit recovery, which is a metric of match success.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Recovery rate vs. L2-weight.</title><p>We varied the weight <inline-formula><mml:math id="inf9"><mml:semantics><mml:mi>ω</mml:mi></mml:semantics></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> used to combine the physical and waveform distances in increments of 500. The vertical line indicates weight = 1500, where the overall recovery rate = 86.29%. The maximum recovery rate = 87.68% occurs at weight = 3000. We chose weight = 1500 for all subsequent analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig2-figsupp4-v1.tif"/></fig></fig-group><p>For each unit, the location is determined by fitting the peak-to-peak amplitudes on the 10 sites nearest the site with peak signal, based on the triangulation method in <xref ref-type="bibr" rid="bib4">Boussard et al., 2021</xref> (section Calculating the EMD metric). The waveform distance is an L2 norm between two spatial-temporal waveforms that spans 22 channels and 2.7 ms (section Calculating the EMD metric). Physical unit distances provide a way to maintain the internal structure and relations between units in the EMD. Waveform similarity metrics will distinguish units in the local neighborhood and likely reduce the effect of new and missing units.</p><p>We analyzed the match assignment results in two ways. First, we compared all subsequent datasets to dataset 1 using recovery rate and accuracy. We define recovery rate <inline-formula><mml:math id="inf10"><mml:semantics><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> as the fraction of unit assignments by our method that are the same as reference unit assignments established using visual responses (section Reference set).<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mo>∣</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mo>∩</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mo>∩</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Since the EMD forces all units from the dataset with fewer neurons to have an assigned match, we use vertical <italic>z</italic>-distance to threshold out the biologically impossible unit assignments. We then calculated the accuracy <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., the fraction of EMD unit assignments within the <italic>z</italic>-distance threshold which agree with the reference assignments.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mo>∣</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∩</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mo>∩</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>We also retrieved non-reference units, i.e., matched units without receptive field information but whose <italic>z</italic>-distance is smaller than the threshold.</p><p>Second, we tracked units between consecutive datasets and summarized and analyzed the waveforms, unit locations, firing rates, and visual responses (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> to Figure 5 for details) of all tracked chains, i.e., units which can be tracked across at least three consecutive datasets.</p></sec><sec id="s2-2"><title>Measuring rigid drift using the EMD</title><p>Drift happens mostly along the direction of probe insertion (vertical or <italic>z</italic>-direction). We want to estimate the amount of vertical drift under the assumption that part of the drift is rigid; this is likely a good assumption given the small (≈720 μm) <italic>z</italic>-range of these recordings. The EMD allows us to extract the homogeneous (rigid) movement of matched units. For ideal datasets with a few units consistently detected across days, this problem is relatively simple (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). In the real data analyzed here, we find that only ≈60% of units are detected across pairs of days, so the rigid motion of the real pairs must be detected against a background of units with no true match. These units with no real match will have <italic>z</italic>-shifts far from the consensus <italic>z</italic>-shift of the paired units (<xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><p>In <xref ref-type="fig" rid="fig2">Figure 2</xref> the EMD match of units from the first dataset (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, open circles) to the dataset recorded the next day (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, closed circles) is indicated by the arrows between them. To demonstrate detection of significant drift, we added a 12 µm upward drift to the <italic>z</italic>-coordinate of the units from the second day. The first stage of the EMD is used to find matches using the combined distance metric as described in section Calculating the EMD metric. We used a kernel fit to the distribution of <italic>z</italic>-distances of all matched units to find the mode (mode = <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>15.65</mml:mn><mml:mtext>µm</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>); this most probable distance is the estimate of the drift (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). It is close to the actual imposed drift (<inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>12</mml:mn><mml:mtext>µm</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>As the EMD is an optimization algorithm with no biological constraints, it assigns matches to all units in the smaller dataset regardless of biophysical plausibility. As a result, some of the assigned matches may have unrealistically long distances. A distance threshold is therefore required to select correct pairs. For the illustration in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the threshold is set to 15 μm, which is chosen to be larger than most of the <italic>z</italic>-shifts observed in our experimental data. The threshold value will be refined later by distribution fitting (Figure 4). In <xref ref-type="fig" rid="fig2">Figure 2</xref> all of the sub-threshold (short) distances belong to upward pairs (<xref ref-type="fig" rid="fig2">Figure 2b and c</xref>, red solid arrows), showing that the EMD can detect the homogeneous movement direction and the amount of imposed drift.</p><p>When determining matched reference units from visual response data, we require that units be spatially nearby (within 30 μm) as well as having similar visual responses. After correcting for drift, we find that we recover more reference units (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), indicating improved spatial match of the two ensembles. This improved recovery provides further evidence of the success of the drift correction.</p></sec><sec id="s2-3"><title>A vertical distance threshold is necessary for accurate tracking</title><p>To detect the homogeneous <italic>z</italic>-shift of correct matches against the background of units without true matches, it is necessary to apply a threshold on the <italic>z</italic>-shift. When tracking units after shift correction, a vertical distance threshold is again required to determine which matches are reasonable in consideration of biological plausibility. The receiver operator characteristic (ROC) curve in <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the fraction of reference units matched correctly and the number of reference pairs retained as a function of <italic>z</italic>-distance threshold. We want to determine the threshold that maximizes the overall accuracy in the reference units (<xref ref-type="fig" rid="fig3">Figure 3</xref>, blue curve) while including as many reference units as possible (<xref ref-type="fig" rid="fig3">Figure 3</xref>, red curve).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The receiver operator characteristic (ROC) curve of matching accuracy vs. distance.</title><p>The blue curve shows the accuracy for reference units. The red line indicates the number of reference units included. The solid vertical line indicates the average <italic>z</italic>-distance across all reference pairs in all animals (<inline-formula><mml:math id="inf14"><mml:semantics><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>6.96</mml:mn><mml:mi>μ</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>). The dashed vertical black line indicates a <italic>z</italic>-distance threshold at <italic>z</italic>=10 μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig3-v1.tif"/></fig><p>Since reference units only account for 29% of KSgood units (units with few ISI violations that are believed to represent a single unit), and the majority of KSgood units did not show a distinguishable visual response, we need to understand how representative the reference units are of all KSgood units.</p><p>We found the distribution of <italic>z</italic>-distances of reference pairs is different from the distribution of all KSgood units (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, top and middle panels). While both distributions may be fit to an exponential decay, the best fit decay constant is significantly different (Kolmogorov-Smirnov test, reject <italic>H</italic>0, p = <inline-formula><mml:math id="inf15"><mml:semantics><mml:mrow><mml:mn>5.5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>31</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>). Therefore, the accuracy predicted by the ROC of reference pairs in <xref ref-type="fig" rid="fig3">Figure 3</xref> will not apply to the set of all KSgood pairs. The difference in distribution is likely due to the reference units being a special subset of KSgood units in which units are guaranteed to be found in both datasets, whereas the remaining units may not have a real match in the second dataset. To estimate the ROC curve for the set of all KSgood units, we must estimate the <italic>z</italic>-distance distribution for a mixture of correct and incorrect pairs.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Recovery rate, accuracy, and putative pairs.</title><p>(<bold>a</bold>) The histogram distribution fit for all KSgood units (top) and reference units alone (middle). False positives for reference units are defined as units matched by earth mover’s distance (EMD) but not matched when using receptive fields. The false positive fraction for the set of all KSgood units is obtained by integration. <italic>z</italic>=10 μm threshold has a false positive rate = 27% for KSgood units. (<bold>b</bold>) Light blue bars represent the number of reference units successfully recovered using only unit location and waveform. The numbers on the bars are the recovery rate of each dataset, and the red portion indicates incorrect matches. Incorrect matches are cases where units with a known match from receptive field data are paired with a different unit by EMD; these errors are false positives. The green bars show matching accuracy for the set of pairs with <italic>z</italic>-distance less than the 10 μm threshold. The orange portion indicates incorrect matches after thresholding. The false positives are mostly eliminated by adding the threshold. Purple bars are the number of putative units (unit with no reference information) inferred with <italic>z</italic>-threshold=10 μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Determining the functional form for the <italic>z</italic>-distance distribution of all pairs.</title><p>As shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, the <italic>z</italic>-distance distribution of reference pairs differs significantly from that of all pairs. The <italic>z</italic>-distance distribution for all pairs is the sum of <italic>z</italic>-distance distributions for true hits (<inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>∣</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and false positives (<inline-formula><mml:math id="inf17"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo>∣</mml:mo><mml:mo>~</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>), weighted by the fraction correct, <italic>f</italic>: <inline-formula><mml:math id="inf18"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo>∣</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo>∣</mml:mo><mml:mo>~</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>. We built a Monte Carlo model, with 150 units (the average density of subject AL032), normally distributed error with <inline-formula><mml:math id="inf19"><mml:semantics><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mi>μ</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> for the measured location of the units in true pairs, and random placement of false positives. For each value of fraction correct, we ran the model 500 times. The figure shows fits to model distributions with fraction correct = 0.23, 0.5, 0.6 (top row) and <italic>f</italic>=0.7, 0.96 (bottom row). The resulting <italic>z</italic>-distance distributions are well fit using a folded Gaussian for the distance distribution of true hits and an exponential for the distance distribution of false positives (see Algorithm 2). We use these functional forms to fit the experimental <italic>z</italic>-distance distribution and estimate the false positive rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Fits of experimental <italic>z</italic>-distance distributions to the model.</title><p>When reference data is available, the <italic>z</italic>-distance distribution of these known true hits can be fit to obtain the width <inline-formula><mml:math id="inf20"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> of the folded Gaussian. <inline-formula><mml:math id="inf21"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> can then be fixed in the fit of the distribution of all KSgood units to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, which is used to estimate the false positive rate. When no reference data are available, <inline-formula><mml:math id="inf22"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> can be estimated from fitting the distribution of all KSgood units to all four parameters in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. Panels a and b show the dataset from <xref ref-type="fig" rid="fig4">Figure 4</xref> fit with and without fixing the folded Gaussian distribution width. The resulting false positive rate from the no-reference fit at threshold <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mtext>µm</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> is larger than that from the fit using reference data, so the procedure gives a conservative estimate of the accuracy. Panel c shows the model fit of data from an unrelated dataset acquired with from mouse prefrontal cortex using Neuropixels 1.0 (<xref ref-type="bibr" rid="bib3">Böhm and Lee, 2023</xref>). The similar shape of the distribution and a 29% false positive rate suggests that the method can be generalized.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>The reference unit recovery rate vs. days between matched recordings.</title><p>Each triangle represents the matching results of two datasets. Animal AL031 has 6 sets of matched units, with one outlier removed. Animal AL032 has 24 sets of matched units. Animal AL036 has 60 sets of matching. The recovery rate is lower for longer durations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig4-figsupp3-v1.tif"/></fig></fig-group><p>We assume that the distribution of <italic>z</italic>-distances <inline-formula><mml:math id="inf24"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> for reference units is the conditional probability <inline-formula><mml:math id="inf25"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo>∣</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>; i.e., we assume all reference units are true hits. The distribution of <italic>z</italic>-distances for all KSgood units <inline-formula><mml:math id="inf26"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> includes both hits and false positives. The distance distribution of false positives is the difference between the two.</p><p>A Monte Carlo simulation determined that the best model for fitting the <italic>z</italic>-distance distribution of reference units <inline-formula><mml:math id="inf27"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Δ</mml:mi><mml:mo>∣</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> is a folded Gaussian distribution (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, middle panel) and an exponential distribution for false positive units (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The KSgood distribution is a weighted combination of the folded Gaussian and an exponential:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:semantics><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>We fit the KSgood distribution to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> to extract the individual distribution parameters and the fraction of true hits (<italic>f</italic>). The full distribution can then be integrated up to any given <italic>z</italic>-threshold value to calculate the false positive rate. (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, bottom panel, see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for details).</p><p>Based on the the estimated false positive rate (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, bottom panel), we used a threshold of 10 μm (<xref ref-type="fig" rid="fig3">Figure 3</xref>, black dotted line) to obtain at least 70% accuracy in the KSgood units. We used the same threshold to calculate the number of matched reference units and the corresponding reference unit accuracy (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, green bars).</p><p>Note that this threshold eliminates most of the known false positive matches of reference pairs (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, red fraction) at the cost of recovering fewer correct pairs (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, green bars). The recovery rate varies from day to day; datasets separated by longer times tend to have higher tracking uncertainty (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>).</p><p>In addition to the units with visual response data, we can track units which have no significant visual response (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, purple bars). All comparisons are between subsequent datasets and the day 1 dataset.</p></sec><sec id="s2-4"><title>Units can be tracked in discontinuous recordings for 48 days</title><p>To assess long-term tracking capabilities, we tracked neurons across all datasets for each mouse. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows a survival plot of the number of unit chains successfully tracked over all durations. All units in the plot can be tracked across at least three consecutive datasets, a chain as the term is used here. We categorized all trackable unit chains into three types: reference chains, mixed chains, and putative chains. Reference chains have receptive field information in all datasets. Putative chains have no reference information in any of the datasets. Mixed units have at least one dataset with no receptive field information. There are 133 reference chains, 135 mixed chains, and 84 putative chains across all the subjects. Among them, 46 reference, 51 mixed, and 9 putative units can be followed across all datasets. We refer to them as fully trackable units. One example trackable unit in each group is shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Number of reference units (deep blue, dark orange, and green for different subjects), putative (medium green, medium orange, and blue) units, and mixed units (light green, yellow, and light blue) tracked for different durations.</title><p>The loss rate is similar for different chain types in the same subject. Note that chains can start on any day in the full set of recordings, so the different sets of neurons have chains with different spans between measurements.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Distribution of waveform L2 similarity change per dataset for each neuron group (reference, putative, and mixed) and across all neurons.</title><p>Box plots indicate 25th percentile, medians, and 75th percentile. Whiskers at the ends of the box plot show maximum and minimum values. <italic>n</italic> and <italic>N</italic> are the number of unit comparisons, i.e., (number of units) × (number of datasets - 1). A Kruskal-Wallis test indicates no difference among the three groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Distributions of individual unit location changes over whole chains (top) and unit location changes between pairs of datasets (bottom), for each neuron group and across all neurons.</title><p>Box plots indicate 25th percentile, medians, and 75th percentile. Whiskers at the ends of the box plot show maximum and minimum values. In the top plot, <italic>n</italic> and <italic>N</italic> are the number of units. In the bottom plot, <italic>n</italic> and <italic>N</italic> are the number of unit comparisons, i.e., (number of units) × (number of datasets - 1). A Kruskal-Wallis test indicates no difference among the three groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Distribution of firing rate fold change per dataset for each neuron group and across all neurons.</title><p>Box plots indicate 25th percentile, medians, and 75th percentile. Whiskers at the ends of the box plot show maximum and minimum values. <italic>n</italic> and <italic>N</italic> are the number of units. A Kruskal-Wallis test indicates no difference among the three groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig5-figsupp3-v1.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>The visual fingerprint and peristimulus time histogram (PSTH) change distributions per dataset for each neuron group and across all neurons.</title><p>Box plots indicate 25th percentile, medians, and 75th percentile. Whiskers at the ends of the box plot show maximum and minimum values. <italic>n</italic> and <italic>N</italic> are the number of unit comparisons, i.e., (number of units) × (number of datasets - 1). A Kruskal-Wallis test indicates no difference among the three groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig5-figsupp4-v1.tif"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 5.</label><caption><title>The similarity score distribution per dataset for each neuron group and across all neurons.</title><p>Box plots indicate 25th percentile, medians, and 75th percentile. Whiskers at the ends of the box plot show maximum and minimum values. <italic>n</italic> and <italic>N</italic> are the number of observations of the units, i.e., <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mstyle></mml:math></inline-formula> (observations of this unit).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig5-figsupp5-v1.tif"/></fig></fig-group><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Example mixed chain.</title><p>(<bold>a</bold>) Above: Firing rates of this neuron on each day (days 1, 2, 13, 23, 48). Below: Firing rate fractional change compared to the previous day. (<bold>b</bold>) Visual response similarity (yellow line), peristimulus time histogram (PSTH) correlation (orange line), and visual fingerprint (vfp) correlation (blue line). The similarity score is the sum of vfp and PSTH. The dashed black line shows the threshold to be considered a reference unit. (<bold>c</bold>) Spatial-temporal waveform of a trackable unit. Each pair of traces represents the waveform on a single channel. (<bold>d</bold>) Estimated location of this unit on different days. Each colored dot represents a unit on 1 day. The orange squares represent the electrodes. (<bold>e</bold>) The pairwise vfp and PSTH traces of this unit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Example reference chain.</title><p>(<bold>a</bold>) Above: Firing rates of this neuron on each day. Below: Firing rate fractional change compared to the previous day. (<bold>b</bold>) Visual response similarity (yellow line), peristimulus time histogram (PSTH) correlation (orange line), and visual fingerprint (vfp) correlation (blue line). The similarity score is the sum of vfp and PSTH. The dashed black line shows the threshold to be considered a reference unit. (<bold>c</bold>) Spatial-temporal waveform of a trackable unit. Each pair of traces represent the waveform on a single channel. (<bold>d</bold>) Estimated location of this unit on different days. Each colored dot represents a unit on 1 day. The orange squares represent the electrodes. (<bold>e</bold>) The pairwise vfp and PSTH traces of this unit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Example putative chain.</title><p>Order is the same as the previous figure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig6-figsupp2-v1.tif"/></fig></fig-group><p>We hypothesize that the three groups of units are not qualitatively different from each other, i.e., all units are equally trackable. In order to check for differences among the three groups, we analyzed the locations, firing rates, waveforms, and receptive fields of the fully trackable units in the three groups: reference, putative, and mixed.</p><p>The spatial-temporal waveform similarity is measured by the L2 distance between waveforms (section Calculating the EMD metric). A Kruskal-Wallis test is performed on the magnitude of L2 change between all pairs of matched waveforms among the three groups. There is no statistical difference in the waveform similarity in reference, putative, and mixed units (<italic>H</italic>=0.59, p=0.75) (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). There is no significant difference in the physical distances of units per dataset (<italic>H</italic>=1.31, p=0.52) (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, bottom panel), nor in the location change of units (<italic>H</italic>=0.23, p=0.89) (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, top panel).</p><p>Firing rate is characterized as the average firing rate fold change of each unit chain, with firing rate of each unit in each dataset normalized by the average firing rate of that dataset. There is no difference in the firing rate fold change in the three groups of units (<italic>H</italic>=1, p=0.6) (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p><p>The receptive field similarity between units in different datasets is described by visual fingerprint (vfp) correlation and peristimulus time histogram (PSTH) correlation between units, and the similarity score, the sum of the two correlations (section Reference set). The change in vfp between matched units is similar among the three groups (<italic>H</italic>=2.23, p=0.33). Similarly, the change in PSTH is not different among the three groups (<italic>H</italic>=1.61, p=0.45) (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We present here an EMD-based neuron tracking algorithm that provides a new, automated way to track neurons over long-term experiments to enable the study of learning and adaptation with state-of-the-art high-density electrophysiology probes. We demonstrate our method by tracking neurons up to 48 days without using receptive field information. Our method achieves 90% recovery rate on average for neurons separated up to 1 week apart and 78% on average for neurons 5–7 weeks apart (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, blue bars). We also achieved 99% accuracy up to 1 week apart and 95% 5–7 weeks apart, when applying a threshold of 10 μm (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, green bars). It also retrieved a total of 552 tracked neurons with partial or no receptive field information, 12 per pair of datasets on average. All the fully trackable unit chains were evaluated by waveforms and estimated locations. Our method is simple and robust; it only requires spike sorting be performed once, independently, per dataset. In order to be more compatible and generalizable with existing sorting methods, we chose Kilosort, one of the most widely used spike sorting methods (<xref ref-type="bibr" rid="bib31">Sauerbrei et al., 2020</xref>; <xref ref-type="bibr" rid="bib33">Stringer et al., 2019</xref>). We show the capability of our method to track neurons with no specific tuning preference (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>).</p><p>The method includes means to identify dataset pairs with very large drift. In our data, we can detect large drift because such datasets have very few reference units, and significantly different EMD cost. For example, datasets 1 and 2 in animal AL036 have very few reference units compared to other datasets (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, AL036). This observation is consistent with the overall relationship between the EMD cost and recovery rate (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Datasets with higher cost tend to have lower unit recovery rate and higher variation in recovery rates. Therefore, these two datasets were excluded in the tracking analysis.</p><p>Our validation relies on identifying reference units. The reference unit definition has limitations. The similarity score is largely driven by PSTHs (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>), the timing of stimulus triggered response, rather than vfp, the response selectivity. As a result, a single neuron can be highly correlated, i.e., similarity score greater than 1, with more than 20 other neurons. For example, in subject AL032 shank 2, one neuron on day 1 has 22 highly correlated neurons on day 2, 4 of which are also within the distance of 30 μm. Non-reference units may also have very similar visual responses: we note that 33 (5 putative neurons and 28 mixed neurons) out of 106 trackable neurons have a similarity score greater than 1 even for days with no reference unit assignment. Coincidentally similar visual responses could potentially contribute to inaccurate assignment of reference units and irregularity in trackable unit analysis. These errors would reduce the measured accuracy of the EMD matching method; since the accuracy is very high (<xref ref-type="fig" rid="fig4">Figure 4</xref>), the impact of mismatches is low.</p><p>We note that the ratio of reference units over KSgood units decreases as recordings are further separated in time (<xref ref-type="fig" rid="fig7">Figure 7</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>). This reduction in fraction of reference units might be partially due to representational drift as well as the fact that the set of active neurons are slightly different in each recording. The vfp similarity of matched neurons decreased to 60% after 40 days (see <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref> supplement).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Summary of dataset.</title><p>(<bold>a</bold>) The recording intervals for each animal. A black dash indicates one recording on that day. (<bold>b</bold>) All recordings are from visual cortex V1 with a 720 μm section of the probe containing 96 recording sites. The blue arrow indicates the main drift direction. (<bold>c</bold>) Examples of visual fingerprint (vfp) and peristimulus time histogram (PSTH) from a high correlation (left column) and a just-above-threshold (right column) correlation unit. Both vfp and PSTH values vary from [–1,1]. (<bold>d</bold>) Kilosort-good and reference unit counts for animal AL032, including units from all four shanks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>An example similarity score (visual fingerprint [vfp] + peristimulus time histogram [PSTH]) heatmap from animal AL032, shank 2, Kilosort-good units between days 1 and 2.</title><p>Each small square represents the similarity score (value range from [–2,2]) between one unit from day 1 and one unit from day 2. A warm colored square indicates a higher score. The clusters are ordered by their physical locations on the probe. There is a diagonal line with brightest color blocks, indicating that units with more similar firing responses across days tend to be physically close. This confirms our assumption that neurons are physically stable over time. Also notice that, on each column, there might be more than one bright block in the more distant clusters. We minimize the effect of distant units by constraining the feasible region during selection of reference units. There are also columns without bright yellow blocks. This happens because some units do not respond to the stimulus and those units are not included in the reference set.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>The Kilosort-good and reference unit counts for the animals AL031 and AL036, as shown for animal AL032 in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>The ratio of the count of reference units to KSgood units decreases for pairs of datasets with larger time intervals.</title><p>However, the variability of the number of reference units is generally large for all time intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92495-fig7-figsupp3-v1.tif"/></fig></fig-group><p>We developed the new tracking algorithm based on an available visual cortex dataset, and used a prominent sorting algorithm (Kilosort 2.5) to spikesort the data. We had reference data to assess the success of the matching and tune parameters. Applying our algorithm in other brain areas and with other sorters may require parameter adjustment. Evaluation of the results in the absence of reference data requires a change to the fitting procedure.</p><p>The algorithm has only two parameters: the weighting factor <inline-formula><mml:math id="inf29"><mml:semantics><mml:mi>ω</mml:mi></mml:semantics></mml:math></inline-formula> that sets the relative weight of waveform distance vs. physical distance, and the <italic>z</italic>-distance threshold that selects matches that are likely correct. We found that recovery rate, and therefore accuracy, is insensitive to the value of <inline-formula><mml:math id="inf30"><mml:semantics><mml:mi>ω</mml:mi></mml:semantics></mml:math></inline-formula> for values larger than 1500 (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), so this parameter does not require precise tuning. However, the false positive rate is strongly dependent on the choice of <italic>z</italic>-distance threshold.</p><p>When reference information (unit matches known from receptive fields or other data) is available, the procedure outlined in <xref ref-type="fig" rid="fig4">Figure 4</xref> can be followed. In that case, the distribution of <italic>z</italic>-distances of known pairs is fit to find the width of the distribution for correct matches. That parameter is then used in the fit of the <italic>z</italic>-distance distribution of all pairs to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. Integrating the distributions of correct and incorrect pairs yields the false positive rate vs. <italic>z</italic>-distance, allowing selection of a <italic>z</italic>-distance threshold for a target false positive rate.</p><p>In most cases, reference information is not available. However, the <italic>z</italic>-distance distributions for correct and incorrect pairs can still be estimated by fitting the distribution of all pairs. In <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> we show the results of fitting the <italic>z</italic>-distribution of all pairs without fixing the width of the distribution of correct matches. The result slightly underestimates this width, and the estimated false positive rate increases. This result is important because it suggests the accuracy estimate from this analysis will be conservative. We detail the procedure for fitting the <italic>z</italic>-distance distribution in Methods section (Algorithm 2).</p><p>As suggested in <xref ref-type="bibr" rid="bib15">Dhawale et al., 2017</xref>, discontinuous recordings will have more false positives. Improving spike sorting and restricting the analysis to reliably sorted units will help decrease the false positive rate. Current spike sorting methods involve fitting many parameters. Due to the stochastic nature of template initialization, only around 60–70% units are found repeatedly in independently executed analysis passes. This leads to unpaired units which decrease EMD matching accuracy. Future users may consider limiting their analysis to the most reliably detected units for tracking; requiring consensus across analysis passes or sorters is a possible strategy. Finally, more frequent data acquisition during experiments will provide more intermediate stages for tracking and involve smaller drift between consecutive recordings.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>Our neuron tracking algorithm uses the EMD optimization algorithm. The minimized distance is a weighted combination of physical distance and ‘waveform distance’: the algorithm seeks to form pairs that are closest in space and have the most similar waveforms. We test the performance of the algorithm by comparing EMD matches to reference pairs determined from visual receptive fields (section Reference set). We calculate two performance metrics. The ‘recovery rate’ is the percentage of reference units that are correctly matched by the EMD procedure. The ‘accuracy’ is the percentage of correctly matched reference units that pass the <italic>z</italic>-distance threshold (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). ‘Putative units’ are units matched by the procedure which do not have reference receptive field information. ‘Chains’ are units that can be tracked across at least three consecutive datasets. The full procedure is summarized in Algorithm 1.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups" id="AL1"><thead><tr><th align="left" valign="bottom">Algorithm 1 Neuron matching procedure</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Input:</bold> channel map, unit cluster label, cluster mean waveforms (with <inline-formula><mml:math id="inf31"><mml:semantics><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:semantics><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> <break/>    rows and <inline-formula><mml:math id="inf33"><mml:semantics><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> columns of channels), and spike times<break/><bold>Step 1</bold> Estimate unit locations<break/>Estimate background amplitude for each unit<break/><bold>for</bold> all KSgood units <inline-formula><mml:math id="inf34"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula><bold>do<break/>   if</bold> peak-to-peak voltage <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>60</mml:mn><mml:mtext>µV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>   Get <inline-formula><mml:math id="inf36"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>’s waveform on channels <inline-formula><mml:math id="inf37"><mml:semantics><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula><break/>   Get the peak-to-peak amplitudes <inline-formula><mml:math id="inf38"><mml:semantics><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> of <inline-formula><mml:math id="inf39"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> background-subtracted waveforms on <break/>   channels <inline-formula><mml:math id="inf40"><mml:semantics><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="inf41"><mml:semantics><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the peak channel<break/>       Estimate the neuron’s 3D location as in <xref ref-type="bibr" rid="bib4">Boussard et al., 2021</xref>;<break/>      <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <italic>x</italic>, <italic>z</italic>, and <italic>y</italic> are the<break/>      horizontal locations, vertical location, and distance of the unit from the probe,<break/>      respectively.<break/>      Find an estimate of the global minimizer of <inline-formula><mml:math id="inf43"><mml:semantics><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> using least-squares<break/>      optimization<break/><bold>   end</bold><break/><bold>end</bold><break/><bold>Step 2</bold> Compute waveform similarity metrics<break/><bold>for</bold> waveforms <inline-formula><mml:math id="inf44"><mml:semantics><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:semantics><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="inf46"><mml:semantics><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> are the set of all units in the<break/>two datasets<break/><bold>do</bold><break/> Centered at peak channel <inline-formula><mml:math id="inf47"><mml:semantics><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:semantics><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, respectively<break/> Get the sets of channels for each unit: <inline-formula><mml:math id="inf49"><mml:semantics><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula><break/> There are <inline-formula><mml:math id="inf50"><mml:semantics><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mn>22</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> channels for each unit<break/> Compute the waveform similarity metric as<break/> <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mspace width="1em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>  for each of the 22 channels<break/><bold>end</bold><break/><bold>Step 3</bold> Between-session drift correction<break/>  Run the EMD with distances in physical and waveform space<break/>  Estimate <italic>z</italic>-distance mode of all matched pairs with Gaussian kernel fit<break/>  Apply correction on physical distances of all units <inline-formula><mml:math id="inf52"><mml:semantics><mml:mrow><mml:mo>∈</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula><break/><bold>Step 4</bold> Unit matching<break/>  Run the EMD with corrected physical distance and waveform metrics<break/>  Set <italic>z</italic>-distance threshold to select unit pairs likely to be the same neuron<break/><bold>Output:</bold> cost <inline-formula><mml:math id="inf53"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>, unit assignments</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Algorithm</title><sec id="s4-1-1"><title>Earth mover’s distance</title><p>The EMD is an optimization-based metric developed in the context of optimal transport and measuring distances between probability distributions. It frames the question as moving dirt, in our case, units from the first dataset, into holes, which here are the neural units in the second dataset. The distance between the ‘dirt’ and the ‘holes’ determines how the optimization program will prioritize a given match. Specifically, the EMD seeks to minimize the total work needed to move the dirt to the holes, i.e., neurons in day 1 to day 2, by solving for a minimum overall effort, the sum of distances (<xref ref-type="bibr" rid="bib2">Bertrand et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Cohen, 1999</xref>).<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ω</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∑</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>∑</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mo>∑</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>in which <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the 3D physical distance between a unit from the first dataset <inline-formula><mml:math id="inf55"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, and a unit from the second dataset <inline-formula><mml:math id="inf56"><mml:semantics><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a scalar representing the similarity between waveforms of units <inline-formula><mml:math id="inf58"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:semantics><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. <inline-formula><mml:math id="inf60"><mml:semantics><mml:mi>ω</mml:mi></mml:semantics></mml:math></inline-formula> is a weight parameter that was tuned to maximize the recovery rate of correctly matched reference units. <italic>F</italic> is the vector of matched objects between the two datasets (see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> for details about selecting weight).</p><p>The EMD has three benefits:</p><list list-type="bullet"><list-item><p>It allows combining different types of information into the ‘distance matrix’ to characterize the features of units.</p></list-item><list-item><p>The EMD can detect homogeneous movement of units (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), thus providing a way for rigid drift correction, as described in section Between-session drift correction.</p></list-item><list-item><p>By minimizing overall distances, the EMD has tolerance for imperfect drift correction, error in the determination of unit positions, and possible non-rigid motion of the units.</p></list-item></list><p>However, since the EMD is an optimization method with no assumptions about the biological properties of the data, it makes all possible matches. We therefore added a threshold on the permissible <italic>z</italic>-distance to select physically plausible matches.</p></sec><sec id="s4-1-2"><title>Calculating the EMD metric</title><p>The unit locations are estimated by fitting 10 peak-to-peak amplitudes from adjacent electrodes and the corresponding channel positions with a 1/<italic>R</italic> distance model (<xref ref-type="bibr" rid="bib4">Boussard et al., 2021</xref>). Unlike <xref ref-type="bibr" rid="bib4">Boussard et al., 2021</xref>, we operate on the mean waveforms for each unit rather than individual spikes. We found using the mean waveform yields comparable results and saves significant computation time. Unit locations are 3D coordinates estimated relative to the probe, where the location of the first electrode on the left column at the tip is considered the origin. The mean waveform is computed by averaging all the spike snippets assigned to the cluster by KS 2.5.</p><p>For 10 channels <inline-formula><mml:math id="inf61"><mml:semantics><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, find the location coordinates <inline-formula><mml:math id="inf62"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> that minimizes the difference between measured amplitudes <inline-formula><mml:math id="inf63"><mml:semantics><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and amplitudes estimated with locations <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi>α</mml:mi><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The locations are used to calculate the physical distance portion of the EMD.</p><p>For the waveform similarity metric, we want to describe the waveform characteristics of each unit with its spatial-temporal waveform at the channels capturing the largest signal. The waveform similarity metric between any two waveforms <inline-formula><mml:math id="inf65"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:semantics><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> in the two datasets is a scalar calculated as a normalized L2 metric (see Algorithm 1 step 2) on the peak channels, namely the channel row with the highest amplitude and 5 rows above and below (a total of 22 channels). The resulting scalar reﬂects the ‘distance’ between the two units in the waveform space and is used to provide information about the waveform similarity of the units. It is used for between-session drift correction and neuron matching. <xref ref-type="fig" rid="fig1">Figure 1c</xref> shows an example waveform of a reference unit.</p></sec><sec id="s4-1-3"><title>Between-session drift correction</title><p>Based on previous understanding of the drift in chronic implants, we assumed that the majority of drift occurs along the direction of the probe insertion, i.e., vertical <italic>z</italic>-direction. This rigid drift amount is estimated by the mode of the <italic>z</italic>-distance distribution of the EMD assigned units using a normal kernel density estimation implemented in MATLAB. We only included KSgood units (<xref ref-type="bibr" rid="bib27">Pachitariu et al., 2016</xref>). The estimated drift is then applied back to correct both the reference units and the EMD matrix by adjusting the <italic>z</italic>-coordinates of the units. For validation, the post-drift correction reference set is compared with the post-drift correction matching results (from step 4 in Algorithm 1).</p></sec></sec><sec id="s4-2"><title>Determining <italic>z</italic>-distance threshold</title><p>Determining the <italic>z</italic>-distance threshold to achieve a target false positive rate requires estimating the widths of the <italic>z</italic>-distance distributions of correct and incorrect pairs. If reference data is available, the <italic>z</italic>-distance distribution of the known correct pairs should be fit to a folded Gaussian as described in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The width of the folded Gaussian, which is the error in determination of the <italic>z</italic>-positions of units, is then fixed in the fit of the <italic>z</italic>-distribution of all pairs found by the algorithm outlined in Algorithm 1. If no reference data is available, the width of the distribution of correct pairs is determined by fitting the <italic>z</italic>-distance distribution of all pairs to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> with the folded Gaussian width as one of the parameters. This procedure is detailed in Algorithm 2. We show two examples of model fitting without reference information in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups" id="AL2"><thead><tr><th align="left" valign="bottom">Algorithm 2 Determining an appropriate <italic>z</italic>-distance threshold</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Input:</bold> <italic>z</italic>-distances of all matched units, target false positive rate, width <inline-formula><mml:math id="inf67"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> of the <italic>z</italic>-distance<break/>      distribution of correct pairs, if available<break/><bold>Step 1</bold> Fit <italic>z</italic>-distance distribution of all pairs to decompose into distributions of correct and<break/>incorrect pairs<break/>    Fit the <italic>z</italic>-distance distribution of all pairs to the sum of a folded Gaussian (for correct<break/>     pairs) and an exponential (for incorrect pairs). If the width <inline-formula><mml:math id="inf68"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> of the distribution of<break/>     correct pairs is known from reference data, fix at that value. Otherwise, include in the<break/>     fit parameters. The functional form is: <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>N</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>    where: <inline-formula><mml:math id="inf70"><mml:semantics><mml:mi>f</mml:mi></mml:semantics></mml:math></inline-formula> = fraction of correct pairs; <inline-formula><mml:math id="inf71"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> = width of the distribution of correct pairs; <break/>    <italic>c</italic> = decay constant of distribution of incorrect pairs; <inline-formula><mml:math id="inf72"><mml:semantics><mml:mi>d</mml:mi></mml:semantics></mml:math></inline-formula> = amplitude normalization; and<break/>    <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>σ</mml:mi><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, the normalization factor of the folded Gaussian.<break/><bold>Step 2</bold> Determine <italic>z</italic>-threshold to achieve a target false positive rate<break/>    For Neuropixels 1.0 and 2.0 probes, the width of the <italic>z</italic>-distance distribution of correct<break/>    matches (<inline-formula><mml:math id="inf74"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula>) should be &lt;10 μm; a larger width, or a very small value of the fraction of<break/>    correct pairs suggests few or no correct matches. In this case, the EMD cost is likely to <break/>    be large as well (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> Animal AL036 first two rows).<break/>    For a range of <italic>z</italic>-values, integrate the <italic>z</italic>-distance distribution of incorrect pairs from 0<break/>    to <italic>z</italic>, and divide by the integral of the distribution of all pairs over that range. This<break/>    generates the false positive rate vs. <italic>z</italic>-distance threshold, as shown in<break/><xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.<break/><bold>Output:</bold> <inline-formula><mml:math id="inf75"><mml:semantics><mml:mi>σ</mml:mi></mml:semantics></mml:math></inline-formula> (uncertainty of position estimation), threshold at the target false positive rate</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>Dataset</title><p>The data used in this work are recordings collected from two chronically implanted NP 2.0 four-shank probes and one chronically implanted one-shank NP 2.0 probe in the visual cortex of three head fixed mice (<xref ref-type="fig" rid="fig7">Figure 7b</xref>, see <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, for experiment details). The recordings were taken while 112 visual stimuli were shown from three surrounding screens (data from <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, Supplement Section 1.2). The same bank of stimuli was presented five times, with order shuffled. The four-shank probes had the 384 recording channels mapped to 96 sites on each shank.</p><p>We analyzed 65 recordings, each from one shank, collected in 17 sessions (5 sessions for animal AL031, 5 sessions for animal AL032, and 7 sessions for animal AL036). The time gap between recordings ranges from 1 to 47 days (<xref ref-type="fig" rid="fig7">Figure 7a</xref>), with recording duration ranging from 1917 to 2522 s. The sample rate is 30 kHz for all recordings. There are a total of 2958 KSgood units analyzed across all animals and shanks, with an average of 56 units per dataset (<xref ref-type="fig" rid="fig7">Figure 7d</xref> and <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>).</p></sec><sec id="s4-4"><title>Reference set</title><p>To track clusters across days, <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, concatenated two recording sessions and took advantage of the within-recording drift correction feature of Kilosort 2.0 to extract spikes from the 2 days with a common set of templates. They first estimated the between-session drift of each recording from the pattern of firing rate and amplitude on the probe and applied a position correction of an integer number of probe rows (15 μm for the probes used). Then two corrected recordings were concatenated and sorted as a single recording. This procedure ensured that the same templates are used to extract spikes across both recordings, so that putative matches are extracted with the same template. A unit from the first half of the recording is counted as the same neuron if its visual response is more similar to that from the same cluster in the second half of the recording than to the visual response of the physically nearest neighbor unit. Using this procedure and matching criteria, 93% of the matches were correct for recordings &lt;16 days apart, and 85% were correct for recordings from 3 to 9 weeks (see <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>). In addition, although mean fingerprint similarity decreases for recordings separated by more than 16 days, this decline is only 40% for the same unit recorded from 40 days apart (see <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, Supplement S3). This procedure, while successful in their setting, was limited to the use of integral row adjustments of the data for between-session drift correction and relied on a customized version of Kilosort 2.0. Although up to three recordings can be sorted together, they must come from recording sessions close in time. In addition, a separate spike sorting session needs to be performed for every pair of recordings to be matched, which is time-consuming and introduces extra sorting uncertainty.</p><p>To find units with matched visual responses, we examine the visual response similarity across all possible pairs. The visual response similarity score follows <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, and consists of two measurements. (1) The PSTH, which is the histogram of the firing of a neuron across all presentations of all images, in a 1800 ms time window starting 400 ms before and ending 400 ms after the stimulus presentation. The PSTH is calculated by histogramming spike times relative to stimulus on time for all stimuli, using 1 ms bins. This histogram is then smoothed with a Gaussian filter. (2) The vfp is the average response of the neuron to each of the 112 images. The vfp is calculated by averaging the spike counts in response to each natural image from the stimulus onset to 1 s afterward across five shuffled trials.</p><p>Following <xref ref-type="bibr" rid="bib32">Steinmetz et al., 2021</xref>, the similarity score between two neurons is the sum of the correlation of the PSTH and the correlation of the vfp across two sessions. The two correlations have values in the range (–1, 1), and the similarity score ranges from (–2, 2).</p><p>The pool of reference units is established with three criteria: (1) The visual response similarity score of the pair, as described above, is greater than 1 and their physical distance, both before and after drift correction, is smaller than 30 μm. A physical distance criterion is necessary, because some units have several potential partners with high visual response similarity (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). We impose the 30 μm threshold on both pre- and post-correction data because the drift is relatively small in our case, and we can reduce false positives by constraining the reference units to be in a smaller region without losing units. In general, one could apply the threshold only on corrected data (after drift correction). (2) A Kruskal-Wallis test is applied on all trials of the vfps to ensure the triggered response to the stimulus is significantly distinguishable from a ﬂat line. (3) Select units from each recording that meet the good criteria in Kilosort. Kilosort assigns a label of either single-unit (good) or multi-unit to all sorted clusters based on ISI violations (<xref ref-type="bibr" rid="bib27">Pachitariu et al., 2016</xref>). This step aims to ensure included units are well separated. If there are multiple potential partners for a unit, the pair with the highest similarity score is selected as the reference unit. The complete pool of reference units includes comparisons of all pairs of recordings for each shank in each animal. The portion of units with qualified visual response ranges from 5% to 61%, depending on the time gap between datasets (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). Overall, these reference units made up 29% of all KSgood units (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>) across all three animals in our dataset. <xref ref-type="fig" rid="fig7">Figure 7c</xref> shows examples of visual responses from a high similarity reference unit and a reference unit with similarity just above threshold.</p></sec><sec id="s4-5"><title>Code sharing</title><p>All code used can be accessed at: <ext-link ext-link-type="uri" xlink:href="https://github.com/janelia-TDHarrisLab/Yuan-Neuron_Tracking">https://github.com/janelia-TDHarrisLab/Yuan-Neuron_Tracking</ext-link> (copy archived at <xref ref-type="bibr" rid="bib20">Janelia-TDHarrisLab, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Supervision, Funding acquisition, Visualization, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-92495-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data may be found at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5522/04/24411841.v1">https://doi.org/10.5522/04/24411841.v1</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Chronic recordings from Neuropixels 2.0 probes in mice</data-title><source>UCL</source><pub-id pub-id-type="doi">10.5522/04/24411841.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>NIH grant U01 NS115587 in part supported TDH and AXY. We thank Claudia Böhm and Albert Lee for allowing us to use their data in Figure 4—figure supplement 2.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Hillel</surname><given-names>A</given-names></name><name><surname>Spiro</surname><given-names>A</given-names></name><name><surname>Stark</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike sorting: Bayesian clustering of non-stationary data</article-title><source>Journal of Neuroscience Methods</source><volume>157</volume><fpage>303</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.04.023</pub-id><pub-id pub-id-type="pmid">16828167</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertrand</surname><given-names>NP</given-names></name><name><surname>Charles</surname><given-names>AS</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Dunn</surname><given-names>PB</given-names></name><name><surname>Rozell</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient tracking of sparse signals via an earth mover’s distance dynamics regularizer</article-title><source>IEEE Signal Processing Letters</source><volume>27</volume><fpage>1120</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1109/LSP.2020.3001760</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Böhm</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Functional specialization and structured representations for space and time in prefrontal cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.16.524214</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Boussard</surname><given-names>J</given-names></name><name><surname>Varol</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>HD</given-names></name><name><surname>Dethe</surname><given-names>N</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Three-Dimensional Spike Localization and Improved Motion Correction for Neuropixels Recordings</article-title><conf-name>NeurIPS Proceedings</conf-name><pub-id pub-id-type="doi">10.1101/2021.11.05.467503</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multiple neural spike train data analysis: state-of-the-art and future challenges</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>456</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1038/nn1228</pub-id><pub-id pub-id-type="pmid">15114358</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Large-scale recording of neuronal ensembles</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>446</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1038/nn1233</pub-id><pub-id pub-id-type="pmid">15114356</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>D</given-names></name><name><surname>Carin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Continuing progress of spike sorting in the era of big data</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>90</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.02.007</pub-id><pub-id pub-id-type="pmid">30856552</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmena</surname><given-names>JM</given-names></name><name><surname>Lebedev</surname><given-names>MA</given-names></name><name><surname>Henriquez</surname><given-names>CS</given-names></name><name><surname>Nicolelis</surname><given-names>MAL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Stable ensemble performance with single-neuron variability during reaching movements in primates</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>10712</fpage><lpage>10716</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2772-05.2005</pub-id><pub-id pub-id-type="pmid">16291944</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chah</surname><given-names>E</given-names></name><name><surname>Hok</surname><given-names>V</given-names></name><name><surname>Della-Chiesa</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>JJH</given-names></name><name><surname>O’Mara</surname><given-names>SM</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Automated spike sorting algorithm based on Laplacian eigenmaps and k-means clustering</article-title><source>Journal of Neural Engineering</source><volume>8</volume><elocation-id>016006</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/8/1/016006</pub-id><pub-id pub-id-type="pmid">21248378</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>JE</given-names></name><name><surname>Magland</surname><given-names>JF</given-names></name><name><surname>Barnett</surname><given-names>AH</given-names></name><name><surname>Tolosa</surname><given-names>VM</given-names></name><name><surname>Tooker</surname><given-names>AC</given-names></name><name><surname>Lee</surname><given-names>KY</given-names></name><name><surname>Shah</surname><given-names>KG</given-names></name><name><surname>Felix</surname><given-names>SH</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Greengard</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A fully automated approach to spike sorting</article-title><source>Neuron</source><volume>95</volume><fpage>1381</fpage><lpage>1394</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.030</pub-id><pub-id pub-id-type="pmid">28910621</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>JE</given-names></name><name><surname>Joo</surname><given-names>HR</given-names></name><name><surname>Fan</surname><given-names>JL</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Barnett</surname><given-names>AH</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Geaghan-Breiner</surname><given-names>C</given-names></name><name><surname>Karlsson</surname><given-names>MP</given-names></name><name><surname>Karlsson</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>KY</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Magland</surname><given-names>JF</given-names></name><name><surname>Pebbles</surname><given-names>JA</given-names></name><name><surname>Tooker</surname><given-names>AC</given-names></name><name><surname>Greengard</surname><given-names>LF</given-names></name><name><surname>Tolosa</surname><given-names>VM</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-density, long-lasting, and multi-region electrophysiological recordings using polymer electrode arrays</article-title><source>Neuron</source><volume>101</volume><fpage>21</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.002</pub-id><pub-id pub-id-type="pmid">30502044</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Rose</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variance and invariance of neuronal long-term representations</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>372</volume><elocation-id>20160161</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0161</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Finding color and shape patterns in images</source><publisher-name>Stanford University</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Colonell</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Ecephys spike sorting</data-title><version designator="ac3357f">ac3357f</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/jenniferColonell/ecephys_spike_sorting">https://github.com/jenniferColonell/ecephys_spike_sorting</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Wolff</surname><given-names>SB</given-names></name><name><surname>Normand</surname><given-names>VA</given-names></name><name><surname>Kopelowitz</surname><given-names>E</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automated long-term recording and analysis of neural activity in behaving animals</article-title><source>eLife</source><volume>6</volume><elocation-id>e27702</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.27702</pub-id><pub-id pub-id-type="pmid">28885141</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>NJ</given-names></name><name><surname>Herzfeld</surname><given-names>DJ</given-names></name><name><surname>Lisberger</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Evaluation and resolution of many challenges of neural spike sorting: a new sorter</article-title><source>Journal of Neurophysiology</source><volume>126</volume><fpage>2065</fpage><lpage>2090</lpage><pub-id pub-id-type="doi">10.1152/jn.00047.2021</pub-id><pub-id pub-id-type="pmid">34788137</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural signatures of cell assembly organization</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>399</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1038/nrn1669</pub-id><pub-id pub-id-type="pmid">15861182</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Improving data quality in neuronal population recordings</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1165</fpage><lpage>1174</lpage><pub-id pub-id-type="doi">10.1038/nn.4365</pub-id><pub-id pub-id-type="pmid">27571195</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Peron</surname><given-names>S</given-names></name><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Wiegert</surname><given-names>JS</given-names></name><name><surname>Tian</surname><given-names>L</given-names></name><name><surname>Oertner</surname><given-names>TG</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multiple dynamic representations in the motor cortex during sensorimotor learning</article-title><source>Nature</source><volume>484</volume><fpage>473</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1038/nature11039</pub-id><pub-id pub-id-type="pmid">22538608</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Janelia-TDHarrisLab</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Yuan-Neuron_Tracking</data-title><version designator="swh:1:rev:fbcebc9458d5a812ab27e8b79bdb86624aa4a5b8">swh:1:rev:fbcebc9458d5a812ab27e8b79bdb86624aa4a5b8</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ac7625475cba1d3c85b0d2cc22b4cba154d62cfb;origin=https://github.com/janelia-TDHarrisLab/Yuan-Neuron_Tracking;visit=swh:1:snp:2ee271e55bd9fffe2bcd9b81bcb0ee66876f1880;anchor=swh:1:rev:fbcebc9458d5a812ab27e8b79bdb86624aa4a5b8">https://archive.softwareheritage.org/swh:1:dir:ac7625475cba1d3c85b0d2cc22b4cba154d62cfb;origin=https://github.com/janelia-TDHarrisLab/Yuan-Neuron_Tracking;visit=swh:1:snp:2ee271e55bd9fffe2bcd9b81bcb0ee66876f1880;anchor=swh:1:rev:fbcebc9458d5a812ab27e8b79bdb86624aa4a5b8</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>KT</given-names></name><name><surname>Kadmon Harpaz</surname><given-names>N</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Wolff</surname><given-names>SBE</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Long-term stability of single neuron activity in the motor system</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>1664</fpage><lpage>1674</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01194-3</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Shokri</surname><given-names>H</given-names></name><name><surname>Kinsella</surname><given-names>I</given-names></name><name><surname>Dethe</surname><given-names>N</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Reyes</surname><given-names>EB</given-names></name><name><surname>Turcu</surname><given-names>D</given-names></name><name><surname>Batty</surname><given-names>E</given-names></name><name><surname>Kim</surname><given-names>YJ</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Goetz</surname><given-names>G</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Carlson</surname><given-names>D</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>YASS: Yet Another Spike Sorter Applied to Large-Scale Multi-Electrode Array Recordings in Primate Retina</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.18.997924</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A review of methods for spike sorting: the detection and classification of neural action potentials</article-title><source>Network</source><volume>9</volume><fpage>R53</fpage><lpage>R78</lpage><pub-id pub-id-type="pmid">10221571</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberti</surname><given-names>WA</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Perkins</surname><given-names>LN</given-names></name><name><surname>Liberti</surname><given-names>DC</given-names></name><name><surname>Leman</surname><given-names>DP</given-names></name><name><surname>Guitchounts</surname><given-names>G</given-names></name><name><surname>Velho</surname><given-names>T</given-names></name><name><surname>Kotton</surname><given-names>DN</given-names></name><name><surname>Lois</surname><given-names>C</given-names></name><name><surname>Gardner</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Unstable neurons underlie a stable learned behavior</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1665</fpage><lpage>1671</lpage><pub-id pub-id-type="doi">10.1038/nn.4405</pub-id><pub-id pub-id-type="pmid">27723744</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>TZ</given-names></name><name><surname>Bondy</surname><given-names>AG</given-names></name><name><surname>Gupta</surname><given-names>D</given-names></name><name><surname>Elliott</surname><given-names>VA</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An approach for long-term, multi-probe Neuropixels recordings in unrestrained rats</article-title><source>eLife</source><volume>9</volume><elocation-id>e59716</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59716</pub-id><pub-id pub-id-type="pmid">33089778</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quian Quiroga</surname><given-names>R</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>173</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/nrn2578</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Nadasdy</surname><given-names>Z</given-names></name><name><surname>Ben-Shaul</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Unsupervised spike detection and sorting with wavelets and superparamagnetic clustering</article-title><source>Neural Computation</source><volume>16</volume><fpage>1661</fpage><lpage>1687</lpage><pub-id pub-id-type="doi">10.1162/089976604774201631</pub-id><pub-id pub-id-type="pmid">15228749</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokni</surname><given-names>U</given-names></name><name><surname>Richardson</surname><given-names>AG</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Motor learning with unstable neural representations</article-title><source>Neuron</source><volume>54</volume><fpage>653</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.04.030</pub-id><pub-id pub-id-type="pmid">17521576</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauerbrei</surname><given-names>BA</given-names></name><name><surname>Guo</surname><given-names>JZ</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Mischiati</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Verma</surname><given-names>N</given-names></name><name><surname>Mensh</surname><given-names>B</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical pattern generation during dexterous movement is input-driven</article-title><source>Nature</source><volume>577</volume><fpage>386</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1869-9</pub-id><pub-id pub-id-type="pmid">31875851</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Aydin</surname><given-names>C</given-names></name><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Beau</surname><given-names>M</given-names></name><name><surname>Bhagat</surname><given-names>J</given-names></name><name><surname>Böhm</surname><given-names>C</given-names></name><name><surname>Broux</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Gardner</surname><given-names>RJ</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Kostadinov</surname><given-names>D</given-names></name><name><surname>Mora-Lopez</surname><given-names>C</given-names></name><name><surname>O’Callaghan</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Sauerbrei</surname><given-names>B</given-names></name><name><surname>van Daal</surname><given-names>RJJ</given-names></name><name><surname>Vollan</surname><given-names>AZ</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Welkenhuysen</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings</article-title><source>Science</source><volume>372</volume><elocation-id>eabf4588</elocation-id><pub-id pub-id-type="doi">10.1126/science.abf4588</pub-id><pub-id pub-id-type="pmid">33859006</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-dimensional geometry of population responses in visual cortex</article-title><source>Nature</source><volume>571</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1346-5</pub-id><pub-id pub-id-type="pmid">31243367</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swindale</surname><given-names>NV</given-names></name><name><surname>Spacek</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spike sorting for polytrodes: a divide and conquer approach</article-title><source>Frontiers in Systems Neuroscience</source><volume>8</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2014.00006</pub-id><pub-id pub-id-type="pmid">24574979</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name><name><surname>Hoenselaar</surname><given-names>A</given-names></name><name><surname>Keliris</surname><given-names>GA</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Recording chronically from the same neurons in awake, behaving primates</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>3780</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.1152/jn.00260.2007</pub-id><pub-id pub-id-type="pmid">17942615</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasil’eva</surname><given-names>LN</given-names></name><name><surname>Badakva</surname><given-names>AM</given-names></name><name><surname>Miller</surname><given-names>NV</given-names></name><name><surname>Zobova</surname><given-names>LN</given-names></name><name><surname>Roshchin</surname><given-names>Vy</given-names></name><name><surname>Bondar’</surname><given-names>IV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Long-term recording of single neurons and criteria for assessment</article-title><source>Neuroscience and Behavioral Physiology</source><volume>46</volume><fpage>264</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1007/s11055-016-0227-8</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92495.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peyrache</surname><given-names>Adrien</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>McGill University</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study proposes a new method for tracking neurons recorded with Neuropixel electrodes across days. The methods and the strength of the evidence are <bold>convincing</bold>, but the authors do not address whether their approach can be generalized to other brain areas, species, behaviors, or tools. Overall, this method will be potentially of interest to many neuroscientists who want to study long-term activity changes of individual neurons in the brain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92495.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Neurons are not static-their activity patterns change as the result of learning, aging, and disease. Reliable tracking of activity from individual neurons across long time periods would enable studies of these important dynamics. For this reason, the authors' efforts to track electrophysiological activity across days without relying on matching neural receptive fields (which can change due to learning, aging, and disease) is very important.</p><p>By utilizing the tightly-spaced electrodes on Neuropixels probes, they are able to measure the physical distance and the waveform shape 'distance' between sorted units recorded on different days. To tune the matching algorithm and to validate the results, they used the visual receptive fields of neurons in the mouse visual cortex (which tend to change little over time) as ground truth. Their approach performs quite well, with a high proportion of neurons accurately matched across multiple weeks.</p><p>This suggests that the method may be useable in other cases where the receptive fields can't be used as ground truth to validate the tracking. This potential extendibility to tougher applications is where this approach holds the most promise. However, the study only looks at one brain area (visual cortex), in one species (mouse), using one type of spike sorter (Kilosort), and one type of behavioral prep (head-fixed). While the authors suggest methods to generalize their technique to other experimental conditions, no validation of those generalizations was done using data from different experimental conditions. Anyone using this method under different conditions would therefore need to perform such validation themselves.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92495.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The manuscript presents a method for tracking neurons recorded with neuropixels across days, based on the matching of cells' spatial layouts and spike waveforms at the population level. The method is tested on neuropixel recordings of the visual cortex carried over 47 days, with the similarity in visual receptive fields used to verify the matches in cell identity.</p><p>This is an important tool as electrophysiological recordings have been notoriously limited in terms of tracking individual neuron's fate over time, unlike imaging approaches. The method is generally sound and properly tested but I think some clarifications would be helpful regarding the implementation of the method and some of the results.</p><p>(1) Page 6: I am not sure I understand the point of the imposed drift and how the value of 12µm is chosen.</p><p>Is it that various values of imposed drift are tried, the EMDs computed to produce histograms as in Fig2c, values of rigid drifts estimated based on the histogram modes, and then the value associated with minimum cost selected? The corresponding manuscript section would need some clarification regarding this aspect.</p><p>(2) The EMD is based on the linear sum, with identical weight, of cell distance and waveform similarity measures. How performance is affected from using a different weighting of the 2 measures (for instance, using only cell distance and no waveform similarity)? It is common that spike waveforms associated to a given neuron appear different on different channels of silicon probes (i.e. the spike waveform changes depending the position of recording sites relative to the neuron), so I wonder if that feature is helping or potentially impeding the tracking.</p><p>(3) Fig.5: I assume the dots are representing time gaps for which cell tracking is estimated. The 3 different groups of colors correspond to the 3 mice used. For a given mouse, I would expect to always see 3 dots (for ref, putative and mixed) for a given tracking gap. However, for mouse AL036 for instance, at tracking duration of 8 days, a dot is visible for mixed but not for ref and putative. How come this is happening?</p><p>(4) Matched visual responses are measured by the sum of correlation of visual fingerprints, which are vectors of cells' average firing rate across visual stimuli, and correlation of PSTHs, which are implemented over all visual stimuli combined. I believe that some information is lost from combining all stimuli in the implementation of PSTHs (assuming that PSTHs show specificity to individual visual stimuli). The authors might consider, as alternative measure of matched visual responses, a correlation of the vector concatenations of all stimulus PSTHs. Such simpler measure would contain both visual fingerprint and PSTH information, and would not lose the information of PSTH specificity across visual stimuli.</p><p>2nd revision</p><p>(1) From reading the authors' response, I could understand several of the points I had previously missed. I still think that some part of the results are not straightforward to understand, the way it is written. Adding a few introductory sentences to the paragraphs (for instance the one related to my previous point #1) would really help the reader comprehend this important work.</p><p>(2) Following on my point #2, the w value used is 1500 and the recovery rate doesn't seems to reach a peak but rather a plateau for larger w values. From such large w value and the absence of a downward trend for increasing values, it would seem that only the 'waveform distance' matter and that the 'location distance' doesn't contribute much to the EMD distance. Is this correct?</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92495.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yuan</surname><given-names>Augustine Xiaoran</given-names></name><role specific-use="author">Author</role><aff><institution>Janelia Research Campus</institution><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Colonell</surname><given-names>Jennifer</given-names></name><role specific-use="author">Author</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Lebedeva</surname><given-names>Anna</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Okun</surname><given-names>Michael</given-names></name><role specific-use="author">Author</role><aff><institution>University of Sheffield</institution><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Charles</surname><given-names>Adam</given-names></name><role specific-use="author">Author</role><aff><institution>John's Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Harris</surname><given-names>Timothy</given-names></name><role specific-use="author">Author</role><aff><institution>Janelia Research Campus</institution><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>Reviewer #1, in both the public review and recommendations to authors, raises the important question of generalizability of the new technique to other brain areas, to analysis with sorters other than Kilosort, and in the absence of reference data. Specifically, how can experimenters working in brain areas other than visual cortex understand if the tracking is functioning, and set the parameters in the tracking pipeline.</p><p>We agree that generalizability of the tracking procedure is a serious issue, especially with respect to other brain areas with varying degrees of measured waveform preservation over time. As the number of potential recording conditions is combinatorial to experimentally test, we instead address these issues in the manuscript by providing a general prescription for interpreting the distribution of vertical distances of matched pairs that can be used for data from any recording using any spike-sorter (Methods section 4.2, Supplement section 8.4, figure S9, paragraphs 7-10 of the Discussion section). This extension of the method allows users to estimate the matching success in the context of their own data, even in the absence of reference data. To address the concern of overfitting, we have also added discussion covering adjustment of the two parameters in the procedure (the relative weight of waveform distance vs. physical distance, and the threshold for accepting matches as real) to the Discussion section.</p><p>Reviewer #2 suggested clarification of the following points in the public review. We answer those here and have also clarified these points in the main text where appropriate.</p><disp-quote content-type="editor-comment"><p>(1) What is the purpose of testing the drift correction with imposed drift (Figure 2, page 6 in the original manuscript), and how the value was chosen?</p></disp-quote><p>To test the ability of EMD to detect substantial drift, we need examples that resemble experimental data, including error in fit unit positions and units with no correct matches. We chose to create these examples by taking waveform and position sets from real data with modest drift, and adding a fixed shift to one dataset. The value of 12 um in the figure is arbitrary, simply an example in the range of real drift. These tests allow us to demonstrate the success of EMD for detection of drift in real data.</p><disp-quote content-type="editor-comment"><p>(2) How is performance affected by using a different weighting of the 2 measures (physical distance and waveform distance) in the EMD?</p></disp-quote><p>Recovery rate (number of reference units successfully matched in EMD) vs weighting of the waveform distance is shown in Supplement section 8.10. Recovery rate increases with low values of waveform weighting, leveling off at a value of 1500. We selected that inflection point for the analysis in this paper, to avoid coincidental matching of physically distant units with similar waveforms.</p><disp-quote content-type="editor-comment"><p>(3) Should the intervals measured in the survival plot in Figure 5 be identical for the three different classes of tracked neurons?</p></disp-quote><p>The plot includes all chains of tracked neurons, which can start on arbitrary days in the set of all recordings (see the definition of chains in section 2.4). As a result, the gaps between days, which determine where there is a point on the plot, can be different for the three sets of neurons (reference, putative, and mixed). We have added a comment to the Figure 5 caption to ensure this is clear.</p><disp-quote content-type="editor-comment"><p>(4) Would other metrics of the similarity of visual responses work better?</p></disp-quote><p>The similarity metric we use was adopted from the original paper using this data (reference 7). We chose to use the same metric both to take advantage of the original authors’ expertise about the data and allow for reasonable comparison of the new technique to theirs. It is correct that this similarity metric alone does not allow for unique matching (see Discussion and Supplement section 8.2). However, the agreement of EMD with reference pairs determined from the combination of position and visual response similarity is very high, suggesting there are few incorrect reference pairs. Any incorrect reference pairs cause an underestimate of the tracking accuracy.</p><disp-quote content-type="editor-comment"><p>(5) Add a definition of ROC.</p></disp-quote><p>Added this definition to the text.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 Recommendation to authors:</bold></p><p>The main text needs proofreading.</p></disp-quote><p>We agree that the manuscript needed more thorough proofreading, and we have made corrections of typos and minor language errors throughout.</p><p>Additional comment from the authors:</p><p>Since the posting of this manuscript, another method for tracking neurons has been introduced:</p><p>Enny H. van Beest, Célian Bimbard, Julie M. J. Fabre, Flóra Takács, Philip Coen, Anna Lebedeva, Kenneth Harris, Matteo Carandini, Tracking neurons across days with high-density probes, bioRxiv 2023.10.12.562040; doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2023.10.12.562040">https://doi.org/10.1101/2023.10.12.562040</ext-link></p></body></sub-article></article>