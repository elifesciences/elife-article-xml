<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">104222</article-id><article-id pub-id-type="doi">10.7554/eLife.104222</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104222.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dynamic estimation of the attentional field from visual cortical activity</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bloem</surname><given-names>Ilona M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7926-6500</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa1">‡</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bakst</surname><given-names>Leah</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2741-5532</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>McGuire</surname><given-names>Joseph T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6259-0809</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Ling</surname><given-names>Sam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6735-2508</contrib-id><email>samling@bu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Department of Psychological &amp; Brain Sciences, Boston University</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meng</surname><given-names>Ming</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008s83205</institution-id><institution>University of Alabama at Birmingham</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>‡</label><p>Computational Cognitive Neuroscience and Neuroimaging, Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and Sciences, Amsterdam, Netherlands</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>11</day><month>09</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP104222</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-10-24"><day>24</day><month>10</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-10-08"><day>08</day><month>10</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.05.611383"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-16"><day>16</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104222.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-07-08"><day>08</day><month>07</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104222.2"/></event></pub-history><permissions><copyright-statement>© 2025, Bloem, Bakst et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Bloem, Bakst et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-104222-v1.pdf"/><abstract><p>Navigating around the world, we must adaptively allocate attention to our surroundings based on anticipated future stimuli and events. This allocation of spatial attention boosts visuocortical representations at attended locations and locally enhances perception. Indeed, spatial attention has often been analogized to a ‘spotlight’ shining on the item of relevance. Although the neural underpinnings of the locus of this attentional spotlight have been relatively well studied, less is known about the size of the spotlight: to what extent can the attentional field be broadened and narrowed in accordance with behavioral demands? In this study, we developed a paradigm for dynamically estimating the locus and spread of covert spatial attention, inferred from visuocortical activity using fMRI in humans. We measured BOLD activity in response to an annulus while participants (four female, four male) used covert visual attention to determine whether more numbers or letters were present in a cued region of the annulus. Importantly, the width of the cued area was systematically varied, calling for different sizes of the attentional spotlight. The deployment of attention was associated with an increase in BOLD activity in corresponding retinotopic regions of visual areas V1–V3. By modeling the visuocortical attentional modulation, we could reliably recover the cued location, as well as a broadening of the attentional modulation with wider attentional cues. This modeling approach offers a useful window into the dynamics of attention and spatial uncertainty.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual attention</kwd><kwd>visual perception</kwd><kwd>attentional field</kwd><kwd>fMRI</kwd><kwd>computational modeling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>SMA-1809071</award-id><principal-award-recipient><name><surname>Bakst</surname><given-names>Leah</given-names></name><name><surname>McGuire</surname><given-names>Joseph T</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS-1755757</award-id><principal-award-recipient><name><surname>McGuire</surname><given-names>Joseph T</given-names></name><name><surname>Ling</surname><given-names>Sam</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>F32-EY029134</award-id><principal-award-recipient><name><surname>Bakst</surname><given-names>Leah</given-names></name><name><surname>McGuire</surname><given-names>Joseph T</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01-EY028163</award-id><principal-award-recipient><name><surname>Ling</surname><given-names>Sam</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH126971</award-id><principal-award-recipient><name><surname>McGuire</surname><given-names>Joseph T</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007161</institution-id><institution>Boston University</institution></institution-wrap></funding-source><award-id>Center for Systems Neuroscience Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Bakst</surname><given-names>Leah</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Visuocortical activity in humans reveals that the spatial focus of covert attention flexibly shifts and expands in accordance with task demands.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>We bounce attention around all the time. Take, for instance, when we’re monitoring oncoming traffic while driving. It isn’t sufficient to attend to the single most likely source of traffic. Instead, attention adaptively broadens and narrows to cover the anticipated spatial distribution of relevant events. The need to spread attention across different swaths of the visual field is driven, to a large degree, by spatial uncertainty: statistical regularities give us a general sense as to where something useful might happen, and this evolves from moment to moment. We navigate this uncertainty by dynamically deploying spatial attention.</p><p>Covert spatial attention improves behavioral performance at attended locations at the cost of performance at unattended locations (<xref ref-type="bibr" rid="bib48">Posner, 1980</xref>), leading to a common metaphor that spatial selective attention acts as a ‘spotlight’ or ‘zoom lens’ (<xref ref-type="bibr" rid="bib56">Shaw and Shaw, 1977</xref>; <xref ref-type="bibr" rid="bib48">Posner, 1980</xref>; <xref ref-type="bibr" rid="bib17">Eriksen and St. James, 1986</xref>; <xref ref-type="bibr" rid="bib11">Carrasco, 2011</xref>). This attentional ‘spotlight’ is characterized by a specific size and location and traverses the visual field based on behavioral demands (<xref ref-type="bibr" rid="bib17">Eriksen and St. James, 1986</xref>; <xref ref-type="bibr" rid="bib12">Castiello and Umiltà, 1990</xref>), selectively boosting information at the attended location within the visual system while suppressing information elsewhere. Animal studies have observed multiplicative increases in visuocortical neural responses at attended locations (<xref ref-type="bibr" rid="bib34">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib33">Maunsell, 2015</xref>), and human neuroimaging studies have found similar focal modulations of population responses (<xref ref-type="bibr" rid="bib29">Kastner et al., 1998</xref>; <xref ref-type="bibr" rid="bib10">Brefczynski and DeYoe, 1999</xref>; <xref ref-type="bibr" rid="bib36">McMains and Somers, 2004</xref>; <xref ref-type="bibr" rid="bib14">Datta and DeYoe, 2009</xref>; <xref ref-type="bibr" rid="bib59">Sprague and Serences, 2013</xref>; <xref ref-type="bibr" rid="bib50">Puckett and DeYoe, 2015</xref>; <xref ref-type="bibr" rid="bib54">Samaha et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Shioiri et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Bloem and Ling, 2019</xref>; <xref ref-type="bibr" rid="bib5">Bartsch et al., 2023</xref>).</p><p>While neural modulation at the locus of attention has been relatively well studied, less is known regarding the neural signatures of the size of the attentional field (<xref ref-type="bibr" rid="bib65">Yeshurun, 2019</xref>). Spreading attention over a larger region of visual space can decrease behavioral performance, but only a handful of studies have interrogated associated effects within visual cortex (<xref ref-type="bibr" rid="bib38">Müller et al., 2003</xref>; <xref ref-type="bibr" rid="bib23">Herrmann et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">Itthipuripat et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Feldmann-Wüstefeld and Awh, 2020</xref>). This is surprising, as the spatial distribution of the attentional field is a key feature in an influential theoretical model of attention (<xref ref-type="bibr" rid="bib53">Reynolds and Heeger, 2009</xref>). The model assumes that the size of the attentional field can be adjusted based on task demands and that the interaction between attentional field size and stimulus-related factors can predict observed attentional gain effects.</p><p>While the studies that have experimentally manipulated the attentional field size found evidence congruent with this prominent theory (<xref ref-type="bibr" rid="bib23">Herrmann et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">Itthipuripat et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Kinıklıoğlu and Boyaci, 2022</xref>), few studies have directly investigated the spatial extent of the attentional window and its concomitant neural representation. One neuroimaging study revealed that the attentional field expanded in the face of greater task-related uncertainty (<xref ref-type="bibr" rid="bib23">Herrmann et al., 2010</xref>), while other studies showed that the responsive area of visual cortex increased in size, coupled with a decrease of the overall population response (<xref ref-type="bibr" rid="bib38">Müller et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Feldmann-Wüstefeld and Awh, 2020</xref>). While these studies are consistent with the notion that the attentional field size can be detected in visual cortex, methods for dynamically recovering location and field size from moment to moment are lacking.</p><p>In this study, we developed a paradigm that allowed us to dynamically characterize the spatial tuning of spatial attention across the visual field. Using fMRI in humans, we examined whether attentional modulation of the BOLD response spanned a larger area of visual cortex when participants were cued to attend to a larger region of space. Behavioral performance confirmed that participants could successfully allocate their attention to different-sized swaths of the visual field. This deployment of attention was associated with a modulation in cortical activity in the corresponding retinotopic areas of visual cortex. By modeling the location and spread of the visuocortical modulation, we dynamically recovered the cued location from the attentional activity with a high degree of fidelity, together with a broadening of the attentional modulation for wider attentional cues.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral performance indicates effective deployment of covert spatial attention</title><p>We set out to investigate the spatial distribution of attentional modulation within visual cortex. To do so, we first ensured that participants (<italic>n</italic>=8) could successfully allocate covert spatial attention to cued portions of the visual field. During the experiment, participants’ task was to fixate the center of the screen and report whether there were more <italic>numbers</italic> or <italic>letters</italic> in a cued peripheral region (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The cued region varied in location and width: it could be centered on any of 20 polar angles and could span any of four widths (18°, 54°, 90°, and 162° of polar angle). Task performance indicated that participants used the cue effectively, as the proportion of correct responses was significantly above chance for all width conditions (<xref ref-type="fig" rid="fig1">Figure 1b</xref>; t-test, all p&lt;0.001). We verified, with eye tracking, that participants performed the task using peripheral vision while maintaining central fixation. The upper bound of the 95% CI for each participant’s average gaze eccentricity ranged from 0.29° (degrees of visual angle) to 0.64° (mean = 0.48°; <xref ref-type="fig" rid="fig1">Figure 1c</xref>), suggesting that gaze did not exceed the cue annulus at fixation and that participants used covert spatial attention to perform the task.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and behavioral performance.</title><p>(<bold>a</bold>) Task schematic. Participants were instructed to maintain central fixation and use covert spatial attention to determine whether there were more numbers or letters present within a cued region of a white noise annulus. On each trial, the red cue was displayed alone for 1.35 s and remained present throughout the trial. Twenty digits and letters were then presented for 0.5 s, equally spaced and overlaid on the annulus. Participants had 1.25 s to indicate via button press whether more digits or letters were present in the cued region. The cue remained stable for five trials (10 TRs, 15.5 s), had a width of 1, 3, 5, or 9 segments (18°, 54°, 90°, or 162°), and was centered on any of the 20 digit/letter slots. (<bold>b</bold>) Behavioral task performance: group mean accuracy for each cue width. Error bars are SEM; gray circles show individual participants (n=8). (<bold>c</bold>) Group mean gaze eccentricity (in degrees of visual angle) for each cue width, conventions as in (<bold>b</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Attentional modulation of BOLD responses broadens with cue width</title><p>We assessed the spatial distribution of attention by visualizing how the BOLD response was modulated by the location and width of the cue. To do so, we used each voxel’s population receptive field (pRF) to project BOLD responses for each attentional cue into the visual field. The resulting 2D visual field maps were averaged across trials for each cue width by rotating the maps, so the attentional cue aligned to 0° polar angle (right horizontal meridian). The reconstructed visual field maps revealed that increasing cue width led to a concomitant broadening of attentional modulation in cortex (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). While this pattern was evident in all three early visual regions (V1–V3), the effect appeared to strengthen when ascending the visuocortical hierarchy.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Spatial distribution of attentional modulation.</title><p>(<bold>a</bold>) BOLD response projected into the visual field for each attentional cue width. Heatmaps represent the group mean BOLD activity using each voxel’s population receptive field (pRF) location within the visual field, shown separately for V1, V2, and V3. Maps were rotated to align all attentional cue locations to 0° polar angle (rightward). Concentric circles indicated by black dashed lines represent the location of the white noise annulus. (<bold>b</bold>) Average spatial modulation profiles at the eccentricity of the annulus. The spatial profiles were recentered to 0° polar angle based on the cue location. Solid lines represent the group mean BOLD response and shaded regions the SEM across participants (n=8).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig2-v1.tif"/></fig><p>Next, we computed the one-dimensional (1D) profile of attentional modulation at a fixed eccentricity. We were able to do this because we manipulated the location of the attentional field only as a function of polar angle, so all cues directed the attentional field to iso-eccentric locations. We selected voxels with pRFs that overlapped the white noise annulus and sorted them according to their polar angle preference.</p><p>For visualization purposes, the spatial response modulations were recentered to align all cues at 0° polar angle and averaged across trials for each cue width separately. Much like in the visual field reconstructions, there was a clear attentional modulation centered on 0°, which broadened and shifted downward with cue width – a pattern that was particularly evident in area V3 (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p></sec><sec id="s2-3"><title>Dynamic model-based recovery of the attentional field</title><p>We next applied a modeling approach to estimate the location and width of attentional modulation, allowing us to further investigate the spread of attention in visual cortex. To do this, we averaged the spatial response profiles across TRs within each 10-TR block, in which the cue maintained a consistent location and width, yielding between 27 and 53 averaged spatial response profiles per participant for each width condition. We fit a generalized Gaussian function to each of these spatial profiles to estimate the location and width of attentional modulation per spatial profile (see <xref ref-type="fig" rid="fig3">Figure 3a</xref>). The width of attentional modulation was quantified in terms of the full width at half maximum (FWHM) of the best fitting model prediction (see <xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Modeling approach.</title><p>(<bold>a</bold>) The generalized Gaussian model is characterized by parameters for location <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$(\mu)$\end{document}</tex-math></alternatives></inline-formula>, scale <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$(\sigma)$\end{document}</tex-math></alternatives></inline-formula>, and shape <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$(\beta )$\end{document}</tex-math></alternatives></inline-formula>. (<bold>b</bold>) Example model fits for two spatial profiles. Dots indicate BOLD response for two attentional cues differing in position and width. Solid lines indicate the best fitting model estimate. To quantify the attentional field, we extracted the location and gain (dashed arrows), as well as the width (full width at half maximum (FWHM); solid arrows).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig3-v1.tif"/></fig><p>Can we dynamically recover the attentional field from activity within visual cortex? Model fits explained a substantial proportion of variance in the spatial profiles of BOLD activity (<bold>V1</bold>: for 18° cues, mean [standard deviation] of <italic>R</italic><sup>2</sup>=0.42 [0.03]; for 54° cues, 0.43 [0.03]; for 90° cues, 0.44 [0.03]; for 162° cues, 0.42 [0.03]; <bold>V2</bold>: for 18° cues, 0.51 [0.05]; for 54° cues, 0.54 [0.05]; for 90° cues, 0.54 [0.04]; for 162° cues, 0.55 [0.04]; <bold>V3</bold>: for 18° cues, 0.50 [0.03]; for 54° cues, 0.56 [0.04]; for 90° cues, 0.55 [0.03]; for 162° cues, 0.51 [0.02]). To interpret the estimated model parameters, we excluded the bottom 20% of fits based on a pooled <italic>R</italic><sup>2</sup> across V1, V2, and V3, leaving roughly equal proportions of included blocks across cue width conditions (18°: mean [standard deviation]=0.78 [0.04], 54°: 0.83 [0.05], 90°: 0.83 [0.04], 162°: 0.77 [0.07]).</p><p>To assess how well the model-estimated attentional field matched the cued location, we first calculated the angular error between the cue center and the model’s estimated location parameter. The angular error distribution across blocks, separated by width condition, is shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> for one example participant to display block-to-block variation. The model reliably captured the location of the attentional field with low angular error and with no systematic directional bias. This result was observed across participants. We next examined the absolute angular error to assess the overall accuracy of our estimates. The group mean absolute angular error in V1 was 41.9° (SEM = 2.86°), in V2 was 32.2° (2.31°), and in V3 was 24.7° (1.54°). Additionally, the absolute angular error did not vary linearly with the width of the cue in V1 or V2 (regression slopes tested against zero at the group level using a t-test; V1: <italic>t</italic>(7)=0.65, p=0.537; V2: <italic>t</italic>(7)=1.24, p=0.253; <xref ref-type="fig" rid="fig5">Figure 5</xref>). In V3, we observed a small but statistically significant increase in absolute angular error associated with greater cue widths (mean slope = 1.4, <italic>t</italic>(7)=4.18, p<italic>=</italic>0.004).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Attentional field parameter estimates for an example participant.</title><p>The full parameter estimate distributions across blocks for location, width, gain, and baseline are shown for one example participant in V1, V2, and V3. Median parameter estimates are shown by the white points, with the box plot representing the 25th to 75th percentile, and whiskers extending to all non-outlier points.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig4-v1.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Attentional field parameter estimates.</title><p>Group results for location, width, gain, and baseline estimates. Overall group mean and SEM (n=8) are shown in solid black, separated by cue width and brain region. Individual participant median estimates are shown in gray. The example participant from <xref ref-type="fig" rid="fig4">Figure 4</xref> is indicated by a denser dashed dark gray line with triangle symbols to aid in comparison.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig5-v1.tif"/></fig><p>Next, we evaluated the width of the attentional field by visualizing the distribution of FWHM for the same example participant (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and at the group level (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Confirming the broadening of the attentional field observed in the visual field reconstruction maps, we found that the estimated FWHM increased with greater cue widths in V2 and V3 (V2 <italic>t</italic>(7)=5.63, p&lt;0.001; V3 <italic>t</italic>(7)=6.49, p&lt;0.001). The effect was not statistically significant in V1 (<italic>t</italic>(7)=1.68, p=0.136).</p><p>We also assessed the gain of the attentional modulation in the model (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref> for the example participant and group data, respectively). We observed no significant relationship between gain and cue width in V1 and V2 (V1 <italic>t</italic>(7)=–0.54, p=0.605; V2 <italic>t</italic>(7)=–2.19, p=0.065), though we did find a significant effect in V3, illustrating that gain decreases with cue width (<italic>t</italic>(7)=–3.12, p=0.017). We also found that the overall gain was greater in V2 and V3 compared to V1 (paired t-test, both p≤0.01).</p><p>Finally, we examined the baseline offset (<xref ref-type="fig" rid="fig4">Figure 4</xref> example participant, and <xref ref-type="fig" rid="fig5">Figure 5</xref> group data). No significant relationship was observed between cue width and baseline offset in any of the three brain regions (V1, <italic>t</italic>(7)=–1.05, p=0.330; V2, <italic>t</italic>(7)=–2.00, p=0.086; V3, <italic>t</italic>(7)=–1.61, p=0.152).</p></sec><sec id="s2-4"><title>Temporal interval analysis</title><p>In the previous analyses, we leveraged the fact that the attentional cue remained constant for five-trial blocks (spatial profiles were computed by averaging BOLD measurements across a block of 10 TRs). We next examined the degree to which we were able to recover the attentional field on a moment-by-moment (TR-by-TR) basis. To do this, we systematically adjusted the number of TRs that contributed to the averaged spatial response profile. To maintain a constant number of observations across the temporal interval conditions, we randomly sampled a subset of TRs from each block. This allowed us to determine the amount of data needed to recover the attentional field, with a goal of examining the usability of our modeling approach in future paradigms involving more dynamic deployment of spatial attention.</p><p>When we systematically varied the number of TRs included for each model fit (1, 2, 3, 5, or 10 TRs), we found a significant effect of cue width on recovered FWHM when averaging two or more TRs in V3 (all <italic>t</italic>(7)≥2.38, all p≤0.049), and 10 TRs in V2 (results as reported in prior section; <xref ref-type="fig" rid="fig6">Figure 6a</xref>). As described above, V1 did not reliably show a significant relationship between cue width and FWHM, even when averaging 10 TRs. We found that increasing the number of TRs had a small but significant positive effect on FWHM estimates in V2 and V3 (V2, mean slope = 2.7, <italic>t</italic>(7)=2.95, p=0.021; V3, mean slope = 1.16, <italic>t</italic>(7)=3.22, p=0.015), although a significant effect was not observed in V1 (<italic>t</italic>(7)=1.82, p=0.111).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Effect of number of TRs.</title><p>Model fits were computed using BOLD data averaged across different temporal intervals (1, 2, 3, 5, or 10 TRs). Group means (with SEM (n=8)) are plotted for (<bold>a</bold>) full width at half maximum (FWHM), (<bold>b</bold>) absolute angular error, (<bold>c</bold>) gain, (<bold>d</bold>) baseline offset, and (<bold>e</bold>) <italic>R</italic><sup>2</sup>, separated by cue width, brain region, and the number of TRs used for each model fit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig6-v1.tif"/></fig><p>The number of TRs significantly affected the absolute angular error associated with the estimated location of the attentional field (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). Error magnitude decreased with TRs in all three visual regions (all <italic>t</italic>(7)≤–4.48, all p≤0.003), suggesting that more data yielded more accurate estimates, though absolute angular error remained consistently below chance (90°) even when fitting the model to single-TR BOLD responses. Absolute angular error remained stable across width conditions in V1 and V2 (V1, <italic>t</italic>(7)=–0.55, p=0.598; V2, <italic>t</italic>(7)=1.92, p=0.098), though we found that larger cue width had a small but significant effect associated with larger errors in V3 (mean slope = 0 .02, <italic>t</italic>(7)=3.28, p=0.014).</p><p>The estimated gain of the attentional modulation showed a dependence on number of TRs, with more TRs associated with lower gain estimates in V1 and V3 (V1, <italic>t</italic>(7)=–7.21, p&lt;0.001; V3, <italic>t</italic>(7)=–9.97, p&lt;0.001), though this was not clearly observed in V2 (<italic>t</italic>(7)=–1.60, p<italic>=</italic>0.154). There was no evident dependence of gain on cue width in V1 and V2 (V1 <italic>t</italic>(7)=–0.19, p=0.856; V2 <italic>t</italic>(7)=–2.34, p=0.052), though we did observe a significant relationship in V3 (<italic>t</italic>(7)=–2.86, p=0.024; <xref ref-type="fig" rid="fig6">Figure 6c</xref>).</p><p>The baseline offset tended to increase with number of TRs across all three brain regions (V1, <italic>t</italic>(7)=8.79, p&lt;0.001; V2, <italic>t</italic>(7)=6.5, p&lt;0.001; V3, <italic>t</italic>(7)=5.59, p=0.001; <xref ref-type="fig" rid="fig6">Figure 6d</xref>). Baseline offset did not show a significant dependence on cue width in any region (V1, <italic>t</italic>(7)=1.47, p=0.186; V2, <italic>t</italic>(7)=–2.16, p=0.068; V3, <italic>t</italic>(7)=–1.67, p=0.139).</p><p>Finally, the model’s goodness of fit improved with more data, with larger <italic>R</italic><sup>2</sup> associated with greater numbers of TRs included in the average profiles (all <italic>t</italic>(7)≥2.99, all p≤0.020), though all <italic>R</italic><sup>2</sup> were above 0.3 across all visual regions even for single-TR model fits. We did not observe a dependence of <italic>R</italic><sup>2</sup> on cue width (all <italic>t</italic>(7)≤1.26, all p≥0.249; <xref ref-type="fig" rid="fig6">Figure 6e</xref>).</p></sec><sec id="s2-5"><title>Width of the attentional field mimics perceptual modulation</title><p>While the attentional field broadened as expected when participants were cued to attend to a larger portion of the white noise annulus, the size of the estimated attentional modulation was greater than the true size of the cued region. The cue width varied between 18° and 162°, whereas the width estimate derived from spatial profiles of BOLD modulation varied between 103° and 179° (<xref ref-type="fig" rid="fig5">Figure 5</xref>). We wondered what the underlying cause of this disparity might be. One possibility is that the BOLD-derived FWHM might tend to overestimate the retinotopic extent of the modulation, perhaps driven by binning and smoothing processing steps to create the 1D spatial profiles. If this were the case, we would expect to obtain similar FWHM estimates when modeling the perceptual modulations as well. Alternatively, the true subjective attentional field might be consistently broader than cued, despite the presence of nearby distractors. If this were the case, modulation driven by perceptual differences should <italic>not</italic> result in the same large FWHM estimates.</p><p>To address this, we compared our estimates of the attentional field with equivalent estimates for spatial profiles induced by a perceptual manipulation. In this additional experiment, we varied the contrast intensity of sections of the white noise annulus. Participants were not asked to deploy spatial attention to the stimulus and were instead instructed to perform a color change detection task at fixation. The regions of increased noise contrast matched the attentional cue widths (18°, 54°, 90°, and 162°, plus an additional intermediate width of 126°) and were centered on one of the four cardinal locations (0°, 90°, 180°, 270° polar angle).</p><p>As expected, we observed a broadening of the spatial profile of BOLD modulation in all three visual areas as the region of increased contrast widened (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). Using an identical modeling procedure, we estimated the spatial profile of the <italic>perceptual</italic> BOLD modulation. The model-based estimates revealed that the mean absolute angular error between the model-estimated location and the center of the contrast stimulus had no significant dependence on contrast width in any of the three brain regions (magnitude of all <italic>t</italic>(4)≤0.915, all p≥0.412). The recovered FWHM increased with contrast width in both V1 and V3 (<xref ref-type="fig" rid="fig7">Figure 7b</xref>; V1, <italic>t</italic>(4)=6.94, p=0.002; V3 <italic>t</italic>(4)=11.34, p<italic>&lt;</italic>0.001), though this effect was not clearly observed in V2 (<italic>t</italic>(4)=1.37, p=0.242). The estimated gain modulation also did not show a relationship to contrast width in any of the visual areas (magnitude of all <italic>t</italic>(4)≤1.71, all p≥0.163). Finally, we did not observe a significant relationship between contrast width and baseline offset in any visual area (magnitude of all <italic>t</italic>(4)≤1.93, all p≥0.125). In sum, the group results for model estimates revealed that: (1) the model was highly accurate in estimating the location of the contrast increment; (2) FWHM of the spatial profiles broadened across contrast widths, (3) the gain and baseline remained stable across contrast widths (<xref ref-type="fig" rid="fig7">Figure 7b</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Spatial distribution of perceptual modulation.</title><p>(<bold>a</bold>) Spatial profiles of perceptual modulation. Solid lines represent the group mean BOLD activity and shaded regions the SEM (n=5). (<bold>b</bold>) Group-level parameter estimates. Overall group mean and SEM are shown for the absolute angular error, full width at half maximum (FWHM), gain, and baseline, separated by contrast width and brain region. (<bold>c</bold>) Comparison of FWHM estimates obtained from the attentional manipulation and the physical contrast manipulation. Dot color indicates brain region; each point represents the mean FWHM for a given width condition across participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-fig7-v1.tif"/></fig><p>Mirroring the results from the attentional manipulation, FWHM estimates systematically exceeded the nominal size of the perceptually modulated region of the visual field. Comparing the estimated FWHMs of the perceptual and attentional spatial profiles (<xref ref-type="fig" rid="fig7">Figure 7c</xref>) revealed that the estimated widths were highly correlated (Pearson correlation <italic>r</italic>=0.749 across width conditions and visual regions), and though the estimated FWHMs from the perceptual task appear to be smaller, they did not significantly differ from the FWHMs derived from the attentional task (t-test p=0.181). Importantly, the relative differences in FWHM show meaningful effects of both cue and contrast width in a similar manner for attentional and perceptual forms of modulation.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We investigated the topographic spread of spatial attention in human visual cortex by characterizing the spatial profile of BOLD responses while participants attended to different portions of the visual field. Behavioral performance confirmed that participants used the fixation cue to dynamically allocate attention to different swaths of the visual field. Attention allocation was associated with a modulation in the BOLD response in corresponding retinotopic areas of visual cortex. To characterize the topography of that modulation, our approach involved selecting voxels with pRF preferred eccentricities that overlapped our white noise annulus and organizing those voxels into 1D profiles of attentional modulation as a function of preferred polar angle. This allowed us to model the location and spread of the attentional field and test how well it tracked the nominal location and width of the cue presented at fixation. Using a generalized Gaussian model, the cued location could be recovered with high fidelity. We observed a broadening of the estimated attentional field in areas V2 and V3 with the cue width, suggesting our method was capable of dynamically recovering the location and size of the attentional field from moment to moment. We also found that the estimated spatial spread of the attentional modulation (as indicated by the recovered FWHM) was consistently wider than the cued region itself. We therefore compared the spread of the attention field with the spatial profile of a <italic>perceptually</italic> induced width manipulation. The results were comparable in both the attentional and perceptual versions of the task, suggesting that cueing attention to a region results in a similar 1D spatial profile to when the stimulus contrast is simply increased in that region.</p><p>This work builds on the concept of an attentional ‘spotlight’ or ‘zoom lens’ that has long been theorized to aid in spatial attention (<xref ref-type="bibr" rid="bib56">Shaw and Shaw, 1977</xref>; <xref ref-type="bibr" rid="bib48">Posner, 1980</xref>; <xref ref-type="bibr" rid="bib17">Eriksen and St. James, 1986</xref>; <xref ref-type="bibr" rid="bib11">Carrasco, 2011</xref>). By flexibly adjusting and shifting the focus of the spotlight, visual representations are selectively enhanced in a region of the visual field. However, the empirical evidence demonstrating that attention can change its <italic>spread</italic> across the visual field by modulating brain responses is surprisingly lacking (<xref ref-type="bibr" rid="bib65">Yeshurun, 2019</xref>). Our understanding of how the attentional window interacts with spatial representations is mainly based on behavioral reports (<xref ref-type="bibr" rid="bib21">Gobell et al., 2004</xref>; <xref ref-type="bibr" rid="bib44">Palmer and Moore, 2009</xref>; <xref ref-type="bibr" rid="bib23">Herrmann et al., 2010</xref>; <xref ref-type="bibr" rid="bib63">van Beilen et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Taylor et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Huang et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Kinıklıoğlu and Boyaci, 2022</xref>), but see <xref ref-type="bibr" rid="bib38">Müller et al., 2003</xref>; <xref ref-type="bibr" rid="bib24">Hopf et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Itthipuripat et al., 2014</xref>; <xref ref-type="bibr" rid="bib61">Tkacz-Domb and Yeshurun, 2018</xref>; <xref ref-type="bibr" rid="bib19">Feldmann-Wüstefeld and Awh, 2020</xref>. We introduced a novel modeling approach that recovered the location and the size of the attentional field. Our data show that the estimated spatial spread of attentional modulation (as indicated by the recovered FWHM) consistently broadened with the cue width, replicating prior work (<xref ref-type="bibr" rid="bib38">Müller et al., 2003</xref>; <xref ref-type="bibr" rid="bib23">Herrmann et al., 2010</xref>). Our results go beyond prior work by linking the spatial profiles to pRF estimates, allowing us to quantify the spread of both attentional and perceptual modulation in degrees of polar angle. Interestingly, the FWHM estimates for the attentional and perceptual spatial profiles were highly similar. Additionally, for area V3, we replicate that the population response magnitude decreased with cue width (<xref ref-type="bibr" rid="bib38">Müller et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Feldmann-Wüstefeld and Awh, 2020</xref>). One innovation of our method is that it directly reconstructs attention-driven modulations of responses in visual cortex, setting it apart from other methods, such as inverted encoding models (e.g. <xref ref-type="bibr" rid="bib59">Sprague and Serences, 2013</xref>). Finally, we demonstrated that our method has potential to be used in more dynamic settings, in which changes in the attentional field need to be tracked on a shorter timescale.</p><p>The ability to change the size of the attentional field is a crucial component in an influential theoretical model of attention. This model proposes that the interaction between stimulus properties (such as its size and specific features) and the attentional field can explain a wide variety of attentional effects reported in behavioral and neurophysiological studies (<xref ref-type="bibr" rid="bib23">Herrmann et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">Itthipuripat et al., 2014</xref>; <xref ref-type="bibr" rid="bib8">Bloem and Ling, 2019</xref>; <xref ref-type="bibr" rid="bib28">Jigo et al., 2021</xref>). The present study sought to address this gap, with our results showing that the visuocortical attentional field broadened as we increased the cue width (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This provides compelling evidence that the attention-related cortical response can, in fact, flexibly vary in its position and spatial distribution.</p><p>The observed effects of attentional field width were unlikely to be directly attributable to variation in task difficulty. Participants’ task in our study was to discriminate whether more numbers or more letters were presented within a cued region of an iso-eccentric annulus of white noise. For our different cue widths, the ratios of numbers and letters were selected to be as similar as possible given the size and spacing of our stimuli. Changes in accuracy across the three larger cue widths were small and non-monotonic, implying task difficulty was dissociable from width per se. This dissociation bolsters the interpretability of our model fits; nevertheless, future work should further investigate how task difficulty interacts with the spread of the attentional field and the amplitude of attention-related BOLD effects (cf. <xref ref-type="bibr" rid="bib51">Ress et al., 2000</xref>).</p><p>In this study, we modeled the attentional field using a 1D distribution. This approach aligned with our experimental design, as the attentional cue was manipulated only as a function of polar angle. However, we know that spatial processing varies substantially as a function of eccentricity. Spatial resolution is highest at the fovea and rapidly drops in the periphery (<xref ref-type="bibr" rid="bib2">Anton-Erxleben and Carrasco, 2013</xref>). The spatial distribution of attention will presumably also vary with eccentricity and will likely take on different functional properties close to the fovea, where spatial resolution is high, compared to the far periphery where spatial resolution is low (<xref ref-type="bibr" rid="bib26">Intriligator and Cavanagh, 2001</xref>; <xref ref-type="bibr" rid="bib28">Jigo et al., 2021</xref>). Future work can help provide a better understanding of the contribution of spatial attention by considering how the attentional field interacts with these well-described spatial variations across the visual field. Measuring the full spatial distribution of the attentional field (across both eccentricity and polar angle) will shed light on how spatial attention guides perception by interacting with the nonuniformity of spatial representations.</p><p>The spread of the attentional field likely influences the degree to which spatial resolution at the attended location is transformed, leading to enhanced behavioral performance. Spatial attention was vital for this task, as enhanced spatial perception allowed the participants to better discriminate all stimuli within the cued region (<xref ref-type="bibr" rid="bib2">Anton-Erxleben and Carrasco, 2013</xref>). Future work could unpack the degree to which the size of the attentional field influences the spatial resolution of visual cortical representations (<xref ref-type="bibr" rid="bib32">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib64">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Tünçok et al., 2024</xref>), and how this influences spatial perception.</p><p>Beyond addressing core questions related to the function of spatial attention, this method also lays groundwork for addressing questions about spatial predictive uncertainty and belief updating. Prior work on these topics has relied almost entirely on inferring participants’ predictions from their behavior, often requiring participants to report overt point predictions (<xref ref-type="bibr" rid="bib40">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="bib35">McGuire et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">d’Acremont and Bossaerts, 2016</xref>; <xref ref-type="bibr" rid="bib42">Nassar et al., 2019</xref>), or inferring participants’ predictions from their sequences of decisions (<xref ref-type="bibr" rid="bib15">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib6">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Payzan-LeNestour and Bossaerts, 2011</xref>; <xref ref-type="bibr" rid="bib46">Payzan-LeNestour et al., 2013</xref>). These approaches have shed light on how we dynamically adapt our learning and belief updating processes over time in differently structured contexts. However, methods for recovering information about full predictive belief distributions have been limited, relying on indirect measurements such as eye movements (<xref ref-type="bibr" rid="bib43">O’Reilly et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Bakst and McGuire, 2021</xref>; <xref ref-type="bibr" rid="bib4">Bakst and McGuire, 2023</xref>), and physiological measures of uncertainty and surprise in EEG and pupillometry (<xref ref-type="bibr" rid="bib49">Preuschoff et al., 2011</xref>; <xref ref-type="bibr" rid="bib41">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Nassar et al., 2019</xref>). The methods developed here offer a potential way to recover the location and width of a spatial predictive distribution via the attentional field in contexts in which it is unknown a priori and might be dependent on how a given participant has integrated previous sequential evidence. Future work could extend this method to more directly interrogate how predictive uncertainty is represented throughout the brain on a moment-by-moment basis.</p><p>In summary, we found evidence that people could dynamically adapt the spread of spatial attention, and that the retinotopic extent of attentional modulation of the BOLD response reflected this dynamic adaptation. These findings address a gap in our understanding of spatial attentional control, supporting core theoretical models of attention. Our modeling approach also lays the groundwork to address further questions related to how the attentional field interacts with the nonuniformity of spatial representations and how uncertainty in spatial contexts is represented in the human brain.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Eight healthy adults (four female, four male, mean age = 30) participated in the main attention experiment, five of whom also participated in a second experiment featuring a contrast manipulation. All participants had normal or corrected-to-normal vision. All procedures were approved by the Boston University Institutional Review Board, and informed consent was obtained from all participants.</p></sec><sec id="s4-2"><title>Apparatus and stimuli</title><p>Participants were presented with stimuli generated using PsychoPy (v1.85.1; <xref ref-type="bibr" rid="bib47">Peirce, 2007</xref>) on a MacBook Pro. The visual stimuli were displayed on a rear-projection screen (subtending ~20°×16° visual angle) using a VPixx Technologies PROPixx DLP LED projector (maximum luminance 306 cd/m<sup>2</sup>). Participants viewed the screen through a front surface mirror. Participants were placed comfortably in the scanner with padding to minimize head motion.</p></sec><sec id="s4-3"><title>Procedure</title><sec id="s4-3-1"><title>Attentional width manipulation</title><p>Participants were instructed to fixate a central point (radius 0.08° visual angle) while dynamic pixelwise white noise (flickering at 10 Hz, 50% contrast) was presented in the periphery (annulus spanning 4.6° to 7.4° visual angle). The annulus was segmented into 20 bins (18° polar angle per bin) by white grid lines radiating from a white circle at the center of the screen (radius 0.25°), passing behind the annulus, and terminating at 8.5° eccentricity. In the middle of each bin, a number or letter (height: 2.1°) was superimposed on the white noise annulus (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). For a subset of the participants (three out of eight), the screen distance inside the scanner was changed; therefore, for those participants, the letter size was 1.86° visual angle, and the white noise annulus spanned 4.1° to 6.5° visual angle. The set of possible letters included all lowercase letters of the Latin alphabet except a, b, e, g, i, o, and u. The set of numbers included 2, 3, 4, 5, 7, and 8.</p><p>Participants were cued to attend covertly to a contiguous subset of the bins, and their task was to report, via button press, whether there were more <italic>numbers</italic> or <italic>letters</italic> present within the cued region. The cue was a bold red segment on the central white circle, which corresponded to 1, 3, 5, or 9 bins (18°, 54°, 90°, or 162° polar angle; see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). The true proportion of letters versus numbers was controlled within each cue width condition. For cued regions of 1 bin, there was either a single number or letter in the bin. For cued regions of 3 bins, the ratio was always 2:1 (either two numbers and one letter or vice versa). For cued regions of 5 bins, the ratio was 3:2, and for cued regions of 9 bins, the ratio was 6:3. The ratios were selected to be as similar as possible given the size and spacing of our stimuli (aside from the one-bin cue, the proportions for the other cues were 0.67, 0.60, and 0.67). Cues could be centered on any of the 20 bins.</p><p>Participants completed 8–12 runs of the task (mean = 10.4), with each run lasting 341 s and containing 100 trials. Each cue remained constant for a block of five trials (lasting 15.5 s, 10 TRs), although the letters and numbers within the cued region changed on every trial. Thus, each participant saw 20 unique cues (combinations of cue location and width) per run. Each run began and ended with 15.5 s of the dynamic noise annulus.</p><p>During each trial, the cue and white noise annulus were presented alone for 1.35 s. The numbers and letters were then displayed for 0.5 s. Thereafter, the cue and white noise remained visible while the participant had 1.25 s to indicate whether there had been more digits or letters within the cued region, resulting in a total trial duration of 3.1 s (2 TRs). No accuracy feedback was provided during the main experiment. However, all participants completed three training runs with trial-by-trial feedback prior to the scan session. During training runs, the response window was shortened to 1 s and the remaining 0.25 s presented feedback in the form of a change in color of the fixation point (blue for correct responses and orange for incorrect responses).</p></sec><sec id="s4-3-2"><title>Physical contrast manipulation</title><p>A subset of participants (<italic>n</italic>=5) also participated in an experiment that enhanced the physical contrast intensity of the dynamic visual noise in segments of the annulus. This additional experiment was carried out during the same scan session and allowed for benchmarking the detectability of stimulus-evoked modulation in visual cortex using our analyses. The stimuli and trial structure were similar to the attentional manipulation. The task differed in the following ways: (1) the contrast of the white noise annulus was increased to 100% for segments of the annulus corresponding to 1, 3, 5, 7, or 9 bins (18°, 54°, 90°, 126°, or 162° polar angle), with a Gaussian rolloff (<italic>σ</italic>=15°) that spanned 25% of the furthest included bins and 25% of the adjacent excluded bins; (2) the enhanced segments were always centered on the cardinal directions (0°, 90°, 180°, and 270° polar angle); (3) the contrast increase remained constant for 15.5 s (10 TRs); (4) participants performed a color change detection task at fixation. Each unique combination of four locations and five widths of the contrast enhancement was shown once per run, with the order randomized. To estimate a baseline response, each run started and ended with 15.5 s without contrast modulation. Participants completed two runs total, each lasting 341 s (220 TRs).</p><p>Throughout the <italic>physical contrast</italic> runs, participants were instructed to fixate on a central point (radius 0.08° visual angle) and to press a button when the fixation point switched color (alternating white and red). The fixation point remained a color for at least 1 s and then had a 10% probability of switching every 100 ms. No cues were presented for the regions of increased contrast. Additionally, no letters or numbers were superimposed on the white noise annulus.</p></sec><sec id="s4-3-3"><title>pRF mapping</title><p>pRF estimates were obtained for each participant in a separate scan session. We used the experimental procedure as described in the Human Connectome Project 7T Retinotopy dataset (<xref ref-type="bibr" rid="bib7">Benson et al., 2018</xref>). Stimuli were composed of a pink noise background with colorful objects and faces at various spatial scales, displayed on a mean luminance gray background. Stimuli were updated at a rate of 15 Hz while participants performed a color change detection task at fixation. Participants viewed two types of mapping stimuli: (1) contracting/expanding rings and rotating wedges; (2) moving bar stimuli (<xref ref-type="bibr" rid="bib16">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib30">Kay et al., 2013</xref>). A total of four to six scans (300 TRs) were collected for each participant (two to three scans per stimulus type). In this session, the field of view was restricted to the occipital cortex to maximize signal-to-noise ratio (SNR), thereby limiting the brain regions for which we had pRF estimates to V1, V2, and V3.</p></sec></sec><sec id="s4-4"><title>MRI data acquisition</title><p>All MRI data were acquired at Boston University’s Cognitive Neuroimaging Center (Boston, MA, USA) on a research-dedicated Siemens Prisma 3T scanner using a 64-channel head coil. A scanning session lasted 2 hr.</p><p>All functional neuroimaging data were acquired using a simultaneous multislice gradient-echo echo-planar acquisition protocol (<xref ref-type="bibr" rid="bib37">Moeller et al., 2010</xref>; <xref ref-type="bibr" rid="bib55">Setsompop et al., 2012</xref>): 2 mm isotropic voxels; FoV = 212 × 212 mm<sup>2</sup>; 72 axial slices; TR = 1.55 s; TE = 35.60 ms; flip angle = 72°; multiband acceleration factor 4. We computed distortion field maps by using a spin echo echoplanar protocol with opposite <italic>y</italic>-axis phase encoding directions (2 mm isotropic voxels; FOV = 212 × 212 mm<sup>2</sup>; TR = 8850 ms; TE = 70.80 ms; flip angle = 90°). During a separate scan session, we acquired a whole-brain anatomical scan using a T1-weighted multi-echo MPRAGE 3d sequence (1 mm isotropic; FoV = 256 × 256 mm<sup>2</sup>; 176 sagittal slices; TR = 2530 ms; TE = 1.69 ms; flip angle = 7°), and the pRF scans (occipital coverage only; right-left phase encoding; 2 mm isotropic voxels; FoV = 136 × 136 mm<sup>2</sup>; 36 slices; TR = 1 s; TE = 35.4 ms; flip angle = 64°; multiband acceleration factor 3).</p></sec><sec id="s4-5"><title>MRI data analysis</title><sec id="s4-5-1"><title>Structural data preprocessing</title><p>Whole-brain T1-weighted anatomical data were analyzed using the standard ‘recon-all’ pipeline provided by FreeSurfer software (FreeSurfer version 5.3, <xref ref-type="bibr" rid="bib20">Fischl, 2012</xref>), generating cortical surface models, whole-brain segmentation, and cortical parcellations.</p></sec><sec id="s4-5-2"><title>Functional data preprocessing</title><p>All analyses were performed in the native space for each participant. First, EPI distortion correction was applied to all fMRI BOLD time-series data using a reverse phase-encode method (<xref ref-type="bibr" rid="bib1">Andersson et al., 2003</xref>) implemented in FSL (<xref ref-type="bibr" rid="bib58">Smith et al., 2004</xref>). All functional data were then preprocessed using FS-FAST (<xref ref-type="bibr" rid="bib20">Fischl, 2012</xref>), including standard motion-correction procedures, Siemens slice timing correction, and boundary-based registration between anatomical and functional volumetric spaces (<xref ref-type="bibr" rid="bib22">Greve and Fischl, 2009</xref>). To facilitate voxel-wise analysis, no volumetric smoothing was performed, and across-run within-modality robust rigid registration was applied (<xref ref-type="bibr" rid="bib52">Reuter et al., 2010</xref>), with the middle time point of the first run serving as the target volume, and the middle time point of each subsequent run used as a movable volume for alignment. Lastly, data were detrended (0.005 Hz high-pass filter) and converted to percent signal change for each voxel independently using custom code written in MATLAB (version 2020b).</p></sec><sec id="s4-5-3"><title>pRF mapping and voxel selection</title><p>The time series were analyzed using the analyzePRF toolbox in MATLAB, implementing a compressive spatial summation pRF model (<xref ref-type="bibr" rid="bib30">Kay et al., 2013</xref>). The results of the pRF analysis were used to manually draw boundaries between early visual regions (V1, V2, and V3), which served as our regions of interest (ROIs).</p><p>Within each ROI, pRF modeling results were used to constrain voxel selection used in the main experiment. We excluded voxels with a preferred eccentricity outside the bounds of the pRF stimulus (&lt;0.7° and &gt;9.1°), with a pRF size smaller than 0.01°, or with poor spatial selectivity as indicated by the pRF model fit (<italic>R</italic><sup>2</sup>&lt;10%). Following our 2D visualizations (see below), we further constrained voxel selection by only including voxels whose pRF overlapped with the white noise annulus. We included all voxels with an estimated eccentricity within the annulus bounds, as well as voxels with an estimated pRF size that would overlap the annulus.</p></sec><sec id="s4-5-4"><title>2D visualizations of attentional modulation</title><p>To visualize the topography of attentional modulation under different cue widths, we projected the average BOLD responses for a given block (10 TRs with a consistent cue location and width, shifted by 3 TRs [4.65 s] to compensate for the hemodynamic delay) into the visual field using each voxel’s pRF location. This method is similar to that described in <xref ref-type="bibr" rid="bib18">Favila et al., 2022</xref>. First, we computed the Cartesian (<italic>x</italic>,<italic>y</italic>) coordinates from the pRF eccentricity and polar angle estimates for each voxel. Then, within a given ROI, we interpolated the BOLD responses over (<italic>x</italic>,<italic>y</italic>) space to produce a full-field representation. Each representation was then z-scored to allow for comparison across blocks, cue conditions, and participants. Finally, the representation was rotated so that the center of the cue was aligned to the right horizontal meridian (see <xref ref-type="fig" rid="fig2">Figure 2a</xref>).</p></sec><sec id="s4-5-5"><title>1D spatial profile of attentional modulation</title><p>We also examined the spatial profile of attentional modulation as a function of polar angle. Voxels with pRFs overlapping the white noise annulus were grouped into 60 bins according to their pRF polar angle estimate (6° polar angle bin width). We computed a median BOLD response within each bin. This facilitated the recentering of each profile to align all cue centers for subsequent combining across trials. To improve the SNR, the resulting profile was smoothed with a moving average filter (width 18° polar angle; see <xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p></sec><sec id="s4-5-6"><title>Model fitting</title><p>We quantified the spatial profile of attentional modulation with a generalized Gaussian model (<xref ref-type="bibr" rid="bib39">Nadarajah, 2005</xref>). The generalized Gaussian function (<italic>G</italic>) combines Gaussian and Laplace distributions:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle G=exp\left \{- \left |\frac{x- \mu }{\sigma }\right |^{\beta }\right \}$$\end{document}</tex-math></alternatives></disp-formula></p><p>The function has free parameters for location <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$(\mu)$\end{document}</tex-math></alternatives></inline-formula>, scale <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$(\sigma)$\end{document}</tex-math></alternatives></inline-formula>, and shape <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$(\beta)$\end{document}</tex-math></alternatives></inline-formula>. The shape parameter enables the tails of the distribution to become heavier than Gaussian (when <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mi>β</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$\beta \lt 2$\end{document}</tex-math></alternatives></inline-formula>), or lighter than Gaussian (when <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$\beta \gt2$\end{document}</tex-math></alternatives></inline-formula>); as <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$\beta \rightarrow \infty $\end{document}</tex-math></alternatives></inline-formula>, the model approaches a uniform distribution.</p><p>Next, <inline-formula><alternatives><mml:math id="inf10"><mml:mi>G</mml:mi></mml:math><tex-math id="inft10">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> was normalized to range between 0 and 1, and vertically scaled and shifted by two additional free parameters for gain <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$(a)$\end{document}</tex-math></alternatives></inline-formula> and baseline offset <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$(b)$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>⋅</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle \hat{y} =a\cdot G+ b$$\end{document}</tex-math></alternatives></disp-formula></p><p>We fit the five free parameters <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$(\mu ,\sigma ,\beta ,a,b)$\end{document}</tex-math></alternatives></inline-formula> using the MATLAB optimization tool <italic>fmincon,</italic> minimizing the squared error between the model prediction and the 1D profile described above. To avoid local minima, we first ran a grid search to find the initialization values with the lowest SSE (six possible values for <italic>μ</italic>, equally spaced between 0° and 360°, crossed with six possible values for <inline-formula><alternatives><mml:math id="inf14"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft14">\begin{document}$\sigma $\end{document}</tex-math></alternatives></inline-formula>, equally spaced between 9° and 162° polar angle; <inline-formula><alternatives><mml:math id="inf15"><mml:mi>β</mml:mi></mml:math><tex-math id="inft15">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula> = 4; <inline-formula><alternatives><mml:math id="inf16"><mml:mi>a</mml:mi></mml:math><tex-math id="inft16">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> = 1; <inline-formula><alternatives><mml:math id="inf17"><mml:mi>b</mml:mi></mml:math><tex-math id="inft17">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> = 0). We imposed the following parameter bounds on the search: <inline-formula><alternatives><mml:math id="inf18"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft18">\begin{document}$\sigma $\end{document}</tex-math></alternatives></inline-formula>: [6°, 180° polar angle], <inline-formula><alternatives><mml:math id="inf19"><mml:mi>β</mml:mi><mml:mo>:</mml:mo></mml:math><tex-math id="inft19">\begin{document}$\beta \colon $\end{document}</tex-math></alternatives></inline-formula> [1.8, 50], and <inline-formula><alternatives><mml:math id="inf20"><mml:mi>a</mml:mi><mml:mo>:</mml:mo></mml:math><tex-math id="inft20">\begin{document}$a\colon $\end{document}</tex-math></alternatives></inline-formula> [0, 20]. <italic>μ</italic> was unbounded, but was wrapped to remain within [0°, 360°].</p><p>From the model fits, we computed the following summary metrics: (1) angular error, defined as the polar-angle distance between the true and estimated location; (2) the FWHM of the best-fitting generalized Gaussian function, which served as our measure of the width of attentional modulation. The FWHM was controlled mainly by the scale parameter (<inline-formula><alternatives><mml:math id="inf21"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft21">\begin{document}$\sigma $\end{document}</tex-math></alternatives></inline-formula>) but also to a lesser degree by the shape parameter (<inline-formula><alternatives><mml:math id="inf22"><mml:mi>β</mml:mi></mml:math><tex-math id="inft22">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula>; see <xref ref-type="fig" rid="fig3">Figure 3a</xref>); (3) the gain modulation of the spatial profile <inline-formula><alternatives><mml:math id="inf23"><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft23">\begin{document}$\left (a\right)$\end{document}</tex-math></alternatives></inline-formula>; (4) the baseline offset (<inline-formula><alternatives><mml:math id="inf24"><mml:mi>b</mml:mi></mml:math><tex-math id="inft24">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>); (5) the model’s goodness of fit quantified as the percentage of explained variance (<italic>R</italic><sup>2</sup>) in the spatial response profile:<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle R^{2}=1- \frac{(y-\hat{y} )^{2}}{(y-\bar{y} )^{2}} $$\end{document}</tex-math></alternatives></disp-formula></p></sec></sec><sec id="s4-6"><title>Statistical testing</title><p>To assess how the attentional cue width manipulation influenced the 1D spatial profile of BOLD modulation, we tested whether the computed summary metrics (absolute angular error, FWHM, gain, and baseline) varied as a function of cue width. Specifically, we performed a linear regression for each metric within each subject and tested whether the slopes differed from zero at the group level using a t-test. This was done independently for each ROI. No multiple comparison correction was applied, as the different tests for each region are treated as separate questions. However, using a threshold of 0.017 for p-values would correct for comparisons across the three brain regions. When testing whether the number of TRs impacted our metrics, the linear regression used both cue width and number of TRs as explanatory variables.</p></sec><sec id="s4-7"><title>Eye-position monitoring</title><p>Gaze data were collected for all participants using an MR-compatible SR Research EyeLink 1000+ eye tracker sampling at 1 kHz. Data from blink periods were excluded from analysis. Participants maintained fixation throughout the task, with average gaze eccentricity below 0.5° for all participants. Gaze eccentricity did not significantly vary by cued width (pairwise comparison of width conditions using a paired t-test, all p≥0.205 with Bonferroni correction for multiple comparisons) nor location (pairwise comparison, all p≥0.522 with Bonferroni correction for multiple comparisons). Additionally, we examined the number of fixations to the white noise annulus itself. No participant had more than 16 fixations (out of 800–1200 trials) to the annulus during the task, further suggesting that participants successfully maintained fixation.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Writing – original draft, Investigation, Funding acquisition, Formal analysis</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Methodology, Writing – original draft, Investigation, Data curation, Formal analysis</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All procedures were approved by the Boston University Institutional Review Board, and informed consent was obtained from all participants.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-104222-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All preprocessed fMRI data (extracted time series from each voxel), behavioral data, and eye tracking data reported in this study, as well as all code necessary to reproduce the analysis and recreate the figures are publicly available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/VEK9M">Open Science Framework</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/ilonabloem/BloemBakstMcGuireLing_eLife_2025">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib9">Bloem, 2025</xref>) respectively.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Bloem</surname><given-names>I</given-names></name><name><surname>Bakst</surname><given-names>L</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Ling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Data and code for: &quot;Dynamic estimation of the attentional field from visual cortical activity&quot;</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/VEK9M</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by National Science Foundation grants SMA-1809071, and BCS-1755757, National Institutes of Health grants F32-EY029134, R01-EY028163, and R01-MH126971, Office of Naval Research grant N00014-17-1-2304, and the Center for Systems Neuroscience Postdoctoral Fellowship at Boston University. The equipment used was funded by NSF Major Instrumentation Grant 1625552. The content of this paper does not necessarily represent the official views of the funding agencies. We thank members of the Ling and McGuire labs for providing helpful feedback and comments.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Skare</surname><given-names>S</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1038/nrn3443</pub-id><pub-id pub-id-type="pmid">23422910</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakst</surname><given-names>L</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Eye movements reflect adaptive predictions and predictive precision</article-title><source>Journal of Experimental Psychology. General</source><volume>150</volume><fpage>915</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1037/xge0000977</pub-id><pub-id pub-id-type="pmid">33048566</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakst</surname><given-names>L</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Experience-driven recalibration of learning from surprising events</article-title><source>Cognition</source><volume>232</volume><elocation-id>105343</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105343</pub-id><pub-id pub-id-type="pmid">36481590</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartsch</surname><given-names>MV</given-names></name><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Strumpf</surname><given-names>H</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A cortical zoom-in operation underlies covert shifts of visual spatial attention</article-title><source>Science Advances</source><volume>9</volume><elocation-id>eade7996</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.ade7996</pub-id><pub-id pub-id-type="pmid">36888705</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The human connectome project 7 Tesla retinotopy dataset: description and population receptive field analysis</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1167/18.13.23</pub-id><pub-id pub-id-type="pmid">30593068</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloem</surname><given-names>IM</given-names></name><name><surname>Ling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Normalization governs attentional modulation within human visual cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5660</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13597-1</pub-id><pub-id pub-id-type="pmid">31827078</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bloem</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>BloemBakstMcGuireLing_eLife_2025</data-title><version designator="swh:1:rev:a15e4e39f056652d4e1bba975e8f50b54dee674b">swh:1:rev:a15e4e39f056652d4e1bba975e8f50b54dee674b</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:59df4308df50c1ce860c4d2c239033a5120e4e5f;origin=https://github.com/ilonabloem/BloemBakstMcGuireLing_eLife_2025;visit=swh:1:snp:7d65d32b3f8c9b7b0fedae14eb9fba53163912f8;anchor=swh:1:rev:a15e4e39f056652d4e1bba975e8f50b54dee674b">https://archive.softwareheritage.org/swh:1:dir:59df4308df50c1ce860c4d2c239033a5120e4e5f;origin=https://github.com/ilonabloem/BloemBakstMcGuireLing_eLife_2025;visit=swh:1:snp:7d65d32b3f8c9b7b0fedae14eb9fba53163912f8;anchor=swh:1:rev:a15e4e39f056652d4e1bba975e8f50b54dee674b</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brefczynski</surname><given-names>JA</given-names></name><name><surname>DeYoe</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A physiological correlate of the “spotlight” of visual attention</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>370</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1038/7280</pub-id><pub-id pub-id-type="pmid">10204545</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual attention: the past 25 years</article-title><source>Vision Research</source><volume>51</volume><fpage>1484</fpage><lpage>1525</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.04.012</pub-id><pub-id pub-id-type="pmid">21549742</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castiello</surname><given-names>U</given-names></name><name><surname>Umiltà</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Size of the attentional focus and efficiency of processing</article-title><source>Acta Psychologica</source><volume>73</volume><fpage>195</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(90)90022-8</pub-id><pub-id pub-id-type="pmid">2353586</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>d’Acremont</surname><given-names>M</given-names></name><name><surname>Bossaerts</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural mechanisms behind identification of leptokurtic noise and adaptive behavioral response</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>1818</fpage><lpage>1830</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw013</pub-id><pub-id pub-id-type="pmid">26850528</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname><given-names>R</given-names></name><name><surname>DeYoe</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>I know where you are secretly attending! The topography of human visual attention revealed with fMRI</article-title><source>Vision Research</source><volume>49</volume><fpage>1037</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2009.01.014</pub-id><pub-id pub-id-type="pmid">19533912</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id><pub-id pub-id-type="pmid">16778890</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eriksen</surname><given-names>CW</given-names></name><name><surname>St. James</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Visual attention within and around the field of focal attention: a zoom lens model</article-title><source>Perception &amp; Psychophysics</source><volume>40</volume><fpage>225</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.3758/BF03211502</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Perception and memory have distinct spatial tuning properties in human visual cortex</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>5864</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-33161-8</pub-id><pub-id pub-id-type="pmid">36257949</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldmann-Wüstefeld</surname><given-names>T</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Alpha-band activity tracks the zoom lens of attention</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>272</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01484</pub-id><pub-id pub-id-type="pmid">31633465</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobell</surname><given-names>JL</given-names></name><name><surname>Tseng</surname><given-names>C</given-names></name><name><surname>Sperling</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The spatial distribution of visual attention</article-title><source>Vision Research</source><volume>44</volume><fpage>1273</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.01.012</pub-id><pub-id pub-id-type="pmid">15066391</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>K</given-names></name><name><surname>Montaser-Kouhsari</surname><given-names>L</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>When size matters: attention affects performance by contrast or response gain</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1554</fpage><lpage>1559</lpage><pub-id pub-id-type="doi">10.1038/nn.2669</pub-id><pub-id pub-id-type="pmid">21057509</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>J-M</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Boelmans</surname><given-names>K</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Rieger</surname><given-names>J</given-names></name><name><surname>Heinze</surname><given-names>H-J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The neural site of attention matches the spatial scale of perception</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>3532</fpage><lpage>3540</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4510-05.2006</pub-id><pub-id pub-id-type="pmid">16571761</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>D</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Xue</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Hu</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The time course of attention modulation elicited by spatial uncertainty</article-title><source>Vision Research</source><volume>138</volume><fpage>50</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2017.06.008</pub-id><pub-id pub-id-type="pmid">28733049</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Intriligator</surname><given-names>J</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The spatial resolution of visual attention</article-title><source>Cognitive Psychology</source><volume>43</volume><fpage>171</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1006/cogp.2001.0755</pub-id><pub-id pub-id-type="pmid">11689021</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itthipuripat</surname><given-names>S</given-names></name><name><surname>Garcia</surname><given-names>JO</given-names></name><name><surname>Rungratsameetaweemana</surname><given-names>N</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Changing the spatial scope of attention alters patterns of neural gain in human cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>112</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3943-13.2014</pub-id><pub-id pub-id-type="pmid">24381272</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jigo</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An image-computable model of how endogenous and exogenous attention differentially alter visual perception</article-title><source>PNAS</source><volume>118</volume><elocation-id>p. e2106436118. 10.1073/pnas.2106436118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2106436118</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Mechanisms of directed attention in the human extrastriate cortex as revealed by functional MRI</article-title><source>Science</source><volume>282</volume><fpage>108</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1126/science.282.5386.108</pub-id><pub-id pub-id-type="pmid">9756472</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Compressive spatial summation in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>481</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1152/jn.00105.2013</pub-id><pub-id pub-id-type="pmid">23615546</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinıklıoğlu</surname><given-names>M</given-names></name><name><surname>Boyaci</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Increasing the spatial extent of attention strengthens surround suppression</article-title><source>Vision Research</source><volume>199</volume><elocation-id>108074</elocation-id><pub-id pub-id-type="doi">10.1016/j.visres.2022.108074</pub-id><pub-id pub-id-type="pmid">35717748</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>BP</given-names></name><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attraction of position preference by spatial attention throughout human visual cortex</article-title><source>Neuron</source><volume>84</volume><fpage>227</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.047</pub-id><pub-id pub-id-type="pmid">25242220</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal mechanisms of visual attention</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>373</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035431</pub-id><pub-id pub-id-type="pmid">28532368</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00431.1999</pub-id><pub-id pub-id-type="pmid">9870971</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functionally dissociable influences on learning rate in a dynamic environment</article-title><source>Neuron</source><volume>84</volume><fpage>870</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.013</pub-id><pub-id pub-id-type="pmid">25459409</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMains</surname><given-names>SA</given-names></name><name><surname>Somers</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multiple spotlights of attentional selection in human visual cortex</article-title><source>Neuron</source><volume>42</volume><fpage>677</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(04)00263-6</pub-id><pub-id pub-id-type="pmid">15157427</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Strupp</surname><given-names>J</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiband multislice GE-EPI at 7 tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fMRI</article-title><source>Magnetic Resonance in Medicine</source><volume>63</volume><fpage>1144</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id><pub-id pub-id-type="pmid">20432285</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>NG</given-names></name><name><surname>Bartelt</surname><given-names>OA</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Villringer</surname><given-names>A</given-names></name><name><surname>Brandt</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A physiological correlate of the “Zoom Lens” of visual attention</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>3561</fpage><lpage>3565</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-09-03561.2003</pub-id><pub-id pub-id-type="pmid">12736325</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadarajah</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A generalized normal distribution</article-title><source>Journal of Applied Statistics</source><volume>32</volume><fpage>685</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1080/02664760500079464</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Rumsey</surname><given-names>KM</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Parikh</surname><given-names>K</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id><pub-id pub-id-type="pmid">22660479</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Bruckner</surname><given-names>R</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical context dictates the relationship between feedback-related EEG signals and learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e46975</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46975</pub-id><pub-id pub-id-type="pmid">31433294</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Schüffelgen</surname><given-names>U</given-names></name><name><surname>Cuell</surname><given-names>SF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title><source>PNAS</source><volume>110</volume><fpage>E3660</fpage><lpage>E3669</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>J</given-names></name><name><surname>Moore</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Using a filtering task to measure the spatial extent of selective attention</article-title><source>Vision Research</source><volume>49</volume><fpage>1045</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.02.022</pub-id><pub-id pub-id-type="pmid">18405935</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payzan-LeNestour</surname><given-names>E</given-names></name><name><surname>Bossaerts</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Risk, unexpected uncertainty, and estimation uncertainty: Bayesian learning in unstable settings</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1001048</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001048</pub-id><pub-id pub-id-type="pmid">21283774</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payzan-LeNestour</surname><given-names>E</given-names></name><name><surname>Dunne</surname><given-names>S</given-names></name><name><surname>Bossaerts</surname><given-names>P</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The neural representation of unexpected uncertainty during value-based decision making</article-title><source>Neuron</source><volume>79</volume><fpage>191</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.037</pub-id><pub-id pub-id-type="pmid">23849203</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>PsychoPy - Psychophysics software in Python</article-title><source>Journal of Neuroscience Methods</source><volume>162</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Orienting of attention</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>32</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/00335558008248231</pub-id><pub-id pub-id-type="pmid">7367577</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preuschoff</surname><given-names>K</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Einhäuser</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupil dilation signals surprise: evidence for noradrenaline’s role in decision making</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><elocation-id>115</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2011.00115</pub-id><pub-id pub-id-type="pmid">21994487</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puckett</surname><given-names>AM</given-names></name><name><surname>DeYoe</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The attentional field revealed by single-voxel modeling of fMRI time courses</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>5030</fpage><lpage>5042</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3754-14.2015</pub-id><pub-id pub-id-type="pmid">25810532</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ress</surname><given-names>D</given-names></name><name><surname>Backus</surname><given-names>BT</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Activity in primary visual cortex predicts performance in a visual detection task</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>940</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1038/78856</pub-id><pub-id pub-id-type="pmid">10966626</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuter</surname><given-names>M</given-names></name><name><surname>Rosas</surname><given-names>HD</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Highly accurate inverse consistent registration: a robust approach</article-title><source>NeuroImage</source><volume>53</volume><fpage>1181</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.020</pub-id><pub-id pub-id-type="pmid">20637289</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samaha</surname><given-names>J</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding and reconstructing the focus of spatial attention from the topography of alpha-band oscillations</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>1090</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00955</pub-id><pub-id pub-id-type="pmid">27003790</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Setsompop</surname><given-names>K</given-names></name><name><surname>Gagoski</surname><given-names>BA</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Witzel</surname><given-names>T</given-names></name><name><surname>Wedeen</surname><given-names>VJ</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Blipped-controlled aliasing in parallel imaging for simultaneous multislice echo planar imaging with reduced g-factor penalty</article-title><source>Magnetic Resonance in Medicine</source><volume>67</volume><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1002/mrm.23097</pub-id><pub-id pub-id-type="pmid">21858868</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaw</surname><given-names>ML</given-names></name><name><surname>Shaw</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Optimal allocation of cognitive resources to spatial locations</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>3</volume><fpage>201</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.3.2.201</pub-id><pub-id pub-id-type="pmid">864393</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shioiri</surname><given-names>S</given-names></name><name><surname>Honjyo</surname><given-names>H</given-names></name><name><surname>Kashiwase</surname><given-names>Y</given-names></name><name><surname>Matsumiya</surname><given-names>K</given-names></name><name><surname>Kuriki</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual attention spreads broadly but selects information locally</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>35513</elocation-id><pub-id pub-id-type="doi">10.1038/srep35513</pub-id><pub-id pub-id-type="pmid">27759056</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Bannister</surname><given-names>PR</given-names></name><name><surname>De Luca</surname><given-names>M</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name><name><surname>Flitney</surname><given-names>DE</given-names></name><name><surname>Niazy</surname><given-names>RK</given-names></name><name><surname>Saunders</surname><given-names>J</given-names></name><name><surname>Vickers</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>De Stefano</surname><given-names>N</given-names></name><name><surname>Brady</surname><given-names>JM</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1879</fpage><lpage>1887</lpage><pub-id pub-id-type="doi">10.1038/nn.3574</pub-id><pub-id pub-id-type="pmid">24212672</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>JET</given-names></name><name><surname>Chan</surname><given-names>D</given-names></name><name><surname>Bennett</surname><given-names>PJ</given-names></name><name><surname>Pratt</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional cartography: mapping the distribution of attention across time and space</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>77</volume><fpage>2240</fpage><lpage>2246</lpage><pub-id pub-id-type="doi">10.3758/s13414-015-0943-0</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkacz-Domb</surname><given-names>S</given-names></name><name><surname>Yeshurun</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The size of the attentional window when measured by the pupillary response to light</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>11878</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-30343-7</pub-id><pub-id pub-id-type="pmid">30089801</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tünçok</surname><given-names>E</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Spatial attention alters visual cortical representation during target anticipation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.03.02.583127</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Beilen</surname><given-names>M</given-names></name><name><surname>Renken</surname><given-names>R</given-names></name><name><surname>Groenewold</surname><given-names>ES</given-names></name><name><surname>Cornelissen</surname><given-names>FW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attentional window set by expected relevance of environmental signals</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e21262</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0021262</pub-id><pub-id pub-id-type="pmid">21698172</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vo</surname><given-names>VA</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial tuning shifts increase the discriminability and fidelity of population codes in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3386</fpage><lpage>3401</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-16.2017</pub-id><pub-id pub-id-type="pmid">28242794</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The spatial distribution of attention</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>76</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2018.12.008</pub-id><pub-id pub-id-type="pmid">30660870</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104222.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meng</surname><given-names>Ming</given-names></name><role specific-use="editor">Reviewing Editor</role></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study addresses a gap in our understanding of how the size of the attentional field is represented within the visual cortex. The evidence supporting the role of visual cortical activity is <bold>convincing</bold>, based on a novel modeling analysis of fMRI data. The results will be of interest to psychologists and cognitive neuroscientists.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104222.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors conducted an fMRI study to investigate the neural effects of sustaining attention to areas of different sizes. Participants were instructed to attend to alphanumeric characters arranged in a circular array. The size of attention field was manipulated in four levels, ranging from small (18 deg) to large (162 deg). They used a model-based method to visualize attentional modulation in early visual cortex V1 to V3, and found spatially congruent modulations of the BOLD response, i.e., as the attended area increased in size, the neural modulation also increased in size in the visual cortex. They suggest that this result is a neural manifestation of the zoom-lens model of attention and that the model-based method can effectively reconstruct the neural modulation in the cortical space.</p><p>The study is well-designed with sophisticated and comprehensive data analysis. The results are robust and show strong support for a well-known model of spatial attention, the zoom-lens model. Overall, I find the results interesting and useful for the field of visual attention research.</p><p>Comments on revisions:</p><p>The authors have addressed my previous comments satisfactorily. I would encourage the authors to make data and code publicly available, which appears to be the custom in this era.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104222.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The study in question utilizes functional magnetic resonance imaging (fMRI) to dynamically estimate the locus and extent of covert spatial attention from visuocortical activity. The authors aim to address an important gap in our understanding of how the size of the attentional field is represented within the visual cortex. They present a novel paradigm that allows for the estimation of the spatial tuning of the attentional field and demonstrate the ability to reliably recover both the location and width of the attentional field based on BOLD responses.</p><p>Strengths:</p><p>(1) Innovative Paradigm: The development of a new approach to estimate the spatial tuning of the attentional field is a significant strength of this study. It provides a fresh perspective on how spatial attention modulates visual perception.</p><p>(2) Refined fMRI Analysis: The use of fMRI to track the spatial tuning of the attentional field across different visual regions is methodologically rigorous and provides valuable insights into the neural mechanisms underlying attentional modulation.</p><p>(3) Clear Presentation: The manuscript is well-organized, and the results are presented clearly, which aids in the reader's comprehension of the complex data and analyses involved.</p><p>Weaknesses:</p><p>(1) Lack of Neutral Cue Condition: The study does not include a neutral cue condition where the cue width spans 360{degree sign}, which could serve as a valuable baseline for assessing the BOLD response enhancements and diminishments in both attended and non-attended areas.</p><p>(2) Clarity on Task Difficulty Ratios: The explicit reasoning for the chosen letter-to-number ratios for various cue widths is not detailed. Ensuring clarity on these ratios is crucial, as it affects the task difficulty and the comparability of behavioral performance across different cue widths. It is essential that observed differences in behavior and BOLD signals are attributable solely to changes in cue width and not confounded by variations in task difficulty.</p><p>Comments on revisions:</p><p>(1) Please standardize the naming of error metrics across Figures 4-6 to improve clarity (e.g., &quot;angular error&quot; (Figure 4), &quot;|angular error|&quot; (Figure 5), and &quot;absolute error&quot; (Figure 6) appear to refer to the same measure). This inconsistency is also present in the main text.</p><p>(2) Consider briefly mentioning the baseline offset in Lines 179-186. It is included in Figures 4-7 and serves as a reference for interpreting attentional modulation alongside gain. Introducing it with other model parameters would improve clarity.</p><p>(3) It may be valuable to examine BOLD responses in unattended visual regions. As shown in Figure 2a, suppression patterns (e.g., the most negative responses) appear to vary in extent and distribution with attentional cue width. Analyzing these unattended regions may offer a more complete view of how attention shapes the spatial profile of cortical activity.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104222.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bloem</surname><given-names>Ilona M</given-names></name><role specific-use="author">Author</role></contrib><contrib contrib-type="author"><name><surname>Bakst</surname><given-names>Leah</given-names></name><role specific-use="author">Author</role></contrib><contrib contrib-type="author"><name><surname>McGuire</surname><given-names>Joseph T</given-names></name><role specific-use="author">Author</role></contrib><contrib contrib-type="author"><name><surname>Ling</surname><given-names>Sam</given-names></name><role specific-use="author">Author</role></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><p>We thank the three reviewers for their insightful feedback. We look forward to addressing the raised concerns in a revised version of the manuscript. There were a few common themes among the reviews that we will briefly touch upon now, and we will provide more details in the revised manuscript.</p><p>First, the reviewers asked for the reasoning behind the task ratios we implemented for the different attentional width conditions. The different ratios were selected to be as similar as possible given the size and spacing of our stimuli (aside from the narrowest cue width of one bin, the ratios for the others were 0.66, .6 and .66). As Figure 1b shows, while the ratios were similar, task difficulty is not constant across cue widths: spreading attention makes the task more difficult generally. But, while the modeled width of the spatial distribution of attention changes monotonically with cue width, task difficulty does not. Furthermore, prior work has indicated that there is a relationship between task difficulty and the overall magnitude of the BOLD response, however we don’t suspect that this will influence the width of the modulation. How task difficulty influences the BOLD response is an important topic, and we hope that future work will investigate this relationship more directly.</p><p>Second, reviewers raised interest in the distribution of spatial attention in higher visual areas. In our study we focus only on early visual regions (V1-V3). This was primarily driven by pragmatic considerations, in that we only have retinotopic estimates for our participants in these early visual areas. Our modeling approach is dependent on having access to the population receptive field estimates for all voxels, and while the main experiment was scanned using whole brain coverage, retinotopy was measured in a separate session using a field of view only covering the occipital cortex.</p><p>Lastly, we appreciate the opportunity to clarify the purpose of the temporal interval analysis. The reviewer is correct in assuming we set out to test how much data is needed to recover the cortical modulation and how dynamic a signal the method can capture. This analysis does show that more data provides more reliable estimates, though the model was still able to recover the location and width of the attentional cue at shorter timescales of as few as two TRs. This has implications for future studies that may involve more dynamic tracking of the attentional field.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews</bold></p><p><bold>Reviewer #1 (Public review):</bold></p><p>The authors conducted an fMRI study to investigate the neural effects of sustaining attention to areas of different sizes. Participants were instructed to attend to alphanumeric characters arranged in a circular array. The size of attention field was manipulated in four levels, ranging from small (18 deg) to large (162 deg). They used a model-based method to visualize attentional modulation in early visual cortex V1 to V3, and found spatially congruent modulations of the BOLD response, i.e., as the attended area increased in size, the neural modulation also increased in size in the visual cortex. They suggest that this result is a neural manifestation of the zoomlens model of attention and that the model-based method can effectively reconstruct the neural modulation in the cortical space.</p><p>The study is well-designed with sophisticated and comprehensive data analysis. The results are robust and show strong support for a well-known model of spatial attention, the zoom-lens model. Overall, I find the results interesting and useful for the field of visual attention research. I have questions about some aspects of the results and analysis as well as the bigger picture.</p><p>(1) It appears that the modulation in V1 is weaker than V2 and V3 (Fig 2). In particular, the width modulation in V1 is not statistically significant (Fig 5). This result seems a bit unexpected. Given the known RF properties of neurons in these areas, in particular, smaller RF in V1, one might expect more spatially sensitive modulation in V1 than V2/V3. Some explanations and discussions would be helpful. Relatedly, one would also naturally wonder if this method can be applied to other extrastriate visual areas such as V4 and what the results look like.</p></disp-quote><p>We agree with the reviewer. It’s very interesting how the spatial resolution within different visual regions contributes to the overall modulation of the attentional field, and how this in turn would influence perception. Our data showed that fits in V1 appeared to be less precise than in V2 and V3. This can be seen in the goodness of fit of the model as well as the gain and absolute angular error estimates. The goodness of fit and gain were lowest in V1 and the absolute angular error was largest in V1 (see Figure 5). We speculate that the finer spatial granularity of V1 RFs was countered by a lower amplitude and SNR of attention-related modulation in V1, resulting in overall lower sensitivity to variation in attentional field width. Prior findings concur that the magnitude of covert spatial attention increases when moving from striate to extrastriate cortex (Bressler &amp; Silver (2010); Buracas &amp; Boynton (2007)). Notably, in our perception condition, V1 showed more spatially sensitive modulation (see Figure 7), consistent with the known RF properties of V1 neurons.</p><p>Regarding the second point: unfortunately, our dataset did not allow us to explore higherorder cortical regions with the model-based approach. While the main experiment was scanned using a sequence with whole brain coverage, the pRF estimates came from a separate scanning session which only had limited occipital coverage. Our modeling approach is dependent on the polar angle estimates from this pRF session. We now explicitly state this limitation in the methods (lines 87-89):</p><p>“In this session, the field of view was restricted to the occipital cortex to maximize SNR, thereby limiting the brain regions for which we had pRF estimates to V1, V2, and V3.”</p><disp-quote content-type="editor-comment"><p>(2) I'm a bit confused about the angular error result. Fig 4 shows that the mean angular error is close to zero, but Fig 5 reports these values to be about 30-40 deg. Why the big discrepancy? Is it due to the latter reporting absolute errors? It seems reporting the overall bias is more useful than absolute value.</p></disp-quote><p>The reviewer’s inference here is exactly right: Figure 4 shows signed error, whereas Figure 5 shows absolute error. We show the signed error for the example participant because, (1) by presenting the full distribution of model estimates for one participant, readers have access to a more direct representation of the data, and (2) at the individual level it is possible to examine potential directional biases in the location estimates (which do not appear to be present). As we don’t suspect a consistent directional bias across the group, we believe the absolute error in location estimates is more informative in depicting the precision in location estimates using the model-based approach. In the revised manuscript, we modified Figure 5 to make the example participant’s data visually distinct for easy comparison. We have clarified this reasoning in the text (results lines 59-64):</p><p>“The angular error distribution across blocks, separated by width condition, is shown in Figure 4 for one example participant to display block-to-block variation. The model reliably captured the location of the attentional field with low angular error and with no systematic directional bias. This result was observed across participants. We next examined the absolute angular error to assess the overall accuracy of our estimates.”</p><disp-quote content-type="editor-comment"><p>(3) A significant effect is reported for amplitude in V3 (line 78), but the graph in Fig 5 shows hardly any difference. Please confirm the finding and also explain the directionality of the effect if there is indeed one.</p></disp-quote><p>We realize that the y-axis scale of Figure 5 was making it difficult to see that gain decreases with cue width in area V3. Instead of keeping the y-axis limits the same across visual regions, we now adapt the y-axis scale of each subplot to the range of data values:</p><p>We now also add the direction of the effect in the text (results lines 83-86):</p><p>“We observed no significant relationship between gain and cue width in V1 and V2 (V1 t(7)=.54, p=.605; V2 t(7)=-2.19, p=.065), though we did find a significant effect in V3 illustrating that gain decreases with cue width (t(7)=-3.12, p=.017).”</p><disp-quote content-type="editor-comment"><p>(4) The purpose of the temporal interval analysis is rather unclear. I assume it has to do with how much data is needed to recover the cortical modulation and hence how dynamic a signal the method can capture. While the results make sense (i.e., more data is better), there is no obvious conclusion and/or interpretation of its meaning.</p></disp-quote><p>We apologize for not making our reasoning clear. We now emphasize our reasoning in the revised manuscript (results lines 110-112). Our objective was to quantify how much data was needed to recover the dynamic signal. As expected, we found that including more data reduces noise (averaging helps), but importantly, we found that we still obtained meaningful model fits even with limited data. We believe this has important implications for future paradigms that explore more dynamic deployment of spatial attention, where one would not want to average over multiple repetitions of a condition.</p><p>The first paragraph of the Temporal Interval Analysis section in the results now reads:</p><p>“In the previous analyses, we leveraged the fact that the attentional cue remained constant for 5-trial blocks (spatial profiles were computed by averaging BOLD measurements across a block of 10 TRs). We next examined the degree to which we were able to recover the attentional field on a moment-by-moment (TR-by-TR) basis. To do this, we systematically adjusted the number of TRs that contributed to the averaged spatial response profile. To maintain a constant number of observations across the temporal interval conditions, we randomly sampled a subset of TRs from each block. This allowed us to determine the amount of data needed to recover the attentional field, with a goal of examining the usability of our modeling approach in future paradigms involving more dynamic deployment of spatial attention.”</p><disp-quote content-type="editor-comment"><p>(5) I think it would be useful for the authors to make a more explicit connection to previous studies in this literature. In particular, two studies seem particularly relevant. First, how do the present results relate to those in Muller et al (2003, reference 37), which also found a zoom-lens type of neural effects. Second, how does the present method compare with spatial encoding model in Sprague &amp; Serences (2013, reference 56), which also reconstructs the neural modulation of spatial attention. More discussions of these studies will help put the current study in the larger context.</p></disp-quote><p>We now make a more explicit connection to prior work in the discussion section (lines 34-54).</p><p>“We introduced a novel modeling approach that recovered the location and the size of the attentional field. Our data show that the estimated spatial spread of attentional modulation (as indicated by the recovered FWHM) consistently broadened with the cue width, replicating prior work (Müller et al., 2003; Herrmann et al., 2010). Our results go beyond prior work by linking the spatial profiles to pRF estimates, allowing us to quantify the spread of both attentional and perceptual modulation in degrees of polar angle. Interestingly, the FWHM estimates for the attentional and perceptual spatial profiles were highly similar. Additionally, for area V3 we replicate that the population response magnitude decreased with cue width (Müller et al., 2003; Feldmann-Wüstefeld and Awh, 2020). One innovation of our method is that it directly reconstructs attention-driven modulations of responses in visual cortex, setting it apart from other methods, such as inverted encoding models (e.g. Sprague &amp; Serences, 2013). Finally, we demonstrated that our method has potential to be used in more dynamic settings, in which changes in the attentional field need to be tracked on a shorter timescale.”</p><disp-quote content-type="editor-comment"><p>(6) Fig 4b, referenced on line 123, does not exist.</p></disp-quote><p>We have corrected the text to reference the appropriate figure (Figure 5, results line 136).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The study in question utilizes functional magnetic resonance imaging (fMRI) to dynamically estimate the locus and extent of covert spatial attention from visuocortical activity. The authors aim to address an important gap in our understanding of how the size of the attentional field is represented within the visual cortex. They present a novel paradigm that allows for the estimation of the spatial tuning of the attentional field and demonstrate the ability to reliably recover both the location and width of the attentional field based on BOLD responses.</p><p>Strengths:</p><p>(1) Innovative Paradigm: The development of a new approach to estimate the spatial tuning of the attentional field is a significant strength of this study. It provides a fresh perspective on how spatial attention modulates visual perception.</p><p>(2) Refined fMRI Analysis: The use of fMRI to track the spatial tuning of the attentional field across different visual regions is methodologically rigorous and provides valuable insights into the neural mechanisms underlying attentional modulation.</p><p>(3) Clear Presentation: The manuscript is well-organized, and the results are presented clearly, which aids in the reader's comprehension of the complex data and analyses involved.</p></disp-quote><p>We thank the reviewer for summarizing the strengths in our work.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) Lack of Neutral Cue Condition: The study does not include a neutral cue condition where the cue width spans 360°, which could serve as a valuable baseline for assessing the BOLD response enhancements and diminishments in both attended and non-attended areas.</p></disp-quote><p>We do not think that the lack of a neutral cue condition substantially limits our ability to address the core questions of interest in the present work. We set out to estimate the locus and the spread of covert spatial attention. By definition, a neutral cue does not have a focus of attention as the whole annulus becomes task relevant. We agree with the reviewer that how spatial attention influences the magnitude of the BOLD response is still not well defined; i.e., does attending a location multiplicatively enhance responses at an attended location or does it instead act to suppress responses outside the focus of attention? A neutral cue condition would be necessary to be able to explore these types of questions. However, our findings don’t rest on any assumptions about this. Instead, we quantify the attentional modulation with a model-based approach and show that we can reliably recover its locus, and reveal a broadening in the attentional modulation with wider cues.</p><p>We realize that throughout the original manuscript we often used the term ‘attentional enhancement,’ which might inadvertently specify an increase with respect to a neutral condition. To be more agnostic to the directionality of the effect, we have changed this to ‘attentional modulation’ and ‘attentional gain’ throughout the manuscript. Additionally, we have added results and visualizations for the baseline parameter to all results figures (Figures 4-7) to help readers further interpret our findings.</p><disp-quote content-type="editor-comment"><p>(2) Clarity on Task Difficulty Ratios: The explicit reasoning for the chosen letter-to-number ratios for various cue widths is not detailed. Ensuring clarity on these ratios is crucial, as it affects the task difficulty and the comparability of behavioral performance across different cue widths. It is essential that observed differences in behavior and BOLD signals are attributable solely to changes in cue width and not confounded by variations in task difficulty.</p></disp-quote><p>The ratios were selected to be as similar as possible given the size and spacing of our stimuli (aside from the narrowest cue width of one bin, the proportions for the others were 0.67, 0.60, and 0.67). We have updated the methods section to state this explicitly (methods lines 36-38):</p><p>“The ratios were selected to be as similar as possible given the size and spacing of our stimuli (aside from the one-bin cue, the proportions for the other cues were 0.67, 0.60, 0.67).”</p><p>As Figure 1b shows, task accuracy showed small and non-monotonic changes across the three larger cue widths, dissociable from the monotonic pattern seen for the modelestimated width of the attentional field. Furthermore, as prior work has indicated that there is a relationship between task difficulty and the overall magnitude of the BOLD response (e.g., Ress, Backus &amp; Heeger, 2000), we would primarily expect effects of task difficulty on the gain or baseline rather than the width. How exactly task difficulty influences the BOLD response and whether this would, in fact, interact with the width of the attentional field is an important topic, and we hope that future work will investigate this relationship more directly.</p><p>We have clarified these points within the text, and now explicitly motivate future work looking at these important interactions (discussion lines 57-67):</p><p>“The observed effects of attentional field width were unlikely to be directly attributable to variation in task difficulty. Participants' task in our study was to discriminate whether more numbers or more letters were presented within a cued region of an iso-eccentric annulus of white noise. For our different cue widths, the ratios of numbers and letters were selected to be as similar as possible given the size and spacing of our stimuli. Changes in accuracy across the three larger cue widths were small and non-monotonic, implying task difficulty was dissociable from width per se. This dissociation bolsters the interpretability of our model fits; nevertheless, future work should further investigate how task difficulty interacts with the spread of the attentional field and the amplitude of attention-related BOLD effects (cf. Ress, Backus &amp; Heeger, 2000).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>In this report, the authors tested how manipulating the contiguous set of stimuli on the screen that should be used to guide behavior - that is, the scope of visual spatial attention - impacts the magnitude and profile of well-established attentional enhancements in visual retinotopic cortex. During fMRI scanning, participants attended to a cued section of the screen for blocks of trials and performed a letter vs digit discrimination task at each attended location (and judged whether the majority of characters were letters/digits). Importantly, the visual stimulus was identical across attention conditions, so any observed response modulations are due to topdown task demands rather than visual input. The authors employ population receptive field (pRF) models, which are used to sort voxel activation with respect to the location and scope of spatial attention and fit a Gaussian-like function to the profile of attentional enhancement from each region and condition. The authors find that attending to a broader region of space expands the profile of attentional enhancement across the cortex (with a larger effect in higher visual areas), but does not strongly impact the magnitude of this enhancement, such that each attended stimulus is enhanced to a similar degree. Interestingly, these modulations, overall, mimic changes in response properties caused by changes to the stimulus itself (increase in contrast matching the attended location in the primary experiment). The finding that attentional enhancement primarily broadens, but does not substantially weaken in most regions, is an important addition to our understanding of the impact of distributed attention on neural responses, and will provide meaningful constraints to neural models of attentional enhancement.</p><p>Strengths:</p><p>(1) Well-designed manipulations (changing location and scope of spatial attention), and careful retinotopic/pRF mapping, allow for a robust assay of the spatial profile of attentional enhancement, which has not been carefully measured in previous studies.</p><p>(2) Results are overall clear, especially concerning width of the spatial region of attentional enhancement, and lack of clear and consistent evidence for reduction in the amplitude of enhancement profile.</p><p>(3) Model-fitting to characterize spatial scope of enhancement improves interpretability of findings.</p></disp-quote><p>We thank the reviewer for highlighting the strengths of our study.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) Task difficulty seems to vary as a function of spatial scope of attention, with varying ratios of letters/digits across spatial scope conditions, which may complicate interpretations of neural modulation results</p></disp-quote><p>The reviewer is correct in observing that task accuracy varied across cue widths. Though we selected the task ratios to be as similar as possible given the size and spacing of our stimuli (aside from the narrowest cue width of one bin, the proportions for the others were 0.67, 0.60, and 0.67), behavioral accuracy across the three larger cue widths was not identical. Prior research has shown that there is a relationship between task difficulty and the overall magnitude of the BOLD response (e.g., Ress, Backus &amp; Heeger, 2000). Thus, we would primarily expect effects of task difficulty on gain rather than width. How task difficulty influences the BOLD response and whether this would, in fact, interact with the width of the attentional field is an important topic, and we hope that future work will investigate this relationship more directly.</p><p>To clarify these points and highlight the potential for future work looking at these important interactions, we added the following text to the discussion section (discussion lines 57-67):</p><p>“The observed effects of attentional field width were unlikely to be directly attributable to variation in task difficulty. Participants' task in our study was to discriminate whether more numbers or more letters were presented within a cued region of an iso-eccentric annulus of white noise. For our different cue widths, the ratios of numbers and letters were selected to be as similar as possible given the size and spacing of our stimuli. Changes in accuracy across the three larger cue widths were small and non-monotonic, implying task difficulty was dissociable from width per se. This dissociation bolsters the interpretability of our model fits; nevertheless, future work should further investigate how task difficulty interacts with the spread of the attentional field and the amplitude of attention-related BOLD effects (cf. Ress, Backus and Heeger, 2000).”</p><disp-quote content-type="editor-comment"><p>(2) Some aspects of analysis/data sorting are unclear (e.g., how are voxels selected for analyses?)</p></disp-quote><p>We apologize for not describing our voxel selection in sufficient detail. Some of the questions raised in the private comments are closely related to this point, we therefore aim to clarify all concerns below:</p><p>- Voxel selection: To select voxels that contribute to the 1D spatial profiles, we relied on the independent pRF dataset. We first defined some general requirements that needed to be met. Specifically, (1) the goodness of fit (R<sup>2</sup>) of the pRF fits needed to be greater than 10%; (2) the estimated eccentricity had to fall within [0.7 9.1] degree eccentricity (to exclude voxels in the fovea and voxels with estimated eccentricities larger than the pRF mapping stimulus); (3) the estimated size must be greater than 0.01 degree visual angle.</p><p>Next, we included only voxels whose pRF overlapped with the white noise annulus. Estimated eccentricity was used to select all voxels whose eccentricity estimate fell within the annulus bounds. However, here it is also important to take the size of the pRF into account. Some voxels’ estimated eccentricity might fall just outside the annulus, but will still have substantial overlap due to the size of their pRF. Therefore, we further included all voxels whose estimated pRF size resulted in overlap with the annulus.</p><p>This implies that some voxels with greater eccentricities and larger pRF sizes contribute to the 1D profile, which will influence the spatial specificity of the 1D profiles. However, we want to emphasize that in our view, the exact FWHM value is not so much of interest, as this will always be dependent on the voxel selection and many other data processing steps. Instead, we focus on the relative differences of the FWHM driven by the parametric attentional cue width manipulation.</p><p>- Data sorting and binning. The reviewer raises an important point about how the FWHM value should be interpreted considering the data processing steps. To generate the 1D spatial profile, we binned voxels based on their estimated polar angle preference into 6degree bins and applied a moving average of 18 degrees to smooth the 1D profiles. Both of these processing steps will influence the spatial specificity of the profile. The binning step facilitates recentering based on cue center and combining across trials.</p><p>To explore the extent to which the moving average substantially impacted our results, we reran our analyses without that smoothing step. The vast majority of the results held. In V1, we found a significant effect of cue width on FWHM where the result was not significant previously (<italic>t</italic>(7)=2.52, <italic>p</italic>=.040). Additionally, when looking at the minimum number of TRs needed to see a significant effect of cue width on FWHM, without the smoothing step in V1 it took 10 TRs (not significant at 10 TRs previously), in V2 it took 5 TRs (10 previously), and in V3 it took 3 TRs (2 previously). The other notable difference is that FWHM was generally a bit larger when the moving average smoothing was performed. We have visualized the group results for the FWHM estimates below to help with comparison.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>No moving average smoothing.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-sa3-fig1-v1.tif"/></fig><p>Voxel selection methods have been clarified in methods section lines 132-139:</p><p>“Within each ROI, pRF modeling results were used to constrain voxel selection used in the main experiment. We excluded voxels with a preferred eccentricity outside the bounds of the pRF stimulus (&lt;0.7° and &gt;9.1°), with a pRF size smaller than 0.01°, or with poor spatial selectivity as indicated by the pRF model fit (R2 &lt; 10%). Following our 2D visualizations (see below), we further constrained voxel selection by only including voxels whose pRF overlapped with the white noise annulus. We included all voxels with an estimated eccentricity within the annulus bounds, as well as voxels with an estimated pRF size that would overlap the annulus.”</p><p>Data binning methods have been clarified in methods section lines 154-159:</p><p>“Voxels with pRFs overlapping the white noise annulus were grouped into 60 bins according to their pRF polar angle estimate (6° polar angle bin width). We computed a median BOLD response within each bin. This facilitated the recentering of each profile to align all cue centers for subsequent combining across trials. To improve the signal-to-noise ratio, the resulting profile was smoothed with a moving average filter (width 18° polar angle; see Figure 2b).”</p><disp-quote content-type="editor-comment"><p>(3) While the focus of this report is on modulations of visual cortex responses due to attention, the lack of inclusion of results from other retinotopic areas (e.g. V3AB, hV4, IPS regions like IPS0/1) is a weakness</p></disp-quote><p>We agree with the reviewer that using this approach in other retinotopic areas would be of significant interest. In this case, population receptive field mapping occurred in a separate session with a field of view only covering the occipital cortex (in contrast to the experimental session, which had whole-brain coverage). Because our modeling approach relies on these pRF estimates, we were unable to explore higher visual areas. However, we hope future work will follow up on this.</p><p>We have added the following text to the methods section describing the pRF mapping session (lines 87-89):</p><p>“In this session, the field of view was restricted to the occipital cortex to maximize SNR, thereby limiting the brain regions for which we had pRF estimates to V1, V2, and V3.”</p><disp-quote content-type="editor-comment"><p>(4) Additional analyses comparing model fits across amounts of data analyzed suggest the model fitting procedure is biased, with some parameters (e.g., FWHM, error, gain) scaling with noise.</p></disp-quote><p>In this analysis, we sought to test how much data was needed to recover the attentional field, in view of the need for additional fMRI-based tools for use in tasks that involve more rapid dynamic adaptation of attention. Though we did find that more data reduced noise (and accordingly decreased absolute error and amplitude while increasing FWHM and R<sup>2</sup>), absolute angular error remained low across different temporal intervals (well below the chance level of 90°). With regard to FWHM, we believe that the more important finding is that the model-estimated FWHM was modulated by cue width at shorter timescales of as few as two TRs while maintaining relatively low angular error. We refrain from drawing conclusions here on the basis of the exact FWHM values, both because we don’t have a ground truth for the attentional field and because various processing pipeline steps can impact the values as well. Rather, we are looking at relative value and overall patterns in the estimates. The observed patterns imply that the model recovers meaningful modulation of the attentional field even at shorter time scales.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>Additional data reporting and discussion of results are needed as outlined in the public review.</p><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) The current experimental design effectively captured the impact of varying cue widths on the BOLD response in the visual cortex. However, the inclusion of a neutral cue condition, where the cue width spans 360{degree sign} and all peripheral stimuli are attended, could serve as a valuable baseline. This would enable a quantitative assessment of how much the BOLD response is enhanced in specific spatial regions due to focused cues and, conversely, how much it is diminished in non-attended areas, along with the spatial extent of these effects.</p></disp-quote><p>Please refer to our response in the public review.</p><disp-quote content-type="editor-comment"><p>(2) While the study provides valuable insights into BOLD signal changes in visual areas corresponding to the focus of attention, it does not extend its analysis to the impact on regions outside the focus of attention. It would be beneficial to explore whether there is a corresponding decrease in BOLD signal in non-attended regions, and if identified, to describe the spatial extent and position of this effect relative to the attended area. Such an analysis could yield deeper insights into how attention influences activity across the visual cortex.</p></disp-quote><p>We agree with the reviewer that it is very interesting to examine the spread of attention across the whole visual field. Our experiment was designed to focus on width modulations at a fixed eccentricity, but future work should explore how the attentional field changes with eccentricity and interacts with spatial variations across the visual field. This is highlighted in our discussion section (lines 76-81):</p><p>“Future work can help provide a better understanding of the contribution of spatial attention by considering how the attentional field interacts with these well described spatial variations across the visual field. Measuring the full spatial distribution of the attentional field (across both eccentricity and polar angle) will shed light on how spatial attention guides perception by interacting with the non-uniformity of spatial representations.”</p><p>The addition of figure panels for the estimated baseline parameter in Figures 4-7 provides further information about BOLD effects in unattended regions of the annulus.</p><disp-quote content-type="editor-comment"><p>(3) The rationale behind the selection of task difficulty ratios for different cue widths, specifically the letter-to-number ratios of 1:0, 1:2, 2:3, and 3:6 (or vice versa) for cue widths of 18{degree sign}, 54{degree sign}, 90{degree sign}, and 162{degree sign} respectively, was not explicitly discussed. It would be beneficial to clarify the basis for these ratios, as they may influence the perceived difficulty of the task and thus the comparability of behavioral performance across different cue widths. Ensuring that the task difficulty is consistent across conditions is crucial for attributing differences in behavior and BOLD signals solely to changes in cue width and not confounded by variations in task difficulty.</p></disp-quote><p>Please refer to our response in the public review. We now clarify why we selected these ratios, and acknowledge more explicitly that behavioral performance differed across width conditions. See also our reply to private comment 1 from Reviewer 3 for some additional analyses examining task related influences.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>(1) Task difficulty: the task seems exceptionally challenging. Stimuli are presented at a relativelyeccentric position for a very brief duration, and a large number of comparisons must be made across a broad region of space. This is reflected in the behavioral performance, which decreases rapidly as the scope of attention increases (Fig. 1). Because trials are blocked, does this change in task difficulty across conditions impact the degree to which neural responses are modulated? How should we consider differences in task difficulty in interpreting the conclusions (especially with respect to the amplitude parameter)? Also, note that the difficulty scales both with number of stimuli - as more need to be compared - but also with the ratio, which differs nonmonotonically across task conditions. One way to dissociate these might be RT: for 54/162, which both employ the same ratio of letter/digits and have similar accuracy, is RT longer for 162, which requires attending more stimuli?</p></disp-quote><p>In addition to our comments in response to the public review, we emphasize that the reviewer makes an important point that there are differences in task difficulty, though the ratios are as close as they can be given the size and spacing of our stimuli. Behavioral performance varied non-monotonically with cue width, bolstering our confidence that our monotonically increasing model-estimated width is likely not entirely driven by task difficulty. There nevertheless remain open questions related to how task difficulty does impact BOLD attentional modulation, which we hope future work will more directly investigate.</p><p>The reviewer's comments identify two ways our data might preliminarily speak to questions about BOLD attentional modulation and task difficulty. First: how might the amplitude parameter reflect task difficulty? This is an apt question as we agree with the reviewer that it would be a likely candidate in which to observe effects of task difficulty. We do find a small effect of <italic>cue width</italic> on our amplitude estimates (amplitude decreases with width) in V3. Using the same analysis technique to look at the relationship between task difficulty and amplitude, we find no clear relationship in any of the visual areas (all <italic>p</italic> &gt;= 0.165, testing whether the slopes differed from zero at the group level using a one-sample t-test). We believe future work using other experimental manipulations should look more systematically at the relationship between task difficulty and amplitude of the attentional BOLD enhancement.</p><p>Second: Does the same ratio at different widths elicit different behavioral responses (namely accuracy and RT)? We followed the reviewer’s suggestion to compare performance between cue widths of three and nine (identical ratios, different widths; see Author response image 2 and Figure 5). We found that, using a paired t-test, behavioral accuracy differed between the two cue widths (mean accuracy of 0.73 versus 0.69, p = 0.008), with better performance for cue width three. RT did not differ significantly between the two conditions (paired t-test, p = 0.729). This could be due to the fact that participants were not incentivized to respond as quickly as possible, they merely needed to respond before the end of the response window (1.25 s) following the stimulus presentation (0.5 s). The comparisons for accuracy and RT (calculated from time of stimulus appearance) are plotted below:</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-sa3-fig2-v1.tif"/></fig><p>In summary, with matched stimulus ratios, the wider cue was associated with worse (though not slower) performance. This could be due to the fact that more elements are involved and/or that tasks become more difficult when attending to a broader swath of space. Given these results, we believe that future studies targeting difficulty effects should use direct and independent manipulations of task difficulty and attentional width.</p><disp-quote content-type="editor-comment"><p>(2) Eye movements: while the authors do a good job addressing the average eccentricity of fixation, I'm not sure this fully addresses concerns with eye movements, especially for the character-discrimination task which surely benefits from foveation (and requires a great deal of control to minimize saccades!). Can the authors additionally provide data on, e.g., # of fixations within the attended stimulus annulus, or fixation heatmap, or # of saccades, or some other indicator of likelihood of fixating the letter stimuli for each condition?</p></disp-quote><p>We agree with the reviewer that this task is surely much easier if one foveated the stimuli, and it did indeed require control to minimize saccades to the annulus. (We appreciate the effort and motivation of our participants!) We are happy to provide additional data to address these reasonable concerns about eye movements. Below, we have visualized the number of fixations to the annulus, separated by participant and width. Though there is variability across participants, there are at most 16 instances of fixations to the annulus for a given participant, combined across all width conditions. The median number of fixations to the annulus per width is zero (shown in red). Considering the amount of time participants engaged in the task (between 8 and 12 runs of the task, each run with 100 trials), this indicates participants were generally successful at maintaining central fixation while the stimuli were presented.</p><fig id="sa3fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-sa3-fig3-v1.tif"/></fig><p>We added the results of this analysis to the methods section (lines 205-208):</p><p>“Additionally, we examined the number of fixations to the white noise annulus itself. No participant had more than 16 fixations (out of 800-1200 trials) to the annulus during the task, further suggesting that participants successfully maintained fixation.”</p><disp-quote content-type="editor-comment"><p>(3) pRF sorting and smoothing: Throughout, the authors are analyzing data binned based on pRF properties with respect to the attended location (&quot;voxels with pRFs overlapping with the white noise annulus&quot;, line 243-244) First, what does this mean? Does the pRF center need to be within the annulus? Or is there a threshold based on the pRF size? If so, how is this implemented? Additionally, considering the methods text in lines 242-247, the authors mention that they bin across 6 deg-wide bins and smooth with a moving average (18 deg), which I think will lead to further expansion of the profile of attentional enhancement (see also below)</p></disp-quote><p>We provide a detailed response in the public review. Furthermore, we have clarified the voxel selection procedure in the Methods (lines 132–139 &amp; 154–159).</p><disp-quote content-type="editor-comment"><p>(4) FWHM values: The authors interpret the larger FWHMs estimated from their model-fitting than the actual size of the attended region as a meaningful result. However, depending on details of the sorting procedure above, this may just be due to the data processing itself. One way to identify how much expansion of FWHM occurs due to analysis is by simulating data given estimates of pRF properties for a 'known' shape of modulation (e.g., square wave exactly spanning the attended aperture) and compare the resulting FWHM to that observed for attention and perception conditions (e.g., Fig. 7c).</p></disp-quote><p>We provide a detailed response in the public review. The essence of our response is to refrain from interpreting the precise recovered FWHM values, which will be influenced by multiple processing steps, and instead to focus on relative differences as a function of the attentional cue width. Accordingly, we did not add simulations to the revised manuscript, although we agree with the reviewer that such simulations could shed light on the underlying spatial resolution, and how binning and smoothing influences the estimated FWHM. We have clarified our interpretation of FWHM results in the manuscript as follows:</p><p>Results lines 137-141:</p><p>“One possibility is that the BOLD-derived FWHM might tend to overestimate the retinotopic extent of the modulation, perhaps driven by binning and smoothing processing steps to create the 1D spatial profiles. If this were the case, we would expect to obtain similar FWHM estimates when modeling the perceptual modulations as well.”</p><p>Results lines 169-175:</p><p>“Mirroring the results from the attentional manipulation, FWHM estimates systematically exceeded the nominal size of the perceptually modulated region of the visual field. Comparing the estimated FWHMs of the perceptual and attentional spatial profiles (Figure 7c) revealed that the estimated widths were highly comparable (Pearson correlation r=0.664 across width conditions and visual regions). Importantly, the relative differences in FWHM show meaningful effects of both cue and contrast width in a similar manner for both attentional and perceptual forms of modulation.”</p><p>Discussion lines 16-22:</p><p>“We also found that the estimated spatial spread of the attentional modulation (as indicated by the recovered FWHM) was consistently wider than the cued region itself. We therefore compared the spread of the attention field with the spatial profile of a perceptually induced width manipulation. The results were comparable in both the attentional and perceptual versions of the task, suggesting that cueing attention to a region results in a similar 1D spatial profile to when the stimulus contrast is simply increased in that region.”</p><disp-quote content-type="editor-comment"><p>(5) Baseline parameter: looking at the 'raw' response profiles shown in Fig. 2b, it looks, at first, like the wider attentional window shows substantially lower enhancement. However, this seems to be mitigated by the shift of the curve downwards. Can the authors analyze the baseline parameter in a similar manner as their amplitude analyses throughout? This is especially interesting in contrast to the perception results (Fig. 7), for which the baseline does not seem to scale in a similar way.</p></disp-quote><p>We agree with the reviewer that the baseline parameter is worth examining, and have therefore added panels displaying the baseline parameter into all results figures (Figures 4-7). There was no significant association between cue width and baseline offset in any of the three visual regions.</p><disp-quote content-type="editor-comment"><p>(6) Outlier: Fig. 5, V2, Amplitude result seems to have a substantial outlier - is there any notable difference in e.g. retinotopy in this participant?</p></disp-quote><p>One participant indeed has a notably larger median amplitude estimate in V2. Below, we plot the spatial coverage from the pRF data for this participant (022), as well as all other participants.</p><fig id="sa3fig4" position="float"><label>Author response image 4.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104222-sa3-fig4-v1.tif"/></fig><p>Each subplot represents a participant's 2D histogram of included voxels for the 1D spatial profiles; the colors indicate the proportion of voxels that fell within a specific x,y coordinate bin. Note that this visualization only shows x and y estimates and does not take into account size of the pRF. While there is variation across participants in the visual field coverage, the overall similarity of the maps indicates that retinotopy is unlikely to be the explanation.</p><p>To further explore whether this participant might be an outlier, we additionally looked at behavioral performance, angular error and FWHM parameters as well as the goodness of fit of the model. On all these criteria this participant did not appear to be an outlier. We therefore see no reason to exclude this participant from the analyses.</p><disp-quote content-type="editor-comment"><p>(7) Fig. 4 vs Fig. 5: I understand that Fig. 4 shows results from a single participant, showing variability across blocks, while Fig. 5 shows aggregate results across participants. However, the Angular Error figure shows complementary results - Fig. 4 shows the variability of best-fit angular error, while Fig. 5 shows the average deviation (approximately the width of the error distribution). This makes sense I think, but perhaps the abs(error) for the single participant shown in Fig. 4 should be included in the caption so we can easily compare between figures.</p></disp-quote><p>That's right: the Figure 4 results show the signed error, whereas the Figure 5 results show the absolute error. We agree that reporting the absolute error values for the example participant would facilitate comparison. Rather than add the values to the text, we have made the example participant’s data visually distinct within Figure 5 for easy comparison.</p><disp-quote content-type="editor-comment"><p>(8) Bias in model fits: the analysis shown in Fig. 6 compares the estimated parameters across amounts of data used to compute attentional modulation profiles for fitting those parameters. If the model-fitting procedure were unbiased, my sense is we would likely see no impact of the number of TRs on the parameters (R^2 should improve, abs(error) should improve, but FWHM, amplitude, baseline, etc should be approximately stable, if noisier). However, instead, it looks like more/less data leads to biased estimates, such that FWHM is biased to be smaller with more noise, and amplitude is biased to be larger. This suggests (to me) that the fit is landing on a spiky function that captures a noise wiggle in the profile. I don't think this is a problem for the primary results across the whole block of 10 TRs, which is the main point of the paper. Indeed, I'm not sure what this figure is really adding, since the single-TR result isn't pursued further (see below).</p></disp-quote><p>Please refer to our response in the public review, comment 4.</p><disp-quote content-type="editor-comment"><p>(9) 'Dynamics': The paper, starting in the title, claims to get at the 'dynamics' of attention fields. At least to me, that word implies something that changes over time (rather than across trials). Maybe I'm misinterpreting the intent of the authors, but at present, I'm not sure the use of the word is justified. That said, if the authors could analyze the temporal evolution of the attention field through each block of trials at 1- or 2-TR resolution, I think that could be a neat addition to the paper and would support the claim that the study assays dynamic attention fields.</p></disp-quote><p>We thank the reviewer for giving us a chance to speak more directly to the dynamic aspect of our approach. Here, we specifically use the word “dynamic” to refer to trial-to-trial dynamics. Importantly, our temporal interval analysis suggests that we can recover information about the attentional field at a relatively fine-grained temporal resolution (a few seconds, or 2 TRs). Following this methodological proof-of-concept to dynamically track the attentional field, we are excited about future work that can more directly investigate the manner in which the attentional field evolves through time, especially in comparison to other methods that first require training on large amounts of data.</p><disp-quote content-type="editor-comment"><p>(10) Correction for multiple comparisons across ROIs: it seems that it may be necessary to correct statistical tests for multiple comparisons across each ROI (e.g., Fig. 5 regression tests). If this isn't necessary, the authors should include some justification. I'm not sure this changes any conclusions, but is worth considering.</p></disp-quote><p>We appreciate the opportunity to explain our reasoning regarding multiple comparisons. We thought it appropriate not to correct as we are not comparing across regions and are not treating tests of V1, V2, and V3 as multiple opportunities to support a common hypothesis. Rather, the presence or absence of an effect in each visual region is a separate question. We would typically perform correction for multiple comparisons to control the familywise error rate when conducting a family of tests addressing a common hypothesis. We have added this to the Methods section (lines 192-195):</p><p>“No multiple comparison correction was applied, as the different tests for each region are treated as separate questions. However, using a threshold of 0.017 for p-values would correct for comparisons across the three brain regions.”</p><p>However, we are happy to provide corrected results. If we use Bonferroni correction across ROIs (i.e. multiply p-values by three), there are some small changes from significant to only trending towards significance, but these changes don’t affect any core results. The changes that go from significant to trending are:</p><p>Associated with Figure 5 – In V3, the relationship of cue width to amplitude goes from a p-value of 0.017 to 0.051.</p><p>Associated with Figure 6 –</p><p>V1: the effect of cue width on FWHM goes from p = 0.043 to 0.128.</p><p>V2: the effect of TR on both FWHM and R2 goes from p = ~0.02 to ~0.06.</p><p>V3: the effect of cue width on amplitude goes from p = 0.024 to 0.073.</p></body></sub-article></article>