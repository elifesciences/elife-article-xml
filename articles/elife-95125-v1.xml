<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">95125</article-id><article-id pub-id-type="doi">10.7554/eLife.95125</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95125.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Predicting individual traits from models of brain dynamics accurately and reliably using the Fisher kernel</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Ahrends</surname><given-names>Christine</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9287-1254</contrib-id><email>christine.ahrends@cfin.au.dk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Woolrich</surname><given-names>Mark W</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Vidaurre</surname><given-names>Diego</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9650-2229</contrib-id><email>dvidaurre@cfin.au.dk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01aj84f44</institution-id><institution>Center of Functionally Integrative Neuroscience, Department of Clinical Medicine, Aarhus University</institution></institution-wrap><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Oxford Centre for Human Brain Activity, Department of Psychiatry, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Psychiatry, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Marquand</surname><given-names>Andre F</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Marquand</surname><given-names>Andre F</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Wellcome Centre for Integrative Neuroimaging, John Radcliffe Hospital, University of Oxford, Oxford, United Kingdom</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>31</day><month>01</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP95125</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-12-22"><day>22</day><month>12</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-07-21"><day>21</day><month>07</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.02.530638"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-07"><day>07</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95125.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-15"><day>15</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95125.2"/></event></pub-history><permissions><copyright-statement>© 2024, Ahrends et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Ahrends et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-95125-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-95125-figures-v1.pdf"/><abstract><p>Predicting an individual’s cognitive traits or clinical condition using brain signals is a central goal in modern neuroscience. This is commonly done using either structural aspects, such as structural connectivity or cortical thickness, or aggregated measures of brain activity that average over time. But these approaches are missing a central aspect of brain function: the unique ways in which an individual’s brain activity unfolds over time. One reason why these dynamic patterns are not usually considered is that they have to be described by complex, high-dimensional models; and it is unclear how best to use these models for prediction. We here propose an approach that describes dynamic functional connectivity and amplitude patterns using a Hidden Markov model (HMM) and combines it with the Fisher kernel, which can be used to predict individual traits. The Fisher kernel is constructed from the HMM in a mathematically principled manner, thereby preserving the structure of the underlying model. We show here, in fMRI data, that the HMM-Fisher kernel approach is accurate and reliable. We compare the Fisher kernel to other prediction methods, both time-varying and time-averaged functional connectivity-based models. Our approach leverages information about an individual’s time-varying amplitude and functional connectivity for prediction and has broad applications in cognitive neuroscience and personalised medicine.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Watching how people behave over time can provide insights into their personality, mental health, as well as how they think and problem solve. Like behaviour, brain activity patterns constantly change, both at rest and in response to external events. These changes might reveal crucial information about a person that cannot be seen when looking at a single snapshot or an average of brain activity.</p><p>It has been difficult for researchers to predict individual traits from the overarching dynamic patterns of brain activity measured using brain scans and other imaging tools. This is due to the patterns being too complex to be analyzed directly. Mathematical models like the Hidden Markov Model can describe dynamic patterns in brain activity, such as how different brain areas’ activity and interaction with one another changes over time. To use this type of model to predict individual traits, Ahrends et al. combined it with a machine learning technique known as the Fisher kernel.</p><p>Using this combination of techniques to model dynamic patterns of brain activity based on scans from 1,000 resting people allowed the researchers to successfully predict an individual’s age and their score in various cognitive tests. This approach was shown to more accurately predict traits than alternative methods.</p><p>In the future, researchers may use this new modeling technique to search for markers of disease in dynamic brain activity patterns. For example, this could provide information about the progression of neuropsychiatric diseases over time. It may also help neuroscientists study how dynamic brain activity patterns contribute to individual cognitive performance.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain dynamics</kwd><kwd>machine learning</kwd><kwd>fMRI</kwd><kwd>Hidden Markov Modelling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009708</institution-id><institution>Novo Nordisk Fonden</institution></institution-wrap></funding-source><award-id>NNF19OC-0054895</award-id><principal-award-recipient><name><surname>Vidaurre</surname><given-names>Diego</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-StG-2019-850404</award-id><principal-award-recipient><name><surname>Vidaurre</surname><given-names>Diego</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>215573/Z/19/Z</award-id><principal-award-recipient><name><surname>Woolrich</surname><given-names>Mark W</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The HMM-Fisher kernel approach leverages individual signatures of brain dynamics for prediction, which can be used, for example, to search for brain dynamics-informed biomarkers of neuropsychiatric disease or predict treatment response.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Observing a person’s behaviour over time is how we understand the individual’s personality, cognitive traits, or psychiatric condition. The same should apply at the brain level, where we may be able to gain crucial insights by observing the patterns in which brain activity unfolds over time, that is brain dynamics. One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain. However, although research into brain dynamics has recently gained traction (<xref ref-type="bibr" rid="bib10">Breakspear, 2017</xref>; <xref ref-type="bibr" rid="bib12">Calhoun et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Fox et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Hutchison et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Liégeois et al., 2017</xref>), it is still unclear how best to use this spatiotemporal level of description to characterise subject differences or predict individual traits from brain signals. One reason why brain dynamics are not usually considered in this context pertains to their representation: They are represented using models of varying complexity that are estimated from modalities such as functional MRI or MEG. Although there exists a variety of methods for estimating time-varying or dynamic FC (<xref ref-type="bibr" rid="bib31">Lurie et al., 2020</xref>), like the commonly used sliding-window approach, there is currently no widely accepted way of using them for prediction problems. This is because these models are usually parametrised by a high number of parameters with complex mathematical relationships between the parameters that reflect the model assumptions. How to leverage these parameters for prediction is currently an open question.</p><p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information from generative models that do not assume any knowledge of task timings. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, in this case acquired during wakeful rest, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM), which is a probabilistic generative model of time-varying amplitude and functional connectivity (FC) dynamics (<xref ref-type="bibr" rid="bib56">Vidaurre et al., 2017</xref>). HMMs have previously been shown to be able to predict certain complex subject traits, such as fluid intelligence, more accurately than structural or static (time-averaged) FC representations (<xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>). We combine the HMM with the Fisher kernel, which allows for the efficient use of the entire set of parameters from the generative model. The Fisher kernel takes the complex relationships between the model parameters into account by preserving the structure of the underlying model (here, the HMM; <xref ref-type="bibr" rid="bib26">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="bib25">Jaakkola and Haussler, 1998</xref>). Mathematically, the HMM parameters lie on a Riemannian manifold (the structure). This defines, for instance, the relation between parameters, such as: how changing one parameter, like the probabilities of transitioning from one state to another, would affect the fitting of other parameters, like the states’ FC. It also defines the relative importance of each parameter; for example, how a change of 0.1 in the transition probabilities would not be the same as a change of 0.1 in one edge of the states’ FC matrices.</p><p>For empirical evaluation, we consider two criteria that are important in both scientific and practical applications. First, predictions should be as accurate as possible, that is the correlation between predicted and actual values should be high. Second, predictions should be reliable, in the sense that a predictive model should never produce excessively large errors, and the outcome should be robust to reasonable variations in the data set, for example the choice of which subjects from the same population are included in the training set. The latter criterion is especially important if we want to be able to meaningfully interpret prediction errors, for example in assessing brain age (<xref ref-type="bibr" rid="bib13">Cole and Franke, 2017</xref>; <xref ref-type="bibr" rid="bib14">Denissen et al., 2022</xref>; <xref ref-type="bibr" rid="bib47">Smith et al., 2019</xref>). Despite this crucial role in interpreting model errors, reliability is not often considered in models predicting individual traits from neuroimaging features.</p><p>In summary, we show that using the Fisher kernel approach, which preserves the mathematical structure of the underlying HMM, we can predict individual traits from patterns of brain dynamics accurately and reliably. We show that our approach significantly outperforms methods that do not take the mathematical structure of the model into account, as well as methods based on time-averaged FC that do not consider brain dynamics. For interpretation, we also investigate which aspects of the model drive the prediction accuracy, both in real data and in simulations. Bringing accuracy, reliability and interpretation together, this work opens possibilities for practical applications such as the development of biomarkers and the investigation of individual differences in cognitive traits.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We here aimed to predict behavioural and demographic variables from a model of brain dynamics using different kernel functions. The general workflow is illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We started with the concatenated fMRI time-series of a group of subjects (<xref ref-type="fig" rid="fig1">Figure 1</xref>, step 1), here the resting-state fMRI timeseries of 1001 subjects from the Human Connectome Project (HCP, described in detail in section ‘HCP imaging and behavioural variables’). We estimated a model of brain dynamics, here the Hidden Markov Model (HMM), which is a state-space model of time-varying amplitude and FC. The HMM and its parameters are explained in detail in section ‘The Hidden Markov Model’. We estimated the model at the group level, where the state descriptions, initial state probabilities, and the state transition probability matrix are shared across subjects (<xref ref-type="fig" rid="fig1">Figure 1</xref>, step 2). Next, we estimated subject-specific versions of this group-level model by dual estimation, where the group-level HMM parameters are re-estimated to fit the individual-level timeseries (<xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 3).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Workflow of the Fisher kernel prediction approach.</title><p>To generate a description of brain dynamics, we (1) concatenate all subjects’ individual timeseries; then (2) estimate a Hidden Markov Model (HMM) on these timeseries to generate a group-level model; then (3) dual-estimate into subject-level HMM models. Steps 1–3 are the same for all kernels. In order to then use this description of all subjects’ individual patterns of brain dynamics, we map each subject into a feature space (4). This mapping can be done in different ways: In the naïve kernels (4a), the manifold (i.e. the curved structure) on which the parameters lie is ignored and examples are treated as if they were in Euclidean space. The Fisher kernel (4b), on the other hand, respects the structure of the parameters in their original Riemannian manifold by working in the gradient space. We then construct kernel matrices (κ), where each pair of subjects has a similarity value given their parameters in the respective embedding space. Finally, we feed κ to kernel ridge regression to predict a variety of demographic and behavioural traits in a cross-validated fashion (5).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig1-v1.tif"/></fig><p>Next, we used this (HMM-mediated) description of the individuals’ brain dynamics to predict their individual traits. The parameters of the model lie on a Riemannian manifold, which is a space that has some degree of curvature, illustrated by the curved structure in <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 4. We mapped these parameters into a feature space. This step works in different ways for the different kernels: In the naïve kernel (step 4a), the features are simply the parameters in the Euclidean space (i.e. ignoring the curvature of the space in <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 4a); while in the Fisher kernel (step 4b), the features are mapped into the gradient space, which is a tangent space to the Riemannian manifold. We then estimated the similarity between each pair of subjects in this feature space using kernel functions. In this way, we can compare kernels that do not take the structure of the underlying model into account (the naïve kernels) with a kernel that preserves this structure (the Fisher kernel). We also compared these kernels to a previously established method based on Kullback-Leibler divergence, which estimates the similarity between the probability distributions of each pair of individual HMMs. The different kernels are described in more detail in section ‘Kernels from Hidden Markov models’.</p><p>Finally, we used these kernels to predict the behavioural variables using kernel ridge regression (<xref ref-type="fig" rid="fig1">Figure 1</xref>, step 5, described in detail in section ‘Predictive model: Kernel ridge regression’). The first three steps are identical for all kernels and therefore carried out only once. The fourth step (mapping the examples and constructing the kernels) is carried out once for each of the different kernels. The last step is repeated 3500 times for each kernel to predict a set of 35 different behavioural variables using 100 randomised iterations of 10-fold nested cross validation (CV). We evaluated 24,500 predictive models using different kernels constructed from the same model of brain dynamics in terms of their ability to predict phenotypes, as well as another 24,500 predictive models based on time-averaged features, described in detail in section ‘Models based on time-averaged FC features’.</p><sec id="s2-1"><title>The Fisher kernel predicts more accurately than Euclidean methods</title><p>Using the resting-state fMRI timeseries from the HCP dataset, we found that among the kernels constructed from HMMs, the linear Fisher kernel had the highest prediction accuracy on average across the range of behavioural variables and CV folds and iterations, as shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>. Compared to the other linear kernels (which do not respect the geometry of the HMM parameters), the linear Fisher kernel (mean <italic>r κ</italic><sub><italic>Fl</italic></sub>: 0.192) was significantly more accurate than the linear naïve kernel (mean <italic>r κ<sub>Nl</sub></italic>: 0.05, <italic>t<sub>rkCV</sub></italic>=2.631, <italic>p<sub>BH</sub></italic>=0.031). The comparison with the linear naïve normalised kernel was not significant (mean <italic>r κ<sub>NNl</sub></italic>: 0.153, <italic>t<sub>rkCV</sub></italic>=1.022, <italic>p<sub>BH</sub></italic>=0.192). This indicates a positive effect of using a tangent space embedding rather than incorrectly treating the HMM parameters as Euclidean, but that this effect can be mitigated by normalising the parameters before constructing the kernel.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Distributions of performance across subject traits and CV iterations when using different methods for prediction of subject traits on HCP data.</title><p>The best-performing methods are highlighted by black arrows in each plot. (<bold>a</bold>) Pearson’s correlation coefficients (r) between predicted and actual variable values in deconfounded space as a measure of prediction accuracy (x-axis) of each method (y-axis). Larger values indicate that the model predicts more accurately. The linear Fisher kernel has the highest average accuracy among the time-varying methods, while the Ridge regression model in Riemannian space had the highest average accuracy among the time-averaged methods. Note that we here show the distribution across target variables and CV iterations but averaged over folds for visualisation purposes, while the fold-wise accuracies were used for significance testing. Asterisks indicate significant Benjamini-Hochberg corrected p-values of repeated k-fold cross-validation corrected t-tests below 0.05 (*). (<bold>b</bold>) Coefficient of determination (R<sup>2</sup>) in deconfounded space (x-axis) for each of the methods (y-axis). The x-axis is cropped at –0.1 for visualisation purposes since individual runs can produce large negative outliers, see panel c. (<bold>c</bold>) Normalised maximum absolute errors (NMAXAE) in original (non-deconfounded) space as a measure of excessive errors (x-axis) by method (y-axis). Large maximum errors indicate that the model predicts very poorly in single cases. Differences between the methods mainly lie in the tails of the distributions, where the naïve normalised Gaussian kernel produces extreme maximum errors in some runs (NMAXAE &gt;10,000), while the linear naïve normalised kernel and the linear Fisher kernel, along with several time-averaged methods have the smallest risk of excessive errors (NMAXAE below 1). The x-axis is plotted on the log-scale. (<bold>d</bold>) Robustness of prediction accuracies. The plot shows the distribution across variables of the standard deviation of correlation coefficients over folds and CV iterations on the x-axis for each method (on the y-axis). Smaller values indicate greater robustness. The linear Fisher kernel and the time-averaged Ridge regression model in Riemannian space are the most robust. Asterisks indicate significant Benjamini-Hochberg corrected p-values for repeated measures t-tests below 0.01 (**) and 0.001 (***). (<bold>a</bold>, <bold>b</bold>, <bold>c</bold>) Each violin plot shows the distribution over 3500 runs (100 iterations of 10-fold CV for all 35 variables) that were predicted from each method. (<bold>d</bold>) Each violin plot shows the distribution over 35 variables that were predicted from each method.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Average errors.</title><p>(<bold>a, b</bold>) Distribution and robustness of average errors. (<bold>c, d</bold>) While a larger correlation coefficient indicates a stronger relationship between the predictor and the predicted variable (<bold>c</bold>), a smaller average error (<bold>d</bold>) can be achieved by predicting closely around the mean. This is the case for the Elastic Net models and the Selected Edges model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Results from using only one scanning session per participant.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Results from HMM where states are defined only in terms of covariance and mean is set to 0.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Effect of hyperparameters on model errors.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig2-figsupp4-v1.tif"/></fig></fig-group><p>Among the Gaussian kernels, the Gaussian Fisher kernel also had the highest average prediction accuracy (mean <italic>r κ<sub>Fg</sub></italic>: 0.166), though the comparisons with the other kernels were not significant (mean <italic>r κ</italic><sub><italic>Ng</italic></sub>: 0.107, <italic>t<sub>rkCV</sub></italic>=1.277, <italic>p<sub>BH</sub></italic>=0.173; mean <italic>r κ</italic><sub><italic>NNg</italic></sub>: 0.094, <italic>t<sub>rkCV</sub></italic>=1.466, <italic>p<sub>BH</sub></italic>=0.145; mean <italic>r κ</italic><sub><italic>KL</italic></sub>: 0.163, <italic>t<sub>rkCV</sub></italic>=0.069, <italic>p<sub>BH</sub></italic>=0.482). Comparing prediction accuracies of the linear with the Gaussian Fisher kernel was not significant (<italic>t<sub>rkCV</sub></italic>=0.993, <italic>p<sub>BH</sub></italic>=0.192).</p><p>Compared to the methods using time-averaged FC for prediction, the linear Fisher kernel significantly outperformed the Selected Edges method (mean <italic>r</italic> Selected Edges: 0.081, <italic>t<sub>rkCV</sub></italic>=2.417, <italic>p<sub>BH</sub></italic>=0.031). The average prediction accuracy of the linear Fisher kernel is also higher than the log-Euclidean kernel (mean <italic>r</italic>: 0.140, <italic>t<sub>rkCV</sub></italic>=1.456, <italic>p<sub>BH</sub></italic>=0.145) and the Ridge regression model (mean <italic>r</italic>: 0.111, <italic>t<sub>rkCV</sub></italic>=1.906, <italic>p<sub>BH</sub></italic>=0.085), but outperformed by time-averaged KL divergence (mean <italic>r</italic>: 0.194, <italic>t<sub>rkCV</sub></italic>=−0.044, <italic>p<sub>BH</sub></italic>=0.482) and Ridge regression in Riemannian space (mean <italic>r</italic> Ridge Riem.: 0.223, <italic>t<sub>rkCV</sub></italic>=−0.999, <italic>p<sub>BH</sub></italic>=0.192), though these comparisons were all not significant. Analogous to the effect of tangent space embedding for the HMM parameters (linear Fisher kernel compared to linear naïve kernel), using a tangent space embedding on the time-averaged covariance matrices (Ridge regression in Riemannian space compared to Ridge regression) also significantly improved the prediction accuracy (<italic>t<sub>rkCV</sub></italic>=−2.537, <italic>p<sub>BH</sub></italic>=0.0313). The Elastic Net and Elastic Net in Riemannian space were not used for statistical comparisons since they failed to converge in a substantial portion of runs (&gt;20%). The Elastic Nets showed similar performance to the Ridge regression models. We also observed that the non-kernel-based Euclidean time-averaged FC models, that is the Ridge regression and the Selected Edges model, make predictions at a smaller range than the actual variables, close to the variables’ means. This leads to weak relationships between predicted and actual variables but smaller errors, as shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The distributions of correlation coefficients for the different methods are shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref> and the explained variance in <xref ref-type="fig" rid="fig2">Figure 2b</xref>. The performance of all methods is also summarised in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref>.</p><p>We found that the linear Fisher kernel also predicted more accurately than other HMM-based kernels when fitting the HMM to only the first resting-state session of each subject, as shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. However, the overall accuracies of all kernels were lower in this case, indicating that predicting traits benefits from a large amount of available data per subject. Similarly, the Fisher kernel outperformed the other kernels when HMM states were defined only in terms of covariance (not mean) at comparable accuracy, as shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.</p><p>As shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, there are differences in how well these demographic or behavioural variables can be predicted from a model of brain dynamics. Certain variables may be more related to static or structural measures (<xref ref-type="bibr" rid="bib30">Liégeois et al., 2019</xref>; <xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>), or just be difficult to predict in general. Overall, age could be best predicted, followed by language-related cognitive items (PicVocab, ReadEng both age adjusted and unadjusted).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model performance estimates over cross-validation (CV) iterations by behavioural variable and method, ordered by accuracy on HCP data.</title><p>Boxplots show the distribution over 100 iterations of 10-fold CV of correlation coefficient values (x-axis) of each method, separately for each of the 35 predicted variables (y-axes). Among the time-varying methods, the linear Fisher kernel (green) predicts at higher accuracy for many variables, and also shows the narrowest range, indicating high robustness. However, for many target variables, it is outperformed by the time-averaged tangent space models (Ridge reg. Riem. and Elastic Net Riem.). Black lines within each boxplot represent the median.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig3-v1.tif"/></fig><p>In summary, the linear Fisher kernel has the highest prediction accuracy of the time-varying methods, significantly outperforming the linear naïve kernel which does not take the geometry of the HMM parameters into account. The linear Fisher kernel also has a higher prediction accuracy than several methods using time-averaged FC features, but it is outperformed by time-averaged methods that work in tangent space.</p></sec><sec id="s2-2"><title>The linear Fisher kernel has a lower risk of excessive errors and is more robust than other methods</title><p>We now show empirically that the linear Fisher kernel is more reliable than other kernels, both in terms of risk of large errors and in terms of robustness over CV iterations.</p><p>The linear versions of the Fisher kernel and the naïve normalised kernel had the overall lowest risk of large errors among the time-varying methods, as shown in <xref ref-type="fig" rid="fig2">Figure 2c</xref>. We assessed the risk of large errors (NMAXAE &gt;10), very large errors (NMAXAE &gt;100), and extreme errors (NMAXAE &gt;1,000), corresponding to one, two, and three orders of magnitude of the range of the actual variable. For the Fisher kernel, the risk of large errors is low: 0% in the linear version <italic>κ</italic><sub><italic>Fl</italic></sub> and 0.029% in the Gaussian version <italic>κ</italic><sub><italic>Fg</italic></sub>. That means that the linear Fisher kernel never makes large errors exceeding the range of the actual variable by orders of magnitude. In the naïve kernel, both the linear <italic>κ</italic><sub><italic>Nl</italic></sub> and the Gaussian version <italic>κ</italic><sub><italic>Ng</italic></sub> have a low risk of large errors at 0.057% for the linear version and 0.029% for the Gaussian version. While the linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub> has a 0% risk of large errors, its Gaussian version <italic>κ</italic><sub><italic>NNg</italic></sub> has the overall highest risk of large errors at 1.229%, a risk of very large errors at 0.143%, and even a risk of extreme errors at 0.029%. The KL divergence model <italic>κ</italic><sub><italic>KL</italic></sub> has a 0.686% risk of large errors and a 0.029% risk of very large errors. The time-averaged KL divergence model performs slightly better than the time-varying KL divergence, but also has a risk of large errors at 0.600%. The other time-averaged models had no risk of excessive errors. The maximum error distributions are shown in <xref ref-type="fig" rid="fig2">Figure 2c</xref>.</p><p>A reason for the higher risk of large errors in the Gaussian kernels is likely that the radius <inline-formula><mml:math id="inf1"><mml:mi>τ</mml:mi></mml:math></inline-formula> of the radial basis function needs to be selected (using cross-validation), introducing an additional factor of variability and leaving more room for error. <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> shows the relation between the estimated hyperparameters (the regularisation parameter <inline-formula><mml:math id="inf2"><mml:mi>λ</mml:mi></mml:math></inline-formula> and the radius <inline-formula><mml:math id="inf3"><mml:mi>τ</mml:mi></mml:math></inline-formula> of the radial basis function) and how large errors in the predictions may be related to poor estimation of these parameters.</p><p>With respect to robustness, we found that the linear Fisher kernel <italic>κ</italic><sub><italic>Fl</italic></sub> had the most robust performance among the time-varying methods, and the Ridge regression model in Riemannian space among the time-averaged methods, on average across the range of variables tested, as shown in <xref ref-type="fig" rid="fig2">Figure 2d</xref>. Robustness was quantified as the standard deviation of the correlation between model-predicted and actual values over 100 iterations and 10 folds of CV. A low standard deviation indicates high robustness since the method’s performance does not differ greatly depending on the specific subjects it was trained and tested on.</p><p>Among the time-varying methods, the linear Fisher kernel was the most robust method (<italic>κ</italic><sub><italic>Fl</italic></sub> mean S.D. <italic>r</italic>: 0.088), though the comparison with the other linear kernels was not significant (linear naïve kernel <italic>κ</italic><sub><italic>Nl</italic></sub> mean S.D. <italic>r</italic>: 0.096, <italic>t</italic>=−2.351, <italic>p<sub>BH</sub></italic>=0.099; linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub> mean S.D. <italic>r</italic>: 0.090, <italic>t</italic>=–1.745, <italic>p<sub>BH</sub></italic>=0.180). Similarly, the comparison between the Gaussian Fisher kernel (<italic>κ</italic><sub><italic>Fg</italic></sub> mean S.D. <italic>r</italic>: 0.089) and the other Gaussian kernels was not significant (Gaussian naïve kernel <italic>κ</italic><sub><italic>Ng</italic></sub> mean S.D. <italic>r</italic>: 0.089, <italic>t</italic>=0.311, <italic>p<sub>BH</sub></italic>=0.826; Gaussian naïve normalised kernel <italic>κ</italic><sub><italic>NNg</italic></sub> mean S.D. <italic>r</italic>: 0.093, <italic>t</italic>=−1.802, <italic>p<sub>BH</sub></italic>=0.180; KL divergence <italic>κ</italic><sub><italic>KL</italic></sub> mean S.D. <italic>r</italic>: 0.093, <italic>t</italic>=−1.758, <italic>p<sub>BH</sub></italic>=0.180). Compared to the time-averaged methods, the linear Fisher kernel was significantly more robust than the time-averaged KL divergence model (mean S.D. <italic>r</italic>: 0.100, <italic>t</italic>=−3.798, <italic>p<sub>BH</sub></italic>=0.003) and the Selected Edges model (mean S.D. <italic>r</italic>: 0.100, <italic>t</italic>=−5.446, <italic>p<sub>BH</sub></italic> &lt;0.0001), while the other comparisons were not significant (log-Euclidean mean S.D. <italic>r</italic>: 0.091, <italic>t</italic>=−1.317, <italic>p<sub>BH</sub></italic>=0.277; Ridge reg. mean S.D. <italic>r</italic>: 0.091, <italic>t</italic>=−1.451, <italic>p<sub>BH</sub></italic>=0.268; Ridge reg. Riem. mean S.D. <italic>r</italic>: 0.088, <italic>t</italic>=0.059, <italic>p<sub>BH</sub></italic>=0.953). This large variation in model performance depending on the CV fold structure in the time-averaged KL divergence model and the Selected Edges method is problematic. Among the time-averaged methods, the Ridge Regression model in Riemannian space was the most robust method. The ranges in model performance across CV iterations for each variable of the different kernels are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><p>Overall, the linear Fisher kernel was the most reliable method among the time-varying methods, and the Ridge regression model in Riemannian space was the most reliable among the time-averaged methods. For both methods, there was no risk of large errors and the variability over CV iterations was the smallest. The Gaussian kernels had higher risks of large errors, and both the time-varying and the time-averaged KL divergence model risked producing large errors, indicating that their performance was less reliable. The Gaussian naïve normalised kernel was the most problematic in terms of reliability with a risk of extreme errors ranging up to four orders of magnitude of the actual variable’s range. Two of the time-averaged methods, the time-averaged KL divergence model and the Selected Edges method, showed problems with robustness, indicating considerable susceptibility to changes in CV folds.</p></sec><sec id="s2-3"><title>State features drive predictions of individual differences for Fisher kernel</title><p>To understand which features drive the prediction, we next simulated timeseries of two groups of subjects that were different either in the mean amplitude of one state or in the transition probabilities. As shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, when we simulated two groups of subjects that are different in terms of the mean amplitude, the Fisher kernel was able to recover this difference in all runs with 0% error, meaning that it identified all subjects correctly in all runs. The Fisher kernel significantly outperformed the other two kernels (Fisher kernel <italic>κ</italic><sub><italic>Fl</italic></sub> vs. naïve kernel <italic>κ</italic><sub><italic>Nl</italic></sub>: p<italic>=</italic>0.0003, vs. naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub>: p<italic>=</italic>0.0001). There was no significant difference between the naïve and the naïve normalised kernel (p=1). However, when we simulated differences in transition probabilities between two groups, neither of the kernels were able to reliably recover this difference. In this simulation, the Fisher kernel performed significantly worse than the other two kernels on average (compared to naïve kernel: p<italic>=</italic>0.006, compared to naïve normalised kernel: p=0.004), as shown in <xref ref-type="fig" rid="fig4">Figure 4b</xref>. As in the previous simulation, the naïve kernel and the naïve normalised kernel did not significantly differ from each other (p=1).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Simulations.</title><p>(<bold>a</bold>) Simulating two groups of subjects that are different in their state means. The error distributions of all 10 iterations show that the Fisher kernel recovers the simulated group difference in all runs with 0% error. (<bold>b</bold>) Simulating two groups of subjects that are different in their transition probabilities. Neither kernel is able to reliably recover the group difference in all 10 iterations. (<bold>c</bold>) Simulating two groups of subjects that are different in their transition probabilities but excluding state parameters when constructing the kernels. The Fisher kernel performs best in recovering the group difference.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig4-v1.tif"/></fig><p>The above results suggest that all kernels, but particularly the Fisher kernel, are most sensitive to differences in state parameters rather than differences in transition probabilities. To understand whether the difference in transition probabilities can be recovered when it is not overshadowed by the more dominant state parameters, we ran the second case of simulations again, where we introduced a group difference in terms of transition probabilities, but this time we exclude the state parameters when we constructed the kernels. As shown in <xref ref-type="fig" rid="fig4">Figure 4c</xref>, the Fisher kernel was now able to recover the group difference with minimal errors, while the naïve normalised kernel improved but did not perform as well as the Fisher kernel. The naïve kernel performed below chance. The Fisher kernel significantly outperformed both the naïve kernel (p=0.0001) and the naïve normalised kernel (p=0.02), and the naïve normalised kernel was significantly more accurate than the naïve kernel (p=0.0004). This shows that the Fisher kernel can recover the group difference in transition probabilities when this difference is not overshadowed by the state parameters. The features and kernel matrices of example runs for all three simulations are shown in <xref ref-type="fig" rid="fig4">Figure 4a–c</xref> middle and right panels.</p><p>When using real data, the features driving the prediction may differ from trait to trait: For some traits, state parameters may be more relevant, while for other traits, transitions may be more relevant. Given our above findings on simulated data, we therefore compared the effects of systematically removing state features or removing transition features in all traits in the real data.</p><p>We found that state parameters were the most relevant features for the Fisher kernel predictions in all traits: As shown in <xref ref-type="fig" rid="fig5">Figure 5a</xref>, the prediction accuracy of the Fisher kernel was significantly diminished when state features were removed (<italic>t<sub>rkCV</sub></italic> = 2.922, <italic>p<sub>BH</sub></italic> = 0.016), while removing transition features had no significant effect (<italic>t<sub>rkCV</sub></italic> = –0.173, <italic>p<sub>BH</sub></italic> = 0.480). We observed the same effect in the naïve normalised kernel (no state features: <italic>t<sub>rkCV</sub></italic> = 2.460, <italic>p<sub>BH</sub></italic> = 0.031; no transition features: <italic>t<sub>rkCV</sub></italic> = 0.264, <italic>p<sub>BH</sub></italic> = 0.480). For the naïve kernel, removing features did not have any significant effects (no state features: <italic>t<sub>rkCV</sub></italic> = 0.501, <italic>p<sub>BH</sub></italic> = 0.462; no transition features: <italic>t<sub>rkCV</sub></italic>=0.050, <italic>p<sub>BH</sub></italic> = 0.480). One reason for the dominance of state parameters may simply be that the state parameters outnumber the other parameters: In the full kernels, we have 15,300 state features (300 features associated with the state means and 15,000 features associated with the state covariances), but only 42 transition features (6 features associated with the state probabilities and 36 features associated with the transition probabilities). To compensate for this imbalance, we also constructed a version of the kernels where state parameters were reduced to the same amount as transition features using Principal Component Analysis (PCA), so that we have 84 features in total (42 transition features and the first 42 PCs of the 15,300 state features). These PCA-kernels performed better than the ones where state features were removed, but worse than the kernels including all features at the original dimensionality, although not significantly (Fisher kernel: <italic>t<sub>rkCV</sub></italic> = 1.434, <italic>p<sub>BH</sub></italic> = 0.170; naïve kernel: <italic>t<sub>rkCV</sub></italic> = 0.860, <italic>p<sub>BH</sub></italic> = 0.351; naïve normalised kernel: <italic>t<sub>rkCV</sub></italic> = 1.555, <italic>p<sub>BH</sub></italic> = 0.170). This indicates that the fact that state parameters are more numerous than transition parameters does not explain why kernels including state features performed better. Instead, the content of the state features is more relevant for the prediction than the content of the transition features.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Effects of removing sets of features from the kernels on prediction accuracies.</title><p>(<bold>a</bold>) In the overall prediction accuracies, removing state features significantly decreased performance in the Fisher kernel and the naïve normalised kernel, while removing transition features had no significant effect. (<bold>b</bold>) Removing features has similar effects on all variables, both better predicted (left panel) and worse predicted ones (right panel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig5-v1.tif"/></fig><p>When looking at the performance separately for each variable (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), we found that all variables were better predicted by the version of the kernel which included state features than the ones where state features were removed, while it did not seem to matter whether transition features were included. This indicates that the simulation case described above, where the relevant changes are in the transition probabilities, did not occur in the real data. In certain variables, reducing state features using PCA improved the accuracy compared to the full kernels. This is not unexpected since feature dimensionality reduction is known to be able to improve prediction accuracy by removing redundant features (<xref ref-type="bibr" rid="bib34">Mwangi et al., 2014</xref>).</p><p>Empirically, we thus found that the Fisher kernel is most sensitive to individual differences in state parameters, both in simulated timeseries and in the real data. This means that predictions are driven more by what an individual’s states look like, rather than by how they transition between states. However, the Fisher kernel can be modified to recover differences in transition probabilities if these are relevant for specific traits.</p></sec><sec id="s2-4"><title>Separation between training and test set in HMM training</title><p>In the results above, we have constructed kernels from HMMs fit to all subjects in the dataset and only separated them into training and test set at the regression (and deconfounding) step. In machine learning, the gold standard is considered to be a full separation between training and test set at all stages of (pre-)processing to avoid data leakage from the test set into the training set (<xref ref-type="bibr" rid="bib28">Kapoor and Narayanan, 2023</xref>; <xref ref-type="bibr" rid="bib36">Poldrack et al., 2020</xref>). However, a recent study in neuroimaging has found minimal effects of data leakage for breaches of separation between training and test set not involving the target variable or feature selection (<xref ref-type="bibr" rid="bib38">Rosenblatt et al., 2024</xref>). To test whether training the HMM on all subjects may have inflated prediction accuracies, we repeated the HMM-based predictions for kernels that were constructed from HMMs trained only on training subjects. Consistent with the results in <xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>, for all kernels, both linear and Gaussian versions, the separation of training and test subjects before vs. after fitting the HMM had no significant effects on prediction accuracies (training together vs. separate: linear Fisher kernel: <italic>t<sub>kCV</sub></italic> = 0, <italic>p<sub>BH</sub></italic> = 1; Gaussian Fisher kernel: <italic>t<sub>kCV</sub></italic> = 0, <italic>p<sub>BH</sub></italic> = 1; linear naïve kernel: <italic>t<sub>kCV</sub></italic> = −0.139, <italic>p<sub>BH</sub></italic> = 1; Gaussian naïve kernel: <italic>t<sub>kCV</sub></italic> = −0.018, <italic>p<sub>BH</sub></italic> = 1; linear naïve normalised kernel: <italic>t<sub>kCV</sub></italic> = −0.030, <italic>p<sub>BH</sub></italic> = 1; Gaussian naïve normalised kernel: <italic>t<sub>kCV</sub></italic> = 0.040, <italic>p<sub>BH</sub></italic> = 1). <xref ref-type="fig" rid="fig6">Figure 6a</xref> shows the distribution of accuracies across target variables and CV folds for all kernels and training schemes, confirming that at this sample size (N=1001), the effect is negligible.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Effects of HMM training scheme.</title><p>(<bold>a</bold>) Prediction accuracies for HMM-based kernels depending on HMM training scheme (training on all subjects: together; training only on training set: separate). In the real data (N=1001), fitting the HMM to all subjects before constructing the kernels compared to fitting it only to the training set to preserve train-test separation has no effect. Note that we are here plotting the fold-wise accuracies (as opposed to averaged over folds, as in the figures above), and we only ran one iteration of CV (rather than 100 repetitions, as in the figures above). (<bold>b</bold>) Prediction accuracies in simulated heterogeneous subject groups depending on training scheme, between-group difference, and target variable (Y) noise. In simulated data, the Fisher kernel’s performance decreases when the test subjects are increasingly different from the training subjects. (<bold>c</bold>) Example kernels for high between-group difference. While the naive kernel underperforms in both cases, the strong difference between training and test subjects is visible in the naive normalised kernel, while it completely dominates the Fisher kernel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95125-fig6-v1.tif"/></fig><p>Importantly, in the Fisher kernel, individual subjects’ features are defined in reference to a group ‘average’. While this has no effect in situations where training and test subjects are taken from the same distribution, it may introduce a bias where test subjects are taken from a different distribution (e.g. the case where one might want to train a model on healthy controls and test on patients). To illustrate this effect, we simulated timeseries for two groups of subjects with varying degrees of between-group difference and noise on the target variable. As shown in <xref ref-type="fig" rid="fig6">Figure 6b</xref>, this does not affect the kernels when training the HMM on all subjects (training scheme: together, middle panel), but the performance of the Fisher kernel worsens as group difference increases when training the HMM only on the training set (training scheme: separate, right panel). This is because in the Fisher kernel, subjects’ similarity is determined by their difference from the group-level HMM. When the test subjects are included in this group-level HMM, their scores will be similar to the training subjects’ scores, but when they are excluded from the group-level HMM, their scores may be overestimated since they are all different from the group-level model. This between-group difference may then overshadow the more subtle differences related to the target variable, as shown in the example kernels in <xref ref-type="fig" rid="fig6">Figure 6c</xref>. Whether or not this behaviour is desired will depend on the use case.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we aimed to establish an approach that allows leveraging a rich description of the patterns in which brain activity unfolds over time to predict individual traits. We showed that the HMM-Fisher kernel approach accurately and reliably predicts traits from brain dynamics models trained on neuroimaging data. It preserves the structure of the underlying brain dynamics model, making it ideal for combining generative and predictive models. We compared the Fisher kernel to kernels which ignore the structure of the brain dynamics model (‘naïve’ kernels), to a previously used method based on Kullback-Leibler divergence (<xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>), and to methods based on time-averaged functional connectivity. The linear Fisher kernel had an overall higher prediction accuracy than all other time-varying methods and several time-averaged methods, though most comparisons were not statistically significant given the narrow margin for improvements. The linear Fisher kernel was also among the most reliable: It never produced excessive errors and was robust to changes in training sets. Like in the time-varying methods, working in Riemannian space also improved prediction for the time-averaged methods, indicating that respecting the geometry of the space that the predictors lie on is an important factor for predictive modelling in neuroscience. While we here focussed on fMRI, the method can also be applied to other modalities like MEG or EEG. It can also be straightforwardly implemented in any kernel-based prediction model or classifier, including kernel ridge regression, support vector machines (SVM), kernel fisher discriminant analysis (k-FDA), kernel logistic regression (KLR), or nonlinear PCA. Indeed, it can also be applied to other probabilistic generative models aside from the HMM, for example Dynamic network modes (<xref ref-type="bibr" rid="bib19">Gohil et al., 2022</xref>).</p><p>Our findings were consistent in two alternative settings: when using less data per subject and when modelling only time-varying FC (rather than amplitude and FC states). This supports the generalisability of the results. However, we observed overall lower accuracies when using only one scanning session consisting of 1200 timepoints per subject. This indicates that more available timepoints per subject allow better characterisation of an individual. We also found that the Gaussian versions of the kernels are generally more error-prone and susceptible to changes in the training set, although they may predict more accurately in certain runs. Implementing Gaussian kernels in a predictive model is also computationally more expensive, making them less practical.</p><p>While we here tested robustness in terms of susceptibility to changes in CV folds, it remains to be shown to what extent model performance is sensitive to the random initialisation of the HMM, which affects the parameter estimation (<xref ref-type="bibr" rid="bib3">Alonso and Vidaurre, 2023</xref>; <xref ref-type="bibr" rid="bib22">Griffin et al., 2024</xref>). We also showed that the Fisher kernel is most sensitive to changes in state descriptions, that is individual differences in the amplitude or functional connectivity of certain brain states. While this could be a disadvantage if a trait was more closely related to how an individual transitions between brain states, we found that this was not the case in any of the traits we tested here. Other traits than the ones we tested here may of course be more related to individual transition patterns. For this case, we showed in simulations that the Fisher kernel can be modified to recognise changes in transitions if they are of interest for the specific research question.</p><p>Finally, we showed that the results we presented here are unaffected by separation between training and test set at the step of training the group-level HMM. However, since the Fisher kernel defines individuals in reference to the group-level model, we showed in simulations that separating training and test subjects prior to fitting the HMM may result in biased kernels that overestimate dissimilarity of the test subjects. This is an important consideration as it may affect studies with small sample sizes and with heterogeneous training and test sets (e.g. where a researcher may want to fit a model to healthy controls and subsequently test it on patients).</p><p>We here aimed to show the potential of the HMM-Fisher kernel approach to leverage information from patterns of brain dynamics to predict individual traits in an example fMRI dataset as well as simulated data. The fMRI dataset we used (HCP 1200 Young Adult) is a large sample taken from a healthy, young population, and it remains to be shown how the exhibited performance generalises to other datasets, for example other modalities such as EEG/MEG, clinical data, older populations, different data quality, or smaller sample sizes both in terms of the number of participants and the scanning duration. Additionally, we only tested our approach for the prediction of a specific set of demographic items and cognitive scores; it may be interesting to test the framework also on clinical variables, such as the presence of a disease or the response to pharmacological treatment.</p><p>There is growing interest in combining different data types or modalities, such as structural, static, and dynamic measures, to predict phenotypes (<xref ref-type="bibr" rid="bib16">Engemann et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Schouten et al., 2016</xref>). While directly combining the features from each modality can be problematic, modality-specific kernels, such as the Fisher kernel for time-varying amplitude and/or FC, can be easily combined using approaches such as stacking (<xref ref-type="bibr" rid="bib11">Breiman, 1996</xref>) or Multi Kernel Learning (MKL; <xref ref-type="bibr" rid="bib20">Gönen and Alpaydın, 2011</xref>). MKL can improve prediction accuracy of multimodal studies (<xref ref-type="bibr" rid="bib50">Vaghari et al., 2022</xref>), and stacking has recently been shown to be a useful framework for combining static and time-varying FC predictions (<xref ref-type="bibr" rid="bib22">Griffin et al., 2024</xref>). A detailed comparison of different multimodal prediction strategies including kernels for time-varying amplitude/FC may be the focus of future work.</p><p>In a clinical context, while there are nowadays highly accurate biomarkers and prognostics for many diseases, others, such as psychiatric diseases, remain poorly understood, diagnosed, and treated. Here, improving the description of individual variability in brain measures may have potential benefits for a variety of clinical goals, for example to diagnose or predict individual patients’ outcomes, find biomarkers, or to deepen our understanding of changes in the brain related to treatment responses like drugs or non-pharmacological therapies (<xref ref-type="bibr" rid="bib33">Marquand et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Stephan et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Wen et al., 2022</xref>; <xref ref-type="bibr" rid="bib62">Wolfers et al., 2015</xref>). However, the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, as one of many possible descriptions of brain dynamics, and it can be flexibly modified or extended to include, for example information about temporally recurring frequency patterns (<xref ref-type="bibr" rid="bib55">Vidaurre et al., 2016</xref>). In order to be able to use predictive models from brain dynamics in a clinical context, predictions must be reliable, particularly if we want to interpret model errors, as in models of “brain age” (<xref ref-type="bibr" rid="bib13">Cole and Franke, 2017</xref>; <xref ref-type="bibr" rid="bib14">Denissen et al., 2022</xref>; <xref ref-type="bibr" rid="bib47">Smith et al., 2019</xref>). As we demonstrated in this work, there can be extreme errors and large variation in some predictive models, and these issues are not resolved by estimating model performance in a standard cross-validated fashion. We here showed that taking the structure of the underlying model or predictors into account, and thoroughly assessing not only accuracy but also errors and robustness, we can reliably use information from brain dynamics to predict individual traits. This will allow gaining crucial insights into cognition and behaviour from how brain function changes over time, beyond structural and static information.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>HCP imaging and behavioural data</title><p>We used data from the open-access Human Connectome Project (HCP) S1200 release (<xref ref-type="bibr" rid="bib45">Smith et al., 2013a</xref>; <xref ref-type="bibr" rid="bib53">Van Essen et al., 2013</xref>), which contains MR imaging data and various demographic and behavioural data from 1200 healthy, young adults (age 22–35). All data described below, that is timecourses of the resting-state fMRI data and demographic and behavioural variables, are publicly available at <ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org">https://db.humanconnectome.org</ext-link>.</p><p>Specifically, we used resting state fMRI data of 1001 subjects, for whom any of the behavioural variables of interest were available. Each participant completed four resting state scanning sessions of 14 min and 33 s duration each. This resulted in 4800 timepoints per subject. For the main results, we used all four resting state scanning sessions of each participant to fit the model of brain dynamics (but see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for results with just one session). The acquisition parameters are described in the HCP acquisition protocols and in <xref ref-type="bibr" rid="bib53">Van Essen et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Van Essen et al., 2012</xref>. Briefly, structural and functional MRI data were acquired on a 3T MRI scanner. The resting state fMRI data was acquired using multiband echo planar imaging sequences with an acceleration factor of 8 at 2 mm isotropic spatial resolution and a repetition time (TR) of 0.72 s. The preprocessing and timecourse extraction pipeline is described in detail in <xref ref-type="bibr" rid="bib45">Smith et al., 2013a</xref>. For the resting state fMRI scans, preprocessing consisted of minimal spatial preprocessing and surface projection (<xref ref-type="bibr" rid="bib18">Glasser et al., 2013</xref>), followed by temporal preprocessing. Temporal preprocessing consisted of single-session Independent Component Analysis (ICA; <xref ref-type="bibr" rid="bib8">Beckmann, 2012</xref>) and removal of noise components (<xref ref-type="bibr" rid="bib21">Griffanti et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Salimi-Khorshidi et al., 2014</xref>). Data were high-pass-filtered with a cut-off at 2000s to remove linear trends.</p><p>The parcellation was estimated from the data using multi-session spatial ICA on the temporally concatenated data from all subjects. Note that this means that there is no strict divide between the subjects used for training and the subjects for testing the later predictive models, so that there is potential for leakage of information between training and test set. However, since this step does not concern the target variable, but only the preprocessing of the predictors, the effect can be expected to be minimal (<xref ref-type="bibr" rid="bib38">Rosenblatt et al., 2024</xref>). Using this approach, a data-driven functional parcellation with 50 parcels was estimated, where all voxels are weighted according to their activity in each parcel, resulting in a weighted, overlapping parcellation. While other parcellations are available for the resting-state fMRI HCP dataset, we chose this parcellation because dynamic changes in FC have been shown to be better detected in this parcellation compared to other functional or anatomical parcellations or more fine-grained parcellations (<xref ref-type="bibr" rid="bib1">Ahrends et al., 2022</xref>). Timecourses were extracted using dual regression (<xref ref-type="bibr" rid="bib7">Beckmann et al., 2009</xref>), where group-level components are regressed onto each subject’s fMRI data to obtain subject-specific versions of the parcels and their timecourses. We normalised the timecourses of each subject to ensure that the model of brain dynamics and, crucially, the kernels were not driven by (averaged) amplitude and variance differences between subjects.</p><p>Subjects in the HCP study completed a range of demographic and behavioural questionnaires. Following <xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>, we here focus on a subset of those items, including age and various cognitive variables. The cognitive variables span items assessing memory, executive function, fluid intelligence, language, processing speed, spatial orientation, and attention. The full list of the 35 behavioural variables used here, as well as their categorisation within the HCP dataset can be found in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>.</p></sec><sec id="s4-2"><title>The Hidden Markov model</title><p>To estimate patterns of time-varying amplitude and FC, we here use the Hidden Markov Model (<xref ref-type="bibr" rid="bib55">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Vidaurre et al., 2017</xref>). However, the kernels, which are explained in detail in the following section, can be constructed from any generative probabilistic model.</p><p>The Hidden Markov model (HMM) is a generative probabilistic model, which assumes that an observed time-series, such as BOLD signal in a given parcellation, was generated by a sequence of ‘hidden states’ (<xref ref-type="bibr" rid="bib6">Baum and Eagon, 1967</xref>; <xref ref-type="bibr" rid="bib5">Baum and Petrie, 1966</xref>). We here model the states as multivariate Gaussian distributions, defined both in terms of mean and covariance —which can be interpreted as distinct patterns of amplitude and FC (<xref ref-type="bibr" rid="bib56">Vidaurre et al., 2017</xref>). For comparison, we also considered a second variety of the HMM where state means were pinned to zero (given that the data was demeaned and the global average is zero) and only the covariance was allowed to vary across states (<xref ref-type="bibr" rid="bib56">Vidaurre et al., 2017</xref>), which is shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>. This is equivalent to using a Wishart state model, and therefore focuses more specifically on time-varying FC.</p><p>The HMM is described by a set of parameters <inline-formula><mml:math id="inf4"><mml:mi>θ</mml:mi></mml:math></inline-formula>, containing the state probabilities <inline-formula><mml:math id="inf5"><mml:mi>π</mml:mi></mml:math></inline-formula>, the transition probabilities <inline-formula><mml:math id="inf6"><mml:mi>A</mml:mi></mml:math></inline-formula>, the mean vectors <italic>μ</italic> of all states (if modelled), and the covariance matrices <inline-formula><mml:math id="inf7"><mml:mi>Σ</mml:mi></mml:math></inline-formula> of all states:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>π</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf8"><mml:mi>K</mml:mi></mml:math></inline-formula> is the number of states and <inline-formula><mml:math id="inf9"><mml:mi>M</mml:mi></mml:math></inline-formula> is the number of parcels in the parcellation. The entire set of parameters <inline-formula><mml:math id="inf10"><mml:mi>θ</mml:mi></mml:math></inline-formula> is estimated from the data. The number of states can be understood as the level of detail or granularity with which we describe the spatiotemporal patterns in the data, akin to a dimensionality reduction, where a small number of states will lead to a very general, coarse description and a large number of states will lead to a very detailed, fine-grained description. Here, we chose a small number of states, <inline-formula><mml:math id="inf11"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:math></inline-formula>, to ensure that the group-level HMM states are general enough to be found in all subjects, since a larger number of states increases the chances of certain states being present only in a subset of subjects. The exact number of states is less relevant in this context, since the same HMM estimation is used for all kernels.</p><p>The HMM is a probabilistic generative model, as the generative process works by sampling probabilistically in this case from a Gaussian distribution with mean <italic>μ</italic><sub><italic>k</italic></sub> and covariance Σ<sub><italic>k</italic></sub> when state <inline-formula><mml:math id="inf12"><mml:mi>k</mml:mi></mml:math></inline-formula> is active:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>X</italic><sub><italic>t</italic></sub> is the timeseries at timepoint <inline-formula><mml:math id="inf13"><mml:mi>t</mml:mi></mml:math></inline-formula> and <italic>q</italic><sub><italic>t</italic></sub> is the currently active state. Which state is active depends on the previous state <italic>q</italic><sub><italic>t-1</italic></sub> and is determined by the transition probabilities <inline-formula><mml:math id="inf14"><mml:mi>A</mml:mi></mml:math></inline-formula>, so that the generated state sequence is sampled from a categorical distribution with parameters:<disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>~</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <italic>A</italic><sub><italic>k</italic></sub> indicates the k-th row of the transition probability matrix.</p><p>The space of parameters <inline-formula><mml:math id="inf15"><mml:mi>θ</mml:mi></mml:math></inline-formula> forms a Riemannian manifold <italic>R</italic><sub><italic>θ</italic></sub>, where the relationships between the different parameters of the HMM are acknowledged by construction. The Fisher kernel, as described below, is built upon a projection on this manifold, so predictions based on this kernel account for the mathematical structure of the HMM.</p><p>Here, we fit the HMM to the concatenated timeseries of all <inline-formula><mml:math id="inf16"><mml:mi>N</mml:mi></mml:math></inline-formula> subjects (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 1). We refer to the group-level estimate as HMM<sup>0</sup>, which is defined by the parameters <inline-formula><mml:math id="inf17"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 2):<disp-formula id="equ4"><mml:math id="m4"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:math></disp-formula></p><p>To use the information from the HMM to predict subjects’ phenotypes, we estimate subject-specific versions of the group-level HMM (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 3) through dual estimation (<xref ref-type="bibr" rid="bib56">Vidaurre et al., 2017</xref>). Dual estimation refers to the process of fitting the previously estimated group-level model again to a single subject’s timeseries, so that the parameters from the group-level model HMM<sup>0</sup> are adapted to fit the individual. We will refer to the subject-specific estimate for subject <italic>n</italic> as HMM<sup>n</sup>, with parameters <inline-formula><mml:math id="inf18"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p>These subject-specific HMM parameters are the features from which we construct the kernels. To understand which features are most important for the predictions, we also construct versions of the kernels that include only subsets of the features. Specifically, we can group the features into two subsets: 1. the state features, describing <italic>what</italic> states look like, containing the mean vectors <italic>μ</italic>, and the covariance matrices <inline-formula><mml:math id="inf19"><mml:mi>Σ</mml:mi></mml:math></inline-formula> of all states, and 2. the transition features, describing <italic>how</italic> individuals transition between these states, containing the initial state probabilities <inline-formula><mml:math id="inf20"><mml:mi>π</mml:mi></mml:math></inline-formula> and the transition probabilities <inline-formula><mml:math id="inf21"><mml:mi>A</mml:mi></mml:math></inline-formula>. By removing one or the other set of features and evaluating how model performance changes compared to the full kernels, we can draw conclusions about the importance of these two different types of changes for the predictions. Since the state features are considerably more numerous than the transition features (15,300 state features compared to 42 transition features in this case), we also construct a version of the kernels where state features have been reduced to the same number as the transition features using PCA, that is we use all 42 transition features and the first 42 PCs of the state features. This allowed us to perform a fairer comparison of what elements in the model are more predictive of the subject traits.</p></sec><sec id="s4-3"><title>Kernels from Hidden Markov models</title><p>Kernels (<xref ref-type="bibr" rid="bib43">Shawe-Taylor and Cristianini, 2004</xref>) are a convenient approach to accommodate nonlinearity and to work with high-dimensional, complex features, such as parameters from a model of brain dynamics. In general, kernels are similarity functions, and they can be used straightforwardly in a prediction algorithm. While feature matrices can be very high dimensional, a kernel is represented by a (no. of subjects by no. of subjects) matrix. Kernel methods can readily be adapted to deal with nonlinear decision boundaries in prediction, by projecting the data into a high-dimensional (possibly infinite-dimensional) space through an embedding <inline-formula><mml:math id="inf22"><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; then, by estimating a linear separating hyperplane on this space, we can effectively have a nonlinear estimator on the original space (<xref ref-type="bibr" rid="bib43">Shawe-Taylor and Cristianini, 2004</xref>). In practice, instead of working explicitly in a higher-dimensional embedding space, the so-called kernel trick uses a kernel function <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> containing the similarity between data points <inline-formula><mml:math id="inf24"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mi>m</mml:mi></mml:math></inline-formula> (here, subjects) in the higher-dimensional embedding space (<xref ref-type="bibr" rid="bib41">Schölkopf et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Shawe-Taylor and Cristianini, 2004</xref>), which can be simpler to calculate. Once <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is computed for each pair of subjects, this is all that is needed for the prediction. This makes kernels computationally very efficient, since in most cases the number of subjects will be smaller than the number of features —which, in the case of HMMs, can be very large (potentially, in the order of millions). However, finding the right kernel can be a challenge because there are many available alternatives for the embedding.</p><p>Here, in combination with a linear predictive model, we apply a kernel that is specifically conceived to be used to compare instances of generative models such as the HMM. We expected this to result in better predictions than existing methods. Using the same HMM estimate, we compare three different kernels, which map the HMM parameters into three distinct spaces, corresponding to different embeddings (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 4): the naïve kernel, the naïve normalised kernel, and the Fisher kernel.</p><p>While the first two kernels (naïve and naïve normalised kernel) do not take into account constraints imposed by the HMM on how the model parameters can change with respect to each other, the Fisher kernel does. The Fisher kernel achieves this by calculating how, and by how much, one should change the group level HMM parameters to make the model generate data that is more like the data from a particular subject.</p><p>We then construct linear and Gaussian versions of the different kernels (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 4), which take the general form <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for the linear kernel and <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for the Gaussian kernel. We compare these to a kernel constructed using Kullback-Leibler divergence, previously used for predicting behavioural phenotypes (<xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>).</p><sec id="s4-3-1"><title>Naïve kernel</title><p>The naïve kernel is based on a simple vectorisation of the subject-specific version of the HMM’s parameters, each on their own scale. This means that the kernel does not take relationships between the parameters into account and the parameters are here on different scales. This procedure can be thought of as computing Euclidean distances between two sets of HMM parameters, ignoring the actual geometry of the space of parameters. For each subject <inline-formula><mml:math id="inf29"><mml:mi>n</mml:mi></mml:math></inline-formula>, we vectorise parameters <inline-formula><mml:math id="inf30"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> obtained through dual estimation of the group-level parameters <inline-formula><mml:math id="inf31"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> to map the example <inline-formula><mml:math id="inf32"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> to:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mi>M</mml:mi><mml:mo>∗</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We will refer to this vectorised version of the subject-specific HMM parameters as “naïve <inline-formula><mml:math id="inf33"><mml:mi>θ</mml:mi></mml:math></inline-formula>”. The naïve <inline-formula><mml:math id="inf34"><mml:mi>θ</mml:mi></mml:math></inline-formula> are the features used in the naïve kernel <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We first construct a linear kernel from the naïve <inline-formula><mml:math id="inf36"><mml:mi>θ</mml:mi></mml:math></inline-formula> features using the inner product of the feature vectors. The linear naïve kernel <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> between subjects <inline-formula><mml:math id="inf38"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:mi>m</mml:mi></mml:math></inline-formula> is thus defined as:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the inner product between <inline-formula><mml:math id="inf41"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Using the same feature vectors, we can also construct a Gaussian kernel from the naïve <inline-formula><mml:math id="inf43"><mml:mi>θ</mml:mi></mml:math></inline-formula>. The Gaussian naïve kernel <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for subjects <inline-formula><mml:math id="inf45"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mi>m</mml:mi></mml:math></inline-formula> is defined as:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf47"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the radius of the radial basis function, and <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the L<sub>2</sub>-norm of the difference of the feature vectors (naïve <inline-formula><mml:math id="inf49"><mml:mi>θ</mml:mi></mml:math></inline-formula> for subjects <inline-formula><mml:math id="inf50"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf51"><mml:mi>m</mml:mi></mml:math></inline-formula>). Compared to a linear kernel, a Gaussian kernel embeds the features into a more complex space, which can potentially improve the accuracy. However, this kernel has an additional parameter <inline-formula><mml:math id="inf52"><mml:mi>τ</mml:mi></mml:math></inline-formula> that needs to be chosen, typically through cross-validation. This makes a Gaussian kernel computationally more expensive and, if the additional parameter <inline-formula><mml:math id="inf53"><mml:mi>τ</mml:mi></mml:math></inline-formula> is poorly estimated, more error-prone. The effect of the hyperparameters on errors is shown in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>.</p><p>While the naïve kernel takes all the information from the HMM into account by using all parameters from a subject-specific version of the model, it uses these parameters in a way that ignores the structure of the model that these parameters come from. In this way, the different parameters in the feature vector are difficult to compare, since for example a change of 0.1 in the transition probabilities between two states is not of the same magnitude as a change of 0.1 in one entry of the covariance matrix of a specific state. In the naïve kernel, these two very different types of changes would be treated indistinctly.</p></sec><sec id="s4-3-2"><title>Naïve normalised kernel</title><p>To address the problem of parameters being on different scales, the naïve normalised kernel makes the scale of the subject-specific vectorised parameters (i.e. the naïve <inline-formula><mml:math id="inf54"><mml:mi>θ</mml:mi></mml:math></inline-formula>) comparable across parameters. Here, the mapping <inline-formula><mml:math id="inf55"><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> consists of a vectorisation and normalisation across subjects of the subject-specific HMM parameters, by subtracting the mean over subjects from each parameter and dividing by the standard deviation. This kernel does not respect the geometry of the space of parameters either.</p><p>As for the naïve kernel, we can then construct a linear kernel from these vectorised, normalised parameters <inline-formula><mml:math id="inf56"><mml:mover accent="true"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> by computing the inner product for all pairs of subjects <inline-formula><mml:math id="inf57"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mi>m</mml:mi></mml:math></inline-formula> to obtain the linear naïve normalised kernel <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We can also compute a Gaussian kernel from the naïve normalised feature vectors to obtain the Gaussian version of the naïve normalised kernel <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In this way, we have constructed a kernel in which parameters are all on the same scale, but which still ignores the complex relationships between parameters originally encoded by the underlying model of brain dynamics.</p></sec></sec><sec id="s4-4"><title>Fisher kernel</title><p>The Fisher kernel (<xref ref-type="bibr" rid="bib26">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="bib25">Jaakkola and Haussler, 1998</xref>) is specifically designed to preserve the structure of a generative probabilistic model (here, the HMM). This can be thought of as a ‘proper’ projection on the manifold, as illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>, step 4b. Similarity between subjects is here defined in reference to a group-level model of brain dynamics. The mapping <inline-formula><mml:math id="inf61"><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is given by the ‘Fisher score’, which indicates how (i.e., in which direction in the Riemannian parameter space) we would have to change the group-level model to better explain a particular subject’s timeseries. The similarity between subjects can then be described based on this score, so that two subjects are defined as similar if the group-level model would have to be changed in a similar direction for both, and dissimilar otherwise.</p><p>More precisely, the Fisher score is given by the gradient of the log-likelihood with respect to each model parameter:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∗</mml:mo><mml:mi>M</mml:mi><mml:mo>∗</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf62"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the timeseries of subject <inline-formula><mml:math id="inf63"><mml:mi>n</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the likelihood of the timeseries <inline-formula><mml:math id="inf65"><mml:mi>x</mml:mi></mml:math></inline-formula> given the model parameters <inline-formula><mml:math id="inf66"><mml:mi>θ</mml:mi></mml:math></inline-formula>. This way, the Fisher score maps an example (i.e. a subject’s timeseries) <inline-formula><mml:math id="inf67"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> into a point in the gradient space of the Riemannian manifold <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> defined by the HMM parameters.</p><p>The invariant Fisher kernel <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is the inner product of the Fisher score <inline-formula><mml:math id="inf70"><mml:mi>g</mml:mi></mml:math></inline-formula>, scaled by the Fisher information matrix <inline-formula><mml:math id="inf71"><mml:mi>F</mml:mi></mml:math></inline-formula>, which gives a local metric on the Riemannian manifold <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mspace width="1em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>for subjects <inline-formula><mml:math id="inf73"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mi>m</mml:mi></mml:math></inline-formula>. <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is the Fisher information matrix, defined as<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the expectation is with respect to <inline-formula><mml:math id="inf76"><mml:mi>x</mml:mi></mml:math></inline-formula> under the distribution <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The Fisher information matrix <inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> can be approximated empirically:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which is simply the covariance matrix of the gradients <inline-formula><mml:math id="inf79"><mml:mi>g</mml:mi></mml:math></inline-formula>. Using <inline-formula><mml:math id="inf80"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> essentially serves to whiten the gradients; therefore, given the large computational cost associated with <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, we here disregard the Fisher information matrix and reduce the invariant Fisher kernel to the so-called practical Fisher kernel (<xref ref-type="bibr" rid="bib26">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="bib25">Jaakkola and Haussler, 1998</xref>; <xref ref-type="bibr" rid="bib51">van der Maaten, 2011</xref>), for which the linear version <inline-formula><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> takes the form:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In this study, we will use the practical Fisher kernel for all computations.</p><p>One issue when working with the linear Fisher kernel is that the gradients of typical examples (i.e. subjects whose timeseries can be described by similar parameters as the group-level model) are close to zero, while gradients of atypical examples (i.e. subjects who are very different from the group-level model) can be very large. This may lead to an underestimation of the similarity between two typical examples because their inner product is very small even though they are very similar. To mitigate this, we can plug the gradient features (i.e. the Fisher scores <inline-formula><mml:math id="inf83"><mml:mi>g</mml:mi></mml:math></inline-formula>) into a Gaussian kernel, which essentially normalises the kernel. For subjects <inline-formula><mml:math id="inf84"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf85"><mml:mi>m</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf86"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the timeseries of subject <inline-formula><mml:math id="inf87"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf88"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the timeseries of subject <inline-formula><mml:math id="inf89"><mml:mi>m</mml:mi></mml:math></inline-formula>, the Gaussian Fisher kernel <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the distance between examples <inline-formula><mml:math id="inf92"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:mi>m</mml:mi></mml:math></inline-formula> in the gradient space, and <inline-formula><mml:math id="inf94"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the width of the Gaussian kernel.</p></sec><sec id="s4-5"><title>Kullback-Leibler divergence</title><p>The Kullback-Leibler (KL) divergence is an information-theoretic distance measure which estimates divergence between probability distributions —in this case between subject-specific versions of the HMM. Here, KL divergence of subject <inline-formula><mml:math id="inf95"><mml:mi>n</mml:mi></mml:math></inline-formula> from subject <inline-formula><mml:math id="inf96"><mml:mi>m</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> can be interpreted as how much new information the HMM of subject <inline-formula><mml:math id="inf98"><mml:mi>n</mml:mi></mml:math></inline-formula> contains if the true distribution was the HMM of subject <inline-formula><mml:math id="inf99"><mml:mi>m</mml:mi></mml:math></inline-formula>. KL divergence is not symmetric, that is <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is different than <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We here use an approximation of KL divergence as in <xref ref-type="bibr" rid="bib15">Do, 2003</xref> and <xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>. That is, given two models <inline-formula><mml:math id="inf102"><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> from <inline-formula><mml:math id="inf103"><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for subject <inline-formula><mml:math id="inf104"><mml:mi>n</mml:mi></mml:math></inline-formula> and subject <inline-formula><mml:math id="inf105"><mml:mi>m</mml:mi></mml:math></inline-formula>, we have<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf106"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are the transition probabilities from state <inline-formula><mml:math id="inf107"><mml:mi>k</mml:mi></mml:math></inline-formula> into any other state according to <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:msubsup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are the state Gaussian distributions for state <inline-formula><mml:math id="inf110"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> (respectively <inline-formula><mml:math id="inf112"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf113"><mml:msubsup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> for <inline-formula><mml:math id="inf114"><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>); see <xref ref-type="bibr" rid="bib32">MacKay et al., 2003</xref>. Since the transition probabilities are Dirichlet-distributed and the state distributions are Gaussian distributed, KL divergence for those has a closed-form solution. Variables <inline-formula><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be computed numerically such that<disp-formula id="equ17">,<mml:math id="m17"><mml:mi>v</mml:mi><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:math></disp-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:math></disp-formula></p><p>To be able to use KL divergence as a kernel, we symmetrise the KL divergence matrix as<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This symmetrised KL divergence can be plugged into a radial basis function, analogous to the Gaussian kernels to obtain a similarity matrix <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula><disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The resulting KL similarity matrix can be used in the predictive model in a similar way as the kernels described above.</p></sec><sec id="s4-6"><title>Predictive model: Kernel ridge regression</title><p>Similarly to <xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>, we use kernel ridge regression (KRR) to predict demographic and behavioural variables from the different kernels (other kernel-based prediction models or classifiers such as a support vector machine are also possible). KRR is the kernelised version of ridge regression (<xref ref-type="bibr" rid="bib40">Saunders et al., 1998</xref>):<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mi>α</mml:mi><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>h</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf117"><mml:mi>α</mml:mi></mml:math></inline-formula> are the regression weights; <inline-formula><mml:math id="inf118"><mml:mi>h</mml:mi></mml:math></inline-formula> is the (number of subjects in test set by number of subjects in training set) kernel matrix between the subjects in the training set and the subjects in the test set; <inline-formula><mml:math id="inf119"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> are the predictions in the (out-of-sample) test set; <inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the number of subjects in the training set; and <inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the number of subjects in the test set. The regression weights α can be estimated using the kernels specified above as<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:mi>y</mml:mi><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>κ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf122"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a regularisation parameter that we can choose through cross-validation; <inline-formula><mml:math id="inf123"><mml:mi>I</mml:mi></mml:math></inline-formula> is the identity matrix; <inline-formula><mml:math id="inf124"><mml:mi>κ</mml:mi></mml:math></inline-formula> is the (<inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by <inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) kernel matrix of the subjects in the training set; and <inline-formula><mml:math id="inf127"><mml:mi>y</mml:mi></mml:math></inline-formula> are the training examples.</p><p>We use KRR to separately predict each of the 35 demographic and behavioural variables from each of the different methods, removing subjects with missing entries from the prediction. We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters <inline-formula><mml:math id="inf128"><mml:mi>λ</mml:mi></mml:math></inline-formula> (and <inline-formula><mml:math id="inf129"><mml:mi>τ</mml:mi></mml:math></inline-formula> in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1] and τ=[1/5, 1/3, 1/2, 1, 2, 3, 5]. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (<xref ref-type="bibr" rid="bib61">Winkler et al., 2015</xref>). Within the CV, we regressed out sex and head motion confounds, that is we estimated the regression coefficients for the confounds on the training set and applied them to the test set (<xref ref-type="bibr" rid="bib48">Snoek et al., 2019</xref>). We repeated the nested 10-fold CV 100 times, so that different combinations of subjects were randomly assigned to the folds at each new CV iteration to obtain a distribution of model performance values for each variable. This is to explicitly show how susceptible each model was to changes in the training folds, which we can take as a measure of the robustness of the estimators, as described below. We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.</p><sec id="s4-6-1"><title>Evaluation criteria</title><p>We evaluate the models in terms of two outcome criteria: prediction accuracy and reliability. For prediction accuracy, we used Pearson’s correlation coefficient <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> between the model-predicted values <inline-formula><mml:math id="inf131"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> and the actual values <inline-formula><mml:math id="inf132"><mml:mi>y</mml:mi></mml:math></inline-formula> of each variable and the coefficient of determination <inline-formula><mml:math id="inf133"><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. The second criterion, reliability, concerns two aspects: (i) that the model will never show excessively large errors for single subjects that could harm interpretation; and (ii) that the model’s accuracy will be consistent across random variations of the training set —in this case by using different (random) iterations for the CV folds. This is important if we want to interpret prediction errors for example in clinical contexts, which assumes that the error size of a model in a specific subject reflects something biologically meaningful, for example whether a certain disease causes the brain to ‘look’ older to a model than the actual age of the subject (<xref ref-type="bibr" rid="bib14">Denissen et al., 2022</xref>). Maximum errors inform us about single cases where a model (that may typically perform well) fails. The maximum absolute error (MAXAE) is the single largest error made by the regression model in each iteration, that is<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since the traits we predict are on different scales, the MAXAE is difficult to interpret, for example a MAXAE of 10 would be considered small if the true range of the variable we are predicting was 1,000, while it would be considered large if the true range of the variable was 1. To make the results comparable across the different traits, we therefore normalise the MAXAE by dividing it by the range of the respective variable. In this way, we obtain the NMAXAE:<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since the NMAXAEs follow extreme value distributions, it is more meaningful to consider the proportion of the values exceeding relevant thresholds than testing for differences in the means of these distributions (<xref ref-type="bibr" rid="bib23">Gumbel, 1958</xref>). We here consider the risk of large errors (NMAXAE &gt;10), very large errors (NMAXAE &gt;100), and extreme errors (NMAXAE &gt;1000) as the percentage of runs (across variables and CV iterations) where the model’s NMAXAE exceeds the given threshold. Since NMAXAE is normalised by the range of the actual variable, these thresholds correspond to one, two, and three orders of magnitude of the actual variable’s range. If we are predicting age, for instance, and the true ages of the subjects range from 25 years to 35 years, an NMAXAE of 1 would mean that the model’s least accurate prediction is off by 10 years, an NMAXAE of 10 would mean that the least accurate prediction is off by 100 years, an NMAXAE of 100 would be off by 1000 years, and an NMAXAE of 1,000 would be off by 10,000 years. A model that makes such large errors, even in single cases, would be unusable for interpretation. Our reliability criterion in terms of maximum errors is therefore that the risk of large errors (NMAXAE &gt;10) should be 0%.</p><p>For a model to be reliable, it should also be robust in the sense of susceptibility to changes in the training examples. Robustness is an important consideration in prediction studies, as it determines how reproducible a study or method is — which is often a shortcoming in neuroimaging-based prediction studies (<xref ref-type="bibr" rid="bib54">Varoquaux et al., 2017</xref>). We evaluated the robustness by iterating nested 10-fold CV 100 times for each variable, randomising the subjects in the folds at each iteration, so that the models would encounter a different combination of subjects in the training and test sets each time they are run. Looking at this range of 1000 accuracies (10 folds * 100 repetitions) for each variable, we can assess whether the model’s performance changes drastically depending on which combinations of subjects it encountered in the training phase, or whether the performance is the same regardless of the subjects encountered in the training phase. The former would be an example of a model that is susceptible to changes in training examples and the latter an example of a model that is robust. We here quantify robustness as the standard deviation (S.D.) of the prediction accuracy <italic>r</italic> across the 10 folds and 100 CV iterations of each variable, where a small mean S.D. over variables indicates higher robustness.</p><p>We test for significant differences in mean prediction accuracy between the methods using repeated k-fold cross-validation corrected t-tests (<xref ref-type="bibr" rid="bib9">Bouckaert and Frank, 2004</xref>). For results obtained through cross-validation with <inline-formula><mml:math id="inf134"><mml:mi>k</mml:mi></mml:math></inline-formula> folds and <inline-formula><mml:math id="inf135"><mml:mi>r</mml:mi></mml:math></inline-formula> repetitions, the corrected t-statistic is calculated as:<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>k</mml:mi><mml:mi>C</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>k</mml:mi><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>k</mml:mi><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf136"><mml:mi>x</mml:mi></mml:math></inline-formula> is the difference in prediction accuracy, <inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the number of samples in the training data, <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> the number of samples in the test data, and <inline-formula><mml:math id="inf139"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> the variance estimate. To test for significant differences in mean robustness (S.D. across CV repetitions), we use paired t-tests. All p-values are corrected across multiple comparisons using Benjamini-Hochberg correction to control the false discovery rate (FDR). For the main results, we separately compare the linear Fisher kernel to the other linear kernels, and the Gaussian Fisher kernel to the other Gaussian kernels, as well as to each other. We also compare the linear Fisher kernel to all time-averaged methods. Finally, to test for the effect of tangent space projection for the time-averaged FC prediction, we also compare the Ridge regression model to the Ridge Regression in Riemannian space. To test for effects of removing sets of features, we use the approach described above to compare the kernels constructed from the full feature sets to their versions where features were removed or reduced. Finally, to test for effects of training the HMM either on all subjects or only on the subjects that were later used as training set, we compare each kernel to the corresponding kernel constructed from HMM parameters, where training and test set were kept separate.</p></sec></sec><sec id="s4-7"><title>Models based on time-averaged FC features</title><p>To compare our approach’s performance to simpler methods that do not take dynamics into account, we compared them to seven different regression models based on time-averaged FC features. For each subject, time-averaged FC was computed as covariance between each pair of regions. The first time-averaged model is, analogous to the HMM-derived KL divergence model, a time-averaged KL divergence model. The model is described in detail in <xref ref-type="bibr" rid="bib57">Vidaurre et al., 2021</xref>. Briefly, we construct symmetrised KL divergence matrices of each subject’s time-averaged FC and predict from these matrices using the KRR pipeline described above. We refer to this model as time-averaged KL divergence. We next used a kernel constructed from a geodesic distance (i.e. a metric defined on the Riemannian manifold) of time-averaged covariance matrices. Specifically, we used a Gaussian kernel on the log-Euclidean distance, which is the Frobenius norm of the logarithm map of the time-averaged covariance matrices (<xref ref-type="bibr" rid="bib27">Jayasumana et al., 2013</xref>). We refer to this model as log-Euclidean. The other five time-averaged FC benchmark models do not involve kernels but predict directly from the features instead. Namely, we use two variants of a Ridge Regression and an Elastic Net model (<xref ref-type="bibr" rid="bib63">Zou and Hastie, 2005</xref>), one using the unwrapped time-averaged FC matrices as input (i.e. in Euclidean space), and one using the time-averaged FC matrices in Riemannian space, where covariance matrices are projected into tangent space (<xref ref-type="bibr" rid="bib4">Barachant et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Smith et al., 2013b</xref>), which we refer to as Ridge Reg. and Ridge Reg. (Riem), and Elastic Net and Elastic Net (Riem.), respectively. Finally, we compare our models to the approach taken in <xref ref-type="bibr" rid="bib37">Rosenberg et al., 2016</xref>, where relevant edges of the time-averaged FC matrices are first selected, and then used as predictors in a regression model. We refer to this model as Selected Edges. All time-averaged FC models are fitted using the same (nested) cross-validation strategy as described above (10-fold CV using the outer loop for model evaluation and the inner loop for model selection using grid-search for hyperparameter tuning, accounting for family structure in the dataset, and repeated 100 times with randomised folds).</p></sec><sec id="s4-8"><title>Simulations</title><sec id="s4-8-1"><title>Feature importance</title><p>To further understand the behaviour of the different kernels, we simulate data and compare the kernels’ ability to recover the ground truth. Specifically, we aim to understand which type of parameter change the kernels are most sensitive to. We generate timeseries for two groups of subjects, timeseries <inline-formula><mml:math id="inf140"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for group 1 and timeseries <inline-formula><mml:math id="inf141"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for group 2, from two separate HMMs with respective sets of parameters <inline-formula><mml:math id="inf142"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>:<disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We simulate timeseries from HMMs with these parameters through the generative process described in 4.2.</p><p>For the simulations, we use the group-level HMM of the real dataset with <inline-formula><mml:math id="inf144"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:math></inline-formula> states used in the main text as basis for group 1, that is <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We then manipulate two different types of parameters, the state means μ and the transition probabilities <inline-formula><mml:math id="inf146"><mml:mi>A</mml:mi></mml:math></inline-formula>, while keeping all remaining parameters the same between the groups. In the first case, we manipulate one state’s mean between the groups, that is:<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where μ<sup>2</sup> is obtained by simply adding a Gaussian noise vector to the state mean vector of one state:<disp-formula id="equ30"><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>φ</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf147"><mml:mi>φ</mml:mi></mml:math></inline-formula> is the Gaussian noise vector of size 1 x <inline-formula><mml:math id="inf148"><mml:mi>M</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf149"><mml:mi>M</mml:mi></mml:math></inline-formula> is the number of parcels, here 50, and <inline-formula><mml:math id="inf150"><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are the first rows (corresponding to the first state) of the state mean matrices for groups 1 and 2, respectively. We control the amplitude of <inline-formula><mml:math id="inf152"><mml:mi>φ</mml:mi></mml:math></inline-formula> so that the difference between <inline-formula><mml:math id="inf153"><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf154"><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is smaller than the minimum distance between any pair of states within one HMM. This is to ensure that the HMM recovers the difference between groups as difference in one state’s mean vector, rather than detecting a new state for group 2 that does not occur in group 1 and consequently collapsing two other states. Since the state means μ and the state covariances <inline-formula><mml:math id="inf155"><mml:mi>Σ</mml:mi></mml:math></inline-formula> are the first- and second-order parameters of the same respective distributions, it is to be expected that, although we only directly manipulate <italic>μ</italic>, <inline-formula><mml:math id="inf156"><mml:mi>Σ</mml:mi></mml:math></inline-formula> changes as well.</p><p>In the second case, we manipulate the transition probabilities for one state between the groups, while keeping all other parameters the same, i.e.:<disp-formula id="equ31"><mml:math id="m31"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:math></disp-formula><disp-formula id="equ32"><mml:math id="m32"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf157"><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is obtained by randomly permuting the probabilities of one state to transition into any of the other states, excluding self-transition probability:<disp-formula id="equ33"><mml:math id="m33"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf158"><mml:mi>p</mml:mi></mml:math></inline-formula> is a random permutation vector of size (<inline-formula><mml:math id="inf159"><mml:mi>K</mml:mi></mml:math></inline-formula>–1) x 1, <inline-formula><mml:math id="inf160"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are the self-transition probabilities of state 1, and <inline-formula><mml:math id="inf162"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>:</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>:</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are the probabilities of state 1 to transition into each of the other states.</p><p>We then concatenate the generated timeseries<disp-formula id="equ35"><mml:math id="m35"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>∗</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>containing 100 subjects for group 1 and 100 subjects for group 2, with 1200 timepoints and 50 parcels per subject. Note that we do not introduce any differences between subjects within a group, so that the between-group difference in HMM parameters should be the most dominant distinction and easily recoverable by the classifiers. We then apply the pipelines described above, running a new group-level HMM and constructing the linear versions of the naïve kernel, the naïve normalised kernel, and the Fisher kernel on these synthetic time series. The second case of simulations, manipulating the transition probabilities, only introduces a difference in few (<inline-formula><mml:math id="inf164"><mml:mi>K</mml:mi></mml:math></inline-formula> - 1) features and keeps the majority of the features the same between the groups, while the first case introduces a difference in a large number of features (<inline-formula><mml:math id="inf165"><mml:mi>M</mml:mi></mml:math></inline-formula> features directly, by changing one state’s mean vector, and an additional <italic>M</italic> x <italic>M</italic> features indirectly, as this state’s covariance matrix will also be affected). To account for this difference, we additionally construct a version of the kernels for the second case of simulations that includes only 𝜋 and <inline-formula><mml:math id="inf166"><mml:mi>A</mml:mi></mml:math></inline-formula>, removing the state parameters <italic>μ</italic> and <inline-formula><mml:math id="inf167"><mml:mi>Σ</mml:mi></mml:math></inline-formula>.</p><p>Finally, we use a support vector machine (SVM) in combination with the different kernels to recover the group labels and measure the error produced by each kernel. We repeat the whole process (generating timeseries, constructing kernels, and running the SVM) 10 times, in each iteration randomising on three levels: generating new random noise/permutation vectors to simulate the timeseries, randomly initialising the HMM parameters when fitting the group-level HMM to the simulated timeseries, and randomly assigning subjects to 10 CV folds in the SVM.</p></sec><sec id="s4-8-2"><title>Separating training and test set</title><p>We also simulate data to understand the sensitivity of the different kernels to separating training and test set before or after running the HMM. While separating training and test set is generally considered to be the gold standard to avoid leakage of information, this strategy may cause issues when the training and test set are very different from each other. In the simplest case, this would cause models to poorly generalise, which we aim to assess with the train-test split. However, in the case of the Fisher kernel, features are not independent but defined in reference to a group set of parameters. That means that, if we train the HMM only on the training set and then construct the Fisher kernel of a group of test subjects that are very different from the training set, this difference will be overrepresented in the kernel and likely overshadow other, more subtle differences between individuals.</p><p>Similar to the feature importance simulations, we generate time courses for two groups of subjects from two HMMs: timeseries <inline-formula><mml:math id="inf168"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for the training set and timeseries <inline-formula><mml:math id="inf169"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for the test set, from HMMs with parameters <inline-formula><mml:math id="inf170"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf171"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>:<disp-formula id="equ36"><mml:math id="m36"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ37"><mml:math id="m37"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We use the group-level HMM fit to real data as the basis for the simulations, so that <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the test set, we then add different levels of noise to one of the state’s mean vector to simulate varying degrees of between-group difference:<disp-formula id="equ38"><mml:math id="m38"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ39"><mml:math id="m39"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ40"><mml:math id="m40"><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>ϵ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf173"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> is the 1 x <inline-formula><mml:math id="inf174"><mml:mi>M</mml:mi></mml:math></inline-formula> Gaussian noise vector representing the heterogeneity between the training and the test set and <inline-formula><mml:math id="inf175"><mml:mi>b</mml:mi></mml:math></inline-formula> is a scalar controlling the noise level.</p><p>We randomly generate a continuous target variable, <inline-formula><mml:math id="inf176"><mml:mi>Y</mml:mi></mml:math></inline-formula>, which the models will aim to predict. We will simulate a single state’s mean to be correlated with this target variable, but we will add varying degrees of noise, which will make it gradually more difficult to predict the target. We do this by adding a fraction of the target variable and the noise to a second state mean vector for each subject in the training and the test set:<disp-formula id="equ41"><mml:math id="m41"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:math></disp-formula><disp-formula id="equ42"><mml:math id="m42"><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mn>10</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mi>φ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf177"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the 1 x <inline-formula><mml:math id="inf178"><mml:mi>M</mml:mi></mml:math></inline-formula> state mean vector for state <inline-formula><mml:math id="inf179"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf180"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the simulated target variable and <inline-formula><mml:math id="inf181"><mml:msup><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the 1 x <inline-formula><mml:math id="inf182"><mml:mi>M</mml:mi></mml:math></inline-formula> Gaussian noise vector, controlled by the scaler <inline-formula><mml:math id="inf183"><mml:mi>c</mml:mi></mml:math></inline-formula>. That means that the two effects (the target variable and the difference between training and test set) are represented in two separate states’ parameters. The models should be able to retrieve the variable of interest in one state while ignoring the between-group noise affecting another state.</p><p>We then simulate individual subjects’ timeseries for 50 training subjects and 50 test subjects, varying the values for <inline-formula><mml:math id="inf184"><mml:mi>b</mml:mi></mml:math></inline-formula> (controlling the level of between-group difference) in the range <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mi>c</mml:mi></mml:math></inline-formula> (controlling the level of noise on the target variable <inline-formula><mml:math id="inf187"><mml:mi>Y</mml:mi></mml:math></inline-formula>) in the range <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mn>1.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1.3</mml:mn><mml:mo>,</mml:mo><mml:mn>1.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1.7</mml:mn><mml:mo>,</mml:mo><mml:mn>1.9</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We then use two different training schemes: the first one where we train the HMM on all subjects (training and test set) before constructing the features (training: together) and the second one where we train the HMM only on the training subjects (training: separate). For the Fisher kernel, that means that the features for the subjects in the test set will be computed in reference to a group-level HMM which they are either part of (training: together) or not a part of (training: separate). We then use the same pipeline described above to predict the simulated target variables from the linear naïve kernel, the linear naïve normalised kernel, and the linear Fisher kernel.</p></sec></sec><sec id="s4-9"><title>Implementation</title><p>All code used in this paper, including scripts to reproduce the figures and additional application examples of the Fisher Kernel can be found in the repository <ext-link ext-link-type="uri" xlink:href="https://github.com/ahrends/FisherKernel">https://github.com/ahrends/FisherKernel</ext-link> (copy archived at <xref ref-type="bibr" rid="bib2">Ahrends, 2025</xref>). The HMM-Fisher kernel pipeline in Matlab is also publicly available within the repository of the HMM-MAR toolbox at <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/HMM-MAR">https://github.com/OHBA-analysis/HMM-MAR</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib35">OHBA Analysis Group, 2025</xref>). A Python-version of the Fisher kernel is also available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/glhmm">https://github.com/vidaurre/glhmm</ext-link> (copy archived at <xref ref-type="bibr" rid="bib58">Vidaurre, 2025a</xref>). Code for Ridge Regression and Elastic Net prediction from time-averaged FC features is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/NetsPredict/blob/master/nets_predict5.m">https://github.com/vidaurre/NetsPredict/blob/master/nets_predict5.m</ext-link> (copy archived at <xref ref-type="bibr" rid="bib59">Vidaurre, 2025b</xref>). The procedure for Selected Edges time-averaged FC prediction is described in detail in <xref ref-type="bibr" rid="bib44">Shen et al., 2017</xref> and code is provided at <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/bioimagesuite/behavioralprediction.m">https://www.nitrc.org/projects/bioimagesuite/behavioralprediction.m</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Supervision, Funding acquisition, Investigation, Methodology, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Overview of target variables (a) and summary of model performance (b).</title></caption><media xlink:href="elife-95125-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-95125-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data were provided by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>The WU-Minn Human Connectome Project</data-title><source>humanconnectome</source><pub-id pub-id-type="accession" xlink:href="https://db.humanconnectome.org">humanconnectome</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Data were provided by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University. DV is supported by a Novo Nordisk Foundation Emerging Investigator Fellowship (NNF19OC-0054895) and an ERC Starting Grant (ERC-StG-2019–850404). This research was funded in part by the Wellcome Trust (215573/Z/19/Z). For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. We thank Ben Griffin and Steve Smith for useful discussions and technical collaboration.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrends</surname><given-names>C</given-names></name><name><surname>Stevner</surname><given-names>A</given-names></name><name><surname>Pervaiz</surname><given-names>U</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Data and model considerations for estimating time-varying functional connectivity in fMRI</article-title><source>NeuroImage</source><volume>252</volume><elocation-id>119026</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119026</pub-id><pub-id pub-id-type="pmid">35217207</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ahrends</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>FisherKernel</data-title><version designator="swh:1:rev:7d3cf78b2c5a7d06c11b00ec95483e384bccc74c">swh:1:rev:7d3cf78b2c5a7d06c11b00ec95483e384bccc74c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:14ba504bc474c309ee3555fb1f3243aa3c37c8d2;origin=https://github.com/ahrends/FisherKernel;visit=swh:1:snp:052475922a5f84402210bbb49bd4eeb662ac9647;anchor=swh:1:rev:7d3cf78b2c5a7d06c11b00ec95483e384bccc74c">https://archive.softwareheritage.org/swh:1:dir:14ba504bc474c309ee3555fb1f3243aa3c37c8d2;origin=https://github.com/ahrends/FisherKernel;visit=swh:1:snp:052475922a5f84402210bbb49bd4eeb662ac9647;anchor=swh:1:rev:7d3cf78b2c5a7d06c11b00ec95483e384bccc74c</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>S</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Towards Stability of Dynamic FC Estimates in Neuroimaging and Electrophysiology: Solutions and Limits</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.18.524539</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barachant</surname><given-names>A</given-names></name><name><surname>Bonnet</surname><given-names>S</given-names></name><name><surname>Congedo</surname><given-names>M</given-names></name><name><surname>Jutten</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Classification of covariance matrices using a Riemannian-based kernel for BCI applications</article-title><source>Neurocomputing</source><volume>112</volume><fpage>172</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2012.12.039</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baum</surname><given-names>LE</given-names></name><name><surname>Petrie</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Statistical inference for probabilistic functions of finite state markov chains</article-title><source>The Annals of Mathematical Statistics</source><volume>37</volume><fpage>1554</fpage><lpage>1563</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177699147</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baum</surname><given-names>LE</given-names></name><name><surname>Eagon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology</article-title><source>Bulletin of the American Mathematical Society</source><volume>73</volume><fpage>360</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1090/S0002-9904-1967-11751-8</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Mackay</surname><given-names>CE</given-names></name><name><surname>Filippini</surname><given-names>N</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Group comparison of resting-state FMRI data using multi-subject ICA and dual regression</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S148</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)71511-3</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Modelling with independent components</article-title><source>NeuroImage</source><volume>62</volume><fpage>891</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.020</pub-id><pub-id pub-id-type="pmid">22369997</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bouckaert</surname><given-names>RR</given-names></name><name><surname>Frank</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Evaluating the replicability of significance tests for comparing learning algorithms</article-title><conf-name>Pacific-Asia Conference on Knowledge Discovery and Data</conf-name><pub-id pub-id-type="doi">10.1007/978-3-540-24775-3_3</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic models of large-scale brain activity</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>340</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1038/nn.4497</pub-id><pub-id pub-id-type="pmid">28230845</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Bagging predictors</article-title><source>Machine Learning</source><volume>24</volume><fpage>123</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1007/BF00058655</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>VD</given-names></name><name><surname>Miller</surname><given-names>R</given-names></name><name><surname>Pearlson</surname><given-names>G</given-names></name><name><surname>Adalı</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The chronnectome: time-varying connectivity networks as the next frontier in fMRI data discovery</article-title><source>Neuron</source><volume>84</volume><fpage>262</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.015</pub-id><pub-id pub-id-type="pmid">25374354</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>JH</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Predicting age using neuroimaging: innovative brain ageing biomarkers</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>681</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.10.001</pub-id><pub-id pub-id-type="pmid">29074032</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denissen</surname><given-names>S</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>De Cock</surname><given-names>A</given-names></name><name><surname>Costers</surname><given-names>L</given-names></name><name><surname>Baijot</surname><given-names>J</given-names></name><name><surname>Laton</surname><given-names>J</given-names></name><name><surname>Penner</surname><given-names>I-K</given-names></name><name><surname>Grothe</surname><given-names>M</given-names></name><name><surname>Kirsch</surname><given-names>M</given-names></name><name><surname>D’hooghe</surname><given-names>MB</given-names></name><name><surname>D’Haeseleer</surname><given-names>M</given-names></name><name><surname>Dive</surname><given-names>D</given-names></name><name><surname>De Mey</surname><given-names>J</given-names></name><name><surname>Van Schependom</surname><given-names>J</given-names></name><name><surname>Sima</surname><given-names>DM</given-names></name><name><surname>Nagels</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Brain age as a surrogate marker for cognitive performance in multiple sclerosis</article-title><source>European Journal of Neurology</source><volume>29</volume><fpage>3039</fpage><lpage>3049</lpage><pub-id pub-id-type="doi">10.1111/ene.15473</pub-id><pub-id pub-id-type="pmid">35737867</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Do</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Fast approximation of Kullback-Leibler distance for dependence trees and hidden Markov models</article-title><source>IEEE Signal Processing Letters</source><volume>10</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1109/LSP.2003.809034</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Kozynets</surname><given-names>O</given-names></name><name><surname>Sabbagh</surname><given-names>D</given-names></name><name><surname>Lemaître</surname><given-names>G</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Liem</surname><given-names>F</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Combining magnetoencephalography with magnetic resonance imaging enhances learning of surrogate-biomarkers</article-title><source>eLife</source><volume>9</volume><elocation-id>e54055</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54055</pub-id><pub-id pub-id-type="pmid">32423528</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>MD</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The human brain is intrinsically organized into dynamic, anticorrelated functional networks</article-title><source>PNAS</source><volume>102</volume><fpage>9673</fpage><lpage>9678</lpage><pub-id pub-id-type="doi">10.1073/pnas.0504136102</pub-id><pub-id pub-id-type="pmid">15976020</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Wilson</surname><given-names>JA</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Webster</surname><given-names>M</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title><source>NeuroImage</source><volume>80</volume><fpage>105</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id><pub-id pub-id-type="pmid">23668970</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gohil</surname><given-names>C</given-names></name><name><surname>Roberts</surname><given-names>E</given-names></name><name><surname>Timms</surname><given-names>R</given-names></name><name><surname>Skates</surname><given-names>A</given-names></name><name><surname>Higgins</surname><given-names>C</given-names></name><name><surname>Quinn</surname><given-names>A</given-names></name><name><surname>Pervaiz</surname><given-names>U</given-names></name><name><surname>van Amersfoort</surname><given-names>J</given-names></name><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Adaszewski</surname><given-names>S</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mixtures of large-scale dynamic functional brain network modes</article-title><source>NeuroImage</source><volume>263</volume><elocation-id>119595</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119595</pub-id><pub-id pub-id-type="pmid">36041643</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gönen</surname><given-names>M</given-names></name><name><surname>Alpaydın</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiple kernel learning algorithms</article-title><source>Journal of Machine Learning Research: JMLR</source><volume>12</volume><fpage>2211</fpage><lpage>2268</lpage><pub-id pub-id-type="doi">10.5555/1953048.2021071</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Sexton</surname><given-names>CE</given-names></name><name><surname>Zsoldos</surname><given-names>E</given-names></name><name><surname>Ebmeier</surname><given-names>KP</given-names></name><name><surname>Filippini</surname><given-names>N</given-names></name><name><surname>Mackay</surname><given-names>CE</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Baselli</surname><given-names>G</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>ICA-based artefact removal and accelerated fMRI acquisition for improved resting state network imaging</article-title><source>NeuroImage</source><volume>95</volume><fpage>232</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.034</pub-id><pub-id pub-id-type="pmid">24657355</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffin</surname><given-names>B</given-names></name><name><surname>Ahrends</surname><given-names>C</given-names></name><name><surname>Gohil</surname><given-names>C</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Stacking models of brain dynamics to improve prediction of subject traits in fMRI</article-title><source>Imaging Neuroscience</source><volume>2</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1162/imag_a_00267</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gumbel</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="1958">1958</year><source>Statistics of Extremes</source><publisher-name>Columbia University Press</publisher-name><pub-id pub-id-type="doi">10.7312/gumb92958</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchison</surname><given-names>RM</given-names></name><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Allen</surname><given-names>EA</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Della Penna</surname><given-names>S</given-names></name><name><surname>Duyn</surname><given-names>JH</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Gonzalez-Castillo</surname><given-names>J</given-names></name><name><surname>Handwerker</surname><given-names>DA</given-names></name><name><surname>Keilholz</surname><given-names>S</given-names></name><name><surname>Kiviniemi</surname><given-names>V</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>de Pasquale</surname><given-names>F</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Walter</surname><given-names>M</given-names></name><name><surname>Chang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic functional connectivity: promise, issues, and interpretations</article-title><source>NeuroImage</source><volume>80</volume><fpage>360</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.079</pub-id><pub-id pub-id-type="pmid">23707587</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jaakkola</surname><given-names>T</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Exploiting Generative Models in Discriminative Classifiers</source><publisher-name>NIPS</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jaakkola</surname><given-names>T</given-names></name><name><surname>Diekhans</surname><given-names>M</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Using the Fisher kernel method to detect remote protein homologies</article-title><conf-name>Proceedings. International Conference on Intelligent Systems for Molecular Biology</conf-name><fpage>149</fpage><lpage>158</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jayasumana</surname><given-names>S</given-names></name><name><surname>Hartley</surname><given-names>R</given-names></name><name><surname>Salzmann</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Harandi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</article-title><conf-name>Portland, OR, USA</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2013.17</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kapoor</surname><given-names>S</given-names></name><name><surname>Narayanan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Leakage and the reproducibility crisis in machine-learning-based science</article-title><source>Patterns</source><volume>4</volume><elocation-id>100804</elocation-id><pub-id pub-id-type="doi">10.1016/j.patter.2023.100804</pub-id><pub-id pub-id-type="pmid">37720327</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname><given-names>R</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interpreting temporal fluctuations in resting-state functional connectivity MRI</article-title><source>NeuroImage</source><volume>163</volume><fpage>437</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.012</pub-id><pub-id pub-id-type="pmid">28916180</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Orban</surname><given-names>C</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name><name><surname>Ge</surname><given-names>T</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Resting brain dynamics at different timescales capture distinct aspects of human behavior</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2317</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10317-7</pub-id><pub-id pub-id-type="pmid">31127095</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lurie</surname><given-names>DJ</given-names></name><name><surname>Kessler</surname><given-names>D</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Kheilholz</surname><given-names>S</given-names></name><name><surname>Kucyi</surname><given-names>A</given-names></name><name><surname>Liégeois</surname><given-names>R</given-names></name><name><surname>Lindquist</surname><given-names>MA</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Shine</surname><given-names>JM</given-names></name><name><surname>Thompson</surname><given-names>WH</given-names></name><name><surname>Bielczyk</surname><given-names>NZ</given-names></name><name><surname>Douw</surname><given-names>L</given-names></name><name><surname>Kraft</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>RL</given-names></name><name><surname>Muthuraman</surname><given-names>M</given-names></name><name><surname>Pasquini</surname><given-names>L</given-names></name><name><surname>Razi</surname><given-names>A</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Xie</surname><given-names>H</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Questions and controversies in the study of time-varying functional connectivity in resting fMRI</article-title><source>Network Neuroscience</source><volume>4</volume><fpage>30</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00116</pub-id><pub-id pub-id-type="pmid">32043043</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name><name><surname>Kay</surname><given-names>D</given-names></name><name><surname>Press</surname><given-names>CU</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Information Theory, Inference and Learning Algorithms</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marquand</surname><given-names>AF</given-names></name><name><surname>Rezek</surname><given-names>I</given-names></name><name><surname>Buitelaar</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Understanding heterogeneity in clinical cohorts using normative models: beyond case-control studies</article-title><source>Biological Psychiatry</source><volume>80</volume><fpage>552</fpage><lpage>561</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2015.12.023</pub-id><pub-id pub-id-type="pmid">26927419</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mwangi</surname><given-names>B</given-names></name><name><surname>Tian</surname><given-names>TS</given-names></name><name><surname>Soares</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A review of feature reduction techniques in neuroimaging</article-title><source>Neuroinformatics</source><volume>12</volume><fpage>229</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1007/s12021-013-9204-3</pub-id><pub-id pub-id-type="pmid">24013948</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><collab>OHBA Analysis Group</collab></person-group><year iso-8601-date="2025">2025</year><data-title>HMM-MAR</data-title><version designator="swh:1:rev:debca347a574c93ae2b2b3aba8eb1e2eaf717bfa">swh:1:rev:debca347a574c93ae2b2b3aba8eb1e2eaf717bfa</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a556d285ff06da9913df67c4572c332e2765b716;origin=https://github.com/OHBA-analysis/HMM-MAR;visit=swh:1:snp:94d40e631243886e5f0d62184c17c8b42562ec5d;anchor=swh:1:rev:debca347a574c93ae2b2b3aba8eb1e2eaf717bfa">https://archive.softwareheritage.org/swh:1:dir:a556d285ff06da9913df67c4572c332e2765b716;origin=https://github.com/OHBA-analysis/HMM-MAR;visit=swh:1:snp:94d40e631243886e5f0d62184c17c8b42562ec5d;anchor=swh:1:rev:debca347a574c93ae2b2b3aba8eb1e2eaf717bfa</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Huckins</surname><given-names>G</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Establishment of best practices for evidence for prediction: a review</article-title><source>JAMA Psychiatry</source><volume>77</volume><fpage>534</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1001/jamapsychiatry.2019.3671</pub-id><pub-id pub-id-type="pmid">31774490</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>MD</given-names></name><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Scheinost</surname><given-names>D</given-names></name><name><surname>Papademetris</surname><given-names>X</given-names></name><name><surname>Shen</surname><given-names>X</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A neuromarker of sustained attention from whole-brain functional connectivity</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>165</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1038/nn.4179</pub-id><pub-id pub-id-type="pmid">26595653</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>M</given-names></name><name><surname>Tejavibulya</surname><given-names>L</given-names></name><name><surname>Jiang</surname><given-names>R</given-names></name><name><surname>Noble</surname><given-names>S</given-names></name><name><surname>Scheinost</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Data leakage inflates prediction performance in connectome-based machine learning models</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>1829</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-46150-w</pub-id><pub-id pub-id-type="pmid">38418819</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Automatic denoising of functional MRI data: combining independent component analysis and hierarchical fusion of classifiers</article-title><source>NeuroImage</source><volume>90</volume><fpage>449</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.046</pub-id><pub-id pub-id-type="pmid">24389422</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Saunders</surname><given-names>C</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name><name><surname>Vovk</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Ridge regression learning algorithm in dual variables</article-title><conf-name>Appears in Proceedings of the 15th International Conference on Machine Learning, ICML</conf-name></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</source><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/4175.001.0001</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schouten</surname><given-names>TM</given-names></name><name><surname>Koini</surname><given-names>M</given-names></name><name><surname>de Vos</surname><given-names>F</given-names></name><name><surname>Seiler</surname><given-names>S</given-names></name><name><surname>van der Grond</surname><given-names>J</given-names></name><name><surname>Lechner</surname><given-names>A</given-names></name><name><surname>Hafkemeijer</surname><given-names>A</given-names></name><name><surname>Möller</surname><given-names>C</given-names></name><name><surname>Schmidt</surname><given-names>R</given-names></name><name><surname>de Rooij</surname><given-names>M</given-names></name><name><surname>Rombouts</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Combining anatomical, diffusion, and resting state functional magnetic resonance imaging for individual classification of mild and moderate Alzheimer’s disease</article-title><source>NeuroImage. Clinical</source><volume>11</volume><fpage>46</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2016.01.002</pub-id><pub-id pub-id-type="pmid">26909327</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name><name><surname>Cristianini</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Kernel Methods for Pattern Analysis</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511809682</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>X</given-names></name><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Scheinost</surname><given-names>D</given-names></name><name><surname>Rosenberg</surname><given-names>MD</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name><name><surname>Papademetris</surname><given-names>X</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Using connectome-based predictive modeling to predict individual behavior from brain connectivity</article-title><source>Nature Protocols</source><volume>12</volume><fpage>506</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1038/nprot.2016.178</pub-id><pub-id pub-id-type="pmid">28182017</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Bijsterbosch</surname><given-names>J</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Duff</surname><given-names>E</given-names></name><name><surname>Feinberg</surname><given-names>DA</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Harms</surname><given-names>MP</given-names></name><name><surname>Kelly</surname><given-names>M</given-names></name><name><surname>Laumann</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Power</surname><given-names>J</given-names></name><name><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013a</year><article-title>Resting-state fMRI in the Human Connectome Project</article-title><source>NeuroImage</source><volume>80</volume><fpage>144</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.039</pub-id><pub-id pub-id-type="pmid">23702415</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Functional connectomics from resting-state fMRI</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>666</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.09.016</pub-id><pub-id pub-id-type="pmid">24238796</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Estimation of brain age delta from brain imaging</article-title><source>NeuroImage</source><volume>200</volume><fpage>528</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.017</pub-id><pub-id pub-id-type="pmid">31201988</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snoek</surname><given-names>L</given-names></name><name><surname>Miletić</surname><given-names>S</given-names></name><name><surname>Scholte</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How to control for confounds in decoding analyses of neuroimaging data</article-title><source>NeuroImage</source><volume>184</volume><fpage>741</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.074</pub-id><pub-id pub-id-type="pmid">30268846</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Schlagenhauf</surname><given-names>F</given-names></name><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Raman</surname><given-names>S</given-names></name><name><surname>Aponte</surname><given-names>EA</given-names></name><name><surname>Brodersen</surname><given-names>KH</given-names></name><name><surname>Rigoux</surname><given-names>L</given-names></name><name><surname>Moran</surname><given-names>RJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Heinz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computational neuroimaging strategies for single patient predictions</article-title><source>NeuroImage</source><volume>145</volume><fpage>180</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.06.038</pub-id><pub-id pub-id-type="pmid">27346545</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaghari</surname><given-names>D</given-names></name><name><surname>Kabir</surname><given-names>E</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Late combination shows that MEG adds to MRI in classifying MCI versus controls</article-title><source>NeuroImage</source><volume>252</volume><elocation-id>119054</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119054</pub-id><pub-id pub-id-type="pmid">35247546</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning Discriminative Fisher Kernels</article-title><conf-name>Proceedings of the 28th International Conference on Machine Learning Bellevue, WA, USA</conf-name></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Barch</surname><given-names>D</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Bucholz</surname><given-names>R</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Curtiss</surname><given-names>SW</given-names></name><name><surname>Della Penna</surname><given-names>S</given-names></name><name><surname>Feinberg</surname><given-names>D</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Heath</surname><given-names>AC</given-names></name><name><surname>Larson-Prior</surname><given-names>L</given-names></name><name><surname>Marcus</surname><given-names>D</given-names></name><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Prior</surname><given-names>F</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2012">2012</year><article-title>The human connectome project: a data acquisition perspective</article-title><source>NeuroImage</source><volume>62</volume><fpage>2222</fpage><lpage>2231</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id><pub-id pub-id-type="pmid">22366334</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The wu-minn human connectome project: an overview</article-title><source>NeuroImage</source><volume>80</volume><fpage>62</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id><pub-id pub-id-type="pmid">23684880</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Raamana</surname><given-names>PR</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Hoyos-Idrobo</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>Y</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines</article-title><source>NeuroImage</source><volume>145</volume><fpage>166</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.038</pub-id><pub-id pub-id-type="pmid">27989847</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Baker</surname><given-names>AP</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name><name><surname>Tejero-Cantero</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spectrally resolved fast transient brain states in electrophysiological data</article-title><source>NeuroImage</source><volume>126</volume><fpage>81</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.047</pub-id><pub-id pub-id-type="pmid">26631815</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brain network dynamics are hierarchically organized in time</article-title><source>PNAS</source><volume>114</volume><fpage>12827</fpage><lpage>12832</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id><pub-id pub-id-type="pmid">29087305</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Llera</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Behavioural relevance of spontaneous, transient brain network interactions in fMRI</article-title><source>NeuroImage</source><volume>229</volume><elocation-id>117713</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117713</pub-id><pub-id pub-id-type="pmid">33421594</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2025">2025a</year><data-title>Vidaurre/glhmm</data-title><version designator="swh:1:rev:dca32fca720d9a7aba23d7f74fb86330e4ea4cc0">swh:1:rev:dca32fca720d9a7aba23d7f74fb86330e4ea4cc0</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:637e401a0335cfda4f3774a0dc569dfdfb1449c4;origin=https://github.com/vidaurre/glhmm;visit=swh:1:snp:06c01c2861121b83c78e05db423b9945532adc07;anchor=swh:1:rev:dca32fca720d9a7aba23d7f74fb86330e4ea4cc0">https://archive.softwareheritage.org/swh:1:dir:637e401a0335cfda4f3774a0dc569dfdfb1449c4;origin=https://github.com/vidaurre/glhmm;visit=swh:1:snp:06c01c2861121b83c78e05db423b9945532adc07;anchor=swh:1:rev:dca32fca720d9a7aba23d7f74fb86330e4ea4cc0</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2025">2025b</year><data-title>NetsPredict</data-title><version designator="swh:1:rev:743c31c16788b3f552e2deed289215ffc29b5f0b">swh:1:rev:743c31c16788b3f552e2deed289215ffc29b5f0b</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e43d38785f43be71c0ba28c936b98f36e20746cf;origin=https://github.com/vidaurre/NetsPredict;visit=swh:1:snp:dc07000c9979a553d19b3ab02f674ca43127f420;anchor=swh:1:rev:743c31c16788b3f552e2deed289215ffc29b5f0b">https://archive.softwareheritage.org/swh:1:dir:e43d38785f43be71c0ba28c936b98f36e20746cf;origin=https://github.com/vidaurre/NetsPredict;visit=swh:1:snp:dc07000c9979a553d19b3ab02f674ca43127f420;anchor=swh:1:rev:743c31c16788b3f552e2deed289215ffc29b5f0b</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>J</given-names></name><name><surname>Varol</surname><given-names>E</given-names></name><name><surname>Sotiras</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Chand</surname><given-names>GB</given-names></name><name><surname>Erus</surname><given-names>G</given-names></name><name><surname>Shou</surname><given-names>H</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Hwang</surname><given-names>G</given-names></name><name><surname>Dwyer</surname><given-names>DB</given-names></name><name><surname>Pigoni</surname><given-names>A</given-names></name><name><surname>Dazzan</surname><given-names>P</given-names></name><name><surname>Kahn</surname><given-names>RS</given-names></name><name><surname>Schnack</surname><given-names>HG</given-names></name><name><surname>Zanetti</surname><given-names>MV</given-names></name><name><surname>Meisenzahl</surname><given-names>E</given-names></name><name><surname>Busatto</surname><given-names>GF</given-names></name><name><surname>Crespo-Facorro</surname><given-names>B</given-names></name><name><surname>Rafael</surname><given-names>RG</given-names></name><name><surname>Pantelis</surname><given-names>C</given-names></name><name><surname>Wood</surname><given-names>SJ</given-names></name><name><surname>Zhuo</surname><given-names>C</given-names></name><name><surname>Shinohara</surname><given-names>RT</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name><name><surname>Gur</surname><given-names>RC</given-names></name><name><surname>Gur</surname><given-names>RE</given-names></name><name><surname>Satterthwaite</surname><given-names>TD</given-names></name><name><surname>Koutsouleris</surname><given-names>N</given-names></name><name><surname>Wolf</surname><given-names>DH</given-names></name><name><surname>Davatzikos</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multi-scale semi-supervised clustering of brain images: Deriving disease subtypes</article-title><source>Medical Image Analysis</source><volume>75</volume><elocation-id>102304</elocation-id><pub-id pub-id-type="doi">10.1016/j.media.2021.102304</pub-id><pub-id pub-id-type="pmid">34818611</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multi-level block permutation</article-title><source>NeuroImage</source><volume>123</volume><fpage>253</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.092</pub-id><pub-id pub-id-type="pmid">26074200</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfers</surname><given-names>T</given-names></name><name><surname>Buitelaar</surname><given-names>JK</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Franke</surname><given-names>B</given-names></name><name><surname>Marquand</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>From estimating activation locality to predicting disorder: A review of pattern recognition for neuroimaging-based psychiatric diagnostics</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>57</volume><fpage>328</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.08.001</pub-id><pub-id pub-id-type="pmid">26254595</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>H</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Regularization and variable selection via the elastic net</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>67</volume><fpage>301</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95125.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Marquand</surname><given-names>Andre F</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Radboud University Nijmegen</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study combines the use of Fisher Kernels with Hidden Markov models aiming to improve brain-behaviour prediction. The evidence supporting the authors' conclusions is <bold>compelling</bold>, comparing brain-behaviour prediction accuracies across a range of different traits, including out of sample assessment. This work is timely and will be of interest to neuroscientists working on functional connectivity for brain-behaviour association.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95125.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors attempt to validate Fisher Kernels on the top of HMM as a way to better describe human brain dynamics at resting-state. The objective criterion was the better prediction of the proposed pipeline of the individual traits.</p><p>Comments on revisions:</p><p>The authors addressed adequately all my comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95125.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work, the authors use a Hidden Markov Model (HMM) to describe dynamic connectivity and amplitude patterns in fMRI data, and propose to integrate these features with the Fisher kernel to improve the prediction of individual traits. The approach is tested using a large sample of healthy young adults from the Human Connectome Project. The HMM-Fisher Kernel approach was shown to achieve higher prediction accuracy with lower variance on many individual traits compared to alternate kernels and measures of static connectivity. As an additional finding, the authors demonstrate that parameters of the HMM state matrix may be more informative in predicting behavioral/cognitive variables in this data compared to state-transition probabilities.</p><p>Comments on revisions:</p><p>The authors have now addressed my comments, and I believe this work will be an interesting contribution to the literature.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95125.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ahrends</surname><given-names>Christine</given-names></name><role specific-use="author">Author</role><aff><institution>Aarhus University</institution><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff></contrib><contrib contrib-type="author"><name><surname>Woolrich</surname><given-names>Mark W</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Vidaurre</surname><given-names>Diego</given-names></name><role specific-use="author">Author</role><aff><institution>Aarhus University</institution><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The authors attempt to validate Fisher Kernels on the top of HMM as a way to better describe human brain dynamics at resting state. The objective criterion was the better prediction of the proposed pipeline of the individual traits.</p><p>Strengths:</p><p>The authors analyzed rs-fMRI dataset from the HCP providing results also from other kernels.</p><p>The authors also provided findings from simulation data.</p><p>Weaknesses:</p><p>(1) The authors should explain in detail how they applied cross-validation across the dataset for both optimization of parameters, and also for cross-validation of the models to predict individual traits.</p></disp-quote><p>Indeed, there were details about the cross-validation for hyperparameter tuning and prediction missing. This problem was also raised by Reviewer #2. We have now rephrased this section in 4.4 and added details: ll. 804-813:</p><p>“We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters λ (and τ in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (Winkler et al., 2015). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (Snoek et al., 2019).“ and ll. 818-820: “We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.”</p><disp-quote content-type="editor-comment"><p>(2) They discussed throughout the paper that their proposed (HMM+Fisher) kernel approach outperformed dynamic functional connectivity (dFC). However, they compared the proposed methodology with just static FC.</p></disp-quote><p>We would like to clarify that the HMM is itself a method for estimating dynamic (or time-varying) FC, just like the sliding window approach, see also Vidaurre, 2024 (https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00363/124983) for an overview of terminology.</p><p>See also our response to Q3.</p><disp-quote content-type="editor-comment"><p>(3) If the authors wanted to claim that their methodology is better than dFC, then they have to demonstrate results based on dFC with the trivial sliding window approach.</p></disp-quote><p>We would like to be clear that we do not claim in the manuscript that our method outperforms other dynamic functional connectivity (dFC) approaches, such as sliding window FC. We have now made changes to the manuscript to make this clearer.</p><p>First, we have clarified our use of the term “brain dynamics” to signify “time-varying amplitude and functional connectivity patterns” in this context, as Reviewer #2 raised the point that the former term is ambiguous (ll.33-35: “One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain.”).</p><p>Second, our focus is on our method being a way of using dFC for predictive modelling, since there currently is no widely accepted way of doing this. One reason why dFC is not usually considered in prediction studies is that it is mathematically not trivial how to use the parameters from estimators of dynamic FC for a prediction. This includes the sliding window approach. We do not aim at comparing across different dFC estimators in this paper. To make these points clearer, we have revised the introduction to now say:</p><p>Ll. 39-50:</p><p>“One reason why brain dynamics are not usually considered in this context pertains to their representation: They are represented using models of varying complexity that are estimated from modalities such as functional MRI or MEG. Although there exists a variety of methods for estimating time-varying or dynamic FC (Lurie et al., 2019), like the commonly used sliding-window approach, there is currently no widely accepted way of using them for prediction problems. This is because these models are usually parametrised by a high number of parameters with complex mathematical relationships between the parameters that reflect the model assumptions. How to leverage these parameters for prediction is currently an open question.</p><p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information from generative models that do not assume any knowledge of task timings. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, in this case acquired during wakeful rest, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM), which is a probabilistic generative model of time-varying amplitude and functional connectivity (FC) dynamics (Vidaurre et al., 2017).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The manuscript presents a valuable investigation into the use of Fisher Kernels for extracting representations from temporal models of brain activity, with the aim of improving regression and classification applications. The authors provide solid evidence through extensive benchmarks and simulations that demonstrate the potential of Fisher Kernels to enhance the accuracy and robustness of regression and classification performance in the context of functional magnetic resonance imaging (fMRI) data. This is an important achievement for the neuroimaging community interested in predictive modeling from brain dynamics and, in particular, state-space models.</p><p>Strengths:</p><p>(1) The study's main contribution is the innovative application of Fisher Kernels to temporal brain activity models, which represents a valuable advancement in the field of human cognitive neuroimaging.</p><p>(2) The evidence presented is solid, supported by extensive benchmarks that showcase the method's effectiveness in various scenarios.</p><p>(3) Model inspection and simulations provide important insights into the nature of the signal picked up by the method, highlighting the importance of state rather than transition probabilities.</p><p>(4) The documentation and description of the methods are solid including sufficient mathematical details and availability of source code, ensuring that the study can be replicated and extended by other researchers.</p><p>Weaknesses:</p><p>(1) The generalizability of the findings is currently limited to the young and healthy population represented in the Human Connectome Project (HCP) dataset. The potential of the method for other populations and modalities remains to be investigated.</p></disp-quote><p>As suggested by the reviewer, we have added a limitations paragraph and included a statement about the dataset: Ll. 477-481: “The fMRI dataset we used (HCP 1200 Young Adult) is a large sample taken from a healthy, young population, and it remains to be shown how our findings generalise to other datasets, e.g. other modalities such as EEG/MEG, clinical data, older populations, different data quality, or smaller sample sizes both in terms of the number of participants and the scanning duration”.</p><p>We would like to emphasise that this is a methodological contribution, rather than a basic science investigation about cognition and brain-behaviour associations. Therefore, the method would be equally usable on different populations, even if the results vary.</p><disp-quote content-type="editor-comment"><p>(2) The possibility of positivity bias in the HMM, due to the use of a population model before cross-validation, needs to be addressed to confirm the robustness of the results.</p></disp-quote><p>As pointed out by both Reviewers #2 and #3, we did not separate subjects into training and test set before fitting the HMM. To address this issue, we have now repeated the predictions for HMMs fit only to the training subjects. We show that this has no effect on the results. Since this question has consequences for the Fisher kernel, we have also added simulations showing how the different kernels react to increasing heterogeneity between training and test set. These new results are added as results section 2.4 (ll. 376-423).</p><disp-quote content-type="editor-comment"><p>(3) The statistical significance testing might be compromised by incorrect assumptions about the independence between cross-validation distributions, which warrants further examination or clearer documentation.</p></disp-quote><p>We have now replaced the significance testing with repeated k-fold cross-validated corrected tests. Note that this required re-running the models to be able to test differences in accuracies on the level of individual folds, resulting in different plots throughout the manuscript and different statistical results. This does not, however, change the main conclusions of our manuscript.</p><disp-quote content-type="editor-comment"><p>(4) The inclusion of the R^2 score, sensitive to scale, would provide a more comprehensive understanding of the method's performance, as the Pearson correlation coefficient alone is not standard in machine learning and may not be sufficient (even if it is common practice in applied machine learning studies in human neuroimaging).</p></disp-quote><p>We have now added the coefficient of determination to the results figures.</p><disp-quote content-type="editor-comment"><p>(5) The process for hyperparameter tuning is not clearly documented in the methods section, both for kernel methods and the elastic net.</p></disp-quote><p>As mentioned above in the response to Reviewer #1, we have now added details about hyperparameter tuning for the kernel methods and the non-kernelised static FC regression models (see also Reviewer #1 comment 1): Ll.804-813: “We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters (and in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <inline-formula><mml:math id="sa3m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (Winkler et al., 2015). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (Snoek et al., 2019).” and ll. 818-820: “We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.”, as well as ll.913-917: “All time-averaged FC models are fitted using the same (nested) cross-validation strategy as described above (10-fold CV using the outer loop for model evaluation and the inner loop for model selection using grid-search for hyperparameter tuning, accounting for family structure in the dataset, and repeated 100 times with randomised folds).”</p><disp-quote content-type="editor-comment"><p>(6) For the time-averaged benchmarks, a comparison with kernel methods using metrics defined on the Riemannian SPD manifold, such as employing the Frobenius norm of the logarithm map within a Gaussian kernel, would strengthen the analysis, cf. Jayasumana (https://arxiv.org/abs/1412.4172) Table 1, log-euclidean metric.</p></disp-quote><p>We have now added the log-Euclidean Gaussian kernel proposed by the reviewer to the model comparisons. The additional model does not change our conclusions.</p><disp-quote content-type="editor-comment"><p>(7) A more nuanced and explicit discussion of the limitations, including the reliance on HCP data, lack of clinical focus, and the context of tasks for which performance is expected to be on the low end (e.g. cognitive scores), is crucial for framing the findings within the appropriate context.</p></disp-quote><p>We have now revised the discussion section and added an explicit limitations paragraph: Ll. 475-484:</p><p>“We here aimed to show the potential of the HMM-Fisher kernel approach to leverage information from patterns of brain dynamics to predict individual traits in an example fMRI dataset as well as simulated data. The fMRI dataset we used (HCP 1200 Young Adult) is a large sample taken from a healthy, young population, and it remains to be shown how the exhibited performance generalises to other datasets, e.g. other modalities such as EEG/MEG, clinical data, older populations, different data quality, or smaller sample sizes both in terms of the number of participants and the scanning duration. Additionally, we only tested our approach for the prediction of a specific set of demographic items and cognitive scores; it may be interesting to test the framework in also on clinical variables, such as the presence of a disease or the response to pharmacological treatment.”</p><disp-quote content-type="editor-comment"><p>(8) While further benchmarks could enhance the study, the authors should provide a critical appraisal of the current findings and outline directions for future research, considering the scope and budget constraints of the work.</p></disp-quote><p>In addition to the new limitations paragraph (see previous comment), we have now rephrased our interpretation of the results and extended the outlook paragraph: Ll. 485-507:</p><p>“There is growing interest in combining different data types or modalities, such as structural, static, and dynamic measures, to predict phenotypes (Engemann et al., 2020; Schouten et al., 2016). While directly combining the features from each modality can be problematic, modality-specific kernels, such as the Fisher kernel for time-varying amplitude and/or FC, can be easily combined using approaches such as stacking (Breiman, 1996) or Multi Kernel Learning (MKL) (Gönen &amp; Alpaydın, 2011). MKL can improve prediction accuracy of multimodal studies (Vaghari et al., 2022), and stacking has recently been shown to be a useful framework for combining static and time-varying FC predictions (Griffin et al., 2024). A detailed comparison of different multimodal prediction strategies including kernels for time-varying amplitude/FC may may be the focus of future work.</p><p>In a clinical context, while there are nowadays highly accurate biomarkers and prognostics for many diseases, others, such as psychiatric diseases, remain poorly understood, diagnosed, and treated. Here, improving the description of individual variability in brain measures may have potential benefits for a variety of clinical goals, e.g., to diagnose or predict individual patients’ outcomes, find biomarkers, or to deepen our understanding of changes in the brain related to treatment responses like drugs or non-pharmacological therapies (Marquand et al., 2016; Stephan et al., 2017; Wen et al., 2022; Wolfers et al., 2015). However, the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (Vidaurre et al., 2016).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>In this work, the authors use a Hidden Markov Model (HMM) to describe dynamic connectivity and amplitude patterns in fMRI data, and propose to integrate these features with the Fisher Kernel to improve the prediction of individual traits. The approach is tested using a large sample of healthy young adults from the Human Connectome Project. The HMM-Fisher Kernel approach was shown to achieve higher prediction accuracy with lower variance on many individual traits compared to alternate kernels and measures of static connectivity. As an additional finding, the authors demonstrate that parameters of the HMM state matrix may be more informative in predicting behavioral/cognitive variables in this data compared to state-transition probabilities.</p><p>Strengths:</p><p>- Overall, this work helps to address the timely challenge of how to leverage high-dimensional dynamic features to describe brain activity in individuals.</p><p>- The idea to use a Fisher Kernel seems novel and suitable in this context.</p><p>- Detailed comparisons are carried out across the set of individual traits, as well as across models with alternate kernels and features.</p><p>- The paper is well-written and clear, and the analysis is thorough.</p><p>Potential weaknesses:</p><p>- One conclusion of the paper is that the Fisher Kernel &quot;predicts more accurately than other methods&quot; (Section 2.1 heading). I was not certain this conclusion is fully justified by the data presented, as it appears that certain individual traits may be better predicted by other approaches (e.g., as shown in Figure 3) and I found it hard to tell if certain pairwise comparisons were performed -- was the linear Fisher Kernel significantly better than the linear Naive normalized kernel, for example?</p></disp-quote><p>We have revised the abstract and the discussion to state the results more appropriately. For instance, we changed the relevant section in the abstract to (ll. 24-26):</p><p>“We show here, in fMRI data, that the HMM-Fisher kernel approach is accurate and reliable. We compare the Fisher kernel to other prediction methods, both time-varying and time-averaged functional connectivity-based models.”,</p><p>and in the discussion, removing the sentence</p><p>“resulting in better generalisability and interpretability compared to other methods”,</p><p>and adding (given the revised statistical results) ll. 435-436:</p><p>“though most comparisons were not statistically significant given the narrow margin for improvements.”</p><p>In conjunction with the new statistical approach (see Reviewer #2, comment 3), we have now streamlined the comparisons. We explained which comparisons were performed in the methods ll.880-890:</p><p>“For the main results, we separately compare the linear Fisher kernel to the other linear kernels, and the Gaussian Fisher kernel to the other Gaussian kernels, as well as to each other. We also compare the linear Fisher kernel to all time-averaged methods. Finally, to test for the effect of tangent space projection for the time-averaged FC prediction, we also compare the Ridge regression model to the Ridge Regression in Riemannian space. To test for effects of removing sets of features, we use the approach described above to compare the kernels constructed from the full feature sets to their versions where features were removed or reduced. Finally, to test for effects of training the HMM either on all subjects or only on the subjects that were later used as training set, we compare each kernel to the corresponding kernel constructed from HMM parameters, where training and test set were kept separate.“</p><p>Model performance evaluation is done on the level of all predictions (i.e., across target variables, CV folds, and CV iterations) rather than for each of the target variables separately. That means different best-performing methods depending on the target variables are to be expected.</p><disp-quote content-type="editor-comment"><p>- While 10-fold cross-validation is used for behavioral prediction, it appears that data from the entire set of subjects is concatenated to produce the initial group-level HMM estimates (which are then customized to individuals). I wonder if this procedure could introduce some shared information between CV training and test sets. This may be a minor issue when comparing the HMM-based models to one another, but it may be more important when comparing with other models such as those based on time-averaged connectivity, which are calculated separately for train/test partitions (if I understood correctly).</p></disp-quote><p>The lack of separation between training and test set before fitting the HMM was also pointed out by Reviewer #2. We are addressing this issue in the new Results section 2.4 (see also our response to Reviewer #2, comment 2).</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p>The individual public reviews all indicate the merits of the study, however, they also highlight relatively consistent questions or issues that ought to be addressed. Most significantly, the authors ought to provide greater clarity surrounding the use of the cross-validation procedures they employ, and the use of a common atlas derived outside the cross-validation loop. Also, the authors should ensure that the statistical testing procedures they employ accommodate the dependencies induced between folds by the cross-validation procedure and give care to ensuring that the conclusions they make are fully supported by the data and statistical tests they present.</p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Overall, the study is interesting but demands further improvements. Below, I summarize my comments:</p><p>(1) The authors should explain in detail how they applied cross-validation across the dataset for both optimization of parameters, and also for cross-validation of the models to predict individual traits.</p><p>How did you split the dataset for both parameters optimization, and for the CV of the prediction of behavioral traits?</p><p>A review and a summary of various CVs that have been applied on the same dataset should be applied.</p></disp-quote><p>We apologise for the oversight and have now added more details to the CV section of the methods, see our response to Reviewer #1 comment 1:</p><p>In ll. 804-813:</p><p>“We used k-fold nested cross-validation (CV) to select and evaluate the models. We used 10 folds for both the outer loop (used to train and test the model) and the inner loop (used to select the optimal hyperparameters) such that 90% were used for training and 10% for testing. The optimal hyperparameters (and in the case of the Gaussian kernels) were selected using grid-search from the vectors λ=[0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1] and <inline-formula><mml:math id="sa3m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. In both the outer and the inner loop, we accounted for family structure in the HCP dataset so that subjects from the same family were never split across folds (Winkler et al., 2015). Within the CV, we regressed out sex and head motion confounds, i.e., we estimated the regression coefficients for the confounds on the training set and applied them to the test set (Snoek et al., 2019).“ and ll. 818-820: “We generated the 100 random repetitions of the 10 outer CV folds once, and then used them for training and prediction of all methods, so that all methods were fit to the same partitions.”</p><disp-quote content-type="editor-comment"><p>(2) The authors should explain in more detail how they applied ICA-based parcellation at the group-level.</p><p>A. Did you apply it across the whole group? If yes, then this is problematic since it rejects the CV approach. It should be applied within the folds.</p><p>B. How did you define the representative time-source per ROI?</p></disp-quote><p>A: How group ICA was applied was stated in the Methods section (4.1 HCP imaging and behavioural data), ll. 543-548:</p><p>“The parcellation was estimated from the data using multi-session spatial ICA on the temporally concatenated data from all subjects.”</p><p>We have now added a disclaimer about the divide between training and test set:</p><p>“Note that this means that there is no strict divide between the subjects used for training and the subjects for testing the later predictive models, so that there is potential for leakage of information between training and test set. However, since this step does not concern the target variable, but only the preprocessing of the predictors, the effect can be expected to be minimal (Rosenblatt et al., 2024).”</p><p>We understand that in order to make sure we avoid data leakage, it would be desirable to estimate and apply group ICA separately for the folds, but the computational load of this would be well beyond the constraints of this particular work, where we have instead used the parcellation provided by the HCP consortium.</p><p>B: This was also stated in 4.1, ll. 554-559: “Timecourses were extracted using dual regression (Beckmann et al., 2009), where group-level components are regressed onto each subject’s fMRI data to obtain subject-specific versions of the parcels and their timecourses. We normalised the timecourses of each subject to ensure that the model of brain dynamics and, crucially, the kernels were not driven by (averaged) amplitude and variance differences between subjects.”</p><disp-quote content-type="editor-comment"><p>(3) The authors discussed throughout the paper that their proposed (HMM+Fisher) kernel approach outperformed dynamic functional connectivity (dFC). However, they compared the proposed methodology with just static FC.</p><p>A. The authors didn't explain how static and dFC have been applied.</p><p>B. If the authors wanted to claim that their methodology is better than dFC, then they have to demonstrate results based on dFC with the trivial sliding window approach.</p><p>C. Moreover, the static FC networks have been constructed by concatenating time samples that belong to the same state across the time course of resting-state activity.</p><p>So, it's HMM-informed static FC analysis, which is problematic since it's derived from HMM applied over the brain dynamics.</p><p>I don't agree that connectivity is derived exclusively from the clustering of human brain dynamics!</p><p>D. A static approach of using the whole time course, and a dFC following the trivial sliding-window approach should be adopted and presented for comparison with (HMM+Fisher) kernel.</p></disp-quote><p>We do not intend to claim our manuscript that our method outperforms other methods for doing dynamic FC. Indeed, we would like to be clear that the HMM itself is a method for capturing dynamic FC. Please see our responses to public review comments 2 and 3 by reviewer #1, copied below, which is intended to clear up this misunderstanding:</p><p>We would like to clarify that the HMM is itself a method for estimating dynamic (or time-varying) FC, just like the sliding window approach, see also Vidaurre, 2024 (https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00363/124983) for an overview of terminology.</p><p>We would like to be clear that we do not claim in the manuscript that our method outperforms other dynamic functional connectivity (dFC) approaches, such as sliding window FC. We have now made changes to the manuscript to make this clearer.</p><p>First, we have clarified our use of the term “brain dynamics” to signify “time-varying amplitude and functional connectivity patterns” in this context, as Reviewer #2 raised the point that the former term is ambiguous.</p><p>Second, our focus is on our method being a way of using dFC for predictive modelling, since there currently is no widely accepted way of doing this. One reason why dFC is not usually considered in prediction studies is that it is mathematically not trivial how to use the parameters from estimators of dynamic FC for a prediction. This includes the sliding window approach. We do not aim at comparing across different dFC estimators in this paper. To make these points clearer, we have revised the introduction to now say:</p><p>Ll. 39-50:</p><p>“One reason why brain dynamics are not usually considered in this context pertains to their representation: They are represented using models of varying complexity that are estimated from modalities such as functional MRI or MEG. Although there exists a variety of methods for estimating time-varying or dynamic FC (Lurie et al., 2019), like the commonly used sliding-window approach, there is currently no widely accepted way of using them for prediction problems. This is because these models are usually parametrised by a high number of parameters with complex mathematical relationships between the parameters that reflect the model assumptions. How to leverage these parameters for prediction is currently an open question.</p><p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information from generative models that do not assume any knowledge of task timings. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, in this case acquired during wakeful rest, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM), which is a probabilistic generative model of time-varying amplitude and functional connectivity (FC) dynamics (Vidaurre et al., 2017).”</p><p>To the additional points raised here:</p><p>A: How static and dynamic FC have been estimated is explicitly stated in the relevant Methods sections 4.2 (The Hidden Markov Model), which explains the details of using the HMM to estimate dynamic functional connectivity; and 4.5 (Regression models based on time-averaged FC features), which explains how static FC was computed.</p><p>B: We are not making this claim. We have now modified the Introduction to avoid further misunderstandings, as per ll. 33-36: “One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain.”</p><p>C: This is not how static FC networks were constructed; we apologise for the confusion. We also do not perform any kind of clustering. The only “HMM-informed static FC analysis” is the static FC KL divergence model to allow for a more direct comparison with the time-varying FC KL divergence model, but we have included several other static FC models (log-Euclidean, Ridge regression, Ridge regression Riem., Elastic Net, Elastic Net Riem., and Selected Edges), which do not use HMMs. This is explained in Methods section 4.5.</p><p>D: As explained above, we have included four (five in the revised manuscript) static approaches using the whole time course, and we do not claim that our method outperforms other dynamic FC models. We also disagree that using the sliding window approach for predictive modelling is trivial, as explained in the introduction of the manuscript and under public review comment 3.</p><disp-quote content-type="editor-comment"><p>(4) Did you correct for multiple comparisons across the various statistical tests?</p></disp-quote><p>All statistical comparisons have been corrected for multiple comparisons. Please find the relevant text in Methods section 4.4.1.</p><disp-quote content-type="editor-comment"><p>(5) Do we expect that behavioral traits are encapsulated in resting-state human brain dynamics, and on which brain areas mostly? Please, elaborate on this.</p></disp-quote><p>While this is certainly an interesting question, our paper is a methodological contribution about how to predict from models of brain dynamics, rather than a basic science study about the relation between resting-state brain dynamics and behaviour. The biological aspects and interpretation of the specific brain-behaviour associations are a secondary point and out of scope for this paper. Our approach uses whole-brain dynamics, which does not require selecting brain areas of interest.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Beyond the general principles included in the public review, here are a few additional pointers to minor issues that I would wish to see addressed.</p><p>Introduction:</p><p>- The term &quot;brain dynamics&quot; encompasses a broad spectrum of phenomena, not limited to those captured by state-space models. It includes various measures such as time-averaged connectivity and mean EEG power within specific frequency bands. To ensure clarity and relevance for a diverse readership, it would be beneficial to adopt a more inclusive and balanced approach to the terminology used.</p></disp-quote><p>The reviewer rightly points out the ambiguity of the term “brain dynamics”, which we use in the interest of readability. The HMM is one of several possible descriptions of brain dynamics. We have now included a statement early in the introduction to narrow this down:</p><p>Ll. 32-35:</p><p>“… the patterns in which brain activity unfolds over time, i.e., brain dynamics. One way of describing brain dynamics are state-space models, which allow capturing recurring patterns of activity and functional connectivity (FC) across the whole brain.”</p><p>And ll. 503-507:</p><p>“Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, as one of many possible descriptions of brain dynamics, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (Vidaurre et al., 2016).”</p><disp-quote content-type="editor-comment"><p>Figures:</p><p>- The font sizes across the figures, particularly in subpanels 2B and 2C, are quite small and may challenge readability. It is advisable to standardize the font sizes throughout all figures to enhance legibility.</p></disp-quote><p>We have slightly increased the overall font sizes, while we are generally following figure recommendations set out by Nature. The font sizes are the same throughout the figures.</p><disp-quote content-type="editor-comment"><p>- When presenting performance comparisons, a horizontal layout is often more intuitive for readers, as it aligns with the natural left-to-right reading direction. This is not just a personal preference; it is supported by visualization best practices as outlined in resources like the NVS Cheat Sheet (https://github.com/GraphicsPrinciples/CheatSheet/blob/master/NVSCheatSheet.pdf) and Kieran Healy's book (https://socviz.co/lookatdata.html).</p></disp-quote><p>We have changed all figures to use horizontal layout, hoping that this will ease visual comparison between the different models.</p><disp-quote content-type="editor-comment"><p>- In the kernel density estimation (KDE) and violin plot representations, it appears that the data displays may be truncated. It is crucial to indicate where the data distribution ends. Overplotting individual data points could provide additional clarity.</p></disp-quote><p>To avoid confusion about the data distribution in the violin plots, we have now overlaid scatter plots, as suggested by the reviewer. Overlaying the fold-level accuracies was not feasible (since this would result in ~1.5 million transparent points for a single figure), so we instead show the accuracies averaged over folds but separate for target variables and CV iterations. Only the newly added coefficient of determination plots had to be truncated, which we have noted in the figure legend.</p><disp-quote content-type="editor-comment"><p>- Figure 3 could inadvertently suggest that time-varying features correspond to panel A and time-averaged features to panel B. To avoid confusion, consider reorganizing the labels at the bottom into two rows for clearer attribution.</p></disp-quote><p>We have changed the layout of the time-varying and time-averaged labels in the new version of the plots to avoid this issue.</p><disp-quote content-type="editor-comment"><p>Discussion:</p><p>- The discussion on multimodal modeling might give the impression that it is more effective with multiple kernel learning (MKL) than with other methods. To present a more balanced view, it would be appropriate to rephrase this section. For instance, stacking, examples of which are cited in the same paragraph, has been successfully applied in practice. The text could be adjusted to reflect that Fisher Kernels via MKL adds to the array of viable options for multimodal modeling. As a side thought: additionally, a well-designed comparison between MKL and stacking methods, conducted by experts in each domain, could greatly benefit the field. In certain scenarios, it might even be demonstrated that the two approaches converge, such as when using linear kernels.</p></disp-quote><p>We would like to thank the reviewer for the suggestion about the discussion concerning multimodal modelling. We agree that there are other relevant methods that may lead to interesting future work and have now included stacking and refined the section: ll. 487-494:</p><p>“While directly combining the features from each modality can be problematic, modality-specific kernels, such as the Fisher kernel for time-varying amplitude and/or FC, can be easily combined using approaches such as stacking (Breiman, 1996) or Multi Kernel Learning (MKL) (Gönen &amp; Alpaydın, 2011). MKL can improve prediction accuracy of multimodal studies (Vaghari et al., 2022), and stacking has recently been shown to be a useful framework for combining static and time-varying FC predictions (Griffin et al., 2024). A detailed comparison of different multimodal prediction strategies including kernels for time-varying amplitude/FC may be the focus of future work.”</p><disp-quote content-type="editor-comment"><p>- The potential clinical applications of brain dynamics extend beyond diagnosis and individual outcome prediction. They play a significant role in the context of biomarkers, including pharmacodynamics, prognostic assessments, responder analysis, and other uses. The current discussion might be misinterpreted as being specific to hidden Markov model (HMM) approaches. For diagnostic purposes, where clinical assessment or established biomarkers are already available, the need for new models may be less pressing. It would be advantageous to reframe the discussion to emphasize the potential for gaining deeper insights into changes in brain activity that could indicate therapeutic effects or improvements not captured by structural brain measures. However, this forward-looking perspective is not the focus of the current work. A nuanced revision of this section is recommended to better reflect the breadth of applications.</p></disp-quote><p>We appreciate the reviewer’s thoughtful suggestions regarding the discussion of potential clinical applications. We have included the suggestions and refined this section of the discussion: Ll. 495-507:</p><p>“In a clinical context, while there are nowadays highly accurate biomarkers and prognostics for many diseases, others, such as psychiatric diseases, remain poorly understood, diagnosed, and treated. Here, improving the description of individual variability in brain measures may have potential benefits for a variety of clinical goals, e.g., to diagnose or predict individual patients’ outcomes, find biomarkers, or to deepen our understanding of changes in the brain related to treatment responses like drugs or non-pharmacological therapies (Marquand et al., 2016; Stephan et al., 2017; Wen et al., 2022; Wolfers et al., 2015). However, the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. Our proposed approach provides one avenue of addressing this by leveraging individual patterns of time-varying amplitude and FC, and it can be flexibly modified or extended to include, e.g., information about temporally recurring frequency patterns (Vidaurre et al., 2016).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>- I wondered if the authors could provide, within the Introduction, an intuitive description for how the Fisher Kernel &quot;preserves the structure of the underlying model of brain dynamics&quot; / &quot;preserves the mathematical structure of the underlying HMM&quot;? Providing more background may help to motivate this study to a general audience.</p></disp-quote><p>We agree that this would be helpful and have now added this to the introduction: Ll.61-67:</p><p>“Mathematically, the HMM parameters lie on a Riemannian manifold (the structure). This defines, for instance, the relation between parameters, such as: how changing one parameter, like the probabilities of transitioning from one state to another, would affect the fitting of other parameters, like the states’ FC. It also defines the relative importance of each parameter; for example, how a change of 0.1 in the transition probabilities would not be the same as a change of 0.1 in one edge of the states’ FC matrices.”</p><p>To communicate the intuition behind the concept, the idea was also illustrated in Figure 1, panel 4 by showing Euclidean distances as straight lines through a curved surface (4a, Naïve kernel), as opposed to the tangent space projection onto the curved manifold (4b, Fisher kernel).</p><disp-quote content-type="editor-comment"><p>- Some clarifications regarding Figure 2a would be helpful. Was the linear Fisher Kernel significantly better than the linear Naive normalized kernel? I couldn't find whether this comparison was carried out. Apologies if I have missed it in the text. For some of the brackets indicating pairwise tests and their significance values, the start/endpoints of the bracket fall between two violins; in this case, were the results of the linear and Gaussian Fisher Kernels pooled together for this comparison?</p></disp-quote><p>We have now streamlined the statistical comparisons and avoided plotting brackets falling between two violin plots. The comparisons that were carried out are stated in the methods section 4.4.1. Please see also our response to above to Reviewer #3 public review, potential weaknesses, point 1, relevant point copied below:</p><p>In conjunction with the new statistical approach (see Reviewer #2, comment 3), we have now streamlined the comparisons. We explained which comparisons were performed in the methods ll.880-890:</p><p>“For the main results, we separately compare the linear Fisher kernel to the other linear kernels, and the Gaussian Fisher kernel to the other Gaussian kernels, as well as to each other. We also compare the linear Fisher kernel to all time-averaged methods. Finally, to test for the effect of tangent space projection for the time-averaged FC prediction, we also compare the Ridge regression model to the Ridge Regression in Riemannian space. To test for effects of removing sets of features, we use the approach described above to compare the kernels constructed from the full feature sets to their versions where features were removed or reduced. Finally, to test for effects of training the HMM either on all subjects or only on the subjects that were later used as training set, we compare each kernel to the corresponding kernel constructed from HMM parameters, where training and test set were kept separate”.</p><disp-quote content-type="editor-comment"><p>- The authors may wish to include, in the Discussion, some remarks on the use of all subjects in fitting the group-level HMM and the implications for the cross-validation performance, and/or try some analysis to ensure that the effect is minor.</p></disp-quote><p>As suggested by reviewers #2 and #3, we have now performed the suggested analysis and show that fitting the group-level HMM to all subjects compared to only to the training subjects has no effect on the results. Please see our response to Reviewer #2, public review, comment 2.</p><disp-quote content-type="editor-comment"><p>- The decision to use k=6 states was made here, and I wondered if the authors may include some support for this choice (e.g., based on findings from prior studies)?</p></disp-quote><p>We have now refined and extended our explanation and rationale behind the number of states: Ll. 586-594: “The number of states can be understood as the level of detail or granularity with which we describe the spatiotemporal patterns in the data, akin to a dimensionality reduction, where a small number of states will lead to a very general, coarse description and a large number of states will lead to a very detailed, fine-grained description. Here, we chose a small number of states, K=6, to ensure that the group-level HMM states are general enough to be found in all subjects, since a larger number of states increases the chances of certain states being present only in a subset of subjects. The exact number of states is less relevant in this context, since the same HMM estimation is used for all kernels.”</p><disp-quote content-type="editor-comment"><p>- (minor) Abstract: &quot;structural aspects&quot; - do you mean structural connectivity?</p></disp-quote><p>With “structural aspects”, we refer to the various measures of brain structure that are used in predictive modelling. We have now specified: Ll. 14-15: “structural aspects, such as structural connectivity or cortical thickness”.</p></body></sub-article></article>