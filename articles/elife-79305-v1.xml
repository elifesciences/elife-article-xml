<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79305</article-id><article-id pub-id-type="doi">10.7554/eLife.79305</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Open-source tools for behavioral video analysis: Setup, methods, and best practices</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-302062"><name><surname>Luxem</surname><given-names>Kevin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-210509"><name><surname>Sun</surname><given-names>Jennifer J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0906-6589</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-275843"><name><surname>Bradley</surname><given-names>Sean P</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-221656"><name><surname>Krishnan</surname><given-names>Keerthi</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-88587"><name><surname>Yttri</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154524"><name><surname>Zimmermann</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-108187"><name><surname>Pereira</surname><given-names>Talmo D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9075-8365</contrib-id><email>talmo@princeton.edu</email><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-253781"><name><surname>Laubach</surname><given-names>Mark</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2403-4497</contrib-id><email>mark.laubach@gmail.com</email><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zwmgk08</institution-id><institution>Cellular Neuroscience, Leibniz Institute for Neurobiology</institution></institution-wrap><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Department of Computing and Mathematical Sciences, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>Rodent Behavioral Core, National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/020f3ap87</institution-id><institution>Department of Biochemistry and Cellular &amp; Molecular Biology, University of Tennessee</institution></institution-wrap><addr-line><named-content content-type="city">Knoxville</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Department of Biological Sciences, Carnegie Mellon University</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>Department of Neuroscience, University of Minnesota</institution></institution-wrap><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>The Salk Institute of Biological Studies</institution></institution-wrap><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052w4zt36</institution-id><institution>Department of Neuroscience, American University</institution></institution-wrap><addr-line><named-content content-type="city">Washington D.C.</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cai</surname><given-names>Denise J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>03</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e79305</elocation-id><history><date date-type="received" iso-8601-date="2022-04-07"><day>07</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-03-03"><day>03</day><month>03</month><year>2023</year></date></history><permissions><copyright-statement>© 2023, Luxem, Sun et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Luxem, Sun et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79305-v1.pdf"/><abstract><p>Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional ‘center of mass’ tracking algorithms to enable video analysis at scale. The expansion of open-source tools for video acquisition and analysis has led to new experimental approaches to understand behavior. Here, we review currently available open-source tools for video analysis and discuss how to set up these methods for labs new to video recording. We also discuss best practices for developing and using video analysis methods, including community-wide standards and critical needs for the open sharing of datasets and code, more widespread comparisons of video analysis methods, and better documentation for these methods especially for new users. We encourage broader adoption and continued development of these tools, which have tremendous potential for accelerating scientific progress in understanding the brain and behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>video</kwd><kwd>pose estimation</kwd><kwd>methods</kwd><kwd>open source</kwd><kwd>behavior</kwd><kwd>reproducibility</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1948181</award-id><principal-award-recipient><name><surname>Laubach</surname><given-names>Mark</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>DA046375</award-id><principal-award-recipient><name><surname>Laubach</surname><given-names>Mark</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>PGSD3-532647-2019</award-id><principal-award-recipient><name><surname>Sun</surname><given-names>Jennifer J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH002952</award-id><principal-award-recipient><name><surname>Bradley</surname><given-names>Sean P</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH124042</award-id><principal-award-recipient><name><surname>Krishnan</surname><given-names>Keerthi</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH128177</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>2024581</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Methods for video analysis of behaving animals are described, with guidance provided on setting up the methods in a research laboratory and best practices for experimenters and developers.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Quantitative tools for video analysis</title><p>Traditional approaches to analyzing video data have involved researchers watching video playback and noting the times and locations of specific events of interest. These analyses are very time-consuming, require expert knowledge in the target species and experimental design, and are prone to user bias (<xref ref-type="bibr" rid="bib2">Anderson and Perona, 2014</xref>). Video recordings are often made for many different animals and behavioral test sessions, but only reviewed for a subset of experiments. Complete sets of videos are rarely made accessible in published studies and the analysis methods are often vaguely described. There are variations in scoring criteria across researchers and labs, even over time by a single researcher. Collectively, these issues present major challenges for research reproducibility and the difficulty and cost of manual video analysis has led to the dominance of easy-to-use measures (lever pressing, beam breaks) in the neuroscience literature, and this has limited our understanding of brain-behavior relationships (<xref ref-type="bibr" rid="bib38">Krakauer et al., 2017</xref>).</p><p>For example, ‘reward seeking’ has been a popular topic in recent years and is typically measured using beam breaks between response and reward ports located inside an operant arena (e.g., <xref ref-type="bibr" rid="bib17">Cowen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Feierstein et al., 2006</xref>; <xref ref-type="bibr" rid="bib39">Lardeux et al., 2009</xref>; <xref ref-type="bibr" rid="bib87">van Duuren et al., 2009</xref>). By relying only on the discrete times when animals make a choice and receive a reward, it is not possible to describe how the animal moves during a choice or how it collects a reward. Animals may not move in the same way to a reward port when they expect a larger or smaller reward (e.g., <xref ref-type="bibr" rid="bib19">Davidson et al., 1980</xref>). This could lead to, for example, a neural recording study labeling a cell as ‘reward encoding’ when it actually reflects differences in movement.</p><p>Commercial products (e.g., Ethovision by Noldus, Any-Maze by Stoelting) and open-source projects (e.g., JAABA: <xref ref-type="bibr" rid="bib33">Kabra et al., 2013</xref>; SCORHE: <xref ref-type="bibr" rid="bib68">Salem et al., 2015</xref>; OptiMouse: <xref ref-type="bibr" rid="bib7">Ben-Shaul, 2017</xref>; ezTrack: <xref ref-type="bibr" rid="bib60">Pennington et al., 2019</xref>) are available for semi-automated annotation and tracking of behaviors. These methods track animals based on differences between the animals and the background color or luminance. This can be challenging to do in naturalistic settings or for species or strains that do not have a uniform color (e.g., Long-Evans rats). These methods provide estimates of the overall position of an animal in its environment and can be used to measure the direction and velocity of its movements. These ‘center of mass’ tracking methods could be used to measure where an animal is and how fast it is moving. More sophisticated versions of these products may also detect the head and tail of common laboratory species such as rodents or zebrafish and draw inferences from the shape and location of the animal to classify a small subset of an animal’s behavioral repertoire. However, these simpler tracking methods cannot account for movements of discrete sets of body parts (e.g., head scanning in rodents, which is associated with a classic measure of reward-guided decisions called ‘vicarious trial-and-error’ behavior: see <xref ref-type="bibr" rid="bib65">Redish, 2016</xref>, for review).</p><p>More advanced analyses could be used to quantify movements across many pixels simultaneously in video recordings. For example, <xref ref-type="bibr" rid="bib79">Stringer et al., 2019</xref>, used dimensionality reduction methods to study the spontaneous coding of visual- and movement-related information in the mouse visual cortex in relation to facial movements. <xref ref-type="bibr" rid="bib50">Musall et al., 2019</xref>, used video recordings of motion data from several parts of the face of mice as they performed a decision-making task and related the measures from the video recordings to cortical imaging data. While these analyses would go beyond what is possible to achieve with a simple tracking method, the multivariate methods developed by Stringer and Musall are not themselves capable of categorizing movements, measuring transitions between different types of movements, or quantifying the dynamics of movement sequences. For these measures, a different approach is needed.</p><p>Methods for capturing the pose of an animal (the location and configuration of its body) have emerged in recent years (e.g., DeepLabCut: <xref ref-type="bibr" rid="bib46">Mathis et al., 2018a</xref>; SLEAP: <xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>). These methods can provide a description of an animal’s movement and posture during a behavioral task. They can be used to understand the dynamics of naturalistic movements and behaviors, as illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Pose estimation methods provide information on the position and orientation of multiple parts of an animal, with recent methods being able to measure pose information for groups of animals (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Lauer et al., 2021</xref>; <xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib88">Walter and Couzin, 2021</xref>). Some recent methods now even allow for pose estimation to be run in real experimental time (<xref ref-type="bibr" rid="bib34">Kane et al., 2020</xref>; <xref ref-type="bibr" rid="bib41">Lopes et al., 2015</xref>; <xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib72">Schweihoff et al., 2021</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Setup for video recording.</title><p>(<bold>A</bold>) Cameras are mounted above and to the side of a behavioral arena. The cameras record sequences of images of an animal performing a behavioral task. The recordings are stored on a computer and analyzed with methods for pose estimation and behavior classification. (<bold>B</bold>) The animal’s pose trajectory captures the relevant kinematics of the animal’s behavior and is used as input to behavior quantification algorithms. Quantification can be done using either unsupervised (learning to recognize behavioral states) or supervised (learning to classify behaviors based on human annotated labels). In this example, transitions among three example behaviors (rearing, walking, and grooming) are depicted on the lower left and classification of video frames into the three main behaviors are depicted on the lower right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79305-fig1-v1.tif"/></fig><p>Methods for pose estimation emerged in computer vision research in the late 1970s (<xref ref-type="bibr" rid="bib45">Marr et al., 1978</xref>; <xref ref-type="bibr" rid="bib51">Nevatia and Binford, 1973</xref>). The methods became widely available for the analysis of pose in human behavior following improvements in computer vision (<xref ref-type="bibr" rid="bib6">Behnke, 2003</xref>), deep learning (<xref ref-type="bibr" rid="bib86">Szegedy et al., 2013</xref>), and computing using graphical processing units (GPUs) (<xref ref-type="bibr" rid="bib55">Oh and Jung, 2004</xref>). However, these methods were often not robust or required a lot of training data, which were at the time not easily available for animal studies. As a result, a number of open-source tools emerged for pose estimation in animals (e.g., DeepLabCut: <xref ref-type="bibr" rid="bib46">Mathis et al., 2018a</xref>, LEAP: <xref ref-type="bibr" rid="bib61">Pereira et al., 2019</xref>; DeepPoseKit: <xref ref-type="bibr" rid="bib25">Graving et al., 2019a</xref>). These tools are especially notable in that they were developed to address specific scientific questions by researchers and are not available from commercial sources. They are an outstanding example of the ‘open-source creative process’ (<xref ref-type="bibr" rid="bib90">White et al., 2019</xref>).</p><p>One of these methods, DeepLabCut, has been shown to outperform the commercial software package EthoVision XT14 and a hardware-based measurement system from TSE Systems, based on IR beam breaks (<xref ref-type="bibr" rid="bib80">Sturman et al., 2020</xref>). When tested across a set of common behavioral assays used in neuroscience (open field test, elevated plus maze, forced swim test), data from the pose estimation method was evaluated using a neural network classifier and performed as well as classifications by human experts, required data from fewer animals to detect differences due to experimental treatments, and in some cases (head dips in an elevated plus maze) detected effects of treatment (a drug) that was not detected by EthoVision.</p><p>In the case of reward seeking behavior, human annotation of videos could resolve the animal’s position and when and for how long specific behaviors occurred. These measurements could be made by annotating frames in the video recordings, using tools such as the VIA annotator (<xref ref-type="bibr" rid="bib21">Dutta and Zisserman, 2019</xref>), and commercial (e.g., EthoVision) or open-source (e.g., ezTrack) methods for whole-animal tracking. These measurements would not be able to account for coordinated movements of multiple body parts or for the dynamics of transitions between different behaviors that together comprise reward seeking behavior. These measurements are easily made using methods for pose estimation. These methods learn to track multiple body parts (for a rodent, the tip of snout, the ears, the base of the tail) and the positions of these body parts can be compared for different kinds of trials (small or large reward) using standard statistical models or machine learning methods. These analyses, together, allow for movements to be categorized (e.g., direct and indirect approach toward a reward port) and for transitions between different types of movements to be quantified (e.g., from turning to walking). It would even be possible to detect unique states associated with deliberation (e.g., head scanning between available choice options). All these measures could then be compared as a function of an experimental manipulation (drug or lesion) or used to assist in the analysis of simultaneously collected electrophysiological or imaging data. None of these measures are possible using conventional methods for annotating video frames or tracking overall the overall position of the animal in a behavioral arena.</p><p>Pose estimation methods have been crucial for several recent publications on topics as diverse as tracking fluid consumption to understand the neural coding of reward prediction errors (<xref ref-type="bibr" rid="bib58">Ottenheimer et al., 2020</xref>), accounting for the effects of wind on the behavior of <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib57">Okubo et al., 2020</xref>), understanding the contributions of tactile afferents and nociceptors to the perception of touch in freely moving mice (<xref ref-type="bibr" rid="bib71">Schorscher-Petcu et al., 2021</xref>), understanding interactions between tactile processing by the rodent whisker system and its ability to guide locomotion (<xref ref-type="bibr" rid="bib89">Warren et al., 2021</xref>), and measuring the relationship between eye movements and neural activity in freely behaving rodents (<xref ref-type="bibr" rid="bib36">Keshavarzi et al., 2022</xref>). While a number of studies are emerging that take advantage of methods for pose estimation, there is still not enough widespread adoption of the methods across the research community, perhaps in part due to the technical nature of collecting high-quality video recordings as well as setting up and using methods for pose estimation. These methods depend on access to computing systems with GPUs and the ability to set up and use the required computer software, which is usually available as computer code written in Python or MATLAB. A researcher who wants to get started with these approaches will therefore face a number of questions about how to set up video methods in a laboratory setting. New users may also need to learn some of the jargon associated with video analysis methods, and some of these terms are defined in <xref ref-type="table" rid="table1">Table 1</xref>. <italic>The primary goals of this document are twofold: to provide information for researchers interested in setting methods for video analysis in a research lab and to propose best practices for the use and development of video analysis methods</italic>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Frequently used terms for video analysis.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="top">pose</td><td align="left" valign="top">The configuration (position and/or orientation) of an animal, object, or body parts in an image or video recording</td></tr><tr><td align="left" valign="top">keypoints/landmarks</td><td align="left" valign="top">Distinct identifiable morphological features (e.g., the tip of the snout or the base of the tail in a rodent) that can be localized in 2D or 3D from images, typically via pose estimation</td></tr><tr><td align="left" valign="top">part grouping</td><td align="left" valign="top">A process for assigning keypoints to individual animals</td></tr><tr><td align="left" valign="top">multi-object tracking</td><td align="left" valign="top">In multi-animal pose tracking, the task of determining which detected poses belong to which individual animal across time</td></tr><tr><td align="left" valign="top">re-identification</td><td align="left" valign="top">A process for identifying all images containing the same individual animal based primarily on their distinct appearance</td></tr><tr><td align="left" valign="top">kinematics</td><td align="left" valign="top">Information about the angles and velocities of a set of keypoints</td></tr><tr><td align="left" valign="top">supervised learning</td><td align="left" valign="top">Machine learning methods that use experimenter-provided labels (e.g., ground truth poses, or ‘running’ vs ‘grooming’) to train a predictive model</td></tr><tr><td align="left" valign="top">unsupervised learning</td><td align="left" valign="top">Machine learning methods that only use unlabeled data to find patterns based on its intrinsic structure (e.g., clustering behavioral motifs based on the statistics of their dynamics)</td></tr><tr><td align="left" valign="top">transfer learning</td><td align="left" valign="top">Machine learning methods that use models trained on one dataset to analyze other datasets (e.g., models of grooming in mice applied to rats)</td></tr><tr><td align="left" valign="top">self-supervised learning</td><td align="left" valign="top">Machine learning methods that use only unlabeled data for training by learning to solve artificially constructed tasks (e.g., comparing two variants of the same image with noise added against other images; predicting the future; or filling in blanks)</td></tr><tr><td align="left" valign="top">embedding</td><td align="left" valign="top">A representation of high-dimensional data into lower dimensional representation</td></tr><tr><td align="left" valign="top">lifting</td><td align="left" valign="top">A process through which 2D pose data are converted to 3D representations</td></tr><tr><td align="left" valign="top">behavioral segmentation</td><td align="left" valign="top">A process for detecting occurrences of behaviors (i.e., starting and ending frames) from video or pose sequences</td></tr></tbody></table></table-wrap></sec><sec id="s2"><title>A basic setup for video recordings in animal experiments</title><p>In a typical setup for video recording, cameras are placed above, and in some cases to the side or below, the behavioral arena (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The cameras send data to a computer and can be integrated with inputs from behavioral devices using custom-written programs using popular libraries such as OpenCV (<xref ref-type="bibr" rid="bib10">Bradski, 2000</xref>), open-source data collection systems such as Bonsai (<xref ref-type="bibr" rid="bib41">Lopes et al., 2015</xref>), or software included with many common commercial video capture boards (loopbio Motif). Video files can then be analyzed using a variety of open-source tools.</p><p>A common approach is to use methods for pose estimation, which track the position and orientation of the animal. This is done by denoting a set of ‘keypoints’ or “landmarks” (body parts) in terms of pixel locations on frames in the video recordings. Packages for pose estimation provide graphical user interfaces for defining keypoints and the keypoints are then analyzed with video analysis methods. In the example shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, keypoints are the colored dots on the tip of the snout, the ears, forelimbs and paw, midpoint of back, hindlimbs and paws, and base, middle, and end of tail. Once body parts have been defined, computer algorithms are used to track the skeleton formed by the points and to track the position and orientation of the skeleton over frames in the video file. Many open-source tools use machine learning methods for these intensive computational processes, which require GPUs to run in reasonable time. To run these analyses, many labs have either dedicated computers, institutional computing clusters, or cloud computing services such as Google Colab. The outputs of pose estimation can be analyzed to account for movement variability associated with different behaviors, to relate position and orientation to simultaneously collected brain activity (electrophysiology, optical imaging), or with algorithms that can describe and predict states and dynamical transitions of behaviors.</p><sec id="s2-1"><title>Data acquisition</title><p>The first step in setting up for video recording is to purchase a camera with an appropriate lens. Researchers should determine if they need precisely timed video frames, for example, for integration with electrical or optical recordings. Inexpensive USB webcams with frame rates of at least 30 fps are suitable for many neuroscience experiments. However, it is important to make sure that each camera is connected to a dedicated USB channel in the computer used for video recording. Webcam cameras can be a challenge to integrate with systems used for behavioral control and electrophysiology or imaging because they lack a means of precisely synchronizing video frames to other related data. As such, the timing of specific behaviors must be based on the animal’s location or an observable event in the video field (e.g., onset of an LED indicating reward availability).</p><p>For more precise recordings, specialized cameras used in computer vision applications are needed (e.g., FLIR, Basler). Power and combined data over Ethernet (GigE PoE) is commonly used as it combines long cable length headroom with joint DC power delivery. Alternatively, USB3 cameras can be used, but have a maximum data cable length of 5 m, although active extender cables are available. Most machine vision cameras (GigE PoE or USB3) have general-purpose input output capabilities that allow for time synchronization of multiple cameras with other laboratory equipment (e.g., electrical or optical recording system). A single camera running at high resolution or frame rate can quickly saturate a standard 1 Gbit Ethernet link. Therefore, it is important to consider the computer used to collect video data, ensuring that it has a fast processor with multiple cores and perhaps also a GPU, which can aid in handling video compression during data collection and can be used for off-line analysis using pose estimation methods.</p><p>After choosing a camera, researchers must determine how to save and archive data from their recordings. By default, recorded videos from cameras may be in several formats, such as MP4 (MPEG-4 AVC/H. 264 encoding), MOV (MPEG-4 encoding), and AVI (DivX codec, higher quality but larger file size). These formats are generally universal and can be read by a variety of tools. Generally, video data files tend to be large (1 hr of RGB video at 30 Hz with resolution 1000×1000 can be 2–20 GB depending on compression) so data storage solutions for large-scale experiments are crucial. File compression should be evaluated before a system is deployed, as the computer used for video recordings must have sufficient memory (RAM) to remain stable over long recording sessions. In addition to considerations of file formats and codecs, it is important to plan for data storage. Many labs maintain internal lab servers for their video data. Cloud storage is another option to enable sharing. For sharing data publicly, there are a variety of hosting services available, such as the Open Science Foundation, Figshare, and Dryad (see section on ‘Best practices for experimenters and developers’ below for further comments on data archives and sharing).</p><p>Once cameras and lenses are acquired and data formats and storage resolved, the next question is where to position the cameras relative to the experimental preparation. Occlusions due to obstacles, cables, or conspecifics will have effects on the usability of some video analysis methods. A bottom-up view (from below the animal) works best in an open-field, while a top-down approach can be useful for studies in operant chambers and home cages. Bottom-up views capture behavioral information from the position of the animal’s feet (<xref ref-type="bibr" rid="bib30">Hsu and Yttri, 2021a</xref>; <xref ref-type="bibr" rid="bib42">Luxem et al., 2022a</xref>). When multiple cameras are used, to reduce the effect of occlusion for downstream video analysis, cameras should be positioned such that at least one camera can visualize each keypoint at all times.</p><p>It is also necessary to think about lighting for the experimental setup. If all or some of the study is to be performed while house lights are off, then infrared (IR) lighting and IR-compatible cameras may be needed. One should consider if diffuse lighting will work or if modifications to eliminate reflective surfaces (especially metals) are necessary. These can lead to artifacts in video recordings from devices like IR LEDs and other sources of illumination and complicate the training and interpretations of measures obtained with analyses such as pose estimation. For example, it is possible to reduce reflections from surfaces and objects that are in direct line with IR LEDs. For top-down recordings, cage floors can be made from colored materials to provide contrast such as Delrin or pre-anodized aluminum (an option for long-term use) and the metal pans typically used below operant chambers to collect animal waste can be painted with flat black paint. Addressing these issues before beginning an experiment can greatly improve the quality of video recordings.</p><p>Finally, for some applications, it is necessary to invest time in calibrating the video system. Calibration is often overlooked and not easily accessible in many current software packages. The intrinsic parameters of a camera include the focal length of the lens and if the lens has obvious distortions (i.e., fisheye lens). Extrinsic parameters also affect the quality of video recordings and are largely due to the camera’s position in the scene. It is fairly easy to calibrate a single camera using a checkerboard or ArUco board. To do so, one sweeps a precalibrated board manually around the field of view of a camera and uses the extracted images to estimate the camera’s intrinsic parameters (focal length and distortions). This approach can scale easily to cameras with overlapping fields of view but becomes difficult if larger camera networks do not share extrinsic parameters or need to be repeatedly recalibrated (e.g., if one of the cameras is moved between experiments). If the environment has enough structure in it, structure from motion can estimate the intrinsic and extrinsic parameters by treating the multiple cameras as an exhaustive sweep of the environment. This process can be fully scripted and automatically performed on a daily basis leading to substantially increased reliability and precision in multi-camera system performance. Several references on these topics include <xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib64">Rameau et al., 2022</xref>; <xref ref-type="bibr" rid="bib70">Schönberger et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Schonberger and Frahm, 2016</xref>.</p></sec><sec id="s2-2"><title>Hardware and software for data analysis</title><p>Once video recordings are acquired, the researcher may proceed to setting up their computing environment for pose estimation and tracking. Modern markerless motion capture software tools like DeepLabCut (<xref ref-type="bibr" rid="bib46">Mathis et al., 2018a</xref>) and SLEAP (<xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>) rely on deep learning to automate this process. The most compute-intensive step of these methods involves a ‘training’ stage in which a deep neural network is optimized to learn to predict poses from user-provided examples. Training is typically accelerated with a GPU, a hardware component traditionally used for computer graphics, but which has been co-opted for deep learning due to its massively parallel processing architecture. Having a GPU can speed up training by 10- to 100-fold, resulting in model training times in as little as minutes with lightweight network architectures (<xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>). For most researchers, the most practical option is to purchase a consumer-grade workstation GPU which can be installed in conventional desktop computers to afford local access to this hardware from the pose tracking software. In this case, any recent NVIDIA GPU with greater than 6 GB of memory will suffice for practical use of pose estimation tools. This type of computer hardware has, in recent years, been significantly impacted by supply chain shortages, driving prices up to &gt;$1000, which makes this a less accessible option for many labs just starting off in video analysis. For this situation, most tools provide the means for using Google Colab, which provides limited access to GPUs on the cloud. This is an excellent way to set up analysis workflows while getting familiar with deep learning-based video analysis but may not be practical for sustained usage (e.g., processing 100 s of videos). Another common scenario is that institutions with a high-performance computing center will typically have GPUs available as a local shared resource. Other than GPUs, most other computer requirements are modest (modern CPU, 8–16 GB of RAM, minimal disk space).</p><p>Researchers will need to set up their software environment to be able to install and use pose tracking tools. Most commonly available open-source methods for pose estimation were developed using the Python language. It is highly recommended to make use of ‘environment managers’ such as Anaconda (‘conda’) which enable the creation of isolated installations of Python for each video analysis method of interest. This allows for the methods to be installed with all its dependencies without affecting other Python libraries on the system. Alternatives include Docker, which allows for running an entire virtual machine in isolation. This is done to facilitate the installation of GPU-related dependencies, which may be technically challenging for novice users.</p></sec><sec id="s2-3"><title>2D pose estimation and tracking</title><p>Pose tracking methods (<xref ref-type="fig" rid="fig2">Figure 2</xref>, part 1) enable researchers to extract positional information about the body parts of animals from video recordings. Tools for pose tracking (see <xref ref-type="table" rid="table2">Table 2</xref>) decompose the problem of pose tracking into sub-tasks outlined below. A note on nomenclature: pose estimation is the term typically reserved to mean single-animal <italic>keypoint localization</italic> within a single image; multi-animal pose estimation refers to<italic>; multi-animal pose estimation refers to keypoint localization</italic> and <italic>part grouping</italic> of multiple animals within a single image; and multi-animal pose tracking refers to combined <italic>keypoint localization</italic>, <italic>part grouping,</italic> and <italic>identification</italic> across video frames.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Pipeline for video analysis.</title><p>Video recordings are analyzed with either keypoints from 2D or 3D pose estimation or directly by computing video features. These videos or trajectory features are then used by downstream algorithms to relate the keypoints to behavioral constructs such as predicting human-defined behavior labels (supervised learning) or discovering behavior motifs (unsupervised learning). Each part of the analysis steps outlined in the figure is described in more detail below.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79305-fig2-v1.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Methods for 2D pose estimation.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">DeepLabCut</ext-link></td><td align="left" valign="top">DeepLabCut (<xref ref-type="bibr" rid="bib46">Mathis et al., 2018a</xref>; <xref ref-type="bibr" rid="bib47">Mathis et al., 2018b</xref>) uses a popular architecture for deep learning (<xref ref-type="bibr" rid="bib29">He et al., 2016</xref>), called ResNet. DeepLabCut models are pre-trained on a massive dataset for object recognition called ImageNet (<xref ref-type="bibr" rid="bib67">Russakovsky et al., 2015</xref>). Through a process called transfer learning, the DeepLabCut model learns the position of keypoints using as few as 200 labeled frames. This makes the model very robust and flexible in terms of what body parts (or objects) users want to label as the model provides a strong backbone of image filters within their ResNet architecture. To detect the keypoint position, DeepLabCut replaces the classification layer of the <italic>ResNet</italic> with deconvolutional layers to produce spatial probability densities from which the model learns to assign high probabilities to regions with the user labeled keypoints. DeepLabCut can provide very accurate pose estimations but can require extensive time for training.</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://sleap.ai/">SLEAP</ext-link></td><td align="left" valign="top">SLEAP (<xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>) is based on an earlier method called LEAP (<xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>), which performed pose estimation on single animals. SLEAP uses simpler CNN architectures with repeated convolutional and pooling layers. This makes the model more lightweight compared to DLC’s ResNet architecture and, hence, the model is faster to train with comparable accuracy. Similar to DeepLabCut, the model uses a stack of upsampling or deconvolutional layers to estimate confidence maps during training and inference. Unlike DLC, SLEAP does not solely rely on transfer learning from general-purpose network models (though this functionality is also provided for flexible experimentation). Instead, it uses customizable neural network architectures that can be tuned to the needs of the dataset. SLEAP can produce highly accurate pose estimates starting at about 100 labeled frames for training combined and is quick to train on a GPU (&lt;1 hour).</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/DeepPoseKit">DeepPoseKit</ext-link></td><td align="left" valign="top">DeepPoseKit (<xref ref-type="bibr" rid="bib25">Graving et al., 2019a</xref>; <xref ref-type="bibr" rid="bib26">Graving et al., 2019b</xref>) uses a type of CNN architecture, called stacked DenseNet, an efficient variant of the stacked hourglass (<xref ref-type="bibr" rid="bib52">Newell et al., 2016</xref>), and uses multiple down- and upsampling steps with densely connected hourglass networks to produce confidence maps on the input image. The model uses only about 5% of the amount of parameters used by DeepLabCut, providing speed improvements over DeepLabCut and LEAP.</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/BKinD">B-KinD</ext-link></td><td align="left" valign="top">B-KinD (<xref ref-type="bibr" rid="bib81">Sun et al., 2021a</xref>; <xref ref-type="bibr" rid="bib82">Sun et al., 2021b</xref>) discovers key points without human supervision. B-KinD has the potential to transform how pose estimation is done, as keypoint analysis is one of the most time-consuming aspects of doing pose estimation analysis. However, there are challenges for the approach when occlusions occur in the video recordings, e.g., recordings of animals tethered to brain recording systems.</td></tr></tbody></table></table-wrap><p>Keypoint localization involves recovering the spatial coordinates of each distinct keypoint. This is normally done by estimating body part confidence maps, that is, image-based representations that encode the probability of the body part being located at each pixel. Recovering the coordinates of each body part is reduced to the task of finding the pixel with highest probability. A key consideration of this task is that the larger the image, the larger the confidence maps. Computer memory requirements can potentially exceed the capacity of most consumer-grade GPUs. This can be compensated by reducing the resolution of the confidence maps, though this comes at the cost of potentially reduced accuracy. Subpixel refinement methods are typically employed to compensate for this, but ultimately confidence map resolution is one of the most impactful choices for achieving reliable keypoint localization.</p><p>For single-animal videos, there will be at most <italic>one</italic> instance of each keypoint type present in the image, so keypoint localization is the only step strictly required. For multi-animal videos, however, there may be multiple instances of each keypoint type, for example, multiple ‘heads’. Part grouping refers to the task of determining the set of keypoint detections that belong to the same animal within an image. This is often approached in either a bottom-up or top-down fashion. In bottom-up models, all parts are detected, the association between them estimated (e.g., by using part affinity fields: <xref ref-type="bibr" rid="bib13">Cao et al., 2017</xref>), and then grouped. In top-down models, the animals are detected, cropped out of the image, and then keypoints are located in the same fashion as in the single-animal case. These approaches have specific trade-offs. Analyses of bottom-up recordings tend to be more memory-intensive but also more robust to transient occlusions and work well with animals with relatively large bodies (e.g., rodents). By contrast, top-down recordings tend to be analyzed in less time since only subsets of the image are processed. Top-down views work best with smaller body types that have fewer complex occlusions (e.g., flies). A notable consideration is that all single-animal pose estimation models can be used in the multi-animal setting if the animals can be detected and cropped as a preprocessing step (<xref ref-type="bibr" rid="bib25">Graving et al., 2019a</xref>; <xref ref-type="bibr" rid="bib61">Pereira et al., 2019</xref>). While both methods will work on most types of data, drastic improvements in performance and accuracy can be obtained by selecting the appropriate one – most pose estimation tools allow users to select between each approach type.</p><p>Once animals are detected and their keypoints located within a frame, the remaining task in multi-animal pose tracking is identification: repeatedly detecting the same animal across frame sequences. This can be approached as a multi-object tracking (MOT) problem, where animals are matched across frames based on a model or assumption about motion; or a re-identification (ReID) problem, where distinctive appearance features are used to unambiguously identify an animal. Both MOT and ReID (and hybrids) are available as standalone functionality in open-source tools, as well as part of multi-animal pose tracking packages. While MOT-based approaches can function on videos of animals with nearly indistinguishable appearances, they are prone to the error propagation issue inherent in methods with temporal dependencies: switching an animal’s identity even once will mean it is wrong for all subsequent frames. This presents a potentially intractable problem for long-term continuous recordings which may be impossible to manually proofread. ReID-like methods circumvent this problem by detecting distinguishing visual features, though this may not be compatible with existing datasets or all experimental paradigms.</p><p>The single most significant experimental consideration that will affect the identification problem is whether animals can be visually distinguished. A common experimental manipulation aimed at ameliorating this issue is to introduce visual markers to aid in unique identification of animals. This includes techniques such as grouping animals with different fur colors, painting them with non-toxic dyes (<xref ref-type="bibr" rid="bib56">Ohayon et al., 2013</xref>), or attaching barcode labels to a highly visible area of their body (<xref ref-type="bibr" rid="bib18">Crall et al., 2015</xref>). Though an essential part of the pose tracking workflow, identification remains a challenging problem in computer vision and its difficulty should not be underestimated when designing studies involving large numbers of interacting animals. We refer interested readers to previous reviews on multi-animal tracking (<xref ref-type="bibr" rid="bib59">Panadeiro et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Pereira et al., 2020</xref>) for more comprehensive overviews of these topics.</p><p>Tools that are based on deep learning work by training deep neural networks (models) to reproduce human annotations of behavior. Methods that strictly depend on learning from human examples are referred to as fully supervised. In the case of animal pose tracking, these supervisory examples (labels) are provided in the form of images and the coordinates of the keypoints of each animal that can be found in them. Most pose tracking software tools fall within this designation and provide graphical interfaces to facilitate labeling. The usability of these interfaces is a crucial consideration as most of the time spent in setting up a pose tracking system will be devoted to manual labeling. The more examples and the greater their diversity, the better that pose tracking models will perform. Previous work has shown that hundreds to thousands of labeled examples may be required to achieve satisfactory results, with a single example taking as much as 2 min to manually label (<xref ref-type="bibr" rid="bib46">Mathis et al., 2018a</xref>; <xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Pereira et al., 2019</xref>). To mitigate this, we strongly recommend adopting a human-in-the-loop labeling workflow. This is a practice in which the user trains a model with few labels, generates (potentially noisy) predictions, and imports those predictions into the labeling interface for manual refinement before retraining the model. This can drastically reduce the amount of time taken to generate thousands of labeled images necessary for reliable pose estimation models.</p><p>The rule of thumb is that ‘if you can see it, you can track it’, but this aphorism strongly depends on the examples provided to train the model. Important factors to consider in the labeling stage include labeling consistency and sample diversity. Labeling consistency involves minimizing the variability of keypoint placement within and across annotators which helps to ensure that models can learn generalizable rules for keypoint localization. This can be accomplished by formalizing a protocol for labeling, especially for ambiguous cases such as when an animal’s body part is occluded. For example, one convention may be to consistently place a ‘paw’ keypoint at the apex of the visible portion of the body rather than guessing where it may be located beneath an occluding object. Similarly, the principle of consistency should inform <italic>which</italic> body parts are selected as tracked keypoints. Body parts that are not easily located by the human eye will suffer from labeling inconsistency which may cause inferior overall performance as models struggle to find reliable solutions to detecting them. Sample diversity, on the other hand, refers to the notion that not all labeled examples have equal value when training neural networks. For example, labeling 1000 consecutive frames will ensure that the model is able to track data that looks similar to that segment of time, but will have limited capacity to generalize to data collected in a different session. As a best practice, labels should be sampled from the widest possible set of experimental conditions, time points, and imaging conditions that will be expected to be present in the final dataset.</p><p>Improving the capability of models to generalize to new data with fewer (or zero) labels is a currently active area of research. Techniques such as transfer learning and self-supervised learning aim to reduce the labeling burden by training models on related datasets or tasks. For example, B-KinD (<xref ref-type="bibr" rid="bib81">Sun et al., 2021a</xref>) is able to discover semantically meaningful keypoints in behavioral videos using self-supervision without requiring human annotations. These approaches work by training models to solve similar problems and/or on similar data than those used for pose estimation, with the intuition that some of that knowledge can be reused and thereby will require fewer (or no) labeled examples before achieving the same performance as fully supervised equivalents. Future work in this domain is on track to produce reusable models for commonly encountered experimental species and conditions. We highly encourage practitioners to adopt open data and model sharing to facilitate these efforts where possible.</p></sec><sec id="s2-4"><title>3D pose estimation</title><p>Several methods have emerged in recent years for 3D tracking based on pose data. For some applications, it is of interest to track animals in complete 3D space. This affords a more detailed representation of the kinematics by resolving ambiguities inherent in 2D projections – an especially desirable property when studying behaviors that involve significant out-of-plane movement, such as in large arenas or non-terrestrial behaviors.</p><p>It is important to note that 3D motion capture comes at a significant increase in technical complexity. As discussed above (see ‘Data acquisition’), camera synchronization and calibration are paramount for applications using 3D tracking as the result of this step will inform downstream algorithms as to the relative spatial configuration of the individual cameras. This step may be sensitive to small camera movements that occur during normal operation of behavioral monitoring systems, potentially requiring frequent recalibration. The number and positioning of cameras are also major determinants of 3D motion capture performance, both of which may depend on the specific behavior of interest, arena size and bandwidth, and computing capabilities on the acquisition computer. In some cases, it may be easiest to use mirrors instead of multiple cameras to allow for recording behavior from multiple perspectives.</p><p>Given a calibrated camera system, several approaches have emerged that can enable 3D pose estimation in animals. The simplest approaches rely on using 2D poses detected in each camera view, such as those produced by SLEAP or DeepLabCut as described above, and then <italic>triangulating</italic> them into 3D. 2D poses can be detected by training 2D pose models on each camera view independently, or by training a single model on all views, with varying results depending on how different the perspectives are. Once 2D poses can be obtained, methods such as Anipose (<xref ref-type="bibr" rid="bib35">Karashchuk et al., 2021</xref>), OpenMonkeyStudio (<xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>), and DeepFly3D (<xref ref-type="bibr" rid="bib27">Günel et al., 2019</xref>) are able to leverage camera calibration information to project poses into 3D for triangulation. This involves optimizing for the best 3D location of each keypoint that still maps back to the detected 2D location in each view. This can be further refined with temporal or spatial constraints, such as known limb lengths. Using this approach, more cameras will usually result in better triangulation, but will suffer (potentially catastrophically) when the initial 2D poses are incorrect. Since many viewpoints will have inherent ambiguities when not all body parts are visible, the 2D pose estimation error issue can be a major impediment to implementing 3D pose systems using the triangulation-based approach.</p><p>Alternative approaches attempt to circumvent triangulation entirely. LiftPose3D (<xref ref-type="bibr" rid="bib24">Gosztolai et al., 2021</xref>) describes a method for predicting 3D poses from single 2D poses, a process known as <italic>lifting</italic>. While this eliminates the need for multiple cameras, it requires a dataset of known 3D poses from which the 2D-3D correspondences can be obtained. This requirement depends on the multi-camera system being similar to the target 2D systems. DANNCE (<xref ref-type="bibr" rid="bib20">Dunn et al., 2021</xref>), on the other hand, achieves full 3D pose estimation by extending the standard 2D confidence map regression approach to 3D using volumetric convolutions. In their approach, images from each camera view are projected onto a common volume based on the calibration, before being fed into a 3D convolutional neural network that outputs a single volumetric part confidence map. This approach has the major advantage that it is not susceptible to 2D pose estimation errors since it solves for the 3D pose in a single step while also being able to reason about information present in distinct views. The trade-offs with this approach are that it requires significantly more computational power due to the 3D convolutions, as well as requiring 2D ground truth annotations on multiple views for a given frame.</p><p>Overall, a practitioner should be mindful of the caveats with implementing 3D pose estimation and is recommended to consider whether the advantages are truly necessary given the added complexity. We note that at the time of writing, none of the above methods can natively support the multi-animal case in 3D, other than by treating them as individual animals after preprocessing with a 2D multi-animal method for pose estimation. This limitation is due to issues with part grouping and identification as outlined above and would seem to be a future area of growth for animal pose estimation.</p></sec><sec id="s2-5"><title>Behavior quantification</title><p>After using pose estimation to quantify the movements of animal body parts, there are a number of analyses that can be used to understand how movements differ by experimental conditions (<xref ref-type="fig" rid="fig2">Figure 2</xref>, parts 2–4). A simple option is to use statistical methods such as ANOVA to assess effects on discrete experimental variables such as the time spent in a given location or the velocity of movement between locations. These measures can also be performed with data from simpler tracking methods, such as the commercially available EthoVision, TopScan, and ANY-maze programs. The primary benefits of the open source pose estimation methods described in this paper over these commercially available programs are the richness of the data obtained from pose estimation (see <xref ref-type="fig" rid="fig1">Figure 1</xref>) and the flexibility and customization of behavioral features are tracked (see <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>If researchers want to go beyond kinematic readouts and investigate the behavior an animal is executing in more detail, then methods for segmenting behavior from the pose tracking data can be used. Behavioral segmentation methods are available to discern discrete episodes of individual events and/or map video or trajectory data to continuous lower-dimensional behavioral representations. Discrete episodes have a defined start and end in which the animal is performing a particular behavior, while continuous representations represent behavior more smoothly over time. For discrete episodes, depending on the experimental conditions, these episodes can last from milliseconds up to minutes or longer. Segmentation can be done per animal, for example, detecting locomotion, or globally per frame, which is especially of interest for social behavior applications. In a global setting researcher might be interested in finding behavioral episodes that are directed between animals such as attacking or mounting behaviors.</p><p>If one wants to understand sequences of behaviors, there are many methods available to embed pose data into lower-dimensional representations. Such structures can be discovered through unsupervised methods. Some methods provide generic embeddings and do not explicitly model the dynamics of the behaving animal. Two examples of this approach are B-SOiD (<xref ref-type="bibr" rid="bib30">Hsu and Yttri, 2021a</xref>), which analyses pose data with unsupervised machine learning, and MotionMapper (<xref ref-type="bibr" rid="bib8">Berman et al., 2014</xref>), a method that does not use pose estimation methods. These models embed data points based on feature dynamics (e.g., distance, speed) into a lower-dimensional space. Within this space it is possible to apply clustering algorithms for the segmentation of behavioral episodes. Generally, dense regions in this space (regions with many data points grouped together) are considered to be conserved behaviors. Other methods are aimed at explicitly capturing structure from the dynamics (<xref ref-type="bibr" rid="bib5">Batty et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Bregler, 1997</xref>; <xref ref-type="bibr" rid="bib16">Costa et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Luxem et al., 2022a</xref>; <xref ref-type="bibr" rid="bib77">Shi et al., 2021</xref>; <xref ref-type="bibr" rid="bib83">Sun et al., 2021c</xref>). These models learn a continuous embedding that can be used to identify lower-dimensional trajectory dynamics that can be correlated to neuronal activity and segmented in relation to significant behavioral events.</p><p>Behavioral segmentation and other methods for quantification require a similar computing environment to that used for pose estimation. The input to those methods is generally the output of a pose estimation method (i.e., keypoint coordinates) or time series from a dimensionality reduction method such as principal component analysis that accounts for the keypoints or the raw video. It is crucial that pose estimation is accurate as the segmentation capabilities of the subsequent methods is bounded by pose tracking quality. Highly noisy key points will drown out biological signals and make the segmentation results hard to interpret, especially for unsupervised methods. Furthermore, identity switches between virtual markers can be catastrophic for multi-animal tracking and segmentation. A summary of methods for behavioral segmentation is provided in <xref ref-type="table" rid="table3">Table 3</xref>.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Methods for behavioral segmentation using pose data.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/sgoldenlab/simba">SimBA</ext-link></td><td align="left" valign="top">SimBA (<xref ref-type="bibr" rid="bib53">Nilsson et al., 2020a</xref>; <xref ref-type="bibr" rid="bib54">Nilsson et al., 2020b</xref>) is a supervised learning pipeline for importing pose estimation data and a graphical interface for interacting with a popular machine learning algorithm called Random Forest (<xref ref-type="bibr" rid="bib12">Breiman, 2001</xref>). SimBA was developed for studies in social behavior and aggression and has been shown to be able to discriminate between attack, pursuit, and threat behaviors in studies using rats and mice.</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/MARS">MARS</ext-link></td><td align="left" valign="top">MARS (<xref ref-type="bibr" rid="bib75">Segalin et al., 2021a</xref>; <xref ref-type="bibr" rid="bib76">Segalin et al., 2021b</xref>) is another supervised learning pipeline developed for studies of social interaction behaviors in rodents, such as attacking, mounting, and sniffing, and uses the XGBoost gradient boosting classifier (<xref ref-type="bibr" rid="bib14">Chen and Guestrin, 2016</xref>).</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/YttriLab/B-SOID">B-SOiD</ext-link></td><td align="left" valign="top">B-SOiD (<xref ref-type="bibr" rid="bib30">Hsu and Yttri, 2021a</xref>; <xref ref-type="bibr" rid="bib31">Hsu and Yttri, 2021b</xref>) uses unsupervised methods to learn and discover the spatiotemporal features in pose data of ongoing behaviors, such as grooming and other naturalistic movements in rodents, flies, or humans. B-SOiD uses UMAP embedding (<xref ref-type="bibr" rid="bib49">McInnes et al., 2020</xref>) to account for dynamic features within video frames that are grouped using an algorithm for cluster analysis, HDBSCAN (<xref ref-type="bibr" rid="bib48">McInnes et al., 2017</xref>). Clustered spatiotemporal features are then used to train a classifier (Random Forest; <xref ref-type="bibr" rid="bib12">Breiman, 2001</xref>) to detect behavioral classes in data sets that were not used to train the model and with millisecond precision.</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/LINCellularNeuroscience/VAME">VAME</ext-link></td><td align="left" valign="top">VAME (<xref ref-type="bibr" rid="bib42">Luxem et al., 2022a</xref>; <xref ref-type="bibr" rid="bib43">Luxem et al., 2022b</xref>) uses self-supervised deep learning models to infer the full range of behavioral dynamics based on the animal movements from pose data. The variational autoencoder framework (<xref ref-type="bibr" rid="bib37">Kingma and Welling, 2019</xref>) is used to learn a generative model. An encoder network learns a representation from the original data space into a latent space. A decoder network learns to decode samples from this space back into the original data space. The encoder and decoder are parameterized with recurrent neural networks. Once trained, the learned latent space is parameterized by a Hidden Markov Model to obtain behavioral motifs.</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/TREBA">TREBA</ext-link></td><td align="left" valign="top">TREBA (<xref ref-type="bibr" rid="bib83">Sun et al., 2021c</xref>; <xref ref-type="bibr" rid="bib84">Sun et al., 2021d</xref>) relates measures from pose estimation to other quantitative or qualitative data associated with each frame in a video recording. Similar to VAME, a neural network is trained to learn to predict movement trajectories in an unsupervised manner. TREBA can then incorporate behavioral attributes, such as movement speed, distance traveled, and heuristic labels for behavior (e.g., sniffing, mounting, attacking) into representations of the pose estimation data learned by its neural networks, thereby bringing aspects of supervised learning. This is achieved using a technique called task programming.</td></tr></tbody></table></table-wrap><p>Before selecting any approach to segment animal behavior, it is important to first define the desired outcome. If the goal is to identify episodes of well-defined behaviors like rearing or walking, then the most straightforward approach is to use a supervised method. Moreover, it is generally a good starting point to use a supervised learning approach and the outputs of these models can be layered on top of unsupervised models to give them immediate interpretability. One tradeoff, however, is the extensive training datasets that are often required to ensure good supervised segmentation. Such methods can be established quite easily using standard machine learning libraries available for the Python, R, and MATLAB, if one has already experience in building these methods. Alternatively, open-source packages such as SimBA (<xref ref-type="bibr" rid="bib53">Nilsson et al., 2020a</xref>) or MARS (<xref ref-type="bibr" rid="bib75">Segalin et al., 2021a</xref>) can be used, and is especially beneficial for those who are relatively new to the topic of machine learning. However, if the researcher wants to understand more about the spatiotemporal structure of the behaving animal, they either need to label many different behaviors within the video or turn to unsupervised methods. Unsupervised methods offer the advantage to identify clusters in the video or keypoint time series and quantify behavior in each frame. Recently, A-SOiD, an active-learning algorithm, iteratively combines these supervised and unsupervised approaches to reduce the amount of training data required and enable the discovery of additional behavior and structure (<xref ref-type="bibr" rid="bib73">Schweihoff et al., 2022</xref>).</p><p>Interpreting the lower-dimensional structures in a 2D/3D projection plot can be difficult and it is advised to visualize examples from this projection space. Generative methods like VAME offer the possibility to sample cluster categories from this embedding space to qualitatively check if similar patterns are learned. Another task unsupervised methods are capable of is fingerprinting. Here, the embedding space is used as a signature to discern general changes in phenotypes (<xref ref-type="bibr" rid="bib92">Wiltschko et al., 2020</xref>). An alternative to using an explicitly supervised or unsupervised approach is to combine these approaches (semi-supervised), as implemented in a package called TREBA (<xref ref-type="bibr" rid="bib83">Sun et al., 2021c</xref>). TREBA uses generative modeling in addition to incorporating behavioral attributes, such as movement speed, distance traveled, or heuristic labels for behavior (e.g., sniffing, mounting, attacking) into learned behavioral representations. It has been used in a number of different experimental contexts, most notably for understanding social interactions between animals.</p><p>Finally, as behavior is highly hierarchically structured, multiple spatio-temporal scales of description may be desired, for example, to account for bouts of locomotion and transitions running to escaping behavior (<xref ref-type="bibr" rid="bib9">Berman, 2018</xref>). It is possible to create a network representation and identify ‘cliques’ or ‘communities’ on the resulting graph (<xref ref-type="bibr" rid="bib42">Luxem et al., 2022a</xref>; <xref ref-type="bibr" rid="bib44">Markowitz et al., 2018</xref>). These descriptions represent human identifiable behavioral categories within highly interconnected sub-second segments of behavior. These representations can provide insights into the connection between different behavioral states and the transitions between states and their biological meaning.</p></sec></sec><sec id="s3"><title>Best practices for experimenters and developers</title><p>Having described how to set up and use video recording methods and analysis methods for pose estimation, we would like to close by discussing some best practices in the use and development of methods for video analysis, including recommendations for the open sharing of video data and analysis code.</p><sec id="s3-1"><title>Best practices for experimenters</title><p>For those using video analysis methods in a laboratory setting, there are several key issues that should be followed as best practices. It is most crucial to develop a means of storing files in a manner in which they can be accessed in the lab, through cloud computing resources, and in data archives. These issues are discussed above in the ‘Hardware and software for data analysis’ section of this paper. Documentation of hardware is also a key best practice. All methods sections of manuscripts that use methods for video analysis should include details on the camera and lens that were used, the locations of and distances from the cameras relative to the behavioral arena, the acquisition rate and image resolution, environmental lighting (e.g., IR grids placed above the behavioral arena), properties of the arena (size, material, color, etc.).</p><p>Beyond within-lab data management and reporting details on hardware used in research manuscripts, more widespread sharing of video data is very much needed and is a core aspect of best practices for experimenters. In accordance with the demands of funders such as the <ext-link ext-link-type="uri" xlink:href="https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html">NIH</ext-link> for data sharing, the open sharing of raw and processed videos and pose tracking data is crucial for research reproducibility and also for training new users on video methods. Several groups have created repositories to address this need (<ext-link ext-link-type="uri" xlink:href="https://sites.google.com/view/computational-behavior/home">Computational Behavior</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://edspace.american.edu/openbehavior/video-repository/video-repository-2/">OpenBehavior</ext-link>). With widespread use, these repositories will help new users learn the required methods for data analysis, enable new analyses of existing datasets that could lead to new findings without having to do new experiments, and would enable comparisons of existing and newly developed methods for pose estimation and behavioral quantification. The latter benefit of data sharing could lead to insight into a major open question about methods for animal pose estimation: how choices about the parameters of any method for pose estimation or subsequent analysis impact analysis time, accuracy, and generalizability. Without these resources, it has not been possible to make confident statements about how existing methods compare across a wide range of datasets involving multiple types of research animals and in different experimental contexts. Guidance for how to implement data sharing can be found in several existing efforts of the machine learning community (<xref ref-type="bibr" rid="bib23">Gebru et al., 2021</xref>; <xref ref-type="bibr" rid="bib32">Hutchinson et al., 2021</xref>; <xref ref-type="bibr" rid="bib78">Stoyanovich and Howe, 2019</xref>). A more widespread use of these frameworks for sharing data can improve the transparency and accessibility of research data for video analysis.</p></sec><sec id="s3-2"><title>Best practices for developers</title><p>We recommend three topics receive more attention by developers of methods for video analysis. First, there is a need for a common file format for storing results from pose estimation. Second, there is a need for methods to compare pose estimation packages and assess the impact of the parameters of each package on performance in terms of accuracy and user time. Third, there is a need for better code documentation and analysis reproducibility. Each of these issues is discussed below. In addition to these topics, we would like to encourage developers to design interfaces to make their tools more accessible to novice users. This will allow the tools to become more widely used and studied, and will further not limit use of the tools to researchers with advanced technical skills such as programming.</p><p>First, it is important to point out that there is no common and efficient data format available for tools that enable pose estimation in animal research. Such a format would allow users to compare methods without having to recode their video data. The FAIR data principles (<xref ref-type="bibr" rid="bib91">Wilkinson et al., 2016</xref>) are particularly apt for developing a common data format for video due to the large heterogeneity of data sources, intermediate analysis outputs, and end goals of the study. These principles call for data to be Findable (available in searchable repositories and with persistent and citable identifiers [DOIs]), Accessible (easily retrieved using the Internet), Interoperable (having a common set of terms to describe video data across datasets), and Reusable (containing information about the experimental conditions and outputs of any analysis or model to allow another group to readily make use of the data). A common file format for saving raw and processed video recordings and data from pose estimation models is needed to address these issues.</p><p>Second, there has also been a general lack of direct comparisons of different methods and parameter exploration within a given method on a standard set of videos. The choice of deep learning method and specific hyperparameters can affect the structural biases embedded in video data, thereby affecting the effectiveness of a given method (<xref ref-type="bibr" rid="bib74">Sculley et al., 2015</xref>). Yet, it seems that many users stick to default parameters available in popular packages. For example, in pose estimation, certain properties of neural network architectures such as its maximum receptive field size can dramatically impact the performance across species owing to the variability in morphological features (<xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>). In addition to the intrinsic properties of particular species (e.g., <xref ref-type="bibr" rid="bib28">Hayden et al., 2022</xref>), the analysis type will also dictate the importance of particular parameters on the task performance. For example, algorithms that achieve temporal smoothness in pose tracking are crucial for studies of fine motor control (<xref ref-type="bibr" rid="bib93">Wu et al., 2020</xref>), but perhaps not as essential as preventing identity swaps for studies of social behavior (<xref ref-type="bibr" rid="bib63">Pereira et al., 2022</xref>; <xref ref-type="bibr" rid="bib75">Segalin et al., 2021a</xref>). Another important issue is that most methods do not report well-calibrated measures of the confidence of model fits or predictions. This is important as it has become clear that machine learning tools tend to be overconfident in their predictions (<xref ref-type="bibr" rid="bib1">Abdar et al., 2021</xref>). Establishing standardized, interoperable data formats and datasets that include estimates of the fitted models and their predictions will enable comprehensive comparisons of existing and new methods for pose estimation and behavioral quantification.</p><p>For evaluating specific methods on lab-specific data, appropriate metrics and baseline methods for the research questions should be chosen. There may be cases where comparable baseline methods may not exist. For example, if a lab develops a new method for quantifying behavior for a specific organism or task on a lab-specific dataset, and there are no existing studies for that task. However, if related methods exist, it would be beneficial to compare performance of the new method against existing methods to study the advantages and disadvantages of the method. For more general claims (e.g., state-of-the-art pose estimator across organisms), evaluations on existing datasets and comparisons with baselines is important (see <xref ref-type="table" rid="table4">Table 4</xref>), to demonstrate the generality of the method and improvements over existing methods. A consensus on a standard set of data in the community for evaluation and an expansion to include more widely used behavioral tasks and assays would facilitate general model development and comparison. We show existing datasets in the community for method development in <xref ref-type="table" rid="table4">Table 4</xref> and encourage the community to continue to open-source data and expand this list of available datasets to accelerate model development.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Datasets for model development.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Dataset</th><th align="left" valign="top">Task</th><th align="left" valign="top">Setting</th><th align="left" valign="top">Organism</th></tr></thead><tbody><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation</td><td align="left" valign="top">Videos from 4 camera views with poses from motion capture</td><td align="left" valign="top">Human (single-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://cocodataset.org/#home">MS COCO</ext-link></td><td align="left" valign="top">2D Pose Estimation</td><td align="left" valign="top">Images from uncontrolled settings with annotated poses</td><td align="left" valign="top">Human (multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1710.10000.pdf">PoseTrack</ext-link></td><td align="left" valign="top">2D Pose Estimation &amp; Tracking</td><td align="left" valign="top">Videos from crowded scenes with annotated poses</td><td align="left" valign="top">Human (multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://openreview.net/pdf?id=rH8yliN6C83">AP-10K</ext-link></td><td align="left" valign="top">2D Pose Estimation</td><td align="left" valign="top">Images of diverse animal species with annotated poses</td><td align="left" valign="top">Diverse species (single &amp; multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/63720">MARS</ext-link></td><td align="left" valign="top">2D Pose Estimation</td><td align="left" valign="top">Videos from 2 camera views with annotated poses</td><td align="left" valign="top">Mouse (multi-agent)</td></tr><tr><td align="char" char="hyphen" valign="top"><ext-link ext-link-type="uri" xlink:href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pedersen_3D-ZeF_A_3D_Zebrafish_Tracking_Benchmark_Dataset_CVPR_2020_paper.pdf">3D-ZEF</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation &amp; Tracking</td><td align="left" valign="top">Videos from 2 camera views with annotated poses</td><td align="left" valign="top">Zebrafish (multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="http://openmonkeystudio.com/data.html">OpenMonkeyStudio</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation</td><td align="left" valign="top">Images with annotated poses from a 62 camera setup</td><td align="left" valign="top">Monkey (single-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.11.23.469743v1.full.pdf">PAIR-R24M</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation &amp; Tracking</td><td align="left" valign="top">Videos from 12 camera views with poses from motion capture</td><td align="left" valign="top">Rat (multi-agent)</td></tr><tr><td align="char" char="." valign="top"><ext-link ext-link-type="uri" xlink:href="https://virtualhumans.mpi-inf.mpg.de/3DPW/">3DPW</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation &amp; Tracking</td><td align="left" valign="top">Videos from moving phone camera in challenging outdoor settings</td><td align="left" valign="top">Human (multi-agent)</td></tr><tr><td align="char" char="." valign="top"><ext-link ext-link-type="uri" xlink:href="https://vcai.mpi-inf.mpg.de/3dhp-dataset/">3DHP</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation</td><td align="left" valign="top">Videos from 14 camera views with poses from motion capture</td><td align="left" valign="top">Human (single-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://figshare.com/collections/Rat_7M/5295370/3">Rat 7M</ext-link></td><td align="left" valign="top">2D/3D Pose Estimation</td><td align="left" valign="top">Videos from 12 camera views with poses from motion capture</td><td align="left" valign="top">Rat (single-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://deepmind.com/research/open-source/kinetics">Kinetics</ext-link></td><td align="left" valign="top">Video-level Action Classification</td><td align="left" valign="top">Videos from uncontrolled settings that cover 700 human actions</td><td align="left" valign="top">Human (single &amp; agent, may interact with other organisms/objects)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1604.02808.pdf">NTU-RGBD</ext-link></td><td align="left" valign="top">Video-level Action Classification (also has 3D poses)</td><td align="left" valign="top">Videos from 80 views and depth with 60 human actions</td><td align="left" valign="top">Human (single &amp; multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://ai.stanford.edu/~syyeung/everymoment.html">MultiTHUMOS</ext-link></td><td align="left" valign="top">Frame-level Action Classification</td><td align="left" valign="top">Videos from uncontrolled settings with 65 action classes</td><td align="left" valign="top">Human (single &amp; multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.microsoft.com/en-us/research/wp-content/uploads/2012/06/CVPR12behavior.pdf">CRIM13</ext-link></td><td align="left" valign="top">Frame-level Behavior Classification</td><td align="left" valign="top">Videos from 2 views, with 13 annotated social behaviors</td><td align="left" valign="top">Mouse (multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://caltechdata.tind.io/records/1893">Fly vs. Fly</ext-link></td><td align="left" valign="top">Frame-level Behavior Classification (also has 2D poses)</td><td align="left" valign="top">Videos &amp; trajectory, with 10 annotated social behaviors</td><td align="left" valign="top">Fly (multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/2104.02710.pdf">CalMS21</ext-link></td><td align="left" valign="top">Frame-level Behavior Classification (also has 2D poses)</td><td align="left" valign="top">Videos &amp; trajectory, with 10 annotated social behaviors</td><td align="left" valign="top">Mouse (multi-agent)</td></tr><tr><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications">MABe</ext-link></td><td align="left" valign="top">Frame-level Behavior Classification (also has 2D poses)</td><td align="left" valign="top">Top-down views, 7 annotated keypoints, hundreds of videos</td><td align="left" valign="top">Mouse (multi-agent)</td></tr></tbody></table></table-wrap><p>Third, reproducibility of results is crucial for acceptance of new methods for video analysis within the research community and for research transparency. Guidance for documenting the details of models and algorithms can be obtained from the <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/2003.12206.pdf">Machine Learning Reproducibility Checklist</ext-link>. It is applicable to any computational model in general. Importantly, the checklist calls for including the range of hyperparameters considered for experiments, mean and variance of results from multiple runs, and an explanation of how samples were allocated for train/validation/test. Further guidance for sharing code is available in this GitHub resource: <ext-link ext-link-type="uri" xlink:href="https://github.com/paperswithcode/releasing-research-code">Publishing Research Code</ext-link>. It provides tips on open-sourcing research code, including specifications of code dependencies, training and evaluation code, and including pre-trained models as part of any code repository. Beyond these resources, we note that there is also a broader definition of reproducibility in that experiments should be <italic>robustly reproducible</italic>: experimental results should ideally not vary significantly under minor perturbations. For example, even if there are minor variations to lighting or arena size from the original experiments, the video analysis results should not change significantly. A framework to ensure robust reproducibility is currently an open question, but the existing frameworks should facilitate producing the same results under the same experimental conditions. <italic>Model interpretability</italic> is another important consideration depending on the purpose of the video analysis experiment. Many machine learning models are ‘black box’ models, and not easily interpretable; as such, post hoc explanations may not always be reliable (<xref ref-type="bibr" rid="bib66">Rudin, 2019</xref>). One way to generate human-interpretable models is through program synthesis (<xref ref-type="bibr" rid="bib4">Balog et al., 2017</xref>) and neurosymbolic learning (<xref ref-type="bibr" rid="bib85">Sun et al., 2022</xref>; <xref ref-type="bibr" rid="bib94">Zhan et al., 2021</xref>). These methods learn compositions of symbolic primitives, which are closer in form to human-constructed models than neural networks. Interpretable models can facilitate reproducibility and trustworthiness in model predictions for scientific applications. Efforts at deploying these approaches for methods for video analysis and behavioral quantification are very much needed.</p></sec></sec><sec id="s4"><title>Summary</title><p>We hope that our review of the current state of open-source tools for behavioral video analysis will be helpful to the community. We described how to set up video methods in a lab, provided an overview on currently available methods, and provided guidance for best practices in using and developing the methods. As newer tools emerge and more research groups become proficient at using available methods, there is a clear potential for the tools to help with advancing our understanding of the neural basis of behavior.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Funding acquisition, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><ack id="ack"><title>Acknowledgements</title><p>This paper emerged from a working group on methods for video analysis organized by the OpenBehavior project in the summer and fall of 2021. Ann Kennedy, Greg Corder, and Sam Golden were major contributors to the working group and their ideas impacted this manuscript. We would like to thank Ann Kennedy, Samantha White, and Jensen Palmer for helpful comments on the manuscript. NSERC Award #PGSD3-532647-2019 to JJS; NIH MH002952 for SPB; NIH MH124042 for KK; NIH MH128177 and NSF 2024581 to JZ; NSF 1948181 and NIH DA046375 to ML.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdar</surname><given-names>M</given-names></name><name><surname>Pourpanah</surname><given-names>F</given-names></name><name><surname>Hussain</surname><given-names>S</given-names></name><name><surname>Rezazadegan</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Ghavamzadeh</surname><given-names>M</given-names></name><name><surname>Fieguth</surname><given-names>P</given-names></name><name><surname>Cao</surname><given-names>X</given-names></name><name><surname>Khosravi</surname><given-names>A</given-names></name><name><surname>Acharya</surname><given-names>UR</given-names></name><name><surname>Makarenkov</surname><given-names>V</given-names></name><name><surname>Nahavandi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A review of uncertainty quantification in deep learning: techniques, applications and challenges</article-title><source>Information Fusion</source><volume>76</volume><fpage>243</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2021.05.008</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Toward a science of computational ethology</article-title><source>Neuron</source><volume>84</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id><pub-id pub-id-type="pmid">25277452</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bala</surname><given-names>PC</given-names></name><name><surname>Eisenreich</surname><given-names>BR</given-names></name><name><surname>Yoo</surname><given-names>SBM</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automated markerless pose estimation in freely moving macaques with openmonkeystudio</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4560</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18441-5</pub-id><pub-id pub-id-type="pmid">32917899</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Balog</surname><given-names>M</given-names></name><name><surname>Gaunt</surname><given-names>AL</given-names></name><name><surname>Brockschmidt</surname><given-names>M</given-names></name><name><surname>Nowozin</surname><given-names>S</given-names></name><name><surname>Tarlow</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DeepCoder: Learning to Write Programs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1611.01989">https://arxiv.org/abs/1611.01989</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Batty</surname><given-names>E</given-names></name><name><surname>Whiteway</surname><given-names>M</given-names></name><name><surname>Saxena</surname><given-names>S</given-names></name><name><surname>Biderman</surname><given-names>D</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Gillis</surname><given-names>W</given-names></name><name><surname>Markowitz</surname><given-names>J</given-names></name><name><surname>Churchland</surname><given-names>A</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Linderman</surname><given-names>S</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>BehaveNet: nonlinear embedding and bayesian neural decoding of behavioral videos</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Behnke</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Hierarchical neural networks for image interpretation</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/b11963</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Shaul</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>OptiMouse: a comprehensive open source program for reliable detection and analysis of mouse body and nose positions</article-title><source>BMC Biology</source><volume>15</volume><elocation-id>41</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-017-0377-3</pub-id><pub-id pub-id-type="pmid">28506280</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Choi</surname><given-names>DM</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id><pub-id pub-id-type="pmid">25142523</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Measuring behavior across scales</article-title><source>BMC Biology</source><volume>16</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-018-0494-7</pub-id><pub-id pub-id-type="pmid">29475451</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The OpenCV library</article-title><source>Dr Dobb’s Journal of Software Tools</source><volume>120</volume><fpage>122</fpage><lpage>125</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bregler</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Learning and recognizing human dynamics in video sequences</article-title><conf-name>Presented at the Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><fpage>568</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1109/CVPR.1997.609382</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Random forests</article-title><source>Machine Learning</source><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Simon</surname><given-names>T</given-names></name><name><surname>Wei</surname><given-names>SE</given-names></name><name><surname>Sheikh</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Honolulu, HI</conf-loc><fpage>7291</fpage><lpage>7299</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.143</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>XGBoost: A Scalable Tree Boosting System</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.02754">https://arxiv.org/abs/1603.02754</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Eva Zhang</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Fang</surname><given-names>HS</given-names></name><name><surname>Rock</surname><given-names>RR</given-names></name><name><surname>Bal</surname><given-names>A</given-names></name><name><surname>Padilla-Coreano</surname><given-names>N</given-names></name><name><surname>Keyes</surname><given-names>L</given-names></name><name><surname>Tye</surname><given-names>KM</given-names></name><name><surname>Lu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>AlphaTracker: A Multi-Animal Tracking and Behavioral Analysis Tool</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.04.405159</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>AC</given-names></name><name><surname>Ahamed</surname><given-names>T</given-names></name><name><surname>Stephens</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptive, locally linear models of complex dynamics</article-title><source>PNAS</source><volume>116</volume><fpage>1501</fpage><lpage>1510</lpage><pub-id pub-id-type="doi">10.1073/pnas.1813476116</pub-id><pub-id pub-id-type="pmid">30655347</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowen</surname><given-names>SL</given-names></name><name><surname>Davis</surname><given-names>GA</given-names></name><name><surname>Nitz</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Anterior cingulate neurons in the rat MAP anticipated effort and reward to their associated action sequences</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>2393</fpage><lpage>2407</lpage><pub-id pub-id-type="doi">10.1152/jn.01012.2011</pub-id><pub-id pub-id-type="pmid">22323629</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crall</surname><given-names>JD</given-names></name><name><surname>Gravish</surname><given-names>N</given-names></name><name><surname>Mountcastle</surname><given-names>AM</given-names></name><name><surname>Combes</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>BEEtag: a low-cost, image-based tracking system for the study of animal behavior and locomotion</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0136487</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0136487</pub-id><pub-id pub-id-type="pmid">26332211</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>TL</given-names></name><name><surname>Capaldi</surname><given-names>ED</given-names></name><name><surname>Myers</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Effects of reward magnitude on running speed following a deprivation upshift</article-title><source>Bulletin of the Psychonomic Society</source><volume>15</volume><fpage>150</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.3758/BF03334493</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>TW</given-names></name><name><surname>Marshall</surname><given-names>JD</given-names></name><name><surname>Severson</surname><given-names>KS</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Hildebrand</surname><given-names>DGC</given-names></name><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Wang</surname><given-names>WL</given-names></name><name><surname>Gellis</surname><given-names>AJ</given-names></name><name><surname>Carlson</surname><given-names>DE</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Geometric deep learning enables 3D kinematic profiling across species and environments</article-title><source>Nature Methods</source><volume>18</volume><fpage>564</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01106-6</pub-id><pub-id pub-id-type="pmid">33875887</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dutta</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The VIA Annotation Software for Images, Audio and Video</article-title><conf-name>MM ’19</conf-name><conf-loc>Nice France</conf-loc><fpage>2276</fpage><lpage>2279</lpage><pub-id pub-id-type="doi">10.1145/3343031.3350535</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Sosulski</surname><given-names>DL</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Representation of spatial goals in rat orbitofrontal cortex</article-title><source>Neuron</source><volume>51</volume><fpage>495</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.032</pub-id><pub-id pub-id-type="pmid">16908414</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gebru</surname><given-names>T</given-names></name><name><surname>Morgenstern</surname><given-names>J</given-names></name><name><surname>Vecchione</surname><given-names>B</given-names></name><name><surname>Vaughan</surname><given-names>JW</given-names></name><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Iii</surname><given-names>HD</given-names></name><name><surname>Crawford</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Datasheets for datasets</article-title><source>Communications of the ACM</source><volume>64</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1145/3458723</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosztolai</surname><given-names>A</given-names></name><name><surname>Günel</surname><given-names>S</given-names></name><name><surname>Lobato-Ríos</surname><given-names>V</given-names></name><name><surname>Pietro Abrate</surname><given-names>M</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals</article-title><source>Nature Methods</source><volume>18</volume><fpage>975</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01226-z</pub-id><pub-id pub-id-type="pmid">34354294</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019b</year><data-title>DeepPoseKit</data-title><version designator="v0.3.6">v0.3.6</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/DeepPoseKit">https://github.com/jgraving/DeepPoseKit</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Günel</surname><given-names>S</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Campagnolo</surname><given-names>J</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic></article-title><source>eLife</source><volume>8</volume><elocation-id>e48571</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48571</pub-id><pub-id pub-id-type="pmid">31584428</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Automated pose estimation in primates</article-title><source>American Journal of Primatology</source><volume>84</volume><elocation-id>e23348</elocation-id><pub-id pub-id-type="doi">10.1002/ajp.23348</pub-id><pub-id pub-id-type="pmid">34855257</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5188</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id><pub-id pub-id-type="pmid">34465784</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>B-SOID</data-title><version designator="3.0">3.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/YttriLab/B-SOID">https://github.com/YttriLab/B-SOID</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hutchinson</surname><given-names>B</given-names></name><name><surname>Smart</surname><given-names>A</given-names></name><name><surname>Hanna</surname><given-names>A</given-names></name><name><surname>Denton</surname><given-names>E</given-names></name><name><surname>Greer</surname><given-names>C</given-names></name><name><surname>Kjartansson</surname><given-names>O</given-names></name><name><surname>Barnes</surname><given-names>P</given-names></name><name><surname>Mitchell</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Towards Accountability for Machine Learning Datasets</article-title><conf-name>FAccT ’21</conf-name><conf-loc>Virtual Event Canada</conf-loc><fpage>560</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1145/3442188.3445918</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Rivera-Alba</surname><given-names>M</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>GA</given-names></name><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Saunders</surname><given-names>JL</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Real-Time, low-latency closed-loop feedback using markerless posture tracking</article-title><source>eLife</source><volume>9</volume><elocation-id>e61909</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61909</pub-id><pub-id pub-id-type="pmid">33289631</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karashchuk</surname><given-names>P</given-names></name><name><surname>Rupp</surname><given-names>KL</given-names></name><name><surname>Dickinson</surname><given-names>ES</given-names></name><name><surname>Walling-Bell</surname><given-names>S</given-names></name><name><surname>Sanders</surname><given-names>E</given-names></name><name><surname>Azim</surname><given-names>E</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Tuthill</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Anipose: a toolkit for robust markerless 3D pose estimation</article-title><source>Cell Reports</source><volume>36</volume><elocation-id>109730</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109730</pub-id><pub-id pub-id-type="pmid">34592148</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshavarzi</surname><given-names>S</given-names></name><name><surname>Bracey</surname><given-names>EF</given-names></name><name><surname>Faville</surname><given-names>RA</given-names></name><name><surname>Campagner</surname><given-names>D</given-names></name><name><surname>Tyson</surname><given-names>AL</given-names></name><name><surname>Lenzi</surname><given-names>SC</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name><name><surname>Margrie</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multisensory coding of angular head velocity in the retrosplenial cortex</article-title><source>Neuron</source><volume>110</volume><fpage>532</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.10.031</pub-id><pub-id pub-id-type="pmid">34788632</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An introduction to variational autoencoders</article-title><source>Foundations and Trends in Machine Learning</source><volume>12</volume><fpage>307</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1561/2200000056</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lardeux</surname><given-names>S</given-names></name><name><surname>Pernaud</surname><given-names>R</given-names></name><name><surname>Paleressompoulle</surname><given-names>D</given-names></name><name><surname>Baunez</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Beyond the reward pathway: coding reward magnitude and error in the rat subthalamic nucleus</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>2526</fpage><lpage>2537</lpage><pub-id pub-id-type="doi">10.1152/jn.91009.2008</pub-id><pub-id pub-id-type="pmid">19710371</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>S</given-names></name><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Nath</surname><given-names>T</given-names></name><name><surname>Rahman</surname><given-names>MM</given-names></name><name><surname>Di Santo</surname><given-names>V</given-names></name><name><surname>Soberanes</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Lauder</surname><given-names>G</given-names></name><name><surname>Dulac</surname><given-names>C</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-Animal Pose Estimation and Tracking with DeepLabCut</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.30.442096</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazão</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><name><surname>Calcaterra</surname><given-names>L</given-names></name><name><surname>Dreosti</surname><given-names>E</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luxem</surname><given-names>K</given-names></name><name><surname>Mocellin</surname><given-names>P</given-names></name><name><surname>Fuhrmann</surname><given-names>F</given-names></name><name><surname>Kürsch</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>SR</given-names></name><name><surname>Palop</surname><given-names>JJ</given-names></name><name><surname>Remy</surname><given-names>S</given-names></name><name><surname>Bauer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Identifying behavioral structure from deep variational embeddings of animal motion</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>1267</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-04080-7</pub-id><pub-id pub-id-type="pmid">36400882</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Luxem</surname><given-names>K</given-names></name><name><surname>Mocellin</surname><given-names>P</given-names></name><name><surname>Fuhrmann</surname><given-names>F</given-names></name><name><surname>Kürsch</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>SR</given-names></name><name><surname>Palop</surname><given-names>JJ</given-names></name><name><surname>Remy</surname><given-names>S</given-names></name><name><surname>Bauer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022b</year><data-title>VAME</data-title><version designator="3.0">3.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/LINCellularNeuroscience/VAME">https://github.com/LINCellularNeuroscience/VAME</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Beron</surname><given-names>CC</given-names></name><name><surname>Neufeld</surname><given-names>SQ</given-names></name><name><surname>Robertson</surname><given-names>K</given-names></name><name><surname>Bhagat</surname><given-names>ND</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Peterson</surname><given-names>E</given-names></name><name><surname>Hyun</surname><given-names>M</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The striatum organizes 3D behavior via moment-to-moment action selection</article-title><source>Cell</source><volume>174</volume><fpage>44</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.04.019</pub-id><pub-id pub-id-type="pmid">29779950</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name><name><surname>Nishihara</surname><given-names>HK</given-names></name><name><surname>Brenner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>200</volume><fpage>269</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018b</year><data-title>DeepLabCut</data-title><version designator="3.0">3.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">https://github.com/DeepLabCut/DeepLabCut</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Astels</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hdbscan: hierarchical density based clustering</article-title><source>The Journal of Open Source Software</source><volume>2</volume><elocation-id>205</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00205</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nevatia</surname><given-names>K</given-names></name><name><surname>Binford</surname><given-names>TO</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Structured descriptions of complex objects</article-title><conf-name>Proceedings of the 3rd international joint conference on artificial intelligence, IJCAI’73</conf-name><fpage>641</fpage><lpage>647</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stacked Hourglass Networks for Human Pose Estimation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.06937">https://arxiv.org/abs/1603.06937</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>SR</given-names></name><name><surname>Goodwin</surname><given-names>NL</given-names></name><name><surname>Choong</surname><given-names>JJ</given-names></name><name><surname>Hwang</surname><given-names>S</given-names></name><name><surname>Wright</surname><given-names>HR</given-names></name><name><surname>Norville</surname><given-names>ZC</given-names></name><name><surname>Tong</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Bentzley</surname><given-names>BS</given-names></name><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>McLaughlin</surname><given-names>RJ</given-names></name><name><surname>Golden</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Simple Behavioral Analysis (SimBA) – An Open Source Toolkit for Computer Classification of Complex Social Behaviors in Experimental Animals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.19.049452</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>SR</given-names></name><name><surname>Goodwin</surname><given-names>NL</given-names></name><name><surname>Choong</surname><given-names>JJ</given-names></name><name><surname>Hwang</surname><given-names>S</given-names></name><name><surname>Wright</surname><given-names>HR</given-names></name><name><surname>Norville</surname><given-names>ZC</given-names></name><name><surname>Tong</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Bentzley</surname><given-names>BS</given-names></name><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>McLaughlin</surname><given-names>RJ</given-names></name><name><surname>Golden</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2020">2020b</year><data-title>Simba</data-title><version designator="v1.3">v1.3</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/sgoldenlab/simba">https://github.com/sgoldenlab/simba</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>KS</given-names></name><name><surname>Jung</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Gpu implementation of neural networks</article-title><source>Pattern Recognition</source><volume>37</volume><fpage>1311</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2004.01.013</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Avni</surname><given-names>O</given-names></name><name><surname>Taylor</surname><given-names>AL</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Roian Egnor</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Automated multi-day tracking of marked mice for the analysis of social behaviour</article-title><source>Journal of Neuroscience Methods</source><volume>219</volume><fpage>10</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.05.013</pub-id><pub-id pub-id-type="pmid">23810825</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okubo</surname><given-names>TS</given-names></name><name><surname>Patella</surname><given-names>P</given-names></name><name><surname>D’Alessandro</surname><given-names>I</given-names></name><name><surname>Wilson</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A neural network for wind-guided COMPASS navigation</article-title><source>Neuron</source><volume>107</volume><fpage>924</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.022</pub-id><pub-id pub-id-type="pmid">32681825</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ottenheimer</surname><given-names>DJ</given-names></name><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Sutlief</surname><given-names>E</given-names></name><name><surname>Fraser</surname><given-names>KM</given-names></name><name><surname>Kim</surname><given-names>TH</given-names></name><name><surname>Richard</surname><given-names>JM</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A quantitative reward prediction error signal in the ventral pallidum</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1267</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0688-5</pub-id><pub-id pub-id-type="pmid">32778791</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panadeiro</surname><given-names>V</given-names></name><name><surname>Rodriguez</surname><given-names>A</given-names></name><name><surname>Henry</surname><given-names>J</given-names></name><name><surname>Wlodkowic</surname><given-names>D</given-names></name><name><surname>Andersson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A review of 28 free animal-tracking software applications: Current features and limitations</article-title><source>Lab Animal</source><volume>50</volume><fpage>246</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1038/s41684-021-00811-1</pub-id><pub-id pub-id-type="pmid">34326537</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>ZT</given-names></name><name><surname>Dong</surname><given-names>Z</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Vetere</surname><given-names>LM</given-names></name><name><surname>Page-Harley</surname><given-names>L</given-names></name><name><surname>Shuman</surname><given-names>T</given-names></name><name><surname>Cai</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>EzTrack: An open-source video analysis pipeline for the investigation of animal behavior</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>19979</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-56408-9</pub-id><pub-id pub-id-type="pmid">31882950</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>T.D</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Willmore</surname><given-names>L</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>SSH</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>T.D</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Tabris</surname><given-names>N</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>DM</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Ravindranath</surname><given-names>S</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Normand</surname><given-names>E</given-names></name><name><surname>Deutsch</surname><given-names>DS</given-names></name><name><surname>Wang</surname><given-names>ZY</given-names></name><name><surname>McKenzie-Smith</surname><given-names>GC</given-names></name><name><surname>Mitelut</surname><given-names>CC</given-names></name><name><surname>Castro</surname><given-names>MD</given-names></name><name><surname>D’Uva</surname><given-names>J</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name><name><surname>Kocher</surname><given-names>SD</given-names></name><name><surname>Wang</surname><given-names>SSH</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title><source>Nature Methods</source><volume>19</volume><fpage>486</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id><pub-id pub-id-type="pmid">35379947</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rameau</surname><given-names>F</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Bailo</surname><given-names>O</given-names></name><name><surname>Kweon</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>MC-calib: A generic and robust calibration toolbox for multi-camera systems</article-title><source>Computer Vision and Image Understanding</source><volume>217</volume><elocation-id>103353</elocation-id><pub-id pub-id-type="doi">10.1016/j.cviu.2021.103353</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vicarious trial and error</article-title><source>Nature Reviews. Neuroscience</source><volume>17</volume><fpage>147</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1038/nrn.2015.30</pub-id><pub-id pub-id-type="pmid">26891625</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</article-title><source>Nature Machine Intelligence</source><volume>1</volume><fpage>206</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0048-x</pub-id><pub-id pub-id-type="pmid">35603010</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Image net large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salem</surname><given-names>GH</given-names></name><name><surname>Dennis</surname><given-names>JU</given-names></name><name><surname>Krynitsky</surname><given-names>J</given-names></name><name><surname>Garmendia-Cedillos</surname><given-names>M</given-names></name><name><surname>Swaroop</surname><given-names>K</given-names></name><name><surname>Malley</surname><given-names>JD</given-names></name><name><surname>Pajevic</surname><given-names>S</given-names></name><name><surname>Abuhatzira</surname><given-names>L</given-names></name><name><surname>Bustin</surname><given-names>M</given-names></name><name><surname>Gillet</surname><given-names>JP</given-names></name><name><surname>Gottesman</surname><given-names>MM</given-names></name><name><surname>Mitchell</surname><given-names>JB</given-names></name><name><surname>Pohida</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>SCORHE: A novel and practical approach to video monitoring of laboratory mice housed in vivarium cage racks</article-title><source>Behavior Research Methods</source><volume>47</volume><fpage>235</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.3758/s13428-014-0451-5</pub-id><pub-id pub-id-type="pmid">24706080</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schonberger</surname><given-names>JL</given-names></name><name><surname>Frahm</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Structure-from-motion revisited</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><fpage>4104</fpage><lpage>4113</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.445</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Zheng</surname><given-names>E</given-names></name><name><surname>Frahm</surname><given-names>JM</given-names></name><name><surname>Pollefeys</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Pixelwise view selection for unstructured multi-view stereo in</chapter-title><person-group person-group-type="editor"><name><surname>Leibe</surname><given-names>B</given-names></name><name><surname>Matas</surname><given-names>J</given-names></name><name><surname>Sebe</surname><given-names>N</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><source>Computer vision – ECCV 2016, Lecture notes in computer science</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>501</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46487-9_31</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schorscher-Petcu</surname><given-names>A</given-names></name><name><surname>Takács</surname><given-names>F</given-names></name><name><surname>Browne</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Scanned optogenetic control of mammalian somatosensory input to map input-specific behavioral outputs</article-title><source>eLife</source><volume>10</volume><elocation-id>e62026</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.62026</pub-id><pub-id pub-id-type="pmid">34323214</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweihoff</surname><given-names>JF</given-names></name><name><surname>Loshakov</surname><given-names>M</given-names></name><name><surname>Pavlova</surname><given-names>I</given-names></name><name><surname>Kück</surname><given-names>L</given-names></name><name><surname>Ewell</surname><given-names>LA</given-names></name><name><surname>Schwarz</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep lab stream enables closed-loop behavioral experiments using deep learning-based markerless, real-time posture detection</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>130</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-01654-9</pub-id><pub-id pub-id-type="pmid">33514883</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schweihoff</surname><given-names>JF</given-names></name><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Schwarz</surname><given-names>MK</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A-SOiD, an Active Learning Platform for Expert-Guided, Data Efficient Discovery of Behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.11.04.515138</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sculley</surname><given-names>D</given-names></name><name><surname>Holt</surname><given-names>G</given-names></name><name><surname>Golovin</surname><given-names>D</given-names></name><name><surname>Davydov</surname><given-names>E</given-names></name><name><surname>Phillips</surname><given-names>T</given-names></name><name><surname>Ebner</surname><given-names>D</given-names></name><name><surname>Chaudhary</surname><given-names>V</given-names></name><name><surname>Young</surname><given-names>M</given-names></name><name><surname>Crespo</surname><given-names>JF</given-names></name><name><surname>Dennison</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Hidden Technical Debt in Machine Learning Systems Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Zelikowsky</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>The mouse action recognition system (mars) software pipeline for automated analysis of social behaviors in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e63720</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63720</pub-id><pub-id pub-id-type="pmid">34846301</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Zelikowsky</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>MARS</data-title><version designator="1.8">1.8</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/MARS">https://github.com/neuroethology/MARS</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>C</given-names></name><name><surname>Schwartz</surname><given-names>S</given-names></name><name><surname>Levy</surname><given-names>S</given-names></name><name><surname>Achvat</surname><given-names>S</given-names></name><name><surname>Abboud</surname><given-names>M</given-names></name><name><surname>Ghanayim</surname><given-names>A</given-names></name><name><surname>Schiller</surname><given-names>J</given-names></name><name><surname>Mishne</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning disentangled behavior embeddings</article-title><conf-name>Advances in neural information processing systems</conf-name><fpage>22562</fpage><lpage>22573</lpage></element-citation></ref><ref id="bib78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stoyanovich</surname><given-names>J</given-names></name><name><surname>Howe</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nutritional labels for data and models</article-title><conf-name>A Quarterly Bulletin of the Computer Society of the IEEE Technical Committee on Data Engineering</conf-name></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sturman</surname><given-names>O</given-names></name><name><surname>von Ziegler</surname><given-names>L</given-names></name><name><surname>Schläppi</surname><given-names>C</given-names></name><name><surname>Akyol</surname><given-names>F</given-names></name><name><surname>Privitera</surname><given-names>M</given-names></name><name><surname>Slominski</surname><given-names>D</given-names></name><name><surname>Grimm</surname><given-names>C</given-names></name><name><surname>Thieren</surname><given-names>L</given-names></name><name><surname>Zerbi</surname><given-names>V</given-names></name><name><surname>Grewe</surname><given-names>B</given-names></name><name><surname>Bohacek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions</article-title><source>Neuropsychopharmacology</source><volume>45</volume><fpage>1942</fpage><lpage>1952</lpage><pub-id pub-id-type="doi">10.1038/s41386-020-0776-y</pub-id><pub-id pub-id-type="pmid">32711402</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Ryou</surname><given-names>S</given-names></name><name><surname>Goldshmid</surname><given-names>RH</given-names></name><name><surname>Weissbourd</surname><given-names>B</given-names></name><name><surname>Dabiri</surname><given-names>JO</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Self-Supervised Keypoint Discovery in Behavioral Videos</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2112.05121">https://arxiv.org/abs/2112.05121</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Ryou</surname><given-names>S</given-names></name><name><surname>Goldshmid</surname><given-names>RH</given-names></name><name><surname>Weissbourd</surname><given-names>B</given-names></name><name><surname>Dabiri</surname><given-names>JO</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>BKinD</data-title><version designator="2.0">2.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/BKinD">https://github.com/neuroethology/BKinD</ext-link></element-citation></ref><ref id="bib83"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Zhan</surname><given-names>E</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021c</year><article-title>Task programming: Learning data efficient behavior representations</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><fpage>2875</fpage><lpage>2884</lpage><pub-id pub-id-type="doi">10.1109/cvpr46437.2021.00290</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Zhan</surname><given-names>E</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021d</year><data-title>TREBA</data-title><version designator="ceacb66">ceacb66</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/TREBA">https://github.com/neuroethology/TREBA</ext-link></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Tjandrasuwita</surname><given-names>M</given-names></name><name><surname>Sehgal</surname><given-names>A</given-names></name><name><surname>Solar-Lezama</surname><given-names>A</given-names></name><name><surname>Chaudhuri</surname><given-names>S</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Costilla-Reyes</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neurosymbolic programming for science</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2210.05050">https://arxiv.org/abs/2210.05050</ext-link></element-citation></ref><ref id="bib86"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Toshev</surname><given-names>A</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deep neural networks for object detection</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Duuren</surname><given-names>E</given-names></name><name><surname>van der Plasse</surname><given-names>G</given-names></name><name><surname>Lankelma</surname><given-names>J</given-names></name><name><surname>Feenstra</surname><given-names>MGP</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Single-cell and population coding of expected reward probability in the orbitofrontal cortex of the rat</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>8965</fpage><lpage>8976</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0005-09.2009</pub-id><pub-id pub-id-type="pmid">19605634</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walter</surname><given-names>T</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Trex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</article-title><source>eLife</source><volume>10</volume><elocation-id>e64000</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id><pub-id pub-id-type="pmid">33634789</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>RA</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Hoffman</surname><given-names>JR</given-names></name><name><surname>Li</surname><given-names>EY</given-names></name><name><surname>Hong</surname><given-names>YK</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name><name><surname>Sawtell</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A rapid whisker-based decision underlying skilled locomotion in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e63596</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63596</pub-id><pub-id pub-id-type="pmid">33428566</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SR</given-names></name><name><surname>Amarante</surname><given-names>LM</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The future is open: Open-source tools for behavioral neuroscience research</article-title><source>ENeuro</source><volume>6</volume><elocation-id>ENEURO.0223-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0223-19.2019</pub-id><pub-id pub-id-type="pmid">31358510</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>MD</given-names></name><name><surname>Dumontier</surname><given-names>M</given-names></name><name><surname>Aalbersberg</surname><given-names>IJJ</given-names></name><name><surname>Appleton</surname><given-names>G</given-names></name><name><surname>Axton</surname><given-names>M</given-names></name><name><surname>Baak</surname><given-names>A</given-names></name><name><surname>Blomberg</surname><given-names>N</given-names></name><name><surname>Boiten</surname><given-names>JW</given-names></name><name><surname>da Silva Santos</surname><given-names>LB</given-names></name><name><surname>Bourne</surname><given-names>PE</given-names></name><name><surname>Bouwman</surname><given-names>J</given-names></name><name><surname>Brookes</surname><given-names>AJ</given-names></name><name><surname>Clark</surname><given-names>T</given-names></name><name><surname>Crosas</surname><given-names>M</given-names></name><name><surname>Dillo</surname><given-names>I</given-names></name><name><surname>Dumon</surname><given-names>O</given-names></name><name><surname>Edmunds</surname><given-names>S</given-names></name><name><surname>Evelo</surname><given-names>CT</given-names></name><name><surname>Finkers</surname><given-names>R</given-names></name><name><surname>Gonzalez-Beltran</surname><given-names>A</given-names></name><name><surname>Gray</surname><given-names>AJG</given-names></name><name><surname>Groth</surname><given-names>P</given-names></name><name><surname>Goble</surname><given-names>C</given-names></name><name><surname>Grethe</surname><given-names>JS</given-names></name><name><surname>Heringa</surname><given-names>J</given-names></name><name><surname>’t Hoen</surname><given-names>PAC</given-names></name><name><surname>Hooft</surname><given-names>R</given-names></name><name><surname>Kuhn</surname><given-names>T</given-names></name><name><surname>Kok</surname><given-names>R</given-names></name><name><surname>Kok</surname><given-names>J</given-names></name><name><surname>Lusher</surname><given-names>SJ</given-names></name><name><surname>Martone</surname><given-names>ME</given-names></name><name><surname>Mons</surname><given-names>A</given-names></name><name><surname>Packer</surname><given-names>AL</given-names></name><name><surname>Persson</surname><given-names>B</given-names></name><name><surname>Rocca-Serra</surname><given-names>P</given-names></name><name><surname>Roos</surname><given-names>M</given-names></name><name><surname>van Schaik</surname><given-names>R</given-names></name><name><surname>Sansone</surname><given-names>SA</given-names></name><name><surname>Schultes</surname><given-names>E</given-names></name><name><surname>Sengstag</surname><given-names>T</given-names></name><name><surname>Slater</surname><given-names>T</given-names></name><name><surname>Strawn</surname><given-names>G</given-names></name><name><surname>Swertz</surname><given-names>MA</given-names></name><name><surname>Thompson</surname><given-names>M</given-names></name><name><surname>van der Lei</surname><given-names>J</given-names></name><name><surname>van Mulligen</surname><given-names>E</given-names></name><name><surname>Velterop</surname><given-names>J</given-names></name><name><surname>Waagmeester</surname><given-names>A</given-names></name><name><surname>Wittenburg</surname><given-names>P</given-names></name><name><surname>Wolstencroft</surname><given-names>K</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Mons</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The fair guiding principles for scientific data management and stewardship</article-title><source>Scientific Data</source><volume>3</volume><elocation-id>160018</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id><pub-id pub-id-type="pmid">26978244</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Tsukahara</surname><given-names>T</given-names></name><name><surname>Zeine</surname><given-names>A</given-names></name><name><surname>Anyoha</surname><given-names>R</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1433</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00706-3</pub-id><pub-id pub-id-type="pmid">32958923</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>A</given-names></name><name><surname>Buchanan</surname><given-names>EK</given-names></name><name><surname>Whiteway</surname><given-names>MR</given-names></name><name><surname>Schartner</surname><given-names>M</given-names></name><name><surname>Meijer</surname><given-names>G</given-names></name><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Rodriguez</surname><given-names>E</given-names></name><name><surname>Everett</surname><given-names>C</given-names></name><name><surname>Norovich</surname><given-names>A</given-names></name><name><surname>Schaffer</surname><given-names>E</given-names></name><name><surname>Mishra</surname><given-names>N</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bendesky</surname><given-names>A</given-names></name><name><surname>Laboratory</surname><given-names>TIB</given-names></name><name><surname>Cunningham</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep Graph Pose: A Semi-Supervised Deep Graphical Model for Improved Animal Pose Tracking</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.20.259705</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>E</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Chaudhuri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised Learning of Neurosymbolic Encoders</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2107.13132">https://arxiv.org/abs/2107.13132</ext-link></element-citation></ref></ref-list></back></article>