<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">105714</article-id><article-id pub-id-type="doi">10.7554/eLife.105714</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105714.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Integration of head and body orientations in the macaque superior temporal sulcus is stronger for upright bodies</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zafirova</surname><given-names>Yordanka</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2153-6926</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Vogels</surname><given-names>Rufin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8778-835X</contrib-id><email>rufin.vogels@kuleuven.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Laboratorium voor Neuro- en Psychofysiologie, Department of Neurosciences</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff><aff id="aff2"><label>2</label><institution>Leuven Brain Institute</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>05</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP105714</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-12-23"><day>23</day><month>12</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-12-30"><day>30</day><month>12</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.12.30.630733"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-03-06"><day>06</day><month>03</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.105714.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-05-08"><day>08</day><month>05</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.105714.2"/></event></pub-history><permissions><copyright-statement>© 2025, Zafirova and Vogels</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zafirova and Vogels</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-105714-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-105714-figures-v1.pdf"/><abstract><p>The neural processing of faces and bodies is often studied separately, despite their natural integration in perception. Unlike prior research on the neural selectivity for either head or body orientation, we investigated their interaction in macaque superior temporal sulcus (STS) using a monkey avatar with diverse head–body orientation angles. STS neurons showed selectivity for specific combinations of head–body orientations. Anterior STS (aSTS) neurons enabled more reliable decoding of head–body configuration angles compared to middle STS neurons. Decoding accuracy in aSTS was lowest for head–body angle pairs differing only in sign (e.g. head–body orientation difference of ±90° relative to the anatomical midline), and highest for aligned (0°) head–body orientations versus those with maximum angular difference. Inverted bodies showed diminished decoding of head–body orientation angle compared to upright bodies. These findings show that aSTS integrates head and body orientation cues, revealing configuration-specific neural mechanisms, and advance our understanding of social perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>faces</kwd><kwd>bodies</kwd><kwd>superior temporal sulcus</kwd><kwd>inferior temporal cortex</kwd><kwd>head–body interaction</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003130</institution-id><institution>Fonds Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>G0E0220N</award-id><principal-award-recipient><name><surname>Vogels</surname><given-names>Rufin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/856495</award-id><principal-award-recipient><name><surname>Vogels</surname><given-names>Rufin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven</institution></institution-wrap></funding-source><award-id>C14/21/111</award-id><principal-award-recipient><name><surname>Vogels</surname><given-names>Rufin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Head and body orientation cues are integrated by macaque superior temporal sulcus neurons, with stronger integration for upright monkey images.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Faces and bodies are typically studied separately, although both are parts of the same agent (<xref ref-type="bibr" rid="bib8">Freiwald et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Tsao and Livingstone, 2008</xref>; <xref ref-type="bibr" rid="bib5">de Gelder et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Peelen and Downing, 2007</xref>). Despite the distinct research traditions, behavioral and fMRI studies in humans suggest that face and body processing interact (<xref ref-type="bibr" rid="bib11">Hu et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Taubert et al., 2022</xref>). Furthermore, face-selective units in macaque mid superior temporal sulcus (STS) face-selective patches respond to an object occluding a face on top of a body (<xref ref-type="bibr" rid="bib1">Arcaro et al., 2020</xref>), which agrees with monkey (<xref ref-type="bibr" rid="bib6">Fisher and Freiwald, 2015</xref>) and human fMRI (<xref ref-type="bibr" rid="bib4">Cox et al., 2004</xref>) studies that showed that a blurred head on top of a body can activate face-selective regions. Recently, we showed that anterior inferior temporal (IT) neurons respond stronger to face–body configurations in which the face is located at its natural position on top of the body than to unnatural face–body configurations, showing that the face and body interact in a configuration-specific manner in IT (<xref ref-type="bibr" rid="bib29">Zafirova et al., 2024</xref>; <xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>). While these studies demonstrate that face and body processing interact in the visual temporal cortex, the extent and underlying mechanisms of this interaction remain unexplored.</p><p>The responses of most face-selective STS neurons are modulated by the 3D orientation of the head (<xref ref-type="bibr" rid="bib7">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib20">Perrett et al., 1992</xref>; <xref ref-type="bibr" rid="bib18">Perrett et al., 1984</xref>; <xref ref-type="bibr" rid="bib19">Perrett et al., 1985</xref>). Likewise, the responses of most body-selective STS neurons are sensitive to body orientation (<xref ref-type="bibr" rid="bib3">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Kumar et al., 2019</xref>). This might not be surprising because both head and body orientation are important social cues, for example for the attention of a viewed agent or the direction of an agent’s action. In these studies, either face or body orientation, but not both, were manipulated. Here we address the outstanding question of whether the orientation of the head and body interact in the STS: does the selectivity for head orientation depend on the body orientation, and vice versa? Are STS neurons selective for a particular combination of head and body orientation, suggesting a selectivity for both head and body features in a particular pose, for example a selective response for a frontal body with a head turned to the right? Our study was also motivated by human psychophysical studies that showed that body and head orientation interact in social perception (<xref ref-type="bibr" rid="bib10">Hietanen, 2002</xref>; <xref ref-type="bibr" rid="bib16">Moors et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Vrancken et al., 2017</xref>), although it is unclear at which level – sensory or higher – the observed interactions occurred.</p><p>Here, we manipulated independently the head and body orientation of a monkey avatar. We employed both anatomically possible and impossible head–body orientations (e.g. a frontal body with a backward-oriented head, i.e. 180° difference), allowing us to assess whether anatomically possible and impossible head–body orientation angles are encoded differently by STS neurons.</p><p>Behavioral studies in humans (<xref ref-type="bibr" rid="bib9">Griffin and Oswald, 2022</xref>) and monkeys (<xref ref-type="bibr" rid="bib14">Matsuno and Fujita, 2018</xref>) provided evidence for greater body discrimination for upright compared to inverted bodies. This body-inversion effect has been interpreted as providing evidence for the holistic encoding of bodies, similar to that proposed for faces. Some studies suggested that the body-inversion effect is driven by the head (for review, see <xref ref-type="bibr" rid="bib9">Griffin and Oswald, 2022</xref>). However, other studies have observed a behavioral body-inversion effect for headless bodies (for a meta-analysis, see <xref ref-type="bibr" rid="bib9">Griffin and Oswald, 2022</xref>). So far, no neural correlates of the body-inversion effect for whole bodies have been described at the single-unit level. Here, we compared the responses and body–head interaction of STS units between upright and inverted bodies, assessing whether these units show stronger responses, or higher body selectivity and body–head interactions for upright bodies.</p><p>We recorded the responses of units at two posterior–anterior levels of the ventral STS, in and surrounding patches that were activated more by images of monkeys compared to objects in an fMRI study with the same subjects (<xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Because it has been proposed that face–body interactions are more prominent in anterior IT (<xref ref-type="bibr" rid="bib6">Fisher and Freiwald, 2015</xref>; <xref ref-type="bibr" rid="bib11">Hu et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>), we expected stronger head–body orientation interactions in the anterior STS (aSTS).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Recording locations and stimulus selectivity.</title><p>(<bold>A</bold>) Targeted patches (red circles) shown in the native space of M1 (left) and M2 (right) on 2 mm coronal slices. Brown vertical lines illustrate penetrations. Monkey patches were defined by the contrast monkeys &gt; monkey control objects (M-O<sub>M</sub>; p &lt; 0.05 FEW). (<bold>B</bold>) Normalized net responses in the Selectivity test (experiment 1) in anterior superior temporal sulcus (aSTS) (left) and mid superior temporal sulcus (mSTS) (right). Rows represent neurons from M1 and M2 (separated by a dotted line). Columns correspond to the stimuli, color-coded per category with a bar and one example image. Below, bars in corresponding colors show the mean normalized net response to each stimulus (95% confidence intervals (1000 resamplings)). <italic>n</italic> indicates the number of neurons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig1-v1.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><p>In experiment 1, we targeted two fMRI-defined patches that were defined by a higher activation to images of monkeys compared to objects (<xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The posterior patch was located in the ventral bank of the mid STS (mSTS) and the anterior one was about 12 mm more anterior in the ventral bank of the STS (aSTS). In experiment 1, we recorded well-isolated single units in both regions. In the Selectivity test (Materials and methods), we compared the responses to a set of monkeys, faces, headless bodies, and objects (see <xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>). We took neurons (Materials and methods) for further examination when their mean net response to the monkey, face, or body was at least twice the mean response to the corresponding object control condition. Thus, we obtained 98 aSTS and 100 mSTS neurons, split approximately equally across the two subjects. In both regions, the averaged responses of the selected neurons were strongest for the monkey images, followed closely by the body and the face, and much weaker for the objects (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Some neurons responded stronger to faces than bodies, or vice versa, and showed strong selectivity for different body or face images (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>These neurons were tested in the Head–body Orientation test in which we manipulated independently the orientation of the head and the body of a monkey avatar (Materials and methods; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). The monkey avatar was either sitting (P1) or standing (P2). For each pose, we had 64 conditions in which the orientation of the head and body was manipulated independently in steps of 45° (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). To separate the effect of head/body location from head or body orientation, we presented all images at two locations: monkey-centered (MC) and head-centered (HC; <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Monkey avatar stimuli for the Head–body Orientation test (experiment 1).</title><p>(<bold>A</bold>) Sixty-four combinations of head and body orientations for the sitting (P1, left) and the standing pose (P2, right). Body orientation varies by row and head orientation varies by column. Different colors group the eight avatar orientations of four head–body angles. (<bold>B</bold>) Example stimuli of P1 and P2 in the monkey-centered (MC, left) and the head-centered (HC, right) conditions, with zero angle between head and body. (<bold>C</bold>) Headless bodies (B) and heads (H) for the zero and straight angle head–body configurations, showing the frontal, back, and two lateral views for P1 and P2. Images are shown for MC (left) and HC (right) conditions and were positioned to match their corresponding avatars in (<bold>A</bold>). The location of the red fixation target is indicated in (<bold>B</bold>) and (<bold>C</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig2-v1.tif"/></fig><sec id="s2-1"><title>Responses for a monkey are on average less than the summed responses for the body and head (experiment 1)</title><p>For each pose, the head and body of the frontal, back, and two lateral orientations were also presented in isolation at the same locations as in the avatar. The latter was done for head–body configurations of zero and straight (0° and 180°) angles between the head and body orientation (<xref ref-type="fig" rid="fig2">Figure 2B, C</xref>). This allowed us to compare the responses to the isolated body and head with those to the monkeys having the same head and body at the same locations. For each case for which there was a significant excitatory response to either the monkey or the corresponding head and body compared to the baseline, we computed the monkey-sum index (MSI) contrasting the net response to the monkey and the sum of the net responses to its corresponding head and body (Materials and methods). MSI larger than 0 corresponds to a superadditive response to the monkey while MSI smaller than 0 indicates a subadditive response. For both subjects and regions, the median MSIs for P1 and P2 were smaller than 0 (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which agrees with a single-unit study in anterior IT that employed images of real macaques (<xref ref-type="bibr" rid="bib29">Zafirova et al., 2024</xref>). Surprisingly, we observed neurons that responded to either the headless body or head but did not respond to the whole monkey that included the same body and head (MSI close to –1; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). The weak responses to the monkey were not limited to cases in which the head–body configuration was anatomically impossible, that is a straight angle between the head and the body. In fact, cases in which MSI was between –0.9 and –1 included 53% and 41% of natural head–body configurations in aSTS and mSTS, respectively. In line with a previous study (<xref ref-type="bibr" rid="bib29">Zafirova et al., 2024</xref>), only a minority of neurons demonstrated strong superadditive responses to the monkey (MSI between 0.9 and 1: 4.3% and 6.2% in aSTS and mSTS, respectively). <xref ref-type="fig" rid="fig3">Figure 3E</xref> illustrates the responses to heads, bodies, and monkeys of example units with different values of MSI. These data show that heads and bodies interact at the level of the STS.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neuronal responses in the Head–body Orientation test (experiment 1).</title><p>(<bold>A</bold>) Distribution of monkey-sum index (MSI) computed for the responses for all orientation, pose (P1 and P2), and centering (monkey-centered (MC) (blue) and head-centered (HC) (red)) combinations in anterior superior temporal sulcus (aSTS) (upper panel) and mid superior temporal sulcus (mSTS) (lower panel). Data for M1 and M2, and medians (triangles) are shown in different hues (n1 = number of cases for M1; n2 = number of cases for M2). (<bold>B</bold>) Significant main effects and interactions of the factors body and head orientation, for P1 and P2 in MC (blue) and HC (red) conditions in corresponding colors in aSTS (left) and mSTS (right). Pooled data of both subjects. Significant interactions in both MC and HC are highlighted with darker blue and red (number of cases (<italic>n</italic>) in black). (<bold>C</bold>) Distribution of the correlation coefficients between responses to the head–body configurations in MC and HC conditions in aSTS (left) and mSTS (right). Data from both subjects and poses were pooled. Correlation coefficients for all cases are shown above the <italic>x</italic>-axis. Below the <italic>x</italic>-axis, the light green bars represent the correlation coefficients for cases with a significant interaction (either MC or HC), while the dark green bars indicate the coefficients for cases with a significant interaction in both MC and HC. <italic>N</italic> and medians (triangles) are shown in corresponding colors. (<bold>D</bold>) Normalized net responses of example neurons to the 64 combinations of head and body orientations in aSTS (left) and mSTS (right) for MC and HC conditions. Body orientation (ORT) varies by row and head orientation varies by column, from 0° to 315°. Main effects and interactions of the factors body and head orientation B, H, and I are shown for each neuron in each centering (significant ones are in red, see (<bold>B</bold>) for details), as well as the correlation coefficients <italic>r</italic> between responses to the head–body configurations in MC and HC conditions. (<bold>E</bold>) Net responses of example neurons to bodies, heads, and the corresponding monkey configurations with MSI of 1, –1, or the median. Data for P1 and P2, MC and HC, and aSTS and mSTS (smoothed with the Matlab function <italic>interp1</italic>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Correlation between head- and body orientation selectivity between the sitting (P1) and standing pose (P2; experiment 1).</title><p>We computed the correlation between the 64 head–body orientation conditions of P1 and P2 for those neurons that were tested with both poses and showed a response for both poses (split-plot ANOVA). This was performed for the head-centered (HC) and monkey-centered (MC) tests of experiment 1 for each monkey and region. Note that not all neurons were tested with both poses (because of failure to maintain isolation of the single unit in both tests or the monkey stopped working) and not all neurons that were recorded in both tests showed a significant response for both poses, which is not unexpected since these neurons can be pose selective. The distribution of the Pearson correlation coefficients of the neurons with a significant response in both tests is shown for each region and centering condition (n1 and n2 indicate the number of neurons for subjects M1 and M2, respectively). The data of the two subjects are shown in different color. The median correlation coefficient (arrow) was significantly larger than zero for each region, monkey, and centering condition (the outcomes of Wilcoxon tests, testing whether the median was different from zero (p1 = p-value for M1; p2 = p-value for M2) are indicated).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig3-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Interaction of head and body orientation tuning (experiment 1)</title><p>The main question of this study was whether head and body <italic>orientation tuning</italic> interact. Thus, we performed a two-way ANOVA on the responses (Materials and methods) to the 64 head–body orientation conditions, with factors the orientation of the head and body (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The large majority of neurons showed a significant main effect (p &lt; 0.05) of body orientation followed by a main effect of head orientation (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Importantly, a large fraction of neurons showed a significant interaction between head and body orientation. This was true for both centerings (MC and HC) and poses (P1 and P2) in each region (aSTS and mSTS, <xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>These results do not necessarily provide evidence for genuine head and body orientation interactions because orientation changes could be confounded with location changes of the monkey part within a centering condition (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). To address this, we first assessed whether the tuning for body/head orientation was location tolerant by correlating the responses to the 64 head–body configurations between MC and HC. This was done for all neuron and pose combinations in which there was a significant response in either centering condition (split-plot ANOVA). The median correlations were 0.68 and 0.57 in aSTS and mSTS, respectively (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), indicating position tolerance of the orientation selectivity in both regions. In line with its higher hierarchical stage and thus expected higher location tolerance, aSTS showed significantly higher correlations than mSTS (Wilcoxon rank-sum test, p = 0.008; tested after averaging the correlation coefficients of the poses per neuron; <italic>n</italic> = 75 aSTS and <italic>n</italic> = 77 mSTS neurons). The higher correlations for aSTS were not a result of differences in response reliability between the two regions: the median Spearman–Brown corrected split-half Pearson correlation (Materials and methods) was 0.82 and 0.81 for the MC and HC aSTS neurons (<italic>n</italic> = 131 cases) and 0.85 and 0.89 for the MC and HC mSTS neurons (<italic>n</italic> = 126 cases). The relatively higher reliability for mSTS shows that the lower correlation of the head–body orientation selectivity between the two centerings for mSTS neurons is not due to lower reliability. Furthermore, correcting the correlations for reliability showed a lower median corrected correlation for mSTS (median: 0.72) compared to aSTS (0.90).</p><p>To assess the position tolerance of the head–body orientation interaction per se, we computed how many neurons for each pose show a significant interaction (two-way ANOVA; see above) of head and body orientation for both centerings. In aSTS, 24% of the neurons responding for both centerings of a pose (<italic>n</italic> = 135) showed a significant interaction in both centerings, and this proportion was similar for mSTS (26%; <italic>n</italic> = 127; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Notably, for neuron and pose combinations with significant head–body interactions in both centerings, the correlations between responses to the 64 head–body orientation conditions were similar to those observed in the whole population (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>Next, we isolated the interaction of head and body orientation by computing the residuals of the responses from a linear additive model in which the responses to a head–body configuration are an addition of the mean responses to the body (averaging across the eight head orientations) and head (averaging across the eight body orientations) orientations. The residuals were computed for both centerings and then correlated across the centerings per neuron and pose combination. We computed these correlations for those cases in which the two-way ANOVA (see above) showed a significant interaction for both centerings. The median correlation between the residuals of the two centerings was 0.23 (quartiles: 0.08–0.36; <italic>n</italic> = 33) for mSTS, whereas it was 0.41 (0.21–0.55; <italic>n</italic> = 33; median larger than 0: p = 9.35e−07) for aSTS, which was significantly greater than for mSTS (Wilcoxon rank-sum test; p = 0.008). These data demonstrate a position tolerance of the interaction of head and body orientation in aSTS, but less so in mSTS.</p><p>Single neurons showed a variety of head–body orientation tunings (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Interestingly, some neurons (e.g. <xref ref-type="fig" rid="fig3">Figure 3D</xref>) were tuned to a particular combination of a head and body orientation irrespective of centering (e.g. the third aSTS example neuron of <xref ref-type="fig" rid="fig3">Figure 3D</xref>). The tuning for head–body orientation generalized across the two poses for the population of mSTS and aSTS neurons that responded significantly to each pose (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec id="s2-3"><title>Decoding of orientation-invariant head–body configuration (experiment 1)</title><p>Given the head–body orientation interactions in the STS, we examined whether we could decode head–body configurations with different angles between head and body in an orientation-invariant way from our sample of neurons using a linear classifier (linear kernel support vector machine (SVM)). We trained the classifier to distinguish two angles after pooling the responses of all eight orientations of a head–body orientation angle configuration (Materials and methods; examples in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Because the training used responses to all orientations per angle and because the same head and body orientations were present in both classes, we forced the classifier to employ the <italic>combination</italic> of head and body orientation. We used eightfold cross-validation with training and test trials evenly distributed across orientations (Materials and methods). We will report below the classification accuracies obtained when training was performed using trials of one centering condition (MC) and testing for the other centering condition (HC), requiring generalization across positions. The classification accuracies for training on the HC and testing on the MC condition were highly similar (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We employed for each region 60 neurons, 30 randomly selected for each monkey subject, as that was the maximum number of neurons with responses in both centering conditions (MC and HC) per subject and recording location (aSTS and mSTS). The classification was performed using raw firing rates. We report the mean and standard deviation, across 100 resamplings, of the classification accuracies for the test trials.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Decoding of head–body angles from neuronal responses in the Head–body Orientation test (experiment 1).</title><p>(<bold>A</bold>) Head–body configurations with zero and straight angles (black arrows in <bold>B</bold>) and leftward and rightward right angles of head and body (gray arrows in <bold>B</bold>), of P1 and P2, at eight avatar orientations. (<bold>B</bold>) Mean decoding accuracies for the head–body configurations with zero, straight, leftward, and rightward right angles versus other head–body angles as indicated, in anterior superior temporal sulcus (aSTS) (top two panels) and mid superior temporal sulcus (mSTS) (bottom two panels), for poses P1 and P2. <italic>N</italic> indicates the number of neurons. Results are for monkey-centered (MC) training and head-centered (HC) testing. Responses to the eight orientations of a configuration (rows in <bold>A</bold>) were pooled when training the classifier. Error bars show standard deviations across resamplings. The shaded area indicates null distribution (stimulus labels permutation). (<bold>C</bold>) Decoding accuracies for different head–body angle pairs in aSTS, for P1 (top) and P2 (bottom). Decoded pairs are connected with lines, with decoding accuracies indicated by color. Training was performed after pooling the eight orientations of a head–body configuration defined by the angle of the head and body. Results are from MC training and HC testing. Avatars of one orientation of the head–body configuration angles are shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Decoding of head–body angles in the Head–body Orientation test (experiment 1).</title><p>(<bold>A</bold>) Decoding accuracies for the head–body configurations with zero, straight, leftward, and rightward right angles versus other head–body angles, in anterior superior temporal sulcus (aSTS) (top two panels) and mid superior temporal sulcus (mSTS) (bottom two panels), for P1 and P2, <italic>n</italic> indicates the number of neurons. Results are for head-centered (HC) training and monkey-centered (MC) testing. The same conventions as <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>B</bold>) Same as (<bold>A</bold>), but with MC training and MC testing. (<bold>C</bold>) Same as (<bold>A</bold>), but with HC training and HC testing. (<bold>D</bold>) Decoding accuracies for different head–body angle pairs in aSTS, for P1 (top) and P2 (bottom). Decoded pairs are connected with lines, with decoding accuracies indicated by color. Results are from HC training and MC testing (see <xref ref-type="fig" rid="fig4">Figure 4</xref> for MC training and HC testing; same conventions apply).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Neuron-dropping analysis (experiment 1).</title><p>We attempted to relate the tuning for body and head orientation with the decoding of the head–body orientation angle. To do this, one needs to identify the neurons that contribute to the decoding of head–body orientation angles. For that, we employed a neuron-dropping analysis, similar to Chiang et al. (Chiang FK, Wallis JD, Rich EL. Cognitive strategies shift information from single neurons to populations in prefrontal cortex. Neuron. 2022 Feb 16;110(4):709–721) to assess the positive (or negative) contribution of each neuron to the decoding performance. We performed cross-validated linear support vector machine (SVM) decoding <italic>N</italic> times (<italic>N</italic> = 60 (the number of neurons in the sample)), each time leaving out a different neuron (using <italic>N</italic> − 1 neurons; 2000 resamplings of pseudo-population vectors). We then ranked decoding accuracies from highest to lowest, identifying the ‘worst’ (rank 1) to ‘best’ (rank <italic>N</italic>) neurons. Next, we conducted <italic>N</italic> decodings, incrementally increasing the number of included neurons from 1 to <italic>N</italic>, starting with the worst-ranked neuron (rank 1) and sequentially adding the next (rank 2, rank 3, etc.). This analysis focused on zero versus straight angle decoding in the anterior superior temporal sulcus (aSTS), as it yielded the highest accuracy. We applied it when training on monkey-centered (MC) and testing on head-centered (HC) for each pose (P1 and P2). Plotting accuracy as a function of the number of included neurons suggested that less than half contributed positively to decoding (<bold>A</bold>). We examined the tuning for head and body orientation of the 10 ‘best’ neurons (<bold>B</bold>). Both subjects (M1, M2) contributed neurons to the 10 best. For half or more of those the two-way ANOVA showed a significant interaction and these are indicated by the red color in (<bold>B</bold>). We performed for each neuron of experiment 1, a one-way ANOVA with as factor head–body orientation angle. To do that, we combined all 64 trials that had the same head–body orientation angle. The percentage of neurons (required to be responsive in the tested condition) for which this one-way ANOVA was computed for each region, separately for each pose, centering condition and monkey subject. These percentages were low but larger than the expected 5% (Type 1 error), with a median of 16.5% (range: 3–23%) in aSTS and 8% for mid superior temporal sulcus (mSTS) (range: 0–19%). A higher percentage of the 10 best neurons for each pose (indicated by * in (<bold>B</bold>)) showed a significant one-way ANOVA for angle (for P1, MC: 50% (95% confidence interval (CI): 19–81%); P1, HC: 70% (CI: 35–93%); P2, MC: 70% (CI: 35–93%); P2: HC: 50% (CI: 19–81%)). These percentages were significantly higher than expected for a random sample from the population of neurons for each pose-centering combination (expected percentages listed in the same order as above: 16%, 13%, 16%, and 10%; all outside CI). Thus, for at least half of the ‘best’ neurons, the response differed significantly among the head orientation angles at the single neuron level. Nonetheless, the tuning profiles were quite diverse, suggesting population coding of head–body orientation angle.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Decoding of head and body orientation (experiment 1).</title><p>We employed the responses to the avatar of experiment 1, using the same sample of neurons of which we decoded the head–body orientation angle. To decode the head orientation, the trials with identical head orientation, irrespective of their body orientation, were given the same label. For this, we employed only responses in the head-centered condition. To decode the body orientation, the trials with identical body orientation, irrespective of their head orientation, had the same label, and we employed only responses in the body-centered condition. The decoding was performed separately for each pose (poses 1 and 2) and region. We decoded either the responses of 20 neurons (10 randomly sampled from each monkey for each of 1000 resamplings), 40 neurons (20 randomly sampled per monkey), or 60 neurons (30 neurons per monkey) since the sample of 60 neurons yielded close to ceiling performance for the body orientation decoding. For each pose, the body orientation decoding was worse for anterior superior temporal sulcus (aSTS) than for mid superior temporal sulcus (mSTS), although this difference reached significance only for P1 and for the 40 neurons sample of P2 (p &lt; 0.025; two-tailed test; same procedure as employed for testing the significance of the decoding of whole-body orientation for upright vs. inverted avatars (experiment 3)). Face orientation decoding was significantly worse for aSTS compared to mSTS.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig4-figsupp3-v1.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig4">Figure 4B</xref> shows the mean classification accuracies for pairs of head–body orientation angles, plotted for each of four reference angles (0° (zero angle), 180° (straight angle), 90° (rightward right angle), and –90° (leftward right angle)) versus other angles. For the zero and straight reference angles, the classification accuracy increased with the absolute difference between the angles, reaching a maximum of about 0.7 classification accuracy for zero versus straight angles. Classification accuracies were smaller for mSTS compared to aSTS. Surprisingly, for the rightward (90°) and leftward (–90°) right angles as references, classification scores were low (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), which contrasts with the higher scores for the same 180° orientation difference when the angles were zero and straight (this difference was significant for both centerings and poses in aSTS; all p-values &lt;0.01). Classification scores tended to be higher for 90° differences in angles (e.g. right vs. straight) than for the 180° difference (rightward vs. leftward right angles) for the rightward or leftward right reference angles.</p><p>The decoding accuracies showed the same pattern, although were higher, when training and testing were performed with the same centering, requiring no generalization across locations (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). A neuron-dropping analysis applied to the decoding of zero versus straight angles for the aSTS sample (MC training and HC testing) suggested that less than half of the neurons contributed positively to the decoding (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Most but not all of the 10 ‘best’ neurons, that is those that contributed most to the decoding, showed a significant interaction between head and body orientation. Furthermore, more of the 10 ‘best’ neurons (at least 50% (95% confidence interval: 19–81%)) showed individually a significant effect of head–body orientation angle on their responses (one-way ANOVA; p &lt; 0.05) than expected for a random sample of the population of neurons (maximal expected percentage 16%; see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for more details). Nonetheless, the tuning profiles were diverse (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), suggesting population coding of head–body orientation angle.</p><p>We applied also a more stringent classification in which the trained and tested orientations differed: we trained the classifier for the frontal, back, and two lateral orientations (0°, 180°, 90°, and –90°, i.e. ‘+’ orientations in <xref ref-type="fig" rid="fig5">Figure 5A</xref>), and tested with the oblique ones (45°, 135°, –135° and –45°, i.e. ‘x’ orientations in <xref ref-type="fig" rid="fig5">Figure 5A</xref>), and vice versa (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Again, the trained and tested responses were from different centerings. For this analysis, we employed the aSTS sample, as that produced the highest decoding accuracies previously. For P1, the performance accuracies for the aSTS sample were well above chance for the zero versus straight angle decoding, and, as before, were lower for the leftward versus rightward right angle decoding (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; for brevity, we will refer to these angles as left and right angles, respectively, below). The same was true for P2 for two of the four centering and orientation training–test combinations (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). However, for the other two combinations, the performance accuracies for the zero versus straight angles were barely above chance and were at a similar level as those for the right versus left angle decoding (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This interaction between generalization across orientation and centering for P2 is likely related to the larger variations in the eccentricity of the head when changing the orientation and centering of the standing avatar (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Nonetheless, when generalization across orientation but not across centering was required, accuracies for P1 and P2 were similar for all decodings (for the zero vs. straight angle: classification accuracies for P1 ranging from 0.64 to 0.72 and for P2 ranging from 0.66 to 0.72; for the right vs. left angle decoding: P1 0.49–0.54 and P2 0.48–0.56).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Decoding of head–body angles across orientations (experiment 1).</title><p>(<bold>A</bold>) Head–body configurations with zero and straight angles, and leftward and rightward right angles for P1 and P2, split into four orientations for training and testing. (<bold>B</bold>) Decoding accuracies for the head–body configurations with zero, straight, leftward, and rightward right angles in anterior superior temporal sulcus (aSTS), for P1 (top) and P2 (bottom); <italic>n</italic> indicates the number of neurons. Results are for monkey-centered (MC) training and head-centered (HC) testing, using four orientations for training and testing. Panels 1 and 3: classifier trained on ‘+’ orientations and tested on ‘x’ orientations; panels 2 and 4: classifier trained on ‘x’ orientations and tested on ‘+’ orientations. Same conventions as <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Decoding of head–body angles across orientations (experiment 1).</title><p>Same as <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>A</bold>) Head–body configurations with zero and straight angles, and leftward and rightward right angles for P1 and P2, split into four orientations for training and testing, except that (<bold>B</bold>) head-centered (HC) responses were employed for training and monkey-centered (MC) for testing. Note the difference in accuracy for P2 when training and testing ‘+’ and ‘x’ orientations in the two centering conditions (see <xref ref-type="fig" rid="fig5">Figure 5</xref> for MC to HC training and testing). This interaction between generalization across orientation and centering for P2 is likely related to the larger variations in the eccentricity of the head when changing the orientation and centering of P2. To decode the angle between the head and the body, the head needs to be processed. For the two lateral views (90° and –90° orientation of the angle pairs) of the MC condition of P2, which are included in the ‘+’ orientations (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), the head is at a larger eccentricity than for the ‘x’ orientations. This difference in head eccentricity is even enlarged when training is performed with the MC ‘+’ orientations and testing with the HC ‘x’ orientations, yielding low generalization performance. The same holds for the opposite generalization condition in which the training is with the HC ‘x’ orientations and the testing is with the MC ‘+’ orientations of P2. In the other two conditions (MC ‘x’ trained – HC ‘+’ tested; HC ‘+’ trained – MC ‘x’ tested) the differences in head eccentricity between the trained and tested conditions are less, yielding better generalization which was similar to that for P1, at least in the case of the zero versus straight angles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Dissimilarity matrices of neural responses for head and body orientations (experiment 1).</title><p>The color-coded dissimilarity values are 1 − Pearson correlation distances between neural response patterns for all body (top panels) and head (bottom panels) orientations of poses P1 and P2 (left and right panels, respectively) in the monkey-centered (MC) and head-centered (HC) conditions. Data are from the same anterior superior temporal sulcus (aSTS) neurons employed in the decoding of head–body angles in experiment 1. Correlations are computed using normalized firing rates, averaging head orientations when correlating body orientations, and averaging body orientations when correlating head orientations. The dissimilarity is represented by the color scale with yellow indicating higher dissimilarities.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig5-figsupp2-v1.tif"/></fig></fig-group><p>The marked difference in accuracy between the zero versus straight angle decoding and the right versus left angle decoding in aSTS was not trivially due to differences in mean response strength since there was no consistent effect of head–body orientation angle on mean response strength across poses (Friedman ANOVA; all p-values for both poses and centerings &gt;0.1).</p><p>The left and right angles have the same absolute angle between head and body (90°), only the sign of the angle differs with respect to the anatomical midline (90° vs. –90°). The confusion between these configurations suggests that this neuronal population is relatively insensitive to the sign of the angle. This was confirmed by other decodings (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We observed a close-to-chance decoding of 45° versus –45° angles and 135° versus –135° angles, unlike the better classification accuracy for the same 90° differences between angles of 0° and 90° or 180° and –90°, the latter but not the former differing in the absolute angle between head and body. Other configurations that differ in absolute angle also produced good decoding performance: 45° versus –135° and 135° versus –45°, better than the left versus right angles, which differed only in sign (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>One explanation of the poor decoding for the right versus left angles is that in those conditions either the body or the face is at a lateral orientation. Neurons in anterior IT are less sensitive to differences between mirror-symmetric images (<xref ref-type="bibr" rid="bib7">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib22">Rollenhagen and Olson, 2000</xref>). If such neurons contribute to the computation of the head–body orientation angle, poor decoding of right and left angles can be expected since these images are, to some extent, mirror-symmetric. Therefore, we assessed the presence of mirror-symmetric tuning for the head and/or body in the same sample of aSTS neurons used for the decoding. To assess this for the head, we first averaged the responses, normalized by the maximum responses across the 64 stimuli, for the 8 monkey images having the same head (but a different body) orientation and then computed the distance matrix using the 1 − Pearson <italic>r</italic> distance metric. Similarly, to assess mirror-symmetric tuning for body orientation, we averaged the responses to the monkeys having the same body orientation. The distance matrices showed evidence for mirror-symmetric responses for the lateral head and body orientations for P2 and face orientations of P1 (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Mirror symmetry was less the case for the body of P1, which fits the poorer image symmetry of the body in the case of the P1 lateral views. Thus, the lower sensitivity for mirror-symmetric heads or bodies may have contributed to the lower decoding of head–body angles that differed only in their sign. However, the difference in decoding accuracy between the zero versus straight angle and the left versus right angle, especially when training on the ‘x’ orientations and testing on ‘+’ orientations (<xref ref-type="fig" rid="fig5">Figure 5</xref>), cannot be fully attributed to the mirror symmetry of the avatar images.</p></sec><sec id="s2-4"><title>Decoding head–body orientation angle from the summed responses to body and face versus from responses to the whole body (experiment 2)</title><p>We obtained a second dataset (experiment 2) from the same subjects in which we could test the main findings of experiment 1 for a larger sample of neurons. Additionally, we evaluated whether the angle between the head and the body could be computed from their individual orientations when presented in isolation. We compared the responses to 16 head–body orientation configurations to the responses to the heads and bodies shown in isolation with the same size and at the same visual field location as in the monkey configuration. We recorded with V-probes in mSTS and aSTS, overlapping the monkey patches defined by fMRI (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; Materials and methods). The stimulus set included 16 MC images of the P1 avatar, crossing 4 head orientations and 4 body orientations (the frontal, the back view, and the two lateral views), yielding 16 conditions. In addition, we included images of the isolated heads, presented at the same position as in the avatar (16 conditions), and of the 4 body orientations (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Decoding of head–body angles in the Sum versus Configuration test (experiment 2).</title><p>(<bold>A</bold>) Stimuli of experiment 2: heads and bodies presented at four orientations (frontal, back, and the two lateral ones; B+H), along with all head–body configurations (M). Images were monkey-centered and the isolated heads and bodies were positioned to match their corresponding avatars. The red square indicates the fixation target. (<bold>B</bold>) Decoding accuracies for head–body configurations (M) and head–body sums (B+H) with zero, straight, leftward, and rightward right angles of head and body (trained pooling the four orientations (<bold>A</bold>)), for anterior superior temporal sulcus (aSTS) and mid superior temporal sulcus (mSTS), <italic>n</italic> indicates the number of units. Same conventions as <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Decoding of head–body angles for head- and body-selective anterior superior temporal sulcus (aSTS) units (experiment 2).</title><p>Our sample of units contains a mixture of units selective for heads and/or bodies. To assess whether the head/body selectivity matters for the decoding of head–body orientation angle, we subdivided our aSTS units into groups based on their head/body selectivity. We performed two analyses, using distinct definitions of head/body selectivity. (<bold>A</bold>) We averaged for each aSTS unit the mean net response to the 16 heads (differing in location and orientation) and to the 4 bodies (differing in orientation) and computed a Head/body Selectivity Index (HBI): (mean response to head – mean response to body)/(abs(mean response to head) + abs(mean response to body)), which ranges from –1 (response to the bodies only) to 1 (response to the heads only). The HBI was computed for those aSTS units (<italic>n</italic> = 359) that responded to either the heads or bodies (Materials and methods). Then, we decoded the head–body orientation angles, using the same procedure as described in the main text, for aSTS units with HBI &lt;–0.33 (<italic>n</italic> = 115; top panels) and those with HBI &gt;0.33 (<italic>n</italic> = 179; bottom panels). The decoding accuracies for head–body configurations with zero, straight, leftward, and rightward right angles between the head and the body for the body-selective (top panels) and head-selective (bottom panels) aSTS units are shown using the same conventions as in <xref ref-type="fig" rid="fig6">Figure 6</xref>, main text. Error bars show standard deviations across resamplings, dotted horizontal lines marking the null distribution. Both groups showed a higher classification accuracy for the zero versus straight angles than for the left versus right angle showing that both body- and head-selective units show similar decoding patterns of head–body orientation angle. (<bold>B</bold>) Because our stimuli depicted the same identity (that differed in orientation), we employed also a second definition of head–body selectivity: instead of averaging the responses across the images of a category, we took the maximum response for each category to compute the HBI. Dividing the units in two groups HBI &lt;−0.33 (<italic>n</italic> = 67; top panels) and HBI &gt;0.33 (<italic>n</italic> = 184; bottom panels) showed again a similar pattern of head–body orientation angle, although this pattern was less pronounced for the body-selective units defined based on the maximum response. Overall, these analyses suggest that head- and body-selective aSTS units show, as a population, a similar decoding bias for zero versus straight angles, both confusing the left and right angles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig6-figsupp1-v1.tif"/></fig></fig-group><p>Spike sorting yielded 497 aSTS (M1 = 280; M2 = 217) and 1856 mSTS (M1 = 804; M2 = 1052) units that responded selectively to the 16 monkey conditions (Materials and methods). We decoded the head–body orientation angle for differences of 90° and 180° between angles of the head and body orientation (zero vs. right angle, zero vs. straight angle, etc.), using the same procedure as in experiment 1. We found a highly similar pattern (<italic>n</italic> = 430 units; equated across the two regions and subjects; randomly sampled per subject per resampling; <xref ref-type="fig" rid="fig6">Figure 6B</xref>) compared to that obtained with the smaller sample of single units of experiment 1 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). For aSTS, the classification accuracy for the zero versus straight angles was about 0.9 (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Again, classification accuracy for the right versus left angles, differing only in their sign, was much lower than that obtained for zero versus right angle, or straight versus left angle, differing in absolute angle (<xref ref-type="fig" rid="fig6">Figure 6B</xref>; p &lt; 0.01). The classification accuracy for mSTS units was again lower than for aSTS (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), although no generalization across centering conditions was required for this decoding. Importantly, in mSTS there was no difference in decoding accuracy for the zero versus straight angle and right versus left angle. The reliability of the responses of the units was similar for aSTS (median Spearman–Brown corrected split-half Pearson correlation; M1: 0.70; M2: 0.69) and mSTS (median corrected <italic>r</italic>: M1: 0.72; M2: 0.71) and therefore cannot explain the difference in classification accuracy between the two regions.</p><p>Having replicated the main findings of experiment 1 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), we assessed whether the simultaneous presentation of the head and body was necessary to decode the angle between head and body orientation. To do this, we summed the net responses to the isolated head and body, presented at the same locations as in the whole-monkey image, for individual trials, thus obtaining 8 head–body sums per orientation and angle (zero, straight, and the left and right angles) of head and body orientation (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We decoded the angle using the face–body sums, employing the same procedure and units as for the whole monkey. For aSTS, the classification accuracies for the sums were markedly lower than those obtained for the monkey images (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), and no difference in classification accuracy between the zero versus straight angles and left versus right angles was present for the sums (all p’s &gt; 0.4). The lower classification accuracy for the head–body sums is not due to a lower reliability of the summed responses compared to the whole-body responses (Spearman–Brown corrected split-half Pearson correlations; median whole-body <italic>r</italic>: M1: 0.7; M2: 0.69; sums <italic>r</italic>: M1: 0.76; M2: 0.66). These data suggest that the difference in classification accuracy between the zero versus straight angles and left versus right angles requires the simultaneous presence of the head and body. For mSTS, the classification accuracies for the head–body sums were similar to those obtained for the monkey images (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), suggesting no genuine encoding of the head–body angle in this region.</p></sec><sec id="s2-5"><title>Head- and body-selective aSTS units show similar decoding patterns of head–body orientation angle (experiment 2)</title><p>To assess whether the head/body selectivity matters for the decoding of the head–body orientation angle, we subdivided our aSTS units into groups based on their head/body selectivity (Materials and methods; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Head- and body-selective aSTS units showed, as a population, a similar decoding bias for zero versus straight angles and both confused the left and right angles (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p></sec><sec id="s2-6"><title>Effect of body inversion on responses and view selectivity (experiment 3)</title><p>We determined whether body inversion impairs the decoding accuracy of the head–body orientation angle. Inverting the body affects the global body configuration, keeping the local features intact (except for a 180° change). First, we assessed for the first time whether STS neurons show a body-inversion effect in response strength and/or capacity to discriminate between views of a pose. For this, we obtained a new dataset of aSTS and mSTS responses to the upright and inverted poses of the avatar. We employed the same two poses as for the single units (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). For each pose, we presented eight orientations (steps of 45°) of four head–body orientation angles (zero, straight, and left and right angles) and this upright and inverted (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). The images were MC, with the heads shifted toward the fixation point, as in experiment 1 (Materials and methods; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for examples). In addition, we included heads and bodies shown in isolation for a subset of these conditions (Materials and methods; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>We recorded with V-probes at the same mSTS and aSTS locations (three anterior–posterior levels in each region) as in experiment 2 (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). First, we selected for each pose those units that responded to the head, headless body, or monkey, either upright or inverted (Materials and methods), having the anatomically possible zero head–body angle (see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for examples). Then, for each pose, we selected the orientation (preferred orientation) that produced the maximum response for the stimuli corresponding to that orientation, irrespective of whether the stimuli were upright or inverted. The latter ensured an unbiased selection of the preferred orientation with respect to the upright versus inversion variable. We observed for each pose, in each region of each subject, a higher mean response for the upright compared to the inverted preferred orientation of the whole monkey (Wilcoxon signed-rank tests on data pooled across monkeys: p’s in <xref ref-type="fig" rid="fig7">Figure 7C</xref>; population PSTHs (Peri-Stimulus Time Histograms) in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). Selecting units that responded either to the upright or inverted heads showed less consistent inversion effects for the head than observed for the monkey: upright heads showed across subjects consistently and significantly greater responses than to inverted heads only for P2 in mSTS but not for the other pose or region (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). The same held for the headless body for those units that responded to either the upright or inverted bodies (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). Thus, the inversion effect was consistently present for the head–body configurations, and less so for the face and body only.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Effect of head–body configuration inversion (experiment 3).</title><p>(<bold>A</bold>) Stimuli of the head–body configurations of P1 with zero and straight angles, and leftward and rightward right angles, at eight orientations, upright and inverted. (<bold>B</bold>) Decoding accuracies for the head–body configurations with zero, straight, leftward, and rightward right angles in anterior superior temporal sulcus (aSTS), for poses P1 (top panels) and P2 (bottom panels), <italic>n</italic> indicates the number of units. Solid and dotted lines represent data for upright and inverted configurations, respectively. Same conventions as <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>C</bold>) Responses to the preferred orientation of upright or inverted head–body configurations for aSTS and mid superior temporal sulcus (mSTS) and P1 and P2 (M1 (dark green); M2 (light green)), for the monkey-responsive units. Medians (diamonds) and the number of units per subject (M1: n1; M2: n2) are shown in corresponding colors. p-values of Wilcoxon signed-rank tests (data pooled across subjects) testing the difference in response between upright and inverted are shown (red: significant).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Monkey avatar stimuli for the Upright versus Inverted test (experiment 3).</title><p>(<bold>A</bold>) Employed combinations of head and body orientations for P1 (left panel) and P2 (right panel) of the avatar, shown on a gray background matching the employed one (the same applies for the backgrounds in (<bold>B</bold>) and (<bold>C</bold>)). We presented head–body configurations with zero, straight, leftward, and rightward right angles of the head and the body, presented at eight whole-monkey orientations, both upright and inverted. (<bold>B</bold>) Example stimuli of P1 and P2 from the upright (left panel) and inverted (right panel) conditions, with zero angle of head and body for the eight avatar orientations. A small red square represents the fixation target. (<bold>C</bold>) Headless bodies (<bold>B</bold>) and heads (<bold>H</bold>) for zero and straight angle configurations of the frontal, back, and two lateral views of P1 and P2. Heads and bodies are from the upright (left panel) and inverted (right panel) conditions and were positioned to match their corresponding avatars in (<bold>A</bold>). A small red square represents the fixation target.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Population PSTHs of anterior superior temporal sulcus (aSTS) and mid superior temporal sulcus (mSTS) units recorded in the Upright versus Inverted test (experiment 3).</title><p>Population PSTHs (20 ms bin width) for aSTS and mSTS (top and bottom panels) units displayed for M1 and M2 (left and right panels) and P1 and P2. Responses are shown for upright (solid lines) and inverted (dotted lines) heads, headless bodies, and head–body configurations with zero (in blue) and straight (in red) angles of the head and the body. Averaged responses are for the preferred orientation of responsive units. Shaded areas represent 95% confidence intervals (bootstrapping units; 1000 resamplings), and <italic>n</italic> denotes the number of responsive units for each subject, pose, and region. The stimulus duration is indicated by the black horizontal line (0 is stimulus onset).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Inversion effect of head- and body-responsive anterior superior temporal sulcus (aSTS) and mid superior temporal sulcus (mSTS) units (experiment 3).</title><p>Responses to the preferred orientation of upright or inverted head–body configurations for P1 and P2, of units which responded significantly to an upright or inverted head (in red) or headless body (in blue). Subjects M1 and M2 are represented by dark and light red or blue, respectively. Medians (diamonds) and the number of units per subject (n1 and n2) are shown in the corresponding color. p-values in red indicate significantly higher responses to the upright compared to the inverted configurations for each pose, region, and subject (Wilcoxon signed-rank tests on data pooled across subjects).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-fig7-figsupp3-v1.tif"/></fig></fig-group><p>Behaviorally, the body-inversion effect corresponds to a higher discriminability of upright bodies. To assess this for our STS neurons, we decoded the eight whole-body orientations employing units responding to the whole-body images with a zero body–head angle, either upright or inverted. The decoding was performed separately for upright and inverted images. This was done for each pose, drawing randomly in each of the 1000 resamplings 60 units per region (Materials and methods). The mean decoding accuracy was significantly higher (all p-values &lt;0.01; Materials and methods) for upright compared to inverted monkeys for both poses in aSTS (P1: upright versus inverted: 0.89 vs. 0.82; P2: 0.89 vs. 0.71, with standard deviations SD = 0.02) and mSTS (P1: 0.89 vs. 0.79; P2: 0.93 vs. 0.75; SDs ranging between 0.01 and 0.02).</p></sec><sec id="s2-7"><title>Decoding of head–body orientation angle is impaired for inverted bodies (experiment 3)</title><p>Next, we asked whether the decoding of the head–body orientation angle is affected by inversion. For each pose, we selected units that responded to our stimulus set of eight orientations of four angles for the upright or inverted avatar (<xref ref-type="fig" rid="fig7">Figure 7A</xref>; Materials and methods). Then, we decoded the head–body orientation angle from the aSTS units using the same classification procedure as above. For both poses (<italic>n</italic> = 460 responsive aSTS units; an equal number of units randomly sampled per subject per resampling), the upright images showed again a higher classification performance for the zero versus straight angles and the lowest for the left versus right angles (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The classification accuracy dropped significantly (zero versus straight angle; all p-values &lt;0.01; <italic>n</italic> = 100 resamplings; Materials and methods) for the inverted bodies (<xref ref-type="fig" rid="fig7">Figure 7B</xref>), and this drop in classification accuracy tended to be larger for the zero versus straight angle decoding than for the left versus right angle decoding (the difference between upright and inverted left versus right angle decoding not significant for the two poses; p = 0.2 and 0.18). Thus, the decoding of the head–body orientation angle is impaired when inverting the monkey image.</p><p>The stronger inversion effect for the zero versus straight angles decoding suggests that the head–body angle signal in aSTS depends on the global head–body configuration and is not driven merely by inversion-invariant local features. This aligns with the marked generalization across head–body configuration orientation that we observed in experiment 1 for P1 (training using four orientations and testing using the other four); such across-orientation generalization would not be expected if local features of individual images were employed by the classifier.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We showed here that head and body orientations interact in ventral bank STS neurons, with some neurons being tuned to a particular combination of head and body orientation. Importantly, the head–body orientation angle, and not only the orientations of the head and the body per se, can be robustly decoded from a population of STS neurons. This decoding was greater for aSTS compared to mSTS neurons. There is a marked bias in the decoding of head–body orientation angle from aSTS with better decoding of absolute angle differences (e.g. 0° and 180° angles). Head–body orientation angles that differ only in their sign with respect to the anatomical plane (e.g. 90° vs. –90°angles) can be decoded less from aSTS neurons. Also, we provide here the first demonstration of an inversion effect for a body with a head in the macaque STS: the response strength, orientation decoding, and head–body orientation angle decoding are less for inverted compared to upright monkey avatars.</p><p>We could decode the orientation of the head–body configurations, which is an ecologically important property. From the same population of neurons, we could also decode the angle between the head and the body, irrespective of the orientation of the configuration (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This can be viewed as a sensitivity for a particular head–body configuration that is defined by the orientation of the head <italic>and</italic> body, and is independent of the overall orientation of the agent. Remarkably, decoding of head–body configuration in the aSTS confused to a greater extent configurations in which the head and body orientation angles differ only in their sign (e.g. head turned to the left vs. to the right with respect to the body). The latter is quite a robust effect in the aSTS: we observed it in three different experiments. Images, especially of the standing pose, that differ only in the sign of the head–body angle are also to some extent mirror-symmetric. Our population of single aSTS neurons showed evidence of a lower sensitivity of mirror-symmetrical views for heads and bodies. Thus, one factor that may have contributed to the lower decoding of head–body orientation angles that differ only in sign is mirror symmetry. The highest decoding of the head–body angle occurred for the zero versus straight angles. Basically, this is a classification of a frequently observed head–body configuration, that is the same orientation of head and body, versus an anatomically impossible angle, at least for primates. In fact, all well-decodable absolute differences in angles consisted of a possible versus an impossible angle, while angles that differ only in their sign are either both anatomically possible or both impossible. Thus, this anatomical possibility factor may explain the better decoding for configurations that differ in their absolute angle. However, dissociating this factor from absolute versus signed angle difference, or mirror symmetry, is not trivial, since these three factors covary in monkey configurations when manipulating the orientation of the head and body.</p><p>The decoding of whole-monkey orientation, which can be based on head and/or body orientation, was present in mSTS and aSTS, which aligns with previous studies (<xref ref-type="bibr" rid="bib3">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib13">Kumar et al., 2019</xref>). In line with previous studies (<xref ref-type="bibr" rid="bib13">Kumar et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Meyers et al., 2015</xref>), the decoding of head and body orientation was better in mSTS compared to aSTS (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). However, the decoding of head–body configuration, that is head–body orientation angle, showed the opposite trend: it was worse in mSTS than in aSTS. This agrees with previous work that suggested the presence of head–body integration in the anterior but less so in the posterior temporal cortex (<xref ref-type="bibr" rid="bib6">Fisher and Freiwald, 2015</xref>; <xref ref-type="bibr" rid="bib11">Hu et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>).</p><p>We observed no significant effect of head–body orientation angle on mean response strength, indicating that anatomically possible and impossible configurations elicited statistically similar averaged responses. This differs from a previous study in which responses to head–body configurations in which the body and head were misaligned were lower than those for a natural head–body configuration (<xref ref-type="bibr" rid="bib29">Zafirova et al., 2024</xref>). However, that study targeted a patch that was defined in fMRI by contrasting aligned versus misaligned head–body configurations, which only partially overlapped the fMRI-defined monkey patch of the present study. Furthermore, the present study examined head–body orientation interactions, while the previous one head–body relative position interactions, which are fundamentally different. Nonetheless, the present study suggests that mSTS and aSTS neurons respond equally strongly to anatomically possible and impossible head–body configurations, at least when these are defined by the head–body orientation angle. However, as a population, they can distinguish strikingly well the possible and impossible configurations since these differ in their absolute angle (see above).</p><p>Our monkeys were performing a passive fixation task during the recordings and thus we do not have behavioral data on the discrimination of head–body orientation angles. Based on our electrophysiological data, one would predict that, at the behavioral level, the discrimination of head–body orientation angle, irrespective of the viewpoint of the avatar, would be more accurate for zero versus straight angles compared to right versus left angles, but this remains to be tested psychophysically.</p><p>Our work demonstrates for the first time an inversion effect for bodies <italic>with</italic> heads in the ventral bank of the STS. Previous studies reported a higher response to upright compared to inverted bodies in the STS (<xref ref-type="bibr" rid="bib2">Ashbridge et al., 2000</xref>; <xref ref-type="bibr" rid="bib21">Popivanov et al., 2015</xref>), but the neurons in these studies were searched with upright bodies. However, in our study, a search bias cannot explain the body-inversion effect since we selected responsive units using both upright and inverted images. A body-inversion effect has been observed in both humans and monkeys at the behavioral level (<xref ref-type="bibr" rid="bib9">Griffin and Oswald, 2022</xref>; <xref ref-type="bibr" rid="bib14">Matsuno and Fujita, 2018</xref>). Some but not all human studies (see <xref ref-type="bibr" rid="bib9">Griffin and Oswald, 2022</xref> for a meta-analysis) observed a stronger inversion effect for a body with than without a head. Our data suggest also a more consistent inversion effect for head–body configurations compared to headless bodies and heads in STS.</p><p>The body-inversion effect likely results from greater exposure to upright than inverted bodies during development, although, as for the face-inversion effect, this remains to be proved (<xref ref-type="bibr" rid="bib12">Kobylkov and Vallortigara, 2024</xref>). Our monkeys were not exposed to the anatomically impossible head–body orientations, but still, the responses to these configurations were similar to those to anatomically possible configurations. This apparent discrepancy between the head-inversion effect and the anatomically (im)possible head–body orientation configuration may result from both head and body being in an infrequent orientation in the case of inversion while for the impossible head–body orientation configuration either the head or the body, but not both, might be at an infrequently exposed orientation.</p><p>In conclusion, we demonstrated that head and body orientations interact in the macaque STS. The population of aSTS neurons can provide a reliable signal for the recognition of particular head–body angles, especially between those that differ in their absolute angle with respect to the anatomical midline. This signal is less reliable for inverted head–body configurations, showing a strong inversion effect.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>The experiments were performed with the two male rhesus monkeys (M1 and M2; <italic>Macaca mulatta</italic>, 6–7 years, 11–12 kg; Supplier: BPRC, Rijswijk, The Netherlands) which took part in our fMRI study (<xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>). They were implanted with an MR-compatible plastic headpost for fixing the head during training, scanning, and recording, and a plastic custom-made recording chamber allowing a slightly oblique approach to the fMRI-defined monkey patches (for details, see Data acquisition). All experimental procedures and animal care complied with national and European regulations and were approved by the Animal Ethical Committee at KU Leuven (reference number: 105/2019).</p></sec><sec id="s4-2"><title>Data acquisition</title><p>Before the three electrophysiological experiments, reported here, we performed a functional MRI experiment on the same monkey subjects to determine the location of patches activated more by images of monkeys compared to objects (for details see <xref ref-type="bibr" rid="bib27">Zafirova et al., 2022</xref>). The fMRI localizer had a block design with a Brown-noise backgrounds-only condition (baseline) and the following 6 conditions, 20 images each: monkeys, monkey faces, monkey headless bodies, and three groups of balanced manmade objects for the monkeys, the faces and the bodies, respectively (centered on the Brown-noise backgrounds). We employed the contrast monkeys versus monkey control objects (p &lt; 0.05, Family-wise error corrected) to define the monkey-selective regions (monkey patches) in each subject’s IT. In experiment 1, we targeted single units in the monkey patches in the ventral bank of the STS, lateral to the fundus, at two anterior–posterior levels (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In experiments 2 and 3, we recorded from a wider region, which overlapped with the two monkey patches and the recording locations of experiment 1. This was done in the context of another electrophysiological study in which we mapped the responses to heads and body parts. In these two experiments, the penetrations were at three anterior–posterior levels, spaced 1 mm apart in aSTS and mSTS. At each anterior–posterior level, we made penetrations in 1 mm steps from the lip of the STS toward the fundus (including the ventral part of the fundus). This resulted in a total of 18 and 27 penetrations in aSTS and mSTS, respectively, for both subjects. We recorded from the left hemisphere of M1 and the right hemisphere of M2 in all experiments.</p><p>Before the recording experiments, the subjects underwent an anatomical MRI (3.0T full-body Siemens scanner Magnetom Prisma Fit; magnetization-prepared rapid acquisition with gradient echo sequence; 0.6 mm isotropic voxel resolution). We inserted into the recording chamber grid (until the dura) long plastic capillaries filled with MRI opaque copper sulfate (CuSO<sub>4</sub>) at predetermined positions and filled the recording chamber with a Gadoteric Acid (Dotarem) solution for visualization of the chamber and the grid. The activations were co-registered to the anatomical scans (Freesurfer <ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>; FSLeyes, <ext-link ext-link-type="uri" xlink:href="http://git.fmrib.ox.ac.uk/fsl/fsleyes/fsleyes/">http://git.fmrib.ox.ac.uk/fsl/fsleyes/fsleyes/</ext-link>, SPM12, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_007037">SCR_007037</ext-link>) and the co-registration was verified by visual examination. The recording locations in the sagittal and coronal planes were verified with structural MRIs that visualized the guide tube tracks. The dorsal/ventral locations were based on observation of the white/gray matter transitions and silence associated with the STS during the recordings.</p><p>The position of one eye was continuously tracked with an infrared video-based tracking system (SR Research EyeLink; sampling rate 1 kHz). Stimuli were displayed on a VPixx LCD display (1920 × 1080 resolution; 120 Hz) at a distance of 57 cm from the subjects’ eyes. The onset and offset of the stimulus were signaled utilizing a photodiode detecting luminance changes of a small square in the corner of the display (not visible to the animal), placed in the same frame as the stimulus events. A digital signal processing-based computer system developed in-house controlled stimulus presentation, event timing, and juice delivery while sampling the photodiode signal, vertical, and horizontal eye positions, spikes, and behavioral events.</p><p>Single-unit recordings in experiment 1 were performed with epoxylite-insulated tungsten microelectrodes (FHC) lowered with a Narishige microdrive into the brain using a stainless-steel guide tube that was fixed in a custom-made grid positioned within the recording chamber. After amplification and filtering, spikes of a single unit were isolated online using a custom amplitude- and time-based discriminator.</p><p>Spiking activity was recorded in experiments 2 and 3 with 24-channel V-probes (Plexon). The V-probe was lowered into the brain through a stainless-steel guide tube with a Narishige microdrive. The same grids were employed in experiment 1. After reaching the recording target, we waited for at least 30 min before the tests were run to obtain stable recordings. During that waiting period, the monkey could watch movies. We employed an INTAN RHD recording system to record spiking activity. The signal of each channel was bandpass-filtered online between 500 and 7500 Hz. The photodiode signal from the display was fed into the INTAN recording system, allowing alignment of stimulus events and the condition and trial data saved by the DSP-based computer system. We employed Plexon Offline Sorter to sort the spiking activity of each channel into single units and small clusters of (unsortable) units (multi-units). In this paper, we use the term ‘units’ for both single- and multi-units, combining single- and multi-units in the analyses of experiments 2 and 3. Sample size was determined based on previous studies examining STS units’ selectivity for animate and inanimate objects.</p></sec><sec id="s4-3"><title>Stimuli and experiments</title><p>We report here three experiments designed to investigate the interaction between the orientations of the head and the body. The first experiment included a Selectivity test to assess the selectivity of the neurons recorded in the main Head–body Orientation test. The Selectivity test employed naturalistic grayscale static images centered on a uniform gray background. All subsequent tests employed static images of realistic 3D monkey avatar in color (<ext-link ext-link-type="uri" xlink:href="https://www.turbosquid.com/3d-models/monkey-fur-rigged-1493219">https://www.turbosquid.com/3d-models/monkey-fur-rigged-1493219</ext-link>) on the same uniform gray background.</p><p>First, the avatar was positioned in two emotionally neutral natural monkey poses – a sitting and a standing one (P1 and P2, respectively; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Using a 3D avatar instead of actual monkey images allowed us to manipulate independently the orientation of the head and the body, for the two poses. The head and body orientations were obtained by rotation around the vertical axis of the avatar. The maximal vertical or horizontal extent of the images was set to 6°. Heads and headless bodies of some of the configurations were presented in isolation at their original location which allowed for direct comparison with the corresponding configurations. Details about the head and body orientations and the centering of the configurations for each experiment are described below.</p></sec><sec id="s4-4"><title>Experiment 1</title><sec id="s4-4-1"><title>Selectivity test</title><p>To assess the category selectivity of the neurons of experiment 1, we employed 60 stimuli from a previous fMRI experiment (for the details about the stimuli, see <xref ref-type="bibr" rid="bib29">Zafirova et al., 2024</xref>). In the Selectivity test, we presented 10 images of monkeys in diverse natural postures and orientations, together with 10 images of headless monkey bodies and 10 images of monkey faces, all cropped from different natural photographs of rhesus macaques. Additionally, we selected 10 control objects for each of the groups (examples in <xref ref-type="fig" rid="fig1">Figure 1</xref>). The objects matched the face-, body-, and monkey images as best as possible, as estimated by different shape-descriptive parameters (for details about the images and the matching procedure, see <xref ref-type="bibr" rid="bib29">Zafirova et al., 2024</xref>). The low-level characteristics, like luminance and contrast, were equated across the whole set (Matlab SHINE toolbox <xref ref-type="bibr" rid="bib26">Willenbockel et al., 2010</xref>). The grayscale images were resized to fit a 6° × 6° stimulus window and gamma-corrected.</p></sec><sec id="s4-4-2"><title>Head–body orientation test</title><p>In this test, we manipulated the orientations of the head and the body independently, starting from the frontal view (0°) with eight 45° consecutive steps, generating 64 head–body combinations for the sitting and the standing pose (P1 and P2; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). In terms of angle between the orientation of the head and the body, this resulted in eight head–body configurations with angles 0° (zero angle), 45°, 90° (rightward right angle), 135°, 180° (maximum possible absolute angle: straight angle), –135°, –90° (leftward right angle), and –45° between the head and the body. Each of these eight head–body configurations was spatially rotated with steps of 45° around its vertical axis, that is, each configuration was presented in eight different orientations. Ultimately, we had eight orientations of the head and the headless bodies (0° to –45° with steps of 45°), eight head–body configurations with angles 0° to –45° with steps of 45° angle between the head and the body, and eight different orientations of each of these configurations (rotated from 0° to –45° with steps of 45° around their vertical axis). As a convention, we will further refer to the orientation of the head–body configurations by the orientation of the body (torso). The head–body configurations with an angle between the head and the body more than an absolute angle of 90° were anatomically impossible.</p><p>The configurations were centered on the head (HC condition) or the monkey avatar (MC condition), for each of the two poses P1 and P2 (examples in <xref ref-type="fig" rid="fig2">Figure 2B</xref>). In the HC condition, the maximum extent of the bodies was 2.8° and 3.3° from the fixation point for P1 and P2, respectively. As we were interested in the possible head–body orientation interactions, and to ensure that the orientation of the head was clearly visible, the images were shifted 1° down in the lower visual field in the MC condition, so that the heads were closer to the fixation point. That way, the maximum extent of the head was 1.6° and 2.6° from the fixation point for P1 and P2, respectively, in the MC condition.</p><p>Additionally, heads and headless bodies from the configurations with zero and straight angles (0° and 180°; anatomically possible and impossible) between the head and the body of P1 and P2 were presented in isolation at the same locations as in the head–body configurations. We did that for four orientations of the two configurations: the frontal, the back, and the two lateral ones (0°, 180°, 90°, and –90°). We had four orientations of the headless bodies for the MC condition and four orientations of the head for the HC condition (one from each orientation; <xref ref-type="fig" rid="fig2">Figure 2C</xref>). These were compared to the head–body configuration with the same body orientation for the MC condition and to the head–body configuration with the same face orientation in HC conditions. We presented two head orientations – one from the zero angle and one from the straight angle configurations for each of the four body orientations in the MC condition, and two body orientations – one from the zero angle and one from the straight angle – for each of the four head orientations in the HC condition. Thus, we had four headless bodies and eight heads in the MC condition and four heads and eight bodies in the HC condition, for P1 and P2 (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), or altogether standalone heads and bodies from the zero and straight angle configurations at four orientations, for each centering and pose. In total, we had 152 images ((64 + 12) for the MC and (64 + 12) for the HC condition) for each pose.</p></sec></sec><sec id="s4-5"><title>Experiment 2</title><sec id="s4-5-1"><title>Sum versus configuration test</title><p>In the second experiment, we presented only the sitting pose (P1) of the monkey avatar, centered on the whole body. Additionally, we employed four out of the eight head–body configurations: the zero angle (0°), the straight angle (180°), and the right and left angle configurations (90° and –90°). These were chosen because experiment 1 showed that the 0°–180° pair and the 90°–90° pair showed the largest difference in the decoding of the head–body orientation angle. Each of these configurations was presented at four orientations: the frontal, the back, and the two lateral ones (0°, 180°, 90°, and –90°), determined by the viewpoint of the body (rotated with steps of 90° around its vertical axis; <xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><p>As before, heads and headless bodies from the configurations were presented in isolation at their original locations. We did that for the four orientations. Thus, we had four orientations of the headless bodies. Here, we presented four head orientations from each of the four angles: the zero, the straight, and the left and right ones, resulting in overall 16 standalone heads and four headless bodies (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Overall, the Sum versus Configuration test employed 36 images (16 configurations and 20 images of heads and headless bodies).</p></sec></sec><sec id="s4-6"><title>Experiment 3</title><sec id="s4-6-1"><title>Upright versus inverted test</title><p>In the last experiment, we presented both the sitting and the standing pose (P1 and P2) of the monkey avatar. We employed the same four head–body configurations as in experiment 2. Each of these configurations was presented at the eight orientations (body rotated with steps of 45° around its vertical axis; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Crucially, we introduced one additional manipulation: all images were presented either upright or flipped around their horizontal axis (inverted; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). All were centered on the monkey avatar, but as in the Head–body Orientation test, we wanted to ensure that the orientation of the heads was visible. To that end, we shifted the upright images 1° down in the lower visual field and the inverted images 1° up in the upper visual field, so that the heads were closer to the fixation point (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>We presented also the heads and headless bodies from the zero and straight angle head–body configurations of the two poses in isolation at their original locations. We did that for four orientations: the frontal, the back, and the two lateral ones (0°, 180°, 90°, and –90°). Thus, we had four orientations of the headless bodies per pose. We presented two head orientations from each configuration head–body angle, resulting in four headless bodies and eight heads for a pose (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Overall, this test included 176 images (the upright and inverted configurations of the two poses and their heads and bodies).</p></sec></sec><sec id="s4-7"><title>Procedure</title><p>In experiment 1 (Selectivity and Head–body Orientation test), we recorded well-isolated single neurons, while single- and multi-units were recorded with V-probes in experiments 2 (Sum vs. Configuration test) and 3 (Upright vs. Inverted test). In each subject, we recorded in (experiments 1–3) and surrounding (experiments 2 and 3; see Results) patches that were activated more by images of monkeys compared to monkey control objects, in the anterior (aSTS) and mid ventral bank (aSTS) of the STS (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To control for potential order effects, we recorded first in aSTS in one subject, and the mSTS in the other for each experiment.</p><p>In experiment 1, we recorded in the Head–body Orientation test well-isolated single neurons that responded to monkeys, monkey faces <italic>or</italic> bodies in the preceding Selectivity test. All stimuli in all experiments were presented for 250 ms, with at least 600-ms interstimulus interval, on a uniform gray background. The subjects had to fixate a small (0.2° × 0.2°) red target. They received juice rewards for maintaining fixation in a 2° × 2° window around the target for 300 ms before the stimulus, during, and 300 ms after the stimulus. Trials in which the monkey failed to fixate continuously during this period were not analyzed further. The juice rewards were administered at fixed intervals in experiment 1 and after a completed trial in experiments 2 and 3. All stimuli of a test were presented pseudorandomly in blocks, and the order of the stimuli within a block was random. A stimulus of an aborted trial was presented again at a random moment during the same block.</p></sec><sec id="s4-8"><title>Data analyses</title><p>All analyses were performed only on those units for which we had 5 unaborted trials per stimulus for the Selectivity test and 8 unaborted trials per stimulus for all other experiments, and on net firing rates (FRs) unless stated otherwise. Net FRs were computed by subtracting the mean FR 100 ms before the stimulus onset (baseline window), from the FR in a response window of 50–300 ms after stimulus onset.</p></sec><sec id="s4-9"><title>Unit selection</title><sec id="s4-9-1"><title>Experiment 1</title><p>To determine the responsiveness of a neuron, we computed mean FRs for each trial in two windows: a 200-ms baseline window that started 200 ms before stimulus onset and a 300-ms response window that started 50 ms after stimulus onset. We ran split-plot ANOVAs with within-trial factor baseline versus response window and between-trial factor stimulus. We considered as responsive only those neurons for which (1) either the main effect of the repeated factor or the interaction of the two factors was significant (p &lt; 0.05), and (2) their response was excitatory. Neurons that were irresponsive in the Selectivity test were excluded from the analyses of the Head–body Orientation test (4 out of 102 recorded aSTS and 5 out of 105 recorded mSTS neurons). The tests of the two poses were analyzed separately for responsiveness. Additionally, for each particular analysis, we excluded neurons that were irresponsive to all conditions of that analysis (e.g. separately for each pose and centering). These selections are described in the Results.</p><p>To compute the MSI (see below) we selected for each unit the monkey and the corresponding isolated head and body images (cases) for which there was a significant response to either the monkey, the head, or the body. The significance was tested with a Wilcoxon rank-sum test, comparing for each image the FR in baseline and response windows. The response needed to be excitatory to be considered for further analyses.</p></sec><sec id="s4-9-2"><title>Experiments 2 and 3</title><p>The responsive units of experiments 2 and 3 were selected using more stringent criteria since these were not isolated online using traditional single-unit recording methodology in which the electrode is moved for each unit to isolate it and responses are examined directly online. First, each unit was tested with a split-plot ANOVA with within-trial factor baseline versus response window and between-trial factor stimulus, as for experiment 1. Second, we employed a Kruskal–Wallis test (p &lt; 0.05), performed on the mean raw FRs in the response window per trial and for all stimuli of a test, to exclude non-selective units. Third, only units with a <italic>z</italic>-score &gt;3 for at least one stimulus were considered further. <italic>Z</italic>-scores for each stimulus were computed, as follows: first, we averaged the mean FRs in the baseline window (meanBaseStim) and the response window (meanRespStim) for the eight trials of each stimulus. Then, we calculated the mean (meanBaseAll) and the standard deviation of the baseline (stdBaseAll) across all stimuli. Finally, we subtracted the mean baseline from the averaged response of each stimulus and divided this difference by the standard deviation of the baseline: (meanRespStim − meanBaseAll)/stdBaseAll. The units in each analysis were subjected to the three criteria (split-plot ANOVA, the Kruskal–Wallis test, and the <italic>z</italic>-scoring).</p></sec></sec><sec id="s4-10"><title>Reliability</title><p>We estimated the reliability of the responses of each responsive unit by randomly splitting for each stimulus the eight trials in two groups of four trials, and correlating the raw FR in the response window between the halves across stimuli. The random splitting was performed 100 times. The reliability corresponded to the mean of the 100 Spearman–Brown corrected Pearson correlation coefficients between the two halves. In some analyses, we normalized the correlation between two sets of responses (e.g. two centerings) by the reliability of the two responses by dividing the original correlation by the geometric mean of the reliability of the two response sets.</p></sec><sec id="s4-11"><title>Monkey-sum index</title><p>For each orientation, pose, and centering combination in which the neuron responded significantly either to the monkey (M), head (H), or body (B) (see Unit selection), we computed an MSI, using net FRs, as follows:</p><p>MSI = (Response M − (Response B + Response H))/(abs(Response M) + abs(Response B + Response H)).</p></sec><sec id="s4-12"><title>Interaction of head and body orientation tuning</title><p>For each responsive single unit of experiment 1, we assessed statistically the interaction between head and body orientation for each pose and centering with a two-way ANOVA with factors of head and body orientation. We performed two ANOVAs, one using the raw spike counts per trial and a second one using the following transformation of the raw spike counts per trial: square root (spike count + 3/8). The latter serves to stabilize the variance across conditions for Poisson-like distributions. Both produced similar results. We report the outcome of the ANOVAs using the square root transformed data. We isolated the interaction of head and body orientation for each neuron by computing the residuals of the square-root transformed responses from an additive model in which the responses to a head–body configuration are the sum of the mean responses to the body (computed by averaging responses across the eight head orientations) and head (averaged across the eight body orientations) orientations. The additive model response to the monkey configuration with a head–body orientation (i, j) was modeled as:</p><p>Mean response head orientation (i) + mean response body orientation (j) – mean response to the 64 stimuli.</p><p>The residuals were computed for both centerings and then correlated across the centerings per neuron and pose combination.</p></sec><sec id="s4-13"><title>Decoding analyses</title><p>We performed a series of decoding analyses (see Results) on the raw responses unless stated otherwise with custom Matlab code employing a linear kernel SVM classifier. For the binary classification (pairs of head–body orientation angles) we employed the Matlab function <italic>fitcsvm</italic>. For multiclass classification, we employed the Matlab function <italic>fitcecoc</italic>. The regularization parameter C was 1 (default) in all cases. Per stimulus, we produced 8 pseudo-population vectors after permuting the order of the 8 trials per unit. We performed eightfold cross-validation, training on 7/8 of the pseudopopulation vectors, and the remaining ones were employed for testing. The reported classification scores are means of the percent correct classifications for the test vectors of 100 (or 1000; see below) resamplings of trials (experiments 1–3) and units (experiments 2 and 3). For the main decodings, we ran the same decoding procedure after permuting the stimulus labels, creating null distributions of chance performance. These were run 200 times, using the same number of resamplings. We plot the range of the mean classification scores (mean of 100 resamplings) obtained after label permutation in the figures.</p><sec id="s4-13-1"><title>Decoding of head–body orientation angle</title><p>We decoded the head–body angle for pairs of angles (e.g. zero vs. the maximum possible straight angle), for all possible orientations of the respective head–body configuration. To do this, we created two classes of trials, each containing responses to the same angle (e.g. zero vs. straight angle). The head and body orientations per se were equal between the two classes. Since we had 8 trials of each of 8 (experiments 1 and 3) or 4 (experiment 2) head (and body) orientations per class, there were 64 trials per class in experiments 1 and 3, and 32 trials per class in experiment 2. For each orientation of a certain angle, we kept 7 trials for training, yielding in total 56 (7 × 8 orientations; experiments 1 and 3) or 28 training trials (experiment 2) per class. Test trials consisted of one trial per orientation, that is eight trials. This was repeated eight times for each resampling, each time using a different trial as a test trial, that is the training and testing trials were chosen pseudorandomly for each class separately. This procedure was critical to avoid unbalancing of head and body orientation between classes, which can produce artifactual results (e.g. classification scores well below chance levels). The pairs of angles employed in the decoding are described in the Results.</p><p>We will illustrate our approach for the binary classification of two head–body angles for experiment 1: zero versus straight angle (0° vs. 180°; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). We pooled 8 trials of the 8 orientations of each angle (e.g. zero-angle configuration shown at a 0° (frontal) orientation, 45°, 90° (right lateral), 135°, 180°(back view), –135°, –90° (left lateral), and –45°; <xref ref-type="fig" rid="fig4">Figure 4A</xref>), providing 64 trials per angle. We employed an eightfold cross-validation approach, sampling in each cross-validation step 7 trials of each orientation per angle, ensuring balanced sampling across orientations. The same was done for the other angle (e.g. straight angle; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). These 2 × 56 trials were then employed to train the SVM classifier. Testing was performed using one trial per orientation per angle (in total eight trials).</p><p>To assess the significance of the differences in classification scores between pairs of angles (e.g. decoding of zero vs. straight angle compared to decoding of the leftward vs. rightward right angles) we computed the difference in classification score between the two pairs for each resampling and the percentile of a difference of zero corresponded to the p-value. The significance of a two-tailed test requires this p-value to be smaller than 0.025. Note that the same set of neurons was taken per resampling when comparing differences in classification accuracy between conditions.</p><p>In experiment 1, we performed two types of decoding: (1) cross-centering decoding, where responses to MC images were used for training, and responses to HC images for testing (and vice versa), and (2) within-centering decoding, where both training and testing were within the same centering condition. We report the outcome of each. Additionally, we conducted decodings using responses to four orientations for training (e.g. frontal, back, and the two lateral views; ‘+’ orientations) and responses to the other four orientations for testing (e.g. the oblique views 45°, 135°, –135°, and –45°; ‘x’ orientations), and vice versa (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><p>For experiment 2, we performed also decoding of the summed responses to the isolated bodies and heads. Thus, for each head–body configuration, we summed the response to the corresponding head and body, using trials in which these stimuli were presented in isolation. Then, the decoding was run on the summed responses, using the same procedure as described above for the responses to monkey images, with two exceptions. First, as we employed summed head–body responses, in this decoding analysis we used net FRs (baseline subtracted) instead of raw responses, to avoid inflating the response when computing the sums. Second, to prevent unbalancing of head and body orientations across classes, testing trials were selected pseudorandomly to ensure that head and body orientations were equally represented in the training trials for both classes. The same balancing during cross-validation and the use of net FRs was also the case for the decoding of the angles for the monkey configurations (simultaneous presence of head and body) in experiment 2.</p></sec><sec id="s4-13-2"><title>Decoding of orientation</title><p>The main decodings in experiment 3 were conducted in line with the described default procedure. Additionally, in experiment 3, we decoded the orientation of the monkey for the zero-angle head–body configuration. This was done separately for the upright and inverted poses. We employed eightfold cross-validation, using 7 trials per orientation for training and one per orientation for testing. In each of the 1000 resamplings, we randomly sampled 60 units from the population of units (aSTS P1 <italic>n</italic> = 332, P2 <italic>n</italic> = 369; mSTS P1 <italic>n</italic> = 1298, P2 <italic>n</italic> = 1332; equated per monkey). This number of units (equal to that of the experiment 1 decoding analyses) was used to avoid ceiling effects. The mean of 10 classification accuracies (i.e. of 10 resamplings) was employed to obtain a distribution (<italic>n</italic> = 100) of the differences in classification accuracy between upright and inverted images. The zero-difference value fell outside the distribution of each of the two regions and poses. The reported standard deviations of the classification accuracies are computed using also the means of 10 resamplings.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures and animal care complied with national and European regulations and were approved by the Animal Ethical Committee at KU Leuven (Permit number 105/2019).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-105714-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and code to generate the figures are available: Data: <ext-link ext-link-type="uri" xlink:href="https://osf.io/7t2e8/">https://osf.io/7t2e8/</ext-link>. Codes: <ext-link ext-link-type="uri" xlink:href="https://github.com/Yozafirova/head-body-orientation-interaction">https://github.com/Yozafirova/head-body-orientation-interaction</ext-link> (copy archived at <xref ref-type="bibr" rid="bib28">Zafirova, 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Zafirova</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Head-body orientation interaction</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/7t2e8/">7t2e8</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank I Puttemans, A Hermans, W Depuydt, C Ulens, S Verstraeten, J Helin, ST Riyahi, M De Paep, and Y Gürsoy for technical and administrative support. This research was supported by Fonds Wetenschappelijk Onderzoek (FWO) Vlaanderen (G0E0220N), KU Leuven grant C14/21/111, and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement 856495).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Ponce</surname><given-names>C</given-names></name><name><surname>Livingstone</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neurons that mistook a hat for a face</article-title><source>eLife</source><volume>9</volume><elocation-id>e53798</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53798</pub-id><pub-id pub-id-type="pmid">32519949</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashbridge</surname><given-names>E</given-names></name><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Oram</surname><given-names>MW</given-names></name><name><surname>Jellema</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Effect of image orientation and size on object recognition: responses of single units in the macaque monkey temporal cortex</article-title><source>Cognitive Neuropsychology</source><volume>17</volume><fpage>13</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1080/026432900380463</pub-id><pub-id pub-id-type="pmid">20945169</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>D</given-names></name><name><surname>Meyers</surname><given-names>E</given-names></name><name><surname>Sinha</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Contextually evoked object-specific responses in human visual cortex</article-title><source>Science</source><volume>304</volume><fpage>115</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1126/science.1093110</pub-id><pub-id pub-id-type="pmid">15001712</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gelder</surname><given-names>B</given-names></name><name><surname>Van den Stock</surname><given-names>J</given-names></name><name><surname>Meeren</surname><given-names>HKM</given-names></name><name><surname>Sinke</surname><given-names>CBA</given-names></name><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>Tamietto</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Standing up for the body: recent progress in uncovering the networks involved in the perception of bodies and bodily expressions</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>34</volume><fpage>513</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2009.10.008</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>C</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Whole-agent selectivity within the macaque face-processing system</article-title><source>PNAS</source><volume>112</volume><fpage>14717</fpage><lpage>14722</lpage><pub-id pub-id-type="doi">10.1073/pnas.1512378112</pub-id><pub-id pub-id-type="pmid">26464511</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>W</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Face processing systems: from neurons to real-world social perception</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>325</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013934</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffin</surname><given-names>JW</given-names></name><name><surname>Oswald</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A multilevel Bayesian meta-analysis of the body inversion effect: evaluating controversies over headless and sexualized bodies</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>29</volume><fpage>1558</fpage><lpage>1593</lpage><pub-id pub-id-type="doi">10.3758/s13423-022-02067-3</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hietanen</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Social attention orienting integrates visual information from head and body orientation</article-title><source>Psychological Research</source><volume>66</volume><fpage>174</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1007/s00426-002-0091-8</pub-id><pub-id pub-id-type="pmid">12192446</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Baragchizadeh</surname><given-names>A</given-names></name><name><surname>O’Toole</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Integrating faces and bodies: psychological and neural perspectives on whole person perception</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>112</volume><fpage>472</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.02.021</pub-id><pub-id pub-id-type="pmid">32088346</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobylkov</surname><given-names>D</given-names></name><name><surname>Vallortigara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Face detection mechanisms: nature vs. nurture</article-title><source>Frontiers in Neuroscience</source><volume>18</volume><elocation-id>1404174</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2024.1404174</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Popivanov</surname><given-names>ID</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Transformation of visual representations across ventral stream body-selective patches</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>215</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx320</pub-id><pub-id pub-id-type="pmid">29186363</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuno</surname><given-names>T</given-names></name><name><surname>Fujita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Body inversion effect in monkeys</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0204353</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0204353</pub-id><pub-id pub-id-type="pmid">30303982</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Borzello</surname><given-names>M</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Intelligent information loss: the coding of facial identity, head pose, and non-face information in the macaque face patch system</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7069</fpage><lpage>7081</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3086-14.2015</pub-id><pub-id pub-id-type="pmid">25948258</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moors</surname><given-names>P</given-names></name><name><surname>Germeys</surname><given-names>F</given-names></name><name><surname>Pomianowska</surname><given-names>I</given-names></name><name><surname>Verfaillie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Perceiving where another person is looking: the integration of head and body information in estimating another person’s gaze</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>909</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00909</pub-id><pub-id pub-id-type="pmid">26175711</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of visual body perception</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>636</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1038/nrn2195</pub-id><pub-id pub-id-type="pmid">17643089</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Smith</surname><given-names>PA</given-names></name><name><surname>Potter</surname><given-names>DD</given-names></name><name><surname>Mistlin</surname><given-names>AJ</given-names></name><name><surname>Head</surname><given-names>AS</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name><name><surname>Jeeves</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Neurones responsive to faces in the temporal cortex: studies of functional organization, sensitivity to identity and relation to perception</article-title><source>Human Neurobiology</source><volume>3</volume><fpage>197</fpage><lpage>208</lpage><pub-id pub-id-type="pmid">6526706</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Smith</surname><given-names>PA</given-names></name><name><surname>Potter</surname><given-names>DD</given-names></name><name><surname>Mistlin</surname><given-names>AJ</given-names></name><name><surname>Head</surname><given-names>AS</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name><name><surname>Jeeves</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Visual cells in the temporal cortex sensitive to face view and gaze direction</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>223</volume><fpage>293</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1098/rspb.1985.0003</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Hietanen</surname><given-names>JK</given-names></name><name><surname>Oram</surname><given-names>MW</given-names></name><name><surname>Benson</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Organization and functions of cells responsive to faces in the temporal cortex</article-title><source>Philos.Trans.R.Soc.Lond B Biol.Sci</source><volume>335</volume><fpage>23</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1093/oso/9780198522614.003.0005</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popivanov</surname><given-names>ID</given-names></name><name><surname>Jastorff</surname><given-names>J</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tolerance of macaque middle STS body patch neurons to shape-preserving stimulus transformations</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>1001</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00762</pub-id><pub-id pub-id-type="pmid">25390202</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rollenhagen</surname><given-names>JE</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mirror-image confusion in single neurons of the macaque inferotemporal cortex</article-title><source>Science</source><volume>287</volume><fpage>1506</fpage><lpage>1508</lpage><pub-id pub-id-type="doi">10.1126/science.287.5457.1506</pub-id><pub-id pub-id-type="pmid">10688803</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taubert</surname><given-names>J</given-names></name><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>One object, two networks? Assessing the relationship between the face and body-selective regions in the primate visual system</article-title><source>Brain Structure &amp; Function</source><volume>227</volume><fpage>1423</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1007/s00429-021-02420-7</pub-id><pub-id pub-id-type="pmid">34792643</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of face perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>411</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094238</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vrancken</surname><given-names>L</given-names></name><name><surname>Germeys</surname><given-names>F</given-names></name><name><surname>Verfaillie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Holistic integration of gaze cues in visual face and body perception: evidence from the composite design</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>24</elocation-id><pub-id pub-id-type="doi">10.1167/17.1.24</pub-id><pub-id pub-id-type="pmid">28114497</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Sadr</surname><given-names>J</given-names></name><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Horne</surname><given-names>GO</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Tanaka</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the SHINE toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zafirova</surname><given-names>Y</given-names></name><name><surname>Cui</surname><given-names>D</given-names></name><name><surname>Raman</surname><given-names>R</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Keep the head in the right place: face-body interactions in inferior temporal cortex</article-title><source>NeuroImage</source><volume>264</volume><elocation-id>119676</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119676</pub-id><pub-id pub-id-type="pmid">36216293</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zafirova</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Head-body-orientation-interaction</data-title><version designator="swh:1:rev:89ed1fcff36d253f3cffada04710fcd19d2f96db">swh:1:rev:89ed1fcff36d253f3cffada04710fcd19d2f96db</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:34c775a3cb6b082900b978849b11021e5369e171;origin=https://github.com/Yozafirova/head-body-orientation-interaction;visit=swh:1:snp:86e73a842692de036ec7506c4ec5ed9cf97d4050;anchor=swh:1:rev:89ed1fcff36d253f3cffada04710fcd19d2f96db">https://archive.softwareheritage.org/swh:1:dir:34c775a3cb6b082900b978849b11021e5369e171;origin=https://github.com/Yozafirova/head-body-orientation-interaction;visit=swh:1:snp:86e73a842692de036ec7506c4ec5ed9cf97d4050;anchor=swh:1:rev:89ed1fcff36d253f3cffada04710fcd19d2f96db</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zafirova</surname><given-names>Y</given-names></name><name><surname>Bognár</surname><given-names>A</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Configuration-sensitive face-body interactions in primate visual cortex</article-title><source>Progress in Neurobiology</source><volume>232</volume><elocation-id>102545</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2023.102545</pub-id><pub-id pub-id-type="pmid">38042248</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.105714.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study examines the neuronal mechanisms underlying visual perception of integrated face and body cues. The innovative paradigm, which employs monkey avatars in combination with electrophysiological recordings from fMRI-defined brain areas, provides <bold>compelling</bold> evidence on face and body integration. These results should be of wide interest to system and cognitive neuroscientists, psychologists, and behavioural biologists working on visual and social cognition.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.105714.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The study addresses how faces and bodies are integrated in two STS face areas revealed by fMRI in the primate brain. It is building upon recordings and analysis of the responses of large populations of neurons to three sets of images, that vary face and body positions. These sets allowed the author to thoroughly investigate invariance to position on the screen (MC HC), to pose (P1 P2), to rotation (0 45 90 135 180 225 270 315), to inversion, to possible and impossible postures (all vs straight), to presentation of head and body together or in isolation. By analyzing neuronal responses, they find that different neurons showed preferences for body orientation, or head orientation or for the interaction between the two. By using a linear support vector machine classifier, they show that the neuronal population can decode head-body angle presented across orientations, in the anterior aSTS patch (but not middle mSTS patch), except for mirror orientation. On the contrary, mSTS neurons show less invariance for head-body angle and more specialization for head or body orientation.</p><p>Strengths:</p><p>These results expand prior work on the role of Anterior STS fundus face area in face-body integration and its invariance to mirror symmetry, with a rigorous set of stimuli revealing the workings of these neuronal populations in processing individuals as a whole, in an important series of carefully designed conditions.</p><p>It also raises questions for future investigations comparing humans and monkeys expertise with upright and inverted configurations, in light of monkey-specific hanging upside-down behavior. Further, using two types of body postures (sitting, standing), they show a correlation in head-body angle between postures, suggesting that monkey orientation might be more meaningful to these neurons than precise posture.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.105714.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper investigates the neuronal encoding of the relationship between head and body orientations in the brain. Specifically, the authors focus on the angular relationship between the head and body by employing virtual avatars. Neuronal responses were recorded electrophysiologically from two fMRI-defined areas in the superior temporal sulcus and analyzed using decoding methods. They found that: (1) anterior STS neurons encode head-body angle configurations; (2) these neurons distinguish aligned and opposite head-body configurations effectively, whereas mirror-symmetric configurations are more difficult to differentiate; and (3) an upside-down inversion diminishes the encoding of head-body angles. These findings advance our understanding of how visual perception of individuals is mediated, providing a fundamental clue as to how the primate brain processes the relationship between head and body-a process that is crucial for social communication.</p><p>Strengths:</p><p>The paper is clearly written, and the experimental design is thoughtfully constructed and detailed. The use of electrophysiological recordings from fMRI-defined areas elucidated the mechanism of head-body angle encoding at the level of local neuronal populations. Multiple experiments, control conditions, and detailed analyses thoroughly examined various factors that could affect the decoding results. The decoding methods effectively and consistently revealed the encoding of head-body angles in the anterior STS neurons. Consequently, this study offers valuable insights into the neuronal mechanisms underlying our capacity to integrate head and body cues for social cognition-a topic that is likely to captivate readers in this field.</p><p>Weaknesses:</p><p>I did not identify any major weaknesses in this paper.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.105714.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Zafirova et al. investigated the interaction of head and body orientation in the macaque superior temporal sulcus (STS). Combining fMRI and electrophysiology, they recorded responses of visual neurons to a monkey avatar with varying head and body orientations. They found that STS neurons integrate head and body information in a nonlinear way, showing selectivity for specific combinations of head-body orientations. Head-body configuration angles can be reliably decoded, particularly for neurons in the anterior STS, suggesting a transformation of face/body orientation signals from the middle to the anterior STS. Furthermore, body inversion resulted in reduced decoding of head-body configuration angles. Compared to previous work that examined face or body alone, this study demonstrates how head and body information are integrated to compute a socially meaningful signal.</p><p>Strengths:</p><p>This work presents an elegant design of visual stimuli, with a monkey avatar of varying head and body orientations, making the analysis and interpretation straightforward. Together with several control experiments, the authors systematically investigated different aspects of head-body integration in the macaque STS. The results and analyses of the paper are convincing.</p><p>Weakness:</p><p>While this work has characterized the neural integration of head and body information in detail, it's unclear how the neural representation relates to the animal's perception. Behavioural experiments using the same set of stimuli could help address this question, but I agree that these additional experiments may be beyond the scope of the current paper.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.105714.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zafirova</surname><given-names>Yordanka</given-names></name><role specific-use="author">Author</role><aff><institution>KU Leuven</institution><addr-line><named-content content-type="city">LEUVEN</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Vogels</surname><given-names>Rufin</given-names></name><role specific-use="author">Author</role><aff><institution>KU Leuven</institution><addr-line><named-content content-type="city">LEUVEN</named-content></addr-line><country>Belgium</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>The study addresses how faces and bodies are integrated in two STS face areas revealed by fMRI in the primate brain. It builds upon recordings and analysis of the responses of large populations of neurons to three sets of images, that vary face and body positions. These sets allowed the authors to thoroughly investigate invariance to position on the screen (MC HC), to pose (P1 P2), to rotation (0 45 90 135 180 225 270 315), to inversion, to possible and impossible postures (all vs straight), to the presentation of head and body together or in isolation. By analyzing neuronal responses, they found that different neurons showed preferences for body orientation, head orientation, or the interaction between the two. By using a linear support vector machine classifier, they show that the neuronal population can decode head-body angle presented across orientations, in the anterior aSTS patch (but not middle mSTS patch), except for mirror orientation.</p><p>Strengths:</p><p>These results extend prior work on the role of Anterior STS fundus face area in face-body integration and its invariance to mirror symmetry, with a rigorous set of stimuli revealing the workings of these neuronal populations in processing individuals as a whole, in an important series of carefully designed conditions.</p><p>Minor issues and questions that could be addressed by the authors:</p><p>(1) Methods. While monkeys certainly infer/recognize that individual pictures refer to the same pose with varying orientations based on prior studies (Wang et al.), I am wondering whether in this study monkeys saw a full rotation of each of the monkey poses as a video before seeing the individual pictures of the different orientations, during recordings.</p></disp-quote><p>The monkeys had not been exposed to videos of a rotating monkey pose before the recordings. However, they were reared and housed with other monkeys, providing them with ample experience of monkey poses from different viewpoints.</p><disp-quote content-type="editor-comment"><p>(2) Experiment 1. The authors mention that neurons are preselected as face-selective, body-selective, or both-selective. Do the Monkey Sum Index and ANOVA main effects change per Neuron type?</p></disp-quote><p>We have performed a new analysis to assess whether the Monkey Sum Index is related to the response strength for the face versus the body as measured in the Selectivity Test of Experiment 1. To do this we selected face- and body-category selective neurons, as well as neurons responding selectively to both faces and bodies. First, we selected those neurons that responded significantly to either faces, bodies, or the two control object categories, using a split-plot ANOVA for these 40 stimuli. From those neurons, we selected face-selective ones having at least a twofold larger mean net response to faces compared to bodies (faces &gt; 2 * bodies) and the control objects for faces (faces &gt; 2* objects). Similarly, a body-selective neuron was defined by a twofold larger mean net response to bodies compared to faces and the control objects for bodies. A body-and-face selective neuron was defined as having a twofold larger net response to the faces compared to their control objects, and to bodies compared to their control objects, with the ratio between mean response to bodies and faces being less than twofold. Then, we compared the distribution of the Monkey Sum Index (MSI) for each region (aSTS; mSTS), pose (P1, P2), and centering (head- (HC) or monkey-centered (MC)) condition. Too few body-and-face selective neurons were present in each combination of region, pose, and centering (a maximum of 7) to allow a comparison of their MSI distribution with the other neuron types. The Figure below shows the distribution of the MSI for the different orientation-neuron combinations for the body- and face-selective neurons (same format as in Figure 3a, main text). The number of body-selective neurons, according to the employed criteria, varied from 21 to 29, whereas the number of face-selective neurons ranged from 14 to 24 (pooled across monkeys). The data of the two subjects are shown in a different color and the number of cases for each subject is indicated (n1: number of cases for M1; n2: number of cases for M2). The arrows indicate the medians for the data pooled across the monkey subjects. For the MC condition, the MSI tended to be more negative (i.e. relatively less response to the monkey compared to the sum of the body and face responses) for the face compared to the body cells, but this was significant only for mSTS and P1 (p = 0.043; Wilcoxon rank sum test; tested after averaging the indices per neuron to avoid dependence of indices within a neuron). No consistent, nor significant tendencies were observed for the HC stimuli. This absence of a consistent relationship between MSI and face- versus body-selectivity is in line with the absence of a correlation between the MSI and face- versus body-selectivity using natural images of monkeys in a previous study (Zafirova Y, Bognár A, Vogels R. Configuration-sensitive face-body interactions in primate visual cortex. Prog Neurobiol. 2024 Jan;232:102545).</p><p>We did not perform a similar analysis for the main effects of the two-way ANOVA because the very large majority of neurons showed a significant effect of body orientation and thus no meaningful difference between the two neuron types can be expected.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-105714-sa4-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(3) I might have missed this information, but the correlation between P1 and P2 seems to not be tested although they carry similar behavioral relevance in terms of where attention is allocated and where the body is facing for each given head-body orientation.</p></disp-quote><p>Indeed, we did not compute this correlation between the responses to the sitting (P1) and standing (P2) pose avatar images. However, as pointed out by the reviewer, one might expect such correlations because of the same head orientations and body-facing directions. Thus, we computed the correlation between the 64 head-body orientation conditions of P1 and P2 for those neurons that were tested with both poses and showed a response for both poses (Split-plot ANOVA). This was performed for the Head-Centered and Monkey-Centered tests of Experiment 1 for each monkey and region. Note that not all neurons were tested with both poses (because of failure to maintain isolation of the single unit in both tests or the monkey stopped working) and not all neurons that were recorded in both tests showed a significant response for both poses, which is not unexpected since these neurons can be pose selective. The distribution of the Pearson correlation coefficients of the neurons with a significant response in both tests is shown in Figure S1. The median correlation coefficient was significantly larger than zero for each region, monkey, and centering condition (outcome of Wilcoxon tests, testing whether the median was different from zero (p1 = p-value for M1; p2: p-value for M2) in Figure), indicating that the effect of head and/or body orientation generalizes across pose. We have noted this now in the Results (page 12) and added the Figure (New Figure S1) in the Suppl. Material.</p><disp-quote content-type="editor-comment"><p>(4) Is the invariance for position HC-MC larger in aSTS neurons compared to mSTS neurons, as could be expected from their larger receptive fields?</p></disp-quote><p>Yes, the position tolerance of the interaction of body and head orientation was significantly larger for aSTS compared to mSTS neurons, as we described on pages 11 and 12 of the Results. This is in line with larger receptive fields in aSTS than in mSTS. However, we did not plot receptive fields in the present study.</p><disp-quote content-type="editor-comment"><p>(5) L492 &quot;The body-inversion effect likely results from greater exposure to upright than inverted bodies during development&quot;. Monkeys display more hanging upside-down behavior than humans, however, does the head appear more tilted in these natural configurations?</p></disp-quote><p>Indeed, infant monkeys do spend some time hanging upside down from their mother's belly. While we lack quantitative data on this behavior, casual observations suggest that even young monkeys spend more time upright. The tilt of the head while hanging upside down can vary, just as it does in standing or sitting monkeys (as when they search for food or orient to other individuals). To our knowledge, no quantitative data exist on the frequency of head tilts in upright versus upside-down monkeys. Therefore, we refrain from further speculation on this interesting point, which warrants more attention.</p><disp-quote content-type="editor-comment"><p>(6) Methods in Experiment 1. SVM. How many neurons are sufficient to decode the orientation?</p></disp-quote><p>The number of neurons that are needed to decode the head-body orientation angle depends on which neurons are included, as we show in a novel analysis of the data of Experiment 1. We employed a neuron-dropping analysis, similar to Chiang et al. (Chiang FK, Wallis JD, Rich EL. Cognitive strategies shift information from single neurons to populations in prefrontal cortex. Neuron. 2022 Feb 16;110(4):709-721) to assess the positive (or negative) contribution of each neuron to the decoding performance. We performed cross-validated linear SVM decoding N times, each time leaving out a different neuron (using N-1 neurons; 2000 resamplings of pseudo-population vectors). We then ranked decoding accuracies from highest to lowest, identifying the ‘worst’ (rank 1) to ‘best’ (rank N) neurons. Next, we conducted N decodings, incrementally increasing the number of included neurons from 1 to N, starting with the worst-ranked neuron (rank 1) and sequentially adding the next (rank 2, rank 3, etc.). This analysis focused on zero versus straight angle decoding in the aSTS, as it yielded the highest accuracy. We applied it when training on MC and testing on HC for each pose. Plotting accuracy as a function of the number of included neurons suggested that less than half contributed positively to decoding. We show also the ten “best” neurons for each centering condition and pose. These have a variety of tuning patterns for head and body orientation suggesting that the decoding of head-body orientation angle depends on a population code. Notably, the best-ranked (rank N) neuron alone achieved above-chance accuracy. We have added this interesting and novel result to the Results (page 16) and Suppl. Material (new Figure S3).</p><disp-quote content-type="editor-comment"><p>(7) Figure 3D 3E. Could the authors please indicate for each of these neurons whether they show a main effect of face, body, or interaction, as well as their median corrected correlation to get a flavor of these numbers for these examples?</p></disp-quote><p>We have indicated these now in Figure 3.</p><disp-quote content-type="editor-comment"><p>(8) Methods and Figure 1A. It could be informative to precise whether the recordings are carried in the lateral part of the STS or in the fundus of the STS both for aSTS and mSTS for comparison to other studies that are using these distinctions (AF, AL, MF, ML).</p></disp-quote><p>In experiment 1, the recording locations were not as medial as the fundus. For experiments 2 and 3, the ventral part of the fundus was included, as described in the Methods. We have added this to the Methods now (page 31).</p><p>Wang, G., Obama, S., Yamashita, W. et al. Prior experience of rotation is not required for recognizing objects seen from different angles. Nat Neurosci 8, 1768-1775 (2005). <ext-link ext-link-type="uri" xlink:href="https://doi-org.insb.bib.cnrs.fr/10.1038/nn1600">https://doi-org.insb.bib.cnrs.fr/10.1038/nn1600</ext-link></p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>This paper investigates the neuronal encoding of the relationship between head and body orientations in the brain. Specifically, the authors focus on the angular relationship between the head and body by employing virtual avatars. Neuronal responses were recorded electrophysiologically from two fMRI-defined areas in the superior temporal sulcus and analyzed using decoding methods. They found that: (1) anterior STS neurons encode head-body angle configurations; (2) these neurons distinguish aligned and opposite head-body configurations effectively, whereas mirror-symmetric configurations are more difficult to differentiate; and (3) an upside-down inversion diminishes the encoding of head-body angles. These findings advance our understanding of how visual perception of individuals is mediated, providing a fundamental clue as to how the primate brain processes the relationship between head and body - a process that is crucial for social communication.</p><p>Strengths:</p><p>The paper is clearly written, and the experimental design is thoughtfully constructed and detailed. The use of electrophysiological recordings from fMRI-defined areas elucidated the mechanism of head-body angle encoding at the level of local neuronal populations. Multiple experiments, control conditions, and detailed analyses thoroughly examined various factors that could affect the decoding results. The decoding methods effectively and consistently revealed the encoding of head-body angles in the anterior STS neurons. Consequently, this study offers valuable insights into the neuronal mechanisms underlying our capacity to integrate head and body cues for social cognition-a topic that is likely to captivate readers in this field.</p><p>Weaknesses:</p><p>I did not identify any major weaknesses in this paper; I only have a few minor comments and suggestions to enhance clarity and further strengthen the manuscript, as detailed in the Private Recommendations section.</p><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>Zafirova et al. investigated the interaction of head and body orientation in the macaque superior temporal sulcus (STS). Combining fMRI and electrophysiology, they recorded responses of visual neurons to a monkey avatar with varying head and body orientations. They found that STS neurons integrate head and body information in a nonlinear way, showing selectivity for specific combinations of head-body orientations. Head-body configuration angles can be reliably decoded, particularly for neurons in the anterior STS. Furthermore, body inversion resulted in reduced decoding of head-body configuration angles. Compared to previous work that examined face or body alone, this study demonstrates how head and body information are integrated to compute a socially meaningful signal.</p><p>Strengths:</p><p>This work presents an elegant design of visual stimuli, with a monkey avatar of varying head and body orientations, making the analysis and interpretation straightforward. Together with several control experiments, the authors systematically investigated different aspects of head-body integration in the macaque STS. The results and analyses of the paper are mostly convincing.</p><p>Weaknesses:</p><p>(1) Using ANOVA, the authors demonstrate the existence of nonlinear interactions between head and body orientations. While this is a conventional way of identifying nonlinear interactions, it does not specify the exact type of the interaction. Although the computation of the head-body configuration angle requires some nonlinearity, it's unclear whether these interactions actually contribute. Figure 3 shows some example neurons, but a more detailed analysis is needed to reveal the diversity of the interactions. One suggestion would be to examine the relationship between the presence of an interaction and the neural encoding of the configuration angle.</p></disp-quote><p>This is an excellent suggestion. To do this, one needs to identify the neurons that contribute to the decoding of head-body orientation angles. For that, we employed a neuron-dropping analysis, similar to Chiang et al. (Chiang FK, Wallis JD, Rich EL. Cognitive strategies shift information from single neurons to populations in prefrontal cortex. Neuron. 2022 Feb 16;110(4):709-721.) to assess the positive (or negative) contribution of each neuron to the decoding performance. We performed cross-validated linear SVM decoding N times, each time leaving out a different neuron (using N-1 neurons; 2000 resamplings of pseudo-population vectors). We then ranked decoding accuracies from highest to lowest, identifying the ‘worst’ (rank 1) to ‘best’ (rank N) neurons. Next, we conducted N decodings, incrementally increasing the number of included neurons from 1 to N, starting with the worst-ranked neuron (rank 1) and sequentially adding the next (rank 2, rank 3, etc.). This analysis focused on zero versus straight angle decoding in the aSTS, as it yielded the highest accuracy. We applied it when training on MC and testing on HC for each pose. Plotting accuracy as a function of the number of included neurons suggested that less than half contributed positively to decoding (see Figure S3). We examined the tuning for head and body orientation of the 10 “best” neurons (Figure S3). For half or more of those the two-way ANOVA showed a significant interaction. These are indicated by the red color in the Figure. They showed a variety of tuning patterns for head and body orientation, suggesting that the decoding of the head-body orientation angle results from a combination of neurons with different tuning profiles. Based on a suggestion from reviewer 2, we performed for each neuron of experiment 1 a one-way ANOVA with as factor head-body orientation angle. To do that, we combined all 64 trials that had the same head-body orientation angle. The percentage of neurons (required to be responsive in the tested condition) for which this one-way ANOVA was significant was low but larger than the expected 5% (Type 1 error), with a median of 16.5% (range: 3 to 23%) in aSTS and 8% for mSTS (range: 0-19%). However, a higher percentage of the 10 best neurons for each pose (indicated by the star) showed a significant one-way ANOVA for angle (for P1, MC: 50% (95% confidence interval (CI): 19% – 81%); P1, HC: 70% (CI: 35% - 93%); P2, MC: 70% (CI: 35% – 93%); P2: HC: 50% (CI: 19%-81%)). These percentages were significantly higher than expected for a random sample from the population of neurons for each pose-centering combination (expected percentages listed in the same order as above: 16%, 13%, 16%, and 10%; all outside CI). Thus, for at least half of the “best” neurons, the response differed significantly among the head-orientation angles at the single neuron level. Nonetheless, the tuning profiles were diverse, suggesting a populationl code for head-body orientation angle. We have added this interesting and novel result to the Results (page 16) and Suppl. Material (Figure S3).</p><disp-quote content-type="editor-comment"><p>(2) Figure 4 of the paper shows a better decoding of the configuration angle in the anterior STS than in the middle STS. This is an interesting result, suggesting a transformation in the neural representation between these two areas. However, some control analyses are needed to further elucidate the nature of this transformation. For example, what about the decoding of head and body orientations - dose absolute orientation information decrease along the hierarchy, accompanying the increase in configuration information?</p></disp-quote><p>We have performed now two additional analyses, one in which we decoded the orientation of the head and another one in which we decoded the orientation of the body. We employed the responses to the avatar of experiment 1, using the same sample of neurons of which we decoded the head-body orientation angle. To decode the head orientation, the trials with identical head orientation, irrespective of their body orientation, were given the same label. For this, we employed only responses in the head-centered condition. To decode the body orientation, the trials with identical body orientation, irrespective of their head orientation, had the same label, and we employed only responses in the body-centered condition. The decoding was performed separately for each pose (P1 and P2) and region. We decoded either the responses of 20 neurons (10 randomly sampled from each monkey for each of the 1000 resamplings), 40 neurons (20 randomly sampled per monkey), or 60 neurons (30 neurons per monkey) since the sample of 60 neurons yielded close to ceiling performance for the body orientation decoding. For each pose, the body orientation decoding was worse for aSTS than for mSTS, although this difference reached significance only for P1 and for the 40 neurons sample of P2 (p &lt; 0.025; two-tailed test; same procedure as employed for testing the significance of the decoding of whole-body orientation for upright versus inverted avatars (Experiment 3)). Face orientation decoding was significantly worse for aSTS compared to mSTS. These results are in line with the previously reported decreased decoding of face orientation in the anterior compared to mid-STS face patches (Meyers EM, Borzello M, Freiwald WA, Tsao D. Intelligent information loss: the coding of facial identity, head pose, and non-face information in the macaque face patch system. J Neurosci. 2015 May 6;35(18):7069-81), and decreased decoding of body orientation in anterior compared to mid-STS body patches (Kumar S, Popivanov ID, Vogels R. Transformation of Visual Representations Across Ventral Stream Body-selective Patches. Cereb Cortex. 2019 Jan 1;29(1):215-229). As mentioned by the reviewer, this contrasts with the decoding of the head-body orientation angle, which increases when moving more anteriorly. We mention this finding now in the Discussion (page 27) and present the new Figure S10 in the Suppl. Material.</p><disp-quote content-type="editor-comment"><p>(3) While this work has characterized the neural integration of head and body information in detail, it's unclear how the neural representation relates to the animal's perception. Behavioural experiments using the same set of stimuli could help address this question, but I agree that these additional experiments may be beyond the scope of the current paper. I think the authors should at least discuss the potential outcomes of such experiments, which can be tested in future studies.</p></disp-quote><p>Unfortunately, we do not have behavioral data. One prediction would be that the discrimination of head-body orientation angle, irrespective of the viewpoint of the avatar, would be more accurate for zero versus straight angles compared to the right versus left angles. We have added this to the Discussion (page 28).</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) P22 L373. It should read Figure S5C instead of S4C.</p></disp-quote><p>Thanks; corrected.</p><disp-quote content-type="editor-comment"><p>(2) Figure 7B. All inverted decoding accuracies, although significantly lower than upright decoding accuracies, appear significantly above baseline. Should the title be amended accordingly?</p></disp-quote><p>Thanks for pointing this out. To avoid future misunderstanding we have changed the title to:</p><p>“Integration of head and body orientations in the macaque superior temporal sulcus is stronger for upright bodies”</p><disp-quote content-type="editor-comment"><p>(3) Discussion L432-33. &quot;with some neurons being tuned to a particular orientation of both the head and the body&quot;. Wouldn't that be visible as a diagonal profile on the normalized net responses in Fig 3D? Or can the Anova evidence such a tuning?</p></disp-quote><p>We meant to say that some neurons were tuned to a particular combination of head and body orientation, like the third aSTS example neuron shown in Figure 3D. We have corrected the sentence.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>Major comment:</p><p>This paper effectively demonstrates that the angular relationship between the head and body can be decoded from population responses in the anterior STS. In other words, these neurons encode information about the head-body angle. However, how exactly do these neurons encode this information? Given that the study employed electrophysiological recordings from a local population of neurons, it might be possible to provide additional data on the response patterns of individual neurons to shed light on the underlying encoding mechanisms.</p><p>Although the paper already presents example response patterns (Figures 3D, E) and shows that STS neurons encode interactions between head and body orientations (Figure 3B), it remains unclear whether the angle difference between the head and body has a systematic effect on neuronal responses. For instance, a description of whether some neurons preferentially encode specific head-body angle differences (e.g., a &quot;45-degree angle neuron&quot;), or additional population analyses such as a one-way ANOVA with angle difference as the main effect (or two-way ANOVA with angle difference as one of the main effect), would be very informative. Such data could offer valuable insights into how individual neurons contribute to the encoding of head-body angle differences-a detail that may also be reflected in the decoding results. Alternatively, it is possible that the encoding of head-body angle is inherently complex and only discernible via decoding methods applied to population activity. Either scenario would provide interesting and useful information to the field.</p></disp-quote><p>We have performed two additional analyses which are relevant to this comment. First, we attempted to relate the tuning for body and head orientation with the decoding of the head-body orientation angle. To do this, one needs to identify the neurons that contribute to the decoding of head-body orientation angles. For that, we employed a neuron-dropping analysis, similar to Chiang et al. (Chiang FK, Wallis JD, Rich EL. Cognitive strategies shift information from single neurons to populations in prefrontal cortex. Neuron. 2022 Feb 16;110(4):709-721.) to assess the positive (or negative) contribution of each neuron to the decoding performance. We performed cross-validated linear SVM decoding N times, each time leaving out a different neuron (using N-1 neurons; 2000 resamplings of pseudo-population vectors). We then ranked decoding accuracies from highest to lowest, identifying the ‘worst’ (rank 1) to ‘best’ (rank N) neurons. Next, we conducted N decodings, incrementally increasing the number of included neurons from 1 to N, starting with the worst-ranked neuron (rank 1) and sequentially adding the next (rank 2, rank 3, etc.). This analysis focused on zero versus straight angle decoding in the aSTS, as it yielded the highest accuracy. We applied it when training on MC and testing on HC for each pose. Plotting accuracy as a function of the number of included neurons suggested that less than half contributed positively to decoding (see Figure S3). We examined the tuning for head and body orientation of the 10 “best” neurons (Figure S3). For half or more of those the two-way ANOVA showed a significant interaction. These are indicated by the red color in the Figure. They showed a variety of tuning patterns for head and body orientation, suggesting that the decoding of the head-body orientation angle results from a combination of neurons with different tuning profiles.</p><p>Second, we have followed the suggestion of the reviewer to perform for each neuron of experiment 1 a one-way ANOVA with as factor head-body orientation angle. To do that, we combined all 64 trials that had the same head-body orientation angle. The percentage of neurons (required to be responsive in the tested condition) for which this one-way ANOVA was significant is shown in the Tables below for each region, separately for each pose (P1, P2), centering condition (MC = monkey-centered; HC = head-centered) and monkey subject (M1, M2). The percentages were low but larger than the expected 5% (Type 1 error), with a median of 16.5% (range: 3 to 23%) in aSTS and 8% for mSTS (range: 0-19%).</p><table-wrap id="sa4table1" position="float"><label>Author response table 1.</label><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">aSTS</th><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/><th valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom">MC</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">HC</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">M1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">M2</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">M1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">M2</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">P1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P2</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P2</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P2</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">P2</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">15%</td><td align="left" valign="bottom">n=34</td><td align="left" valign="bottom">9%</td><td align="left" valign="bottom">n=33</td><td align="left" valign="bottom">17%</td><td align="left" valign="bottom">n=35</td><td align="left" valign="bottom">23%</td><td align="left" valign="bottom">n=35</td><td align="left" valign="bottom">16%</td><td align="left" valign="bottom">n=32</td><td align="left" valign="bottom">3%</td><td align="left" valign="bottom">n=33</td><td align="left" valign="bottom">11%</td><td align="left" valign="bottom">n=36</td><td align="left" valign="bottom">17%</td><td align="left" valign="bottom">n=35</td></tr></tbody></table></table-wrap><p>Interestingly, a higher percentage of the 10 best neurons for each pose (indicated by the star in the Figure above) showed a significant one-way ANOVA for angle (for P1, MC: 50% (95% confidence interval (CI): 19% – 81%); P1, HC: 70% (CI: 35% - 93%); P2, MC: 70% (CI: 35% – 93%); P2: HC: 50% (CI: 19%-81%)). These percentages were significantly higher than expected for a random sample from the population of neurons for each pose-centering combination (expected percentages listed in the same order as above: 16%, 13%, 16%, and 10%; all outside CI). Thus, for at least half of the “best” neurons, the response differed significantly among the head-orientation angles at the single neuron level. Nonetheless, the tuning profiles were quite diverse, suggesting population coding of head-body orientation angle. We have added this interesting and novel result to the Results (page 16) and Suppl. Material (Figure S3).</p><disp-quote content-type="editor-comment"><p>Minor comments:</p><p>(1) Figure 4A, Fourth Row Example (Zero Angle vs. Straight Angle, Bottom of the P2 Examples): The order of the example stimuli might be incorrect- the 0{degree sign} head with 180{degree sign} body stimulus (leftmost) might be swapped with the 180{degree sign} head with 0{degree sign} body stimulus (5th from the left). While this ordering may be acceptable, please double-check whether it reflects the authors' intended arrangement.</p></disp-quote><p>We have changed the order of the two stimuli in Figure 4A, following the suggestion of the reviewer.</p><disp-quote content-type="editor-comment"><p>(2) Page 12, Lines 192-194: The text states, &quot;Interestingly, some neurons (e.g. Figure 3D) were tuned to a particular combination of a head and body irrespective of centering.&quot; However, Figure 3D displays data for a total of 10 neurons. Could you please specify which of these neurons are being referred to in this context?</p></disp-quote><p>The wording was not optimal. We meant to say that some neurons were tuned to a particular combination of head and body orientation, like the third aSTS example neuron of Figure 3D. We have rephrased the sentence and clarified which example neuron we referred to.</p><disp-quote content-type="editor-comment"><p>(3) Page 28, Lines 470-471: The text states, &quot;We observed no difference in response strength between anatomically possible and impossible configurations.&quot; Please clarify which data were compared for response strength, as I could not locate the corresponding analyses.</p></disp-quote><p>The anatomically possible and impossible configurations differ in the head-body orientation angle. However, as we reported before in the Results, there was no effect of head-body orientation angle on mean response strength across poses (Friedman ANOVA; all p-values for both poses and centerings &gt; 0.1). We have clarified this now in the Discussion (page 28).</p><disp-quote content-type="editor-comment"><p>(4) Pages 40-43, Decoding Analyses: In experiments 2 and 3, were the decoding analyses performed on simultaneously recorded neurons? If so, such analyses might leverage trial-by-trial correlations and thus avoid confounds from trial-to-trial variability. In contrast, experiment 1, which used single-shank electrodes, would lack this temporal information. Please clarify how trial numbers were assigned to neurons in each experiment and how this assignment may have influenced the decoding performance.</p></disp-quote><p>For the decoding analyses of experiments 2 and 3, we combined data from different daily penetrations, with only units from the same penetration being recorded simultaneously. In the decoding analyses of each experiment, the trials were assigned randomly to the pseudo-population vectors, shuffling on each resampling the trial order per neuron. This shuffling abolishes noise correlations in the analysis of each experiment.</p><disp-quote content-type="editor-comment"><p>(5) Page 41, Lines 792-802: The authors state that &quot;To assess the significance of the differences in classification scores between pairs of angles ... we computed the difference in classification score between the two pairs for each resampling and the percentile of 0 difference corresponded to the p-value.&quot; In a two-sided test under the null hypothesis of no difference between the distributions, the conventional approach would be to compute the p-value as the proportion of resampled differences that are as extreme or more extreme than the observed difference. Since a zero difference might be relatively rare, relying solely on its percentile could potentially misrepresent the tail probabilities relevant to a two-sided test. Could you clarify how their method addresses this issue?</p></disp-quote><p>This test is based on the computation of the distribution of the difference between classification accuracies across resamplings. This is similar to the computation of the confidence interval of a difference. Thus, we assess whether the theoretical zero value (=no difference; = null hypothesis) is outside the 2.5 and 97.5 percentile interval of the computed distribution of the empirically observed differences. We clarified now in the Methods (page 41) that for a two-tailed test the computed p-value (the percentile of the zero value) should be smaller than 0.025.</p><disp-quote content-type="editor-comment"><p>(6) Page 43, Lines 829-834: The manuscript explains: &quot;The mean of 10 classification accuracies (i.e., of 10 resamplings) was employed to obtain a distribution (n=100) of the differences in classification accuracy ... The reported standard deviations of the classification accuracies are computed using also the means of 10 resamplings.&quot; I am unfamiliar with this type of analysis and am unclear about the rationale for calculating distributions and standard deviations based on the means of 10 resamplings rather than using the original distribution of classification accuracies. This resampling procedure appears to yield a narrower distribution and smaller standard deviations than the original data. Could you please justify this approach?</p></disp-quote><p>The logic of the analysis is to reduce the noise in the data, by averaging across 10 randomly selected resamplings, but still keeping a sufficient number of data (100 values) for a test.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>(1) Some sentences are too long and difficult to parse. For example, in line 177: &quot;the correlations between the responses to the 64 head-body orientation conditions of the two centerings for the neuron and pose combinations showing significant head-body interactions for the two centerings were similar to those observed for the whole population.&quot;</p></disp-quote><p>We have modified this sentence: For neuron and pose combinations with significant head-body interactions in both centerings, the correlations between responses to the 64 head-body orientation conditions were similar to those observed in the whole population.</p><disp-quote content-type="editor-comment"><p>(2) The authors argue in line 485: &quot;in our study, a search bias cannot explain the body-inversion effect since we selected responsive units using both upright and inverted images.&quot; However, the body-selective patches were localized using upright images, correct?</p></disp-quote><p>The monkey-selective patches were localized using upright images indeed. However, we recorded in experiment 3 (and 2) also outside the localized patches (as we noted before in the Methods: “In experiments 2 and 3 we recorded from a wider region, which overlapped with the two monkey patches and the recording locations of experiment 1”). Furthermore, the preference for upright monkey images is not an all-or-nothing phenomenon: most units still responded to inverted monkeys. Also, we believe it is likely that the mean responses to the inverted bodies in the monkey patches, defined by upright bodies versus objects, would be larger than those to objects and we would be surprised to learn that there is a patch selective for inverted bodies that we would have missed with our localizer.</p><disp-quote content-type="editor-comment"><p>(3) Typo: line 447, &quot;this independent&quot;-&gt;&quot;is independent&quot;?</p></disp-quote><p>Corrected.</p></body></sub-article></article>