<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72484</article-id><article-id pub-id-type="doi">10.7554/eLife.72484</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The pupillary light response as a physiological index of aphantasia, sensory and phenomenological imagery strength</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-252027"><name><surname>Kay</surname><given-names>Lachlan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-149660"><name><surname>Keogh</surname><given-names>Rebecca</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4814-433X</contrib-id><email>rebeccalkeogh@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-252028"><name><surname>Andrillon</surname><given-names>Thomas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2794-8494</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-55858"><name><surname>Pearson</surname><given-names>Joel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3704-5037</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03r8z3t63</institution-id><institution>School of Psychology, University of New South Wales</institution></institution-wrap><addr-line><named-content content-type="city">Sydney</named-content></addr-line><country>Australia</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sf06y89</institution-id><institution>School of Psychological Sciences, Macquarie University</institution></institution-wrap><addr-line><named-content content-type="city">Sydney</named-content></addr-line><country>Australia</country></aff><aff id="aff3"><label>3</label><institution>Sorbonne Université, Institut du Cerveau - Paris Brain Institute - ICM, Inserm, CNRS</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>31</day><month>03</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e72484</elocation-id><history><date date-type="received" iso-8601-date="2021-07-26"><day>26</day><month>07</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-03-30"><day>30</day><month>03</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-09-03"><day>03</day><month>09</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.02.457617"/></event></pub-history><permissions><copyright-statement>© 2022, Kay et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Kay et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72484-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-72484-figures-v2.pdf"/><abstract><p>The pupillary light response is an important automatic physiological response which optimizes light reaching the retina. Recent work has shown that the pupil also adjusts in response to illusory brightness and a range of cognitive functions, however, it remains unclear what exactly drives these endogenous changes. Here, we show that the imagery pupillary light response correlates with objective measures of sensory imagery strength. Further, the trial-by-trial phenomenological vividness of visual imagery is tracked by the imagery pupillary light response. We also demonstrated that a group of individuals without visual imagery (aphantasia) do not show any significant evidence of an imagery pupillary light response, however they do show perceptual pupil light responses and pupil dilation with larger cognitive load. Our results provide evidence that the pupillary light response indexes the sensory strength of visual imagery. This work also provides the first physiological validation of aphantasia.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual imagery</kwd><kwd>pupillometry</kwd><kwd>aphantasia</kwd><kwd>individual differences</kwd><kwd>imagery</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>APP1024800</award-id><principal-award-recipient><name><surname>Pearson</surname><given-names>Joel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>APP1046198</award-id><principal-award-recipient><name><surname>Pearson</surname><given-names>Joel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>APP1085404</award-id><principal-award-recipient><name><surname>Pearson</surname><given-names>Joel</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>APP1049596</award-id><principal-award-recipient><name><surname>Pearson</surname><given-names>Joel</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>DP140101560</award-id><principal-award-recipient><name><surname>Pearson</surname><given-names>Joel</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004412</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>LT000362/2018-L</award-id><principal-award-recipient><name><surname>Andrillon</surname><given-names>Thomas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Physiological evidence shows that the pupillary response to imagined light can be used to index the strength and vividness of an individual’s visual imagery and as a new tool for confirming aphantasia.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Our pupil’s ability to change size is an important physiological response that adjusts the amount of light hitting the retina to optimize vision and protect the retina. Pupils constrict in response to brightness whereas they dilate in response to dark conditions (known as the pupillary light response or reflex); while these responses are related, they are considered to be driven by different neural pathways (see <xref ref-type="bibr" rid="bib47">Mathôt, 2018</xref> for a review). These involuntary pupil responses were once thought to be driven only by afferent visual stimulation, or automatic activation from emotional responses (<xref ref-type="bibr" rid="bib8">Bradley et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Partala and Surakka, 2003</xref>), however, recent studies suggest that pupil size is sensitive to higher order perceptual and cognitive processes. For example, subjective interpretation of equiluminant stimuli, such as greyscale images of the sun elicit greater pupil constriction than those of the moon (<xref ref-type="bibr" rid="bib5">Binda et al., 2013b</xref>). The target of covert visual attention can drive pupillary light responses (<xref ref-type="bibr" rid="bib4">Binda et al., 2013a</xref>), as can visual working memory content (<xref ref-type="bibr" rid="bib77">Zokaei et al., 2019</xref>), but see <xref ref-type="bibr" rid="bib6">Blom et al., 2016</xref>. Further, evidence suggests that it might be mental imagery that is driving some of these cognitively induced pupil responses (<xref ref-type="bibr" rid="bib40">Laeng and Sulutvedt, 2014</xref>) and recent work has shown that there are pupillary light responses even when reading or listening to words conveying some level of brightness (<xref ref-type="bibr" rid="bib46">Mathôt et al., 2017</xref>). Hence, it remains unknown if the variations in pupil response to equiluminant stimuli are due to high-level semantic content or low-level visual imagery.</p><p>Visual imagery is considered a useful and often essential tool in many aspects of cognition. It plays an important role in the retrieval of items from short- and long-term memory (<xref ref-type="bibr" rid="bib55">Pearson, 2019</xref>), visual working memory (<xref ref-type="bibr" rid="bib31">Keogh and Pearson, 2011</xref>; <xref ref-type="bibr" rid="bib32">Keogh and Pearson, 2014</xref>; <xref ref-type="bibr" rid="bib56">Pearson and Keogh, 2019</xref>), acquisition of language (<xref ref-type="bibr" rid="bib28">Just et al., 2004</xref>), and spatial navigation (<xref ref-type="bibr" rid="bib60">Sack et al., 2005</xref>; <xref ref-type="bibr" rid="bib22">Guariglia and Pizzamiglio, 2007</xref>). It is also used for simulating both past and potential future events (<xref ref-type="bibr" rid="bib61">Schacter et al., 2012</xref>; <xref ref-type="bibr" rid="bib62">Schacter and Madore, 2016</xref>), the latter often as a form of self-motivation for goal attainment (<xref ref-type="bibr" rid="bib65">Szpunar et al., 2007</xref>). As essential to cognition as it might appear, large individual differences exist in visual imagery and its vividness. Some people report imagery as so vivid it feels almost like perception, while a small percentage of otherwise healthy people seemingly do not have the capacity for visual imagery at all – they report that when they think about how an object looks, there is no sensory-like experience of it whatsoever (<xref ref-type="bibr" rid="bib20">Galton, 1880</xref>). This condition has been recently termed ‘aphantasia’ (<xref ref-type="bibr" rid="bib76">Zeman et al., 2015</xref>); it can be congenital, persisting throughout one’s lifetime (<xref ref-type="bibr" rid="bib76">Zeman et al., 2015</xref>) or acquired (<xref ref-type="bibr" rid="bib75">Zeman et al., 2010</xref>), is associated with a range of differences in general cognition (<xref ref-type="bibr" rid="bib11">Dawes et al., 2020</xref>; <xref ref-type="bibr" rid="bib36">Keogh et al., 2021a</xref>, <xref ref-type="bibr" rid="bib35">Keogh and Pearson, 2021</xref>), including dampened fear responses to imagined scary scenarios (<xref ref-type="bibr" rid="bib73">Wicken et al., 2021</xref>). The existence of aphantasia has also been established using objective techniques that measure the low-level sensory elements of imagery (<xref ref-type="bibr" rid="bib34">Keogh and Pearson, 2018</xref>).</p><p>The rationale of the current study was to accurately and objectively utilize individual differences in mental imagery (both in the general population and aphantasia) to provide strong evidence that it is the sensory strength and subjective vividness of imagery that drives the cognitive pupillary light response. Similar rationale has been previously used by linking the vividness and objective sensory strength of imagery to behavioural or neurological measures (<xref ref-type="bibr" rid="bib3">Bergmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">Shine et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Wassell et al., 2015</xref>). If imagery plays a causal role in endogenous pupil size changes, then individual differences in imagery should be reflected in these measures.</p><p>Here, we utilized both subjective and objective measures of visual imagery ability and show that, within the same individual, greater pupillary light responses during imagery are associated with reports of stronger and more vivid imagery. We then used this task to compare imagery strength between individuals and test the veracity of the self-reported lack of imagery in aphantasia. We show that while aphantasic individuals display pupil contraction to perceptual brightness and dilation with effort (cognitive load), they do not show any evidence of pupil change in response to attempts at imagery – providing the first objective physiological evidence confirming the existence of aphantasia.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The imagery pupillary light response in the general population</title><p>In the pupillometry imagery task (based on <xref ref-type="bibr" rid="bib40">Laeng and Sulutvedt, 2014</xref>; see <xref ref-type="fig" rid="fig1">Figure 1A</xref>), participants who reported having visual imagery were presented with one or four ‘Bright’ or ‘Dark’ triangles for 5 s (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for images used). Following this they viewed a blank screen for 8 s (which allowed any after-images to fade) and were then instructed to imagine the prior image/s for 6 s, after which they rated the vividness of their imagery from 1 to 4. Pupils showed a clear pupillary light response to perceptual images (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; perception section; a significant effect of perceptual luminance <italic>F</italic>(1, 41) = 190.02, p &lt; 0.001.) This trend was mirrored in the imagery period showing a significant main effect of imagery luminance (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, box insets: imagery section; <italic>F</italic>(1, 41) = 67.42, p &lt; 0.0001), indicating that imagery also demonstrates a pupillary light response. Post hoc analysis using the Bonferroni correction for multiple comparisons found that for both Set-Size-One and Set-Size-Four, the pupil size in the Dark condition was significantly greater than in the Light condition during imagery (p &lt; 0.001 and p &lt; 0.05, respectively, see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). There was no main effect of set size during perception <italic>F</italic>(1, 42) = 2.67, p = 0.11. However, there was a significant main effect of set size during imagery <italic>F</italic>(1, 41) = 6.48, p = 0.015, with less constriction/more dilation for Set-Size-Four (when averaged across the brightness conditions). This is consistent with previous studies suggesting that pupil size is influenced by cognitive load (<xref ref-type="bibr" rid="bib29">Kahneman and Beatty, 1966</xref>; <xref ref-type="bibr" rid="bib39">Laeng et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">van der Wel and van Steenbergen, 2018</xref>). Post hoc analysis also demonstrated that in the Bright condition, Set-Size-Four resulted in significantly more pupil dilation during imagery than Set-Size-One (p = 0.001). However, in the Dark condition pupil dilation during Set-Size-Four imagery was not significantly different to Set-Size-One (p = 0.266).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Pupillary response task schematic and eye-tracker results for the general population.</title><p>(<bold>A</bold>) Pupillometry imagery experiment timeline. Each trial began with the presentation of a white fixation cross at the centre of a grey screen (baseline screen) for 1 s. An image was then presented at the centre of this grey screen for 5 s (either one or four triangles of varying brightness, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for illustrations of all stimuli). Participants were instructed to focus on the stimuli during this time and memorize its size, orientation, and level of brightness. Next, a black screen with a white fixation cross was presented for 8 s, allowing the perceived after-image to completely fade and pupils to dilate back to equivalent resting levels. The grey baseline screen was then presented again for 6 s. During this time, participants were cued (via two auditory beeps) to actively start imagining the stimuli observed previously during that trial, while maintaining focus on the fixation cross. These beeps were presented 1 s into the grey screen period leaving 5 s of imagery time. Lastly, participants were prompted to report the vividness of their imagery during those previous 5 s on a scale of 1–4 (1 being ‘not vivid at all – no shape appeared in imagery’; 4 being ‘very vivid – almost like seeing it’) via key response. (<bold>B</bold>) Mean pupil size waveforms for the general population, presented as mm change from baseline. Left panel: data averaged across the course of a trial for Bright (red lines) and Dark (blue lines) conditions for the general population. <italic>Right panels:</italic> Set-Size-One and Set-Size-Four conditions are shown separately during the imagery period (i.e. pupil size from seconds 15 to 20). Shaded error bands represent the standard error of the mean (± standard error of the mean [SEM]). (<bold>C</bold>) Mean pupil size change from baseline during imagery (i.e. averaged from seconds 15 to 20 of trials) of Bright (red bars) and Dark stimuli (blue bars). (<bold>D</bold>) Pupil-difference scores (difference in pupil size during imagery between bright and dark conditions) as a function of subjective vividness ratings for Set-Size-One and Set-Size-Four conditions. Data points represent one participant. Error bars indicate ± SEM, calculated across participants. *p &lt; 0.05, ***p &lt; 0.0001.</p><p><supplementary-material id="fig1scode1"><label>Figure 1—source code 1.</label><caption><title>r Code for LME analysis of vividness ratings.</title><p>Source code file 1 provides the r code for the LME used to analyse the vividness data in <xref ref-type="fig" rid="fig1">Figure 1D</xref>.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-72484-fig1-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Source data for <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-72484-fig1-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig1sdata2"><label>Figure 1—source data 2.</label><caption><title>Source data for <xref ref-type="fig" rid="fig1">Figure 1D</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-72484-fig1-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Stimuli used in the experiment.</title><p>(<bold>A</bold>) Illustrations of all Set-Size-One stimuli used in the pupillary response task. (<bold>B</bold>) Illustrations of all Set-Size-Four stimuli used in the pupillary response task.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Correlations between pupil-difference scores and mean trial vividness.</title><p>Correlation between visual imagery strength, as measured by the pupillary reflex task (pupil-difference score: difference between bright and dark conditions) and visual imagery strength as measured by mean trial-by-trial vividness ratings. (<bold>A</bold>) For Set-Size-One there was no significant correlation between these two variables, <italic>r</italic><sub>p</sub>(41) = −0.16, p = 0.31. (<bold>B</bold>) Due to the Set-Size-Four data set violating normality, Spearman’s correlational coefficient was used to analyse the relationship between Set-Size-Four pupil-difference scores and vividness ratings, with no significant correlation observed, <italic>r</italic><sub>s</sub>(41) = 0.06, p = 0.69. Scatterplots show the general population (green circles and black trendline) data only. All data points represent one participant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Correlations between pupil-difference scores and Vividness of Visual Imagery Questionnaire (VVIQ).</title><p>Correlation between visual imagery strength, as measured by the pupillary reflex task (pupil-difference score: difference between bright and dark conditions) and visual imagery strength as measured by the VVIQ. (<bold>A</bold>) Set-Size-One shows there was no significant correlation between difference scores and the VVIQ <italic>r</italic><sub>p</sub>(41) = 0.02, p = 0.88. (<bold>B</bold>) For Set-Size-Four Spearman’s correlational coefficient indicated no significant correlation between Set-Size-Four difference scores and mean vividness ratings, <italic>r</italic><sub>s</sub>(41) = 0.20, p = 0.19. Scatterplots show the general population (green circles and black trendline) data only. All data points represent one participant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig1-figsupp3-v2.tif"/></fig></fig-group><p>Prior behavioural work suggests we have reasonable metacognition of visual imagery, that is we are able to estimate the strength of imagery on a trial-by-trial basis (<xref ref-type="bibr" rid="bib53">Pearson et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Rademaker and Pearson, 2012</xref>). Here, we compared pupil responses to the trial-by-trial ratings of vividness. Pupil-difference scores are shown as a function of intraindividual vividness ratings for Set-Size-One and Set-Size-Four (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). A 2 × 4 linear mixed-effects analysis (2 (set size: 1, 4) × 4 (vividness rating: 1, 2, 3, 4)) demonstrated there was a significant effect of vividness (<italic>χ</italic><sup>2</sup>(3) = 49.54, p = 1.004e<sup>−10</sup>), with larger pupillary light response for more vivid imagery trials (for both set sizes, see <xref ref-type="fig" rid="fig1">Figure 1D</xref> and fixed effects estimates in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). These data demonstrate that the pupillary light response tracks the phenomenological vividness of visual imagery from moment to moment.</p><p>If the sensory strength of imagery is indeed driving the imagery pupillary light response, then the degree to which this response occurs should be related to independent objective measures of imagery strength in each individual. To assess this, we utilized the binocular rivalry method (<xref ref-type="bibr" rid="bib54">Pearson, 2014</xref>; <xref ref-type="bibr" rid="bib52">Pearson et al., 2008</xref>), which allows the objective assessment of the sensory strength of imagery, without relying on any subjective reports (<xref ref-type="bibr" rid="bib10">Chang and Pearson, 2018</xref>). This is achieved by measuring the degree to which an individual’s imagery biases subsequent binocular rivalry perception. We compared pupil-difference scores (imagery of dark stimuli–bright stimuli, such that larger scores indicate a larger pupillary light response) with imagery strength measured using the binocular rivalry paradigm, in which higher priming scores indicate stronger imagery (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; <xref ref-type="bibr" rid="bib52">Pearson et al., 2008</xref>; <xref ref-type="bibr" rid="bib53">Pearson et al., 2011</xref>). Within the general population, degree of pupil change in the Set-Size-One condition correlated positively with imagery strength, using Pearson’s correlation coefficient (<italic>r</italic><sub>p</sub>(41) = 0.62, p = &lt;0.0001, see <xref ref-type="fig" rid="fig2">Figure 2B</xref>: green circles and green trendline). The Set-Size-Four pupil data set violated normality (Shapiro–Wilk test, p = 0.003), therefore, the Spearman’s correlational coefficient was used to assess its relationship with binocular rivalry priming. A significant positive correlation was found between Set-Size-Four pupil-difference scores and binocular rivalry priming (<italic>r</italic><sub>s</sub>(41) = 0.46, p = 0.002, see <xref ref-type="fig" rid="fig2">Figure 2C</xref>: green circles and green trendline). This provides further evidence that the sensory strength of imagery content is driving the imagery pupillary light response.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Binocular rivalry task schematic and correlational results.</title><p>(<bold>A</bold>) Example of an imagery trial for the binocular rivalry paradigm. Participants were cued to imagine either a red or green Gabor pattern prior to binocular rivalry with the letter ‘R’ or ‘G’ (750 ms). Participants then imagined the image for 6 s, after which they were presented with the binocular rivalry display (750 ms) and were asked to indicate which image was dominant. Trials where participants reported seeing the pattern they were cued to imagine as dominant were denoted as ‘primed’ trials. The number of primed trials divided by the total number of trials (excluding mock trials and mixed percepts) was used to calculate a percent primed score for each participant. (<bold>B</bold>) Correlation between visual imagery strength, as measured by the pupillary response task (pupil-difference score: difference between bright and dark conditions) and visual imagery strength as measured by the binocular rivalry task. Set-Size-One (left) and Set-Size-Four (right) conditions are shown. Scatterplots show the general population (green circles and green trendline) and aphantasic individuals (yellow triangles and yellow trendline) data. Correlation coefficients refer to the general population only (green trendline). All data points represent one participant.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source date for <xref ref-type="fig" rid="fig2">Figure 2B</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-72484-fig2-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Correlations between pupil-difference scores and VVIQ.</title><p>The correlations between baselined pupil sizes during perceptually viewing images and imagining them for SS1 bright images (<bold>A, </bold><italic>r</italic><sub>s</sub> = −0.13, p = 0.40), SS4 bright images (<bold>B</bold>, <italic>r</italic><sub>s</sub> = 0.11, p = 0.49), SS1 dark images (<bold>C</bold>, <italic>r</italic><sub>s</sub> = 0.13, p = 0.40), and SS4 dark images (<bold>D</bold>, <italic>r</italic><sub>s</sub> = 0.01, p = 0.97), plus difference scores for SS1 (<bold>E</bold>, <italic>r</italic><sub>s</sub> = −0.25, p = 0.11) and SS4 (<bold>F</bold>, <italic>r</italic><sub>s</sub> = −0.15, p = 0.36), all correlation values are Spearman’s rho.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Correlations between pupil-difference scores and binocular rivalry priming during perception.</title><p>The relationship between participants pupil size difference scores during perception and priming in the binocular rivalry task for Set-Size-One (left panel: <italic>r</italic><sub>s</sub> = −0.11, p = 0.475) and Set-Size-Four (right panel: <italic>r</italic><sub>s</sub> = 0.08, p = 0.607).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Correlations between mean eccentricity and pupil-difference scores during imagery for control and aphantasic groups.</title><p>The relationship between mean gaze eccentricity during imagery and pupil-difference scores during imagery from the general population (green circles) and aphantasic individuals (orange triangles) for Set-Size-One (top panel) and Set-Size-Four (bottom panel). Assumptions of normality and sphericity were met for all data sets except for the general population Set-Size-Four data set (Shapiro–Wilk test, p = 0.003). On examining participants’ Set-Size-One pupil-difference scores and mean eccentricity during Set-Size-One trials, using the Pearson’s correlational coefficient, no significant correlation was found for the general population, <italic>r</italic><sub>p</sub>(41) = −0.05, p = 0.747 or aphantasic individuals, <italic>r</italic><sub>p</sub>(17) = 0.16, p = 0.537 (see top panel). Spearman’s correlational coefficient indicated no significant correlation between Set-Size-Four pupil-difference scores and mean eccentricity for the general population, <italic>r</italic><sub>s</sub>(41) = −0.09, p = 0.587. Pearson’s correlational coefficient also indicated no significant correlation between Set-Size-Four difference scores and mean eccentricity for aphantasic individuals, <italic>r</italic><sub>p</sub>(17) = −0.15, p = 0.553 (see bottom panel).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Correlations between mean eccentricity during imagery and binocular rivalry priming for control and aphantasic groups.</title><p>The relationship between mean gaze eccentricity during imagery and priming in the binocular rivalry task from the general population (green circles) and aphantasic individuals (orange triangles). Set-Size-One (top panel) and Set-Size-Four (bottom panel) conditions are shown. Assumptions of normality and sphericity were met for all data sets. On examining participants’ binocular rivalry priming scores and mean gaze eccentricity during imagery of Set-Size-One stimuli, using the Pearson’s correlational coefficient, no significant correlation was found for the general population, <italic>r</italic><sub>p</sub>(41) = −0.07, p = 0.664, or aphantasic individuals, <italic>r</italic><sub>p</sub>(17) = 0.23, p = 0.363 (see top panel). Pearson’s correlational coefficient also indicated no significant correlation between priming scores and mean gaze eccentricity during imagery of Set-Size-Four stimuli for the general population, <italic>r</italic><sub>p</sub>(41) = −0.13, p = 0.428 or aphantasic individuals, <italic>r</italic><sub>p</sub>(17) = −0.01, p = 0.979 (see bottom panel).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Correlations between mean eccentricity during imagery and mean trial-by-trial vividness scores for the control group.</title><p>The relationship between participants’ mean gaze eccentricity during imagery and their mean trial-by-trial vividness ratings for Set-Size-One and Set-Size-Four conditions, from the general population. Assumptions of normality and sphericity were met for all data sets. Using Pearson’s correlational coefficient, no significant correlation was found between eccentricity and vividness ratings during Set-Size-One trials, <italic>r</italic><sub>p</sub>(41) = −0.04, p = 0.816 (see top panel). Similarly, no significant correlation was observed between eccentricity and vividness ratings during Set-Size-Four trials, <italic>r</italic><sub>s</sub>(41) &lt; −0.01, p = 0.998 (see bottom panel).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Binocular rivalry and mock rivalry priming scores for control and aphantasic individuals.</title><p>Binocular rivalry priming scores on the imagery task for control (grey violin plot) and aphantasic (teal violin plot) participants for real binocular rivalry trials (left) and mock (right). A mixed repeated measures analysis of variance (ANOVA) was run to compare the two groups priming for the real and mock binocular rivalry conditions. There was a significant interaction between condition (real vs. mock) and group (aphantasia vs. control): <italic>F</italic>(1, 58) = 12.46, p &lt; 0.001. Post hoc analysis using the Bonferroni correction for multiple comparisons found that control participants priming was significantly higher than the aphantasic individuals in the real condition (<italic>t</italic> = 5.70, p &lt; 0.001) but there was no significant difference between the groups for the mock condition (<italic>t</italic> = 1.07, p = 0.99). In addition, the control participants priming was higher in the real vs. the mock condition (<italic>t</italic> = 6.85, p &lt; 0.001) whereas the real and mock conditions were not significantly different for the aphantasic population (<italic>t</italic> = 0.78, p = 99). Each circle represents an individual. * represent significant differences.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 7.</label><caption><title>One-sample <italic>t</italic>-test prior and posterior plots.</title><p>The prior and posterior plots for the Bayesian <italic>t</italic>-tests run to assess the evidence for aphantasic individuals performing significantly worse than chance in both the binocular rivalry task (top row) and imaginary pupillary light response (middle and bottom row). When assessing both groups there was no evidence that their priming scores were significantly lower than chance (comparing scores to 50%: aphantasic individuals BF = 0.162, controls BF = 0.012). Similarly, a one-sample <italic>t</italic>-test found no evidence that either groups pupil-difference scores were lower than chance (comparing to 0: aphantasic individuals SS1 BF = 0.860, aphantasic individuals SS4 BF = 0.187, controls SS1 BF = 0.091, controls SS4 BF = 0.050). * represent significant differences.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp7-v2.tif"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 8.</label><caption><title>VVIQ scores for control and aphantasic individuals.</title><p>Vividness of visual imagery (VVIQ) scores for control (grey violin plot) and aphantasic (teal violin plot) participants. A Mann–Whitney test was run due to compare the VVIQ scores of the two groups due to unequal group sizes, variance, and the aphantasic group being non-normal. The two groups differed significantly (<italic>W</italic> = 0, p &lt; 0.001) with aphantasic individuals having much lower scores on the VVIQ than the control group.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig2-figsupp8-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Aphantasia and the imagery pupillary light response</title><p>Our results indicate that the strength of the content of imagery drives the imagery pupillary light response in participants who experience visual imagery. The involuntary nature of this response provides a valuable objective measure of imagery strength. Accordingly, we sought to utilize this finding to test the veracity of a condition called aphantasia, that is if these individuals truly lack visual imagery, they should not show a pupillary light response to imagined images. However, if aphantasic individuals do show an imagery-based pupillary light response, one might interpret this as a form of imagery existing, but below threshold for conscious phenomenological awareness. We ran this same study in 18 aphantasic participants and compared their performance to that of the general population. These participants had contacted the lab reporting their lack of visual imagery and asked to participate in our research. They were also unaware of the goals and hypotheses of the current study. Aphantasia was confirmed in these individuals using self-report questionnaires (Vividness of Visual Imagery Questionnaire [VVIQ] score &lt;32) and by means of our binocular rivalry priming method (priming &lt;65%), based on cut-off points used in previous research (<xref ref-type="bibr" rid="bib34">Keogh and Pearson, 2018</xref>).</p><p>Here, we again found a strong effect of stimulus luminance in the perceptual phase of the task for the aphantasic participants (<xref ref-type="fig" rid="fig3">Figure 3A</xref>: perception section; <italic>F</italic>(1, 17) = 81.18, p &lt; 0.001), reflecting a functional pupillary light response. However, we found no significant effect of luminance on pupil size during imagery <xref ref-type="fig" rid="fig3">Figure 3A</xref>, box insets: imagery section; <italic>F</italic>(1, 17) = 0.193, p = 0.67 and <xref ref-type="fig" rid="fig3">Figure 3B</xref> shows the lack of pupil diameter change for bright stimuli (red bars) and dark stimuli (blue bars). Similarly, to the general population, there was no main effect of set size during perception <italic>F</italic>(1, 17) = 1.92, p = 0.18, however interestingly, there was a significant main effect of set size during imagery <italic>F</italic>(1, 17) = 6.185, p = 0.02, with greater pupil diameters for Set-Size-Four compared to Set-Size-One (when averaged across the brightness conditions). This suggests that the aphantasic participants were actively engaging in the imagery task and exerting greater cognitive effort for the larger set size (<xref ref-type="bibr" rid="bib67">van der Wel and van Steenbergen, 2018</xref>). In comparison to the general population, 61.11% (11/18) of the aphantasic individuals had difference scores that were lower than or equal to 0 for set size one as compared to 9.5% (4/42) of the general population (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). To confirm this absence of an imagery effect in the aphantasia population, we compared the pupil-difference score obtained when comparing the bright and dark conditions for the control and aphantasia groups, and computed a Bayes Factor (H0: score = 0; H1: score ≠ 0; see Materials and methods). Controls showed very strong evidence for H1 (BF<sub>10</sub> &gt; 10<sup>10</sup>; Bayesian one-sample <italic>t</italic>-test), whereas the aphantasia population showed evidence for the null effect (BF<sub>01</sub> = 3.180). A direct comparison between the control and aphantasia groups using a Bayesian repeated measure analysis of variance (ANOVA; see Materials and methods) showed very strong evidence for an effect of group (BF<sub>10</sub> &gt; 10<sup>6</sup>). Finally, and as expected, pupil-difference scores (imagery of dark stimuli–bright stimuli) did not significantly predict imagery strength (measured using the binocular rivalry paradigm) for the aphantasic population (<xref ref-type="fig" rid="fig2">Figure 2B</xref>: yellow triangles; Set-Size-One: <italic>r</italic><sub>p</sub>(17) = 0.20, p = 0.44); Set-Size-Four: (<italic>r</italic><sub>p</sub>(17) = −0.08, p = 0.76). It should be noted that we could not perform an analysis on the vividness data in the same way as was done with the general population (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) as the aphantasic individuals did not have any variation in their vividness ratings, reflecting their lack of subjective visual imagery (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Pupillary response eye-tracker results for the aphantasic population.</title><p>(<bold>A</bold>) Mean pupil size waveforms over time. <italic>Left panel:</italic> data averaged across the course of a trial for Bright (red lines) and Dark (blue lines) conditions for the aphantasic population. <italic>Right panels:</italic> Set-Size-One and Set-Size-Four conditions are shown separately during the imagery period. (<bold>B</bold>) Mean pupil size change from baseline during imagery (i.e. averaged from seconds 15 to 20 of trials) of Bright (red bars) and Dark stimuli (blue bars). Error bars indicate ± standard error of the mean (SEM), calculated across participants. *p &lt; 0.05.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Source data for <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-72484-fig3-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Mean vividness ratings for set size and group.</title><p>The mean vividness ratings for general population (left panel) and aphantasic individuals (right panels) for Set-Size-One (SS1) and Set-Size-Four (SS4) for bright (red) and dark (blue) images. Aphantasic individuals score at floor on vividness ratings, for this reason a repeated measures analysis of variance (ANOVA) was run only on the general population to assess what effect set size and image brightness had on vividness ratings. A 2 (Set-Size: One and Four) × 2 (Stimuli: Bright and Dark) repeated measures ANOVA was run. There was a significant main effect of stimuli <italic>F</italic>(1,41) = 37.85, p &lt; 0.001, with the general population rating imagery of dark images as being more vivid than bright images. The main effect of set size was also significant <italic>F</italic>(1,41) = 8.042, p = 0.007, with participants rating imagery for Set-Size-One as being more vivid than Set-Size-Four. There was no significant interaction between the two factors <italic>F</italic>(1, 41) = 1.28, p = 0.264. Bars show mean values and error bars represent ± standard error of the mean (SEMs). Each circle represents an individual.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Pupil diameter changes during perception for control and aphantasic participant’s.</title><p>Data show the pupil diameter change from baseline during perception for control (left panel) and aphantasic individual’s (right panel) for bright (red) and dark (blue) images for both Set-Size-One (SS1) and Set-Size-Four (SS4). Bars show mean values and error bars represent ± standard error of the mean (SEMs). Each circle represents an individual. A 2 (image: bright × dark) × 2 (set size (SS): 1 × 4) × 2 (group: aphantasic × controls) repeated measures analysis of variance (ANOVA) was run. There was a main effect of image <italic>F</italic>(1, 58) = 211.24, p &lt; 0.001 and set size <italic>F</italic>(1, 58) = 4.16, p = 0.046. There was no significant effect of group <italic>F</italic>(1, 58) = 1.15, p = 0.288 and there were no significant interactions (image × group: <italic>F</italic>(1, 58) = 1.57, p = 0.216, SS × group: <italic>F</italic>(1,58) = 0.06, p = 0.816, image × SS × group: <italic>F</italic>(1, 58) = 0.03, p = 0.875).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Eccentricity values for control and aphantasic individuals.</title><p>Gaze eccentricity waveforms during bright and dark stimuli conditions (for both Set-Size-One and -Four) for control (left panel) and aphantasic (right panel) participants. Data are averaged across the course of a trial for Bright (red lines) and Dark (blue lines) conditions. Solid lines indicate Set-Size-One stimuli and dotted lines indicate Set-Size-Four stimuli. Shaded error bands represent ± standard error of the mean (SEM) calculated across participants. <bold><underline>PERCEPTION PERIOD:</underline></bold> A 2 (image: bright × dark) × 2 (set size (SS): 1 × 4) × 2 (group: aphantasic × controls) repeated measures analysis of variance (ANOVA) was run for mean eccentricity during the perception phase of trials. There was no significant main effect of image <italic>F</italic>(1, 58) = 0.46, p = 0.497, set size <italic>F</italic>(1, 58) &lt;0.001, p = 0.983, or group <italic>F</italic>(1, 58) = 3.66, p = 0.061. There were also no significant interactions (image × SS: <italic>F</italic>(1, 58) = 0.07, p = 0.787, image × group: <italic>F</italic>(1, 58) = 0.33, p = 0.568, SS × group: <italic>F</italic>(1, 58) = 0.48, p = 0.491, image × SS × group: <italic>F</italic>(1, 58) = 3.2, p = 0.079). <bold><underline>IMAGERY PERIOD:</underline></bold> A 2 (image: bright × dark) × 2 (set size (SS): 1 × 4) × 2 (group: aphantasic × controls) repeated measures ANOVA was run for mean eccentricity during the imagery phase of trials. There was a significant main effect of image <italic>F</italic>(1, 58) = 1.07, p = 0.026. Post hoc analysis using the Bonferroni correction for multiple comparisons found that eccentricity was significantly higher during imagery of bright stimuli than imagery of dark stimuli when averaged across levels of set size and group (<italic>t</italic> = 2.29, p = 0.026). There was no significant effect of set size (<italic>F</italic>(1,58) = 2.66, p = 0.108) or group (<italic>F</italic>(1,58) = 0.41, p = 0.525), and there were no significant interactions (image × SS: <italic>F</italic>(1, 58) = 1.17, p = 0.283, image × group: <italic>F</italic>(1, 58) = 3.84, p = 0.055, SS × group: <italic>F</italic>(1, 58) = .08, p = 0.78, image × SS × group: <italic>F</italic>(1, 58) = 0.58, p = 0.448).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>The number of saccades during perception for control and aphantasic participant’s.</title><p>The mean number of saccades for general population (left panel) and aphantasic individual’s (right panel) during the perception phase of trials. Bright (red) and dark (blue) stimuli for both Set-Size-One (SS1) and Set-Size-Four (SS4) are depicted. Bars show mean values and error bars represent ± standard error of the mean (SEMs) calculated across participants. Each circle represents an individual. A 2 (image: bright × dark) × 2 (set size (SS): 1 × 4) × 2 (group: aphantasic × controls) repeated measures analysis of variance (ANOVA) was run. There was a main effect of image <italic>F</italic>(1, 58) = 6.72, p = 0.012 and a significant interaction of image × SS: <italic>F</italic>(1, 58) = 8.89, p = 0.004. Post hoc analysis using the Bonferroni correction for multiple comparisons found a significantly greater number of saccades during the perception of dark stimuli (<italic>M</italic> = 9.17, SD = 4.19) compared to bright stimuli (<italic>M</italic> = 7.8, SD = 3.54) for the Set-Size-Four condition (<italic>t</italic> = −3.91, p = &lt;0.001) but not the Set-Size-One condition (<italic>t</italic> = 0.05, p = 1) when averaged across groups. There was no significant effect of set size (<italic>F</italic>(1, 58) = 0.46, p = .501) or group (<italic>F</italic>(1, 58) = 0.16, p = 0.688). There were no other significant interactions (image × group: <italic>F</italic>(1, 58) = 0.21, p = 0.647, SS × group: <italic>F</italic>(1, 58) = 0.22, p = 0.643, image × SS × group: <italic>F</italic>(1, 58) = 0.17, p = 0.677).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig3-figsupp4-v2.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>The number of saccades during imagery for control and aphantasic participant’s.</title><p>The mean number of saccades for general population (left panel) and aphantasic individual’s (right panel) during the imagery phase of trials. Bright (red) and dark (blue) stimuli for both Set-Size-One (SS1) and Set-Size-Four (SS4) are depicted. Bars show mean values and error bars represent ± standard error of the mean (SEMs) calculated across participants. Each circle represents an individual. A 2 (image: bright × dark) × 2 (set size (SS): 1 × 4) × 2 (group: aphantasic × controls) repeated measures analysis of variance (ANOVA) was run. There were no significant main effects image: <italic>F</italic>(1, 58) = 0.02, p = 0.885, set size: <italic>F</italic>(1, 58) = 0.82, p = 0.369, group: <italic>F</italic>(1, 58) = 0.12, p = 0.735. There were also no significant interactions image × group: <italic>F</italic>(1, 58) = 0.66, p = 0.419, SS × group: <italic>F</italic>(1, 58) = 1.41, p = 0.240, image × SS: <italic>F</italic>(1, 58) = 0.16, p = 0.693, image × SS × group: <italic>F</italic>(1, 58) = 0.20, p = 0.660.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72484-fig3-figsupp5-v2.tif"/></fig></fig-group><p>Age disparities between the groups are a potential confounding variable. This factor is of particular importance because the sensitivity of the pupillary light response, as well as maximum pupillary constriction velocity and acceleration, are thought to decline with age, beginning at 40–50 years old (<xref ref-type="bibr" rid="bib18">Fotiou et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Lobato-Rincón et al., 2014</xref>). However, trial time-course pupil waveforms are very similar for both general and aphantasic populations (<xref ref-type="fig" rid="fig1">Figures 1B</xref> and <xref ref-type="fig" rid="fig3">3A</xref>, respectively). Both groups exhibited similar levels of pupil change during the perception phase of the task. Furthermore, a two-way ANCOVA was run on pupil-difference scores between general population and aphantasic groups with age as a covariate. Levene’s test and normality checks were carried out and the assumptions were met. We found a significant difference in pupil-difference score (<italic>F</italic>(1, 57) = 4.763, p = 0.033) between the groups when accounting for age. This provides evidence that decreased pupil responsiveness with age was not driving the observed effects.</p><p>Another possible explanation of our findings could be that the passive viewing of the perceptual images, lingering visual persistence and sluggish pupil responses could be driving our results. If this is the case, we would expect that pupil diameter during the perception of the images should correlate with pupil size during imagery for the corresponding images. Further, the pupillary light reflex during perception should be more pronounced in the control than the aphantasic populations. To investigate this possible alternative explanation of our data we first assessed the correlations between pupil diameter during perception of bright and dark images for Set-Size-One and -Four and their corresponding imagery conditions (control participants only). We found there were no significant correlations between any of the perception and imagery conditions, or the difference scores for set size one and four (all p &gt; 0.40, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). This lack of a correlation suggests that those individuals who have the largest pupillary light response while viewing the images, do not also have the greatest imagery driven pupillary light responses, making it unlikely that the pupil response while seeing the image is driving the mental imagery pupillary response. Next, we assessed whether the aphantasic individuals demonstrated any significant difference in their pupil responses to perceptual stimuli by running a 2 (image: bright and dark) × 2 (set size: 1 and 4) × 2 (group: aphantasic and controls) repeated measures ANOVA on the pupil diameter during the 5-s perceptual period of the task (see <xref ref-type="fig" rid="fig1">Figure 1A</xref> for task timeline). There was no main effect of imagery group <italic>F</italic>(1, 58) = 1.15, p = 0.29 and no significant interactions between imagery groups and any other factor (all p &gt; 0.22, see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). These findings suggest the observed pupil responses during the imagery period of the task is unlikely to be a carry-over effect of the previous sensory response to perceived images.</p><p>Pupil size has been shown to depend on eye position (<xref ref-type="bibr" rid="bib16">Drewes et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Gagl et al., 2011</xref>) and the preparation to make a saccade to an upcoming image (<xref ref-type="bibr" rid="bib26">Jainta et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Mathôt et al., 2015</xref>; <xref ref-type="bibr" rid="bib69">Wang et al., 2018</xref>). Pupil modulation and eye position are also both controlled by largely overlapping circuitry (<xref ref-type="bibr" rid="bib70">Wang and Munoz, 2018</xref>). It could then be the case that group differences in eye position or saccades (either while viewing or imagining the triangles) may explain our data. To assess how eye movements and position (eccentricity) might be related to our findings, we analysed both eye position and saccades made while viewing and imagining the images, to see if these differed as a function of group. Eccentricity was extracted using the ‘saccades’ package in R (<xref ref-type="bibr" rid="bib68">von der Malsburg, 2015</xref>) and saccades were detected using a velocity-based algorithm (<xref ref-type="bibr" rid="bib17">Engbert and Kliegl, 2003</xref>) using the same R package. There were no significant differences between the groups for the number of saccades made during perception or imagery of the stimuli (see <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref> and <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>). There were also no differences in mean eccentricity values when comparing the two groups (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>), and no correlation with eccentricity and the pupillary light response (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), binocular priming (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), or vividness ratings (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>), suggesting that differences in fixation or eye movements between the two groups is unlikely to drive the observed group differences in regard to the mental imagery pupillary light response.</p><p>Taken together these data from the general population and aphantasic individuals suggest that it is the content and ability to form vivid visual images, not the voluntary attempt to do so or the semantic content, that is driving the imaginary pupillary light response, providing the first evidence that these pupil changes are due to the sensory strength of imagery content and are not driven by higher-level semantic content.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our results provide novel evidence that our pupils respond to the vividness and strength of a visual image being held in mind, the stronger and more vivid that image, the greater the pupillary light response. Our data provide the first evidence linking the pupil response to strength and vividness of imagery, not only between individuals, but also within an individual as imagery vividness fluctuates from moment to moment (<xref ref-type="bibr" rid="bib14">Dijkstra et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Pearson et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Rademaker and Pearson, 2012</xref>). Finally, we show that, as a group, there is no evidence of this pupil response in individuals without mental imagery (aphantasia).</p><p>How might the content of mental imagery be driving the pupillary light response? One interpretation of these findings is that this imagery pupillary response is a by-product of the top-down modulation of midbrain-level visual circuitry (pretectal olivary nucleus, superior colliculus; <xref ref-type="bibr" rid="bib27">Joshi and Gold, 2020</xref>), which occurs when imagining vividly, resulting in these regions interpreting this modulation as coming from external or afferent stimuli, and responding accordingly (<xref ref-type="bibr" rid="bib41">Larsen and Waters, 2018</xref>; <xref ref-type="bibr" rid="bib63">Schwalm and Rosales Jubal, 2017</xref>). In this case, the pupil would be responding to imagined luminance in much the same way that it responds to retina-based light sources. This is consistent with current data and models proposing shared mechanisms between visual imagery and perception (<xref ref-type="bibr" rid="bib14">Dijkstra et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Dijkstra et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Ganis et al., 2004</xref>; <xref ref-type="bibr" rid="bib50">Naselaris et al., 2015</xref>; <xref ref-type="bibr" rid="bib74">Xie et al., 2020</xref>) and the idea that visual imagery functions much like a weak version of afferent perception (<xref ref-type="bibr" rid="bib55">Pearson, 2019</xref>), supporting the idea that the stronger or more vivid an individual’s imagery is, the more ‘perception like’ their imagery is.</p><p>An alternative mechanistic account might be that pupil diameter is encoded along with the original visual information for example bright object, and hence is replayed during memory decoding to form the mental image. This would be in a similar manner to theories proposing a functional role of eye movements during imagery generation from memory (<xref ref-type="bibr" rid="bib71">Wang et al., 2020</xref>). It will be up to future work to uncover the exact mechanist account of imagery induced pupil changes.</p><p>Here we also provide the first objective physiological evidence of an extreme lack of visual imagery in aphantasic individuals. Aphantasia has largely been defined using subjective means (<xref ref-type="bibr" rid="bib11">Dawes et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Jacobs et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Pounder et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Zeman et al., 2015</xref>, but see <xref ref-type="bibr" rid="bib34">Keogh and Pearson, 2018</xref>). Accordingly, people have remained sceptical about its true nature and possible psychogenic basis (<xref ref-type="bibr" rid="bib12">de Vito and Bartolomeo, 2016</xref>). Our data demonstrate that using a non-visual strategy (no imagery in aphantasia) to think about bright and dark objects does not induce a pupillary light response. These data simultaneously provide strong evidence linking the pupillary light response to mental imagery, as well as supporting the behavioural work showing that aphantasic individuals indeed lack visual sensory imagery (<xref ref-type="bibr" rid="bib34">Keogh and Pearson, 2018</xref>). Because the pupillary light response is involuntary (<xref ref-type="bibr" rid="bib7">Bouffard, 2019</xref>), we can consider these findings as an unbiased neurophysiological measure of aphantasia. Not only do these data show that pupillary light response can be an objective index of imagery strength in studies of imagery in general populations, our data also provide a new low-cost objective measure for aphantasia that is uniquely based on a physiological mechanism and not reliant on self-report.</p><p>Could a lack of active engagement during imagery explain the aphantasia results? Put another way, are such participants refusing to imagine (<xref ref-type="bibr" rid="bib12">de Vito and Bartolomeo, 2016</xref>)? We think this is highly unlikely as pupil size <italic>did</italic> increase as a function of set size for aphantasic individuals when attempting imagery, as has previously been shown in the general population, demonstrating the typical relationship between cognitive effort or arousal and pupil dilation (<xref ref-type="bibr" rid="bib29">Kahneman and Beatty, 1966</xref>; <xref ref-type="bibr" rid="bib67">van der Wel and van Steenbergen, 2018</xref>). This demonstrates active task engagement, suggesting that aphantasic individuals were not simply ‘refusing’ to actively participate in the task due to demand characteristics or a belief that they are unable to imagine (<xref ref-type="bibr" rid="bib12">de Vito and Bartolomeo, 2016</xref>).</p><p>Further, we ran Bayesian one-sample <italic>t</italic>-tests on the binocular rivalry and pupillary light response difference scores (see <xref ref-type="fig" rid="fig2">Figure 2</xref>) comparing their performance to chance to see if there was any evidence they were performing significantly below chance. We found no significant evidence of below chance performance for either group on either the binocular rivalry or pupillometry imagery tasks (see <xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref>). Taken together, with the set-size pupillary effect we observed in our aphantasic participants, it seems unlikely that our aphantasic individuals were not engaging in the tasks. However, we cannot fully rule out this possibility. Further, there was no significant evidence of an abnormal pupillary response in our aphantasic cohort when viewing images, thus it is likely the lack of an imaginary pupillary light response is due to their lack of visual imagery. It also reveals that regardless of what imagery strategy aphantasic participants are implementing (e.g. propositional, spatial, language-like) to recall information about the shapes, they require greater cognitive effort to simultaneously maintain a larger number of shapes in their mind.</p><p>One limitation of our study is we did not include catch trails in our pupillometry task, that is we did not include trials where we asked participants to report on what image they had been asked to imagine. We did however include catch trials in our binocular rivalry task through presenting mock binocular rivalry trials. If aphantasic participants are showing a response bias we would expect see a reduction in these mock priming trials when compared to the control population, which we did not find (see <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). Adding catch trials to future experiments, in addition to set-size manipulations, may help to further confirm participant engagement. However, adding a simultaneous memory component to the task may lead some subjects to use a non-visual imagery strategy and as such, a reduction or dilution of the pupillary light response (see <xref ref-type="bibr" rid="bib56">Pearson and Keogh, 2019</xref>). Future studies of visual imagery, and even more importantly when investigating aphantasia, should aim to include appropriate positive controls that allow for the identification of task engagement even when an individual doesn’t have visual imagery. This will allow researchers to exclude the alternate explanation that those individuals who do not show evidence of imagery are not just refusing to imagine or not completing the task correctly.</p><p>Another possible explanation of our results is that perceptual pupillary light responses are lingering throughout each trial and driving the observed imagery pupil response. If this is the case, then pupil responses during perceptual viewing and imagery should be correlated, however we did not find any such correlations (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Further, when directly comparing the perceptual pupil responses between the general population and aphantasic individuals, there was no main effect of group or interaction between group and stimuli brightness or set size (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). This demonstrates that there is no significant difference in the perceptual pupillary responses between the two groups, making it unlikely that aphantasic individual’s lack of an imagery pupillary response is due to a lack of perceptual response. Finally, we also asked participants if they perceived any after images during the imagery period and any participants who reported they did were excluded from the study. Taken together, these results suggest that it is unlikely that the pupillary response to perceptually viewing the images is driving our observed imagery pupillary responses, and the lack thereof in the aphantasic individuals. Instead, it appears the pupillary light response during the visual imagery period reflects the wilful generation of imagery in the mind’s eye of those who experience visual imagery. This is further substantiated by the strength of visual imagery (measured using the binocular rivalry paradigm) correlating with the imagery pupillary light reflex, but not the perceptual pupillary light reflex (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><p>We also found that in the imagery task, higher within-trial reports of vividness are reflected by greater pupillary light responses (within-subjects effects; see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). This indicates that participants were able to accurately evaluate the vividness of individual episodes of imagery in comparison to other vividness episodes on previous trials. However, average vividness ratings did not correlate with their pupil-difference scores, that is, participants who gave higher vividness ratings on average did not necessarily have increased pupil light responses in response to imagery (between-subjects effects; see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Participant’s scores on the VVIQ also did not correlate with pupil-difference scores (between-subjects effects; see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). This suggests that participants might have difficulties in accurately reporting their strength of sensory visual imagery on an absolute scale (i.e. from ‘no image’ to ‘as vivid as perception’), and brings into question the reliability of these subjective measures of imagery and highlights the utility of using objective or online (i.e. in a task), and less trait-like measures when studying visual imagery.</p><p>Recent studies have shown pupil size is also modulated by the content of visual working memory (<xref ref-type="bibr" rid="bib6">Blom et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Hustá et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Zokaei et al., 2019</xref>). It is interesting to note here that previous work has shown that imagery has been implicated as one mnemonic that can be used to retain information in mind during visual working memory tasks (<xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Keogh and Pearson, 2011</xref>; <xref ref-type="bibr" rid="bib32">Keogh and Pearson, 2014</xref>; <xref ref-type="bibr" rid="bib33">Keogh and Pearson, 2017</xref>). This highlights the possibility that it is imagery, being used as a mnemonic strategy, that is driving the pupillary light response observed in visual working memory experiments (<xref ref-type="bibr" rid="bib56">Pearson and Keogh, 2019</xref>). Although many participants report using a visual imagery strategy during these tasks, some participants report using a non-visual imagery strategy when remembering visual information, and recent work demonstrates that aphantasic individuals can perform traditional visual working memory tasks just as well as control populations (<xref ref-type="bibr" rid="bib37">Keogh et al., 2021b</xref>). Measuring the pupillary light response in aphantasic individuals, and those who report not using an imagery strategy, while performing classic visual working memory tasks may help to further elucidate these differences in cognitive strategy use in a more objective manner.</p><p>One limitation that is important to note here is that our aphantasic sample contained a relatively small sample (18 participants) due to the relative rarity of this condition. Further our two samples were not age matched, which may have affected our results, however seeing as there was no difference between the two groups for the perceptual pupillary light response, we think this is unlikely to be driving our findings. Future studies should aim to replicate and extend these findings with a larger group of aphantasic individuals and age matched controls.</p><p>To conclude, the present study demonstrates that the pupillary light response can be used as a physiological index of individual differences in the sensory and phenomenological strength of visual imagery, including the lack of visual imagery – aphantasia. Combining this measure with the binocular rivalry paradigm in favour of subjective alternatives will increase the reliability and objectivity of imagery test batteries and may lead to the development of more congenial theories of the mind’s eye.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Fifty-six psychology students with a mean age of 19.8 years (range 18–31, 27 females) were recruited for the study and participated for course credit. We aimed to obtain analysable data from a minimum of 40 participants, which should be a large enough sample to identify a strong positive correlation between pupil dilation and imagery, which is what we would expect if imagery content were driving the previously observed imagery pupillary light response (g*Power effect size = 0.5, <italic>α</italic> = 0.05, <italic>β</italic> = 0.95). Fourteen of these participants were excluded from data analysis for not meeting a priori criteria (see Exclusion criteria), leaving 42 participants in the final general population sample.</p><p>The aphantasic individuals come from a rare population and for this reason we did not run a specific power analysis but aimed to collect a minimum of 15 participants. We had nineteen aphantasic individuals agree to participate in the study with a mean age of 35.8 years (aged 18–54, 12 females). One of these individuals was excluded from data analysis for not meeting a priori criteria (see Exclusion criteria), leaving 18 in the final sample. These participants had all contacted the lab regarding their aphantasia and asked to participate in our research. They were all reimbursed $20 AUD per hour for their participation. All participants had normal or corrected to normal vision (i.e. glasses or contacts). Both experiments were approved by the UNSW Human Research Ethics Advisory Panel (HREAP-C 3182).</p></sec><sec id="s4-2"><title>Apparatus</title><p>Apparatus stimuli in all experiments were presented on an LCD display monitor (Dell UltraSharp U2419H) with 60 Hz refresh rate and a 1920 × 1080 resolution. Luminance values of all stimuli were measured using a Konica Minolta chroma meter (CS-100A). Participants placed their chin on a chin rest throughout the experiment to maintain fixation at a distance of 57 cm from the monitor 13 and to limit head movements. The tasks were performed in a blackened room to eliminate any possible fluctuations in ambient light.</p><p>In the <bold>pupillary response task</bold>, pupil sizes and eye movements were recorded using head mounted eye-tracking glasses (Pupil, Pupil Labs GmbH, Berlin, Germany) (<xref ref-type="bibr" rid="bib30">Kassner et al., 2014</xref>). Pupil diameter of participants’ right eye was continuously sampled at 200 Hz throughout the task. A pupil detection 3D algorithm locates the dark pupil in the infrared illuminated eye camera image, thus recording capabilities are not compromised by an absence of room lighting. Pupil diameter is then scaled to millimetres (mm) based on mean anthropomorphic eyeball diameter and corrected for perspective. The algorithm does not depend on corneal reflection, and is compatible with users who wear contact lenses and most eyeglasses (<xref ref-type="bibr" rid="bib30">Kassner et al., 2014</xref>).</p><p>A second camera mounted on the glasses continuously recorded participants’ field of view. Footage from this camera was subsequently assessed to ensure fixation on the computer monitor was maintained throughout the task. The experiment was designed using MATLAB (version R2017b). ZeroMQ plug-ins were used for cross-communication between eye-tracking and stimulus presentation platforms (<xref ref-type="bibr" rid="bib1">Akgul, 2013</xref>). Pupil data were recorded with Pupil Capture v.1.10.20 (Pupil Labs) installed on an ASUS (GL502V) PC (Windows 10).</p><p>In the <bold>binocular rivalry task</bold>, participants wore red-green anaglyph glasses to ensure rivalrous stimuli were presented to left and right eyes in isolation. Responses of 1, 2, or 3 on a keyboard were used by participants to indicate which image dominated their perception during binocular rivalry (1 for green; 3 for red; 2 for perceptually mixed green and red).</p></sec><sec id="s4-3"><title>Stimuli</title><p>For the <bold>pupillary response task</bold>, 32 achromatic shape stimuli were created for participants to perceive and then later imagine in their absence, across 32 trials. The stimuli were evenly divided based on a 2 × 2 factorial design, belonging to one of two luminance conditions (‘Bright’ or ‘Dark’) and one of two set-size conditions (‘Set-Size-One’ or ‘Set-Size-Four’). Shapes belonging to the Bright condition were either white with a luminance of 117 cd/m<sup>2</sup> or light grey with a luminance of 65 cd/m<sup>2</sup>. Shapes in the Dark condition were black (1 cd/m<sup>2</sup>) of dark grey (9 cd/m<sup>2</sup>). Set-Size-One stimuli consisted of a single equilateral triangle with 12.5 cm sides, subtending 12.5° of visual angle. Set-Size-Four stimuli consisted of an arrangement of four smaller equilateral triangles with a total surface area and luminance equal to that of the corresponding Set-Size-One triangles (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for illustration of all stimuli). Stimuli were also uniquely orientated at either 0°, 90°, 180°, or 270° (e.g. four Set-Size-One black triangles, each with a different orientation. See <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for examples all possible shape orientations). This ensured that all 32 stimuli were unique and participants and were encoding information about a new stimulus on each trial, therefore avoiding the use of long-term memory. Set-Size-Four stimuli therefore subtended either 10.8° or 18.9° of visual angle depending on their orientation.</p><p>All stimuli were presented on a grey background screen with a luminance of 26 cd/m<sup>2</sup>. This same level of background luminance was used during measurement of baseline and imagery phases. A fixation cross on a black background with a luminance of 1 cd/m<sup>2</sup> was presented during the resting phase of each trial. All stimuli were created in MATLAB, using the Psychophysics Toolbox 3 extensions (<xref ref-type="bibr" rid="bib9">Brainard, 1997</xref>).</p><p>For the <bold>binocular rivalry task</bold>, sinusoidal luminance modulated Gabor patterns were used as rivalrous stimuli; vertical-green (CIE chromaticity coordinates: <italic>x</italic> = 0.275, <italic>y</italic> = 0.590) and horizontal-red (CIE chromaticity coordinates: <italic>x</italic> = 0.492, <italic>y</italic> = 0.372), both with a mean luminance of 8.35 cd/m<sup>2</sup> and 7.1° of visual angle. In each trial, both patterns were presented at the same time around a fixation point at the centre of a black background screen. Mock rivalry stimuli (a single 15 Gabor pattern spatially divided into half vertical-green and half horizontal-red) were used on 12.5% of trials to measure the influence of decisional bias or lack of attention to the task. More details on the binocular rivalry task can be found in <xref ref-type="bibr" rid="bib32">Keogh and Pearson, 2014</xref>.</p><p>All participants also complete the VVIQ (<xref ref-type="bibr" rid="bib43">Marks, 1973</xref>) to get a self-report measure of trait imagery vividness (see <xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref> for VVIQ data for general population and aphantasic individuals).</p></sec><sec id="s4-4"><title>Procedure</title><sec id="s4-4-1"><title>Pupillometry imagery experiment timeline</title><p>Each trial began with the presentation of a white fixation cross at the centre of a grey screen (baseline) for 1 s. An image was then presented at the centre of this grey screen for 5 s (either one or four triangles of varying brightness, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for illustrations of all stimuli). Participants were instructed to focus on the stimuli during this time and memorize its size, orientation, and level of brightness. Next, a black screen with a white fixation cross was presented for 8 s, allowing the perceived after-image to completely fade and pupils to dilate back to equivalent resting levels. The grey baseline screen was then presented again for 6 s. During this time, participants were cued (via two auditory beeps) to actively imagine the stimuli observed previously during that trial. Lastly, participants were prompted to report the vividness of their imagery during those previous 5 s on a scale of 1–4 (1 being ‘not vivid at all – no shape appeared in imagery’; 4 being ‘very vivid – almost like seeing it’) via key response.</p></sec><sec id="s4-4-2"><title>Binocular rivalry paradigm</title><p>Participants were cued to imagine either a red or green Gabor pattern prior to binocular rivalry with the letter ‘R’ or ‘G’ (750 ms). Participants then imagined the image for 6 s, after which they were presented with the binocular rivalry display (750 ms) and were asked to indicate which image was dominant (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Trials where participants reported seeing the pattern they were cued to imagine as dominant were denoted as ‘primed’ trials. Participants completed 2 blocks of 48 trials resulting in a total of 96 trials in total (84 real and 12 mock trials). The number of primed trials divided by the total number of trials (excluding mock trials and mixed percepts) was used to calculate a percent primed score for each participant. Mock trial priming was calcuated by giving a value to each mock trial as either 0 (reporting the catch trial as the opposite colour to that primed), 50 (reporting the catch trial as being mixed), or 100% (reporting the catch trial to be the same as the cued image) (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). These values are then averaged to get a priming value where 50% indicates no bias, while higher values indicate a bais towards reporting the mock trails as being the same as the imagined image, while negative numbers indicate a bias towards reporting the oppoiste image to that which was imagined.</p></sec></sec><sec id="s4-5"><title>Exclusion criteria</title><p>Of the 56 participants recruited for the general population sample, 14 in total were excluded from data analysis due to not meeting a priori criteria.</p><p>Pupillary response task exclusions: eight participants were excluded because more than 50% of their pupil data points were below the pupil detection algorithm confidence value of 0.6, provided by the Pupil Capture system. This cut-off point was derived prior to data collection and is the recommended cut-off point for obtaining accurate pupil size data (Pupil Labs). Three participants were excluded due to reporting (during systematic post-task questioning) seeing after-images of the shape stimuli for longer than the 8 s black screen presentation (i.e. seeing after-images during the imagery phase of trials), because pupil size is known to be influenced by the induced compensatory light perception of an after-image (<xref ref-type="bibr" rid="bib66">Tsujimura et al., 2003</xref>).</p><p>Binocular rivalry task exclusions: three participants were excluded due to having mock rivalry priming &gt;66.67% (more than one incorrect response on the mock trials), which indicated either an influence of decisional bias or lack of attention to the task. An a priori cuff-off point of scoring <italic>both</italic> below 65% priming on the binocular rivalry task and below 32 on the VVIQ was used to exclude participants who potentially did not have visual imagery (i.e. may be aphantasic). No participants fell below this combined cut-off point thus none were excluded on this basis.</p><p>Of the 19 participants recruited for the aphantasic population, 1 was excluded from data analysis because more than 50% of their pupil data points were below the pupil detection algorithm confidence value of 0.6, given by the Pupil Capture system. All participants scored below both of the a priori cut-off points of 32 on the VVIQ and 65% on the binocular rivalry task, therefore, no participants were excluded due to this criterion.</p></sec><sec id="s4-6"><title>Data analysis</title><p>For the pupillary response task, cubic spline interpolation was used to estimate pupil diameter during periods where subjects’ pupils were occluded due to blinking (in accordance with <xref ref-type="bibr" rid="bib44">Mathôt et al., 2013</xref>). Artefacts in the pupil data were then smoothed using a moving average Hanning window (<xref ref-type="bibr" rid="bib38">Kret and Sjak-Shie, 2019</xref>). Individual trials in which mean pupil diameter while passively viewing the grey baseline screen was lower than 2 mm or higher than 8 mm were excluded (<italic>N</italic>(total trials from whole sample) = 8) as values outside this range are unnatural pupil sizes and were clear outliers based on inspection of participants’ pupil-baseline histograms (<xref ref-type="bibr" rid="bib48">Mathôt et al., 2018</xref>). Trials were averaged to form condition-specific pupil diameter waveforms to represent change in pupil size over time. Mean pupil diameter values during imagery in each trial were baseline corrected using a within-trial baseline subtraction approach (<xref ref-type="bibr" rid="bib48">Mathôt et al., 2018</xref>) (i.e. subtracted from mean pupil diameter during 0.5 s prior to stimulus perception onset) to account for temporal shifts in pupil size across the experimental session due to fatigue (<xref ref-type="bibr" rid="bib49">Morad et al., 2000</xref>). A two-way repeated measures ANOVA was used to compare Dark and Bright means during perception and imagery within both set-size conditions. ‘Pupil-difference’ scores were calculated by subtracting Dark condition means from Bright condition means of the corresponding set size for comparison with binocular rivalry percent primed scores. Pupil-difference scores were also separated based on the discrete within-trial vividness ratings to assess metacognition and whether pupil size changes in response to imagery were reflective of subjects’ own experience of vividness of visual imagery.</p><p>In the binocular rivalry task, trials where participants reported seeing the pattern they were cued to imagine as dominant in the subsequent binocular rivalry display were denoted as ‘primed’ trials. The number of primed trials divided by the total number of trials (excluding mock trials and mixed percepts) was used to calculate a percent primed score for each participant. Participants’ percent primed scores in binocular rivalry were correlated with their pupil-difference scores (both Set-Size-One and Set-Size-Four) to assess potential for the pupillary response task to measure individual variability in visual imagery strength.</p><p>The LMEs were run in R (<xref ref-type="bibr" rid="bib58">R Development Core Team, 2018</xref>) using the lme4 package and ANOVA’s and ANCOVA were run in SPSS v.25.0 (<xref ref-type="bibr" rid="bib24">IBM Corp. Released, 2017</xref> ). For the linear mixed-effects models set size (1 or 4) and vividness ratings (1, 2, 3, and 4) were entered into the model as fixed effects. As random effects intercepts for subjects were entered into the model. p values were obtained by likelihood ratio tests of the full model with vividness included vs. the model without vividness included.</p><p>Bayesian statistics were used to determine whether null findings can be interpreted as evidence for an absence of effect (<xref ref-type="bibr" rid="bib13">Dienes, 2014</xref>). We used Bayesian repeated measure ANOVA (within-subject effect: set size; between-subject effect: group) to compare the control and aphantasia groups as well as Bayesian one-sample <italic>t</italic>-tests to compare each group with H0, defined as the absence of effect. All Bayesian analysed were performed with JASP (Version 0.10.2).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Supervision, Visualization, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Methodology, Software, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Informed written consent was obtained from all participants to participate in the experiment and to publish their anonymized data in a journal article. Both experiments were approved by the UNSW Human Research Ethics Advisory Panel (HREAP-C 3182).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Fixed effects estimates for pupil-difference scores as a function of vividness ratings.</title><p>This file provides the fixed effects estimates for the LME run on the pupil-difference scores for the general population as a function of vividness ratings and set size.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-72484-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-72484-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Figure 1 - Source Data 1&amp; 2, Figure 2 - Source Data 3, and Figure 3 - Source Data 4 contain the numerical data used to generate the figures.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Akgul</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>ZeroMQ: Use ZeroMQ and Learn How to Apply Different Message Patterns</source><publisher-name>Packt Publishing</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergmann</surname><given-names>J</given-names></name><name><surname>Genç</surname><given-names>E</given-names></name><name><surname>Kohler</surname><given-names>A</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Smaller Primary Visual Cortex Is Associated with Stronger, but Less Precise Mental Imagery</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>26</volume><fpage>3838</fpage><lpage>3850</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv186</pub-id><pub-id pub-id-type="pmid">26286919</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binda</surname><given-names>P</given-names></name><name><surname>Pereverzeva</surname><given-names>M</given-names></name><name><surname>Murray</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Attention to bright surfaces enhances the pupillary light reflex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>2199</fpage><lpage>2204</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3440-12.2013</pub-id><pub-id pub-id-type="pmid">23365255</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binda</surname><given-names>P</given-names></name><name><surname>Pereverzeva</surname><given-names>M</given-names></name><name><surname>Murray</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Pupil constrictions to photographs of the sun</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/13.6.8</pub-id><pub-id pub-id-type="pmid">23685391</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blom</surname><given-names>T</given-names></name><name><surname>Mathôt</surname><given-names>S</given-names></name><name><surname>Olivers</surname><given-names>CNL</given-names></name><name><surname>Van der Stigchel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The pupillary light response reflects encoding, but not maintenance, in visual working memory</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>42</volume><fpage>1716</fpage><lpage>1723</lpage><pub-id pub-id-type="doi">10.1037/xhp0000252</pub-id><pub-id pub-id-type="pmid">27428779</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouffard</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Pupil</article-title><source>CONTINUUM (Minneapolis, Minn.)</source><volume>25</volume><fpage>1194</fpage><lpage>1214</lpage><pub-id pub-id-type="doi">10.1212/CON.0000000000000771</pub-id><pub-id pub-id-type="pmid">31584534</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>MM</given-names></name><name><surname>Miccoli</surname><given-names>L</given-names></name><name><surname>Escrig</surname><given-names>MA</given-names></name><name><surname>Lang</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The pupil as a measure of emotional arousal and autonomic activation</article-title><source>Psychophysiology</source><volume>45</volume><fpage>602</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00654.x</pub-id><pub-id pub-id-type="pmid">18282202</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The functional effects of prior motion imagery and motion perception</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>105</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.08.036</pub-id><pub-id pub-id-type="pmid">28958416</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawes</surname><given-names>AJ</given-names></name><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Andrillon</surname><given-names>T</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A cognitive profile of multi-sensory imagery, memory and dreaming in aphantasia</article-title><source>Scientific Reports</source><volume>10</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41598-020-65705-7</pub-id><pub-id pub-id-type="pmid">32572039</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vito</surname><given-names>S</given-names></name><name><surname>Bartolomeo</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Refusing to imagine? On the possibility of psychogenic aphantasia</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>74</volume><fpage>334</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.06.013</pub-id><pub-id pub-id-type="pmid">26195151</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dienes</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Using Bayes to get the most out of non-significant results</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>781</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00781</pub-id><pub-id pub-id-type="pmid">25120503</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Vividness of Visual Imagery Depends on the Neural Overlap with Perception in Visual Areas</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>1367</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3022-16.2016</pub-id><pub-id pub-id-type="pmid">28073940</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared Neural Mechanisms of Visual Perception and Imagery</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>423</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.02.004</pub-id><pub-id pub-id-type="pmid">30876729</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drewes</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>W</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Smaller is better: drift in gaze measurements due to pupil dynamics</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e111197</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0111197</pub-id><pub-id pub-id-type="pmid">25338168</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Microsaccades uncover the orientation of covert attention</article-title><source>Vision Research</source><volume>43</volume><fpage>1035</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(03)00084-1</pub-id><pub-id pub-id-type="pmid">12676246</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fotiou</surname><given-names>DF</given-names></name><name><surname>Brozou</surname><given-names>CG</given-names></name><name><surname>Tsiptsios</surname><given-names>DJ</given-names></name><name><surname>Fotiou</surname><given-names>A</given-names></name><name><surname>Kabitsi</surname><given-names>A</given-names></name><name><surname>Nakou</surname><given-names>M</given-names></name><name><surname>Giantselidis</surname><given-names>C</given-names></name><name><surname>Goula</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Effect of Age on Pupillary Light Reflex: Evaluation of Pupil Mobility for Clinical Practice and Research</source><publisher-name>Electromyography and Clinical Neurophysiology</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagl</surname><given-names>B</given-names></name><name><surname>Hawelka</surname><given-names>S</given-names></name><name><surname>Hutzler</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Systematic influence of gaze position on pupil size measurement: analysis and correction</article-title><source>Behavior Research Methods</source><volume>43</volume><fpage>1171</fpage><lpage>1181</lpage><pub-id pub-id-type="doi">10.3758/s13428-011-0109-5</pub-id><pub-id pub-id-type="pmid">21637943</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galton</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1880">1880</year><article-title>I.--STATISTICS OF MENTAL IMAGERY</article-title><source>Mind</source><volume>os-V</volume><fpage>301</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1093/mind/os-V.19.301</pub-id><pub-id pub-id-type="pmid">196863</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganis</surname><given-names>G</given-names></name><name><surname>Thompson</surname><given-names>WL</given-names></name><name><surname>Kosslyn</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Brain areas underlying visual mental imagery and visual perception: an fMRI study</article-title><source>Brain Research. Cognitive Brain Research</source><volume>20</volume><fpage>226</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.02.012</pub-id><pub-id pub-id-type="pmid">15183394</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Guariglia</surname><given-names>C</given-names></name><name><surname>Pizzamiglio</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>The Role of Imagery in Navigation: Neuropsychological Evidence</chapter-title><person-group person-group-type="editor"><name><surname>Mast</surname><given-names>F</given-names></name><name><surname>Jäncke</surname><given-names>L</given-names></name></person-group><source>Spatial Processing in Navigation, Imagery and Perception</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer</publisher-name><fpage>17</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-71978-8</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hustá</surname><given-names>C</given-names></name><name><surname>Dalmaijer</surname><given-names>E</given-names></name><name><surname>Belopolsky</surname><given-names>A</given-names></name><name><surname>Mathôt</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The pupillary light response reflects visual working memory content</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>45</volume><fpage>1522</fpage><lpage>1528</lpage><pub-id pub-id-type="doi">10.1037/xhp0000689</pub-id><pub-id pub-id-type="pmid">31436453</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><collab>IBM Corp. Released</collab></person-group><year iso-8601-date="2017">2017</year><data-title>IBM SPSS Statistics version 25.0</data-title><publisher-loc>Armonk, NY</publisher-loc><publisher-name>IBM Corp</publisher-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>C</given-names></name><name><surname>Schwarzkopf</surname><given-names>DS</given-names></name><name><surname>Silvanto</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual working memory performance in aphantasia</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>105</volume><fpage>61</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.10.014</pub-id><pub-id pub-id-type="pmid">29150139</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jainta</surname><given-names>S</given-names></name><name><surname>Vernet</surname><given-names>V</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Kapoula</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The pupil reflects motor preparation for saccades – even before the eye starts to move</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>97</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00097</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil Size as a Window on Neural Substrates of Cognition</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>466</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.03.005</pub-id><pub-id pub-id-type="pmid">32331857</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Just</surname><given-names>MA</given-names></name><name><surname>Newman</surname><given-names>SD</given-names></name><name><surname>Keller</surname><given-names>TA</given-names></name><name><surname>McEleney</surname><given-names>A</given-names></name><name><surname>Carpenter</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Imagery in sentence comprehension: an fMRI study</article-title><source>NeuroImage</source><volume>21</volume><fpage>112</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.08.042</pub-id><pub-id pub-id-type="pmid">14741648</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Beatty</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Pupil diameter and load on memory</article-title><source>Science (New York, N.Y.)</source><volume>154</volume><fpage>1583</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1126/science.154.3756.1583</pub-id><pub-id pub-id-type="pmid">5924930</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kassner</surname><given-names>M</given-names></name><name><surname>Patera</surname><given-names>W</given-names></name><name><surname>Bulling</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil: An open source platform for pervasive eye tracking and mobile gaze-based interaction</article-title><conf-name>UbiComp 2014 - Adjunct Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</conf-name><pub-id pub-id-type="doi">10.1145/2638728.2641695</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mental imagery and visual working memory</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e29221</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0029221</pub-id><pub-id pub-id-type="pmid">22195024</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The sensory strength of voluntary visual imagery predicts visual working memory capacity</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/14.12.7</pub-id><pub-id pub-id-type="pmid">25301015</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The perceptual and phenomenal capacity of mental imagery</article-title><source>Cognition</source><volume>162</volume><fpage>124</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2017.02.004</pub-id><pub-id pub-id-type="pmid">28235663</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The blind mind: No sensory visual imagery in aphantasia</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>105</volume><fpage>53</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.10.012</pub-id><pub-id pub-id-type="pmid">29175093</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attention driven phantom vision: measuring the sensory strength of attentional templates and their relation to visual mental imagery and aphantasia</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>376</volume><elocation-id>20190688</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0688</pub-id><pub-id pub-id-type="pmid">33308064</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Zeman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021a</year><source>Handbook of Clinical Neurology</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1016/B978-0-12-821377-3.00012-X</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Wicken</surname><given-names>M</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Visual working memory in aphantasia: Retained accuracy and capacity with a different strategy</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>143</volume><fpage>237</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2021.07.012</pub-id><pub-id pub-id-type="pmid">34482017</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>Sjak-Shie</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Preprocessing pupil size data: Guidelines and code</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>1336</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1075-y</pub-id><pub-id pub-id-type="pmid">29992408</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laeng</surname><given-names>B</given-names></name><name><surname>Ørbo</surname><given-names>M</given-names></name><name><surname>Holmlund</surname><given-names>T</given-names></name><name><surname>Miozzo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupillary stroop effects</article-title><source>Cognitive Processing</source><volume>1</volume><elocation-id>e370</elocation-id><pub-id pub-id-type="doi">10.1007/s10339-010-0370-z</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laeng</surname><given-names>B</given-names></name><name><surname>Sulutvedt</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Eye Pupil Adjusts to Imaginary Light</article-title><source>Psychological Science</source><volume>25</volume><fpage>188</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1177/0956797613503556</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>RS</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neuromodulatory correlates of pupil dilation</article-title><source>Frontiers in Neural Circuits</source><volume>1</volume><elocation-id>e21</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00021</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lobato-Rincón</surname><given-names>LL</given-names></name><name><surname>Cabanillas-Campos</surname><given-names>M</given-names></name><name><surname>Bonnin-Arias</surname><given-names>C</given-names></name><name><surname>Chamorro-Gutiérrez</surname><given-names>E</given-names></name><name><surname>Murciano-Cespedosa</surname><given-names>A</given-names></name><name><surname>Roda</surname><given-names>CSR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupillary behavior in relation to wavelength and age</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>221</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00221</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Visual imagery differences in the recall of pictures</article-title><source>British Journal of Psychology (London, England: 1953)</source><volume>64</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1973.tb01322.x</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S</given-names></name><name><surname>Aarts</surname><given-names>E</given-names></name><name><surname>Verhage</surname><given-names>M</given-names></name><name><surname>Veenvliet</surname><given-names>JV</given-names></name><name><surname>Dolan</surname><given-names>CV</given-names></name><name><surname>Sluis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A simple way to reconstruct pupil size during eye blinks</article-title><source>Figshare</source><volume>10</volume><elocation-id>3648</elocation-id><pub-id pub-id-type="doi">10.1038/nn.3648</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S</given-names></name><name><surname>van der Linden</surname><given-names>L</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Vitu</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The pupillary light response reflects eye-movement preparation</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>41</volume><fpage>28</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1037/a0038653</pub-id><pub-id pub-id-type="pmid">25621584</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S.</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Strijkers</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pupillary Responses to Words That Convey a Sense of Brightness or Darkness</article-title><source>Psychological Science</source><volume>28</volume><fpage>1116</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1177/0956797617702699</pub-id><pub-id pub-id-type="pmid">28613135</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pupillometry: Psychology, Physiology, and Function</article-title><source>Journal of Cognition</source><volume>1</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.5334/joc.18</pub-id><pub-id pub-id-type="pmid">31517190</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S</given-names></name><name><surname>Fabius</surname><given-names>J</given-names></name><name><surname>Van Heusden</surname><given-names>E</given-names></name><name><surname>Van der Stigchel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Safe and sensible preprocessing and baseline correction of pupil-size data</article-title><source>Behavior Research Methods</source><volume>50</volume><fpage>94</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.3758/s13428-017-1007-2</pub-id><pub-id pub-id-type="pmid">29330763</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morad</surname><given-names>Y</given-names></name><name><surname>Lemberg</surname><given-names>H</given-names></name><name><surname>Yofe</surname><given-names>N</given-names></name><name><surname>Dagan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Pupillography as an objective indicator of fatigue</article-title><source>Current Eye Research</source><volume>21</volume><fpage>535</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1076/0271-3683(200007)2111-ZFT535</pub-id><pub-id pub-id-type="pmid">11035533</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Stansbury</surname><given-names>DE</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title><source>NeuroImage</source><volume>105</volume><fpage>215</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.018</pub-id><pub-id pub-id-type="pmid">25451480</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Partala</surname><given-names>T</given-names></name><name><surname>Surakka</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pupil size variation as an indication of affective processing</article-title><source>International Journal of Human-Computer Studies</source><volume>59</volume><fpage>185</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/S1071-5819(03)00017-X</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Clifford</surname><given-names>CWG</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The functional impact of mental imagery on conscious perception</article-title><source>Current Biology</source><volume>18</volume><fpage>982</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.05.048</pub-id><pub-id pub-id-type="pmid">18583132</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evaluating the mind’s eye: the metacognition of visual imagery</article-title><source>Psychological Science</source><volume>22</volume><fpage>1535</fpage><lpage>1542</lpage><pub-id pub-id-type="doi">10.1177/0956797611417134</pub-id><pub-id pub-id-type="pmid">22058106</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>New Directions in Mental-Imagery Research: The Binocular-Rivalry Technique and Decoding fMRI Patterns</article-title><conf-name>Current Directions in Psychological Science</conf-name><pub-id pub-id-type="doi">10.1177/0963721414532287</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The human imagination: the cognitive neuroscience of visual mental imagery</article-title><source>Nature Reviews. Neuroscience</source><volume>20</volume><fpage>624</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1038/s41583-019-0202-9</pub-id><pub-id pub-id-type="pmid">31384033</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Keogh</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Redefining Visual Working Memory: A Cognitive-Strategy, Brain-Region Approach</article-title><source>Current Directions in Psychological Science</source><volume>28</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1177/0963721419835210</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pounder</surname><given-names>Z</given-names></name><name><surname>Jacob</surname><given-names>J</given-names></name><name><surname>Jacobs</surname><given-names>C</given-names></name><name><surname>Loveday</surname><given-names>C</given-names></name><name><surname>Towell</surname><given-names>T</given-names></name><name><surname>Silvanto</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mental rotation performance in aphantasia</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>1123</elocation-id><pub-id pub-id-type="doi">10.1167/18.10.1123</pub-id><pub-id pub-id-type="pmid">30544085</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2018">2018</year><data-title>R: A language and environment for statistical computing</data-title><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="http://www.r-project.org">http://www.r-project.org</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Training Visual Imagery: Improvements of Metacognition, but not Imagery Strength</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>224</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00224</pub-id><pub-id pub-id-type="pmid">22787452</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sack</surname><given-names>AT</given-names></name><name><surname>Camprodon</surname><given-names>JA</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The dynamics of interhemispheric compensatory processes in mental imagery</article-title><source>Science (New York, N.Y.)</source><volume>308</volume><fpage>702</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1126/science.1107784</pub-id><pub-id pub-id-type="pmid">15860630</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Martin</surname><given-names>VC</given-names></name><name><surname>Spreng</surname><given-names>RN</given-names></name><name><surname>Szpunar</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The future of memory: remembering, imagining, and the brain</article-title><source>Neuron</source><volume>76</volume><fpage>677</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.11.001</pub-id><pub-id pub-id-type="pmid">23177955</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Madore</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Remembering the past and imagining the future: Identifying and enhancing the contribution of episodic memory</article-title><source>Memory Studies</source><volume>9</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1177/1750698016645230</pub-id><pub-id pub-id-type="pmid">28163775</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwalm</surname><given-names>M</given-names></name><name><surname>Rosales Jubal</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Back to Pupillometry: How Cortical Network State Fluctuations Tracked by Pupil Dynamics Could Explain Neural Signal Variability in Human Cognitive Neuroscience</article-title><source>ENeuro</source><volume>4</volume><elocation-id>ENEURO.0293-16.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0293-16.2017</pub-id><pub-id pub-id-type="pmid">29379876</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shine</surname><given-names>JM</given-names></name><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>O’Callaghan</surname><given-names>C</given-names></name><name><surname>Muller</surname><given-names>AJ</given-names></name><name><surname>Lewis</surname><given-names>SJG</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Imagine that: elevated sensory strength of mental imagery in individuals with Parkinson’s disease and visual hallucinations</article-title><source>Proceedings. Biological Sciences</source><volume>282</volume><elocation-id>20142047</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2014.2047</pub-id><pub-id pub-id-type="pmid">25429016</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szpunar</surname><given-names>KK</given-names></name><name><surname>Watson</surname><given-names>JM</given-names></name><name><surname>McDermott</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural substrates of envisioning the future</article-title><source>PNAS</source><volume>104</volume><fpage>642</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1073/pnas.0610082104</pub-id><pub-id pub-id-type="pmid">17202254</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsujimura</surname><given-names>S</given-names></name><name><surname>Wolffsohn</surname><given-names>JS</given-names></name><name><surname>Gilmartin</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pupil responses associated with coloured afterimages are mediated by the magno-cellular pathway</article-title><source>Vision Research</source><volume>43</volume><fpage>1423</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(03)00145-7</pub-id><pub-id pub-id-type="pmid">12767310</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Wel</surname><given-names>P</given-names></name><name><surname>van Steenbergen</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pupil dilation as an index of effort in cognitive control tasks: A review</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>2005</fpage><lpage>2015</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1432-y</pub-id><pub-id pub-id-type="pmid">29435963</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>von der Malsburg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Saccades: An R package for detecting fixations in raw eye tracking data</data-title><version designator="v0.1">v0.1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.31799">https://doi.org/10.5281/zenodo.31799</ext-link></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>CA</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Yep</surname><given-names>R</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Comparing Pupil Light Response Modulation between Saccade Planning and Working Memory</article-title><source>Journal of Cognition</source><volume>1</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.5334/joc.33</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>CA</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural basis of location-specific pupil luminance modulation</article-title><source>PNAS</source><volume>115</volume><fpage>10446</fpage><lpage>10451</lpage><pub-id pub-id-type="doi">10.1073/pnas.1809668115</pub-id><pub-id pub-id-type="pmid">30249636</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Ley</surname><given-names>A</given-names></name><name><surname>Koch</surname><given-names>S</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Holmqvist</surname><given-names>K</given-names></name><name><surname>Alexa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computational discrimination between natural images based on gaze during mental imagery</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>13035</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-69807-0</pub-id><pub-id pub-id-type="pmid">32747683</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wassell</surname><given-names>J</given-names></name><name><surname>Rogers</surname><given-names>SL</given-names></name><name><surname>Felmingam</surname><given-names>KL</given-names></name><name><surname>Bryant</surname><given-names>RA</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sex hormones predict the sensory strength and vividness of mental imagery</article-title><source>Biological Psychology</source><volume>107</volume><fpage>61</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2015.02.003</pub-id><pub-id pub-id-type="pmid">25703930</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wicken</surname><given-names>M</given-names></name><name><surname>Keogh</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The critical role of mental imagery in human emotion: insights from fear-based imagery and aphantasia</article-title><source>Proceedings. Biological Sciences</source><volume>288</volume><elocation-id>20210267</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2021.0267</pub-id><pub-id pub-id-type="pmid">33715433</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Visual Imagery and Perception Share Neural Representations in the Alpha Frequency Band</article-title><source>Current Biology</source><volume>30</volume><fpage>2621</fpage><lpage>2627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.074</pub-id><pub-id pub-id-type="pmid">32531274</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeman</surname><given-names>AZJ</given-names></name><name><surname>Della Sala</surname><given-names>S</given-names></name><name><surname>Torrens</surname><given-names>LA</given-names></name><name><surname>Gountouna</surname><given-names>VE</given-names></name><name><surname>McGonigle</surname><given-names>DJ</given-names></name><name><surname>Logie</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Loss of imagery phenomenology with intact visuo-spatial task performance: a case of “blind imagination.”</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>145</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.08.024</pub-id><pub-id pub-id-type="pmid">19733188</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeman</surname><given-names>A</given-names></name><name><surname>Dewar</surname><given-names>M</given-names></name><name><surname>Della Sala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Lives without imagery - Congenital aphantasia</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>73</volume><fpage>378</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.05.019</pub-id><pub-id pub-id-type="pmid">26115582</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zokaei</surname><given-names>N</given-names></name><name><surname>Board</surname><given-names>AG</given-names></name><name><surname>Manohar</surname><given-names>SG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modulation of the pupillary response by the content of visual working memory</article-title><source>PNAS</source><volume>116</volume><fpage>22802</fpage><lpage>22810</lpage><pub-id pub-id-type="doi">10.1073/pnas.1909959116</pub-id><pub-id pub-id-type="pmid">31636213</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72484.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Serences</surname><given-names>John T</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.09.02.457617" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.02.457617"/></front-stub><body><p>This is a rigorous study of the relation between the vividness of visual imagery and the pupillary light response that can result from it. It provides evidence for the absence of imagery in individuals that self-report as aphantasic. The results will likely be of interest to researchers in a range of disciplines such as psychology, neuroscience and philosophy.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72484.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rolfs</surname><given-names>Martin</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Humboldt Universität zu Berlin</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Breedlove</surname><given-names>Jesse</given-names></name><role>Reviewer</role><aff><institution>UMN</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.02.457617">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.09.02.457617v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;The eyes have it: The pupillary light response as a physiological index of aphantasia, sensory and phenomenological imagery strength&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Chris Baker as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Martin Rolfs (Reviewer #2); Jessica Breedlove (Reviewer #3).</p><p>We all thought that the paper was interesting and that it addresses a timely topic with a novel approach. There is a lot to like here in terms of learning about mental imagery but also about people who seem to lack it. That said, you'll see that the reviewers brought up some substantive points, and, after consultation, we think that a revision might be possible to address these points but that it will require more analyses and most likely more data.</p><p>In consultation, we focused primarily on two issues: eye movements and demand characteristics.</p><p>With respect to eye movements, there are a number of reasons why we think further analyses are crucial:</p><p>1. Pupil size (in particular, constriction velocity, maximum constriction, and mean pupil change) depends stimulus eccentricity. (e.g., https://doi.org/10.1016/j.visres.2020.03.008)</p><p>2. Measured pupil size depends on eye position, and vice versa. The origin of these effects are measurement errors related to video-based eye tracking, as it this dependency is seen even for artificial, fixed-size pupils (e.g., https://doi.org/10.3758/s13428-011-0109-5; https://doi.org/10.1371/journal.pone.0111197).</p><p>3. There is now plenty of evidence that saccade preparation alters pupil size (https://doi.org/10.1037/a0038653; https://doi.org/10.3389/fnhum.2011.00097; https://doi.org/10.5334/joc.33; https://doi.org/10.1111/ejn.12883) and that both are controlled by largely-overlapping circuitry (e.g., https://doi.org/10.1073/pnas.1809668115).</p><p>4. Saccades greatly alter content in visual short-term memory (e.g., https://doi.org/10.1037/xlm0000338; reviewed in https://doi.org/10.1080/13506285.2020.1764156).</p><p>5. Eye movements can be correlated with imagery (e.g., https://doi.org/10.1162/jocn.1997.9.1.27); comparison between groups might thus provide additional indicators of imagery.</p><p>As a consequence, a comparison of eye position and saccade statistics is very important.</p><p>With respect to demand characteristics, R3 has some detailed suggestions, and we all feel that a revision of the design would potentially yield significant gains. It doesn't seem technically difficult to collect the data, but we also understand that collecting any data in person these days can be challenging. However, at the very least we think you should report on the mock trials in the binocular rivalry task (to show there wasn't a difference between groups in what you did check for, a BR bias), pull back on your claims accordingly, and provide a discussion of this limitation to the results.</p><p>Again, please see the reviews for more details, but we hope that you will find the comments helpful in deciding next steps for the paper.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. Please analyze and report eye movement parameters in each experiment and whether correlate with vividness of imagery. Please also clarify where observers were required to look during each phase of the experiments.</p><p>2. Please provide evidence that the correlations reported in Figure 1D persist at the level of individuals.</p><p>3. Please revise this Discussion section to clarify that such tests would always have to be combined with positive tests that show the commitment of participants to the task instructions.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>My largest (and maybe only) major concern is the possibility that aphantasic subjects were not attempting to imagine during both the main experiment and the BR task. This concern is somewhat exacerbated by the fact that subjects reached out to the lab to participate, making risk for demand characteristics high. The increase in pupil size in the aphantasic group in the set-size-4 condition is encouraging but this increase could also represent a number of other things. While I am hard pressed to come up with a way to ever eliminate this potential fully, I do think a bit more could be done to strengthen the argument against lack of participation and so I'm wary of the claim that this has been fully &quot;ruled out&quot; (line 322). My suggestions are as follows:</p><p>– Most ideally, there would be an addition of a probe task following the imagery period that would require subjects to report on the objects that they were supposed to remember and imagine (for example, like the one the in Zokaei 2019 which the authors cited). Both groups should be able to perform fairly well since, as the authors point out, aphantasic individuals can perform visual working memory tasks just as well as the general population. This would serve as a better indicator of effort and attention and require that subjects at the very least engage with the visual features of the stimuli (maybe they would have to adjust a probe's brightness until it matched the imagined one or adjust an angle to match that of the triangle, etc.).</p><p>– I also appreciate the use of a more objective measure of imagery vividness with the binocular rivalry task, but this suffers from the same issue. Aphantasic subjects could be not attempting to imagine or have an unintentional bias pushing them away from indicating the priming effect. The latter can be addressed using catch/mock trials as the authors did use. However if my math is correct, there were 24 trials total with only 3 mock trials. So while they did exclude subjects for making more than 1 incorrect response to mock trials, that still means that subjects could have shown a bias on 33% of the trials and still be included in the study. This might account for some of the findings if there were more aphantasic than control subjects who incorrectly answered a mock trial. My suggestion is then to increase the number and type of mock trials (all green and all red in addition to mixed) and (or at the very least) report the missed mock trials for each group.</p><p>– I would also suggest estimating the null hypothesis and significance bound through random sampling of the experiment under the null to show that the aphantasia subjects did not perform significantly worse than chance in Figure 2b.</p><p>Lines 124 – 127 &quot;These data provide novel evidence individuals can reliably evaluate the comparative vividness of single episodes of imagery. Further, these data demonstrate that the pupillary light response also tracks the phenomenological vividness of visual imagery from moment to moment.&quot; These two lines are circular. The authors are using changes in pupil size as evidence that subjects can reliably report their vividness and then using subject reports as evidence that pupil size is a good indicator of vividness. Suggest rewriting /rewording</p><p>Around line 256: the authors state there is no correlation between perception and imagery conditions. I initially found this confusing because if imagery is like faint vision, the pupil change during imagery of an object should mimic the pupil change during viewing of the same object. The point I believe they were making is that there is no correlation when all subjects are pooled together. I think this would be clearer if the authors first pointed out that there is a correlation between perception and imagery, but in the general population only (if true).</p><p>Line 312 – I believe the authors might be conflating psychogenic and fabrication and suggest that they revise the discussion to be clearer on this. The results speak to the latter but not so much the former. While psychogenic implies there might not be a traceable physical cause, it doesn't necessitate that the patient is acting intentionally and they likely experience it as real (for example, in non-epileptic seizures the brain is in fact not seizing, but the patient often isn't consciously convulsing and truly believes they are having a seizure). The lack of experience of imagery could lead to a lack of pupillary light response. A psychogenic source would not be inconsistent with the findings since the authors are not claiming to identify what is blocking or restricting the visual experience of imagery, just that there is a link between that experience and a physiological response. I suggest removing the claim that the study rules out a psychogenic source to aphantasia.</p><p>The timing diagram in Figure 1A seems off. The caption and methods say that the black screen rest was presented for 8s, but only 7s exist between the 2nd and 3rd dotted lines, and this doesn't match Figure 3. The reason for using a black screen for rest was also not clear. Together, these made the large dip in pupil size before imagery a bit confusing at first.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72484.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>We all thought that the paper was interesting and that it addresses a timely topic with a novel approach. There is a lot to like here in terms of learning about mental imagery but also about people who seem to lack it. That said, you'll see that the reviewers brought up some substantive points, and, after consultation, we think that a revision might be possible to address these points but that it will require more analyses and most likely more data.</p><p>In consultation, we focused primarily on two issues: eye movements and demand characteristics.</p><p>With respect to eye movements, there are a number of reasons why we think further analyses are crucial:</p><p>1. Pupil size (in particular, constriction velocity, maximum constriction, and mean pupil change) depends stimulus eccentricity. (e.g., https://doi.org/10.1016/j.visres.2020.03.008)</p><p>2. Measured pupil size depends on eye position, and vice versa. The origin of these effects are measurement errors related to video-based eye tracking, as it this dependency is seen even for artificial, fixed-size pupils (e.g., https://doi.org/10.3758/s13428-011-0109-5; https://doi.org/10.1371/journal.pone.0111197).</p><p>3. There is now plenty of evidence that saccade preparation alters pupil size (https://doi.org/10.1037/a0038653; https://doi.org/10.3389/fnhum.2011.00097; https://doi.org/10.5334/joc.33; https://doi.org/10.1111/ejn.12883) and that both are controlled by largely-overlapping circuitry (e.g., https://doi.org/10.1073/pnas.1809668115).</p><p>4. Saccades greatly alter content in visual short-term memory (e.g., https://doi.org/10.1037/xlm0000338; reviewed in https://doi.org/10.1080/13506285.2020.1764156).</p><p>5. Eye movements can be correlated with imagery (e.g., https://doi.org/10.1162/jocn.1997.9.1.27); comparison between groups might thus provide additional indicators of imagery.</p><p>As a consequence, a comparison of eye position and saccade statistics is very important.</p></disp-quote><p>We thank the reviewers and editor for their detailed and thoughtful points regarding eye movements and pupil diameter. We think the points raised are fair and we have added an extra supplementary analysis to the manuscript analysing the eccentricity and saccade data. Assessing the eccentricity data we found that in general the participants mostly maintained fixation throughout the experiment, and there was no significant difference between the groups in their average eccentricity values (see Figure 2 —figure supplement 2). There was also no significant correlations between eccentricity and the pupillary light response for either group during the imagery period (see Figure 3 —figure supplement 3). We also assessed whether the number of saccades during perception and imagery was different across groups or as a function of either set size or luminance. We found that there were no consistent differences in the number of saccades across the two groups for these variables (see Figure 2 —figure supplements 3 and 4). We believe taken together these results suggest that it is unlikely that our pupil diameter findings are driven by different eccentricity/fixation or saccades between the two groups and we thank the Reviewers for helping us address this important potential alternative explanation of our data. We believe that by showing eye-movements are unlikely to be driving the observed imaginary pupillary light reflex our paper has been strengthened.</p><disp-quote content-type="editor-comment"><p>With respect to demand characteristics, R3 has some detailed suggestions, and we all feel that a revision of the design would potentially yield significant gains. It doesn't seem technically difficult to collect the data, but we also understand that collecting any data in person these days can be challenging. However, at the very least we think you should report on the mock trials in the binocular rivalry task (to show there wasn't a difference between groups in what you did check for, a BR bias),</p></disp-quote><p>Thank you for these suggestions. We will endeavour to incorporate their excellent suggestions into future studies we run. Due to the current testing environment however it will take a long time to collect enough data to run a full new study as suggested. As suggested, we have added in the mock trial data to the supplementary material (Figure 3 —figure supplement 5) and additional information regarding these trials (see response to point 3 from Reviewer #3).</p><disp-quote content-type="editor-comment"><p>pull back on your claims accordingly, and provide a discussion of this limitation to the results.</p></disp-quote><p>We have toned down our claims and added in discussion of the limitations of the study, see the response to point 2 from Review #3.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. Please analyze and report eye movement parameters in each experiment and whether correlate with vividness of imagery. Please also clarify where observers were required to look during each phase of the experiments.</p></disp-quote><p>Participants were instructed to maintain fixation on the fixation cross, throughout the experiment, which has been clarified in Figure 1’s legend. However, following Reviewer 2’s suggestion, we have now added the analysis of eccentricity and saccade data to the supplementary materials (Figure 2 —figure supplement 2 and 3, Figure 3 —figure supplement 3, see also above response to general points). We found that there were no consistent differences between the groups making it unlikely that differences in eyeposition is driving the lack of a pupillary light response in the aphantasic population.</p><disp-quote content-type="editor-comment"><p>2. Please provide evidence that the correlations reported in Figure 1D persist at the level of individuals.</p></disp-quote><p>We are not sure we understand the Reviewer’s comment correctly. We do not report correlations for Figure 1D, but the results of 2 x 4 linear mixed-effects analysis. This model included subject identity as a random effect (see Methods) and therefore the effects reported were computed at the subject level. We report in the text, effects that are significant at the level of the sample. This does not exclude the possibility of inter-individual differences, but we are not sure how interpretable a single-subject analysis is in the current study.</p><disp-quote content-type="editor-comment"><p>3. Please revise this Discussion section to clarify that such tests would always have to be combined with positive tests that show the commitment of participants to the task instructions.</p></disp-quote><p>We have now added the following to the discussion regarding the importance of including these commitment controls to imagery studies:</p><p>“Future studies of visual imagery, and even more importantly when investigating aphantasia, should aim to include appropriate controls that allow for the identification of task engagement even when an individual doesn’t have visual imagery. This will allow researchers to exclude the alternate explanation that those individuals who do not show evidence of imagery are not just refusing to imagine or not completing the task correctly.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>My largest (and maybe only) major concern is the possibility that aphantasic subjects were not attempting to imagine during both the main experiment and the BR task. This concern is somewhat exacerbated by the fact that subjects reached out to the lab to participate, making risk for demand characteristics high. The increase in pupil size in the aphantasic group in the set-size-4 condition is encouraging but this increase could also represent a number of other things. While I am hard pressed to come up with a way to ever eliminate this potential fully, I do think a bit more could be done to strengthen the argument against lack of participation and so I'm wary of the claim that this has been fully &quot;ruled out&quot; (line 322).</p></disp-quote><p>We have re-worded parts of the discussion to tone down such claims:</p><p>“This demonstrates active task engagement suggesting that aphantasic individuals were most likely not simply ‘refusing’ to actively participate in the task due to demand characteristics or a belief that they are unable to imagine (de Vito and Bartolomeo, 2016).&quot;</p><p>“…However, we cannot fully rule out this possibility. Further, there was no evidence of an abnormal pupillary response in our aphantasic cohort when viewing images, thus it is likely the lack of an imaginary pupillary light response is due to their self-reported lack of visual imagery.”</p><disp-quote content-type="editor-comment"><p>My suggestions are as follows:</p><p>– Most ideally, there would be an addition of a probe task following the imagery period that would require subjects to report on the objects that they were supposed to remember and imagine (for example, like the one the in Zokaei 2019 which the authors cited). Both groups should be able to perform fairly well since, as the authors point out, aphantasic individuals can perform visual working memory tasks just as well as the general population. This would serve as a better indicator of effort and attention and require that subjects at the very least engage with the visual features of the stimuli (maybe they would have to adjust a probe's brightness until it matched the imagined one or adjust an angle to match that of the triangle, etc.).</p></disp-quote><p>Thank you for this excellent suggestion, which could be implemented in follow-up studies. However, as mentioned above, the current situation makes the planning of new experiments extremely uncertain. In addition, we did not find evidence suggesting aphantasic participants did not engage in the task. In fact, the modulation of pupil size by stimulus complexity suggests that these individuals engaged in the task, at least sufficiently for this effect to emerge. We agree that the methodology can be improved, and we are thankful for the Reviewer’s suggestion. But we do think our conclusions are warranted by the data at hand.</p><p>We have now further clarified our reasoning and outlined better the limitations of our study in the Discussion section. Indeed, we wanted the participants to focus more on holding the image in their mind and creating the most vivid image they were able to. Having them rate their vividness reinforces the imagery component of the task. If we had asked participants to remember the items instead, it is possible that some participants may have imagined the images as a mnemonic strategy. However, it is also possible that they may have also changed the type of strategy they used to remember the items which might not have involved imagery. This, in of itself, is interesting, however it was outside of the scope of this current study. In addition, if multiple participants were not usuing visual imagery to remember the images, this may have diluted the imaginary pupillary light response and replicating and extending this finding was central to the research question of this study. We have added in a limitations and future directions section to our discussion that speaks to these points:</p><p>“One limitation to our study is we did not include catch trails in our pupillometry task, i.e. we did not include trials where we asked participants to report on what image they had been asked to imagine. We did however include catch trials in our binocular rivalry task through presenting mock binocular rivalry trails. If aphantasic participants are showing a response bias we would expect see a reduction in these mock priming trails when compared to the control population, which we did not find (see Figure 3 —figure supplement 5). Adding catch trials to future experiments, in addition to setsize manipulations, may help to further confirm participant engagement. However, adding a simultaneous memory component to the task may lead some subjects to use a non-visual imagery strategy and as such, a reduction or dilution of the pupillary light response (see Pearson and Keogh (2019)). Future studies of visual imagery, and even more importantly when investigating aphantasia, should aim to include appropriate positive controls that allow for the identification of task engagement even when an individual doesn’t have visual imagery. This will allow researchers to exclude the alternate explanation that those individuals who do not show evidence of imagery are not just refusing to imagine or not completing the task correctly.”</p><disp-quote content-type="editor-comment"><p>– I also appreciate the use of a more objective measure of imagery vividness with the binocular rivalry task, but this suffers from the same issue. Aphantasic subjects could be not attempting to imagine or have an unintentional bias pushing them away from indicating the priming effect. The latter can be addressed using catch/mock trials as the authors did use. However if my math is correct, there were 24 trials total with only 3 mock trials. So while they did exclude subjects for making more than 1 incorrect response to mock trials, that still means that subjects could have shown a bias on 33% of the trials and still be included in the study. This might account for some of the findings if there were more aphantasic than control subjects who incorrectly answered a mock trial. My suggestion is then to increase the number and type of mock trials (all green and all red in addition to mixed) and (or at the very least) report the missed mock trials for each group.</p></disp-quote><p>We take the Reviewer’s point seriously. However, we note it is standard for mock or catch trials to represent a minority of the total number of trials. In fact, too frequent mock trials could make the subjects aware of the existence of the mock trials and fundamentally alter the results. In addition, we think our interpretation of the data (that the absence of priming is due to a lack of a reported imagery, in accordance with individuals’ self-reports) is more parsimonious than hypothesising the existence of an unintentional bias.</p><p>To give readers the clearest account of our data, we have now included the data for mock trials for both the controls and undergraduate students in the supplementary material in addition to VVIQ scores (Figure 3 —figure supplement 5 and Figure 2 —figure supplement 6). We have also further clarified how mock trials were calculated in the procedure as, upon re-reading the manuscript, we realised we did not include the number of trials participants completed in the binocular rivalry task, this has now been updated in the procedure section (84 real and 12 mock trials). The mock trials we use have a bespoke zig-zag walk border between the red and green patterns, and thus are not exactly the same each presentation, and appear slightly more red or green. We hope this explanation helps to clarify the mock trial data.</p><p>“Participants completed 2 blocks of 48 trials resulting in a total of 96 trials in total (84 real and 12 mock trials). The number of primed trials divided by the total number of trials (excluding mock trials and mixed percepts) was used to calculate a percent primed score for each participant. Mock trial priming was calcuated by giving a value to each mock trial as either 0 (reporting the catch trial as the opposite colour to that primed) , 50 (reporting the catch trial as being mixed) or 100% (reporting the catch trial to be the same as the cued image) (Figure 3 —figure supplement 5). These values are then averaged to get a priming value where 50% indicates no bias, while higher values indicate a bais towards reporting the mock trails as being the same as the imagined image, while negative numbers indicate a bias towards reporting the oppoiste image to that which was imagined.”</p><disp-quote content-type="editor-comment"><p>– I would also suggest estimating the null hypothesis and significance bound through random sampling of the experiment under the null to show that the aphantasia subjects did not perform significantly worse than chance in Figure 2b.</p></disp-quote><p>We have now run one-sample Bayesian t-tests to assess the evidence for aphantasic individuals performing significantly below chance in both the imaginary pupillary light response and binocular rivalry task (figure 1D). When assessing both groups there was no significant evidence that their priming scores were significantly lower than chance (Comparing scores to 50%: Aphantasic individuals BF = .162, Controls BF = .012). Similarly, a one sample t-test found no significant evidence that either group’s pupil difference scores were lower than chance (comparing to 0: Aphantasic individuals SS1 BF = .860, Aphantasic individuals SS4 BF = .187, Controls SS1 BF = .091, Controls SS4 BF = .050). These analysis have been added to figure 2B as Figure supplement 6 and the following has been added to the discussion:</p><p>“We ran Bayesian one-sample t-tests on the binocular rivalry and pupillary light response difference scores (see figure 2) comparing their performance to chance to see if there was any evidence they were performing significantly below chance. We found no significant evidence of below chance performance for either group on either the binocular rivalry or pupillometry imagery tasks (see Figure 2 —figure supplement 6). Taken together, with the set-size pupillary effect we observed in our aphantasic participants, it seems unlikely that our aphantasic individuals were not engaging in the tasks.”</p><disp-quote content-type="editor-comment"><p>Lines 124 – 127 &quot;These data provide novel evidence individuals can reliably evaluate the comparative vividness of single episodes of imagery. Further, these data demonstrate that the pupillary light response also tracks the phenomenological vividness of visual imagery from moment to moment.&quot; These two lines are circular. The authors are using changes in pupil size as evidence that subjects can reliably report their vividness and then using subject reports as evidence that pupil size is a good indicator of vividness. Suggest rewriting /rewording</p></disp-quote><p>We have now removed the first line from this paragraph (see manuscript).</p><disp-quote content-type="editor-comment"><p>Around line 256: the authors state there is no correlation between perception and imagery conditions. I initially found this confusing because if imagery is like faint vision, the pupil change during imagery of an object should mimic the pupil change during viewing of the same object. The point I believe they were making is that there is no correlation when all subjects are pooled together. I think this would be clearer if the authors first pointed out that there is a correlation between perception and imagery, but in the general population only (if true).</p></disp-quote><p>The reviewer is correct, that there is no correlation at a group-level between pupil responses during perception, specifically when looking at the control population. Having re-read this section we can see the confusion, because the point of the study is to show that imagery acts like weak perception by showing a pupillary light response. To clarify it was important to run this analysis as it might just be the case that pupil responses during imagery passively reflect the amount the pupils responded during the perception phase, rather than reflecting the effortful generation and maintenance of mental images. Specifically, imagery does act like perception through demonstrating a pupillary light reflex, however this analysis shows that this effect is not just due to lingering pupil responses to the previously seen images. We have added in further clarification of this point to the manuscript and we hope this distinction is now more readily understandable from the added text:</p><p>“Another possible explanation of our findings could be that the passive viewing of the images, and lingering visual persistence and sluggish pupil responses could be driving our results. If this is the case, we would expect that pupil diameter during the perception of the images should correlate with pupil size during imagery for the corresponding images. Further, the pupillary light reflex during perception should be more pronounced in the control than the aphantasic populations. To investigate this possible alternative explanation of our data we first assessed the correlations between pupil diameter during perception of bright and dark images for set size one and four and their corresponding imagery conditions (control participants only). We found there were no significant correlations between any of the perception and imagery conditions, or the difference scores for set size one and four (all p &gt;.40, see Figure 2 —figure supplement 1). This lack of a correlation suggests that those individuals who have the largest pupillary light response while viewing the images, do not also have the greatest imagery driven pupillary light responses, making it unlikely that the pupil response while seeing the image is driving the mental imagery pupillary response.”</p><disp-quote content-type="editor-comment"><p>Line 312 – I believe the authors might be conflating psychogenic and fabrication and suggest that they revise the discussion to be clearer on this. The results speak to the latter but not so much the former. While psychogenic implies there might not be a traceable physical cause, it doesn't necessitate that the patient is acting intentionally and they likely experience it as real (for example, in non-epileptic seizures the brain is in fact not seizing, but the patient often isn't consciously convulsing and truly believes they are having a seizure). The lack of experience of imagery could lead to a lack of pupillary light response. A psychogenic source would not be inconsistent with the findings since the authors are not claiming to identify what is blocking or restricting the visual experience of imagery, just that there is a link between that experience and a physiological response. I suggest removing the claim that the study rules out a psychogenic source to aphantasia.</p></disp-quote><p>We thank the reviewer for their thoughtful response, we have now removed this claim in the discussion.</p><disp-quote content-type="editor-comment"><p>The timing diagram in Figure 1A seems off. The caption and methods say that the black screen rest was presented for 8s, but only 7s exist between the 2nd and 3rd dotted lines, and this doesn't match Figure 3. The reason for using a black screen for rest was also not clear. Together, these made the large dip in pupil size before imagery a bit confusing at first.</p></disp-quote><p>Thank you for noting this error in Figure 1A. This has now been amended to correctly show an 8s black screen rest period. The reason for using a black screen for the rest period was firstly, to accurately replicate the experimental design used in Laeng and Sulutvedt, 2014 (The Eye Pupil Adjusts to Imaginary Light). This black screen is included to be a wash out period for after-images caused by stimuli during the perception phase of the trial. Additionally this black screen is used to bring the pupils to a similar diameter so that they are at a similar size for the beginning of the imagery component of the task.</p></body></sub-article></article>